2469
AINE: an immunological approach to data mining
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
false
[Algorithm design and analysis, Machine learning algorithms, Data analysis, Artificial immune systems, Laboratories, Electronic mail, Data mining, Viruses (medical), Immune system, Testing]
Mixtures of ARMA models for model-based time series clustering
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Clustering problems are central to many knowledge discovery and data mining tasks. However, most existing clustering methods can only work with fixed-dimensional representations of data patterns. In this paper we study the clustering of data patterns that are represented as sequences or time series possibly of different lengths. We propose a model-based approach to this problem using mixtures of autoregressive moving average (ARMA) models. We derive an expectation-maximization (EM) algorithm for learning the mixing coefficients as well as the parameters of component models. Experiments were conducted on simulated and real datasets. Results show that our method compares favorably with another method recently proposed by others for similar time series clustering problems.
[mixing coefficient learning, Uncertainty, Clustering methods, autoregressive moving average processes, data mining, Predictive models, knowledge discovery, datasets, Data mining, sequences, parameter learning, model-based time series clustering, ARMA model mixtures, Clustering algorithms, Multidimensional systems, expectation-maximization algorithm, time series, data pattern clustering, unsupervised learning, Computer science, autoregressive moving average models, Bayesian methods, pattern clustering, Hidden Markov models, Autoregressive processes]
Privacy-preserving collaborative filtering using randomized perturbation techniques
Third IEEE International Conference on Data Mining
None
2003
Collaborative filtering (CF) techniques are becoming increasingly popular with the evolution of the Internet. To conduct collaborative filtering, data from customers are needed. However, collecting high quality data from customers is not an easy task because many customers are so concerned about their privacy that they might decide to give false information. We propose a randomized perturbation (RP) technique to protect users' privacy while still producing accurate recommendations.
[Data privacy, perturbation techniques, collaborative filtering, data mining, Information filtering, Electronic mail, information filters, customer profiles, Databases, security of data, Perturbation methods, Collaboration, randomized perturbation techniques, customer data, Search engines, Information filters, data privacy, Internet, user privacy-preserving, Protection]
Detection of significant sets of episodes in event sequences
Fourth IEEE International Conference on Data Mining
None
2004
We present a method for a reliable detection of "unusual" sets of episodes in the form of many pattern sequences, scanned simultaneously for an occurrence as a subsequence in a large event stream within a window of size w. We also investigate the important special case of all permutations of the same sequence, which models the situation where the order of events in an episode does not matter, e.g., when events correspond to purchased market basket items. In order to build a reliable monitoring system, we compare obtained measurements to a reference model which in our case is a probabilistic model (Bernoulli or Markov). We first present a precise analysis that leads to a construction of a threshold. The difficulties of carrying out a probabilistic analysis for an arbitrary set of patterns, stems from the possible simultaneous occurrence of many members of the set as subsequences in the same window, the fact that the different patterns typically do have common symbols or common subsequences or possibly common prefixes, and that they may have different lengths. We also report on extensive experimental results, carried out on the Wal-Mart transactions database, that show a remarkable agreement with our theoretical analysis. This paper is an extension of our previous work where we laid out foundation for the problem of the reliable detection of an "unusual" episodes, but did not consider more than one episode scanned simultaneously for an occurrence.
[Sequences, pattern sequences, Event detection, probability, large event stream, reference model, Probability, market basket items, probabilistic analysis, Transaction databases, set theory, sequences, significant episode sets, event sequences, reliable monitoring system, Information security, Intrusion detection, Pattern analysis, National security, Wal-Mart transactions database, Monitoring, Contracts, pattern recognition]
Handling generalized cost functions in the partitioning optimization problem through sequential binary programming
Fifth IEEE International Conference on Data Mining
None
2005
This paper proposes a framework for cost-sensitive classification under a generalized cost function. By combining decision trees with sequential binary programming, we can handle unequal misclassification costs, constrained classification, and complex objective functions that other methods cannot. Our approach has two main contributions. First, it provides a new method for cost-sensitive classification that outperforms a traditional, accuracy-based method and some current cost-sensitive approaches. Second, and more important, our approach can handle a generalized cost function, instead of the simpler misclassification cost matrix to which other approaches are limited.
[pattern classification, partitioning optimization problem, Error analysis, generalized cost function, cost-sensitive classification, Linear matrix inequalities, Information management, mathematical programming, constrained classification, sequential binary programming, decision trees, Cost function, Systems engineering and theory, Functional programming, Decision trees, Classification tree analysis, Mathematical programming, Testing, objective function]
Data Mining Methods for Modeling Gene Expression Regulation and Their Applications
Sixth International Conference on Data Mining
None
2006
This paper demonstrates machine learning and data mining methods that can be developed and applied to analyzing large quantities of genomic information and gene expression data for characterizing and modeling gene expression regulation. In particular, there will be a discussion on some of the methods that have been developed for modeling gene expression regulation underlying abiotic stress (e.g., drought, low temperature and salinity) tolerance, for identifying gene responsive to particular environmental stress conditions, and for characterizing the functions of microRNA genes for stress regulation in model plant Arabidopsis thaliana.
[Genomics, Humans, data mining, genomic information, Data mining, Gene expression, Application software, machine learning, Stress, abiotic stress, gene expression regulation, Satellites, genetics, biology computing, Machine learning, learning (artificial intelligence), molecular biophysics, Bioinformatics, environmental stress conditions, Regression tree analysis, microRNA genes]
Efficient Data Sampling in Heterogeneous Peer-to-Peer Networks
Seventh IEEE International Conference on Data Mining
None
2007
Performing data-mining tasks such as clustering, classification, and prediction on large datasets is an arduous task and, many times, it is an infeasible task given current hardware limitations. The distributed nature of peer-to-peer databases further complicates this issue by introducing an access overhead cost in addition to the cost of sending individual tuples over the network. We propose a two-level sampling approach focusing on peer-to-peer databases for maximizing sample quality given a user-defined communication budget. Given that individual peers may have varying cardinality we propose an algorithm for determining the optimal sample rate (the percentage of tuples to sample from a peer) for each peer. We do this by analyzing the variance of individual peers, ultimately minimizing the total variance of the entire sample. By performing local optimization of individual peer sample rates we maximize approximation accuracy of the samples. We also offer several techniques for sampling in peer-to-peer databases given various amounts of known and unknown information about the network and its peers.
[approximation theory, user-defined communication budget, Costs, peer-to-peer computing, sample quality, Peer to peer computing, data mining, Data engineering, local optimization, Data mining, database management systems, overhead cost, Computer science, Histograms, peer-to-peer databases, Network topology, Aggregates, Distributed databases, Sampling methods, data sampling, data-mining tasks, heterogeneous peer-to-peer networks]
Unsupervised Cross-Domain Learning by Interaction Information Co-clustering
2008 Eighth IEEE International Conference on Data Mining
None
2008
In real-world data mining applications, one often has access to multiple datasets that are relevant to the task at hand. However, learning from such datasets can be difficult as they are often drawn from different domains, i.e., not identically distributed or differ in class or feature sets. In this paper, we consider the problem of learning the class structures %, unique and shared, of related domains in an unsupervised manner. Its setting generalizes that of information filtering and novelty detection applications which addresses both known and unknown classes. We propose a co-clustering framework for estimating and adapting the class structures of two related domains, {enabling the analyses of shared and unique classes.} We define an objective function using interaction information to take account of the divergence between the corresponding clusters of respective domains. We present an iterative algorithm which alternates object and feature clustering and converges to a local minimum of the objective function. We present empirical results using text benchmarks, comparing the proposed algorithm and combinations of conventional approaches in problems of partitioning documents and detecting unknown topics.
[Minority Clustering, co-clustering, novelty detection, interactive information, data mining, information filtering, unsupervised cross-domain learning, Data mining, domain adaptation, interaction information coclustering, unsupervised learning, information theoretic clustering]
Rule Ensembles for Multi-target Regression
2009 Ninth IEEE International Conference on Data Mining
None
2009
Methods for learning decision rules are being successfully applied to many problem domains, especially where understanding and interpretation of the learned model is necessary. In many real life problems, we would like to predict multiple related (nominal or numeric) target attributes simultaneously. Methods for learning rules that predict multiple targets at once already exist, but are unfortunately based on the covering algorithm, which is not very well suited for regression problems. A better solution for regression problems may be a rule ensemble approach that transcribes an ensemble of decision trees into a large collection of rules. An optimization procedure is then used for selecting the best (and much smaller) subset of these rules, and to determine their weights. Using the rule ensembles approach we have developed a new system for learning rule ensembles for multi-target regression problems. The newly developed method was extensively evaluated and the results show that the accuracy of multi-target regression rule ensembles is better than the accuracy of multi-target regression trees, but somewhat worse than the accuracy of multi-target random forests. The rules are significantly more concise than random forests, and it is also possible to create very small rule sets that are still comparable in accuracy to single regression trees.
[covering algorithm, Meetings, regression analysis, Conference management, multitarget random forests, Distributed computing, Publishing, Engineering management, Rule Learning, Universal Serial Bus, Books, learning (artificial intelligence), learning decision rules, optimization procedure, target attributes, multitarget regression rule ensembles, Regression, Computer science, learning rules, decision trees, learning rule ensembles, Multi-Target Prediction, multitarget regression trees, Portals, Software engineering]
Multi-agent Random Walks for Local Clustering on Graphs
2010 IEEE International Conference on Data Mining
None
2010
We consider the problem of local graph clustering where the aim is to discover the local cluster corresponding to a point of interest. The most popular algorithms to solve this problem start a random walk at the point of interest and let it run until some stopping criterion is met. The vertices visited are then considered the local cluster. We suggest a more powerful alternative, the multi-agent random walk. It consists of several ``agents'' connected by a fixed rope of length l. All agents move independently like a standard random walk on the graph, but they are constrained to have distance at most l from each other. The main insight is that for several agents it is harder to simultaneously travel over the bottleneck of a graph than for just one agent. Hence, the multi-agent random walk has less tendency to mistakenly merge two different clusters than the original random walk. In our paper we analyze the multi-agent random walk theoretically and compare it experimentally to the major local graph clustering algorithms from the literature. We find that our multi-agent random walk consistently outperforms these algorithms.
[Algorithm design and analysis, Machine learning algorithms, multi-agent systems, Communities, data mining, Complexity theory, Data mining, multiagent random walk, graph clustering, graphs, pattern clustering, Random Walk, Mixing Time, Clustering algorithms, Graph Clustering, Machine learning, Local Clustering]
Algorithms for Mining the Evolution of Conserved Relational States in Dynamic Networks
2011 IEEE 11th International Conference on Data Mining
None
2011
Dynamic networks have recently being recognized as a powerful abstraction to model and represent the temporal changes and dynamic aspects of the data underlying many complex systems. Significant insights regarding the stable relational patterns among the entities can be gained by analyzing temporal evolution of the complex entity relations. This can help identify the transitions from one conserved state to the next and may provide evidence to the existence of external factors that are responsible for changing the stable relational patterns in these networks. This paper presents a new data mining method that analyzes the time-persistent relations or states between the entities of the dynamic networks and captures all maximal non-redundant evolution paths of the stable relational states. Experimental results based on multiple datasets from real world applications show that the method is efficient and scalable.
[Algorithm design and analysis, Patents, Heuristic algorithms, data mining, relational patterns, Electronic mail, evolution, Data mining, Equations, temporal evolution mining, complex systems, time-persistent relations, dynamic networks, relational state, Silicon, Dynamic network, conserved relational states, complex entity relations, data mining method, maximal nonredundant evolution paths]
Self-Adjusting Models for Semi-supervised Learning in Partially Observed Settings
2012 IEEE 12th International Conference on Data Mining
None
2012
We present a new direction for semi-supervised learning where self-adjusting generative models replace fixed ones and unlabeled data can potentially improve learning even when labeled data is only partially-observed. We model each class data by a mixture model and use a hierarchical Dirichlet process (HDP) to model observed as well as unobserved classes. We extend the standard HDP model to accommodate unlabeled samples and introduce a new sharing strategy, within the context of Gaussian mixture models, that restricts sharing with covariance matrices while leaving the mean vectors free. Our research is mainly driven by real-world applications with evolving data-generating mechanisms where obtaining a fully-observed labeled data set is impractical. We demonstrate the feasibility of the proposed approach for semi-supervised learning in two such applications.
[data model, semisupervised learning, class discovery, self-adjusting generative models, partially observed settings, Gaussian mixture model, mean vectors, HDP, hierarchical Dirichlet process, unobserved classes, learning (artificial intelligence), sharing strategy, covariance matrices, real-world applications, gaussian mixture model, partially-observed data sets, data-generating mechanisms, Vectors, Covariance matrix, data models, hierarchical dirichlet process, Gaussian processes, Semisupervised learning, Gaussian mixture models, Data models, unlabeled data, semi-supervised learning, standard HDP model, Context modeling]
Tree-Like Structure in Large Social and Information Networks
2013 IEEE 13th International Conference on Data Mining
None
2013
Although large social and information networks are often thought of as having hierarchical or tree-like structure, this assumption is rarely tested. We have performed a detailed empirical analysis of the tree-like properties of realistic informatics graphs using two very different notions of tree-likeness: Gromov's d-hyperbolicity, which is a notion from geometric group theory that measures how tree-like a graph is in terms of its metric structure, and tree decompositions, tools from structural graph theory which measure how tree-like a graph is in terms of its cut structure. Although realistic informatics graphs often do not have meaningful tree-like structure when viewed with respect to the simplest and most popular metrics, e.g., the value of d or the tree width, we conclude that many such graphs do have meaningful tree-like structure when viewed with respect to more refined metrics, e.g., a size-resolved notion of d or a closer analysis of the tree decompositions. We also show that, although these two rigorous notions of tree-likeness capture very different tree-like structures in worst-case, for realistic informatics graphs they empirically identify surprisingly similar structure. We interpret this tree-like structure in terms of the recently-characterized "nested core-periphery" property of large informatics graphs, and we show that the fast and scalable k-core heuristic can be used to identify this tree-like structure.
[tree width, data mining, Gromov d-hyperbolicity, structural graph theory, tree decompositions, large social networks, hyperbolicity, tree-like structure, Facebook, Informatics, cut structure, network structure, Erbium, metric structure, network science, information network, trees (mathematics), hierarchical structure, Extraterrestrial measurements, Graph theory, information networks, k-core heuristic, geometric group theory, social networking (online), realistic informatics graphs, core-periphery property, Peer-to-peer computing, k-core]
Quick Detection of High-Degree Entities in Large Directed Networks
2014 IEEE International Conference on Data Mining
None
2014
In this paper we address the problem of quick detection of high-degree entities in large online social networks. Practical importance of this problem is attested by a large number of companies that continuously collect and update statistics about popular entities, usually using the degree of an entity as an approximation of its popularity. We suggest a simple, efficient, and easy to implement two-stage randomized algorithm that provides highly accurate solutions to this problem. For instance, our algorithm needs only one thousand API requests in order to find the top-100 most followed users, with more than 90% precision, in the online social network Twitter with approximately a billion of registered users. Our algorithm significantly outperforms existing methods and serves many different purposes such as finding the most popular users or the most popular interest groups in social networks. An important contribution of this work is the analysis of the proposed algorithm using Extreme Value Theory - a branch of probability that studies extreme events and properties of largest order statistics in random samples. Using this theory we derive an accurate prediction for the algorithm's performance and show that the number of API requests for finding the top-k most popular entities is sub linear in the number of entities. Moreover, we formally show that the high variability of the entities, expressed through heavy-tailed distributions, is the reason for the algorithm's efficiency. We quantify this phenomenon in a rigorous mathematical way.
[Algorithm design and analysis, large directed networks, approximation, online social network Twitter, application program interfaces, quick detection, social networks, online social networks, Twitter, randomized algorithm, API requests, Indexes, randomised algorithms, high-degree entities, top-k most popular entities, Approximation algorithms, Prediction algorithms, social networking (online), Peer-to-peer computing, extreme value theory, sublinear complexity, statistics]
Efficient Graphlet Counting for Large Networks
2015 IEEE International Conference on Data Mining
None
2015
From social science to biology, numerous applications often rely on graphlets for intuitive and meaningful characterization of networks at both the global macro-level as well as the local micro-level. While graphlets have witnessed a tremendous success and impact in a variety of domains, there has yet to be a fast and efficient approach for computing the frequencies of these subgraph patterns. However, existing methods are not scalable to large networks with millions of nodes and edges, which impedes the application of graphlets to new problems that require large-scale network analysis. To address these problems, we propose a fast, efficient, and parallel algorithm for counting graphlets of size k={3,4}-nodes that take only a fraction of the time to compute when compared with the current methods used. The proposed graphlet counting algorithms leverages a number of proven combinatorial arguments for different graphlets. For each edge, we count a few graphlets, and with these counts along with the combinatorial arguments, we obtain the exact counts of others in constant time. On a large collection of 300+ networks from a variety of domains, our graphlet counting strategies are on average 460x faster than current methods. This brings new opportunities to investigate the use of graphlets on much larger networks and newer applications as we show in the experiments. To the best of our knowledge, this paper provides the largest graphlet computations to date as well as the largest systematic investigation on over 300+ networks from a variety of domains.
[Algorithm design and analysis, parallel algorithms, Frequency-domain analysis, Scalability, graph theory, mathematics computing, parallel algorithm, visual analytics, parallel method, large-scale network analysis, motif counting, social science, graphlet, graphlet counting algorithm, Systematics, graph classification, Peer-to-peer computing, graphlet counting strategy, Kernel, graph kernel]
ADAGIO: Fast Data-Aware Near-Isometric Linear Embeddings
2016 IEEE 16th International Conference on Data Mining
None
2016
Many important applications, including signal reconstruction, parameter estimation, and signal processing in a compressed domain, rely on a low-dimensional representation of the dataset that preserves all pairwise distances between the data points and leverages the inherent geometric structure that is typically present. Recently Hedge, Sankaranarayanan, Yin and Baraniuk [19] proposed the first data-aware near-isometric linear embedding which achieves the best of both worlds. However, their method NuMax does not scale to large-scale datasets. Our main contribution is a simple, data-aware, near-isometric linear dimensionality reduction method which significantly outperforms a state-of-the-art method [19] with respect to scalability while achieving high quality near-isometries. Furthermore, our method comes with strong worst-case theoretical guarantees that allow us to guarantee the quality of the obtained nearisometry. We verify experimentally the efficiency of our method on numerous real-world datasets, where we find that our method (&lt;;10 secs) is more than 3000&#x00D7; faster than the state-of-the-art method [19] (&gt;9 hours) on medium scale datasets with 60000 datapoints in 784 dimensions. Finally, we use our method as a preprocessing step to increase the computational efficiency of a classification application and for speeding up approximate nearest neighbor queries.
[Algorithm design and analysis, Parameter estimation, signal processing, ADAGIO, query processing, fast data-aware near-isometric linear embeddings, data reduction, low-dimensional representation, Nonlinear distortion, Signal processing algorithms, approximate nearest neighbor queries, signal reconstruction, parameter estimation, NuMax, principal component analysis, Principal component analysis]
Many Heads are Better than One: Local Community Detection by the Multi-walker Chain
2017 IEEE International Conference on Data Mining
None
2017
Local community detection (or local clustering) is of fundamental importance in large network analysis. Random walk based methods have been routinely used in this task. Most existing random walk methods are based on the single-walker model. However, without any guidance, a single-walker may not be adequate to effectively capture the local cluster. In this paper, we study a multi-walker chain (MWC) model, which allows multiple walkers to explore the network. Each walker is influenced (or pulled back) by all other walkers when deciding the next steps. This helps the walkers to stay as a group and within the cluster. We introduce two measures based on the mean and standard deviation of the visiting probabilities of the walkers. These measures not only can accurately identify the local cluster, but also help detect the cluster center and boundary, which cannot be achieved by the existing single-walker methods. We provide rigorous theoretical foundation for MWC, and devise efficient algorithms to compute it. Extensive experimental results on a variety of real-world networks demonstrate that MWC outperforms the state-of-the-art local community detection methods by a large margin.
[Heating systems, single-walker model, cluster center, random walk methods, Multi-Walker Chain, random processes, network theory (graphs), Nonhomogeneous media, local community detection methods, Local community detection, local clustering, Convergence, Image segmentation, pattern clustering, Clustering algorithms, single-walker methods, MWC, Robustness, network analysis, multiwalker chain model, Mathematical model]
Text clustering based on good aggregations
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Text clustering typically involves clustering in a high dimensional space, which appears difficult with regard to virtually all practical settings. In addition, given a particular clustering result it is typically very hard to come up with a good explanation of why the text clusters have been constructed the way they are. We propose a new approach for applying background knowledge (in terms of an ontology) during preprocessing in order to improve clustering results and allow for selection between results. The results may be distinguished and explained by the corresponding selection of concepts in the ontology. Our results compare favourably with a sophisticated baseline preprocessing strategy.
[text analysis, high dimensional space, Navigation, Heuristic algorithms, Clustering methods, text clustering, preprocessing, Humans, data mining, Ontologies, Knowledge management, Measurement standards, pattern clustering, background knowledge, Clustering algorithms, Web pages, data warehouses, good aggregations, ontology]
Discovering representative episodal association rules from event sequences using frequent closed episode sets and event constraints
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Discovering association rules from time-series data is an important data mining problem. The number of potential rules grows quickly as the number of items in the antecedent grows. It is therefore difficult for an expert to analyze the rules and identify the useful. An approach for generating representative association rules for transactions that uses only a subset of the set of frequent itemsets called frequent closed itemsets was presented by Saquer and Deogun (2000). We employ formal concept analysis to develop the notion of frequent closed episodes. The concept of representative association rules is formalized in the context of event sequences. Applying constraints to target highly, significant rules further reduces the number of rules. Our approach results in a significant reduction of the number of rules generated, while maintaining the minimum set of relevant association rules and retaining the ability to generate the entire set of association rules with respect to the given constraints. We show how our method can be used to discover associations in a drought risk management decision support system and use multiple climatology datasets related to automated weather stations.
[Decision support systems, data mining, automated weather stations, Data mining, event sequences, representative episodal association rule discovery, multiple climatology datasets, Itemsets, Clustering algorithms, time series data, Pattern analysis, event constraints, frequent itemsets, risk management, hydrology, Government, geophysics computing, time series, frequent closed episode sets, Time measurement, Association rules, transactions, formal concept analysis, decision support systems, climatology, Risk management, drought risk management decision support system]
Dependency derivation in industrial process data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In many industrial processes, finding dependencies and the creation of dependency graphs can increase the understanding of the system significantly. This knowledge can then be used for further optimization and variable selection. Most of the measured attributes in these cases come in the form of time series. There are several ways of determining correlation between series, most of them suffering from specific problems when applied to real-world data. Here, a well performing measure based on the mutual information rate is derived and discussed with results from both synthetic and real data.
[Performance evaluation, dependency derivation, Delay effects, data mining, time series, Time measurement, Entropy, Probability distribution, industrial process data, manufacturing data processing, Computer science, mutual information rate, optimization, Gain measurement, Computer industry, variable selection, Mutual information, dependency graphs, Disk recording]
Significance tests for patterns in continuous data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The authors consider the question of uncertainty of detected patterns in data mining. In particular, we develop statistical tests for patterns found in continuous data, indicating the significance of these patterns in terms of the probability that they have occurred by chance. We examine the performance of these tests on patterns detected in several large data sets, including a data set describing the locations of earthquakes in California and another describing flow cytometry measurements on phytoplankton.
[Sequences, Uncertainty, Buildings, data mining, probability, Probability, earthquake locations, flow cytometry measurements, Educational institutions, uncertainty handling, Mathematics, California, Data mining, continuous data patterns, statistical tests, Earthquakes, very large databases, large data sets, detected pattern uncertainty, phytoplankton, statistical analysis, Pattern analysis, significance tests, Testing]
Combining labeled and unlabeled data for text classification with a large number of categories
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
We develop a framework to incorporate unlabeled data in the error-correcting output coding (ECOC) setup by decomposing multiclass problems into multiple binary problems and then use co-training to learn the individual binary classification problems. We show that our method is especially useful for classification tasks involving a large number of categories where co-training doesn't perform very well by itself and when combined with ECOC, outperforms several other algorithms that combine labeled and unlabeled data for text classification in terms of accuracy, precision-recall tradeoff, and efficiency.
[pattern classification, text analysis, error correction codes, multiple binary problems, binary classification problems, precision-recall tradeoff, accuracy, Classification algorithms, text classification, error correcting output coding setup, multiclass problems, Text categorization, Supervised learning, labeled data, unlabeled data, co-training, categories, Labeling, learning (artificial intelligence), Testing]
Data analysis and mining in ordered information tables
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Many real-world problems deal with ordering objects instead of classifying objects, although the majority of the research in machine learning and data mining has been focused on the latter. For the modeling of ordering problems, we generalize the notion of information tables to ordered information tables by adding order relations on attribute values. The problem of mining ordering rules is formulated as finding associations between the orderings of attribute values and the overall ordering of objects. An ordering rule may state, for example, that "if the value of an object x on an attribute a is ordered ahead of the value of another object y on the same attribute, then x is ordered ahead of y". For mining ordering rules, we first transform an ordered information table into binary information, and then apply any standard machine learning and data mining algorithms. As an illustration, we analyze in detail the Maclean's university ranking for the year 2000.
[Warranties, associations, Data analysis, Machine learning algorithms, data analysis, Consumer products, data mining, ordering rules, educational administrative data processing, Electronic mail, Data mining, object ordering, machine learning, attribute values, Maclean's university rankings, Computer science, Rough sets, Machine learning, order relations, Manufacturing, learning (artificial intelligence), ordered information tables, binary information]
Better rules, fewer features: a semantic approach to selecting features from text
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The choice of features used to represent a domain has a profound effect on the quality of the model produced; yet, few researchers have investigated the relationship between the features used to represent text and the quality of the final model. We explored this relationship for medical texts by comparing association rules based on features with three different semantic levels: (1) words (2) manually assigned keywords and (3) automatically selected medical concepts. Our preliminary findings indicate that bi-directional association rules based on concepts or keywords are more plausible and more useful than those based on word features. The concept and keyword representations also required 90% fewer features than the word representation. This drastic dimensionality reduction suggests that this approach is well suited to large textual corpora of medical text, such as parts of the Web.
[text analysis, semantic approach, automatically selected medical concepts, computational linguistics, data mining, large textual corpus, Predictive models, text representation, association rules, bibliographic systems, Data mining, dimensionality reduction, bi-directional association rules, Breast neoplasms, feature selection, word features, word representation, Natural languages, manually assigned keywords, Medical treatment, words, Breast cancer, medical information systems, Association rules, Diseases, Computer science, keyword representations, Web, Bidirectional control, medical texts, semantic levels]
Mining generalized association rules for sequential and path data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
While association rules for set data use and describe relations between parts of set valued objects completely, association rules for sequential data are restricted by specific interpretations of the subsequence relation: contiguous subsequences describe local features of a sequence valued object, noncontiguous subsequences its global features. We model both types of features with generalized subsequences that describe local deviations by wild cards, and present a new algorithm of a priori type for mining all generalized subsequences with prescribed minimum support from a given database of sequences. Furthermore we show that the given algorithm automatically takes into account an eventually underlying graph structure, i.e., is applicable to path data also.
[Algorithm design and analysis, local features, data mining, generalized association rule mining, subsequence relation, Data mining, Association rules, sequences, contiguous subsequences, database theory, global features, wild cards, Databases, sequential data, noncontiguous subsequences, path data, sequence valued object, database sequences, set valued objects, graph structure, State estimation]
An online algorithm for segmenting time series
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In recent years, there has been an explosion of interest in mining time-series databases. As with most computer science problems, representation of the data is the key to efficient and effective solutions. One of the most commonly used representations is piecewise linear approximation. This representation has been used by various researchers to support clustering, classification, indexing and association rule mining of time-series data. A variety of algorithms have been proposed to obtain this representation, with several algorithms having been independently rediscovered several times. In this paper, we undertake the first extensive review and empirical comparison of all proposed techniques. We show that all these algorithms have fatal flaws from a data-mining perspective. We introduce a novel algorithm that we empirically show to be superior to all others in the literature.
[data representation, Piecewise linear approximation, data mining, empirical comparison, Data mining, time-series database mining, piecewise linear approximation, Databases, reviews, Clustering algorithms, review, Piecewise linear techniques, Change detection algorithms, indexing, piecewise linear techniques, online algorithm, time series segmentation, time series, Explosions, Association rules, classification, association rule mining, Computer science, online operation, clustering, Indexing]
Interestingness preprocessing
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
As the size of databases increases, the number of rules mined from them also increases, often to an extent that overwhelms users. To address this problem, an important part of the knowledge discovery in databases (KDD) process is dedicated to determining which of these patterns is interesting. In this paper, we define the interestingness pre-processing (IPP) step and introduce a new framework for interestingness analysis. In a similar fashion to data pre-processing, this pre-processing should always be applied prior to interestingness processing. A strict requirement, and the biggest challenge, in defining IPP techniques is that the pre-processing does not eliminate any potentially interesting patterns. That is, the pre-processing methods must be domain-, task- and user-independent. This property differentiates the pre-processing methods from existing interestingness criteria and, since they can be applied automatically, makes them very useful. This generic nature also makes them rare: pre-processing methods are very challenging to define. We define the first two IPP techniques (overfitting and transition) and present empirical results of applying them to six databases. The results indicate that the IPP step is very powerful: in most cases, an average of half the rules mined were eliminated by the application of the two IPP techniques. These results are particularly significant since no user interaction is required to achieve them.
[overfitting, Humans, data mining, task-independent methods, interestingness pre-processing, knowledge discovery, importance sampling, Data mining, Association rules, interestingness analysis framework, transition, database size, potentially interesting patterns, Databases, rule mining, very large databases, user-independent methods, domain-independent methods]
Efficient yet accurate clustering
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The authors show that most hierarchical agglomerative clustering (HAC) algorithms follow a 90-10 rule where roughly 90% iterations from the beginning merge cluster pairs with dissimilarity less than 10% of the maximum dissimilarity. We propose two algorithms: 2-phase and nested, based on partially overlapping partitioning (POP). To handle high-dimensional data efficiently, we propose a tree structure particularly suitable for POP. Extensive experiments show that the proposed algorithms reduce the time and memory requirement of existing HAC algorithms significantly without compromising accuracy.
[HAC algorithms, tree structure, efficient accurate clustering, high-dimensional data, World Wide Web, memory requirement, Data mining, 90-10 rule, very large databases, Clustering algorithms, Robustness, Computational efficiency, tree data structures, Labeling, Tree data structures, data analysis, cluster pair merging, partially overlapping partitioning, Partitioning algorithms, POP, pattern clustering, maximum dissimilarity, Sampling methods, Iterative algorithms, hierarchical agglomerative clustering algorithms]
Who links to whom: mining linkage between Web sites
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Previous studies of the Web graph structure have focused on the graph structure at the level of individual pages. In actuality the Web is a hierarchically nested graph, with domains, hosts and Web sites introducing intermediate levels of affiliation and administrative control. To better understand the growth of the Web we need to understand its macro-structure, in terms of the linkage between Web sites. We approximate this by studying the graph of the linkage between hosts on the Web. This was done based on snapshots of the Web taken by Google in Oct 1999, Aug 2000 and Jun 2001. The connectivity between hosts is represented by a directed graph, with hosts as nodes and weighted edges representing the count of hyperlinks between pages on the corresponding hosts. We demonstrate how such a "hostgraph" can be used to study connectivity properties of hosts and domains over time, and discuss a modified "copy model" to explain observed link weight distributions as a function of subgraph size. We discuss changes in the Web over time in the size and connectivity of Web sites and country domains. We also describe a data mining application of the hostgraph: a related host finding algorithm which achieves a precision of 0.65 at rank 3.
[data mining, hypermedia, subgraph size, hostgraph, Data mining, host finding algorithm, connectivity properties, Uniform resource locators, intermediate levels, observed link weight distributions, Bibliometrics, directed graph, administrative control, macro-structure, information resources, Web graph structure, Web page design, Navigation, Citation analysis, weighted edges, Couplings, Computer science, country domains, Aggregates, directed graphs, hierarchically nested graph, Web pages, modified copy model, Web sites, data mining application, hyperlinks]
Metric rule generation with septic shock patient data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The article present an application of metric rule generation in the domain of medical research. We consider intensive care unit patients developing a septic shock during their stay at the hospital. To analyse the patient data, rule generation is embedded in a medical data mining cycle. For rule generation, we improve an architecture based on a growing trapezoidal basis function network.
[septic shock patient data, Data analysis, Electric shock, Neurons, Data preprocessing, data mining, patient data analysis, intensive care unit patients, medical information systems, Data mining, medical research, growing trapezoidal basis function network, Abdomen, metric rule generation, medical data mining cycle, Hospitals, Databases, medical expert systems, Fuzzy neural networks, learning (artificial intelligence), hospital, Immune system]
Interestingness, peculiarity, and multi-database mining
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In order to discover new, surprising, interesting patterns hidden in data, peculiarity oriented mining and multidatabase mining are required. In the paper, we introduce peculiarity rules as a new class of rules, which can be discovered from a relatively low number of peculiar data by searching the relevance among the peculiar data. We give a formal interpretation and comparison of three classes of rules: association rules, exception rules, and peculiarity rules, as well as describe how to mine more interesting peculiarity rules in multiple databases.
[hidden pattern discovery, Acoustic testing, peculiarity oriented mining, Filtration, data mining, Relational databases, peculiarity rules, association rules, Transaction databases, Electronic mail, Data mining, Association rules, relevance, Computer science, Distributed processing, Image databases, interestingness, distributed databases, multidatabase mining, exception rules]
Incremental support vector machine construction
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
SVMs (support vector machines) suffer from the problem of large memory requirement and CPU time when trained in batch mode on large data sets. We overcome these limitations, and at the same time make SVMs suitable for learning with data streams, by constructing incremental learning algorithms. We first introduce and compare different incremental learning techniques, and show that they are capable of producing performance results similar to the batch algorithm, and in some cases superior condensation properties. We then consider the problem of training SVMs using stream data. Our objective is to maintain an updated representation of recent batches of data. We apply incremental schemes to the problem and show that their accuracy is comparable to the batch algorithm.
[learning automata, incremental support vector machine construction, CPU time, batch mode, condensation properties, updated representation, data streams, stream data, large memory requirement, SVMs, incremental schemes, very large databases, Training data, batch processing (computers), large data sets, Telephony, Marketing and sales, learning (artificial intelligence), data analysis, batch algorithm, Partitioning algorithms, incremental learning algorithms, Support vector machines, Computer science, Support vector machine classification, Solids]
Closing the loop: heuristics for autonomous discovery
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Autonomous discovery systems will be able to peruse very large databases more thoroughly than people can. In a companion paper by G.R. Livingston et al. (see ibid., p.385-92, 2001), we describe a general framework for autonomous systems. We present and evaluate heuristics for use in this framework. Although these heuristics were designed for a prototype system, we believe they provide good initial solutions to problems encountered when implementing fully autonomous discovery systems. As such, these heuristics may be used as the starting point for future research into fully autonomous discovery systems.
[domain-specific knowledge, rule-induction targets, Buildings, data mining, Crystallization, Patient rehabilitation, autonomous discovery systems, Minerals, HAMB, autonomous discovery heuristics, justification based framework, Databases, heuristic programming, Lungs, very large databases, Prototypes, domain-independent heuristics, Mining industry]
Mining decision trees from data streams in a mobile environment
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
This paper presents a novel Fourier analysis-based technique to aggregate, communicate and visualize decision trees in a mobile environment. A Fourier representation of a decision tree has several useful properties that are particularly useful for mining continuous data streams from small mobile computing devices. This paper presents algorithms to compute the Fourier spectrum of a decision tree and vice versa. It offers a framework to aggregate decision trees in their Fourier representations. It also describes a touchpad/ticker-based approach to visualize decision trees using their Fourier spectrum and an implementation for PDAs.
[data mining, microcomputer applications, ticker-based approach, Data mining, mobile computing, touchpad, Wireless networks, data visualisation, Fourier spectrum, notebook computers, tree data structures, Decision trees, Personal digital assistants, continuous data streams, personal digital assistants, decision tree visualization, mobile environment, decision tree aggregation, decision tree mining, Fourier analysis, Fourier representation, Computer science, Aggregates, Cellular phones, Data visualization, decision trees, Fourier transform spectra, Time factors, decision tree communication, small mobile computing devices, Mobile computing]
FIExPat: flexible extraction of sequential patterns
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
This paper addresses sequential data mining, a sub-area of data mining where the data to be analyzed is organized in sequences. In many problem domains a natural ordering exists over data. Examples of sequential databases (SDBs) include: (a) collections of temporal data sequences, such as chronological series of daily stock indices or multimedia data (sound, music, video, etc.); and (b) macromolecule banks, where amino acid or proteic sequences are represented as strings. In a SDB it is often valuable to detect regularities through one or several sequences. In particular, finding exact or approximate repetitions of segments can be utilized directly (e.g. for determining the biochemical activity of a protein region) or indirectly, e.g. for prediction in finance. To this end, we present concepts and an algorithm for automatically extracting sequential patterns from a sequential database. Such a pattern is defined as a group of significantly similar segments from one or several sequences. Appropriate functions for measuring similarity between sequence segments are proposed, generalizing the edit distance framework. There is a trade off between flexibility, particularly in sequence data representation and in associated similarity metrics, and computational efficiency. We designed the FlExPat algorithm to satisfactorily cope with this trade-off. FlExPat's complexity is in practice lesser than quadratic in the total length of the SDB analyzed, while allowing high flexibility. Some experimental results obtained with FlExPat on music data are presented and commented.
[Algorithm design and analysis, amino acid sequences, sequential databases, data mining, music data, sequential data mining, Multimedia databases, Amino acids, computational efficiency, temporal data sequences, Data mining, sequences, regularity detection, Proteins, chronological series, music, strings, Computational efficiency, flexible sequential pattern extraction, macromolecule banks, proteic sequences, pattern recognition, Sequences, Data analysis, multimedia data, multimedia databases, Finance, FlExPat, sequence segment similarity, similarity metrics, daily stock indices, Music, segment repetition]
Efficient determination of dynamic split points in a decision tree
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
We consider the problem of choosing split points for continuous predictor variables in a decision tree. Previous approaches to this problem typically either: (1) discretize the continuous predictor values prior to learning, or (2) apply a dynamic method that considers all possible split points for each potential split. We describe a number of alternative approaches that generate a small number of candidate split points dynamically with little overhead. We argue that these approaches are preferable to pre-discretization, and provide experimental evidence that they yield probabilistic decision trees with the same prediction accuracy as the traditional dynamic approach. Furthermore, because the time to grow a decision tree is proportional to the number of split points evaluated, our approach is significantly faster than the traditional dynamic approach.
[Heuristic algorithms, dynamic approach, Probability distribution, potential split, candidate split points, split points, probabilistic decision trees, Learning systems, Degradation, decision tree, Prediction algorithms, Decision trees, learning (artificial intelligence), dynamic split point determination, Classification tree analysis, Testing, continuous predictor value discretization, traditional dynamic approach, data analysis, continuous predictor variables, probability, dynamic method, Bayesian methods, decision trees, Context modeling]
Provably fast training algorithms for support vector machines
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Support vector machines are a family of data analysis algorithms based on convex quadratic programming. We focus on their use for classification: in that case, the SVM algorithms work by maximizing the margin of a classifying hyperplane in a feature space. The feature space is handled by means of kernels if the problems are formulated in dual form. Random sampling techniques successfully used for similar problems are studied. The main contribution is a randomized algorithm for training SVMs, for which we can formally prove an upper bound on the expected running time that is quasilinear on the number of data points. To our knowledge, this is the first algorithm for training SVMs in dual formulation and with kernels for which such a quasilinear time bound has been formally proved.
[learning automata, data analysis algorithms, provably fast training algorithms, Data mining, quasilinear time bound, Space technology, classifying hyperplane, kernels, learning (artificial intelligence), Kernel, Biomedical engineering, Data analysis, sampling methods, data analysis, support vector machines, feature space, data points, convex programming, convex quadratic programming, randomized algorithm, Quadratic programming, quadratic programming, SVM algorithms, randomised algorithms, Support vector machines, Upper bound, quasilinear, Support vector machine classification, Sampling methods, random sampling techniques, expected running time, dual formulation]
A fast algorithm to cluster high dimensional basket data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Clustering is a data mining problem that has received significant attention by the database community. Data set size, dimensionality and sparsity have been identified as aspects that make clustering more difficult. The article introduces a fast algorithm to cluster large binary data sets where data points have high dimensionality and most of their coordinates are zero. This is the case with basket data transactions containing items, that can be represented as sparse binary vectors with very high dimensionality. An experimental section shows performance, advantages and limitations of the proposed approach.
[Maximum likelihood estimation, Multidimensional systems, Statistical analysis, data mining, data points, Educational institutions, Partitioning algorithms, Sparse matrices, Data mining, Association rules, basket data transactions, Databases, pattern clustering, very large databases, data set dimensionality, Clustering algorithms, high dimensional basket data clustering, database community, sparse binary vectors, large binary data set clustering, fast algorithm, data mining problem, data set size]
Using boosting to simplify classification models
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Ensemble classification techniques such as bagging, boosting and arcing algorithms have been shown to lead to reduced classification errors on unseen cases and seem immune to the problem of overfitting. Several explanations for the reduction in generalisation error have been presented, with recent authors defining and applying diagnostics such as "edge" and "margin". These measures provide insight into the behaviour of ensemble classifiers, but can they be exploited further? In this paper, a four-stage classification procedure in introduced, which is based on an extension of edge and margin analysis. This new procedure allows inverse sub-contexts and difficult border regions to be detected using properties of the edge distribution. It is widely known that ensemble classifiers 'balance' the margin as the number of iterations increases. However, by exploiting this balancing property and flagging observations whose edges (and margins) are not 'balanced', data sets can often be partitioned into sub contexts and the classification can be made more robust as confounding within a data set is removed. In the majority of cases, the sub-contexts detected are inverse to each other or, quite possibly, the smaller sub-context contains mis-labelled observations. The majority of classification techniques have not been adapted to detect contexts within a data set, and the generalisation error reported in studies to date is based on the entire data set and can be improved by partitioning the data set in question. The aim of this study is to move towards interpretability, and it is shown that, by training on a sub-set of the original training data, we gain simplicity of models and reduced generalisation error.
[Error analysis, ensemble classifiers, unseen cases, data mining, Mathematics, Data mining, edge analysis, Acoustical engineering, border region detection, classification model simplification, margin balancing, training data, interpretability, Robustness, classification error, pattern classification, overfitting, boosting, error analysis, generalisation error, inverse sub-contexts, Boosting, diagnostic measures, generalisation (artificial intelligence), margin analysis, Information technology, modelling, iterations, data set partitioning, Computer science, mislabelled observations, edge distribution, Computer errors, Australia, confounding removal]
Knowledge discovery from diagrammatically represented data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Knowledge discovery from diagrammatic data can be facilitated by a language that permits queries on such data. Such a language (Diagrammatic SQL) is being developed to expedite the development of an autonomous artificial intelligent agent with a capacity to deal with diagrammatic information. This language is described and examples of how it can be used to facilitate diagrammatic data mining are detailed.
[autonomous artificially intelligent agent, Buildings, Humans, data mining, Gray-scale, diagrammatic information, diagrammatic data mining, Information retrieval, diagrams, knowledge discovery, Computational Intelligence Society, Diagrammatic SQL, Data mining, Automobiles, Database languages, software agents, Intelligent agent, Information systems, SQL, knowledge representation, diagrammatically represented data, visual programming]
On mining general temporal association rules in a publication database
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In this paper, we explore a new problem of mining general temporal association rules in publication databases. In essence, a publication database is a set of transactions where each transaction T is a set of items, each containing an individual exhibition period. The current model of association rule mining is not able to handle a publication database due to the following fundamental problems: (1) lack of consideration of the exhibition period of each individual item; and (2) lack of an equitable support counting basis for each item. To remedy this, we propose an innovative algorithm, progressive-partition-miner (PPM), to discover general temporal association rules in a publication database. The basic idea of PPM is to first partition the publication database into exhibition periods of items and then progressively accumulate the occurrence count of each candidate 2-itemset based on the intrinsic partitioning characteristics. PPM is also designed to employ a filtering threshold in each partition to prune out those cumulatively infrequent 2-itemsets at an early stage. Explicitly, the execution time of PPM is, in orders of magnitude, smaller than those required by schemes which are directly extended from existing methods.
[transaction processing, 2-itemset, filtering threshold, data mining, Transaction databases, Partitioning algorithms, Electronic mail, marketing data processing, Data mining, Association rules, transactions, exhibition periods, Itemsets, progressive-partition-miner algorithm, very large databases, general temporal association rule mining, publication database]
A synchronization based algorithm for discovering ellipsoidal clusters in large datasets
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
This paper introduces a new scalable approach to clustering based on the synchronization of pulse-coupled oscillators. Each data point is represented by an integrate-and-fire oscillator and the interaction between oscillators is defined according to the relative similarity between the points. The set of oscillators self-organizes into stable phase-locked subgroups. Our approach proceeds by loading only a subset of the data and allowing it to self-organize. Groups of synchronized oscillators are then summarized and purged from memory. We show that our method is robust, scales linearly and can determine the number of clusters. The proposed approach is empirically evaluated with several synthetic data sets and is used to segment large color images.
[cluster number determination, self-organization, data mining, data subset loading, Multimedia databases, large color image segmentation, Mathematics, Data mining, data point representation, oscillator interactions, scaling phenomena, image segmentation, self-organising feature maps, Clustering algorithms, ellipsoidal cluster discovery, large data sets, pulse-coupled oscillator synchronization, Robustness, image colour analysis, scalable clustering approach, stability, oscillators, robust method, Data analysis, integrate-and-fire oscillators, Color, self-adjusting systems, synchronization-based algorithm, Partitioning algorithms, stable phase-locked subgroups, Oscillators, synchronisation, Image segmentation, pattern clustering, relative point similarity, synchronized oscillator group summarization, memory purging]
Inexact field learning: an approach to induce high quality rules from low quality data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
To avoid low quality problems caused by low quality data, the paper introduces an inexact field learning approach which derives rules by working on the fields of attributes with respect to classes, rather than on individual point values of attributes. The experimental results show that field learning achieved a higher prediction accuracy rate on new unseen test cases which is particularly true when the learning is performed on large low quality data.
[Performance evaluation, Gold, Machine learning algorithms, Automation, data analysis, high quality rule induction, prediction accuracy rate, rule derivation, uncertainty handling, Mathematics, learning, inexact field learning approach, inexact field learning, Accuracy, low quality problem, low quality data, attribute point values, very large databases, Training data, unseen test cases, learning (artificial intelligence), Testing]
Learning automatic acquisition of subcategorization frames using Bayesian inference and support vector machines
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Learning Bayesian belief networks (BBN) from corpora and support vector machines (SVM) have been applied to the automatic acquisition of verb subcategorization frames for Modern Greek. We are incorporating minimal linguistic resources, i.e. basic morphological tagging and phrase chunking, to demonstrate that verb subcategorization, which is of great significance for developing robust natural language human computer interaction systems, could be achieved using large corpora, without having any general-purpose, syntactic parser at all. In addition, apart from BBN and SVM, which have not previously used for this task, we have experimented with three well-known machine learning methods (feedforward backpropagation neural networks, learning vector quantization and decision tables), which are also being applied to the task of verb subcategorization frame defection for the first time. We argue that both BBN and SVM are well suited for learning to identify verb subcategorization frames. Empirical results will support this claim. Performance has been methodically evaluated using two different corpora types, one balanced and one domain-specific in order to determine the unbiased behaviour of the trained models. Limited training data are proved to endow with satisfactory results. We have been able to achieve precision exceeding 80% on the identification of subcategorization frames which were not known beforehand.
[learning automata, feedforward backpropagation neural networks, computational linguistics, Modern Greek, natural language human computer interaction systems, Learning systems, phrase chunking, corpora, morphological tagging, Robustness, belief networks, Bayesian inference, Backpropagation, support vector machines, Learning automata, Natural languages, automatic verb subcategorization frame acquisition, machine learning methods, performance evaluation, inference mechanisms, decision tables, Support vector machines, Human computer interaction, vector quantisation, Bayesian belief network learning, Bayesian methods, Machine learning, backpropagation, learning vector quantization, Tagging, minimal linguistic resources, feedforward neural nets]
Closing the loop: an agenda- and justification-based framework for selecting the next discovery task to perform
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
We propose and evaluate an agenda- and justification-based architecture for discovery systems that selects the next tasks to perform. This framework has many desirable properties: (1) it facilitates the encoding of general discovery strategies using a variety of background knowledge, (2) it reasons about the appropriateness of the tasks being considered, and (3) it tailors its behavior toward a user's interests. A prototype discovery program called HAMB demonstrates that both reasons and estimates of interestingness contribute to performance in the domains of protein crystallization and patient rehabilitation.
[Performance evaluation, macromolecules, Humans, data mining, justification-based architecture, user interfaces, patient rehabilitation, Proteins, Learning systems, Databases, biology computing, protein crystallization, very large databases, background knowledge, Prototypes, autonomous machine learning, learning (artificial intelligence), next discovery task selection, Erbium, health care, agenda-based framework, discovery systems, general discovery strategies, Crystallization, prototype discovery program, Encoding, inference mechanisms, patient care, justification-based framework, HAMB, knowledge discovery in databases, Machine learning, user interests]
Distance measures for effective clustering of ARIMA time-series
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Much environmental and socioeconomic time-series data can be adequately modeled using autoregressive integrated moving average (ARIMA) models. We call such time series "ARIMA time series". We propose the use of the linear predictive coding (LPC) cepstrum for clustering ARIMA time series, by using the Euclidean distance between the LPC cepstra of two time series as their dissimilarity measure. We demonstrate that LPC cepstral coefficients have the desired features for accurate clustering and efficient indexing of ARIMA time series. For example, just a few LPC cepstral coefficients are sufficient in order to discriminate between time series that are modeled by different ARIMA models. In fact, this approach requires fewer coefficients than traditional approaches, such as DFT (discrete Fourier transform) and DWT (discrete wavelet transform). The proposed distance measure can be used for measuring the similarity between different ARIMA models as well. We cluster ARIMA time series using the "partition around medoids" method with various similarity measures. We present experimental results demonstrating that, using the proposed measure, we achieve significantly better clusterings of ARIMA time series data as compared to clusterings obtained by using other traditional similarity measures, such as DFT, DWT, PCA (principal component analysis), etc. Experiments were performed both on simulated and real data.
[Fourier transforms, autoregressive moving average processes, data mining, LPC cepstral coefficients, socioeconomic data, cepstral analysis, linear predictive coding, socio-economic effects, dissimilarity measure, indexing, Discrete Fourier transforms, environmental factors, ARIMA time-series clustering, time series, Linear predictive coding, Time measurement, Discrete wavelet transforms, similarity measures, Cepstral analysis, pattern clustering, temporal databases, Cepstrum, social sciences, Euclidean distance, economic cybernetics, autoregressive integrated moving average, distance measure, partition-around-medoids method, environmental data, Principal component analysis, Indexing]
Theory and applications of attribute decomposition
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
This paper examines the attribute decomposition approach with simple Bayesian combination for dealing with classification problems that contain high number of attributes and moderate numbers of records. According to the attribute decomposition approach, the set of input attributes is automatically decomposed into several subsets. A classification model is built for each subset, then all the models are combined using simple Bayesian combination. This paper presents theoretical and practical foundation for the attribute decomposition approach. A greedy procedure, called D-IFN, is developed to decompose the input attributes set into subsets and build a classification model for each subset separately. The results achieved in the empirical compart. son testing with well-known classification methods (like C4.5) indicate the superiority of the decomposition approach.
[pattern classification, attribute decomposition, records, Humans, data mining, Industrial engineering, Predictive models, greedy procedure, Data mining, classification model, Databases, Bayesian methods, Data visualization, Large-scale systems, Bayes methods, D-IFN, learning (artificial intelligence), Bayesian combination, Testing, Principal component analysis, subsets]
A hypergraph based clustering algorithm for spatial data sets
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Clustering is a discovery process in data mining and can be used to group together the objects of a database into meaningful subclasses which serve as the foundation for other data analysis techniques. The authors focus on dealing with a set of spatial data. For the spatial data, the clustering problem becomes that of finding the densely populated regions of the space and thus grouping these regions into clusters such that the intracluster similarity is maximized and the intercluster similarity is minimized. We develop a novel hierarchical clustering algorithm that uses a hypergraph to represent a set of spatial data. This hypergraph is initially constructed from the Delaunay triangulation graph of the data set and can correctly capture the relationships among sets of data points. Two phases are developed for the proposed clustering algorithm to find the clusters in the data set. We evaluate our hierarchical clustering algorithm with some spatial data sets which contain clusters of different sizes, shapes, densities, and noise. Experimental results on these data sets are very encouraging.
[spatial data, intracluster similarity, Shape, graph theory, data mining, visual databases, Electronic mail, Data mining, discovery process, data analysis techniques, hypergraph, clustering problem, hierarchical clustering algorithm, Clustering algorithms, Cities and towns, densely populated regions, intercluster similarity, Data analysis, clustering algorithm, data set, data points, mesh generation, Spatial databases, Noise shaping, hypergraph based clustering algorithm, Consumer behavior, Delaunay triangulation graph, spatial data sets, Aggregates, pattern clustering]
Classification with degree of membership: a fuzzy approach
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Classification is an important topic in data mining research. It is concerned with the prediction of the values of some attribute in a database based on other attributes. To tackle this problem, most of the existing data mining algorithms adopt either a decision tree based approach or an approach that requires users to provide some user-specified thresholds to guide the search for interesting rules. The authors propose a new approach based on the use of an objective interestingness measure to distinguish interesting rules from uninteresting ones. Using linguistic terms to represent the revealed regularities and exceptions, this approach is especially useful when the discovered rules are presented to human experts for examination because of the affinity with the human knowledge representation. The use of a fuzzy technique allows the prediction of attribute values to be associated with degree of membership. Our approach is therefore able to deal with the cases where an object can belong to more than one class. Furthermore, our approach is more resilient to noise and missing data values because of the use of a fuzzy technique. To evaluate the performance of our approach, we tested it using several real-life databases. The experimental results show that it can be very effective at data mining tasks. When compared to popular data mining algorithms, the approach is better able to uncover useful rules hidden in databases.
[degree of membership, computational linguistics, data mining algorithms, uninteresting rules, Humans, data mining, fuzzy set theory, interesting rules, Electronic mail, Data mining, Databases, very large databases, Production, discovered rules, Decision trees, objective interestingness measure, real-life databases, Testing, Gold, pattern classification, data mining tasks, human knowledge representation, Knowledge representation, decision tree based approach, Marketing management, classification, attribute values, linguistic terms, fuzzy approach, human experts, fuzzy technique]
An experimental comparison of supervised and unsupervised approaches to text summarization
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The paper presents a direct comparison of supervised and unsupervised approaches to text summarization. As a representative supervised method, we use the C4.5 decision tree algorithm, extended with the minimum description length principle (MDL), and compare it against several unsupervised methods. It is found that a particular unsupervised method based on an extension of the K-means clustering algorithm, performs equal to and in some cases superior to the decision tree based method.
[text analysis, Costs, Natural languages, Humans, Diversity methods, K-means clustering algorithm, unsupervised text summarization, supervised text summarization, pattern clustering, Clustering algorithms, minimum description length principle, decision trees, C4.5 decision tree algorithm, Decision trees, learning (artificial intelligence), Classification tree analysis]
Fast parallel association rule mining without candidacy generation
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In this paper we introduce a new parallel algorithm MLFPT (multiple local frequent pattern tree) for parallel mining of frequent patterns, based on FP-growth mining, that uses only two full I/O scans of the database, eliminating the need for generating candidate items, and distributing the work fairly among processors. We have devised partitioning strategies at different stages of the mining process to achieve near optimal balancing between processors. We have successfully tested our algorithm on datasets larger than 50 million transactions.
[parallel algorithms, Memory architecture, frequent patterns, data mining, Partitioning algorithms, Transaction databases, datasets, Association rules, Data mining, transactions, Parallel algorithms, MLFPT parallel algorithm, optimal processor balancing, partitioning strategies, Itemsets, resource allocation, fast parallel association rule mining, very large databases, FP-growth mining, Marketing and sales, I/O scans, multiple local frequent pattern tree, Recommender systems, Testing]
/spl alpha/-surface and its application to mining protein data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Given a finite set of points in three dimensional Euclidean space R/sup 3/, the subset that forms its surface could be different when observed at different levels of detail. In this paper we introduce a notion called /spl alpha/-surface. We present an algorithm that extracts the /spl alpha/-surface from a finite set of points in R/sup 3/. We apply the algorithm to extracting the /spl alpha/-surfaces of proteins and discover patterns from these surface structures using a pattern discovery algorithm. We then use these patterns to classify the proteins. Experimental results demonstrate the good performance of the proposed approach.
[Protein engineering, pattern classification, protein data mining, surface structures, data mining, pattern discovery, Classification algorithms, Data mining, Application software, finite points set, Computer science, biology computing, proteins, Euclidean distance, molecular biophysics, 3D Euclidean space, /spl alpha/-surface extraction algorithm, protein classification]
A clustering method for very large mixed data sets
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In developed countries, especially over the last decade, there has been an explosive growth in the capability to generate, collect and use very large data sets. The objects of these data sets could be simultaneously described by quantitative and qualitative attributes. At present, algorithms able to process either very large data sets (in metric spaces) or mixed (qualitative and quantitative) incomplete data (missing value) sets have been developed, but not for very large mixed incomplete data sets. In this paper we introduce a new clustering method named GLC+ to process very large mixed incomplete data sets in order to obtain a partition in connected sets.
[Art, Clustering methods, clustering method, Finance, data mining, Extraterrestrial measurements, Mathematics, connected set partition, GLC+, Cybernetics, Physics, very large mixed incomplete data sets, pattern clustering, very large databases, Clustering algorithms, Skeleton, Explosives]
A pattern decomposition (PD) algorithm for finding all frequent patterns in large datasets
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Efficient algorithms to mine frequent patterns are crucial to many tasks in data mining. Since the Apriori algorithm was proposed (R. Agrawal and R. Srikant, 1994), there have been several methods proposed to improve its performance. However, most still adopt its candidate set generation-and-test approach. We propose a pattern decomposition (PD) algorithm that can significantly reduce the size of the dataset on each pass, making it more efficient to mine frequent patterns in a large dataset. The proposed algorithm avoids the costly process of candidate set generation and saves time by reducing dataset. Our empirical evaluation shows that the algorithm outperforms Apriori by one order of magnitude and is faster than FP-tree. Further, PD is more scalable than both Apriori and FP-tree.
[pattern decomposition algorithm, data mining, FP-tree, set theory, Data mining, Association rules, large datasets, Computer science, Itemsets, candidate set generation-and-test approach, very large databases, candidate set generation, Apriori algorithm, pattern recognition, frequent pattern mining]
Measuring real-time predictive models
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In this paper we examine the problem of comparing real-time predictive models and propose a number of measures for selecting the best model, based on a combination of accuracy, timeliness, and cost. We apply the measure to the real-time attrition problem.
[Costs, cost, Area measurement, data mining, Predictive models, accuracy, Time measurement, Current measurement, timeliness, real-time systems, real-time predictive model measurement, bank data processing, real-time attrition problem]
Classification through maximizing density
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
This paper presents a novel method for classification, which makes use of models built by the lattice machine (LM). The LM approximates data resulting in, as a model of data, a set of hyper tuples that are equilabelled, supported and maximal. The method presented uses the LM model of data to classify new data with a view to maximising the density of the model. Experiments show that this method, when used with the LM, outperforms the C2 algorithm and is comparable to the C5.0 classification algorithm.
[Algorithm design and analysis, pattern classification, data model, Lattices, Partitioning algorithms, classification, density maximization, Computer science, data models, Measurement units, Supervised learning, hyper tuples, Tail, Decision trees, learning (artificial intelligence), Software engineering, lattice machine]
Meta-patterns: revealing hidden periodic patterns
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Discovery of periodic patterns in time series data has become an active research area with many applications. These patterns can be hierarchical in nature, where a higher level pattern may consist of repetitions of lower level patterns. Unfortunately, the presence of noise may prevent these higher level patterns from being recognized in the sense that two portions (of a data sequence) that support the same (high level) pattern may have different layouts of occurrences of basic symbols. There may not exist any common representation in terms of raw symbol combinations; and hence such (high level) patterns may not be expressed by any previous model (defined on raw symbols or symbol combinations) and would not be properly recognized by any existing method. In this paper, we propose a novel model, namely meta-pattern, to capture these high level patterns. As a more flexible model, the number of potential meta-patterns could be very large. A substantial difficulty is how to identify the proper pattern candidates. However the well-known a priori properly is not able to provide sufficient pruning power. A new property, namely component location, is identified and used to conduct candidate generation so that an efficient computation-based mining algorithm can be developed. We apply our algorithm to real and synthetic sequences and interesting patterns are discovered.
[Fluctuations, data mining, component location property, time series, periodic pattern discovery, Pattern recognition, meta-patterns, History, sequences, repetitions, symbols, Noise level, candidate generation, computation-based mining algorithm, Influenza, Back, noise, Frequency, hidden periodic patterns, time series data, Power generation, Pattern matching, pattern recognition]
Indiscernibility degree of objects for evaluating simplicity of knowledge in the clustering procedure
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The paper presents a novel, rough set-based clustering method that enables the evaluation of classification knowledge simplicity during the clustering procedure. The method iteratively refines equivalence relations so that they become a more simple set of relations that give adequate coarse classification to the objects. At each step of the iteration, the importance of the equivalence relation is evaluated on the basis of the newly introduced measure, indiscernibility degree. An indiscernibility degree is defined as a ratio of equivalence relations that classify the two objects into the same equivalence class. If an equivalence relation has the ability to discern two objects that have a high indiscernibility degree, a very fine classification is performed and then modified to regard them as indiscernible objects. The refinement is repeated, decreasing the threshold level of indiscernibility degree, and finally simple clusters can be obtained. Experimental results on the artificial data shows that iterative refinement of equivalence relation leads to successful generation of coarse clusters that can be represented by simple knowledge.
[Algorithm design and analysis, Clustering methods, Scalability, classification knowledge simplicity evaluation, data mining, clustering procedure, iterative refinement, equivalence relations, threshold level, Databases, simple clusters, Sections, very large databases, Rough sets, Clustering algorithms, coarse classification, Data analysis, Biomedical informatics, equivalence class, pattern clustering, Set theory, indiscernibility degree, equivalence relation, coarse clusters, rough set-based clustering method, rough set theory, equivalence classes, artificial data]
Comparisons of classification methods for screening potential compounds
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
We compare a number of data mining and statistical methods on the drug design problem of modeling molecular structure-activity relationships. The relationships can be used to identify active compounds based on their chemical structures from a large inventory of chemical compounds. The data set of this application has a highly skewed class distribution, in which only 2% of the compounds are considered active. We apply a number of classification methods to this extremely imbalanced data set and propose to use different performance measures to evaluate these methods. We report our findings on the characteristics of the performance measures, the effect of using pruning techniques in this application and a comparison of local learning methods with global techniques. We also investigate whether reducing the imbalance in the training data by up-sampling or down-sampling would improve the predictive performance.
[Drugs, highly skewed class distribution, pharmaceutical industry, classification methods, data mining, Throughput, High temperature superconductors, imbalanced data set, Data mining, performance measures, chemical compounds, training data, learning (artificial intelligence), Protection, Testing, pattern classification, drug design problem, data set, predictive performance, down-sampling, Chemical compounds, Statistics, potential compound screening, chemical structures, pruning techniques, global techniques, Computer science, up-sampling, chemistry computing, active compounds, molecular structure-activity relationships, Human immunodeficiency virus, local learning methods, statistical methods]
Subject classification in the Oxford English Dictionary
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The Oxford English Dictionary is a valuable source of lexical information and a rich testing ground for mining highly structured text. Each entry is organized into a hierarchy of senses, which include definitions, labels and cited quotations. Subject labels distinguish the subject classification of a sense, for example they signal how a word may be used in anthropology, music or computing. Unfortunately subject labeling in the dictionary is incomplete. To overcome this incompleteness, we attempt to classify the senses (i.e., definitions) in the dictionary by their subjects, using the citations as an information guide. We report on four different approaches: k nearest neighbors, a standard classification technique; term weighting, an information retrieval method dealing with text; naive Bayes, a probabilistic method; and expectation maximization, an iterative probabilistic method. Experimental performance of these methods is compared based on standard classification metrics.
[Dictionaries, k nearest neighbors, senses, data mining, classification metrics, Data mining, subject classification, term weighting, information retrieval method, Labeling, Iterative methods, definitions, Testing, lexical information, Information retrieval, Multiple signal classification, classification, cited quotations, Nearest neighbor searches, labels, Computer science, information guide, subject labels, iterative probabilistic method, naive Bayes, Speech, expectation maximization, Bayes methods, Oxford English Dictionary, probabilistic method, highly structured text mining, dictionaries]
Web cartography for online site promotion: an algorithm for clustering Web resources
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Presents a Web cartography approach to be used in the context of online site promotion. The overall objective is to provide users with handy maps offering information about candidate sites for the creation of hyperlinks that enable a large flow of targeted visitors. Two main types of data must be considered: texts and hyperlinks. We propose to exploit the latter to construct a relevant corpus on which semantic as well as graph analyses can be applied. The stress is put on the clustering of Web resources based on the link network, which makes it possible to highlight groups of strongly connected sites which are of the utmost interest for our application. To tackle the site graph partitioning problem, we turn to a promising iterative approach initially developed in the context of computer-aided design. It uses spectral decomposition of the Laplacian matrix to embed the considered graph in a geometric space where efficient methods can be applied. An algorithm that was adapted from an existing one implements the method. Experiments were conducted on a real application case concerning the promotion of a site dealing with Cognac. We present the obtained map as well as leads to exploit it.
[spectral decomposition, iterative methods, Design automation, hyperlink creation, data mining, hypermedia, corpus, matrix decomposition, Data mining, business graphics, Web cartography, candidate sites, graphs, text data, Clustering algorithms, data visualisation, iterative approach, Search engines, geometric space, online site promotion, Laplacian matrix, Iterative methods, strongly connected sites, information resources, Laplace equations, Navigation, site graph partitioning problem, maps, link network, computer-aided design, targeted visitor flow, marketing data processing, Application software, Matrix decomposition, Stress, pattern clustering, semantic analyses, graph analyses, Cognac, World Wide Web resource clustering algorithm]
Using rule sets to maximize ROC performance
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Rules are commonly used for classification because they are modular intelligible and easy to learn. Existing work in classification rule learning assumes the goal is to produce categorical classifications to maximize classification accuracy. Recent work in machine learning has pointed out the limitations of classification accuracy: when class distributions are skewed or error costs are unequal, an accuracy-maximizing rule set can perform poorly. A more flexible use of a rule set is to produce instance scores indicating the likelihood that an instance belongs to a given class. With such an ability, we can apply rule sets effectively when distributions are skewed or error costs are unequal. This paper empirically investigates different strategies for evaluating rule sets when the goal is to maximize the scoring (ROC) performance.
[Costs, Error analysis, Laboratories, sensitivity analysis, data mining, skewed class distributions, classification accuracy, Data mining, scoring performance maximization, optimisation, Robustness, performance index, instance scores, learning (artificial intelligence), receiver operating characteristic, unequal error costs, Classification tree analysis, categorical classifications, pattern classification, rule sets, ROC performance maximization, probability, Association rules, machine learning, Milling machines, Decision theory, Machine learning, classification rule learning, errors]
Efficient splitting rules based on the probabilities of pre-assigned intervals
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The paper describes novel methods for classification in order to find an optimal tree. Unlike the current splitting rules that are provided by searching all threshold values, the paper proposes splitting rules that are based on the probabilities of pre-assigned intervals. In experiments, we demonstrate that our methods properly classify image objects based on new split rules.
[image object classification, Error analysis, image classification, classification methods, probability, Rivers, tree searching, splitting rules, Radio access networks, Milling machines, optimisation, split rules, optimal tree, Search methods, pre-assigned interval probabilities, knowledge based systems, threshold values, optimal cutoff point, computational complexity]
RPCL-based local PCA algorithm
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Mining local structure is important in data analysis. Gaussian mixture is able to describe local structure through covariance matrices, but when used on high-dimensional data, specifying such a large number of d(d+1)/2 free elements in each covariance matrix is difficult. By constraining the covariance matrix in decomposed orthonormal form, we propose a Local PCA algorithm to tackle this problem with the help of RPCL (Rival Penalized Competitive Learning), which can automatically determine the number of local structures.
[covariance matrices, Data analysis, Costs, Shape, data analysis, local structure, data mining, Covariance matrix, local structure mining, decomposed orthonormal form, unsupervised learning, Computer science, Rival Penalized Competitive Learning, RPCL competitive learning, local structures, high dimensional data, Clustering algorithms, Gaussian processes, RPCL-based Local PCA Algorithm, Gaussian mixture, Principal component analysis]
Analyzing the interestingness of association rules from the temporal dimension
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Rule discovery is one of the central tasks of data mining. Existing research has produced many algorithms for the purpose. These algorithms, however, often generate too many rules. In the past few years, rule interestingness techniques were proposed to help the user find interesting rules. These techniques typically employ the dataset as a whole to mine rules, and then filter and/or rank the discovered rules in various ways. We argue that this is insufficient. These techniques are unable to answer a question that is of critical importance to the application of rules, i.e., can the rules be trusted? In practice, the users are always concerned with the question. They want to know whether the rules indeed represent some true and stable (or reliable) underlying relationships in the domain. If a rule is not stable, does it show any systematic pattern such as a trend? Before any rule can be used, these questions must be answered. The paper proposes a technique to use statistical methods to analyze rules from the temporal dimension to answer these questions. Experimental results show that the proposed technique is very effective.
[rule interestingness techniques, Statistical analysis, data mining, Drives, Probability, temporal logic, rule discovery, interesting rules, Association rules, Data mining, Statistics, trusted rules, Filters, associative processing, Databases, temporal databases, knowledge based systems, discovered rules, temporal dimension, systematic pattern, association rule interestingness, statistical analysis, dataset, statistical methods]
Clustering validity assessment: finding the optimal partitioning of a data set
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Clustering is a mostly unsupervised procedure and the majority of clustering algorithms depend on certain assumptions in order to define the subgroups present in a data set. As a consequence, in most applications the resulting clustering scheme requires some sort of evaluation regarding its validity. In this paper we present a clustering validity procedure, which evaluates the results of clustering algorithms on data sets. We define a validity index, S Dbw, based on well-defined clustering criteria enabling the selection of optimal input parameter values for a clustering algorithm that result in the best partitioning of a data set. We evaluate the reliability of our index both theoretically and experimentally, considering three representative clustering algorithms run on synthetic and real data sets. We also carried out an evaluation study to compare S Dbw performance with other known validity indices. Our approach performed favorably in all cases, even those in which other indices failed to indicate the correct partitions in a data set.
[Multidimensional systems, Humans, data mining, reliability, Reliability theory, Partitioning algorithms, clustering validity assessment, optimal partitioning data set, Radio access networks, Visual perception, SDbw validity index, Geometry, pattern clustering, clustering algorithms, Clustering algorithms, Data visualization, Informatics]
An agglomerative hierarchical clustering using partial maximum array and incremental similarity computation method
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
As the tractable amount of data grows in the computer science area, fast clustering algorithms are required, because traditional clustering algorithms are not feasible for very large and high-dimensional data. Many studies have been reported on the clustering of large databases, but most of them circumvent this problem by using an approximation method, resulting in the deterioration of accuracy. In this paper, we propose a new clustering algorithm by means of a partial maximum array, which can realize agglomerative hierarchical clustering with the same accuracy as the brute-force algorithm and has O(N/sup 2/) time complexity. We also present an incremental method of similarity computation which substitutes a scalar calculation for the time-consuming calculation of vector similarity. Experimental results show that clustering becomes significantly fast for large and high-dimensional data.
[Clustering methods, Merging, data mining, high-dimensional data, Approximation methods, accuracy deterioration, fast clustering algorithm, Databases, approximation method, very large databases, Clustering algorithms, large data sets, data structures, Computational efficiency, partial maximum array, incremental similarity computation method, scalar calculation, time complexity, agglomerative hierarchical clustering, Machine intelligence, Nearest neighbor searches, database theory, Computer science, pattern clustering, Iterative algorithms, arrays, large databases, computational complexity]
Discovery of association rules in tabular data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In this paper we address the problem of finding all association rules in tabular data. An algorithm, ARA, for finding rules, that satisfy clearly specified constraints, in tabular data is presented. ARA is based on the dense miner algorithm but includes an additional constraint and an improved method of calculating support. ARA is tested and compared with our implementation of dense miner; it is concluded that ARA is usually more efficient than dense miner and is often considerably more so. We also consider the potential for modifying the constraints used in ARA in order to find more general rules.
[Data analysis, ARA algorithm, Wheels, data mining, Spatial databases, Transaction databases, Association rules, Data mining, tabular data, constraints, Information systems, association rule discovery, Diesel engines, dense miner algorithm, Testing]
Ad hoc association rule mining as SQL3 queries
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Although there have been several encouraging attempts at developing methods for data mining using SQL, simplicity and efficiency still remain significant impediments for further development. In this paper, we propose a significantly new approach and show that any object relational database can be mined for association rules without any restructuring or preprocessing using only basic SQL3 constructs and functions, and hence no additional machinery is necessary. In particular, we show that the cost of computing association rules for a given database does not depend on support and confidence thresholds. More precisely, the set of large items can be computed using one simple join query and an aggregation once the set of all possible meets (least fixpoint) of item set patterns in the input table is known. The principal focus of this paper is to demonstrate that several SQL3 expressions exists for the mining of association rules.
[Costs, Data analysis, object-oriented databases, data mining, Relational databases, item set patterns, aggregation, Transaction databases, Association rules, Data mining, relational databases, Machinery, Engines, SQL, large item set, query processing, ad hoc association rule mining, Frequency, join query, Impedance, object relational database, SQL3 queries]
Distributed Web mining using Bayesian networks from multiple data streams
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
We present a collective approach to mining Bayesian networks from distributed heterogenous Web-log data streams. In this approach we first learn a local Bayesian network at each site using the local data. Then each site identifies the observations that are most likely to be evidence of coupling between local and non-local variables and transmits a subset of these observations to a central site. Another Bayesian network is learnt at the central site using the data transmitted from the local site. The local and central Bayesian networks are combined to obtain a collective Bayesian network that models the entire data. We applied this technique to mining multiple data streams, where data centralization is difficult because of large response time and scalability issues. Experimental results and theoretical justification that demonstrate the feasibility of our approach are presented.
[scalability issues, data mining, World Wide Web, Data mining, Delay, Design optimization, Network servers, Web mining, collective approach, local Bayesian network, belief networks, learning (artificial intelligence), Web server, information resources, distributed Web mining, collective Bayesian network, nonlocal variables, Web design, multiple data streams, Bayesian methods, distributed algorithms, response time, local variables, distributed heterogenous Web-log data streams, Web sites, data centralization]
Concise representation of frequent patterns based on disjunction-free generators
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Many data mining problems require the discovery of frequent patterns in order to be solved. Frequent itemsets are useful in the discovery of association rules, episode rules, sequential patterns and clusters. The number of frequent itemsets is usually huge. Therefore, it is important to work out concise representations of frequent itemsets. We describe three basic lossless representations of frequent patterns in a uniform way and offer a new lossless representation of frequent patterns based on disjunction-free generators. The new representation is more concise than two of the basic representations and more efficiently computable than the third representation. We propose an algorithm for determining the new representation.
[pattern classification, concise representation, frequent patterns, data mining, Relational databases, disjunction-free generators, association rules, sequential patterns, rule discovery, Transaction databases, set theory, Association rules, Data mining, lossless representations, Computer science, Itemsets, very large databases, Clustering algorithms, knowledge based systems, theorem proving, frequent pattern discovery, data mining problems, frequent itemsets]
LPMiner: an algorithm for finding frequent itemsets using length-decreasing support constraint
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Over the years, a variety of algorithms for finding frequent item sets in very large transaction databases has been developed. The key feature in most of these algorithms is that they use a constant support constraint to control the inherently exponential complexity of the problem. In general, item sets that contain only a few items tend to be interesting if they have a high support, whereas long item sets can still be interesting even if their support is relatively small. Ideally, we desire to have an algorithm that finds all the frequent item sets whose support decreases as a function of their length. In this paper, we present an algorithm called LPMiner (Long Pattern Miner) that finds all item sets that satisfy a length-decreasing support constraint. Our experimental evaluation shows that LPMiner is up to two orders of magnitude faster than the FP-growth algorithm for finding item sets at a constant support constraint, and that its run-time increases gradually as the average length of the transactions (and the discovered item sets) increases.
[transaction processing, US Department of Energy, data mining, Data engineering, constant support constraint, long item sets, smallest valid extension, Data mining, length-decreasing support constraint, Runtime, Itemsets, very large databases, very large transaction databases, tree data structures, Contracts, pattern recognition, transaction length, Transaction databases, Association rules, frequent item-set discovery, exponential complexity, run time, Computer science, High performance computing, FP-trees, LPMiner algorithm, FP-growth algorithm, computational complexity]
A min-max cut algorithm for graph partitioning and data clustering
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
An important application of graph partitioning is data clustering using a graph model - the pairwise similarities between all data objects form a weighted graph adjacency matrix that contains all necessary information for clustering. In this paper, we propose a new algorithm for graph partitioning with an objective function that follows the min-max clustering principle. The relaxed version of the optimization of the min-max cut objective function leads to the Fiedler vector in spectral graph partitioning. Theoretical analyses of min-max cut indicate that it leads to balanced partitions, and lower bounds are derived. The min-max cut algorithm is tested on newsgroup data sets and is found to out-perform other current popular partitioning/clustering methods. The linkage-based refinements to the algorithm further improve the quality of clustering substantially. We also demonstrate that a linearized search order based on linkage differential is better than that based on the Fiedler vector, providing another effective partitioning method.
[data object pairwise similarities, linkage-based refinements, Clustering methods, Laboratories, graph theory, data mining, data clustering, Mathematics, minimax techniques, linkage differential, Clustering algorithms, graph model, weighted graph adjacency matrix, newsgroup data sets, Mathematical model, graph partitioning, Fiedler vector, Testing, objective function, linearized search order, relaxed optimization, Partitioning algorithms, Helium, Application software, lower bounds, Computer science, balanced partitions, clustering quality, vectors, pattern clustering, spectral graph partition, algorithm performance, min-max cut algorithm]
The representative basis for association rules
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
We define the concept of the representative basis for interesting association rules, and an inference system which is purely qualitative. The representative basis is unique, and minimal with respect to the inference system. On the representative basis, the inference system is correct and complete. Experimental results show that the number of rules in the representative basis is significantly reduced with respect to the number of rules generated by other existing approaches.
[representative basis, generating sets, data mining, association rules, set theory, Association rules, inference mechanisms, associative processing, Itemsets, knowledge representation, association rule extraction, qualitative inference system, interesting association rules]
Discovering similar patterns for characterising time series in a medical domain
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In this article, we describe the process of discovering similar patterns in time series and creating reference models for population groups in a medical domain, and particularly in the field of physiotherapy, using data mining techniques on a set of isokinetic data. The discovered knowledge was evaluated against the expertise of a physician specialising in isokinetic techniques, and applied in the I4 (Intelligent Interpretation of Isokinetic Information) project developed in conjunction with the Spanish National Centre for Sports Research and Sciences and the School of Physiotherapy of the Spanish National Organisation for the Blind for muscular diagnosis and rehabilitation, injury prevention, training evaluation and planning, etc., of elite and blind athletes.
[Algorithm design and analysis, Knee, time series characterisation, muscular rehabilitation, reference models, data mining, I4 project, training, isokinetic data set, elite athletes, similar patterns discovery, Data mining, patient rehabilitation, medical domain, Physics computing, patient treatment, data mining techniques, muscle, Delta modulation, sports research, physician expertise, injury prevention, Injuries, training planning, pattern classification, muscular diagnosis, Instruments, intelligent information interpretation, physiotherapy, training evaluation, Muscles, time series, population groups, blind athletes, Artificial intelligence, medical computing, sport, Medical diagnostic imaging, patient diagnosis]
Anchor text mining for translation of Web queries
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The paper presents an approach to automatically extracting translations of Web query terms through mining of Web anchor texts and link structures. One of the existing difficulties in cross-language information retrieval (CLIR) and Web search is the lack of the appropriate translations of new terminology and proper names. Such a difficult problem can be effectively alleviated by our proposed approach, and the resource of anchor texts in the Web is proven a valuable corpus for this kind of term translation.
[text analysis, Terminology, data mining, Ontologies, term translation, Data mining, CLIR, Information science, Databases, terminology, cross-language information retrieval, Web query translation, Text mining, information resources, Natural languages, information retrieval, Information retrieval, Web anchor texts, link structures, Web query terms, proper names, Web pages, automatic translation extraction, anchor text mining, Web search, language translation]
Integrating e-commerce and data mining: architecture and challenges
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
We show that the e-commerce domain can provide all the right ingredients for successful data mining. We describe an integrated architecture for supporting this integration. The architecture can dramatically reduce the pre-processing, cleaning, and data understanding effort often documented to take 80% of the time in knowledge discovery projects. We emphasize the need for data collection at the application server layer (not the Web server) in order to support logging of data and metadata that is essential to the discovery process. We describe the data transformation bridges required from the transaction processing systems and customer event streams (e.g., clickstreams) to the data warehouse. We detail the mining workbench, which needs to provide multiple views of the data through reporting, data mining algorithms, visualization, and OLAP. We conclude with a set of challenges.
[transaction processing, knowledge discovery projects, data warehouse, metadata, visualization, data mining algorithms, data mining, Data mining, Fuels, transaction processing systems, multiple views, integrated architecture, e-commerce domain, data visualisation, Computer architecture, customer event streams, clickstreams, Web server, electronic commerce, application server layer, information resources, mining workbench, meta data, data understanding, data logging, Data warehouses, data transformation bridges, OLAP, Cleaning, data collection, Bridges, e-commerce/data mining integration, Web pages, Data visualization, User interfaces, data warehouses]
Preparations for semantics-based XML mining
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
XML allows users to define elements using arbitrary words and organize them in a nested structure. These features of XML offer both challenges and opportunities in information retrieval, document management, and data mining. In this paper, we propose a new methodology for preparing XML documents for quantitative determination of similarity between XML documents by taking into account XML semantics (i.e., meanings of the elements and nested structures of XML documents). Accurate quantitative determination of similarity between XML documents provides an important basis for a variety of applications of XML document mining and processing. Experiments with XML documents show that our methodology provides a 50-100% improvement in determining similarity over the traditional vector-space model that considers only term-frequency and 100% accuracy in identifying the category of each document from an on-line bookstore.
[Power system management, Buildings, data mining, on-line bookstore, information retrieval, document management, quantitative similarity determination, Information retrieval, Data engineering, Spatial databases, Information management, XML document preparation, Data mining, Indexes, semantics-based XML mining, query processing, XML, Books, hypermedia markup languages]
Functional trees for classification
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The design of algorithms that explore multiple representation languages and explore different search spaces has an intuitive appeal. In the context of classification problems, algorithms that generate multivariate trees are able to explore multiple representation languages by using decision tests based on a combination of attributes. The same applies to model-tree algorithms in regression domains, but using linear models at leaf nodes. In this paper, we study where to use combinations of attributes in decision tree learning. We present an algorithm for multivariate tree learning that combines a univariate decision tree with a discriminant function by means of constructive induction. This algorithm is able to use decision nodes with multivariate tests, and leaf nodes that predict a class using a discriminant function. Multivariate decision nodes are built when growing the tree, while functional leaves are built when pruning the tree. Functional trees can be seen as a generalization of multivariate trees. Our algorithm was compared against to its components and two simplified versions using 30 benchmark data sets. The experimental evaluation shows that our algorithm has clear advantages with respect to the generalization ability and model sizes at statistically significant confidence levels.
[Algorithm design and analysis, discriminant function, benchmark data sets, data mining, generalization ability, multiple representation languages, statistically significant confidence levels, leaf nodes, multivariate tests, decision tree learning, functional leaves, regression domains, model sizes, univariate decision tree, Space exploration, Decision trees, Regression tree analysis, Classification tree analysis, Testing, pattern classification, functional trees, multivariate tree generation, model-tree algorithms, tree searching, generalisation (artificial intelligence), classification, linear models, attribute combinations, Supervised learning, decision trees, search spaces, constructive induction, tree pruning, decision tests, decision nodes, learning by example, algorithm design]
Bayesian data mining on the Web with B-Course
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
B-Course is a free Web based Bayesian data mining service. This service allows the users to analyze their own data for multivariate probabilistic dependencies represented as Bayesian network models. In addition to this, B-Course also offers facilities for inferring certain types of causal dependencies from the data. The software is especially suitable for educational purposes as the tutorial style user friendly interface intertwines steps in the data analysis with support material that gives an informal introduction to the Bayesian approach adopted. Nevertheless, although the analysis methods, modeling assumptions and restrictions are totally transparent to the user, this transparency is not achieved at the expense of analysis power: with the restrictions stated in the support material, B-Course is a powerful analysis tool exploiting several theoretically elaborate results developed recently in the fields of Bayesian and causal modeling.
[causal dependency inference, data mining, World Wide Web, user interfaces, B-Course, Data mining, Bayesian network models, Uniform resource locators, modeling assumptions, tutorial style user friendly interface, Bayesian modeling, free Web based Bayesian data mining service, multivariate probabilistic dependencies, Standards development, belief networks, information resources, Data analysis, data analysis, educational purposes, Application specific processors, analysis methods, Application software, inference mechanisms, causal modeling, Computer science, Bayesian approach, Bayesian methods, powerful analysis tool, Software tools, Context modeling]
A comparison of stacking with meta decision trees to bagging, boosting, and stacking with other methods
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Meta decision trees (MDTs) are a method for combining multiple classifiers. We present an integration of the algorithm MLC4.5 for learning MDTs in the Weka data mining suite. We compare classifier ensembles combined with MDTs to bagged and boosted decision trees, and to classifier ensembles combined with other methods: voting and stacking with three different meta-level classifiers (ordinary decision trees, naive Bayes, and multi-response linear regression, MLR).
[Machine learning algorithms, Error analysis, Stacking, MLC, data mining, Weka data mining suite, voting, Probability distribution, Data mining, bagged boosted decision trees, MDTs, MLR, Voting, ordinary decision trees, Decision trees, learning (artificial intelligence), multiple classifiers, Classification tree analysis, meta-level classifiers, pattern classification, meta decision trees, Boosting, classifier ensembles, multi-response linear regression, decision trees, naive Bayes, Bagging]
An efficient data mining technique for discovering interesting sequential patterns
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Mining sequential patterns is used to discover sequential purchasing behaviors of most customers from a large amount of customer transactions. A data mining language is presented. From the data mining language, users can specify the interested items and the criteria of the sequential patterns to be discovered. Also, an efficient data mining technique is proposed to extract sequential patterns according to user requests.
[sequential pattern discovery, user requests, customer transactions, data mining, high level languages, interested items, Information management, data mining language, Data mining, interesting sequential pattern discovery, Petroleum, Computer science, Itemsets, Databases, sequential purchasing behavior discovery, very large databases, Bismuth, data mining technique, retail data processing]
Mining the Web with active hidden Markov models
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Given the enormous amounts of information available only in unstructured or semi-structured textual documents, tools for information extraction (IE) have become enormously important. IE tools identify the relevant information in such documents and convert it into a structured format such as a database or an XML document. While first IE algorithms were hand-crafted sets of rules, researchers soon turned to learning extraction rules from hand-labeled documents. Unfortunately, rule-based approaches sometimes fail to provide the necessary robustness against the inherent variability of document, structure, which has led to the recent interest in using hidden Markov models (HMMs). By using additional unlabeled documents as they are usually readily available in most applications, we can perform active learning of HMMs. The idea of active learning algorithms is to identify unlabeled observations that would be most useful when labeled by the user. Such algorithms are known for classification, clustering, and regression; we present the first algorithm for active learning of hidden Markov models.
[information resources, Sequences, active hidden Markov models, data mining, information retrieval, Probability, semi-structured textual documents, unlabeled documents, Data mining, hidden Markov models, unstructured textual documents, information extraction, active learning, Databases, Hidden Markov models, Clustering algorithms, XML, Speech recognition, Web mining, Tin, Robustness, learning (artificial intelligence)]
Incremental learning of Bayesian networks with hidden variables
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
An incremental method for learning Bayesian networks based on evolutionary computing, IEMA, is put forward. IEMA introduces the evolutionary algorithm and EM algorithm into the process of incremental learning; it can avoid getting into local maxima, and also incrementally learn Bayesian networks with high accuracy in the presence of missing values and hidden variables. In addition, we improved the incremental learning process by N. Friedman and M. Goldschmidt (1997). The experimental results verified the validity of IEMA. In terms of storage cost, IEMA is comparable with the incremental learning method of Friedman et al, while it is more accurate.
[Costs, Laboratories, evolutionary computing, Evolutionary computation, Bayesian network learning, local maxima, storage cost, Statistics, Learning systems, Computer science, Intelligent networks, evolutionary computation, hidden variables, Bayesian methods, incremental learning, Computer networks, belief networks, learning (artificial intelligence), Intelligent systems, EM algorithm, incremental method, missing values]
An immune neural network used for classification
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Based on an analysis of immune phenomena in nature and utilizing performances of ANN, a novel network mode, i.e., an immune neural network (INN), is proposed which integrates the immune mechanism and the function of neural information processing. The learning algorithm of an INN selects an excitation function and adaptive algorithm of the network. This model makes it easy for a user to utilize directly the characteristic information of a problem and to simplify the original structure by adjusting the excitation function with prior knowledge, improving the working efficiency and searching accuracy. A theoretical analysis and a simulation test for the twin-spiral problem show that, compared with an artificial neural network, INN is not only effective but also feasible. INN can simplify the structure of the existent model and show good working performance.
[pattern classification, learning algorithm, Neurons, Artificial neural networks, searching, classification, Biological neural networks, Information analysis, Neural networks, Information processing, excitation function, adaptive algorithm, Concrete, Radar signal processing, Performance analysis, learning (artificial intelligence), Signal analysis, neural nets, immune neural network]
Mining coverage-based fuzzy rules by evolutional computation
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The authors propose a novel mining approach based on the genetic process and an evaluation mechanism to automatically construct an effective fuzzy rule base. The proposed approach consists of three phases: fuzzy-rule generating, fuzzy-rule encoding and fuzzy-rule evolution. In the fuzzy-rule generating phase, a number of fuzzy rules are randomly generated. In the fuzzy-rule encoding phase, all the rules generated are translated into fixed-length bit strings to form an initial population. In the fuzzy-rule evolution phase, genetic operations and credit assignment are applied at the rule level. The proposed mining approach chooses good individuals in the population for mating, gradually creating better offspring fuzzy rules. A concise and compact fuzzy rule base is thus constructed effectively without human expert intervention.
[offspring fuzzy rules, fuzzy set, credit assignment, fuzzy-rule evolution phase, Humans, data mining, fuzzy set theory, fuzzy-rule encoding phase, Data mining, Genetic algorithms, genetic algorithm, Fuzzy sets, very large databases, learning (artificial intelligence), compact fuzzy rule base, Random number generation, Fuzzy systems, Knowledge acquisition, fixed-length bit strings, fuzzy-rule generating phase, mining approach, evaluation mechanism, genetic operations, Encoding, evolutional computation, genetic algorithms, coverage-based fuzzy rule mining, rule level, machine learning, fuzzy-rule generation, Machine learning, Genetic engineering, genetic process]
Maintenance of sequential patterns for record deletion
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
We previously proposed an incremental mining algorithm for maintenance of sequential patterns based on the concept of pre-large sequences as new records were inserted. In this paper we attempt to apply the concept of pre-large sequences to maintain sequential patterns as records are deleted. Pre-large sequences are defined by a lower support threshold and an upper support threshold. They act as buffers to avoid the movements of sequential patterns directly from large to small and, vice-versa. Our proposed algorithm does not require rescanning original databases until the accumulative amount of deleted customer sequences exceeds a safety bound, which depends on database size. As databases grow larger, the number of deleted customer sequences allowed before database rescanning is required also grows. The proposed approach is thus efficient for a large database.
[Costs, Adaptive algorithm, record deletion, data mining, deleted customer sequences, sequential pattern maintenance, Transaction databases, Data mining, sequences, lower support threshold, Itemsets, very large databases, upper support threshold, safety bound, incremental mining algorithm, Safety, pre-large sequences]
Automatic topic identification using webpage clustering
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Grouping Web pages into distinct topics is one way of organizing the large amount of retrieved information on the Web. In this paper, we report that, based on a similarity metric, which incorporates textual information, hyperlink structure and co-citation relations, an unsupervised clustering method can automatically and effectively identify relevant topics, as shown in experiments on several retrieved sets of Web pages. The clustering method is a state-of-art spectral graph partitioning method based on the normalized cut criterion first developed for image segmentation.
[information resources, textual information, unsupervised clustering method, Clustering methods, Laboratories, Taxonomy, co-citation relations, information retrieval, Information retrieval, information analysis, Organizing, Computer science, Image segmentation, pattern clustering, Clustering algorithms, Search engines, automatic topic identification, normalized cut criterion, Web page clustering, Web sites, similarity metric, spectral graph partitioning method, hyperlink structure]
Heuristic optimization for decentralized frequent itemset counting
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The choices for mining of decentralized data are numerous, and we have developed techniques to enumerate and optimize decentralized frequent itemset counting. We introduce our heuristic approach to improve the performance of such techniques developed in ways similar to query processing in database systems. We also describe empirical results that validate our heuristic techniques.
[Demography, Merging, data mining, Partitioning algorithms, Data mining, Computer science, query processing, decentralized frequent itemset counting, heuristic techniques, optimisation, Itemsets, Algebra, heuristic programming, Query processing, very large databases, distributed algorithms, Cost function, Database systems, heuristic optimization, decentralized data mining, heuristic approach, database systems]
Frequent subgraph discovery
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
As data mining techniques are being increasingly applied to non-traditional domains, existing approaches for finding frequent itemsets cannot be used as they cannot model the requirement of these domains. An alternate way of modeling the objects in these data sets is to use graphs. Within that model, the problem of finding frequent patterns becomes that of discovering subgraphs that occur frequently over the entire set of graphs.The authors present a computationally efficient algorithm for finding all frequent subgraphs in large graph databases. We evaluated the performance of the algorithm by experiments with synthetic datasets as well as a chemical compound dataset. The empirical results show that our algorithm scales linearly with the number of input transactions and it is able to discover frequent subgraphs from a set of graph transactions reasonably fast, even though we have to deal with computationally hard problems such as canonical labeling of graphs and subgraph isomorphism which are not necessary for traditional frequent itemset discovery.
[Image recognition, large graph databases, data mining, visual databases, Data mining, subgraph isomorphism, Itemsets, frequent subgraph discovery, very large databases, data mining techniques, subgraph discovery, computationally hard problems, data sets, chemical compound dataset, object modeling, Labeling, computationally efficient algorithm, frequent itemsets, synthetic datasets, Computer vision, canonical labeling, graph transactions, Transaction databases, Association rules, Chemical compounds, input transactions, Computer science, Image databases, chemistry computing, computational complexity]
Document clustering and cluster topic extraction in multilingual corpora
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
A statistics-based approach for clustering documents and for extracting cluster topics is described relevant (meaningful) expressions (REs) automatically extracted from corpora are used as clustering base features. These features are transformed and its number is strongly reduced in order to obtain a small set of document classification features. This is achieved on the basis of principal components analysis. Model-based clustering analysis finds the best number of clusters. Then, the most important REs are extracted from each cluster and taken as document cluster topics.
[document handling, Instruction sets, data mining, document clustering, Probability, relevant expressions, Size measurement, document classification features, Data mining, cluster topic extraction, Organizing, Dispersion, principal components analysis, model-based clustering analysis, pattern clustering, Feature extraction, Agriculture, statistics-based approach, multilingual corpora]
Preprocessing opportunities in optimal numerical range partitioning
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
We show that only segment borders have to be taken into account as cut point candidates when searching for the optimal multisplit of a numerical value range with respect to convex attribute evaluation functions. Segment borders can be found efficiently in a linear-time preprocessing step. With training set error, which is not strictly convex, the data can be preprocessed into an even smaller number of cut point candidates, called alternations, when striving for the optimal partition. We show that no segment borders (resp. alternations) can be overlooked with strictly convex functions (resp. training set error) without risking the loss of optimality. Our experiments show that while in real-world domains a significant reduction in the number of cut point candidates can be obtained for training set error, the number of segment borders is usually not much lower than that of boundary points.
[Heuristic algorithms, segment borders, data mining, optimal multisplit, linear-time preprocessing step, alternations, Partitioning algorithms, Computer science, convex attribute evaluation functions, Upper bound, cutpoint candidates, Computer errors, optimal numerical range partitioning, Dynamic programming, learning (artificial intelligence), training set error]
Incremental learning with support vector machines
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Support vector machines (SVMs) have become a popular tool for machine learning with large amounts of high dimensional data. In this paper an approach for incremental learning with support vector machines is presented, that improves the existing approach of Syed et al. (1999). An insight into the interpretability of support vectors is also given.
[learning automata, support vector machines, machine learning, Support vector machines, Computer science, high dimensional data, incremental learning, Training data, Machine learning, Robustness, learning (artificial intelligence), Artificial intelligence, Testing]
Creating ensembles of classifiers
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Ensembles of classifiers offer promise in increasing overall classification accuracy. The availability of extremely large datasets has opened avenues for application of distributed and/or parallel learning to efficiently learn models of them. In this paper, distributed learning is done by training classifiers on disjoint subsets of the data. We examine a random partitioning method to create disjoint subsets and propose a more intelligent way of partitioning into disjoint subsets using clustering. It was observed that the intelligent method of partitioning generally performs better than random partitioning for our datasets. In both methods a significant gain in accuracy may be obtained by applying bagging to each of the disjoint subsets, creating multiple diverse classifiers. The significance of our finding is that a partition strategy for even small/moderate sized datasets when combined with bagging can yield better performance than applying a single learner using the entire dataset.
[data mining, classification accuracy, Partitioning algorithms, Application software, Distributed computing, large datasets, Computer science, parallel learning, pattern clustering, Clustering algorithms, random partitioning method, Machine learning, distributed learning, disjoint subsets, Tires, clustering, classifier ensemble creation, Decision trees, learning (artificial intelligence), Bagging, Classification tree analysis]
Mining mutually dependent patterns
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In some domains, such as isolating problems in computer networks and discovering stock market irregularities, there is more interest in patterns consisting of infrequent, but highly correlated items rather than patterns that occur frequently (as defined by minsup, the minimum support level). We describe the m-pattern, a new pattern that is defined in terms of minp, the minimum probability of mutual dependence of items in the pattern. We show that all infrequent m-patterns can be discovered by an efficient algorithm that makes use of: (a) a linear algorithm to qualify an m-pattern; (b) an effective technique for candidate pruning based on a necessary condition for the presence of an m-pattern; and (c) a level-wise search for m-pattern discovery (which is possible because m-patterns are downward closed). Further, we consider frequent m-patterns, which are defined in terms of both minp and minsup. Using synthetic data, we study the scalability of our algorithm. Then, we apply our algorithm to data from a production computer network both to show the m-patterns present and to contrast with frequent patterns. We show that when minp=0, our algorithm is equivalent to finding frequent patterns. However, with a larger minp, our algorithm yields a modest number of highly correlated items, which makes it possible to mine for infrequent but highly correlated itemsets. To date, many actionable m-patterns have been discovered in production systems.
[m-pattern, Scalability, level-wise search, frequent patterns, data mining, minsup, Association rules, Application software, Data mining, mutually dependent pattern mining, minp, minimum probability, scalability, infrequent patterns, production computer network, Itemsets, Intrusion detection, Production, candidate pruning, linear algorithm, Computer networks, Stock markets, Pattern analysis]
Mining image features for efficient query processing
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The number of features required to depict an image can be very large. Using all features simultaneously to measure image similarity and to learn image query-concepts can suffer from the problem of dimensionality curse, which degrades both search accuracy and search speed. Regarding search accuracy, the presence of irrelevant features with respect to a query can contaminate similarity measurement, and hence decrease both the recall and precision of that query. To remedy this problem, we present a mining method that learns online users' query concepts and identifies important features quickly. Regarding search speed, the presence of a large number of features can slow down query-concept learning and indexing performance. We propose a divide-and-conquer method that divides the concept-learning task into G subtasks to achieve speedup. We notice that a task must be divided carefully, or search accuracy may suffer. We thus propose a genetic-based mining algorithm to discover good feature groupings. Through analysis and mining results, we observe that organizing image features in a multi-resolution manner and minimizing intra-group feature correlation, can speed up query-concept learning substantially while maintaining high search accuracy.
[efficient query processing, divide and conquer methods, data mining, image similarity measurement, Data mining, divide-and-conquer method, feature groupings, minimized intra-group feature correlation, Degradation, image query concept learning, database indexing, feature extraction, search speed, Pollution measurement, Decision trees, indexing, multimedia databases, genetic algorithms, content-based retrieval, genetic-based mining algorithm, Organizing, Image analysis, Query processing, Neural networks, image feature mining, image retrieval, Time factors, Indexing]
A tight upper bound on the number of candidate patterns
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In the context of mining for frequent patterns using the standard level-wise algorithm, the following question arises: given the current level and the current set of frequent patterns, what is the maximal number of candidate patterns that can be generated on the next level? We answer this question by providing a tight upper bound, derived from a combinatorial result by J. Kruskal (1963) and G. Katona (1968). Our result is useful for reducing the number of database scans.
[Heart, Costs, combinatorial mathematics, database scans, data mining, level-wise algorithm, maximal candidate pattern number, Transaction databases, database theory, Upper bound, tight upper bound, pattern recognition, frequent pattern mining]
Mining California vital statistics data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Vital statistics data offer a fertile ground for data mining. The authors discuss the results of a data mining project on the causes of death aspect of the vital statistics data in the state of California. A data mining tool called Cubist is used to build predictive models out of two million cases over a nine-year period. The objective of our study is to discover knowledge that can be used to gain insight into various aspects of mortality in California, to predict health issues related to the causes of death, to offer an aid to decision- or policy-making processes, and to provide useful information services to the customers. The results obtained in our study contain valuable new information.
[Data preprocessing, data mining, Predictive models, Cubist, knowledge discovery, Data mining, data mining tool, health issues, demography, very large databases, decision-making process, social sciences computing, policy-making process, Public healthcare, Monitoring, health care, Testing, Information resources, causes of death, data mining project, Cleaning, Statistics, information services, Computer science, California vital statistics data mining, statistical analysis, government data processing]
A simple KNN algorithm for text categorization
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Text categorization (also called text classification) is the process of identifying the class to which a text document belongs. This paper proposes to use a simple non-weighted features KNN algorithm for text categorization. We propose to use a feature selection method that finds the relevant features for the learning task at hand using feature interaction (based on word interdependencies). This will allow us to reduce considerably the number Of selected features from which to learn, making our KNN algorithm applicable in contexts where both the volume of documents and the size of the vocabulary are high, like with the World Wide Web. Therefore, the KNN algorithm that we propose becomes efficient for classifying text documents in that context (in terms of its predictability and interpretability), as is demonstrated. Its simplicity (WRT its implementation and fine-tuning) becomes its main assets for in-the-field applications.
[Vocabulary, text analysis, learning task, Unsolicited electronic mail, Frequency conversion, text categorization, World Wide Web, text classification, feature selection method, feature interaction, classification, Computer science, word interdependencies, text document, Text categorization, feature extraction, nonweighted features KNN algorithm, Solids, Web sites, Testing]
Mining frequent closed itemsets with the frequent pattern list
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The mining of a complete set of frequent itemsets will lead to a huge number of itemsets. Fortunately, this problem can be reduced to the mining of frequent closed itemsets (FCIs), which results in a much smaller number of itemsets. The approaches to mining frequent closed itemsets can be categorized into two groups: those with candidate generation and those without. In this paper, we propose an approach to mining frequent closed itemsets without candidate generation with a data structure called the frequent pattern list (FPL). We designed the algorithm FPLCI-mining to mine the FCIs. Experimental results show that our method is faster than previous ones.
[Algorithm design and analysis, Filtering, data mining, data structure, frequent pattern list, Data structures, Transaction databases, Data mining, Association rules, Computer science, candidate generation, Itemsets, frequent closed itemset mining, FPLCI-mining]
An efficient Fuzzy C-Means clustering algorithm
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The Fuzzy C-Means (FCM) algorithm is commonly used for clustering. The performance of the FCM algorithm depends on the selection of the initial cluster center and/or the initial membership value. If a good initial cluster center that is close to the actual final cluster center can be found, the FCM algorithm will converge very quickly and the processing time can be drastically reduced. The authors propose a novel algorithm for efficient clustering. This algorithm is a modified FCM called the psFCM algorithm, which significantly reduces the computation time required to partition a dataset into desired clusters. We find the actual cluster center by using a simplified set of the original complete dataset. It refines the initial value of the FCM algorithm to speed up the convergence time. Our experiments show that the proposed psFCM algorithm is on average four times faster than the original FCM algorithm. We also demonstrate that the quality of the proposed psFCM algorithm is the same as the FCM algorithm.
[Clustering methods, FCM algorithm, final cluster center, fuzzy set theory, initial cluster center, modified FCM, psFCM algorithm, convergence time, Data mining, database management systems, Convergence, optimisation, System performance, computation time, Clustering algorithms, Data analysis, Target tracking, data analysis, initial value, Partitioning algorithms, Pattern recognition, initial membership value, Image segmentation, pattern clustering, Fuzzy C-Means clustering algorithm]
SSDT: a scalable subspace-splitting classifier for biased data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Decision trees are extensively used data mining models. Recently, a number of efficient, scalable algorithms for constructing decision trees on large disk-resident datasets have been introduced. In this paper, we study the problem of learning scalable decision trees from datasets with biased class distribution. Our objective is to build decision trees that are more concise and more interpretable while maintaining the scalability of the model. To achieve this, our approach searches for subspace clusters of data cases of the biased class to enable multivariate splittings based on weighted distances to such clusters. In order to build concise and interpretable models, other approaches including multivariate decision trees and association rules, often introduce scalability and performance issues. The SSDT algorithm we present achieves the objective without loss in efficiency, scalability, and accuracy.
[subspace cluster search, Scalability, data mining, Predictive models, association rules, Data mining, scalability, efficient scalable algorithms, multivariate splittings, Intrusion detection, Clustering algorithms, Training data, SSDT, weighted distances, disk-resident dataset, Decision trees, learning (artificial intelligence), biased data, Testing, pattern classification, biased class distribution, data mining models, Partitioning algorithms, Association rules, scalable subspace-splitting classifier, performance, decision trees, scalable decision tree learning]
Time series segmentation for context recognition in mobile devices
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Recognizing the context of use is important in making mobile devices as simple to use as possible. Finding out what the user's situation is can help the device and underlying service in providing an adaptive and personalized user interface. The device can infer parts of the context of the user from sensor data: the mobile device can include sensors for acceleration, noise level, luminosity, humidity, etc. In this paper we consider context recognition by unsupervised segmentation of time series produced by sensors. Dynamic programming can be used to find segments that minimize the intra-segment variances. While this method produces optimal solutions, it is too slow for long sequences of data. We present and analyze randomized variations of the algorithm. One of them, global iterative replacement or GIR, gives approximately optimal results in a fraction of the time required by dynamic programming. We demonstrate the use of time series segmentation in context recognition for mobile phone applications.
[Algorithm design and analysis, sensor data, Laboratories, Context awareness, mobile phone applications, Mobile communication, sensor fusion, Mobile handsets, minimized intrasegment variances, user interfaces, Noise level, adaptive personalized user interface, acceleration, context recognition, Humidity, Cost function, noise level, luminosity, humidity, Dynamic programming, global iterative replacement, dynamic programming, time series segmentation, time series, randomized algorithm, mobile devices, Acceleration, cellular radio]
On effective conceptual indexing and similarity search in text data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Similarity search in text has proven to be an interesting problem from the qualitative perspective because of inherent redundancies and ambiguities in textual descriptions. The methods used in search engines in order to retrieve documents most similar to user-defined sets of keywords are not applicable to targets which are medium to large size documents, because of even greater noise effects, stemming from the presence of a large number of words unrelated to the overall topic in the document. Inverted representation is the dominant method for indexing text, but it is not as suitable for document-to-document similarity search, as for short user queries. One way of improving the quality of similarity search is Latent Semantic Indexing (LSI), which maps the documents from the original set of words to a concept space. Unfortunately, LSI maps the data into a domain, in which it is not possible to provide effective indexing techniques. The authors investigate new ways of providing conceptual search among documents by creating a representation in terms of conceptual word-chains. This technique also allows effective indexing techniques so that similarity queries can be performed on large collections of documents by accessing a small amount of data. We demonstrate that our scheme outperforms standard textual similarity search on the inverted representation both in terms of quality and search efficiency.
[Vocabulary, text analysis, search engines, similarity search, keywords, short user queries, textual descriptions, noise effects, Latent Semantic Indexing, conceptual document search, indexing techniques, query processing, qualitative perspective, document retrieval, text data, Intrusion detection, concept space, similarity queries, inverted representation, Search engines, Libraries, document-to-document similarity search, Recommender systems, search problems, conceptual indexing, LSI, indexing, user-defined sets, Information retrieval, Large scale integration, conceptual word-chains, standard textual similarity search, Indexing]
Evolutionary structure learning algorithm for Bayesian network and Penalized Mutual Information metric
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The paper formulates the problem of learning Bayesian network structures from data as determining the structure that best approximates the probability distribution indicated by the data. A new metric, Penalized Mutual Information metric, is proposed, and an evolutionary algorithm is designed to search for the best structure among alternatives. The experimental results show that this approach is reliable and promising.
[Algorithm design and analysis, data analysis, Bayesian network structure learning, Genetic mutations, probability, Evolutionary computation, evolutionary structure learning algorithm, Probability distribution, Mathematics, uncertainty, Distributed computing, evolutionary algorithm, evolutionary computation, Databases, Bayesian methods, Penalized Mutual Information metric, probability distribution, Computer networks, structure search, belief networks, learning (artificial intelligence), Mutual information, search problems]
Statistical considerations in learning from data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In this paper, we focus on statistics. Classical statistics and Bayesian statistics are both employed in data mining. Both have advantages but both also have severe limitations in this context. We point out some of these limitations as well as some of the advantages. The fact that we may need to take account of evidence both internal and external to the data set presents a difficulty for classical statistics. The need to incorporate an objective measure of reliability creates a difficulty for Bayesian statistics. We outline an approach to uncertainty that promises to capture the best of both worlds by incorporating both background knowledge and objectivity.
[Uncertainty, Humans, data mining, classical statistics, uncertainty handling, Cognition, Probability distribution, reliability theory, Data mining, uncertainty, Statistics, objectivity, Sociotechnical systems, Databases, Bayesian methods, learning from data, Bayesian statistics, background knowledge, Sampling methods, Bayes methods, learning (artificial intelligence), objective reliability measure, statistics]
Hierarchical text classification and evaluation
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Hierarchical classification refers to the assignment of one or more suitable categories from a hierarchical category space to a document. While previous work in hierarchical classification focused on virtual category trees where documents are assigned only to the leaf categories, we propose a top-down level-based classification method that can classify documents to both leaf and internal categories. As the standard performance measures assume independence between categories, they have not considered the documents incorrectly classified into categories that are similar to or not far from correct ones in the category tree. We therefore propose category-similarity measures and distance-based measures to consider the degree of misclassification in measuring the classification performance. An experiment has been carried out to measure the performance of our proposed hierarchical classification method. The results showed that our method performs well for a Reuters text collection when enough training documents are given and the new measures have indeed considered the contributions of misclassified documents.
[text analysis, leaf categories, topdown level-based classification method, hierarchical text evaluation, category-similarity measures, trees (mathematics), document, classification, performance measures, Sun, Information systems, distance-based measures, hierarchical category space, internal categories, Tree graphs, hierarchical text classification, Space technology, Text categorization, Reuters text collection, category tree, Classification tree analysis, Testing]
Using artificial anomalies to detect unknown and known network intrusions
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Intrusion detection systems (IDSs) must be capable of detecting new and unknown attacks, or anomalies. We study the problem of building detection models for both pure anomaly detection and combined misuse and anomaly detection (i.e., detection of both known and unknown intrusions). We propose an algorithm to generate artificial anomalies to coerce the inductive learner into discovering an accurate boundary between known classes (normal connections and known intrusions) and anomalies. Empirical studies show that our pure anomaly detection model trained using normal and artificial anomalies is capable of detecting more than 77% of all unknown intrusion classes with more than. 50% accuracy per intrusion class. The combined misuse and anomaly detection models are as accurate as a pure misuse detection model in detecting known intrusions and are capable of detecting at least 50% of unknown intrusion classes with accuracy measurements between 75% and 100% per class.
[intrusion detection systems, known network intrusion detection, Machine learning algorithms, Data analysis, Event detection, Government, artificial anomalies, combined misuse/anomaly detection, inductive learner, Educational institutions, Computer science, computer network management, pure anomaly detection, security of data, Web and internet services, Intrusion detection, Training data, unknown network intrusion detection, Pattern matching]
Association rules enhanced classification of underwater acoustic signal
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The classification of underwater acoustic signals is one of the important fields of pattern recognition. Inspired by the experience of training human experts in sonar, we propose a two-phase training algorithm to exploit association rules to reveal understandable intrinsic rules which contribute to correct classification in known mis-classification data sets. Preliminary experimental results demonstrate the potential of these classification association rules to enhance the classification accuracy of underwater acoustic signals.
[underwater acoustic signal classification, misclassification data sets, Laboratories, Sonar, underwater sound, data mining, Pattern recognition, Classification algorithms, sonar, Association rules, Data mining, classification accuracy enhancement, signal classification, two-phase training algorithm, understandable intrinsic rules, sonar signal processing, Neural networks, Feature extraction, learning (artificial intelligence), Underwater acoustics, classification association rules, Classification tree analysis, pattern recognition]
The EQ framework for learning equivalence classes of Bayesian networks
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
This paper proposes a theoretical and an algorithmic framework for the analysis and the design of efficient learning algorithms which explore the space of equivalence classes of Bayesian network structures. This framework is composed of a generic learning model which uses essential graphs and more general partially directed graphs in order to represent the equivalence classes evaluated during search, operational characterizations of these graphs, processing procedures and formulas for directly calculating their score. The experimental results of the algorithms designed within this framework show that the space of equivalence classes may be explored efficiently and with better results than the classical search in the space of Bayesian network structures.
[Algorithm design and analysis, processing procedures, Decision making, data mining, essential graphs, operational characterizations, Probability distribution, Data mining, generic learning model, EQ framework, score, search, Databases, Bayesian methods, Sections, directed graphs, Space exploration, Bayesian networks, belief networks, learning (artificial intelligence), equivalence classes, equivalence class learning, search problems, Testing]
Mining the smallest association rule set for predictions
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Mining transaction databases for association rules usually generates a large number of rules, most of which are unnecessary when used for subsequent prediction. In this paper we define a rule set for a given transaction database that is much smaller than the association rule set but makes the same predictions as the association rule set by the confidence priority. We call this subset the informative rule set. The informative rule set is not constrained to particular target items; and it is smaller than the non-redundant association rule set. We present an algorithm to directly generate the informative rule set, i.e., without generating all frequent itemsets first, and that accesses the database less often than other unconstrained direct methods. We show experimentally that the informative rule set is much smaller than both the association rule set and the non-redundant association rule set, and that it can be generated more efficiently.
[transaction processing, Costs, informative rule set, data mining, Transaction databases, Data mining, Association rules, Information technology, predictions, Itemsets, very large databases, database access, smallest association rule set mining, transaction database mining]
Efficiently mining maximal frequent itemsets
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
We present GenMax, a backtracking search based algorithm for mining maximal frequent itemsets. GenMax uses a number of optimizations to prune the search space. It uses a novel technique called progressive focusing to perform maximality checking, and diffset propagation to perform fast frequency computation. Systematic experimental comparison with previous work indicates that different methods have varying strengths and weaknesses based on dataset characteristics. We found GenMax to be a highly efficient method to mine the exact set of maximal patterns.
[progressive focusing, data mining, Transaction databases, Data mining, Association rules, Computer science, optimisation, Itemsets, efficient maximal frequent itemset mining, Frequency, backtracking, backtrack search based algorithm, diffset propagation, dataset, GenMax, search space pruning, optimizations, maximality checking]
Using rough sets theory and database operations to construct a good ensemble of classifiers for data mining applications
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The article presents a novel approach to constructing a good ensemble of classifiers using rough set theory and database operations. Ensembles of classifiers are formulated precisely within the framework of rough set theory and constructed very efficiently by using set-oriented database operations. Our method first computes a set of reducts which include all the indispensable attributes required for the decision categories. For each reduct, a reduct table is generated by removing those attributes which are not in the reduct. Next, a novel rule induction algorithm is used to compute the maximal generalized rules for each reduct table and a set of reduct classifiers is formed based on the corresponding reducts. The distinctive features of our method as compared to other methods of constructing ensembles of classifiers are: (1) presents a theoretical model to explain the mechanism of constructing ensemble of classifiers; (2) each reduct is a minimum subset of attributes and has the same classification ability as the entire attributes; (3) each reduct classifier constructed from the corresponding reduct has a minimal set of classification rules, and is as accurate and complete as possible and at the same time as diverse as possible from the other classifiers; (4) the test indicates that the number of classifiers used to improve the accuracy is much less than other methods.
[data mining, reduct classifiers, reducts, classification ability, Data mining, decision categories, reduct classifier, indispensable attributes, Databases, database operations, Voting, very large databases, Rough sets, Training data, Decision trees, data mining applications, rule induction algorithm, Testing, maximal generalized rules, pattern classification, Induction generators, Boosting, minimum subset, inference mechanisms, decision tables, classification rules, rough set theory, reduct table, Bagging, set-oriented database operations]
Applications of data mining in hydrology
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Long-term range streamflow forecast plays an invaluable role in water resource planning and management. The potential applicability and limitations of the time series forecasting approach using neural network with the multiresolution learning paradigm (NNMLP) are investigated. The predicted longterm range streamflows using the NNMLP are compared with the observations. The results show that the time series forecasting approach of NNMLP has good predicting skill. The NNMLP requires only historical streamflow information. The time series forecasting approach of NNMLP has great potential for being used alone in regions with limited available information, and for being combined with other approaches to improve long-term range streamflow forecasts.
[Weather forecasting, data mining, Predictive models, Data mining, long-term range streamflow forecast, Environmental management, neural network, time series forecasting approach, learning (artificial intelligence), Feedforward systems, Water resources, data mining applications, hydrology, Artificial neural networks, geophysics computing, time series, water supply, Feedforward neural networks, water resource planning, predicted long-term range streamflows, historical streamflow information, multiresolution learning paradigm, Hydrology, Neural networks, forecasting theory, predicting skill, neural nets, NNMLP]
Visualizing association mining results through hierarchical clusters
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
We propose a new methodology for visualizing association mining results. Inter-item distances are computed from combinations of itemset supports. The new distances retain a simple pairwise structure, and are consistent with important frequently occurring itemsets. Thus standard tools of visualization, e.g. hierarchical clustering dendrograms can still be applied, while the distance information upon which they are based is richer. Our approach is applicable to general association mining applications, as well as applications involving information spaces modeled by directed graphs, e.g. the Web. In the context of collections of hypertext documents, the inter-document distances capture the information inherent in a collection's link structure, a form of link mining. We demonstrate our methodology with document sets extracted from the Science Citation Index, applying a metric that measures consistency between clusters and frequent itemsets.
[Visualization, data mining, hypermedia, Data mining, Information systems, itemset supports, Itemsets, hierarchical clustering dendrograms, Science Citation Index, inter-item distances, link mining, Search engines, citation analysis, Libraries, Performance analysis, information spaces, Keyword search, information retrieval, hypertext documents, hierarchical clusters, association mining results visualization, directed graphs, Web pages, WWW, frequently occurring itemsets, Web sites, collection link structure]
CMAR: accurate and efficient classification based on multiple class-association rules
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Previous studies propose that associative classification has high classification accuracy and strong flexibility at handling unstructured data. However, it still suffers from the huge set of mined rules and sometimes biased classification or overfitting since the classification is based on only a single high-confidence rule. The authors propose a new associative classification method, CMAR, i.e., Classification based on Multiple Association Rules. The method extends an efficient frequent pattern mining method, FP-growth, constructs a class distribution-associated FP-tree, and mines large databases efficiently. Moreover, it applies a CR-tree structure to store and retrieve mined association rules efficiently, and prunes rules effectively based on confidence, correlation and database coverage. The classification is performed based on a weighted /spl chi//sup 2/ analysis using multiple strong association rules. Our extensive experiments on 26 databases from the UCI machine learning database repository show that CMAR is consistent, highly effective at classification of various kinds of databases and has better average classification accuracy in comparison with CBA and C4.5. Moreover, our performance study shows that the method is highly efficient and scalable in comparison with other reported associative classification methods.
[CMAR, UCI machine learning database repository, data mining, Predictive models, classification accuracy, Electronic mail, Data mining, mined association rules, mined rules, Databases, biased classification, very large databases, Training data, single high-confidence rule, Classification based on Multiple Association Rules, tree data structures, learning (artificial intelligence), Classification tree analysis, FP-growth, large database mining, pattern classification, overfitting, multiple strong association rules, class distribution-associated FP-tree, Information retrieval, Association rules, multiple class-association rules, associative processing, frequent pattern mining method, associative classification method, Councils, database coverage, Machine learning, unstructured data, CR-tree structure, efficient classification, weighted /spl chi//sup 2/ analysis]
The DIAsDEM framework for converting domain-specific texts into XML documents with data mining techniques
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Modern organizations are accumulating huge volumes of textual documents. To turn archives into valuable knowledge sources, textual content must become explicit and able to be queried. Semantic tagging with markup languages such as XML satisfies both requirements. We thus introduce the DIAsDEM* framework for extracting semantics from structural text units (e.g., sentences), assigning XML tags to them and deriving a flat XML DTD for the archive. DIAsDEM focuses on archives characterized by a peculiar terminology and by an implicit structure such as court filings and company reports. In the knowledge discovery phase, text units are iteratively clustered by similarity of their content. Each iteration outputs clusters satisfying a set of quality criteria. Text units contained in these clusters are tagged with semiautomatically determined cluster labels and XML tags respectively. Additionally, extracted named entities (e.g., persons) serve as attributes of XML tags. We apply the framework in a case study on the German Commercial Register.
[Vocabulary, Terminology, Project management, data mining, XML documents, Relational databases, semantic tagging, knowledge discovery, Data mining, DIAsDEM framework, markup languages, domain-specific text conversion, terminology, court filings, hypermedia markup languages, Text mining, structural text units, semiautomatically determined cluster labels, flat XML DTD, company reports, content similarity, Knowledge management, archive, iterative clustering, German Commercial Register, Markup languages, XML, quality criteria, Tagging, data warehouses]
Fuzzy data mining: effect of fuzzy discretization
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
When we generate association rules, continuous attributes have to be discretized into intervals while our knowledge representation is not always based on such discretization. For example, we usually use some linguistic terms (e.g., young, middle age, and old) for dividing our ages into some fuzzy categories. We describe the extraction of linguistic association rules and examine the performance of extracted rules. First we modify the definitions of the two basic measures (i.e., confidence and support) of association rules for extracting linguistic association rules. The main difference between standard and linguistic association rules is the discretization of continuous attributes. We divide the domain interval of each attribute into some fuzzy regions (i.e., linguistic terms) when we extract linguistic association rules. Next, we compare fuzzy discretization with standard non-fuzzy discretization through computer simulations on a pattern classification problem with many continuous attributes. The classification performance of extracted rules on unseen test patterns is examined under various conditions. Simulation results show that linguistic association rules with rule weights have high generalization ability even when the domain of each continuous attribute is homogeneously partitioned.
[domain interval, pattern classification problem, continuous attribute, computational linguistics, Humans, data mining, fuzzy set theory, extracted rules, generalization ability, Data mining, rule weights, fuzzy data mining, knowledge based systems, Testing, pattern classification, standard nonfuzzy discretization, Computer simulation, Computational modeling, basic measures, Knowledge representation, Industrial engineering, continuous attributes, computer simulations, Association rules, fuzzy categories, Fuzzy logic, linguistic terms, associative processing, fuzzy regions, knowledge representation, fuzzy discretization, Machine learning, unseen test patterns, linguistic association rules, association rule generation, discretization]
Mining constrained association rules to predict heart disease
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
This work describes our experiences in discovering association rules in medical data to predict heart disease. We focus on two aspects of this work: mapping medical data to a transaction format suitable for mining association rules, and identifying useful constraints. Based on these aspects we introduce an improved algorithm to discover constrained association rules. We present an experimental section explaining several interesting discovered rules.
[transaction processing, Data analysis, transaction format, Cardiac disease, data mining, Educational institutions, diseases, medical information systems, heart disease prediction, Data mining, Association rules, Diagnostic expert systems, cardiology, medical data, medical computing, Medical diagnostic imaging, Regression tree analysis, constrained association rule mining, Biomedical imaging, Classification tree analysis]
A scalable algorithm for clustering sequential data
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
In recent years, we have seen an enormous growth in the amount of available commercial and scientific data. Data from domains such as protein sequences, retail transactions, intrusion detection, and Web-logs have an inherent sequential nature. Clustering of such data sets is useful for various purposes. For example, clustering of sequences from commercial data sets may help marketer identify different customer groups based upon their purchasing patterns. Grouping protein sequences that share similar structure helps in identifying sequences with similar functionality. Over the years, many methods have been developed for clustering objects according to their similarity. However these methods tend to have a computational complexity that is at least quadratic on the number of sequences. In this paper we present an entirely different approach to sequence clustering that does not require an all-against-all analysis and uses a near-linear complexity K-means based clustering algorithm. Our experiments using data sets derived from sequences of purchasing transactions and protein sequences show that this approach is scalable and leads to reasonably good clusters.
[Algorithm design and analysis, dataset clustering, US Department of Energy, sequential data clustering, Military computing, data mining, scalable algorithm, Partitioning algorithms, sequences, Computational complexity, Proteins, Computer science, near-linear complexity K-means based clustering algorithm, pattern clustering, biology computing, Clustering algorithms, Intrusion detection, proteins, purchasing transaction sequences, molecular biophysics, protein sequences, Contracts, retail data processing, computational complexity]
The computational complexity of high-dimensional correlation search
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
There is a growing awareness that the popular support metric (often used to guide search in market-basket analysis) is not appropriate for use in every association mining application. Support measures only the co-occurrence frequency of a set of events when determining which patterns to report back to the user. It incorporates no rigorous statistical notion of surprise or interest, and many of the patterns deemed interesting by the support metric are uninteresting to the user. However, a positive aspect of support is that search using support is very efficient. The question addresses in the paper is: can we retain this efficiency if we move beyond support, and to other more rigorous metrics? We consider the computational implications of incorporating simple expectation into the data mining task. It turns out that many variations on the problem which incorporate more rigorous tests of dependence (or independence) result in NP-hard problem definitions.
[association mining application, Statistical analysis, data mining, market-basket analysis searching, Probability, Educational institutions, NP-hard problem definitions, Frequency measurement, Data mining, Appropriate technology, rigorous metrics, Computational complexity, computational implications, support metric, associative processing, NP-hard problem, simple expectation, high-dimensional correlation search, Statistical distributions, data mining task, search problems, co-occurrence frequency, Testing, computational complexity]
H-mine: hyper-structure mining of frequent patterns in large databases
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Methods for efficient mining of frequent patterns have been studied extensively by many researchers. However, the previously proposed methods still encounter some performance bottlenecks when mining databases with different data characteristics, such as dense vs. sparse, long vs. short patterns, memory-based vs. disk-based, etc. In this study, we propose a simple and novel hyper-linked data structure, H-struct and a new mining algorithm, H-mine, which takes advantage of this data structure and dynamically adjusts links in the mining process. A distinct feature of this method is that it has very limited and precisely predictable space overhead and runs really fast in memory-based setting. Moreover it can be scaled up to very large databases by database partitioning, and when the data set becomes dense, (conditional) FP-trees can be constructed dynamically as part of the mining process. Our study shows that H-mine has high performance in various kinds of data, outperforms the previously developed algorithms in different settings, and is highly scalable in mining large databases. This study also proposes a new data mining methodology, space-preserving mining, which may have strong impact in the future development of efficient and scalable data mining methods.
[dynamic link adjustment, space overhead, space-preserving mining, frequent patterns, data mining, Data mining, Counting circuits, hyperlinked data structure, very large databases, Clustering algorithms, hyper-structure mining, H-struct, data structures, Assembly, Data analysis, Data structures, Spatial databases, Partitioning algorithms, memory-based setting, database theory, H-mine, FP-trees, Frequency, Iterative algorithms, large databases]
Evaluating boosting algorithms to classify rare classes: comparison and improvements
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Classification of rare events has many important data mining applications. Boosting is a promising meta-technique that improves the classification performance of any weak classifier. So far, no systematic study has been conducted to evaluate how boosting performs for the task of mining rare classes. The authors evaluate three existing categories of boosting algorithms from the single viewpoint of how they update the example weights in each iteration, and discuss their possible effect on recall and precision of the rare class. We propose enhanced algorithms in two of the categories, and justify their choice of weight updating parameters theoretically. Using some specially designed synthetic datasets, we compare the capability of all the algorithms from the rare class perspective. The results support our qualitative analysis, and also indicate that our enhancements bring an extra capability for achieving better balance between recall and precision in mining rare classes.
[Performance evaluation, Algorithm design and analysis, Costs, Error analysis, enhanced algorithms, data mining, Classification algorithms, boosting algorithms, Data mining, database management systems, example weights, Voting, rare classes, learning (artificial intelligence), data mining applications, synthetic datasets, pattern classification, rare event classification, Boosting, Surges, weight updating parameters, Computer science, classification performance, qualitative analysis, weak classifier, meta technique]
FARM: a framework for exploring mining spaces with multiple attributes
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
Mining for frequent itemsets typically involves a preprocessing step in which data with multiple attributes are grouped into transactions, and items are defined based on attribute values. We hake observed that such fixed attribute mining can severely constrain the patterns that are discovered. Herein, we introduce mining spaces, a new framework for mining multi-attribute data that includes the discovery of transaction and item definitions (with the exploitation of taxonomies and functional dependencies if they are available). We prove that special downward closure properties (or anti-monotonic property) hold for mining spaces, a result that allows us to construct efficient algorithms for mining patterns without the constraints of fixed attribute mining. We apply our algorithms to real world data collected from a production computer network. The results show that by exploiting the special kinds of downward closure in mining spaces, execution times for mining can be reduced by a factor of three to four.
[transaction processing, Pediatrics, Taxonomy, data mining, multiple attributes, Data mining, transactions, frequent itemset mining, downward closure properties, mining space exploration, fixed attribute mining, production computer network, Itemsets, FARM, Production, Complex networks, transaction definition discovery, Computer networks, item definition discovery, Space exploration, efficient algorithms, Computer network management]
Neural analysis of mobile radio access network
Proceedings 2001 IEEE International Conference on Data Mining
None
2001
The self-organizing map (SOM) is an efficient tool for visualization and clustering of multidimensional data. It transforms the input vectors on two-dimensional grid of prototype vectors and orders them. The ordered prototype vectors are easier to visualize and explore than the original data. Mobile networks produce a huge amount of spatiotemporal data. The data consists of parameters of base stations (BS) and quality information of calls. There are two alternatives in starting the data analysis. We can build either a general one-cell-model trained using state vectors from all cells, or a model of the network using state vectors with parameters from all mobile cells. In both methods, further analysis is needed to understand the reasons for various operational states of the entire network.
[multidimensional data clustering, input vector transformation, operational states, Laboratories, data mining, Data engineering, Data mining, Multiaccess communication, telecommunication computing, state vectors, Information science, self-organising feature maps, Clustering algorithms, Prototypes, call quality information, self-organizing map, one cell model, multidimensional data visualization, Data analysis, mobile radio, data analysis, spatiotemporal data, radio access networks, Land mobile radio, pattern clustering, Data visualization, 2D prototype vector grid, neural analysis, mobile radio access network, base stations]
SLPMiner: an algorithm for finding frequent sequential patterns using length-decreasing support constraint
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Over the years, a variety of algorithms for finding frequent sequential patterns in very large sequential databases have been developed. The key feature in most of these algorithms is that they use a constant support constraint to control the inherently exponential complexity of the problem. In general, patterns that contain only a few items will tend to be interesting if they have good support, whereas long patterns can still be interesting even if their support is relatively small. Ideally, we need an algorithm that finds all the frequent patterns whose support decreases as a function of their length. In this paper we present an algorithm called SLPMiner that finds all sequential patterns that satisfy a length-decreasing support constraint. Our experimental evaluation shows that SLPMiner achieves up to two orders of magnitude of speedup by effectively exploiting the length-decreasing support constraint, and that its runtime increases gradually as the average length of the sequences (and the discovered frequent patterns) increases.
[US Department of Energy, algorithms, SLPMiner, data mining, frequent sequential pattern finding, speedup, constant support constraint, runtime, Spatial databases, Data mining, Association rules, sequences, very large sequential databases, exponential complexity, Computer science, length-decreasing support constraint, Runtime, Itemsets, High performance computing, very large databases, Contracts]
Evaluating the utility of statistical phrases and latent semantic indexing for text classification
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The term-based vector space model is a prominent technique for retrieving textual information. In this paper we examine the usefulness of phrases as terms in vector-based document classification. We focus on statistical techniques to extract both adjacent and window phrases from documents. We discover that the positive effect of adding phrase terms is very limited, if we have already achieved good performance using single-word terms, even when SVD/LSI is used as the dimensionality reduction method.
[text analysis, Dictionaries, latent semantic indexing, adjacent phrase extraction, text classification, Data mining, statistical phrases, vector-based document classification, dimensionality reduction method, indexing, information retrieval, textual information retrieval, Information retrieval, Large scale integration, classification, Computer science, Support vector machines, term-based vector space model, Text categorization, Support vector machine classification, window phrase extraction, Frequency, statistical analysis, Indexing, single-word terms]
Convex Hull Ensemble Machine
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We propose a new ensemble algorithm called Convex Hull Ensemble Machine (CHEM). CHEM in Hilbert space is developed first and it is modified to regression and classification problems. Empirical studies show that in classification problems CHEM has similar prediction accuracy as AdaBoost, but CHEM is much more robust to output noise. In regression problems, CHEM works competitively with other ensemble methods such as Gradient Boost and Bagging.
[Solid modeling, Machine learning algorithms, data mining, Accuracy, CHEM, Gradient Boost, Hilbert space, Noise robustness, Decision trees, learning (artificial intelligence), ensemble algorithm, pattern classification, output noise, Hilbert spaces, classification, machine learning, Statistics, Geometry, AdaBoost, Convex Hull Ensemble Machine, regression, Machine learning, decision trees, statistical analysis, Bagging]
Linear Causal Model discovery using the MML criterion
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Determining the causal structure of a domain is a key task in the area of data mining and knowledge discovery. The algorithm proposed by Wallace et al. (1996) has demonstrated its strong ability in discovering Linear Causal Models from given data sets. However some experiments showed that this algorithm experienced difficulty in discovering linear relations with small deviation, and it occasionally gives a negative message length, which should not be allowed. In this paper a more efficient and precise MML encoding scheme is proposed to describe the model structure and the nodes in a Linear Causal Model. The estimation of different parameters is also derived. Empirical results show that the new algorithm outperformed the previous MML-based algorithm in terms of both speed and precision.
[MML criterion, Data analysis, Linear Causal Model discovery, Power system management, data mining, large database, directed acyclic graph, linear relation discovery, knowledge discovery, Information technology, Statistics, Diagnostic expert systems, database theory, Road transportation, Graphical models, very large databases, directed graphs, data sets, parameter estimation, Database systems, Australia, Energy management, software performance evaluation]
ESRS: a case selection algorithm using extended similarity-based rough sets
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
A case selection algorithm selects representative cases from a large data set for future case-based reasoning tasks. This paper proposes the ESRS algorithm, based on extended similarity-based rough set theory, which selects a reasonable number of the representative cases while maintaining satisfactory classification accuracy. It also can handle noise and inconsistent data. Experimental results on synthetic and real sets of cases showed that its predictive accuracy is similar to that of well-known machine learning systems on standard data sets, while it has the advantage of being applicable to any data set where a similarity function can be defined.
[complexity, Paramagnetic resonance, Computer aided software engineering, Machine learning algorithms, case selection, rough sets, data mining, machine learning, Nearest neighbor searches, ESRS, Computer science, Accuracy, Rough sets, Clustering algorithms, Frequency, Set theory, case-based reasoning, learning (artificial intelligence), rough set theory]
A comparative study of RNN for outlier detection in data mining
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We have proposed replicator neural networks (RNNs) for outlier detection. We compare RNN for outlier detection with three other methods using both publicly available statistical datasets (generally small) and data mining datasets (generally much larger and generally real data). The smaller datasets provide insights into the relative strengths and weaknesses of RNNs. The larger datasets in particular test scalability and practicality of application.
[Recurrent neural networks, data mining datasets, data mining, Data mining, Helium, outlier detection, scalability, Multi-layer neural network, Intelligent networks, Databases, Neural networks, very large databases, statistical databases, Australia, Feedforward systems, statistical datasets, neural nets, Testing, replicator neural networks]
A hybrid approach to discover Bayesian networks from databases using evolutionary programming
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Describes a data mining approach that employs evolutionary programming to discover knowledge represented in Bayesian networks. There are two different approaches to the network learning problem. The first one uses dependency analysis, while the second one searches good network structures according to a metric. Unfortunately, both approaches have their own drawbacks. Thus, we propose a hybrid algorithm of the two approaches, which consists of two phases, namely, the conditional independence test and the search phases. A new operator is introduced to further enhance the search efficiency. We conduct a number of experiments and compare the hybrid algorithm with our previous algorithm, MDLEP, which uses EP for network learning. The empirical results illustrate that the new approach has better performance. We apply the approach to data sets of direct marketing and compare the performance of the evolved Bayesian networks obtained by the new algorithm with the models generated by other methods. In the comparison, the induced Bayesian networks produced by the new algorithm outperform the other models.
[Algorithm design and analysis, dependency analysis, data mining, knowledge discovery, Data mining, Information systems, Databases, Ores, Genetic programming, network learning problem, search efficiency, belief networks, learning (artificial intelligence), search phases, MDLEP, search problems, Testing, hybrid approach, Data analysis, evolutionary programming, direct marketing, Computer science, conditional independence test, marketing, evolutionary computation, Bayesian methods, knowledge representation, Bayesian networks]
Efficient discovery of common substructures in macromolecules
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Biological macromolecules play a fundamental role in disease; therefore, they are of great interest to fields such as pharmacology and chemical genomics. Yet due to macromolecules' complexity, development of effective techniques for elucidating structure-function macromolecular relationships has been ill explored. Previous techniques have either focused on sequence analysis, which only approximates structure-function relationships, or on small coordinate datasets, which does not scale to large datasets or handle noise. We present a novel scalable approach to efficiently discover macromolecule substructures based on three-dimensional coordinate data, without domain-specific knowledge. The approach combines structure-based frequent pattern discovery with search space reduction and coordinate noise handling. We analyze computational performance compared to traditional approaches, validate that our approach can discover meaningful substructures in noisy macromolecule data by automated discovery of primary and secondary protein structures, and show that our technique is superior to sequence-based approaches at determining structural, and thus functional, similarity between proteins.
[macromolecules, Sequences, Genomics, data mining, coordinate noise handling, Data mining, Chemicals, search space reduction, Diseases, Proteins, Information science, Molecular biophysics, pharmacology, Biology computing, common substructures discovery, medical administrative data processing, structure-based frequent pattern discovery, Marine vehicles, biological macromolecules, chemical genomics]
A formal model for user preference
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Personalization and recommendation systems require a formalized model for user preference. We present the formal model of preference including positive preference and negative preference. For rare events, we apply the probability of random occurrence in order to reduce noise effects caused by data sparseness. Pareto distribution is adopted for the random occurrence probability. We also present the method for combining information of joint feature variables in different sizes by dynamic weighting using random occurrence probability.
[positive preference, Noise reduction, data mining, noise effects, Information filtering, History, user modelling, user preference model, personalization, Training data, Information filters, e-commerce, rare events, electronic commerce, information resources, dynamic weighting, Navigation, data sparseness, Machine intelligence, recommendation systems, Pareto distribution, formal model, Lungs, Decision theory, negative preference, Collaboration, random occurrence, random occurrence probability]
Adapting classification rule induction to subgroup discovery
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Rule learning is typically used for solving classification and prediction tasks. However learning of classification rules can be adapted also to subgroup discovery. This paper shows how this can be achieved by modifying the covering algorithm and the search heuristic, performing probabilistic classification of instances, and using an appropriate measure for evaluating the results of subgroup discovery. Experimental evaluation of the CN2-SD subgroup discovery algorithm on 17 UCI data sets demonstrates substantial reduction of the number of induced rules, increased rule coverage and rule significance, as well as slight improvements in terms of the area under the ROC curve.
[Performance evaluation, Algorithm design and analysis, CN2-SD, Machine learning algorithms, covering algorithm, Area measurement, data mining, Data mining, experimental evaluation, Databases, heuristic programming, very large databases, rule coverage, ROC curve, learning (artificial intelligence), search problems, subgroup discovery, Java, pattern classification, classification rule induction, UCI data sets, rule learning, probability, prediction tasks, Association rules, probabilistic classification, search heuristic, Machine learning]
Ensemble modeling through multiplicative adjustment of class probability
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We develop a new concept for aggregating items of evidence for class probability estimation. In Naive Bayes, each feature contributes an independent multiplicative factor to the estimated class probability. We modify this model to include an exponent in each factor in order to introduce feature importance. These exponents are chosen to maximize the accuracy of estimated class probabilities on the training data. For Naive Bayes, this modification accomplishes more than what feature selection can. More generally, since the individual features can be the outputs of separate probability models, this yields a new ensemble modeling approach, which we call APM (Adjusted Probability Model), along with a regularized version called APMR.
[Additives, Parameter estimation, machine learning data set, UCI dataset, data mining, Predictive models, APMR, Electronic mail, Niobium, Sections, very large databases, training data, learning (artificial intelligence), probability, multiplicative adjustment, Boosting, Yield estimation, class probability estimation, Adjusted Probability Model, Naive Bayes, feature importance, Bayes methods, ensemble modeling, Bagging, Logistics]
A new algorithm for learning parameters of a Bayesian network from distributed data
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We present a novel approach for learning parameters of a Bayesian network from distributed heterogeneous dataset. In this case, the whole dataset is distributed in several sites and each site contains observations for a different subset of features. The new method uses the collective learning approach proposed in our earlier work and substantially reduces the computational and transmission overhead. Theoretical analysis is given and experimental results are provided to illustrate the accuracy and efficiency of our method.
[collective learning, Costs, Data security, Decision making, data mining, directed acyclic graph, Learning systems, Bayesian methods, Asia, Distributed databases, probabilistic graph model, Bandwidth, distributed databases, decision making, learning parameters, Data communication, belief networks, learning (artificial intelligence), distributed heterogeneous dataset, Bayesian network]
On active learning for data acquisition
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Many applications are characterized by having naturally incomplete data on customers - where data on only some fixed set of local variables is gathered However, having a more complete picture can help build better models. The naive solution to this problem - acquiring complete data for all customers s often impractical due to the costs of doing so. A possible alternative is to acquire complete data for "some" customers and to use this to improve the models built. The data acquisition problem is determining how many, and which, customers to acquire additional data from. In this paper we suggest using active learning based approaches for the data acquisition problem. In particular, we present initial methods for data acquisition and evaluate these methods experimentally on web usage data and UCI datasets. Results show that the methods perform well and indicate that active learning based methods for data acquisition can be a promising area for data mining research.
[Costs, Data acquisition, data mining, naturally incomplete data, UCI datasets, Companies, Credit cards, Information management, Data mining, Learning systems, data acquisition, web usage data, learning (artificial intelligence), active learn]
Clustering spatial data when facing physical constraints
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Clustering spatial data is a well-known problem that has been extensively studied to find hidden patterns or meaningful sub-groups and has many applications such as satellite imagery, geographic information systems, medical image analysis, etc. Although many methods have been proposed in the literature, very few have considered constraints such that physical obstacles and bridges linking clusters may have significant consequences on the effectiveness of the clustering. Taking into account these constraints during the clustering process is costly, and the effective modeling of the constraints is of paramount importance for good performance. In this paper we define the clustering problem in the presence of constraints - obstacles and crossings - and investigate its efficiency and effectiveness for large databases. In addition, we introduce a new approach to model these constraints to prune the search space and reduce the number of polygons to test during clustering. The algorithm DBCluC we present detects clusters of arbitrary shape and is insensitive to noise and the input order Its average running complexity is O(NlogN) where N is the number of data objects.
[Geographic Information Systems, data mining, visual databases, geographic information systems, DBCluC algorithm, medical information systems, Bridges, hidden patterns, Satellites, Image analysis, Databases, satellite imagery, Clustering algorithms, Object detection, spatial data clustering, physical constraints, search space, Joining processes, medical image analysis, Biomedical imaging, Testing]
Heterogeneous learner for Web page classification
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Classification of an interesting class of Web pages has been an interesting problem. Typical machine learning algorithms for this problem require two classes of data for training: positive and negative training examples. However in application to Web page classification, gathering an unbiased sample of negative examples appears to be difficult. We propose a heterogeneous learning framework for classifying Web pages, which (1) eliminates the need for negative training data, and (2) increases classification accuracy by using two heterogeneous learners. Our framework uses two heterogeneous learners-a decision list and a linear separator which complement each other-to eliminate the need for negative training data in the training phase and to increase the accuracy in the testing phase. Our results show that our heterogeneous framework achieves high accuracy without requiring negative training data; it enhances the accuracy of linear separators by reducing the errors on "low-margin data". That is, it classifies more accurately while requiring less human efforts in training.
[document handling, pattern classification, Machine learning algorithms, Particle separators, Resumes, Humans, classification accuracy, heterogeneous learner, decision list, linear separator, Computer science, feature extraction, Web pages, Training data, XML, Search engines, low-margin data, Web sites, learning (artificial intelligence), machine learning algorithms, Testing, Web page classification]
On incorporating subjective interestingness into the mining process
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Subjective interestingness is at the heart of the successful discovery of association rules. To determine what is subjectively interesting, users' domain knowledge must be applied. The author (1999) introduced an approach that requires very little domain knowledge and interaction to eliminate the majority of the rules that are subjectively not interesting. In this paper we investigate how this approach can be incorporated into the mining process, the benefits and disadvantages of doing so, and examine the results of its application to real databases.
[Heart, pattern classification, domain knowledge, ancestor item set classification, databases, data mining, Association rules, Data mining, database management systems, Zirconium, Itemsets, Databases, association rule discovery, subjective interestingness]
Speed-up iterative frequent itemset mining with constraint changes
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Mining of frequent itemsets is a fundamental data mining task. Past research has proposed many efficient algorithms for this purpose. Recent work also highlighted the importance of using constraints to focus the mining process to mine only those relevant itemsets. In practice, data mining is often an interactive and iterative process. The user typically changes constraints and runs the mining algorithm many times before being satisfied with the final results. This interactive process is very time consuming. Existing mining algorithms are unable to take advantage of this iterative process to use previous mining results to speed up the current mining process. This results in an enormous waste of time and computation. In this paper, we propose an efficient technique to utilize previous mining results to improve the efficiency of current mining when constraints are changed. We first introduce the concept of tree boundary to summarize useful information available from previous mining. We then show that the tree boundary provides an effective and efficient framework for the new mining. The proposed technique has been implemented in the context of two existing frequent itemset mining algorithms, FP-tree and tree projection. Experiment results on both synthetic and real-life datasets show that the proposed approach achieves a dramatic saving of computation.
[data mining, trees (mathematics), constraint changes, FP-tree, Electronic mail, tree boundary, Data mining, Association rules, interactive iterative process, iterative frequent itemset mining speedup, Itemsets, tree projection, Ores, Frequency, Iterative algorithms]
Considering both intra-pattern and inter-pattern anomalies for intrusion detection
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Various approaches have been proposed to discover patterns from system call trails of UNIX processes to better model application behavior. However, these techniques only consider the relationship between system calls (or system audit events). We first refine the definition of maximal patterns given in (Wespi et al., 2000) and provide a pattern extraction algorithm to identify such maximal patterns. We then add one additional dimension to the problem domain by also taking into consideration the overlap relationship between patterns. We argue that an execution path of an application is usually not an arbitrary combination of various patterns; but rather, they overlap each other in some specific order. Such overlap relationship characterizes the normal behavior of the application. Finally, a novel pattern matching module is proposed to detect intrusions based on both intra-pattern and inter-pattern anomalies. We test this idea using the data sets obtained from the University of New Mexico. The experimental results indicate that our scheme detects significantly more anomalies than the scheme presented in (Wespi et al., 2000) while maintaining a very low false alarm rate.
[Unix, pattern matching, experimental results, data mining, inter-pattern anomalies, intrusion detection, pattern extraction algorithm, Windows, Application software, UNIX, Computer science, pattern matching module, false alarm rate, security of data, intra-pattern anomalies, very large databases, Intrusion detection, data sets, system calls, maximal patterns, Pattern matching, Testing]
Reviewing RELIEF and its extensions: a new approach for estimating attributes considering high-correlated features
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
RELIEF algorithm and its extensions are some of the most known filter methods for estimating the quality of attributes in classification problems dealing with both dependent and independent features. These methods attend to find all meaningful features for each problem (both weakly and strongly ones) so they are usually employed like a first stage for detecting irrelevant attributes. Nevertheless, in this paper we checked that RELIEF-family algorithms present some important limitations that could distort the selection of the final features' subset, specially in the presence of high-correlated attributes. To overcome these difficulties, a new approach has been developed (WACSA algorithm), which performance and validity are verified on wellknown data sets.
[Algorithm design and analysis, Statistical analysis, Linear regression, Optimization methods, data mining, Probability distribution, learning, knowledge discovery, Pattern recognition, Filters, Machine learning, RELIEF algorithm, classification problems, attributes, Problem-solving, learning (artificial intelligence), Pattern analysis, pattern recognition]
webSPADE: a parallel sequence mining algorithm to analyze web log data
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Enterprise-class web sites receive a large amount of traffic, from both registered and anonymous users. Data warehouses are built to store and help analyze the click streams within this traffic to provide companies with valuable insights into the behavior of their customers. This article proposes a parallel sequence mining algorithm, webSPADE, to analyze the click streams found in site web logs. In this process, raw web logs are first cleaned and inserted into a data warehouse. The click streams are then mined by webSPADE. An innovative web-based front-end is used to visualize and query the sequence mining results. The webSPADE algorithm is currently used by Verizon to analyze the daily traffic of the Verizon.com web site.
[Algorithm design and analysis, Data analysis, webSPADE, Service oriented architecture, data mining, Relational databases, Companies, web-based front-end, Data warehouses, Appropriate technology, Information technology, parallel sequence mining algorithm, raw web logs, Data visualization, enterprise-class web sites, Frequency, Web sites, data warehouses, Web log data, sequence mining]
An incremental approach to building a cluster hierarchy
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In this paper we present a novel incremental hierarchical clustering (IHC) algorithm. Our approach aims to construct a hierarchy that satisfies homogeneity and monotonicity properties. Working in a bottom-up fashion, a new instance is placed in the hierarchy and a sequence of hierarchy restructuring processes is performed only in regions that have been affected by the presence of the new instance. The experimental results on a variety of domains demonstrate that our algorithm is not sensitive to input ordering, can produce a quality cluster hierarchy, and is efficient in terms of computational time.
[Tree data structures, Data analysis, Clustering methods, data mining, Ontologies, Information retrieval, monotonicity, Data mining, Information analysis, Nearest neighbor searches, homogeneity, incremental hierarchical clustering algorithm, USA Councils, pattern clustering, Clustering algorithms, hierarchy restructuring process sequence, tree data structures, computational time, cluster hierarchy, computational complexity]
Progressive and interactive analysis of event data using event miner
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Exploring large data sets typically involves activities that iterate between data selection and data analysis, in which insights obtained from analysis result in new data selection. Further, data analysis needs to use a combination of analysis techniques: data summarization, mining algorithms and visualization. This interweaving of functions arises both from the semantics of what the analyst hopes to achieve and from scalability requirements for dealing with large data volumes. We refer to such a process as a progressive analysis. Herein is described a tool, Event Miner, that integrates data selection, mining and visualization for progressive analysis of temporal, categorical data. We discuss a data model and architecture. We illustrate how our tool can be used for complex mining tasks such as finding patterns not occurring on Monday. Further, we discuss the novel visualization employed, such as visualizing categorical data and the results of data mining. Also, we discuss the extension of the existing mining framework needed to mine temporal events with multiple attributes. Throughout, we illustrate the capabilities of Event Miner by applying it to event data from large computer networks.
[Data analysis, Event Miner, data analysis, data selection, Data security, Scalability, data summarization, data mining, knowledge discovery, Data mining, Application software, Information analysis, event data mining, Data visualization, Information security, data visualisation, Data models, data visualization, interactive data analysis, Pattern analysis]
Comparison of lazy Bayesian rule, and tree-augmented Bayesian learning
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The naive Bayes classifier is widely used in interactive applications due to its computational efficiency, direct theoretical base, and competitive accuracy. However its attribute independence assumption can result in sub-optimal accuracy. A number of techniques have explored simple relaxations of the attribute independence assumption in order to increase accuracy. Among these, the lazy Bayesian rule (LBR) and the tree-augmented naive Bayes (TAN) have demonstrated strong prediction accuracy. However their relative performance has never been evaluated. The paper compares and contrasts these two techniques, finding that they have comparable accuracy and hence should be selected according to computational profile. LBR is desirable when small numbers of objects are to be classified while TAN is desirable when large numbers of objects are to be classified.
[pattern classification, Machine learning algorithms, lazy Bayesian rule, probability, Weka system, Frequency estimation, Partitioning algorithms, Data mining, Information technology, tree-augmented Bayesian learning, Accuracy, Bayesian methods, naive Bayes classifier, Machine learning, Decision trees, belief networks, learning (artificial intelligence), prediction accuracy, Bayesian network, Classification tree analysis]
Efficient progressive sampling for association rules
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In data mining, sampling has often been suggested as an effective tool to reduce the size of the dataset operated at some cost to accuracy. However this loss to accuracy is often difficult to measure and characterize since the exact nature of the learning curve (accuracy vs. sample size) is parameter and data dependent, i.e., we do not know a priori what sample size is needed to achieve a desired accuracy on a particular dataset for a particular set of parameters. In this article we propose the use of progressive sampling, to determine the required sample size for association rule mining. We first show that a naive application of progressive sampling is not very efficient for association rule mining. We then present a refinement based on equivalence classes, that seems to work extremely well in practice and is able to converge to the desired sample size very quickly and very accurately. An additional novelty of our approach is the definition of a support-sensitive, interactive measure of accuracy across progressive samples.
[Costs, progressive sampling, data mining, association rules, Size measurement, Loss measurement, Association rules, Data mining, Delay, fractals, Information science, Databases, rule mining, Pressing, Sampling methods, equivalence classes, dataset]
Mining generalized association rules using pruning techniques
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The goal of the paper is to mine generalized association rules using pruning techniques. Given a large transaction database and a hierarchical taxonomy tree of the items, we try to find the association rules between the items at different levels in the taxonomy tree under the assumption that original frequent itemsets and association rules have already been generated beforehand In the proposed algorithm GMAR, we use join methods and pruning techniques to generate new generalized association rules. Through several comprehensive experiments, we find that the GMAR algorithm is much better than BASIC and Cumulate algorithms.
[transaction processing, Taxonomy, data mining, trees (mathematics), generalized association rule mining, Transaction databases, Partitioning algorithms, Paper technology, Data mining, Association rules, database management systems, hierarchical taxonomy tree, pruning techniques, associative processing, Itemsets, Tree graphs, large transaction database, Internet, Mining industry, GAMR, frequent itemsets]
TreeFinder: a first step towards XML data mining
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In this paper we consider the problem of searching frequent trees from a collection of tree-structured data modeling XML data. The TreeFinder algorithm aims at finding trees, such that their exact or perturbed copies are frequent in a collection of labelled trees. To cope with complexity issues, TreeFinder is correct but not complete: it finds a subset of actually frequent trees. The default of completeness is experimentally investigated on artificial medium size datasets; it is shown that TreeFinder reaches completeness or falls short for a range of experimental settings.
[XML data mining, perturbed copies, complexity, Corporate acquisitions, data mining, TreeFinder algorithm, labelled trees, completeness, Data mining, data models, artificial medium size datasets, XML, frequent tree searching, Robustness, Concrete, tree data structures, tree-structured data, exactor copies, hypermedia markup languages, computational complexity]
InfoMiner+: mining partial periodic patterns with gap penalties
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In this paper we focus on mining periodic patterns allowing some degree of imperfection in the form of random replacement from a perfect periodic pattern. Information gain was proposed to identify patterns with events of vastly different occurrence frequencies and adjust for deviation from a pattern. However, it does not involve a penalty if there exists some gap between pattern occurrences. In many applications, e.g., bioinformatics, it is important to identify subsequences that a pattern repeats perfectly (or near perfectly). As a solution, we extend the information gain measure to include a penalty for gaps between pattern occurrences. We call this measure generalized information gain. Furthermore, we need to find a subsequence S' such that for a pattern P, the generalized information gain of P in S' is high. This is particularly useful in locating repeats in DNA sequences. In this paper, we developed an effective mining algorithm, InfoMiner+, to simultaneously mine significant patterns and associated subsequences.
[Sequences, Scattering, data mining, time series, pattern occurrences, Time measurement, Data mining, Association rules, sequences, DNA sequence repeat location, Aggregates, biology computing, partial periodic pattern mining, DNA, generalized information gain, Gain measurement, Frequency, gap penalties, random replacement, information gain measure, subsequences, imperfection, InfoMiner+]
Discovering frequent geometric subgraphs
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
As data mining techniques are being increasingly applied to non-traditional domains, existing approaches for finding frequent itemsets cannot be used as they cannot model the requirement of these domains. An alternate way of modeling the objects in these data sets, is to use a graph to model the database objects. Within that model, the problem of finding frequent patterns becomes that of discovering subgraphs that occur frequently over the entire set of graphs. We present a computationally efficient algorithm for finding frequent geometric subgraphs in a large collection of geometric graphs. Our algorithm is able to discover geometric subgraphs that can be rotation, scaling and translation invariant, and it can accommodate inherent errors on the coordinates of the vertices. Our experimental results show that our algorithms require relatively little time, can accommodate low support values, and scale linearly on the number of transactions.
[US Department of Energy, experimental results, Military computing, graph theory, data mining, Spatial databases, Transaction databases, Data mining, transactions, Chemical compounds, database objects, Computer science, frequent geometric subgraph discovery, Itemsets, High performance computing, very large databases, data sets, frequent pattern discovery, computationally efficient algorithm, Contracts, pattern recognition, frequent itemsets]
O-Cluster: scalable clustering of large high dimensional data sets
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Clustering large data sets of high dimensionality has always been a challenge for clustering algorithms. Many recently developed clustering algorithms have attempted to address either handling data sets with a very large number of records and/or with a very high number of dimensions. We provide a discussion of the advantages and limitations of existing algorithms when they operate on very large multidimensional data sets. To simultaneously overcome both the "curse of dimensionality" and the scalability problems associated with large amounts of data, we propose a new clustering algorithm called O-Cluster. O-Cluster combines a novel active sampling technique with an axis-parallel partitioning strategy to identify continuous areas of high density in the input space. The method operates on a limited memory buffer and requires at most a single scan through the data. We demonstrate the high quality of the obtained clustering solutions, their robustness to noise, and O-Cluster's excellent scalability.
[complexity, active sampling technique, Multidimensional systems, Shape, Scalability, data mining, Multimedia databases, Information retrieval, large high dimensional data sets, axis-parallel partitioning strategy, Partitioning algorithms, Data mining, Computational complexity, scalability, pattern clustering, very large databases, Clustering algorithms, scalable clustering, Sampling methods, limited memory buffer, O-Cluster, data handling, multidimensional data sets, computational complexity]
Wavelet based UXO detection
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The detection and classification of unexploded ordnance (UXO) is considered a multidimensional pattern recognition problem. Standard techniques in solving multidimensional detection and classification problems involve using large sets of templates or libraries. This paper shows that by using wavelet transformation a single library will allow a particular class of ordnance to be classified over a range of depths.
[wavelet based UXO detection, pattern classification, large template sets, Costs, unexploded ordnance classification, wavelet transforms, data mining, Discrete wavelet transforms, Calibration, Environmental economics, Signal resolution, Computer science, Azimuth, library, Frequency, multidimensional pattern recognition problem, Libraries, Australia, landmine detection, military computing]
From path tree to frequent patterns: a framework for mining frequent patterns
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We propose a framework for mining frequent patterns from large transactional databases. The core of the framework is a coded prefix-path tree with two representations, namely, a memory-based prefix-path tree and a disk-based prefix-path tree. The disk-based prefix-path tree is simple in its data structure yet rich in information contained, and is small in size. The memory-based prefix-path tree is simple and compact. Based on the memory-based prefix-path tree, a new depth-first frequent pattern discovery algorithm, called PP-Mine, is proposed that outperforms FP-growth significantly. The memory-based prefix-path tree can be stored on disk using a disk-based prefix-path tree with assistance of the new coding scheme. We present loading algorithms to load the minimal required disk-based prefix-path tree into main memory. Our technique is to push constraints into the loading process, which has not been well studied yet.
[Tree data structures, transaction processing, depth-first frequent pattern discovery algorithm, Costs, data mining, trees (mathematics), large transactional databases, data structure, Data structures, coded prefix-path tree, Transaction databases, set theory, frequent patterns mining, memory-based prefix-path tree, loading algorithms, PP-Mine, tree data structures, Assembly, disk-based prefix-path tree]
Neighborgram clustering. Interactive exploration of cluster neighborhoods
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We describe an interactive way to generate a set of clusters for a given data set. The clustering is done by constructing local histograms, which can then be used to visualize, select, and fine-tune potential cluster candidates. The accompanying algorithm can also generate clusters automatically, allowing for an automatic or semi-automatic clustering process where the user only occasionally interacts with the algorithm. We illustrate the ability to automatically identify and visualize clusters using NCI's AIDS Antiviral Screen data set.
[Greedy algorithms, Algorithm design and analysis, Data analysis, data analysis, expert knowledge, data set, local histograms, Histograms, pattern clustering, Clustering algorithms, Data visualization, Training data, Prototypes, large data sets, Neighborgrarn, clustering, interactive visualization, program visualisation, Pattern analysis, Bioinformatics, computational complexity]
A comparison study on algorithms for incremental update of frequent sequences
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The problem of mining frequent sequences is to extract frequently occurring subsequences in a sequence database. Algorithms on this mining problem include GSP, MFS, and SPADE. The problem of incremental update of frequent sequences is to keep track of the set of frequent sequences as the underlying database changes. Previous studies have extended the traditional algorithms to efficiently solve the update problem. These incremental algorithms include ISM, GSP+ and MFS+. Each incremental algorithm has its own characteristics and they have been studied and evaluated separately under different scenarios. This paper presents a comprehensive study on the relative performance of the incremental algorithms as well as their non-incremental counterparts. Our goal is to provide guidelines on the choice of an algorithm for solving the incremental update problem given the various characteristics of a sequence database.
[Availability, sequence database, GSP, data mining, Transaction databases, Data mining, sequences, database management systems, mining frequent sequences, incremental algorithm, Guidelines, Information systems, Computer science, relative performance, Itemsets, SPADE, Councils, Intrusion detection, concurrency control, mining problem, MFS, frequent sequences, incremental update]
Mining associated implication networks: computational intermarket analysis
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Current attempts to analyze international financial markets include the use of financial technical analysis and data mining techniques. In this paper, we propose a new approach that incorporates implication networks and association rules to form an associated network structure. The proposed approach explicitly addresses the issue of local vs. global influences between financial markets.
[Data analysis, inference, data mining, probability, association rules, Association rules, Data mining, inference mechanisms, probabilistic networks, Computer science, Bayesian methods, Computer networks, Inference algorithms, associated network structure, financial data processing, stock markets, belief networks, Stock markets, Bonding, inter market actions, international financial markets, association rules mining, Testing]
Learning from order examples
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We advocate a new learning task that deals with orders of items, and we call this the learning from order examples (LOE) task. The aim of the task is to acquire the rule that is used for estimating the proper order of a given unordered item set. The rule is acquired from training examples that are ordered item sets. We present several solution methods for this task, and evaluate the performance and the characteristics of these methods based on the experimental results of tests using both artificial data and realistic data.
[pattern classification, experimental results, data mining, performance evaluation, learning from order examples task, proper order estimation, classification, Sorting, Sections, regression, statistical analysis, ordered item set, learning by example, Testing, unordered item set]
Discovery of interesting association rules from Livelink web log data
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We present our experience in mining web usage patterns from a large collection of Livelink log data. Livelink is a web-based product of Open Text, which provides automatic management and retrieval of different types of information objects over an intranet or extranet. We report our experience in preprocessing raw log data and post-processing the mining results for finding interesting rules. In particular we compare and evaluate a number of rule interestingness measures and find that two of the measures that have not been used in association rule learning work very well.
[Extranets, Data preprocessing, data mining, Information retrieval, web usage patterns mining, Information management, Livelink Web log data, Association rules, Data mining, association rules discovery, Computer science, Technology management, Volume measurement, rule interestingness measures, Internet, rules mining]
Using sequential and non-sequential patterns in predictive Web usage mining tasks
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We describe an efficient framework for Web personalization based on sequential and non-sequential pattern discovery from usage data. Our experimental results performed on real usage data indicate that more restrictive patterns, such as contiguous sequential patterns (e.g., frequent navigational paths) are more suitable for predictive tasks, such as Web prefetching, (which involve predicting which item is accessed next by a user), while less constrained patterns, such as frequent item sets or general sequential patterns are more effective alternatives in the context of Web personalization and recommender systems.
[search engines, Navigation, Prefetching, Scalability, pattern mining, data mining, Web recommender systems, Information filtering, Web prefetching, Data mining, sequential pattern, Information systems, association rule mining, Computer science, Web personalization systems, Web usage mining, Collaboration, Information filters, Web sites, Recommender systems]
Mining genes in DNA using GeneScout
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In this paper we present a new system, called GeneScout, for predicting gene structures in vertebrate genomic DNA. The system contains specially designed hidden Markov models (HMMs) for detecting functional sites including protein-translation start sites, mRNA splicing junction donor and acceptor sites, etc. Our main hypothesis is that, given a vertebrate genomic DNA sequence S, it is always possible to construct a directed acyclic graph G such that the path for the actual coding region of S is in the set of all paths on G. Thus, the gene detection problem is reduced to that of analyzing the paths in the graph G. A dynamic programming algorithm is used to find the optimal path in G. The proposed system is trained using an expectation-maximization (EM) algorithm and its performance on vertebrate gene prediction is evaluated using the 10-way cross-validation method. Experimental results show the good performance of the proposed system and its complementarity to a widely used gene detection system.
[Algorithm design and analysis, gene structures, Sequences, vertebrate gene prediction, Splicing, expectation-maximization algorithm, Genomics, data mining, acceptor sites, dynamic programming, Proteins, gene detection system, hidden Markov models, GeneScout, protein-translation start sites, DNA, Hidden Markov models, mRNA splicing junction donor, Performance analysis, Dynamic programming, Bioinformatics, medical computing, genes mining, vertebrate genomic DNA]
Feature selection algorithms: a survey and experimental evaluation
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In view of the substantial number of existing feature selection algorithms, the need arises to count on criteria that enables to adequately decide which algorithm to use in certain situations. This work assesses the performance of several fundamental algorithms found in the literature in a controlled scenario. A scoring measure ranks the algorithms by taking into account the amount of relevance, irrelevance and redundance on sample data sets. This measure computes the degree of matching between the output given by the algorithm and the known optimal solution. Sample size effects are also studied.
[Noise reduction, data mining, probability, Size measurement, Noise generators, experimental evaluation, Rain, performance, Impedance matching, scoring measure, sample data sets, very large databases, feature selection algorithms, supervised inductive learning, Particle measurements, survey, learning by example]
Adaptive ripple down rules method based on minimum description length principle
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
When class distribution changes, some pieces of knowledge previously acquired become worthless, and the existence of such knowledge may hinder acquisition of new knowledge. The paper proposes an adaptive ripple down rules (RDR) method based on the minimum description length principle aiming at knowledge acquisition in a dynamically changing environment. To cope with the change of class distribution, knowledge deletion is carried out as well as knowledge acquisition so that useless knowledge is properly discarded. To cope with the change of the source of knowledge, RDR knowledge based systems can be constructed adaptively by acquiring knowledge from both domain experts and data. By incorporating inductive learning methods, knowledge acquisition can be carried out even when only either data or experts are available by switching the source of knowledge from domain experts to data and vice versa at any time of knowledge acquisition. Since experts need not be available all the time, it contributes to reducing the cost of personnel expenses. Experiments were conducted by simulating the change of the source of knowledge and the change of class distribution using the datasets in UCI repository. The results are encouraging.
[Costs, Knowledge acquisition, Computational modeling, Knowledge based systems, Laboratories, Humans, knowledge acquisition, Switches, inductive learning methods, useless knowledge, Personnel, encoding, Research and development, adaptive ripple down rules method, knowledge deletion, Computer network reliability, knowledge based systems, minimum description length principle, class distribution, dynamically changing environment, UCI repository, search problems]
Exploring interestingness through clustering: a framework
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Determining interestingness is a notoriously difficult problem: it is subjective and elusive to capture. It is also becoming an increasingly more important problem in knowledge discovery from database as the number of mined patterns increases. In this work we introduce and investigate a framework for association rule clustering that enables automating much of the laborious manual effort normally involved in the exploration and understanding of interestingness. Clustering is ideally suited for this task; it is the unsupervised organization of patterns into groups, so that patterns in the same group are more similar to each other than to patterns in other groups. We also define a data-driven inferred labeling of these clusters, the ancestor coverage, which provides an intuitive, concise representation of the clusters.
[association rule clustering, ancestor coverage, cluster representation, pattern clustering, interestingness, data mining, knowledge discovery, Transaction databases, Association rules, Data mining, Labeling, Organizing]
On a capacity control using Boolean kernels for the learning of Boolean functions
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
This paper concerns the classification task in discrete attribute spaces, but considers the task in a more fundamental framework: the learning of Boolean functions. The purpose of this paper is to present a new learning algorithm for Boolean functions called Boolean kernel classifier (BKC) employing capacity control using Boolean kernels. BKC uses support vector machines (SVMs) as learning engines and Boolean kernels are primarily used for running SVMs in feature spaces spanned by conjunctions of Boolean literals. However, another important role of Boolean kernels is to appropriately control the size of its hypothesis space, to avoid overfitting. After applying a SVM to learn a classifier f in a feature space H induced by a Boolean kernel, BKC uses another Boolean kernel to compute the projections f/sup k/ of f onto a subspace H/sub k/ of H spanned by conjunctions with length at most k. By evaluating the accuracy of f/sup k/ on training data for any k, BKC can determine the smallest k such that f/sup k/ is as accurate as f and learn another f' in H/sub k/ expected to have lower error for unseen data. By an empirical study on learning of randomly generated Boolean functions, it is shown that the capacity control is effective, and BKC outperforms C4.5 and naive Bayes classifiers.
[learning automata, projections, capacity control, Industrial control, data mining, Engines, Aerospace industry, Centralized control, Boolean literals, Boolean functions, Space technology, Boolean function learning algorithm, discrete attribute spaces, training data, hypothesis space, Boolean kernels, learning (artificial intelligence), Kernel, pattern classification, subspace, support vector machines, Boolean kernel classifier, feature spaces, classification task, Support vector machines, Support vector machine classification, Machine learning]
Mining molecular fragments: finding relevant substructures of molecules
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We present an algorithm to find fragments in a set of molecules that help to discriminate between different classes of for instance, activity in a drug discovery context. Instead of carrying out a brute-force search, our method generates fragments by embedding them in all appropriate molecules in parallel and prunes the search tree based on a local order of the atoms and bonds, which results in substantially faster search by eliminating the need for frequent, computationally expensive reembeddings and by suppressing redundant search. We prove the usefulness of our algorithm by demonstrating the discovery of activity-related groups of chemical compounds in the well-known National Cancer Institute's HIV-screening dataset.
[Drugs, Atomic measurements, pharmaceutical industry, Data analysis, data mining, molecules, Electronic mail, Data mining, Association rules, Chemical compounds, association rule mining, Computer science, biology computing, drug discovery, biochemistry, bioinformatics, chemical structure, molecular biophysics, Bioinformatics, search strategy, Cancer]
Unsupervised segmentation of categorical time series into episodes
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
This paper describes an unsupervised algorithm for segmenting categorical time series into episodes. The VOTING-EXPERTS algorithm first collects statistics about the frequency and boundary entropy of ngrams, then passes a window over the series and has two "expert methods" decide where in the window boundaries should be drawn. The algorithm successfully segments text into words in four languages. The algorithm also segments time series of robot sensor data into subsequences that represent episodes in the life of the robot. We claim that VOTING-EXPERTS finds meaningful episodes in categorical time series because it exploits two statistical characteristics of meaningful episodes.
[data mining, categorical time series, VOTING-EXPERTS algorithm, Sensor phenomena and characterization, Mathematics, frequency, text segmentation, entropy, expert methods, Robot sensing systems, ngrams, subsequences, episodes, document handling, Sequences, languages, unsupervised segmentation algorithm, words, Educational institutions, time series, robot sensor data, unsupervised learning, Computer science, DNA, Writing, Frequency, Inference algorithms, boundary entropy, statistics]
A parameterless method for efficiently discovering clusters of arbitrary shape in large datasets
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Clustering is the problem of grouping data based on similarity and consists of maximizing the intra-group similarity while minimizing the inter-group similarity. The problem Of clustering data sets is also known as unsupervised classification, since no class labels are given. However, all existing clustering algorithms require some parameters to steer the clustering process, such as the famous k for the number of expected clusters, which constitutes a supervision of a sort. We present in this paper a new, efficient, fast and scalable clustering algorithm that clusters over a range of resolutions and finds a potential optimum clustering without requiring any parameter input. Our experiments show that our algorithm outperforms most existing clustering algorithms in quality and speed for large data sets.
[Multi-stage noise shaping, Shape, Scalability, Clustering methods, data mining, efficient clustering algorithm, scalable clustering algorithm, Partitioning algorithms, Noise shaping, unsupervised classification, arbitrarily shaped cluster discovery, large datasets, parameterless method, intra-group similarity maximization, fast clustering algorithm, pattern clustering, Clustering algorithms, clustering, inter-group similarity minimization, minimisation, Gravity]
Investigative profiling with computer forensic log data and association rules
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Investigative profiling is an important activity in computer forensics that can narrow the search for one or more computer perpetrators. Data mining is a technique that has produced good results in providing insight into large volumes of data. This paper describes how the association rule data mining technique may be employed to generate profiles from log data and the methodology used for the interpretation of the resulting rule sets. The process relies on background knowledge in the form of concept hierarchies and beliefs, commonly available from, or attainable by, the computer forensic investigative team. Results obtained with the profiling system has identified irregularities in computer logs.
[computer logs, Forensics, data mining, profiling system, computer perpetrators, Association rules, Data mining, Application software, Computer crime, Aggregates, Web pages, Collaboration, computer crime, Computer networks, Australia, computer forensics, investigative profiling]
Implementation of a least fixpoint operator for fast mining of relational databases
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Recent research has focused on computing large item sets for association rule mining using SQL3 least fixpoint computation, and by exploiting the monotonic nature of the SQL3 aggregate functions such as sum and create view recursive constructs. Such approaches allow us to view mining as an ad hoc querying exercise and treat the efficiency issue as an optimization problem. We present a recursive implementation of a recently proposed least fixpoint operator for computing large item sets from object-relational databases. We present experimental evidence to show that our implementation compares well with several well-regarded and contemporary algorithms for large item set generation.
[large item set generation, data mining, Relational databases, Data mining, Database languages, Engines, least fixpoint operator, query processing, experiment, very large databases, optimization, Data analysis, SQL3, object-oriented databases, object oriented database, Association rules, relational databases, SQL, association rule mining, Computer science, Aggregates, Ear, Deductive databases, aggregate functions, ad hoc querying, recursive constructs, object-relational databases]
Extraction techniques for mining services from Web sources
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The Web has established itself as the dominant medium for doing electronic commerce. Consequently the number of service providers, both large and small, advertising their services on the web continues to proliferate. In this paper we describe new extraction algorithms for mining service directories from web pages. We develop a novel propagation technique for identifying and accumulating all of the attributes related to a service entity in a web page. We provide experimental results of the effectiveness of our extraction techniques by mining a database of veterinarian service providers from web sources.
[web pages, Taxonomy, data mining, Ontologies, web sites, Electronic commerce, Computer science, Databases, Web pages, extraction algorithms, Cities and towns, learning (artificial intelligence), mining service directories, Advertising, electronic commerce]
Telecommunications strategic marketing - KDD and economic modeling
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The Italian deregulation process of telecommunications market in the last years has produced a large economic impact since it has altered equilibriums that were established for a long time. In this framework, we notice a strong need for adequate tools to analyze the market and its trends and, at the same time, a lack of specific solutions within the scientific literature, due to the new technical challenges issued by the problem. In particular, in the context of building a Decision Support System (DSS) for the strategic marketing unit of TELECOM Italia (TI) we have devised a new methodology to profitably combine most powerful tools from KDD and Economic Sciences. We have tested our approach by analyzing the residential telecommunications market demand in Italy during the transition from a monopolistic structure to an oligopolistic one. In this paper, we first address the state of the art in DSS design, then we describe the proposed methodology and its application in the case study.
[Decision support systems, TELECOM Italia, data mining, Power generation economics, economic modeling, Telecommunications, Data mining, Environmental economics, telecommunication computing, decision support systems, Italian deregulation process, decision support system, Microeconomics, Economic forecasting, Sampling methods, Econometrics, Testing, residential telecommunications market]
Iterative clustering of high dimensional text data augmented by local search
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The k-means algorithm with cosine similarity, also known as the spherical k-means algorithm, is a popular method for clustering document collections. However spherical k-means can often yield qualitatively poor results, especially when cluster sizes are small, say 25-30 documents per cluster, where it tends to get stuck at a local maximum far away from the optimal solution. In this paper, we present a local search procedure, which we call 'first-variation" that refines a given clustering by incrementally moving data points between clusters, thus achieving a higher objective function value. An enhancement of first variation allows a chain of such moves in a Kernighan-Lin fashion and leads to a better local maximum. Combining the enhanced first-variation with spherical k-means yields a powerful "ping-pong" strategy that often qualitatively improves k-means clustering and is computationally efficient. We present several experimental results to highlight the improvement achieved by our proposed algorithm in clustering high-dimensional and sparse text data.
[text analysis, incremental data point movement, data mining, Mathematics, sparse text data clustering, Data mining, cosine similarity, local search, objective function value, spherical k-means algorithm, first variation, Clustering algorithms, local maximum, high dimensional text data, search problems, Information retrieval, Statistics, document collection clustering, iterative clustering, pattern clustering, Refining, Euclidean distance, ping-pong strategy, Frequency, Iterative algorithms]
/spl Delta/B/sup +/ tree: indexing 3D point sets for pattern discovery
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Three-dimensional point sets can be used to represent data in different domains. Given a database of 3D point sets, pattern discovery looks for similar subsets that occur in multiple point sets. Geometric hashing has proved to be an effective technique in discovering patterns in 3D point sets. However, the method are has shortcomings. We propose a new indexing technique called /spl Delta/B/sup +/ trees. It is an extension of B/sup +/-trees that stores point triplet information and overcomes shortcomings of the geometric hashing technique. We introduce four different ways of constructing the key from a triplet. We give an analytical comparison between the new index structure and the geometric hashing technique. We also conduct experiments on both synthetic data and real data to evaluate performance.
[/spl Delta/B/sup +/ tree, Computer vision, Design automation, Shape, data representation, 3D point set indexing, multiple point sets, data mining, pattern discovery, performance evaluation, Spatial databases, index structure, geometric hashing technique, Data mining, Proteins, Computer science, database indexing, Feedback, DNA, tree data structures, Indexing, pattern recognition, subsets, point triplet information storage]
Improving medical/biological data classification performance by wavelet preprocessing
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Many real-world datasets contain noise which could degrade the performances of learning algorithms. Motivated from the success of wavelet denoising techniques in image data, we explore a general solution to alleviate the effect of noisy data by wavelet preprocessing for medical/biological data classification. Our experiments are divided into two categories: one is of different classification algorithms on a specific database, and the other is of a specific classification algorithm (decision tree) on different databases. The experiment results show that the wavelet denoising of noisy data is able to improve the accuracies of those classification methods, if the localities of the attributes are strong enough.
[wavelet transforms, Noise reduction, data mining, Classification algorithms, datasets, minimax techniques, data classification, noise, learning (artificial intelligence), Biomedical imaging, Classification tree analysis, pattern classification, Smoothing methods, Wavelet domain, biological data, learning algorithms, wavelet denoising, wavelet preprocessing, Computational Intelligence Society, Noise measurement, medical data, Computer errors, Biomembranes, minimax threshold, medical computing]
A lazy approach to pruning classification rules
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Associative classification is a promising technique for the generation of highly precise classifiers. Previous works propose several clever techniques to prune the huge set of generated rules, with the twofold aim of selecting a small set of high quality rules, and reducing the chance of overfitting. In this paper, we argue that pruning should be reduced to a minimum and that the availability of a large rule base may improve the precision of the classifier without affecting its performance. In L/sup 3/ (Live and Let Live), a new algorithm for associative classification, a lazy pruning technique iteratively discards all rules that only yield wrong case classifications. Classification is performed in two steps. Initially, rules which have already correctly classified at least one training case, sorted by confidence, are considered If the case is still unclassified, the remaining rules (unused during the training phase) are considered, again sorted by confidence. Extensive experiments on 26 databases from the UCI machine learning database repository show that L/sup 3/ improves the classification precision with respect to previous approaches.
[pattern classification, pruning, lazy pruning, Finance, data mining, Data mining, Association rules, machine learning, associative processing, Databases, association rule discovery, associative classification, Training data, Machine learning, classifiers, rule base, Decision trees, learning (artificial intelligence), Testing, Classification tree analysis]
Mining associations by pattern structure in large relational tables
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Association rule mining aims at discovering patterns whose support is beyond a given threshold. Mining patterns composed of items described by an arbitrary subset of attributes in a large relational table represents a new challenge and has various practical applications, including the event management systems that motivated this work. The attribute combinations that define the items in a pattern provide the structural information of the pattern. Current association algorithms do not make full use of the structural information of the patterns: the information is either lost after it is encoded with attribute values, or is constrained by a given hierarchy or taxonomy. Pattern structures convey important knowledge about the patterns. We present an architecture that organizes the mining space based on pattern structures. By exploiting the interrelationships among pattern structures, execution times for mining can be reduced significantly. This advantage is demonstrated by our experiments using both synthetic and real-life datasets.
[Algorithm design and analysis, Data security, Software algorithms, data mining, patterns discovery, large relational tables, Data mining, Association rules, History, relational databases, Zinc, mining space, association rule mining, Authorization, Filters, Itemsets, execution times, structural information, event management systems, attribute, pattern structure, search problems]
Demand forecasting by the neural network with discrete Fourier transform
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
This paper proposes a new demand forecasting method using a neural network and Fourier transform. In this method, time series data of sales results considered as a combination of frequency are transformed into several frequency data. They are identified from objective indexes that consist of product properties or economic indicators and so forth. This method is efficient for demand forecasting aimed at new products that have no historical data.
[Fourier transforms, Supply chain management, Economic indicators, Supply chains, Demand forecasting, data mining, neural network, sales results, product properties, indexes, very large databases, Economic forecasting, Marketing and sales, time series data, demand forecasting, discrete Fourier transforms, Discrete Fourier transforms, time series, marketing data processing, discrete Fourier transform, marketing, Neural networks, economic indicators, Frequency, neural nets]
Evolutionary time series segmentation for stock data mining
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Stock data in the form of multiple time series are difficult to process, analyze and mine. However, when they can be transformed into meaningful symbols like technical patterns, it becomes easier. Most recent work on time series queries concentrates only on how to identify a given pattern from a time series. Researchers do not consider the problem of identifying a suitable set of time points for segmenting the time series in accordance with a given set of pattern templates (e.g., a set of technical patterns for stock analysis). On the other hand, using fixed length segmentation is a primitive approach to this problem; hence, a dynamic approach (with high controllability) is preferred so that the time series can be segmented flexibly and effectively according to the needs of users and applications. In view of the fact that such a segmentation problem is an optimization problem and evolutionary computation is an appropriate tool to solve it, we propose an evolutionary time series segmentation algorithm. This approach allows a sizeable set of stock patterns to be generated for mining or query. In addition, defining the similarity between time series (or time series segments) is of fundamental importance in fitness computation. By identifying perceptually important points directly from the time domain, time series segments and templates of different lengths can be compared and intuitive pattern matching can be carried out in an effective and efficient manner. Encouraging experimental results are reported from tests that segment the time series of selected Hong Kong stocks.
[pattern matching, multiple time series, Shape, dynamic approach, fitness computation, data mining, Evolutionary computation, pattern templates, perceptually important points, Data mining, Research and development, stock patterns, optimisation, evolutionary time series segmentation, optimization, intuitive pattern matching, technical patterns, Controllability, financial data processing, stock markets, Pattern analysis, Testing, Time series analysis, time series, Transaction databases, meaningful symbols, evolutionary computation, stock data mining, Hong Kong stocks, Pattern matching]
Progressive modeling
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Presently, inductive learning is still performed in a frustrating batch process. The user has little interaction with the system and no control over the final accuracy and training time. If the accuracy of the produced model is too low, all the computing resources are misspent. In this paper we propose a progressive modeling framework. In progressive modeling, the learning algorithm estimates online both the accuracy of the final model and remaining training time. If the estimated accuracy is far below expectation, the user can terminate training prior to completion without wasting further resources. If the user chooses to complete the learning process, progressive modeling will compute a model with expected accuracy in expected time. We describe one implementation of progressive modeling using ensemble of classifiers.
[pattern classification, Costs, ISO standards, data mining, Software performance, Data mining, Association rules, Statistics, Petroleum, Computer science, inductive learning, IEC standards, batch process, interactive systems, Database systems, progressive modeling, learning by example]
A new implementation technique for fast spectral based document retrieval systems
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The traditional methods of spectral text retrieval (FDS,CDS) create an index of spatial data and convert the data to its spectral form at query time. We present a new method of implementing and querying an index containing spectral data which will conserve the high precision performance of the spectral methods, reduce the time needed to resolve the query, and maintain an acceptable size for the index. This is done by taking advantage of the properties of the discrete cosine transform and by applying ideas from vector space document ranking methods.
[Text mining, spatial data, Gold, discrete cosine transforms, spectral text retrieval, Navigation, information retrieval, query time, visual databases, Information retrieval, Data engineering, Data mining, discrete cosine transform, vector space document ranking methods, Search engines, spectral based document retrieval systems, Discrete cosine transforms, Web sites, Spatial resolution]
Mining similar temporal patterns in long time-series data and its application to medicine
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Data mining in time-series medical databases has been receiving considerable attention since it provides a way of revealing useful information hidden in the database; for example relationships between temporal course of examination results and onset time of diseases. This paper presents a new method for finding similar patterns in temporal sequences. The method is a hybridization of phase-constraint multiscale matching and rough clustering. Multiscale matching enables us cross-scale comparison of the sequences, namely, it enable us to compare temporal patterns by partially changing observation scales. Rough clustering enable us to construct interpretable clusters of the sequences even if their similarities are given as relative similarities. We combine these methods and cluster the sequences according to multiscale similarity of patterns. Experimental results on the chronic hepatitis dataset showed that clusters demonstrating interesting temporal patterns were successfully discovered.
[temporal sequences, phase-constraint multiscale matching, chronic hepatitis dataset, Data analysis, Liver diseases, Clustering methods, Laboratories, Biomedical informatics, data mining, time-series medical databases, time series, Data mining, long time-series data, rough clustering, Databases, Automatic testing, pattern clustering, interpretable sequence clusters, similar temporal pattern mining, disease onset time, medical examination results, Medical tests, medical computing, Pattern matching]
On the mining of substitution rules for statistically dependent items
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In this paper a new mining capability, called mining of substitution rules, is explored. A substitution refers to the choice made by a customer to replace the purchase of items with that of others. The process of mining substitution rules can be decomposed into two procedures. The first identifies concrete itemsets among a large number of frequent itemsets, where a concrete itemset is a frequent itemset whose items are statistically dependent. The second is substitution rule generation. Two concrete itemsets X and Y form a substitution rule, denoted by X /spl utri/ Y to mean that X is a substitute for Y if and only if X and Y are negatively correlated and the negative association rule X /spl rarr/ Y~ exists. We derive theoretical properties for the model of substitution rule mining. Then, in light of these properties, the SRM algorithm (substitution rule mining) is designed and implemented to discover substitution rules efficiently while attaining good statistical significance. Empirical studies are performed to evaluate the performance of the SRM algorithm. It is shown that SRM produces substitution rules of very high quality.
[Taxonomy, data mining, customer purchase, statistically dependent items, Transaction databases, Electronic mail, Frequency measurement, Data mining, Association rules, substitution rule mining, substitution rule generation, concrete itemsets, Itemsets, algorithm performance evaluation, Concrete, Marketing and sales, retail data processing, frequent itemsets]
gSpan: graph-based substructure pattern mining
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We investigate new approaches for frequent graph-based pattern mining in graph datasets and propose a novel algorithm called gSpan (graph-based substructure pattern mining), which discovers frequent substructures without candidate generation. gSpan builds a new lexicographic order among graphs, and maps each graph to a unique minimum DFS code as its canonical label. Based on this lexicographic order gSpan adopts the depth-first search strategy to mine frequent connected subgraphs efficiently. Our performance study shows that gSpan substantially outperforms previous algorithms, sometimes by an order of magnitude.
[Costs, lexicographic order, frequent connected subgraph mining, gSpan, data mining, Data structures, graph datasets, Data mining, tree searching, graph-based substructure pattern mining, Chemical compounds, Computer science, Graphics, performance study, frequent graph-based pattern mining, Tree graphs, Itemsets, unique minimum DFS code, canonical label, Kernel, depth-first search strategy, Testing, algorithm, frequent substructure discovery]
Objective-oriented utility-based association mining
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The necessity of developing methods for discovering association patterns to increase business utility of an enterprise has long been recognized in the data mining community. This requires modeling specific association patterns that are both statistically (based on support and confidence) and semantically (based on objective utility) related to a given objective that a user wants to achieve or is interested in. However, no such general model has been reported in the literature. Traditional association mining focuses on deriving correlations among a set of items and their association rules; diaper /spl rarr/ beer only tells us that a pattern like {diaper} is statistically related to an item like beer. In this paper we present a new approach, called objective-oriented utility-based association (OOA) mining, to modeling such association patterns that are explicitly related to a user's objective and its utility. Due to its focus on a user's objective and the use of objective utility as key semantic information to measure the usefulness of association patterns, OOA mining differs significantly from existing approaches such as existing constraint-based association mining. We formally define OOA mining and develop an algorithm for mining OOA rules. The algorithm is an enhancement of a priori with specific mechanisms for handling objective utility. We prove that the utility constraint is neither monotone nor anti-monotone, succinct or convertible and present a novel pruning strategy based on the utility constraint to improve the efficiency of OOA mining.
[business, Laboratories, data mining, enterprise, Probability, association pattern discovery, association pattern modeling, Pattern recognition, Data mining, Association rules, Computer science, Itemsets, a priori algorithm, pruning strategy, Frequency, semantic information, objective-oriented utility-based association mining]
Recognition of common areas in a Web page using visual information: a possible application in a page classification
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Extracting and processing information from Web pages is an important task in many areas like constructing search engines, information retrieval, and data mining from the Web. A common approach in the extraction process is to represent a page as a "bag of words" and then to perform additional processing on such a flat representation. We propose a new, hierarchical representation that includes browser screen coordinates for every HTML object in a page. Using visual information one is able to define heuristics for the recognition of common page areas such as header, left and right menu, footer and center of a page. We show in initial experiments that using our heuristics defined objects are recognized properly in 73% of cases. Finally, we show that a Naive Bayes classifier, taking into account the proposed representation, clearly outperforms the same classifier using only information about the content of documents.
[search engines, Crawlers, Humans, data mining, information retrieval, experiments, Information retrieval, HTML, Data mining, classification, browser screen coordinates, hierarchical representation, heuristics, Web page common area recognition, Web pages, Machine learning, Search engines, Frequency, page classification, Web sites, Naive Bayes classifier, Civil engineering, hypermedia markup languages, visual information]
Attribute (feature) completion - the theory of attributes from data mining prospect
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
A "correct" selection of attributes (features) is vital in data mining. As a first step, this paper constructs all possible attributes of a given relation. The results are based on the observations that each relation is isomorphic to a unique abstract relation, called a canonical model. The complete set of attributes of the canonical model is, then, constructed. Any attribute of a relation can be interpreted (via isomorphism) from such a complete set.
[data model, data mining, large database, canonical model, Spatial databases, Reflection, Mathematics, relational database, Data mining, Association rules, relational databases, isomorphism, database theory, Computer science, data models, very large databases, Data models, Mathematical model, abstract relation, Artificial intelligence, feature selection, attribute completion]
An algebraic approach to data mining: some examples
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We introduce an algebraic approach to the foundations of data mining. Our approach is based upon two algebras of functions defined over a common state space X and a pairing between them. One algebra is an algebra of state space observations, and the other is an algebra of labeled sets of states. We interpret H as the algebraic encoding of the data and the pairing as the misclassification rate when the classifier f is applied to the set of states X. We give a realization theorem giving conditions on formal series of data sets built from D that imply there is a realization involving a state space X, a classifier f /spl isin/ R and a set of labeled states /spl chi/ /spl isin/ R/sub 0/ that yield this series.
[pattern classification, Learning automata, functions, Laboratories, data mining, large database, state space observations, Predictive models, Encoding, algebra, State-space methods, Data mining, algebraic approach, Convergence, database theory, Algebra, labeled sets of states, algebraic encoding, misclassification rate, very large databases, data sets, classifier, Control theory, Erbium]
Adapting information extraction knowledge for unseen Web sites
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We propose a wrapper adaptation framework which aims at adapting a learned wrapper to an unseen Web site. It significantly reduces human effort in constructing wrappers. Our framework makes use of extraction rules previously discovered from a particular site to seek potential training example candidates for an unseen site. Rule generalization and text categorization are employed for finding suitable example candidates. Another feature of our approach is that it makes use of the previously discovered lexicon to classify good training examples automatically for the new site. We conducted extensive experiments to evaluate the quality of the extraction performance and the adaptability of our approach.
[Automation, information extraction knowledge, Natural languages, Keyword search, Humans, data mining, Information retrieval, text categorization, Data mining, unseen Web site, Text categorization, Web pages, rule generalization, Systems engineering and theory, Web sites, learning (artificial intelligence), wrapper adaptation framework, Research and development management, extraction rules, lexicon]
Mining motifs in massive time series databases
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The problem of efficiently locating previously known patterns in a time series database (i.e., query by content) has received much attention and may now largely be regarded as a solved problem. However, from a knowledge discovery viewpoint, a more interesting problem is the enumeration of previously unknown, frequently occurring patterns. We call such patterns "motifs\
[computation biology, data mining, Data engineering, time series, knowledge discovery, Visual databases, Data mining, Association rules, database management systems, query by content, classification, massive time series databases, Convergence, Computer science, motifs mining, Clustering algorithms, Data visualization, Prototypes, Biology computing, real world datasets]
Mining a set of coregulated RNA sequences
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Post-transcriptional regulation, though less studied, is an important research topic in bioinformatics. In a set of post-transcriptionally coregulated RNAs, the basepair interactions can organize the molecules into domains and provide a framework for functional interactions. Their consensus motifs may represent the binding sites of RNA regulatory proteins. Unlike DNA motifs, RNA motifs are more conserved in structures than in sequences. Knowing the structural motifs can help us better understand the regulation activities. We propose a novel data mining approach to RNA secondary structure prediction. To demonstrate the performance of our new approach, we first tested it on the same data sets previously used and published in literature. Secondly, to show the flexibility of our new approach, we also tested it on a data set that contains pseudoknot motifs that most current systems cannot identify.
[RNA, Stochastic processes, data mining, coregulated RNA sequence mining, post-transcriptional regulation, supervised learning, Prediction methods, RNA secondary structure prediction, functional interactions, Genetic algorithms, Proteins, Information science, pseudoknot motifs, biology computing, very large databases, data sets, Dynamic programming, learning (artificial intelligence), Testing, Sequences, DNA, bioinformatics, RNA regulatory proteins, scientific information systems, RNA motifs]
On computing condensed frequent pattern bases
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Frequent pattern mining has been studied extensively. However, the effectiveness and efficiency of this mining is often limited, since the number of frequent patterns generated is often too large. In many applications it is sufficient to generate and examine only frequent patterns with support frequency in close-enough approximation instead of in full precision. Such a compact but close-enough frequent pattern base is called a condensed frequent patterns-base. In this paper we propose and examine several alternatives at the design, representation, and implementation of such condensed frequent pattern-bases. A few algorithms for computing such pattern-bases are proposed. Their effectiveness at pattern compression and their efficient computation methods are investigated. A systematic performance study is conducted on different kinds of databases, which demonstrates the effectiveness and efficiency of our approach at handling frequent pattern mining in large databases.
[pattern compression, data mining, condensed frequent pattern bases computing, Frequency estimation, Pattern recognition, Transaction databases, Proposals, database management systems, Pattern analysis, large databases, Information analysis, frequent pattern mining]
Optimal projections of high dimensional data
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In this paper, we compare two artificial neural network algorithms for performing Exploratory Projection Pursuit, a statistical technique for investigating data by projecting it onto lower dimensional manifolds. The neural networks are extensions of a network which performs Principal Component Analysis. We illustrate the technique on artificial data before applying it to real data.
[artificial neural network, Nonlinear equations, Statistical analysis, Neurons, Exploratory Projection Pursuit, Artificial neural networks, neural networks, Data mining, lower dimensional manifolds, Negative feedback, Mean square error methods, data structures, learning (artificial intelligence), Joining processes, Principal Component Analysis, Computational intelligence, principal component analysis, neural nets, Principal component analysis]
SmartMiner: a depth first algorithm guided by tail information for mining maximal frequent itemsets
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Maximal frequent itemsets (MR) are crucial to many tasks in data mining. Since the MaxMiner algorithm first introduced enumeration trees for mining MR in 1998, several methods have been proposed to use depth first search to improve performance. To further improve the performance of mining MR, we proposed a technique that takes advantage of the information gathered from previous steps to discover new MR. More specifically, our algorithm called SmartMiner gathers and passes tail information and uses a heuristic select function which uses the tail information to select the next node to explore. Compared with Mafia and GenMax, SmartMiner generates a smaller search tree, requires a smaller number of support counting, and does not require superset checking. Using the datasets Mushroom and Connect, our experimental study reveals that SmartMiner generates the same MFI as Mafia and GenMax, but yields an order of magnitude improvement in speed.
[enumeration trees, data mining, Mushroom, Connect, datasets, Data mining, Association rules, tree searching, transactions, Computer science, maximal frequent itemsets, computation complexity, Itemsets, Tail, Sampling methods, heuristic select function, Personal digital assistants, Pattern analysis, SmartMiner, discovering association rules, Testing]
Mining surveillance video for independent motion detection
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
This paper addresses the special applications of data mining techniques in homeland defense. The problem targeted, which is frequently encountered in military/intelligence surveillance, is to mine a massive surveillance video database automatically collected to retrieve the shots containing independently moving targets. A novel solution to this problem is presented in this paper, which offers a completely qualitative approach to solving for the automatic independent motion detection problem directly from the compressed surveillance video in a faster than real-time mining performance. This approach is based on the linear system consistency analysis, and consequently is called QLS. Since the QLS approach only focuses on what exactly is necessary to compute a solution, it saves the computation to a minimum and achieves the efficacy to the maximum. Evaluations from real data show that QLS delivers effective mining performance at the achieved efficiency.
[Gunshot detection systems, Video sequences, data mining, compressed surveillance video, surveillance video mining, Information retrieval, automatic independent motion detection, Data mining, homeland defense, image motion analysis, Databases, Surveillance, Layout, Video compression, Cameras, Motion detection, linear system consistency analysis, independent motion detection, real-time mining performance]
Using category-based adherence to cluster market-basket data
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We devise an efficient algorithm for clustering market-basket data. Different from those of the traditional data, the features of market-basket data are known to be of high dimensionality, sparsity, and with massive outliers. Without explicitly considering the presence of the taxonomy, most prior efforts on clustering market-basket data can be viewed as dealing with items in the leaf level of the taxonomy tree. Clustering transactions across different levels of the taxonomy is of great importance for marketing strategies as well as for the result representation of the clustering techniques for market-basket data. In view of the features of market-basket data, we devise a measurement, called the category-based adherence, and utilize this measurement to perform the clustering. The distance of an item to a given cluster is defined as the number of links between this item and its nearest large node in the taxonomy tree where a large node is an item or a category node whose occurrence count exceeds a given threshold. The category-based adherence of a transaction to a cluster is then defined as the average distance of the items in this transaction to that cluster With this category-based adherence measurement, we develop an efficient clustering algorithm, called algorithm CBA, for market-basket data with the objective to minimize the category-based adherence. A validation model based on information gain is also devised to assess the quality of clustering for market-basket data. As validated by both real and synthetic datasets, it is shown by our experimental results, with the taxonomy information, algorithm CBA significantly outperforms the prior works in both the execution efficiency and the clustering quality for market-basket data.
[market-basket data, Data analysis, marketing strategies, Taxonomy, data mining, data clustering, Data engineering, Information retrieval, taxonomy, Data mining, Association rules, Tellurium, marketing, entropy, category-based adherence, pattern clustering, Clustering algorithms, Machine learning, transactions clustering, category node, Integrated circuit modeling]
Exploring the parameter state space of stacking
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Ensemble learning schemes are a new field in data mining. While current research concentrates mainly on improving the performance of single learning algorithms, an alternative is to combine learners with different biases. Stacking is the best-known such scheme which tries to combine learners' predictions or confidences via another learning algorithm. However, the adoption of stacking into the data mining community is hampered by its large parameter space, consisting mainly of other learning algorithms: (1) the set of learning algorithms to combine, (2) the meta-learner responsible for the combining; and (3) the type of meta-data to use - confidences or predictions. None of these parameters are obvious choices. Furthermore, little is known about the relation between the parameter settings and performance of stacking. By exploring all of stacking's parameter settings and their interdependencies, we attempt to make stacking a suitable choice for mainstream data mining applications.
[meta data, Stacking, Linear regression, learning algorithms, data mining, probability, Boosting, meta-data, Probability distribution, State-space methods, Data mining, Learning systems, parameter state space, Machine learning, confidences, probability distribution, learning (artificial intelligence), Artificial intelligence, Bagging, state-space methods, stacking]
Feature selection for clustering - a filter solution
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Processing applications with a large number of dimensions has been a challenge for the KDD community. Feature selection, an effective dimensionality reduction technique, is an essential pre-processing method to remove noisy features. In the literature only a few methods have been proposed for feature selection for clustering, and almost all these methods are 'wrapper' techniques that require a clustering algorithm to evaluate candidate feature subsets. The wrapper approach is largely unsuitable in real-world applications due to its heavy reliance on clustering algorithms that require parameters such as the number of clusters, and the lack of suitable clustering criteria to evaluate clustering in different subspaces. In this paper we propose a 'filter' method that is independent of any clustering algorithm. The proposed method is based on the observation that data with clusters has a very different point-to-point distance histogram to that of data without clusters. By exploiting this we propose an entropy measure that is low if data has distinct clusters and high if it does not. The entropy measure is suitable for selecting the most important subset of features because it is invariant with the number of dimensions, and is affected only by the quality of clustering. Extensive performance evaluation over synthetic, benchmark, and real datasets shows its effectiveness.
[entropy measure, Noise reduction, data mining, Entropy, dimensionality reduction technique, noisy feature removal, Unsupervised learning, filter method, Degradation, Histograms, knowledge discovery in databases, Filters, entropy, pattern clustering, pre-processing method, feature extraction, Clustering algorithms, clustering, point-to-point distance histogram, feature selection]
On evaluating performance of classifiers for rare classes
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Predicting rare classes effectively is an important problem. The definition of effective classifier, embodied in the classifier evaluation metric, is however very subjective, dependent on the application domain. In this paper a wide variety of point-metrics are put into a common analytical context defined by the recall and precision of the target rare class. This enables us to compare various metrics in an objective, domain-independent manner. We judge their suitability for the rare class problems along the dimensions of learning difficulty and levels of rarity. This yields many valuable insights. In order to address the goal of achieving better recall and precision, we also propose a way of comparing classifiers directly based on the relationships between recall and precision values. It resorts to a composite point-metric only when recall-precision based comparisons yield conflicting results.
[pattern classification, precision values, Costs, Machine learning algorithms, Event detection, point-metrics, data mining, rare class prediction, Predictive models, learning, Computational Intelligence Society, classifier evaluation metric, recall, learning (artificial intelligence), software performance evaluation, Testing, software metrics, classifier performance evaluation]
Mining online users' access records for web business intelligence
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
This paper discusses about how business intelligence on a website could be obtained from users' access records instead of web logs of "hits". Users' access records are captured by implementing an Access-Control (AC) architectural model on the website. This model requires users to register their profiles in an exchange of a password; and thereafter they have to login before gaining access to certain resources on the website. The links to the resources on the website have been modified such that a record of information about the access would be recorded in the database when clicked. This way, datamining can be performed on a relatively clean set of access records about the users. Hence, a good deal of business intelligence about the users' behaviors, preferences and about the popularities of the resources (products) on the website can be gained. In this paper, we also discussed how the business intelligence acquired, in turn, can be used to provide e-CRM for the users.
[website, Service oriented architecture, data mining, Companies, information retrieval, access-control architectural model, Intelligent agent, e-CRM, Databases, Asia, Computer architecture, Bismuth, online users access records mining, Software agents, Internet, Web sites, Monitoring, Web business intelligence]
Toward XML-based knowledge discovery systems
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Inductive databases are intended to be general purpose databases in which both source data and mined patterns can be represented, retrieved and manipulated. However, the heterogeneity of models for mined patterns makes difficult to realize them. In this paper, we explore the feasibility of using XML as the unifying framework for inductive databases, introducing a suitable data model called XDM (XML for data mining). XDM is designed to describe source raw data, heterogeneous mined patterns and data mining statements, so that they can be stored inside a unique XML-based inductive database.
[XDM, Data analysis, database management system, open systems, heterogeneous mined patterns, data mining, Containers, Information retrieval, Spatial databases, Data mining, deductive databases, XML, Open systems, knowledge discovery systems, inductive databases, Data models, Pattern analysis, Classification tree analysis, hypermedia markup languages]
Computing frequent graph patterns from semistructured data
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Whereas data mining in structured data focuses on frequent data values, in semistructured and graph data the emphasis is on frequent labels and common topologies. Here, the structure of the data is just as important as its content. We study the problem of discovering typical patterns of graph data. The discovered patterns can be useful for many applications, including: compact representation of source information and a road-map for browsing and querying information sources. Difficulties arise in the discovery task from the complexity of some of the required sub-tasks, such as sub-graph isomorphism. This paper proposes a new algorithm for mining graph data, based on a novel definition of support. Empirical evidence shows practical, as well as theoretical, advantages of our approach.
[semistructured data, complexity, frequent labels, sub-graph isomorphism, frequent graph pattern computation, data mining, graph data, pattern discovery, compact source information representation, information source querying, information source browsing, Topology, common topologies, Data mining, Association rules, Computer science, graphs, Databases, Tree graphs, XML, Frequency, Indexing]
FD/spl I.bar/Mine: discovering functional dependencies in a database using equivalences
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The discovery of FDs from databases has recently become a significant research problem. In this paper, we propose a new algorithm, called FD-Mine. FD-Mine takes advantage of the rich theory of FDs to reduce both the size of the dataset and the number of FDs to be checked by using discovered equivalences. We show that the pruning does not lead to loss of information. Experiments on 15 UCI datasets show that FD-Mine can prune more candidates than previous methods.
[databases, pruning, Lattices, data mining, UCI datasets, Relational databases, Independent component analysis, Partitioning algorithms, relational databases, Chemical compounds, discovered equivalences, Sorting, FD/spl I.bar/Mine algorithm, Computer science, functional dependence discovery]
Employing discrete Bayes error rate for discretization and feature selection tasks
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The tasks of discretization and feature selection are frequently used to improve classification accuracy. We use discrete approximation of Bayes error rate to perform discretization on the features. The discretization procedure targets minimization of Bayes error rate within each partition. A class-pair discriminatory measure can be defined on discretized partitions which forms the basis of the feature selection algorithm. A small value of this measure for a class-pair indicates that the class-pair in consideration is confusing and the features which distinguish them well should be chosen first. A video classification problem on a large database is considered for showing the comparison of a classifier using our discretization and feature selection tasks with SVM, neural network classifier, decision trees and K-nearest neighbor classifier.
[learning automata, Error analysis, video databases, image classification, data mining, large database, K-nearest neighbor classifier, feature selection tasks, Probability distribution, SVM, discrete approximation, very large databases, discrete Bayes error rate, Minimization methods, Spatial databases, Partitioning algorithms, video classification problem, classification, neural network classifier, Computer science, Support vector machines, minimization, Support vector machine classification, decision trees, class-pair discriminatory measure, Computer errors, Inference algorithms, Bayes methods, neural nets, discretization]
Mining case bases for action recommendation
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Corporations and institutions are often interested in deriving marketing strategies from corporate data and providing informed advice for their customers or employees. For example, a financial institution may derive marketing strategies for turning their reluctant customers into active ones and a telecommunications company may plan actions to stop their valuable customers from leaving. In data mining terms, these advice and action plans are aimed at converting individuals from an undesirable class to a desirable one, or to help devising a direct-marketing plan in order to increase the profit for the institution. We present an approach which uses 'role models' for generating such advice and plans. These role models are typical cases that form a case base and can be used for customer advice generation. For each new customer seeking advice, a nearest-neighbor algorithm is used to find a cost-effective and highly probable plan for switching a customer to the most desirable role models. We explore the tradeoff among time, space and quality of computation in this case-based reasoning framework. We demonstrate the effectiveness of the methods through empirical results.
[Computer aided software engineering, Costs, corporate data, data mining, Turning, Data mining, informed advice, Databases, customers, action recommendation, marketing strategies, case bases mining, Telecommunications, Marketing management, Remuneration, role models, corporations, Computer science, institutions, case-based reasoning, employees, Artificial intelligence, nearest-neighbor algorithm, finance, business data processing]
Automatic web page classification in a dynamic and hierarchical way
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Automatic classification of web pages is an effective way to deal with the difficulty of retrieving information from the Internet. Although there are many automatic classification algorithms and systems that have been proposed, most of them ignore the conflict between the fixed number of categories and the growing number of web pages going into the system. They also require searching through all existing categories to make any classification. We propose a dynamic and hierarchical classification system that is capable of adding new categories as required, organizing the web pages into a tree structure, and classifying web pages by searching through only one path of the tree structure. Our test results show that our proposed single-path search technique reduces the search complexity and increases the accuracy by 6% comparing to related algorithms. Our dynamic-category expansion technique also achieves satisfying results on adding new categories into our system as required.
[tree structure, Humans, information retrieval, Information retrieval, Educational institutions, Classification algorithms, Organizing, dynamic-category expansion technique, Computer science, automatic web page classification, Web pages, Frequency, Internet, learning (artificial intelligence), Classification tree analysis]
Maintenance of sequential patterns for record modification using pre-large sequences
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In previous work we proposed incremental mining algorithms for maintenance of sequential patterns based on the concept of pre-large sequences as records were inserted or deleted. Although maintenance of sequential patterns for record modification can be performed by using the deletion procedure and then the insertion procedure, double the computation time of a single procedure is needed. In this paper, we attempt to apply the concept of pre-large sequences to maintain sequential patterns as records are modified. The proposed algorithm does not require rescanning original databases until the accumulative number of modified customer sequences exceeds a safety bound derived by a pre-large concept. As databases grow larger, the number of modified customer sequences allowed before database rescanning also needs to grow.
[databases, Costs, record modification, data mining, rescanning, sequential pattern maintenance, Transaction databases, sequences, modified customer sequences, records management, computation time, very large databases, Safety, pre-large sequences]
A personalized music filtering system based on melody style classification
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
With the growth of digital music, the personalized music filtering system is helpful for users. Melody style is one of the music features to represent user's music preference. We present a personalized content-based music filtering system to support music recommendation based on user's preference of melody style. We propose the multitype melody style classification approach to recommend the music objects. The system learns the user preference by mining the melody patterns from the music access behavior of the user. A two-way melody preference classifier is therefore constructed for each user. Music recommendation is made through this melody preference classifier. Performance evaluation shows that the filtering effect of the proposed approach meets user's preference.
[data mining, learning, Information filtering, user interfaces, Digital filters, music, music recommendation, Information filters, user preference, learning (artificial intelligence), Recommender systems, personalized music filtering system, Navigation, digital music, performance evaluation, melody style classification, Spatial databases, content-based filtering system, Multiple signal classification, classification, content-based retrieval, Computer science, Collaboration, Feature extraction, two-way melody preference classifier]
Mining optimal actions for profitable CRM
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Data mining has been applied to CRM (Customer Relationship Management) in many industries with a limited success. Most data mining tools can only discover customer models or profiles (such as customers who are likely attritors and customers who are loyal), but not actions that would improve customer relationship (such as changing attritors to loyal customers). We describe a novel algorithm that suggests actions to change customers from an undesired status (such as attritors) to a desired one (such as loyal). Our algorithm takes into account the cost of actions, and further it attempts to maximize the expected net profit. To our best knowledge, no data mining algorithms or tools today can accomplish this important task in CRM. The algorithm is implemented, with many advanced features, in a specialized and highly effective data mining software called Proactive Solution.
[Software algorithms, data mining, Electronic mail, optimal actions mining, Data mining, Intelligent structures, customer relationship management, Computer science, Technology management, Customer relationship management, Cost function, data mining tools, Marine vehicles, Proactive Solution, Business]
Intersection based generalization rules for the analysis of symbolic septic shock patient data
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In intensive care units much data is irregularly recorded. Here, we consider the analysis of symbolic septic shock patient data. We show that it could be worth considering the generalization paradigm (individual cases generalized to more general rules) instead of the association paradigm (combining single attributes) when considering very individual cases (e.g. patients) and when expecting longer rules than shorter ones. We present an algorithm for rule generation and classification based on heuristically generated set-based intersections. We demonstrate the usefulness of our algorithm by analysing our septic shock patient data.
[septic shock patient data, pattern classification, Data analysis, rule generation, Electric shock, Heuristic algorithms, Medical treatment, data mining, heuristic, Spatial databases, Association rules, generalisation (artificial intelligence), intensive care units, set-based intersections, optimisation, generalization rules, Itemsets, rule classification, Robustness, medical computing]
Multivariate supervised discretization, a neighborhood graph approach
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We present a new discretization method in the context of supervised learning. This method entitled HyperCluster Finder is characterized by its supervised and polythetic behavior. The method is based on the notion of clusters and processes in two steps. First, a neighborhood graph construction from the learning database allows discovering homogenous clusters. Second, the minimal and maximal values of each cluster are transferred to each dimension in order to define some boundaries to cut the continuous attribute in a set of intervals. The discretization abilities of this method are illustrated by some examples, in particular processing the XOR problem.
[Machine learning algorithms, Laboratories, data mining, supervised learning, polythetic behavior, Gaussian distribution, neighborhood graph construction, Data mining, HyperCluster Finder, Learning systems, learning database, Supervised learning, Clustering algorithms, Machine learning, multivariate supervised discretization, supervised behavior, Performance loss, learning (artificial intelligence), Artificial intelligence, XOR problem]
Mining general temporal association rules for items with different exhibition periods
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In this paper we explore a new model of mining general temporal association rules from large databases where the exhibition periods of the items are allowed to be different from one to another. Note that in this new model, the downward closure property which all prior Apriori-based algorithms relied upon to attain good efficiency is no longer valid. As a result, how to efficiently generate candidate itemsets form large databases has become the major challenge. To address this issue, we develop an efficient algorithm, referred to as algorithm SPF (standing for Segmented Progressive Filter) in this paper The basic idea behind SPF is to first segment the database into sub-databases in such a way that items in each sub-database will have either the common starting time or the common ending time. Then, for each sub-database, SPF progressively filters candidate 2-itemsets with cumulative filtering thresholds either forward or backward in time. This feature allows SPF of adopting the scan reduction technique by generating all candidate k-itemsets (k>2) from candidate 2-itemsets directly. The experimental results show that algorithm SPF significantly outperforms other schemes which are extended from prior methods in terms of the execution time and scalability.
[Filtering, Scalability, data mining, temporal association rules, Segmented Progressive Filter, association rules, Transaction databases, Partitioning algorithms, Data mining, Association rules, exhibition periods, Filters, Itemsets, very large databases, Marketing and sales, large databases]
Discriminative category matching: efficient text classification for huge document collections
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
With the rapid growth of textual information available on the Internet, having a good model for classifying and managing documents automatically is undoubtedly important. When more documents are archived, new terms, new concepts and concept-drift will frequently appear Without a doubt, updating the classification model frequently, rather than using the old model for a very long period is absolutely essential. Here, the challenges are: a) obtain a high accuracy classification model; b) consume low computational time for both model training and operation; and c) occupy low storage space. However, none of the existing classification approaches could achieve all of these requirements. In this paper, we propose a novel text classification approach, called discriminative category matching, which could achieve all of the stated characteristics. Extensive experiments using two benchmarks and a large real-life collection are conducted. The encouraging results indicated that our approach is highly feasible.
[text analysis, discriminative category matching, Costs, pattern matching, Document handling, Government, data mining, document management, efficient text classification, document classification, Computer science, Support vector machines, concept-drift, Technology management, Text categorization, Support vector machine classification, huge document collections, Internet, Computational efficiency, computational time, computational complexity]
Text document categorization by term association
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
A good text classifier is a classifier that efficiently categorizes large sets of text documents in a reasonable time frame and with an acceptable accuracy, and that provides classification rules that are human readable for possible fine-tuning. If the training of the classifier is also quick, this could become in some application domains a good asset for the classifier. Many techniques and algorithms for automatic text categorization have been devised. According to published literature, some are more accurate than others, and some provide more interpretable classification models than others. However, none can combine all the beneficial properties enumerated above. In this paper we present a novel approach for automatic text categorization that borrows from market basket analysis techniques using association rule mining in the data-mining field. We focus on two major problems: (1) finding the best term association rules in a textual database by generating and pruning; and (2) using the rules to build a text classifier. Our text categorization method proves to be efficient and effective, and experiments on well-known collections show that the classifier performs well. In addition, training as well as classification are both fast and the generated rules are human readable.
[text analysis, pattern classification, Humans, data mining, text documents, Information retrieval, text categorization, Transaction databases, Electronic mail, Data mining, Association rules, machine learning, association rule mining, automatic text categorization, text classifier, Image databases, rule mining, Text categorization, Machine learning, automatic text classification, term association, learning (artificial intelligence), Indexing]
Cluster merging and splitting in hierarchical clustering algorithms
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Hierarchical clustering constructs a hierarchy of clusters by either repeatedly merging two smaller clusters into a larger one or splitting a larger cluster into smaller ones. The crucial step is how to best select the next cluster(s) to split or merge. We provide a comprehensive analysis of selection methods and propose several new methods. We perform extensive clustering experiments to test 8 selection methods, and find that the average similarity is the best method in divisive clustering and the minmax linkage is the best in agglomerative clustering. Cluster balance is a key factor to achieve good performance. We also introduce the concept of objective function saturation and clustering target distance to effectively assess the quality of clustering.
[Performance evaluation, selection methods, agglomerative clustering, merging, Clustering methods, cluster splitting, cluster merging, Merging, Laboratories, Minimax techniques, minmax linkage, cluster balance, Helium, Couplings, objective function saturation, pattern clustering, hierarchical clustering algorithms, Clustering algorithms, Binary trees, clustering target distance, divisive clustering, Testing]
Mining significant associations in large scale text corpora
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Mining large-scale text corpora is an essential step in extracting the key themes in a corpus. We motivate a quantitative measure for significant associations through the distributions of pairs and triplets of co-occurring words. We consider the algorithmic problem of efficiently enumerating such significant associations and present pruning algorithms for these problems, with theoretical as well as empirical analyses. Our algorithms make use of two novel mining methods: (1) matrix mining, and (2) shortened documents. We present evidence from a diverse set of documents that our measure does in fact elicit interesting co-occurrences.
[Algorithm design and analysis, Text mining, Text analysis, large-scale text corpora mining, data mining, co-occurring word pair distribution, shortened documents, Data mining, Association rules, Computer science, co-occurring word triplet distribution, quantitative measure, Databases, key theme extraction, Text categorization, Statistical distributions, algorithmic problem, pruning algorithms, Large-scale systems, significant association mining, matrix mining]
High performance data mining using the nearest neighbor join
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The similarity join has become an important database primitive to support similarity search and data mining. A similarity join combines two sets of complex objects such that the result contains all pairs of similar objects. Well-known are two types of the similarity join, the distance range join where the user defines a distance threshold for the join, and the closest point query or k-distance join which retrieves the k most similar pairs. In this paper, we investigate an important, third similarity join operation called k-nearest neighbor join which combines each point Of one point set with its k nearest neighbors in the other set. It has been shown that many standard algorithms of Knowledge Discovery in Databases (KDD) such as k-means and k-medoid clustering, nearest neighbor classification, data cleansing, postprocessing of sampling-based data mining etc. can be implemented on top of the k-nn join operation to achieve performance improvements without affecting the quality of the result of these algorithms. We propose a new algorithm to compute the k-nearest neighbor join using the multipage index (MuX), a specialized index structure for the similarity join. To reduce both CPU and I/O cost, we develop optimal loading and processing strategies.
[Multidimensional systems, Data analysis, similarity search, Biomedical informatics, data mining, Performance gain, multipage index, Data mining, multidimensional databases, Nearest neighbor searches, database theory, query processing, similarity join, Databases, Clustering algorithms, database primitive, Cost function, Acceleration]
Using functional PCA for cardiac motion exploration
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Principal component analysis (PCA) is a major tool in multivariate data analysis. Its paradigms are also used in Karhunen-Loeve decomposition, a standard tool in image processing. Extensions of PCA to the framework of functional data have been proposed. The analysis provided by functional PCA seems to be a powerful tool for finding principal sources of variability in curves or images, but fails to provide easy interpretations in the case of multifunctional data. Guidelines aiming at spot information from the outputs of PCA applied to functionals with values in the space of continuous functions upon a bounded domain are proposed. An application to cardiac motion analysis illustrates the complexity of the multifunctional framework and the results provided by functional PCA.
[Thyristors, biomedical MRI, multifunctional data, Image processing, MRI, Space charge, variability, Information analysis, functional data, multivariate data analysis, Hilbert space, medical image processing, Meteorology, image processing, Symmetric matrices, continuous functions, bounded domain, cardiac motion analysis, cardiology, image motion analysis, Karhunen-Loeve decomposition, Graphics, curves, Random variables, principal component analysis, functional principal component analysis, Principal component analysis]
Unsupervised clustering of symbol strings and context recognition
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The representation of information based on symbol strings has been applied to the recognition of context. A framework for approaching the context recognition problem has been described and interpreted in terms of symbol string recognition. The symbol string clustering map (SCM) is introduced as an efficient algorithm for the unsupervised clustering and recognition of symbol string data. The SCM can be implemented in an online manner using a computationally simple similarity measure based on a weighted average. It is shown how measured sensor data can be processed by the SCM algorithm to learn, represent and distinguish different user contexts without any user input.
[symbol string recognition, Multidimensional systems, Statistical analysis, Area measurement, symbol string clustering map, weighted average, Context awareness, Data mining, unsupervised clustering, information representation, context recognition, pattern clustering, online operation, Hidden Markov models, Clustering algorithms, Keyboards, symbol manipulation, computationally simple similarity measure, data structures, Man machine systems, string matching, SCM, Mobile computing, computational complexity]
A self-organizing map with expanding force for data clustering and visualization
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The self-organizing map (SOM) is a powerful tool in the exploratory phase of data mining. However, due to the dimensional conflict, neighborhood preservation cannot always lead to perfect topology preservation. In this paper we establish an expanding SOM (ESOM) to detect and preserve better topology correspondence between the two spaces. Our experiment results demonstrate that the ESOM constructs better mappings than the classic SOM in terms of both topological and quantization errors. Furthermore, clustering results generated by the ESOM are more accurate than those of the SOM.
[expanding self-organizing map, Data analysis, exploratory phase, Neurons, data mining, Quantization, topology correspondence, data clustering, Data engineering, Data mining, Information systems, Computer science, quantization errors, topological errors, Network topology, expanding force, pattern clustering, self-organising feature maps, dimensional conflict, Data visualization, data visualisation, topology preservation, Space exploration, data visualization]
Empirical comparison of various reinforcement learning strategies for sequential targeted marketing
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We empirically evaluate the performance of various reinforcement learning methods in applications to sequential targeted marketing. In particular we propose and evaluate a progression of reinforcement learning methods, ranging from the "direct" or "batch" methods to "indirect" or "simulation based" methods, and those that we call "semidirect" methods that fall between them. We conduct a number of controlled experiments to evaluate the performance of these competing methods. Our results indicate that while the indirect methods can perform better in a situation in which nearly perfect modeling is possible, under the more realistic situations in which the system's modeling parameters have restricted attention, the indirect methods' performance tend to degrade. We also show that semi-direct methods are effective in reducing the amount of computation necessary to attain a given level of performance, and often result in more profitable policies.
[Costs, decision theory, Decision making, cost-sensitive learning, data mining, sequential targeted marketing, Data mining, History, reinforcement learning, Learning systems, Degradation, marketing, performance, targeted marketing, decision making, Sampling methods, Mirrors, learning (artificial intelligence)]
Association analysis with one scan of databases
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Mining frequent patterns with an FP-tree avoids costly candidate generation and repeatedly occurrence frequency checking against the support threshold. It therefore achieves better performance and efficiency than Apriori-like algorithms. However the database still needs to be scanned twice to get the FP-tree. This can be very time-consuming when new data are added to an existing database because two scans may be needed for not only the new data but also the existing data. This paper presents a new data structure P-tree, Pattern Tree, and a new technique, which can get the P-tree through only one scan of the database and can obtain the corresponding FP-tree with a specified support threshold. Updating a P-tree with new data needs one scan of the new data only, and the existing data do not need to be re-scanned.
[association rule, occurrence frequency checking, data mining, large database, support threshold, candidate generation, Itemsets, very large databases, tree data structures, pattern recognition, Pattern Tree, Tree data structures, P-tree data structure, Data structures, FP-tree, Transaction databases, Association rules, Computer science, performance, database scan, association analysis, Frequency, Iterative algorithms, Apriori-like algorithms, frequent pattern mining]
Generating an informative cover for association rules
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Mining association rules may generate a large numbers of rules making the results hard to analyze manually. Pasquier et al. have discussed the generation of Guigues-Duquenne-Luxenburger basis (GD-L basis). Using a similar approach, we introduce a new rule of inference and define the notion of association rules cover as a minimal set of rules that are non-redundant with respect to this new rule of inference. Our experimental results (obtained using both synthetic and real data sets) show that our covers are smaller than the GD-L basis and they are computed in time that is comparable to the classic Apriori algorithm for generating rules.
[Algorithm design and analysis, Guigues-Duquenne-Luxenburger basis, Terminology, inference, Humans, data mining, dense databases, association rules, mining, Association rules, Data mining, inference mechanisms, Computer science, Mushroom database, Itemsets, Inference algorithms, Artificial intelligence]
Adaptive parallel sentences mining from web bilingual news collection
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In this paper a robust, adaptive approach for mining parallel sentences from a bilingual comparable news collection is described Sentence length models and lexicon-based models are combined under a maximum likelihood criterion. Specific models are proposed to handle insertions and deletions that are frequent in bilingual data collected from the web. The proposed approach is adaptive, updating the translation lexicon iteratively using the mined parallel data to get better vocabulary coverage and translation probability parameter estimation. Experiments are carried out on 10 years of Xinhua bilingual news collection. Using the mined data, we get significant improvement in word-to-word alignment accuracy in machine translation modeling.
[Vocabulary, sentence length models, Parameter estimation, adaptive approach, data mining, maximum likelihood estimation, machine translation modeling, lexicon-based models, Xinhua bilingual news collection, Robustness, Natural language processing, Maximum likelihood estimation, Web bilingual news collection, Natural languages, Probability, dynamic programming, adaptive parallel sentences mining, Information retrieval, mined parallel data, Computer science, translation probability parameter estimation, vocabulary coverage, Web pages, maximum likelihood criterion, language translation]
A theory of inductive query answering
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We introduce the Boolean inductive query evaluation problem, which is concerned with answering inductive queries that are arbitrary Boolean expressions over monotonic and anti-monotonic predicates. Secondly, we develop a decomposition theory for inductive query evaluation in which a Boolean query Q is reformulated into k sub-queries Q/sub i/ = Q/sub A/ /spl and/ Q/sub M/ that are the conjunction of a monotonic and an anti-monotonic predicate. The solution to each subquery can be represented using a version space. We investigate how the number of version spaces k needed to answer the query can be minimized. Thirdly, for the pattern domain of strings, we show how the version spaces can be represented using a novel data structure, called the version space tree, and can be computed using a variant of the famous a priori algorithm. Finally, we present experiments that validate the approach.
[data mining, inductive query answering theory, data structure, Data mining, History, database management systems, Database languages, monotonic predicates, query processing, decomposition theory, strings, a priori algorithm, anti-monotonic predicates, version space tree, Database systems, version space, Data structures, pattern domain, Boolean algebra, Association rules, Information technology, database theory, Boolean query reformulation, Boolean inductive query evaluation problem, Query processing, Machine learning, Frequency, sub-queries]
Estimating the number of segments in time series data using permutation tests
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Segmentation is a popular technique for discovering structure in time series data. We address the largely open problem of estimating the number of segments that can be reliably discovered. We introduce a novel method for the problem, called Pete. Pete is based on permutation testing. The problem is an instance of model (dimension) selection. The proposed method analyzes the possible overfit of a model to the available data rather than using a term for penalizing model complexity. In this respect the approach is more similar to cross-validation than regularization based techniques (e.g., AIC, BIC, MDL, MML). Furthermore, the method produces a p value for each increase in the number of segments. This gives the user an overview of the statistical significance of segmentations. We evaluate the performance of the proposed method using both synthetic and real time series data. The experiments show that permutation testing gives realistic results for the number of reliably identifiable segments and compares favorably with Monte Carlo cross-validation (MCCV) and commonly used BIC criteria.
[Multidimensional systems, statistical significance, Pete, overfit model, Time series analysis, data mining, permutation tests, model selection, performance evaluation, time series, Organisms, Time measurement, cross-validation, Sediments, Data mining, Computer science, Monte Carlo methods, pattern clustering, segment number estimation, Lakes, segmentation, time series data, Testing]
User-directed exploration of mining space with multiple attributes
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
There has been a growing interest in mining frequent itemsets in relational data with multiple attributes. A key step in this approach is to select a set of attributes that group data into transactions and a separate set of attributes that labels data into items. Unsupervised and unrestricted mining, however is stymied by the combinatorial complexity and the quantity of patterns as the number of attributes grows. In this paper we focus on leveraging the semantics of the underlying data for mining frequent itemsets. For instance, there are usually taxonomies in the data schema and functional dependencies among the attributes. Domain knowledge and user preferences often have the potential to significantly reduce the exponentially growing mining space. These observations motivate the design of a user-directed data mining framework that allows such domain knowledge to guide the mining process and control the mining strategy. We show examples of tremendous reduction in computation by using domain knowledge in mining relational data with multiple attributes.
[transaction processing, domain knowledge, Taxonomy, data mining, multiple attributes, user preferences, user-directed mining space exploration, semantics, Data mining, functional dependencies, combinatorial complexity, Itemsets, data grouping, Complex networks, Production, exponentially growing mining space reduction, data labelling, Space exploration, Pattern analysis, Marine vehicles, unsupervised unrestricted mining, relational databases, transactions, Computational complexity, frequent itemset mining, relational data, data schema, taxonomies, user directed data mining framework]
Concept tree based clustering visualization with shaded similarity matrices
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
One problem with existing clustering methods is that the interpretation of clusters may be difficult. Two different approaches have been used to solve this problem: conceptual clustering in machine learning and clustering visualization in statistics and graphics. The purpose of this paper is to investigate the benefits of combining clustering visualization and conceptual clustering to obtain better cluster interpretations. In our research we have combined concept trees for conceptual clustering with shaded similarity matrices for visualization. Experimentation shows that the two interpretation approaches can complement each other to help us understand data better.
[Symmetric matrices, conceptual clustering, Clustering methods, data mining, graphics, cluster interpretations, machine learning, Statistics, matrix algebra, Graphics, Iris, Information science, Tree graphs, pattern clustering, Data visualization, data visualisation, Machine learning, concept tree based clustering visualization, shaded similarity matrices, Libraries, tree data structures, learning (artificial intelligence), statistics]
Solving the fragmentation problem of decision trees by discovering boundary emerging patterns
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The single coverage constraint discourages a decision tree to contain many significant rules. The loss of significant rules leads to a loss in accuracy. On the other hand, the fragmentation problem causes a decision tree to contain too many minor rules. The presence of minor rules decreases the accuracy. We propose to use emerging patterns to solve these problems. In our approach, many globally significant rules can be discovered. Extensive expert. mental results on gene expression datasets show that our approach are more accurate than single C4.5 trees, and are also better than bagged or boosted C4.5 trees.
[minor rules, Laboratories, data mining, gene expression datasets, rule discovery, fragmentation problem, Gene expression, Information technology, emerging pattern, Support vector machines, Neural networks, Training data, Support vector machine classification, decision trees, Decision trees, Classification tree analysis, Testing]
Visually mining Web user clickpaths
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
As powerful as clickpath mining methods can be, they often lead to huge incomprehensible and non-interesting result sets. Our clickpath mining practice at MSN was faced with challenges of keeping analysts closer to the data exploration process, revealing powerful insight from clickpath mining that business owners can directly act upon. These challenges stressed the importance of an interactive and visual representation of clickpath mining results. Most products today that can perform clickpath visualization do so by presenting massive cross-weaving web graphs. We present a new type of clickpath visualization which focuses only on clickpaths of interest, simplifying the visualization space while still retaining the same degree of mineable knowledge in the data. We also describe visualization techniques we have used to enhance the detection of interesting clickpath patterns from data, and provide a real-life case study that has benefited from the use of our implemented clickpath visualizer PAVE.
[visual representation, Web page design, Data analysis, Navigation, data mining, Data mining, Uniform resource locators, clickpath mining methods, Feedback, clickpath visualizer PAVE, Data visualization, Packaging, interactive representation, Internet, clickpath visualization, Web user clickpaths mining, mineable knowledge]
Mining association rules from stars
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Association rule mining is an important data mining problem. It is found to be useful for conventional relational data. However, previous work has mostly targeted on mining a single table. In real life, a database is typically made up of multiple tables and one important case is where some of the tables form a star schema. The tables typically correspond to entity sets and joining the tables in a star schema gives relationships among entity sets which can be very interesting information. Hence mining on the join result is an important problem. Based on characteristics of the star schema we propose an efficient algorithm for mining association rules on the join result but without actually performing the join operation. We show that this approach can significantly out-perform the join-then-mine approach even when the latter adopts a fastest known mining algorithm.
[entity sets, data mining, Relational databases, Data engineering, Frequency conversion, Transaction databases, Data mining, Association rules, relational databases, join-then-mine approach, Distributed computing, star schema, Computer science, relational data, Itemsets, association rules mining]
Learning with progressive transductive Support Vector Machine
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Support Vector Machine (SVM) is a new learning method developed in recent years based on the foundations of statistical learning theory. By taking a transductive approach instead of an inductive one in support vector classifiers, the test set can be used as an additional source of information about margins. Intuitively, we would expect transductive learning to yield improvements when the training sets are small or when there is a significant deviation between the training and working set subsamples of the total population. In this paper, a progressive transductive support vector machine is addressed to extend Joachims' Transductive SVM to handle different class distributions. It solves the problem of having to estimate the ratio of positive/negative examples from the working set. The experimental results show that the algorithm is very promising.
[learning automata, statistical learning, Laboratories, progressive transductive support vector machine, transductive learning, SVM, transductive inference, support vector classifiers, Support vector machines, Human computer interaction, support vector machine, Support vector machine classification, Machine learning, training sets, Inference algorithms, Iterative algorithms, Risk management, Labeling, learning by example, Testing, pattern recognition]
Using text mining to infer semantic attributes for retail data mining
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Current data mining techniques usually do not have a mechanism to automatically infer semantic features inherent in the data being "mined". The semantics are either injected in the initial stages (by feature construction) or by interpreting the results produced by the algorithms. Both of these techniques have proved effective but require a lot of human effort. In many domains, semantic information is implicitly available and can be extracted automatically to improve data mining systems. In this paper we present a case study of a system that is trained to extract semantic features for apparel products and populate a knowledge base with these products and features. We show that semantic features of these items can be successfully extracted by applying text learning techniques to the descriptions obtained from websites of retailers. We also describe several applications of such a knowledge base of product semantics that we have built including recommender systems and competitive intelligence tools and provide evidence that our approach can successfully build a knowledge base with accurate facts which can then be used to create profiles of individual customers, groups of customers, or entire retail stores.
[semantic attribute inference, text learning techniques, text analysis, retail data mining, Humans, data mining, Data engineering, Data mining, knowledge based systems, apparel products, Competitive intelligence, text mining, Decision trees, semantic features, retailer Web sites, Recommender systems, retail data processing, Text mining, Data analysis, retailer Websites, product semantics, byfeature construction, Association rules, inference mechanisms, semantic feature extraction, recommender systems, Neural networks, competitive intelligence tools]
Online algorithms for mining semi-structured data stream
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In this paper, we study an online data mining problem from streams of semi-structured data such as XML data. Modeling semi-structured data and patterns as labeled ordered trees, we present an online algorithm StreamT that receives fragments of an unseen possibly infinite semi-structured data in the document order through a data stream, and can return the current set of frequent patterns immediately on request at any time. A crucial part of our algorithm is the incremental maintenance of the occurrences of possibly frequent patterns using a tree sweeping technique. We give modifications of the algorithm to other online mining model. We present theoretical and empirical analyses to evaluate the performance of the algorithm.
[Algorithm design and analysis, online data mining, incremental maintenance, data mining, online algorithm, Data mining, tree sweeping technique, Technology management, XML, Web pages, semi-structured data, StreamT, data structures, XML data, Performance analysis, frequent pattern discovery, Data communication, Pattern analysis, Informatics, Monitoring, hypermedia markup languages, pattern recognition]
Adaptive dimension reduction for clustering high dimensional data
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
It is well-known that for high dimensional data clustering, standard algorithms such as EM and K-means are often trapped in a local minimum. Many initialization methods have been proposed to tackle this problem, with only limited success. In this paper we propose a new approach to resolve this problem by repeated dimension reductions such that K-means or EM are performed only in very low dimensions. Cluster membership is utilized as a bridge between the reduced dimensional subspace and the original space, providing flexibility and ease of implementation. Clustering analysis performed on highly overlapped Gaussians, DNA gene expression profiles and Internet newsgroups demonstrate the effectiveness of the proposed algorithm.
[Algorithm design and analysis, Image processing, data mining, highly overlapped Gaussians, Gene expression, reduced dimensional subspace, adaptive systems, Information analysis, Bridges, DNA gene expression profiles, K-means algorithm, Internet newsgroups, Image analysis, high dimensional data clustering, local minimum, cluster membership, pattern clustering, Clustering algorithms, Gaussian processes, Performance analysis, EM algorithm, adaptive dimension reduction, Principal component analysis]
Predicting rare events in temporal domains
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Temporal data mining aims at finding patterns in historical data. Our work proposes an approach to extract temporal patterns from data to predict the occurrence of target events, such as computer attacks on host networks, or fraudulent transactions in financial institutions. Our problem formulation exhibits two major challenges: 1) we assume events being characterized by categorical features and displaying uneven inter-arrival times; such an assumption falls outside the scope of classical time-series analysis, 2) we assume target events are highly infrequent; predictive techniques must deal with the class-imbalance problem. We propose an efficient algorithm that tackles the challenges above by transforming the event prediction problem into a search for all frequent eventsets preceding target events. The class imbalance problem is overcome by a search for patterns on the minority class exclusively; the discrimination power of patterns is then validated against other classes. Patterns are then combined into a rule-based model for prediction. Our experimental analysis indicates the types of event sequences where target events can be accurately predicted.
[financial institutions, Target recognition, data mining, discrimination power, temporal patterns extraction, rule-based model, set theory, Computer crime, computer attacks, Network servers, class-imbalance problem, frequent eventsets, USA Councils, categorical features, predictive techniques, rare events prediction, temporal domains, Testing, host networks, Particle separators, Knowledge based systems, fraudulent transactions, Computer science, Computer displays, uneven inter-arrival times, Speech recognition, minority class, temporal data mining]
Experimentation and self learning in continuous database marketing
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We present a method for continuous database marketing that identifies target customers for a number of marketing offers using predictive models. The algorithm then selects the appropriate offer for the customer. Experimental design principles are encapsulated to capture more information that will be used to monitor and refine the predictive models. The updated predictive models are then used for the next round of marketing offers.
[self learning, Optimization methods, data mining, Predictive models, Electronic switching systems, experimental design principles, Marketing management, marketing data processing, Data mining, Databases, predictive models, very large databases, Mathematical model, learning (artificial intelligence), Mining industry, continuous database marketing, Monitoring, Testing]
PERUSE: An unsupervised algorithm for finding recurring patterns in time series
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
This paper describes PERUSE, an unsupervised algorithm for finding recurring patterns in time series. It was initially developed and tested with sensor data from a mobile robot, i.e. noisy, real-valued, multivariate time series with variable intervals between observations. The pattern discovery problem is decomposed into two subproblems: (1) a supervised learning problem in which a teacher provides exemplars of patterns and labels time series according to whether they contain the patterns; (2) an unsupervised learning problem in which the time series are used to generate an approximation to the teacher. Experimental results show that PERUSE can discover patterns in audio data corresponding to recurring words in natural language utterances and patterns in the sensor data of a mobile robot corresponding to qualitatively distinct outcomes of taking actions.
[Natural languages, multivariate time series, data mining, supervised learning, dynamic programming, pattern discovery problem, time series, mobile robot, Time measurement, natural language utterances, mobile robots, Mobile robots, Data mining, Radio broadcasting, Unsupervised learning, Computer science, Acoustic noise, Supervised learning, unsupervised algorithm, PERUSE, Testing]
Towards automatic generation of query taxonomy: a hierarchical query clustering approach
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Most previous work on automatic query clustering generated a flat, un-nested partition of query terms. In this work, we discuss the organization of query terms into a hierarchical structure and construct a query taxonomy in an automatic way. The proposed approach is designed based on a hierarchical agglomerative clustering algorithm to hierarchically group similar queries and generate cluster hierarchies using a novel cluster partition technique. The search processes of real-world search engines are combined to obtain highly ranked Web documents as the feature source for each query term. Preliminary experiments show that the proposed approach is effective for obtaining thesaurus information for query terms, and is also feasible for constructing a query taxonomy which provides a basis for in-depth analysis of users' search interests and domain-specific vocabulary on a larger scale.
[Vocabulary, thesauri, search engines, Terminology, automatic query taxonomy generation, Taxonomy, information retrieval, Thesauri, highly ranked Web documents, Information science, cluster partition technique, Clustering algorithms, thesaurus information, hierarchical query clustering approach, Search engines, domain-specific vocabulary, user search interests, Performance analysis, information needs, Marine vehicles, Classification tree analysis, hierarchical agglomerative clustering algorithm]
Phrase-based document similarity based on an index graph model
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
Document clustering techniques mostly rely on single term analysis of the document data set, such as the vector space model. To better capture the structure of documents, the underlying data model should be able to represent the phrases in the document as well as single terms. We present a novel data model, the document index graph, which indexes web documents based on phrases, rather than single terms only. The semi-structured web documents help in identifying potential phrases that when matched with other documents indicate strong similarity between the documents. The document index graph captures this information, and finding significant matching phrases between documents becomes easy and efficient with such model. The similarity between documents is based on both single term weights and matching phrases weights. The combined similarities are used with standard document clustering techniques to test their effect on the clustering quality. Experimental results show that our phrase-based similarity, combined with single-term similarity measures, enhances web document clustering quality significantly.
[text analysis, Clustering methods, indexing, vector space model, document structure capture, document data set, Data engineering, Functional analysis, Data mining, phrase representation, System analysis and design, Design engineering, phrase-based document similarity, single term weights, document clustering techniques, directed graphs, Web mining, single term analysis, matching phrases weights, Systems engineering and theory, Web documents, Data models, Web sites, document index graph]
Modal-style operators in qualitative data analysis
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
We explore the usage of the modal possibility operator (and its dual necessity operator) in qualitative data analysis, and show that it-quite literally-complements the derivation operator of formal concept analysis; we also propose a new generalization of the rough set approximation operators. As an example for the applicability of the concepts we investigate the Morse data set which has been frequently studied in multidimensional scaling procedures.
[qualitative data analysis, rough set approximation operator generalization, approximation theory, Data analysis, Costs, modal-style operators, data analysis, modal possibility operator, modal analysis, dual necessity operator, Computer science, concept analysis, Morse data set, multidimensional scaling, Logic, rough set theory]
Adaptive and resource-aware mining of frequent sets
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
The performance of an algorithm that mines frequent sets from transactional databases may severely depend on the specific features of the data being analyzed. Moreover, some architectural characteristics of the computational platform used - e.g. the available main memory - can dramatically change its runtime behavior. In this paper we present DCI (Direct Count & Intersect), an efficient algorithm for discovering frequent sets from large databases. Due to the multiple heuristics strategies adopted, DCI can adapt its behavior not only to the features of the specific computing platform, but also to the features of the dataset being mined, so that it results very effective in mining both short and long patterns from sparse and dense datasets. Finally we also discuss the parallelization strategies adopted in the design of ParDCI, a distributed and multi-threaded implementation of DCI.
[Algorithm design and analysis, Data analysis, ParDCl, data mining, transactional databases, frequent sets, Spatial databases, Transaction databases, Data mining, Association rules, database management systems, DCl, multiple heuristics strategies, resource-aware mining, Runtime, adaptive mining, Itemsets, Power capacitors, Performance analysis, large databases]
Mining top-k frequent closed patterns without minimum support
2002 IEEE International Conference on Data Mining, 2002. Proceedings.
None
2002
In this paper, we propose a new mining task: mining top-k frequent closed patterns of length no less than min_/spl lscr/, where k is the desired number of frequent closed patterns to be mined, and min_/spl lscr/ is the minimal length of each pattern. An efficient algorithm, called TFP, is developed for mining such patterns without minimum support. Two methods, closed-node-count and descendant-sum are proposed to effectively raise support threshold and prune FP-tree both during and after the construction of FP-tree. During the mining process, a novel top-down and bottom-up combined FP-tree mining strategy is developed to speed-up support-raising and closed frequent pattern discovering. In addition, a fast hash-based closed pattern verification scheme has been employed to check efficiently if a potential closed pattern is really closed. Our performance study shows that in most cases, TFP outperforms CLOSET and CHARM, two efficient frequent closed pattern mining algorithms, even when both are running with the best tuned min-support. Furthermore, the method can be extended to generate association rules and to incorporate user-specified constraints.
[efficient algorithm, minimum support, fast hash-based closed pattern verification scheme, data mining, trees (mathematics), support threshold, descendant-sum method, Transaction databases, Data mining, Association rules, closed frequent pattern discovery, support-raising, top-k frequent closed pattern mining, closed-node-count method, FP-tree pruning, TFP]
Structure search and stability enhancement of Bayesian networks
Third IEEE International Conference on Data Mining
None
2003
Learning Bayesian network structure from large-scale data sets, without any expert-specified ordering of variables, remains a difficult problem. We propose systematic improvements to automatically learn Bayesian network structure from data. (1) We propose a linear parent search method to generate candidate graph. (2) We propose a comprehensive approach to eliminate cycles using minimal likelihood loss, a short cycle first heuristic, and a cut-edge repairing. (3) We propose structure perturbation to assess the stability of the network and a stability-improvement method to refine the network structure. The algorithms are easy to implement and efficient for large networks. Experimental results on two data sets show that our new approach outperforms existing methods.
[Stability, structure perturbation, Bayesian network structure learning, Laboratories, data mining, candidate graph, cut-edge repairing, Data mining, minimal likelihood loss, Bayesian methods, Search methods, parent search method, very large databases, large-scale data sets, Computer networks, Large-scale systems, belief networks, learning (artificial intelligence), stability enhancement method, search problems, Biomedical imaging, Testing, computational complexity]
Mining frequent itemsets in distributed and dynamic databases
Third IEEE International Conference on Data Mining
None
2003
Traditional methods for frequent itemset mining typically assume that data is centralized and static. Such methods impose excessive communication overhead when data is distributed, and they waste computational resources when data is dynamic. We present what we believe to be the first unified approach that overcomes these assumptions. Our approach makes use of parallel and incremental techniques to generate frequent itemsets in the presence of data updates without examining the entire database, and imposes minimal communication overhead when mining distributed databases. Further, our approach is able to generate both local and global frequent itemsets. This ability permits our approach to identify high-contrast frequent itemsets, which allows one to examine how the data is skewed over different sites.
[parallel algorithms, data mining, incremental techniques, Transaction databases, dynamic databases, Data mining, Distributed computing, Parallel algorithms, Computer science, query processing, parallel techniques, Information science, Itemsets, Distributed databases, query response time, distributed databases, Frequency, Computer networks, minimisation, frequent itemsets mining, communication overhead minimization]
The rough set approach to association rule mining
Third IEEE International Conference on Data Mining
None
2003
In transaction processing, an association is said to exist between two sets of items when a transaction containing one set is likely to also contain the other. In information retrieval, an association between two sets of keywords occurs when they cooccur in a document. Similarly, in data mining, an association occurs when one attribute set occurs together with another. As the number of such associations may be large, maximal association rules are sought, e.g., Feldman et al. (1997, 1998). Rough set theory is a successful tool for data mining. By using this theory, rules similar to maximal associations can be found. However, we show that the rough set approach to discovering knowledge is much simpler than the maximal association method.
[transaction processing, Vocabulary, Temperature, data mining, information retrieval, Information retrieval, Educational institutions, maximal association rule mining, knowledge discovery, Association rules, Data mining, Computer science, USA Councils, Set theory, rough set theory, Thermostats]
Identifying Markov blankets with decision tree induction
Third IEEE International Conference on Data Mining
None
2003
The Markov blanket of a target variable is the minimum conditioning set of variables that makes the target independent of all other variables. Markov blankets inform feature selection, aid in causal discovery and serve as a basis for scalable methods of constructing Bayesian networks. We apply decision tree induction to the task of Markov blanket identification. Notably, we compare (a) C5.0, a widely used algorithm for decision rule induction, (b) C5C, which post-processes C5.0 's rule set to retain the most frequently referenced variables and (c) PC, a standard method for Bayesian network induction. C5C performs as well as or better than C5.0 and PC across a number of data sets. Our modest variation of an inexpensive, accurate, off-the-shelf induction engine mitigates the need for specialized procedures, and establishes baseline performance against which specialized algorithms can be compared.
[Performance evaluation, Costs, Biomedical informatics, Organisms, Gene expression, Engines, Computer science, Bayesian methods, decision tree induction, decision trees, Markov processes, Bayesian networks, Decision trees, belief networks, learning (artificial intelligence), Markov blanket identification, feature selection, Testing]
The hybrid Poisson aspect model for personalized shopping recommendation
Third IEEE International Conference on Data Mining
None
2003
Predicting an individual customer's likelihood of purchasing a specific item forms the basis of many marketing activities, such as personalized shopping recommendation. Collaborative filtering and association rule mining can be applied to this problem, but in retail supermarkets, the problem becomes particularly challenging because of the sparsity and skewness of transaction data. We present HyPAM (hybrid Poisson aspect model), a new probabilistic graphical model that combines a Poisson mixture with a latent aspect class model to model customers' shopping behavior. We empirically compare HyPAM with two well-known recommenders, GroupLens (a correlation-based method), and IBM SmartPad (association rules and cosine similarity). Experimental results show that HyPAM outperforms the other recommenders by a large margin for two real-world retail supermarkets, ranking most of actual purchases in the top ten percent of the most likely purchased items. We also present a new visualization method, rank plot, to evaluate the quality of recommendations.
[customer likelihood prediction, personalized shopping recommendation, hybrid Poisson aspect model, collaborative filtering, IBM SmartPad recommenders, purchasing, data mining, retail supermarket, Data mining, Poisson distribution, information filters, association rule mining, GroupLens recommenders, marketing, transaction data processing, customers shopping, retail data processing]
Protecting sensitive knowledge by data sanitization
Third IEEE International Conference on Data Mining
None
2003
We address the problem of protecting some sensitive knowledge in transactional databases. The challenge is on protecting actionable knowledge for strategic decisions, but at the same time not losing the great benefit of association rule mining. To accomplish that, we introduce a new, efficient one-scan algorithm that meets privacy protection and accuracy in association rule mining, without putting at risk the effectiveness of the data mining per se.
[transaction processing, Data privacy, Data security, data mining, transactional databases, Transaction databases, strategic decisions, Association rules, Data mining, one-scan algorithm, Information analysis, association rule mining, data sanitization, associative processing, NP-hard problem, very large databases, Information security, Collaboration, sensitive knowledge privacy protection, data privacy, Protection]
Applying noise handling techniques to genomic data: a case study
Third IEEE International Conference on Data Mining
None
2003
Osteogenesis Imperfecta (OI) is a genetic collagenous disease associated with mutations in one or both of the genes COLIA1 and COLIA2. There are at least four known phenotypes of OI, of which type II is the severest and often lethal. We identified three approaches to noise handling, namely, robust algorithms, filtering, and polishing, and evaluated their effectiveness when applied to the problem of classifying the disease OI based on a data set of amino acid sequences and associated information of point mutations of COLIA1. Preliminary results suggest that each noise handling mechanism is useful under different circumstances. Filtering is stable across all cases. Pruning with robust c4.5 increased the classification accuracy in some cases, and polishing gave rise to some additional improvement in classifying the lethal OI phenotype.
[Osteogenesis Imperfecta phenotype, Computer aided software engineering, Genetic mutations, Genomics, Humans, data mining, Amino acids, diseases, Bone diseases, Information filtering, information filters, genetic collagenous disease, noise handling technique, robust algorithm, genetics, amino acid sequence, proteins, noise, Information filters, Noise robustness, genomic data, Bioinformatics, medical computing, filtering]
Statistical relational learning for document mining
Third IEEE International Conference on Data Mining
None
2003
A major obstacle to fully integrated deployment of many data mining algorithms is the assumption that data sits in a single table, even though most real-world databases have complex relational structures. We propose an integrated approach to statistical modelling from relational databases. We structure the search space based on "refinement graphs\
[decision theory, Statistical learning, Laboratories, data mining, Relational databases, regression analysis, statistical model, Data mining, Information science, real-world database, National electric code, learning logic description, refinement graphs, search space, learning (artificial intelligence), logistic regression, inductive logic programming, document handling, Logic programming, feature selection decision, statistical relational learning, Spatial databases, relational database, relational databases, Statistics, scientific paper, complex relational structure, data mining algorithm, document mining, classical flat feature, Logistics]
Fast PNN-based clustering using k-nearest neighbor graph
Third IEEE International Conference on Data Mining
None
2003
Search for nearest neighbor is the main source of computation in most clustering algorithms. We propose the use of nearest neighbor graph for reducing the number of candidates. The number of distance calculations per search can be reduced from O(N) to O(k) or where N is the number of clusters, and k is the number of neighbors in the graph. We apply the proposed scheme within agglomerative clustering algorithm known as the PNN algorithm.
[Costs, pairwise nearest neighbor, Vector quantization, graph theory, Optimization methods, search problem, k-nearest neighbor graph, Nearest neighbor searches, Computer science, vector quantisation, PNN, agglomerative clustering algorithm, Tree graphs, Clustering algorithms, Mean square error methods, vector quantization, Iterative algorithms, Distortion measurement, statistical analysis, search problems]
An algebra for inductive query evaluation
Third IEEE International Conference on Data Mining
None
2003
Inductive queries are queries that generate pattern sets. We study properties of Boolean inductive queries, i.e. queries that are Boolean expressions over monotonic and antimonotonic constraints. More specifically, we introduce and study algebraic operations on the answer sets of such queries and show how these can be used for constructing and optimizing query plans. Special attention is devoted to the dimension of the queries, i.e. the minimum number of version spaces needed to represent the answer sets. The framework has been implemented for the pattern domain of strings and experimentally validated.
[inductive query evaluation, antimonotonic constraint, constraint theory, data mining, monotonic constraint, Relational databases, Boolean algebra, Data mining, query processing, Boolean expression, Algebra, Query processing, algebraic operation, query plan optimization, Database systems]
Findings from a practical project concerning Web usage mining
Third IEEE International Conference on Data Mining
None
2003
In a practical project a statistical analysis of the Web log files of the domain www.volkswagen.de was carried out by using the CRISP-DM procedure. For the preprocessing phase, more profound findings could be gained than are usually described in many studies. Since the aim was to deduce significant statements while measuring the effect, tests of significance for e-metrics were used in addition to the commonly described procedure.
[Data analysis, Statistical analysis, Navigation, semantic Web, data understanding, data mining, Companies, Pattern recognition, user interfaces, Web log files, Automobiles, practical project, Web usage mining, e-metrics, data preprocessing phase, Internet, Web sites, statistical analysis, Robots, Testing, Distribution functions]
Is random model better? On its accuracy and efficiency
Third IEEE International Conference on Data Mining
None
2003
Inductive learning searches an optimal hypothesis that minimizes a given loss function. It is usually assumed that the simplest hypothesis that fits the data is the best approximate to an optimal hypothesis. Since finding the simplest hypothesis is NP-hard for most representations, we generally employ various heuristics to search its closest match. Computing these heuristics incurs significant cost, making learning inefficient and unscalable for large dataset. At the same time, it is still questionable if the simplest hypothesis is indeed the closest approximate to the optimal model. Recent success of combining multiple models, such as bagging, boosting and meta-learning, has greatly improved the accuracy of the simplest hypothesis, providing a strong argument against the optimality of the simplest hypothesis. However, computing these combined hypotheses incurs significantly higher cost. We first advert that as long as the error of a hypothesis on each example is within a range dictated by a given loss function, it can still be optimal. Contrary to common beliefs, we propose a completely random decision tree algorithm that achieves much higher accuracy than the single best hypothesis and is comparable to boosted or bagged multiple best hypotheses. The advantage of multiple random tree is its training efficiency as well as minimal memory requirement.
[Costs, random decision tree algorithm, data mining, Predictive models, Boosting, Data mining, inductive learning, optimisation, heuristic programming, NP-hard problem, heuristics, very large databases, large dataset, meta-learning, Machine learning, decision trees, Performance loss, Decision trees, optimal hypothesis, Bagging, learning by example, computational complexity]
Mining strong affinity association patterns in data sets with skewed support distribution
Third IEEE International Conference on Data Mining
None
2003
Existing association-rule mining algorithms often rely on the support-based pruning strategy to prune its combinatorial search space. This strategy is not quite effective for data sets with skewed support distributions because they tend to generate many spurious patterns involving items from different support levels or miss potentially interesting low-support patterns. To overcome these problems, we propose the concept of hyperclique pattern, which uses an objective measure called h-confidence to identify strong affinity patterns. We also introduce the novel concept of cross-support property for eliminating patterns involving items with substantially different support levels. Our experimental results demonstrate the effectiveness of this method for finding patterns in dense data sets even at very low support thresholds, where most of the existing algorithms would break down. Finally, hyperclique patterns also show great promise for clustering items in high dimensional space.
[Dairy products, Pairwise error probability, skewed support distribution, data mining, hyperclique pattern clustering, Data mining, Association rules, association rule mining, Computer science, Degradation, pattern clustering, very large databases, h-confidence measure, Clustering algorithms, Cities and towns, support-based pruning strategy, Frequency, strong affinity association pattern mining]
A user-driven and quality-oriented visualization for mining association rules
Third IEEE International Conference on Data Mining
None
2003
On account of the enormous amounts of rules that can be produced by data mining algorithms, knowledge validation is one of the most problematic steps in an association rule discovery process. In order to find relevant knowledge for decision-making, the user needs to really rummage through the rules. Visualization can be very beneficial to support him/her in this task by improving the intelligibility of the large rule sets and enabling the user to navigate inside them. We propose to answer the association rule validation problem by designing a human-centered visualization method for the rule rummaging task. This new approach based on a specific rummaging model relies on rule interestingness measures and on interactive rule subset focusing and mining. We have implemented our representation by developing a first experimental prototype called ARVis.
[Decision support systems, knowledge validation, data mining algorithms, data mining, association rule discovery process, rule rummaging task, user interfaces, Data mining, Database languages, very large databases, Prototypes, data visualisation, interactive systems, experimental prototype ARVis, interactive rule subset focusing, Navigation, Decision making, information retrieval, Association rules, quality-oriented visualization, association rule validation problem, decision-making, Data visualization, Information processing, decision making, human-centered visualization method, rule interestingness measures, intelligibility, Context modeling]
Ontologies improve text document clustering
Third IEEE International Conference on Data Mining
None
2003
Text document clustering plays an important role in providing intuitive navigation and browsing mechanisms by organizing large sets of documents into a small number of meaningful clusters. The bag of words representation used for these clustering methods is often unsatisfactory as it ignores relationships between important terms that do not cooccur literally. In order to deal with the problem, we integrate core ontologies as background knowledge into the process of clustering text documents. Our experimental evaluations compare clustering techniques based on pre-categorizations of texts from Reuters newsfeeds and on a smaller domain of an eLearning course about Java. In the experiments, improvements of results by background knowledge compared to a baseline without background knowledge can be shown in many interesting combinations.
[document handling, Java, Navigation, Clustering methods, data mining, Ontologies, Information retrieval, Knowledge management, information navigation, information browsing, Organizing, distance learning, text document clustering, Electronic learning, pattern clustering, Reuters newsfeeds, Java elearning course, Clustering algorithms, Web sites, ontology]
Introducing uncertainty into pattern discovery in temporal event sequences
Third IEEE International Conference on Data Mining
None
2003
Pattern discovery in temporal event sequences is of great importance in many application domains, such as telecommunication network fault analysis. In reality, not every type of event has an accurate timestamp. Some of them, defined as inaccurate events may only have an interval as possible time of occurrence. The existence of inaccurate events may cause uncertainty in event ordering. The traditional support model cannot deal with this uncertainty, which would cause some interesting patterns to be missing. A new concept, precise support, is introduced to evaluate the probability of a pattern contained in a sequence. Based on this new metric, we define the uncertainty model and present an algorithm to discover interesting patterns in the sequence database that has one type of inaccurate event. In our model, the number of types of inaccurate events can be extended to k readily, however, at a cost of increasing computational complexity.
[sequence database, Uncertainty, precise support, data mining, pattern discovery, uncertainty handling, Sun, Information technology, Computational complexity, uncertainty model, Information analysis, Intelligent networks, Databases, temporal databases, telecommunication networks, telecommunication network fault analysis, inaccurate event, Computational efficiency, Pattern analysis, Monitoring, temporal event sequence, computational complexity]
Icon-based visualization of large high-dimensional datasets
Third IEEE International Conference on Data Mining
None
2003
High dimensional data visualization is critical to data analysts since it gives a direct view of original data. We present a method to visualize large amount of high dimensional data. We divide dimensions of data into several groups. Then, we use one icon to represent each group, and associate visual properties of each icon with dimensions in each group. A high dimensional data record will be represented by multiple different types of icons located in the same position. Furthermore, we use summary icons to display local details of viewer's interests and the whole data set at meantime. We show its effectiveness and efficiency through a case study on a real large data set.
[Data analysis, graphical user interfaces, icon visual properties, Humans, Scattering, knowledge acquisition, Displays, icon-based visualization, summary icons, Pattern recognition, Data mining, high dimensional data visualization, Histograms, Shape control, very large databases, Data visualization, data visualisation, data analysts, high dimensional data record, Rendering (computer graphics), rendering (computer graphics)]
Simple estimators for relational Bayesian classifiers
Third IEEE International Conference on Data Mining
None
2003
We present the relational Bayesian classifier (RBC), a modification of the simple Bayesian classifier (SBC) for relational data. There exist several Bayesian classifiers that learn predictive models of relational data, but each uses a different estimation technique for modelling heterogeneous sets of attribute values. The effects of data characteristics on estimation have not been explored. We consider four simple estimation techniques and evaluate them on three real-world data sets. The estimator that assumes each multiset value is independently drawn from the same distribution (INDEPVAL) achieves the best empirical results. We examine bias and variance tradeoffs over a range of data sets and show that INDEPVAL's ability to model more multiset information results in lower bias estimates and contributes to its superior performance.
[INDEPVAL, estimation theory, estimation technique, Laboratories, relational Bayesian classifiers, Predictive models, Drives, Data mining, relational databases, multiset information, Computer science, Bayesian methods, relational data sets, Motion pictures, belief networks, learning (artificial intelligence)]
Algorithms for spatial outlier detection
Third IEEE International Conference on Data Mining
None
2003
A spatial outlier is a spatially referenced object whose non-spatial attribute values are significantly different from the values of its neighborhood. Identification of spatial outliers can lead to the discovery of unexpected, interesting, and useful spatial patterns for further analysis. One drawback of existing methods is that normal objects tend to be falsely detected as spatial outliers when their neighborhood contains true spatial outliers. We propose a suite of spatial outlier detection algorithms to overcome this disadvantage. We formulate the spatial outlier detection problem in a general way and design algorithms which can accurately detect spatial outliers. In addition, using a real-world census data set, we demonstrate that our approaches can not only avoid detecting false spatial outliers but also find true spatial outliers ignored by existing methods.
[Biometrics, iterative methods, data analysis, spatial outlier detection algorithm, Scattering, visual databases, iteration algorithm, spatial data analysis, Computer science, Graphics, Object detection, median algorithm, real-world census data set, Iterative algorithms, statistical databases, Performance analysis, Pattern analysis, Detection algorithms, Testing]
Regulatory element discovery using tree-structured models
Third IEEE International Conference on Data Mining
None
2003
Computational discovery of transcriptional regulatory regions in DNA sequences provides an efficient way to broaden our understanding of how cellular processes are controlled. We formulate the regulatory element discovery problem in the regression framework with regulatory regions treated as predictor variables and gene expression levels as responses. We use regression tree models to identify structural relationships between predictors and responses. The regression tree methodology is extended to handle multiple responses from different experiments by modifying the split function. We apply this method to two data sets of the yeast Saccharomyces cerevisiae. The method successfully identifies most of regulatory motifs that are known to control gene transcription under the given experimental conditions. Our method also suggests several putative motifs that present novel regulatory motifs.
[gene transcription, cellular process, data mining, regression analysis, Predictive models, Fungi, Proteins, genetics, biology computing, predictor variables, structural relationship, Regression tree analysis, DNA computing, Sequences, regression tree models, Biological system modeling, DNA sequences, Saccharomyces cerevisiae yeast, Process control, trees (mathematics), putative motifs, Multivariate regression, Gene expression, DNA, regulatory element discovery]
Regression clustering
Third IEEE International Conference on Data Mining
None
2003
Complex distribution in real-world data is often modeled by a mixture of simpler distributions. Clustering is one of the tools to reveal the structure of this mixture. The same is true to the datasets with chosen response variables that people run regression on. Without separating the clusters with very different response properties, the residue error of the regression is large. Input variable selection could also be misguided to a higher complexity by the mixture. In regression clustering (RC), K (>1) regression functions are applied to the dataset simultaneously which guide the clustering of the dataset into K subsets each with a simpler distribution matching its guiding function. Each function is regressed on its own subset of data with a much smaller residue error. Both the regressions and the clustering optimize a common objective function. We present a RC algorithm based on K-harmonic means clustering algorithm and compare it with other existing RC algorithms based on K-means and EM.
[Input variables, Laboratories, data mining, regression analysis, K-harmonic means clustering algorithm, real-world data, regression functions, Data mining, statistical distributions, optimisation, Clustering algorithms, regression clustering algorithm, Density functional theory, Marketing and sales, response variables, error statistics, Linear regression, complex distribution, Partitioning algorithms, input variable selection, Couplings, pattern clustering, guiding function, Mean square error methods, statistical analysis, regression residue error]
Exploiting unlabeled data for improving accuracy of predictive data mining
Third IEEE International Conference on Data Mining
None
2003
Predictive data mining typically relies on labeled data without exploiting a much larger amount of available unlabeled data. We show that using unlabeled data can be beneficial in a range of important prediction problems and therefore should be an integral part of the learning process. Given an unlabeled dataset representative of the underlying distribution and a K-class labeled sample that might be biased, our approach is to learn K contrast classifiers each trained to discriminate a certain class of labeled data from the unlabeled population. We illustrate that contrast classifiers can be useful in one-class classification, outlier detection, density estimation, and learning from biased data. The advantages of the proposed approach are demonstrated by an extensive evaluation on synthetic data followed by real-life bioinformatics applications for (1) ranking PubMed articles by their relevance to protein disorder and (2) cost-effective enlargement of a disordered protein database.
[Costs, K-class labeled sample, data mining, Data mining, outlier detection, disordered protein database, Proteins, Information science, Accuracy, Databases, very large databases, prediction problem, synthetic data, PubMed article ranking, unlabeled data exploitation, Labeling, learning (artificial intelligence), Bioinformatics, biased data, pattern classification, probability, medical information systems, predictive data mining accuracy improvement, Supervised learning, real-life bioinformatics application, one-class classification, Sampling methods, K contrast classifier learning]
Combining multiple weak clusterings
Third IEEE International Conference on Data Mining
None
2003
A data set can be clustered in many ways depending on the clustering algorithm employed, parameter settings used and other factors. Can multiple clusterings be combined so that the final partitioning of data provides better clustering? The answer depends on the quality of clusterings to be combined as well as the properties of the fusion method. First, we introduce a unified representation for multiple clusterings and formulate the corresponding categorical clustering problem. As a result, we show that the consensus function is related to the classical intra-class variance criterion using the generalized mutual information definition. Second, we show the efficacy of combining partitions generated by weak clustering algorithms that use data projections and random data splits. A simple explanatory model is offered for the behavior of combinations of such weak clustering components. We analyze the combination accuracy as a function of parameters controlling the power and resolution of component partitions as well as the learning dynamics vs. the number of clusterings involved. Finally, some empirical studies compare the effectiveness of several consensus functions.
[component partition, random data split, Taxonomy, data mining, multiple weak clustering algorithm, Classification algorithms, Data mining, parameter setting, Clustering algorithms, Training data, Robustness, learning (artificial intelligence), data projection, data set, consensus function, Partitioning algorithms, Computer science, intra-class variance criterion, pattern clustering, fusion method property, Fusion power generation, mutual information definition, learning dynamics, statistical analysis, categorical clustering problem, Mutual information]
Links between Kleinberg's hubs and authorities, correspondence analysis, and Markov chains
Third IEEE International Conference on Data Mining
None
2003
We show that Kleinberg's hubs and authorities model is closely related to both correspondence analysis, a well-known multivariate statistical technique, and a particular Markov chain model of navigation through the Web. The only difference between correspondence analysis and Kleinberg's method is the use of the average value of the hubs (authorities) scores for computing the authorities (hubs) scores, instead of the sum for Kleinberg's method. We also show that correspondence analysis and our Markov model are related to SALSA, a variant of Kleinberg's model.
[Navigation, Statistical analysis, Kleinberg hubs, Europe, data mining, correspondence analysis, Relational databases, Data mining, Kleinberg authorities model, Web navigation, Computer science, Markov chain, Web pages, Markov processes, Frequency, statistical analysis, Web sites, multivariate statistical technique]
Localized prediction of continuous target variables using hierarchical clustering
Third IEEE International Conference on Data Mining
None
2003
We propose a novel technique for the efficient prediction of multiple continuous target variables from high-dimensional and heterogeneous data sets using a hierarchical clustering approach. The proposed approach consists of three phases applied recursively: partitioning, localization and prediction. In the partitioning step, similar target variables are grouped together by a clustering algorithm. In the localization step, a classification model is used to predict which group of target variables is of particular interest. If the identified group of target variables still contains a large number of target variables, the partitioning and localization steps are repeated recursively and the identified group is further split into subgroups with more similar target variables. When the number of target variables per identified subgroup is sufficiently small, the third step predicts target variables using localized prediction models built from only those data records that correspond to the particular subgroup. Experiments performed on the problem of damage prediction in complex mechanical structures indicate that our proposed hierarchical approach is computationally more efficient and more accurate than straightforward methods of predicting each target variable individually or simultaneously using global prediction models.
[Data analysis, Scientific computing, clustering algorithm, Laboratories, Predictive models, Partitioning algorithms, Mechanical engineering, partitioning step, classification model, Computer science, Manufacturing processes, localization step, very large databases, Clustering algorithms, distributed databases, continuous target variables, localized prediction, hierarchical clustering, target variables, Large-scale systems, complex mechanical structures, statistical analysis, learning (artificial intelligence), heterogeneous data sets]
A K-NN associated fuzzy evidential reasoning classifier with adaptive neighbor selection
Third IEEE International Conference on Data Mining
None
2003
We present a fuzzy evidential reasoning algorithm in light of the Dempster-Shafer evidence theory and the K-nearest neighbor algorithm for pattern classification. Given an input pattern to be classified, each of its K nearest neighbors is viewed as an evidence source, in terms of a fuzzy evidence structure. The distance between the input pattern and each of its K nearest neighbors is used for mass determination while the contextual information of the nearest neighbor in the training sample space is formulated by a fuzzy set in determining a fuzzy focal element. Therefore, pooling evidence provided by neighbors is realized by a fuzzy evidential reasoning, where feature selection is further considered through ranking and adaptive combination of neighbors. A fast implementation scheme of the fuzzy evidential reasoning is also developed. Experimental results of classifying multichannel remote sensing images have shown that the proposed approach outperforms the K-nearest neighbor (K-NN) algorithm [T.M. Cover et al. (1967)], the fuzzy K-nearest neighbor (F-KNN) algorithm [J.M. Keller et al. (1985)], the evidence-theoretic K-nearest neighbor (E-KNN) algorithm [T. Denoex (1995)], and the fuzzy extended version of E-KNN (FE-KNN) [L.M. Zouhal et al. (1997)], in terms of the classification accuracy and insensitivity to the number K of nearest neighbors.
[adaptive neighbor selection, pattern classification, fuzzy set, Dempster-Shafer evidence theory, fuzzy set theory, K-nearest neighbor algorithm, training sample space, uncertainty handling, Data mining, contextual information, Remote sensing, Nearest neighbor searches, Multi-layer neural network, fuzzy evidential reasoning classifier, Fuzzy sets, Neural networks, Pattern classification, case-based reasoning, Fuzzy set theory, learning (artificial intelligence), Fuzzy reasoning, multichannel remote sensing images, Fuzzy systems]
Tree-structured partitioning based on splitting histograms of distances
Third IEEE International Conference on Data Mining
None
2003
We propose a novel clustering algorithm that is similar in spirit to classification trees. The data is recursively split using a criterion that applies a discrete curve evolution method to the histogram of distances. The algorithm can be depicted through tree diagrams with triple splits. Leaf nodes represent either clusters or sets of observations that can not yet be clearly assigned to a cluster. After constructing the tree, unclassified data points are mapped to their closest clusters. The algorithm has several advantages. First, it deals effectively with observations that can not be unambiguously assigned to a cluster by allowing a "margin of error". Second, it automatically determines the number of clusters; apart from the margin of error the user only needs to specify the minimal cluster size but not the number of clusters. Third, it is linear with respect to the number of data points and thus suitable for very large data sets. Experiments involving both simulated and real data from different domains show that the proposed method is effective and efficient.
[clustering algorithm, Clustering methods, Humans, data mining, tree diagram, discrete curve evolution method, Partitioning algorithms, Statistics, Histograms, Region 2, pattern clustering, very large databases, Clustering algorithms, Genetics, Iterative algorithms, tree data structures, tree-structured partitioning, Classification tree analysis, very large data set]
Optimized disjunctive association rules via sampling
Third IEEE International Conference on Data Mining
None
2003
The problem of finding optimized support association rules for a single numerical attribute, where the optimized region is a union of k disjoint intervals from the range of the attribute, is investigated. The first polynomial time algorithm for the problem of finding such a region maximizing support and meeting a minimum cumulative confidence threshold is given. Because the algorithm is not practical, an ostensibly easier, more constrained version of the problem is considered. Experiments demonstrate that the best extant algorithm for the constrained version has significant performance degradation on both a synthetic model of patterned data and on real world data sets. Running the algorithm on a small random sample is proposed as a means of obtaining near optimal results with high probability. Theoretical bounds on sufficient sample size to achieve a given performance level are proved, and rapid convergence on synthetic and real-world data is validated experimentally.
[sampling methods, data mining, sampling, Time measurement, performance degradation, Association rules, Data mining, History, polynomial time algorithm, Convergence, Computer science, Degradation, optimisation, Databases, very large databases, cumulative confidence threshold, Sampling methods, Polynomials, optimized disjunctive association rules]
A hybrid data-mining approach in genomics and text structures
Third IEEE International Conference on Data Mining
None
2003
We introduce a genetic sequence identifier based on a hierarchical system using fuzzy and classic (crisp) neural networks. The system is based on a set of predictors and on a decision network. The prediction of the structure of the genes is addressed using a new method and tools, involving the sequence of distances between bases and neuro-fuzzy predictors. The method and system have been successful in predicting genomic sequences and text structures.
[data-mining approach, decision network, Genomics, Hierarchical systems, data mining, fuzzy neural nets, text structures, genetic sequence identifier, Statistics, neuro-fuzzy predictors, Intelligent networks, Databases, genetics, biology computing, Neural networks, classic neural networks, decision making, fuzzy neural networks, Fuzzy neural networks, Genetics, Bioinformatics, Fuzzy systems]
Efficient data mining for maximal frequent subtrees
Third IEEE International Conference on Data Mining
None
2003
A new type of tree mining is defined, which uncovers maximal frequent induced subtrees from a database of unordered labeled trees. A novel algorithm, PathJoin, is proposed. The algorithm uses a compact data structure, FST-Forest, which compresses the trees and still keeps the original tree structure. PathJoin generates candidate subtrees by joining the frequent paths in FST-Forest. Such candidate subtree generation is localized and thus substantially reduces the number of candidate subtrees. Experiments with synthetic data sets show that the algorithm is effective and efficient.
[Tree data structures, FST-Forest data structure, PathJoin algorithm, data mining, Educational institutions, Data engineering, maximal frequent subtrees, Data mining, Association rules, candidate subtree generation, Computer science, unordered labeled trees database, Tree graphs, Databases, directed graphs, tree data structures, tree mining, Bioinformatics, Pattern analysis, synthetic data sets]
Mining relevant text from unlabelled documents
Third IEEE International Conference on Data Mining
None
2003
Automatic classification of documents is an important area of research with many applications in the fields of document searching, forensics and others. Methods to perform classification of text rely on the existence of a sample of documents whose class labels are known. However, in many situations, obtaining this sample may not be an easy (or even possible) task. We focus on the classification of unlabelled documents into two classes: relevant and irrelevant, given a topic of interest. By dividing the set of documents into buckets (for instance, answers returned by different search engines), and using association rule mining to find common sets of words among the buckets, we can efficiently obtain a sample of documents that has a large percentage of relevant ones. This sample can be used to train models to classify the entire set of documents. We prove, via experimentation, that our method is capable of filtering relevant documents even in adverse conditions where the percentage of irrelevant documents in the buckets is relatively high.
[document handling, Content based retrieval, search engines, sampling methods, document searching, Forensics, Image retrieval, data mining, information retrieval, Information retrieval, relevant text, Data mining, Application software, Association rules, classification, information filters, unlabelled document classification, association rule mining, forensics, document filtering, Web pages, Search engines, class labels, Software engineering]
ExAMiner: optimized level-wise frequent pattern mining with monotone constraints
Third IEEE International Conference on Data Mining
None
2003
The key point is that, in frequent pattern mining, the most appropriate way of exploiting monotone constraints in conjunction with frequency is to use them in order to reduce the problem input together with the search space. Following this intuition, we introduce ExAMiner, a level-wise algorithm which exploits the real synergy of antimonotone and monotone constraints: the total benefit is greater than the sum of the two individual benefits. ExAMiner generalizes the basic idea of the preprocessing algorithm ExAnte [F. Bonchi et al., (2003)], embedding such ideas at all levels of an Apriori-like computation. The resulting algorithm is the generalization of the Apriori algorithm when a conjunction of monotone constraints is conjoined to the frequency antimonotone constraint. Experimental results confirm that this is, so far, the most efficient way of attacking the computational problem in analysis.
[Embedded computing, monotone constraints, pattern mining, Laboratories, constraint theory, data mining, Transaction databases, ExAMiner algorithm, Data mining, Constraint optimization, optimized level-wise frequent pattern mining, frequency antimonotone constraint, optimisation, Itemsets, very large databases, Apriori algorithm, Frequency, ExAnte preprocessing algorithm]
Improving home automation by discovering regularly occurring device usage patterns
Third IEEE International Conference on Data Mining
None
2003
The data stream captured by recording inhabitant-device interactions in an environment can be mined to discover significant patterns, which an intelligent agent could use to automate device interactions. However, this knowledge discovery problem is complicated by several challenges, such as excessive noise in the data, data that does not naturally exist as transactions, a need to operate in real time, and a domain where frequency may not be the best discriminator. We propose a novel data mining technique that addresses these challenges and discovers regularly-occurring interactions with a smart home. We also discuss a case study that shows the data mining technique can improve the accuracy of two prediction algorithms, thus demonstrating multiple uses for a home automation system. Finally, we present an analysis of the algorithm and results obtained using inhabitant interactions.
[Home automation, data mining, Smart homes, pattern discovery, Displays, inhabitant-device interaction, knowledge discovery, Data mining, data stream, Intelligent sensors, Intelligent agent, Sensor arrays, home automation, Home appliances, intelligent agent, Neural networks, prediction algorithm, Prediction algorithms, cooperative systems, data mining technique, home automation system]
A dynamic adaptive self-organising hybrid model for text clustering
Third IEEE International Conference on Data Mining
None
2003
Clustering by document concepts is a powerful way of retrieving information from a large number of documents. This task in general does not make any assumption on the data distribution. For this task we propose a new competitive self-organising (SOM) model, namely the dynamic adaptive self-organising hybrid model (DASH). The features of DASH are a dynamic structure, hierarchical clustering, nonstationary data learning and parameter self-adjustment. All features are data-oriented: DASH adjusts its behaviour not only by modifying its parameters but also by an adaptive structure. The hierarchical growing architecture is a useful facility for such a competitive neural model which is designed for text clustering. We have presented a new type of self-organising dynamic growing neural network which can deal with the nonuniform data distribution and the nonstationary data sets and represent the inner data structure by a hierarchical view.
[text analysis, pattern clustering, text clustering, self-organising feature maps, data mining, self-adjusting systems, data structures, Data mining, statistical analysis, neural network, dynamic adaptive self-organising hybrid model]
A high-performance distributed algorithm for mining association rules
Third IEEE International Conference on Data Mining
None
2003
We present a new distributed association rule mining (D-ARM) algorithm that demonstrates superlinear speedup with the number of computing nodes. The algorithm is the first D-ARM algorithm to perform a single scan over the database. As such, its performance is unmatched by any previous algorithm. Scale-up experiments over standard synthetic benchmarks demonstrate stable run time regardless of the number of computers. Theoretical analysis reveals a tighter bound on error probability than the one shown in the corresponding sequential algorithm.
[high-performance distributed algorithm, sequential algorithm, Costs, data mining, error probability, Partitioning algorithms, Transaction databases, Data mining, Association rules, distributed association rule mining, Itemsets, distributed algorithms, very large databases, Clustering algorithms, Distributed databases, Sampling methods, D-ARM algorithm, Distributed algorithms, scale-up experiment, error statistics]
Spatial interest pixels (SIPs): useful low-level features of visual media data
Third IEEE International Conference on Data Mining
None
2003
Visual media data such as an image is the raw data representation for many important applications. The biggest challenge in using visual media data comes from the extremely high dimensionality. We present a comparative study on spatial interest pixels (SIPs), including eight-way (a novel SIP miner), Harris, and Lucas-Kanade, whose extraction is considered as an important step in reducing the dimensionality of visual media data. With extensive case studies, we have shown the usefulness of SIPs as the low-level features of visual media data. A class-preserving dimension reduction algorithm (using GSVD) is applied to further reduce the dimension of feature vectors based on SIPs. The experiments showed its superiority over PCA.
[Image recognition, Shape, data representation, visual databases, Data mining, face recognition, data structures, visual media data, Linear discriminant analysis, eight-way, Computer vision, Face recognition, Image retrieval, Harris, Computational Intelligence Society, PCA, computer graphics, Lucas-Kanade, class-preserving dimension reduction algorithm, SIP, GSVD, spatial interest pixel, Pixel, principal component analysis, Principal component analysis]
Mining plans for customer-class transformation
Third IEEE International Conference on Data Mining
None
2003
We consider the problem of mining high-utility plans from historical plan databases that can be used to transform customers from one class to other, more desirable classes. Traditional data mining algorithms are focused on finding frequent sequences. But high frequency may not imply low costs and high benefits. Traditional Markov decision process (MDP) algorithms are designed to address this issue by bringing in the concept of utility, but these algorithms are also known to be expensive to execute. We present a novel algorithm AUPlan, which automatically generates sequential plans with high utility by combining data mining and AI planning. These high-utility plans could be used to convert groups of customers from less desirable states to more desirable ones. Our algorithm adapts the Apriori algorithm by considering the concepts of plans and utilities. We show through empirical studies that planning using our integrated algorithm produces high-utility plans efficiently.
[Algorithm design and analysis, AUPlan algorithm, Costs, data mining, Turning, AI planning, Data mining, Markov decision process algorithms, Guidelines, Computer science, Learning systems, planning (artificial intelligence), Databases, customer-class transformation, decision making, high-utility plan data mining, Markov processes, Apriori algorithm, Frequency, learning (artificial intelligence), Artificial intelligence]
Validating and refining clusters via visual rendering
Third IEEE International Conference on Data Mining
None
2003
The automatic clustering algorithms are known to work well in dealing with clusters of regular shapes, e.g. compact spherical/elongated shapes, but may incur higher error rates when dealing with arbitrarily shaped clusters. Although some efforts have been devoted to addressing the problem of skewed datasets, the problem of handling clusters with irregular shapes is still in its infancy, especially in terms of dimensionality of the datasets and the precision of the clustering results considered. Not surprisingly, the statistical indices works ineffective in validating clusters of irregular shapes, too. We address the problem of clustering and validating arbitrarily shaped clusters with a visual framework (VISTA). The main idea of the VISTA approach is to capitalize on the power of visualization and interactive feedbacks to encourage domain experts to participate in the clustering revision and clustering validation process.
[Visualization, Shape, Error analysis, Humans, cluster validation, VISTA visual framework, computational geometry, Feedback, Clustering algorithms, data visualisation, statistical indices, data visualization, irregular shaped cluster, rendering (computer graphics), skewed datasets, automatic clustering algorithms, Multidimensional systems, arbitrarily shaped clusters, Statistical analysis, Educational institutions, interactive feedbacks, domain experts, visual rendering, cluster refining process, Iterative algorithms, statistical analysis]
An algorithm for the exact computation of the centroid of higher dimensional polyhedra and its application to kernel machines
Third IEEE International Conference on Data Mining
None
2003
The support vector machine (SVM) solution corresponds to the centre of the largest sphere inscribed in version space. Alternative approaches like Bayesian point machines (BPM) and analytic centre machines have suggested that the generalization performance can be further enhanced by considering other possible centres of version space like the centroid (centre of mass) or the analytic centre. We present an algorithm to compute exactly the centroid of higher dimensional polyhedra, then derive approximation algorithms to build a new learning machine whose performance is comparable to BPM. We also show that for regular kernel matrices (Gaussian kernels for example), the SVM solution can be obtained by solving a linear system of equalities.
[approximation theory, support vector machines, Laboratories, polyhedra centroid computation, learning machine, Gaussian kernel matrices, approximation algorithms, Support vector machines, support vector machine, Space technology, Bayesian methods, Voting, High performance computing, analytic centre machines, Bayesian point machines, Gaussian processes, Approximation algorithms, Performance analysis, Bayes methods, Australia, learning (artificial intelligence), Kernel]
Ensembles of cascading trees
Third IEEE International Conference on Data Mining
None
2003
We introduce a new method, called CS4, to construct committees of decision trees for classification. The method considers different top-ranked features as the root nodes of member trees. This idea is particularly suitable for dealing with high-dimensional bio-medical data as top-ranked features in this type of data usually possess similar merits for classification. To make a decision, the committee combines the power of individual trees in a weighted manner. Unlike Bagging or Boosting which uses bootstrapped training data, our method builds all the member trees of a committee using exactly the same set of training data. We have tested these ideas on UCI data sets as well as recent bio-medical data sets of gene expression or proteomic profiles that are usually described by more than 10,000 features. All the experimental results show that our method is efficient and that the classification performance are superior to C4.5 family algorithms.
[UCI data sets, CS4, Boosting, medical information systems, Gene expression, classification, decision tree, biomedical data, cascading trees, Training data, Proteomics, decision trees, Gain measurement, Decision trees, learning (artificial intelligence), Bioinformatics, Bagging, gene expression, Classification tree analysis, Testing]
Enhancing techniques for efficient topic hierarchy integration
Third IEEE International Conference on Data Mining
None
2003
Here, we study the problem of integrating documents from different sources into a comprehensive topic hierarchy. Our objective is to develop efficient techniques that improve the accuracy of traditional categorization methods by incorporating categorization information provided by data sources into categorization process. Notice that in the World-Wide Web, categorization information is often available from information sources. We present several enhancing techniques that use categorization information to enhance traditional methods such as naive Bayes and support vector machines. Experiment on collections from Openfind and Yam, and Google and Yahoo!, well-known popular Web sites in Taiwan and USA, respectively, shows that our techniques significantly improve the classification accuracy from, for example, 55% to 66% for Naive Bayes, and from 57% to 67% for SVM for the data set collected from Yam and Openfind.
[document handling, data sources, support vector machines, data set, Niobium, Support vector machines, Computer science, document integration, information sources, Naive Bayes method, Councils, Bayesian methods, World-Wide Web, Text categorization, Neural networks, Support vector machine classification, Bayes methods, categorization information, Decision trees, Web sites, Classification tree analysis]
Scalable model-based clustering by working on data summaries
Third IEEE International Conference on Data Mining
None
2003
The scalability problem in data mining involves the development of methods for handling large databases with limited computational resources. We present a two-phase scalable model-based clustering framework: first, a large data set is summed up into subclusters; Then, clusters are directly generated from the summary statistics of subclusters by a specifically designed expectation-maximization (EM) algorithm. Taking example for Gaussian mixture models, we establish a provably convergent EM algorithm, EMADS, which embodies cardinality, mean, and covariance information of each subcluster explicitly. Combining with different data summarization procedures, EMADS is used to construct two clustering systems: gEMADS and bEMADS. The experimental results demonstrate that they run several orders of magnitude faster than the classic EM algorithm with little loss of accuracy. They generate significantly better results than other model-based clustering systems using similar computational resources.
[Algorithm design and analysis, Scalability, data mining, data summarization procedures, covariance information, Data mining, Gaussian mixture model, Information systems, gEMADS, Databases, clustering system, very large databases, Clustering algorithms, expectation-maximization algorithm, EM, two-phase scalable model-based clustering, Statistics, bEMADS, Bridges, pattern clustering, Gaussian processes, covariance analysis, Iterative algorithms, Explosives, large databases, computational complexity]
Building text classifiers using positive and unlabeled examples
Third IEEE International Conference on Data Mining
None
2003
We study the problem of building text classifiers using positive and unlabeled examples. The key feature of this problem is that there is no negative example for learning. Recently, a few techniques for solving this problem were proposed in the literature. These techniques are based on the same idea, which builds a classifier in two steps. Each existing technique uses a different method for each step. We first introduce some new methods for the two steps, and perform a comprehensive evaluation of all possible combinations of methods of the two steps. We then propose a more principled approach to solving the problem based on a biased formulation of SVM, and show experimentally that it is more accurate than the existing techniques.
[Performance evaluation, text analysis, pattern classification, support vector machines, positive example, unlabeled example, SVM, Sun, Niobium, Support vector machines, Computer science, text classifier, Text categorization, Support vector machine classification, Iterative algorithms, Bayes methods, Labeling, belief networks, Biomedical engineering]
Interpretations of association rules by granular computing
Third IEEE International Conference on Data Mining
None
2003
We present interpretations for association rules. We first introduce Pawlak's method, and the corresponding algorithm of finding decision rules (a kind of association rules). We then use extended random sets to present a new algorithm of finding interesting rules. We prove that the new algorithm is faster than Pawlak's algorithm. The extended random sets are easily to include more than one criterion for determining interesting rules. We also provide two measures for dealing with uncertainties in association rules.
[Vehicle driving, data mining, random set, uncertainty handling, set theory, Association rules, Data mining, decision tables, decision rule, Road accidents, Databases, association rule interpretation, Pawlak method, Road vehicles, granular computing, Frequency, Data communication, Australia, Software engineering]
Semantic log analysis based on a user query behavior model
Third IEEE International Conference on Data Mining
None
2003
We propose a novel log analysis method to capture the semantic relations among words appearing in Web search logs. Our method focuses on the reciprocal relations among a user's intentions, stages of information need, and query behavior in seeking information via a search engine. The approach works because it is based on the assumption that a user's intentions in each query can be derived as a model on the basis of his stage of information need and query behavior, through multiple empirical observations of search logs. The user's intentions drive user to change the words in each successive queries and can thus be used to clarify the semantic relations among words. As a result, this method has the advantage of capturing the semantic relations among words without requiring either manual or natural language processing. Our experimental results indicate that semantic relations could successfully be derived from search logs, confirming that an ontology and thesaurus could be constructed automatically.
[thesauri, search engines, semantic Web, search engine, natural language processing, data mining, thesaurus, Data mining, word processing, semantic log analysis, semantic word relation, Web search logs, Internet, user query behavior model, information needs, information need, query formulation, ontology]
Inference of protein-protein interactions by unlikely profile pair
Third IEEE International Conference on Data Mining
None
2003
We note that a set of statistically "unusual" protein-profile pairs in experimentally determined database of protein-protein interactions can typify protein-protein interactions, and propose a novel method called PICUPP that sifts such protein-profile pairs using a statistical simulation. It is demonstrated that unusual Pfam and InterPro profile pairs can be extracted from the DIP database using a bootstrapping approach. We particularly illustrate that such protein-profile pairs can be used for predicting putative pairs of interacting proteins. Their prediction accuracies are around 86% and 90% when InterPro and Pfam profiles are used, respectively at 75% confidence level.
[Sequences, Biological system modeling, Genomics, data mining, InterPro profile pairs, Data mining, protein-protein interactions, Proteins, protein-profile pairs, bootstrapping approach, Databases, biology computing, protein interaction database, proteins, statistical simulation, statistical analysis, Bioinformatics, Electronics packaging, Bars, Computational biology]
CBC: clustering based text classification requiring minimal labeled data
Third IEEE International Conference on Data Mining
None
2003
Semisupervised learning methods construct classifiers using both labeled and unlabeled training data samples. While unlabeled data samples can help to improve the accuracy of trained models to certain extent, existing methods still face difficulties when labeled data is not sufficient and biased against the underlying data distribution. We present a clustering based classification (CBC) approach. Using this approach, training data, including both the labeled and unlabeled data, is first clustered with the guidance of the labeled data. Some of unlabeled data samples are then labeled based on the clusters obtained. Discriminative classifiers can subsequently be trained with the expanded labeled dataset. The effectiveness of the proposed method is justified analytically. Our experimental results demonstrated that CBC outperforms existing algorithms when the size of labeled dataset is very small.
[clustering based text classification, pattern classification, minimal labeled data, support vector machines, semisupervised learning, discriminative classifiers, Classification algorithms, maximum likelihood estimation, Support vector machines, Computer science, pattern clustering, Text categorization, Supervised learning, Asia, Training data, Support vector machine classification, Clustering algorithms, transductive support vector machines, Semisupervised learning, learning (artificial intelligence)]
MaPle: a fast algorithm for maximal pattern-based clustering
Third IEEE International Conference on Data Mining
None
2003
Pattern-based clustering is important in many applications, such as DNA micro-array data analysis, automatic recommendation systems and target marketing systems. However, pattern-based clustering in large databases is challenging. On the one hand, there can be a huge number of clusters and many of them can be redundant and thus make the pattern-based clustering ineffective. On the other hand, the previous proposed methods may not be efficient or scalable in mining large databases. We study the problem of maximal pattern-based clustering. Redundant clusters are avoided completely by mining only the maximal pattern-based clusters. MaPle, an efficient and scalable mining algorithm is developed. It conducts a depth-first, divide-and-conquer search and prunes unnecessary branches smartly. Our extensive performance study on both synthetic data sets and real data sets shows that maximal pattern-based clustering is effective. It reduces the number of clusters substantially. Moreover, MaPle is more efficient and scalable than the previously proposed pattern-based clustering methods in mining large databases.
[MaPle mining algorithm, divide and conquer methods, Clustering methods, real data set, data mining, redundant cluster, Data mining, optimisation, Databases, very large databases, Clustering algorithms, divide-and-conquer search, Motion pictures, search problems, large database mining, Data analysis, synthetic data set, Gene expression, pattern clustering, DNA micro-array data analysis, automatic recommendation system, DNA, target marketing system, Concrete, maximal pattern-based clustering, statistical analysis]
Text mining for a clear picture of defect reports: a praxis report
Third IEEE International Conference on Data Mining
None
2003
We applied the text mining categorization technology, in the publicly available, IBM Enterprise Information Portal V8.1 to more than 15,000 customer reported, product problem records. We used a proven software quality category set to categorize these problem records into different areas of interest. Our intent was to develop a clear picture of potential areas for quality improvement in each of the software products reviewed, and to provide this information to development's management. We present the benefits that can be gained from categorizing problem records, as well as the limitations.
[Text mining, document handling, Laboratories, software product review, data mining, portals, software quality category set, Information management, software quality, Software development management, Technology management, quality management, Customer satisfaction, Software quality, Silicon, text mining, problem record categorization, Portals, Quality management, IBM Enterprise Information Portal V8.1]
Learning Bayesian networks from incomplete data based on EMI method
Third IEEE International Conference on Data Mining
None
2003
Currently, there are few efficient methods in practice for learning Bayesian networks from incomplete data, which affects their use in real world data mining applications. We present a general-duty method that estimates the (conditional) mutual information directly from incomplete datasets, EMI. EMI starts by computing the interval estimates of a joint probability of a variable set, which are obtained from the possible completions of the incomplete dataset. And then computes a point estimate via a convex combination of the extreme points, with weights depending on the assumed pattern of missing data. Finally, based on these point estimates, EMI gets the estimated (conditional) mutual information. We also apply EMI to the dependency analysis based learning algorithm by J. Cheng so as to efficiently learn BNs with incomplete data. The experimental results on Asia and Alarm networks show that EMI based algorithm is much more efficient than two search & scoring based algorithms, SEM and EM-EA algorithms. In terms of accuracy, EMI based algorithm is more accurate than SEM algorithm, and comparable with EM-EA algorithm.
[Algorithm design and analysis, dependency analysis based learning algorithm, estimation theory, Alarm network, EM-EA algorithm, data mining, Probability distribution, Data mining, Convergence, general-duty method, SEM algorithm, variable set, conditional mutual information estimation, Electromagnetic interference, joint probability, real world data mining application, belief networks, learning (artificial intelligence), EMI method, point estimates, probability, Bayesian network learning, Application software, Computer science, search &amp; scoring based algorithm, Bayesian methods, directed graphs, Sampling methods, Mutual information, incomplete data]
Information theoretic clustering of sparse cooccurrence data
Third IEEE International Conference on Data Mining
None
2003
A novel approach to clustering cooccurrence data poses it as an optimization problem in information theory which minimizes the resulting loss in mutual information. A divisive clustering algorithm that monotonically reduces this loss function was recently proposed. We show that sparse high-dimensional data presents special challenges which can result in the algorithm getting stuck at poor local minima. We propose two solutions to this problem: (a) a "prior" to overcome infinite relative entropy values as in the supervised Naive Bayes algorithm, and (b) local search to escape local minima. Finally, we combine these solutions to get a robust algorithm that is computationally efficient. We present experimental results to show that the proposed method is effective in clustering document collections and outperform previous information-theoretic clustering approaches.
[divisive clustering algorithm, supervised Naive Bayes algorithm, document clustering, sparse high-dimensional cooccurrence data, Entropy, Probability distribution, Loss measurement, Unsupervised learning, local minima, optimisation, pattern clustering, Clustering algorithms, Character generation, Robustness, Random variables, information theory, Bayes methods, learning (artificial intelligence), Mutual information, Information theory]
Sentiment analyzer: extracting sentiments about a given topic using natural language processing techniques
Third IEEE International Conference on Data Mining
None
2003
We present sentiment analyzer (SA) that extracts sentiment (or opinion) about a subject from online text documents. Instead of classifying the sentiment of an entire document about a subject, SA detects all references to the given subject, and determines sentiment in each of the references using natural language processing (NLP) techniques. Our sentiment analysis consists of 1) a topic specific feature term extraction, 2) sentiment extraction, and 3) (subject, sentiment) association by relationship analysis. SA utilizes two linguistic resources for the analysis: the sentiment lexicon and the sentiment pattern database. The performance of the algorithms was verified on online product review articles ("digital camera" and "music" reviews), and more general documents including general Webpages and news articles.
[text analysis, feature term extraction, Text analysis, natural language processing, computational linguistics, sentiment pattern database, Spatial databases, Marketing management, Data mining, Computer science, feature extraction, Web pages, Feature extraction, natural languages, Natural language processing, Product development, sentiment extraction, Internet, sentiment lexicon, Pattern analysis, sentiment analyzer, online text documents]
Probabilistic user behavior models
Third IEEE International Conference on Data Mining
None
2003
We present a mixture model based approach for learning individualized behavior models for the Web users. We investigate the use of maximum entropy and Markov mixture models for generating probabilistic behavior models. We first build a global behavior model for the entire population and then personalize this global model for the existing users by assigning each user individual component weights for the mixture model. We then use these individual weights to group the users into behavior model clusters. We show that the clusters generated in this manner are interpretable and able to represent dominant behavior patterns. We conduct offline experiments on around two months worth of data from CiteSeer, an online digital library for computer science research papers currently storing more than 470,000 documents. We show that both maximum entropy and Markov based personal user behavior models are strong predictive models. We also show that maximum entropy based mixture model outperforms Markov mixture models in recognizing complex user behavior patterns.
[Web users, Markov mixture models, data mining, Predictive models, maximum entropy, probabilistic user behavior models, Entropy, Pattern recognition, Data mining, Association rules, CiteSeer data, user modelling, global model, Computer science, Software libraries, Consumer behavior, Collaboration, online digital library, Markov processes, maximum entropy methods, Internet, statistical analysis, Pattern analysis]
Probabilistic noise identification and data cleaning
Third IEEE International Conference on Data Mining
None
2003
Real world data is never as perfect as we would like it to be and can often suffer from corruptions that may impact interpretations of the data, models created from the data, and decisions made based on the data. One approach to this problem is to identify and remove records that contain corruptions. Unfortunately, if only certain fields in a record have been corrupted then usable, uncorrupted data will be lost. We present LENS, an approach for identifying corrupted fields and using the remaining noncorrupted fields for subsequent modeling and analysis. Our approach uses the data to learn a probabilistic model containing three components: a generative model of the clean records, a generative model of the noise values, and a probabilistic model of the corruption process. We provide an algorithm for the unsupervised discovery of such models and empirically evaluate both its performance at detecting corrupted fields and, as one example application, the resulting improvement this gives to a classifier.
[corruption process, data mining, probability, probabilistic noise identification, Cleaning, Data mining, conformance testing, probabilistic model, Gaussian noise, data cleaning, corrupted field identifying, noise value, generative model]
A feature selection framework for text filtering
Third IEEE International Conference on Data Mining
None
2003
We present a new framework for local feature selection in text filtering. In this framework, a feature set is constructed per category by first selecting a set of terms highly indicative of membership (positive set) and another set of terms highly indicative of nonmembership (negative set), and then combining these two sets. This feature selection framework not only unifies several standard feature selection methods, but also facilitates the proposal of a new method that optimally combines the positive and negative sets. The experimental comparison between the proposed method and standard methods was conducted on six feature selection metrics: chi-square, correlation coefficient, odds ratio, GSS coefficient and two proposed variants of odds ratio and GSS coefficient: OR-square and GSS-square respectively. The results show that the proposed feature selection method improves text filtering performance.
[chi-square metric, text analysis, data mining, feature set, text filtering, Frequency measurement, Information filtering, feature selection method, Proposals, Data mining, Computer science, Feedback, feature extraction, Chromium, Gain measurement, Information filters, statistical analysis, GSS coefficient, correlation coefficient, Mutual information, correlation methods]
Tractable group detection on large link data sets
Third IEEE International Conference on Data Mining
None
2003
Discovering underlying structure from co-occurrence data is an important task in a variety of fields, including: insurance, intelligence, criminal investigation, epidemiology, human resources, and marketing. Previously Kubica et al. presented the group detection algorithm (GDA) - an algorithm for finding underlying groupings of entities from co-occurrence data. This algorithm is based on a probabilistic generative model and produces coherent groups that are consistent with prior knowledge. Unfortunately, the optimization used in GDA is slow, potentially making it infeasible for many large data sets. To this end, we present k-groups - an algorithm that uses an approach similar to that of k-means to significantly accelerate the discovery of groups while retaining GDA's probabilistic model. We compare the performance of GDA and k-groups on a variety of data, showing that k-groups' sacrifice in solution quality is significantly offset by its increase in speed.
[k-group algorithm, probabilistic generative model, co-occurrence data, very large databases, data mining, probability, large link data set, group detection algorithm, Data mining, learning (artificial intelligence), belief networks, maximum likelihood estimation]
General MC: estimating boundary of positive class from small positive data
Third IEEE International Conference on Data Mining
None
2003
Single-class classification (SCC) seeks to distinguish one class of data from the universal set of multiple classes. We propose a SCC method called general MC that estimates an accurate classification boundary of positive class from small positive data using the distribution of unlabeled data. Our theoretical and empirical analyses show that, as long as the distribution of unlabeled data is not highly skewed in the feature space, general MC significantly outperforms other recent SCC methods when the positive data set is highly under-sampled.
[Algorithm design and analysis, pattern classification, positive data set, support vector machines, feature space, Resumes, Functional analysis, character recognition, empirical analyses, Convergence, Computer science, Image analysis, Image databases, Training data, Web pages, classification boundary, Performance analysis, unlabeled data, learning (artificial intelligence), statistical analysis, general MC, single-class classification]
Facilitating fuzzy association rules mining by using multi-objective genetic algorithms for automated clustering
Third IEEE International Conference on Data Mining
None
2003
We propose an automated clustering method based on multiobjective genetic algorithms (GA); the aim of this method is to automatically cluster values of a given quantitative attribute to obtain large number of large itemsets in low duration (time). We compare the proposed multi-objective GA-based approach with CURE-based approach. In addition to the autonomous specification of fuzzy sets, experimental results showed that the proposed automated clustering exhibits good performance over CURE-based approach in terms of runtime as well as the number of large itemsets and interesting association rules.
[Clustering methods, fuzzy set theory, data mining, genetic algorithms, Association rules, Data mining, fuzzy association rule mining, Genetic algorithms, Computer science, Fuzzy sets, Runtime, Itemsets, pattern clustering, Clustering algorithms, automated clustering, CURE-based approach, Genetic engineering, multiobjective genetic algorithm]
Frequent sub-structure-based approaches for classifying chemical compounds
Third IEEE International Conference on Data Mining
None
2003
We study the problem of classifying chemical compound datasets. We present a substructure-based classification algorithm that decouples the substructure discovery process from the classification model construction and uses frequent subgraph discovery algorithms to find all topological and geometric substructures present in the dataset. The advantage of our approach is that during classification model construction, all relevant substructures are available allowing the classifier to intelligently select the most discriminating ones. The computational scalability is ensured by the use of highly efficient frequent subgraph discovery algorithms coupled with aggressive feature selection. Our experimental evaluation on eight different classification problems shows that our approach is computationally scalable and on the average, outperforms existing schemes by 10% to 35%.
[Drugs, Solid modeling, pattern classification, support vector machines, subgraph discovery algorithm, Scalability, graph theory, chemical compound dataset classification, High temperature superconductors, Classification algorithms, Chemical compounds, substructure discovery process, Computer science, Computer displays, Biology computing, geometric substructure, chemical structure, Computational intelligence]
Mining the Web to discover the meanings of an ambiguous word
Third IEEE International Conference on Data Mining
None
2003
In information retrieval and text mining, information on word senses is usually taken from dictionaries or lexical databases that have been prepared by lexicographers. We propose an automatic method for word sense induction, i.e. for the discovery of a set of sense descriptors to a given ambiguous word. The approach is based on the statistics of word co-occurrence as derived from Web pages. The underlying assumption is that the senses of an ambiguous word are best described by terms that, although bearing a strong association to this word, are mutually exclusive, i.e. whose association strength within the retrieved Web pages is as weak as possible. Measuring association strength is based upon a novel confidence gain approach that relates the observed co-occurrence frequency for two sense descriptor candidates to an average co-occurrence frequency for pairs of arbitrary words. The proposed approach is fully unsupervised and takes into account the contemporary meanings of words, as reflected in texts from the Internet. Our results are evaluated using a list of ambiguous words commonly referred to in the literature.
[Text mining, text analysis, Dictionaries, Natural languages, computational linguistics, data mining, information retrieval, ambiguous word meanings, word co-occurrence, Information retrieval, Statistics, contemporary word meanings, Databases, confidence gain approach, Web pages, word sense induction, Gain measurement, Frequency, Particle measurements, natural languages, text mining, Internet]
MPIS: maximal-profit item selection with cross-selling considerations
Third IEEE International Conference on Data Mining
None
2003
In the literature of data mining, many different algorithms for association rule mining have been proposed. However, there is relatively little study on how association rules can aid in more specific targets. One of the applications for association rules - maximal-profit item selection with cross-selling effect (MPIS) problem - is investigated. The problem is about selecting a subset of items, which can give the maximal profit with the consideration of cross-selling. We prove that a simple version of this problem is NP-hard. We propose a new approach to the problem with the consideration of the loss rule - a kind of association rule to model the cross-selling effect. We show that the problem can be transformed to a quadratic programming problem. In case quadratic programming is not applicable, we also propose a heuristic approach. Experiments are conducted to show that both of the proposed methods are highly effective and efficient.
[profitability, quadratic programming problem, Decision making, data mining, Companies, Data engineering, heuristic method, Association rules, Data mining, Quadratic programming, History, Application software, quadratic programming, Computer science, cross-selling considerations, heuristic programming, NP-hard problem, maximal-profit item selection, Marketing and sales, association rule mining algorithms]
Efficient nonlinear dimension reduction for clustered data using kernel functions
Third IEEE International Conference on Data Mining
None
2003
We propose a nonlinear feature extraction method which is based on centroids and kernel functions. The dimension reducing nonlinear transformation is obtained by implicitly mapping the input data into a feature space using a kernel function, and then finding a linear mapping based on an orthonormal basis of centroids in the feature space that maximally separates the between-class relationship. The proposed method utilizes an efficient algorithm to compute an orthonormal basis of centroids in the feature space transformed by a kernel function and achieves dramatic computational savings. The experimental results demonstrate that our method is capable of extracting nonlinear features effectively so that competitive performance of classification can be obtained in the reduced dimensional space.
[Symmetric matrices, Data analysis, data analysis, support vector machines, Noise reduction, data cluster, Data engineering, nonlinear dimension reduction method, Data mining, kernel function, linear mapping, Computer science, feature extraction, self-organising feature maps, Feature extraction, Linear discriminant analysis, linear discriminant analysis, Kernel, principal component analysis, Principal component analysis]
A fast algorithm for computing hypergraph transversals and its application in mining emerging patterns
Third IEEE International Conference on Data Mining
None
2003
Computing the minimal transversals of a hypergraph is an important problem in computer science that has significant applications in data mining. We present a new algorithm for computing hypergraph transversals and highlight their close connection to an important class of patterns known as emerging patterns. We evaluate our technique on a number of large datasets and show that it outperforms previous approaches by a factor of 9-29 times.
[Software algorithms, graph theory, data mining, Mathematics, Partitioning algorithms, Data mining, Application software, Computer science, Itemsets, computer science, very large databases, Frequency, minimisation, minimal hypergraph transversals, Software engineering, computational complexity, emerging patterns]
Center-based indexing for nearest neighbors search
Third IEEE International Conference on Data Mining
None
2003
We address the problem of indexing data for the k nearest neighbors (k-nn) search. We present a tree-based top-down indexing method that uses an iterative k-means algorithm for tree node splitting and combines three different search pruning criteria from BST, GHT and GNAT into one. The experiments show that the presented indexing tree accelerates the k-nn searching up to several thousands times in case of large data sets.
[iterative k-means algorithm, Binary search trees, Vectors, K nearest neighbors search, search pruning criteria, Data mining, tree searching, Nearest neighbor searches, center-based data indexing, Databases, database indexing, tree-based top-down indexing, very large databases, tree node splitting, data sets, Iterative algorithms, tree data structures, Acceleration, Iterative methods, Informatics, Indexing]
Efficient multidimensional quantitative hypotheses generation
Third IEEE International Conference on Data Mining
None
2003
Finding local interrelations (hypotheses) among attributes within very large databases of high dimensionality is an acute problem for many databases and data mining applications. These include, dependency modeling, clustering large databases, correlation and link analysis. Traditional statistical methods are concerned with the corroboration of (a set of) hypotheses on a given body of data. Testing all of the hypotheses that can be generated from a database with millions of records and dozens of fields is clearly infeasible. Generating, on the other hand, a set of the most "promising" hypotheses (to be corroborated) requires much intuition and ingenuity. We present an efficient method for ranking the multidimensional hypotheses using image processing of data visualization. In the heart of the method lies the use of visualization techniques and image processing ideas to rank subsets of attributes according to the relation between them in the databases. Some of the scalability issues are solved by concise generalized histograms and by using an efficient on-line computation of clustering around a median with only five additional memory words. In addition to presenting our algorithmic methodology, we demonstrate its efficiency and performance by applying it to real census data sets, as well as synthetic data sets.
[Heart, image processing, Multidimensional systems, Statistical analysis, Image processing, Scalability, data mining, very large database, Visual databases, Data mining, multidimensional quantitative hypotheses generation, Image databases, dependency modeling, very large databases, Data visualization, data visualisation, data visualization, statistical analysis, Testing]
Semantic role parsing: adding semantic structure to unstructured text
Third IEEE International Conference on Data Mining
None
2003
There is an ever-growing need to add structure in the form of semantic markup to the huge amounts of unstructured text data now available. We present the technique of shallow semantic parsing, the process of assigning a simple WHO did WHAT to WHOM, etc., structure to sentences in text, as a useful tool in achieving this goal. We formulate the semantic parsing problem as a classification problem using support vector machines. Using a hand-labeled training set and a set of features drawn from earlier work together with some feature enhancements, we demonstrate a system that performs better than all other published results on shallow semantic parsing.
[pattern classification, text analysis, unstructured text data, support vector machines, Natural languages, computational linguistics, Data mining, Support vector machines, classification problem, Waste materials, grammars, hand-labeled training set, Support vector machine classification, feature enhancements, Tagging, shallow semantic parsing, learning (artificial intelligence), Contracts, Testing, Classification tree analysis]
Comparing pure parallel ensemble creation techniques against bagging
Third IEEE International Conference on Data Mining
None
2003
We experimentally evaluate randomization-based approaches to creating an ensemble of decision-tree classifiers. Unlike methods related to boosting, all of the eight approaches considered here create each classifier in an ensemble independently of the other classifiers. Experiments were performed on 28 publicly available datasets, using C4.5 release 8 as the base classifier. While each of the other seven approaches has some strengths, we find that none of them is consistently more accurate than standard bagging when tested for statistical significance.
[pattern classification, training datasets, boosting, bagging, random processes, Boosting, Data mining, Computer science, decision-tree classifier, randomization, parallel ensemble creation techniques, decision trees, Decision trees, learning (artificial intelligence), statistical testing, Bagging, Testing]
Reliable detection of episodes in event sequences
Third IEEE International Conference on Data Mining
None
2003
Suppose one wants to detect "bad" or "suspicious" subsequences in event sequences. Whether an observed pattern of activity (in the form of a particular subsequence) is significant and should be a cause for alarm, depends on how likely it is to occur fortuitously. A long enough sequence of observed events will almost certainly contain any subsequence, and setting thresholds for alarm is an important issue in a monitoring system that seeks to avoid false alarms. Suppose a long sequence T of observed events contains a suspicious subsequence pattern S within it, where the suspicious subsequence S consists of m events and spans a window of size w within T. We address the fundamental problem: is a certain number of occurrences of a particular subsequence unlikely to be fortuitous (i.e., indicative of suspicious activity)? If the probability of fortuitous occurrences is high and an automated monitoring system flags it as suspicious anyway, then such a system will suffer from generating too many false alarms. We quantify the probability of such an S occurring in T within a window of size w, the number of distinct windows containing S as a subsequence, the expected number of such occurrences, its variance, and establishes its limiting distribution that allows to set up an alarm threshold so that the probability of false alarms is very small. We report on experiments confirming the theory and showing that we can detect bad subsequences with low false alarm rate.
[event sequence, Sequences, pattern matching, Event detection, Computerized monitoring, probability, reliability, Transaction databases, Data mining, security of data, monitoring system, Information security, Intrusion detection, subsequence pattern detection, alarm threshold, National security, Contracts]
Efficient mining of frequent subgraphs in the presence of isomorphism
Third IEEE International Conference on Data Mining
None
2003
Frequent subgraph mining is an active research topic in the data mining community. A graph is a general model to represent data and has been used in many domains like cheminformatics and bioinformatics. Mining patterns from graph databases is challenging since graph related operations, such as subgraph testing, generally have higher time complexity than the corresponding operations on itemsets, sequences, and trees, which have been studied extensively. We propose a novel frequent subgraph mining algorithm: FFSM, which employs a vertical search scheme within an algebraic graph framework we have developed to reduce the number of redundant candidates proposed. Our empirical study on synthetic and real datasets demonstrates that FFSM achieves a substantial performance gain over the current start-of-the-art subgraph mining algorithm gSpan.
[Tree data structures, data representation, graph theory, subgraph testing, data mining, gSpan mining algorithm, Performance gain, visual databases, Data mining, isomorphism, Computer science, Databases, Tree graphs, Itemsets, fast frequent subgraph mining, FFSM, Bioinformatics, search problems, Testing, Indexing, graph databases]
Detecting patterns of change using enhanced parallel coordinates visualization
Third IEEE International Conference on Data Mining
None
2003
Analyzing data to find trends, correlations, and stable patterns is an important problem for many industrial applications. We propose a new technique based on parallel coordinates visualization. Previous work on parallel coordinates method has shown that they are effective only when variables that are correlated and/or show similar patterns are displayed adjacently. Although current parallel coordinates tools allow the user to manually rearrange the order of variables, this process is very time-consuming when the number of variables is large. Automated assistance is needed. We propose an edit-distance based technique to rearrange variables so that interesting patterns can be easily detected. Our system, V-Miner, includes both automated methods for visualizing common patterns and a query tool that enables the user to describe specific target patterns to be mined/displayed by the system. Following an overview of the system, a case study is presented to explain how Motorola engineers have used V-Miner to identify significant patterns in their product test and design data.
[Process design, Performance evaluation, System testing, Data analysis, product test, change pattern detection, data mining, Data engineering, Mobile handsets, Product design, industrial application, Design engineering, product design, query tool, parallel coordinates visualization, Feedback, Data visualization, data visualisation, edit-distance based technique, V-Miner system, design data, pattern recognition]
Integrating customer value considerations into predictive modeling
Third IEEE International Conference on Data Mining
None
2003
The success of prediction models for business purposes should not be measured by their accuracy only. Their evaluation should also take into account the higher importance of precise prediction for "valuable" customers. We illustrate this idea through the example of churn modelling in telecommunications, where it is obviously much more important to identify potential churn among valuable customers. We discuss, both theoretically and empirically, the optimal use of "customer value" data in the model training, model evaluation and scoring stages. Our main conclusion is that a nontrivial approach of using "decayed" value-weights for training is usually preferable to the two obvious approaches of either using nondecayed customer values as weights or ignoring them.
[data analysis, prediction model, customer value data, data mining, regression analysis, Predictive models, Data mining, telecommunication, telecommunication computing, customer relationship management, telecommunications, optimisation, customer value consideration integration, predictive modeling, learning (artificial intelligence), churn modelling, nondecayed customer value data]
Direct interesting rule generation
Third IEEE International Conference on Data Mining
None
2003
An association rule generation algorithm usually generates too many rules including a lot of uninteresting ones. Many interestingness criteria are proposed to prune those uninteresting rules. However, they work in post-pruning process and hence do not improve the rule generation efficiency. We discuss properties of informative rule set and conclude that the informative rule set includes all interesting rules measured by many commonly used interestingness criteria, and that rules excluded by the informative rule set are forwardly prunable, i.e. they can be removed in the rule generation process instead of post pruning. Based on these properties, we propose a direct interesting rule generation algorithm, DIG, to directly generate interesting rules defined by any of 12 interestingness criteria. We further show experimentally that DIG is faster and uses less memory than Apriori.
[informative rule set, DIG, data mining, Mathematics, post-pruning process, Apriori, Association rules, Data mining, direct interesting rule generation algorithm, Computer science, Itemsets, rule generation process, learning (artificial intelligence), interestingness criteria, computational complexity]
Predicting distribution of a new forest disease using one-class SVMs
Third IEEE International Conference on Data Mining
None
2003
In California, a newly discovered virulent pathogen (Phytophthora ramorum) has killed thousands of native oak trees. Mapping the potential distribution of the pathogen is essential for decision makers to assess the risk of the pathogen and aid in preventing its further spread. Most methods used to map potential ranges of species (e.g. multivariate or logistic regression) require both presence and absence data, the latter of which is not always feasibly collected. We present the one-class support vector machine (SVM) to predict the potential distribution of sudden oak death in California. The model was developed using presence data collected throughout the state, and tested for accuracy using a 5-fold cross-validation approach. The model performed well, and provided 91% predicted accuracy. We believe one-class SVM when coupled with geographical information systems (GIS) become a very useful method to deal with presence-only data in ecological analysis over a range of scales.
[Pathogens, ecological analysis, Geographic Information Systems, geographical information systems, support vector machines, Biological system modeling, oak trees, one-class SVM, Predictive models, geographic information systems, statistical distributions, cross-validation approach, Diseases, Information systems, Support vector machines, Accuracy, ecology, support vector machine, California forest disease, virulent pathogen, potential distribution, forestry, Logistics, Testing]
On precision and recall of multi-attribute data extraction from semistructured sources
Third IEEE International Conference on Data Mining
None
2003
Machine learning techniques for data extraction from semistructured sources exhibit different precision and recall characteristics. However to date the formal relationship between learning algorithms and their impact on these two metrics remains unexplored. We propose a formalization of precision and recall of extraction and investigates the complexity-theoretic aspects of learning algorithms for multiattribute data extraction based on this formalism. We show that there is a tradeoff between precision/recall of extraction and computational efficiency and present experimental results to demonstrate the practical utility of these concepts in designing scalable data extraction algorithms for improving recall without compromising on precision.
[semistructured sources, Machine learning algorithms, data mining, Data engineering, complexity-theoretic aspects, Data mining, Computer science, Animals, Hospitals, Web pages, Machine learning, multiattribute data extraction, Computational efficiency, Internet, Labeling, learning (artificial intelligence), machine learning algorithms, computational complexity]
Towards simple, easy-to-understand, yet accurate classifiers
Third IEEE International Conference on Data Mining
None
2003
We design a method for weighting linear support vector machine classifiers or random hyperplanes, to obtain classifiers whose accuracy is comparable to the accuracy of a nonlinear support vector machine classifier, and whose results can be readily visualized. We conduct a simulation study to examine how our weighted linear classifiers behave in the presence of known structure. The results show that the weighted linear classifiers might perform well compared to the nonlinear support vector machine classifiers, while they are more readily interpretable than the nonlinear classifiers.
[support vector machines, Design methodology, Laboratories, probability, digital simulation, random hyperplanes, weighted linear classifiers, Support vector machines, Computer science, Computer displays, Support vector machine classification, Data visualization, data visualisation, linear support vector machine classifiers, nonlinear support vector machine classifier, Space exploration, statistical analysis, Artificial intelligence, Kernel]
Evolutionary Gabor filter optimization with application to vehicle detection
Third IEEE International Conference on Data Mining
None
2003
Despite the considerable amount of research work on the application of Gabor filters in pattern classification, their design and selection have been mostly done on a trial and error basis. Existing techniques are either only suitable for a small number of filters or less problem-oriented. A systematic and general evolutionary Gabor filter optimization (EGFO) approach that yields a more optimal, problem-specific, set of filters is proposed in this study. The EGFO approach unifies filter design with filter selection by integrating genetic algorithms (GAs) with an incremental clustering approach. Specifically, filter design is performed using GAs, a global optimization approach that encodes the parameters of the Gabor filters in a chromosome and uses genetic operators to optimize them. Filter selection is performed by grouping together filters having similar characteristics (i.e., similar parameters) using incremental clustering in the parameter space. Each group of filters is represented by a single filter whose parameters correspond to the average parameters of the filters in the group. This step eliminates redundant filters, leading to a compact, optimized set of filters. The average filters are evaluated using an application-oriented fitness criterion based on support vector machines (SVMs). To demonstrate the effectiveness of the proposed framework, we have considered the challenging problem of vehicle detection from gray-scale images. Our experimental results illustrate that the set of Gabor filters, specifically optimized for the problem of vehicle detection, yield better performance than using traditional filter banks.
[Algorithm design and analysis, average filter, Gray-scale, gray-scale image, object detection, EGFO, SVM, Biological cells, Design optimization, Genetic algorithms, genetic algorithm, incremental clustering approach, Vehicle detection, feature extraction, image segmentation, Filter bank, GA, Gabor filters, pattern classification, support vector machines, filtering theory, genetic algorithms, global optimization approach, Support vector machines, vehicle detection, redundant filter elimination, support vector machine, Pattern classification, Gabor filter optimization, computer vision, traditional filter bank, automated highways, application-oriented fitness criterion]
Indexing and mining free trees
Third IEEE International Conference on Data Mining
None
2003
Tree structures are used extensively in domains such as computational biology, pattern recognition, computer networks, and so on. We present an indexing technique for free trees and apply this indexing technique to the problem of mining frequent subtrees. We first define a novel representation, the canonical form, for rooted trees and extend the definition to free trees. We also introduce another concept, the canonical string, as a simpler representation for free trees in their canonical forms. We then apply our tree indexing technique to the frequent subtree mining problem and present FreeTreeMiner, a computationally efficient algorithm that discovers all frequently occurring subtrees in a database of free trees. We study the performance and the scalability of our algorithms through extensive experiments based on both synthetic data and datasets from two real applications: a dataset of chemical compounds and a dataset of Internet multicast trees.
[Tree data structures, canonical representation, Scalability, data mining, trees (mathematics), Internet multicast trees dataset, Pattern recognition, subtrees mining, FreeTreeMiner algorithm, Chemical compounds, rooted trees, Multicast algorithms, Databases, database indexing, tree structures, free tree indexing technique, chemical compound dataset, Computer networks, Internet, tree data structures, Indexing, Computational biology]
Learning rules for anomaly detection of hostile network traffic
Third IEEE International Conference on Data Mining
None
2003
We introduce an algorithm called LERAD that learns rules for finding rare events in nominal time-series data with long range dependencies. We use LERAD to find anomalies in network packets and TCP sessions to detect novel intrusions. We evaluated LERAD on the 1999 DARPA/Lincoln Laboratory intrusion detection evaluation data set and on traffic collected in a university departmental server environment.
[TCP, Protocols, Event detection, Telecommunication traffic, hostile network traffic anomaly detection, time series, telecommunication computing, Network servers, File systems, Operating systems, transport protocols, Intrusion detection, knowledge based systems, LERAD algorithm, learning (artificial intelligence), Computer security, network packet, Viruses (medical), telecommunication traffic, Testing]
Model-based clustering with soft balancing
Third IEEE International Conference on Data Mining
None
2003
Balanced clustering algorithms can be useful in a variety of applications and have recently attracted increasing research interest. Most recent work, however, addressed only hard balancing by constraining each cluster to have equal or a certain minimum number of data objects. We provide a soft balancing strategy built upon a soft mixture-of-models clustering framework. This strategy constrains the sum of posterior probabilities of object membership for each cluster to be equal and thus balances the expected number of data objects in each cluster. We first derive soft model-based clustering from an information-theoretic viewpoint and then show that the proposed balanced clustering can be parameterized by a temperature parameter that controls the softness of clustering as well as that of balancing. As the temperature decreases, the resulting partitioning becomes more and more balanced. In the limit, when temperature becomes zero, the balancing becomes hard and the actual partitioning becomes perfectly balanced. The effectiveness of the proposed soft balanced clustering algorithm is demonstrated on both synthetic and real text data.
[posterior probabilities, Clustering methods, data mining, Data mining, maximum likelihood estimation, soft model-based clustering, Databases, Clustering algorithms, soft balanced clustering algorithms, information-theoretic viewpoint, graph partitioning, cluster constraints, object membership, Maximum likelihood estimation, hard balancing, data objects, probability, Partitioning algorithms, Application software, Computer science, pattern clustering, temperature parameter, Temperature control, Indexing, computational complexity]
Visualization of rule's similarity using multidimensional scaling
Third IEEE International Conference on Data Mining
None
2003
One of the most important problems with rule induction methods is that it is very difficult for domain experts to check millions of rules generated from large datasets. The discovery from these rules requires deep interpretation from domain knowledge. Although several solutions have been proposed in the studies on data mining and knowledge discovery, these studies are not focused on similarities between rules obtained. When one rule r/sub 1/ has reasonable features and the other rule r/sub 2/ with high similarity to r/sub 1/ includes unexpected factors, the relations between these rules will become a trigger to the discovery of knowledge. We propose a visualization approach to show the similar relations between rules based on multidimensional scaling, which assign a two-dimensional cartesian coordinate to each data point from the information about similarities between this data and others data. We evaluated this method on two medical data sets, whose experimental results show that knowledge useful for domain experts could be found.
[domain knowledge, Multidimensional systems, Liver diseases, Induction generators, Biomedical informatics, data mining, data point, medical data set, rule induction method, medical information systems, knowledge discovery, Bone diseases, Data mining, Microorganisms, medical expert systems, very large databases, Data visualization, data visualisation, large dataset, Cities and towns, Diabetes, multidimensional scaling, two-dimensional cartesian coordinate, rule similarity visualization]
CoMine: efficient mining of correlated patterns
Third IEEE International Conference on Data Mining
None
2003
Association rule mining often generates a huge number of rules, but a majority of them either are redundant or do not reflect the true correlation relationship among data objects. We re-examine this problem and show that two interesting measures, all-confidence (denoted as /spl alpha/) and coherence (denoted as /spl gamma/), both disclose genuine correlation relationships and can be computed efficiently. Moreover, we propose two interesting algorithms, CoMine(/spl alpha/) and CoMine(/spl gamma/), based on extensions of a pattern-growth methodology. Our performance study shows that the CoMine algorithms have high performance in comparison with their Apriori-based counterpart algorithms.
[correlated pattern mining, data mining, disclose genuine correlation relationship, Apriori-based counterpart algorithm, pattern-growth methodology, Data mining, statistical analysis, correlation methods, association rule mining, CoMine algorithm]
Model stability: a key factor in determining whether an algorithm produces an optimal model from a matching distribution
Third IEEE International Conference on Data Mining
None
2003
We investigate the factors leading to producing suboptimal models when training and test class distributions (or misclassification costs) are matched. Our result shows that model stability plays a key role in determining whether the algorithm produces an optimal model from a matching distribution (cost). The performance difference between a model trained from the matching distribution (cost) and the optimal model generally increases as the degree of model stability decreases. The practical implication of our result is that one should only follow the conventional wisdom of using a training class distribution (cost) that matches the test class distribution (cost) to train a classifier if the learning algorithm is known to be stable.
[pattern classification, learning algorithm, Terminology, optimal model, training class distribution, Distributed computing, Information technology, model stability, test class distribution, matching distribution, Bayesian methods, Stability criteria, Optimal control, decision trees, Cost function, Bayes methods, Decision trees, Size control, learning (artificial intelligence), Testing]
Mining high utility itemsets
Third IEEE International Conference on Data Mining
None
2003
Traditional association rule mining algorithms only generate a large number of highly frequent rules, but these rules do not provide useful answers for what the high utility rules are. We develop a novel idea of top-K objective-directed data mining, which focuses on mining the top-K high utility closed patterns that directly support a given business objective. To association mining, we add the concept of utility to capture highly desirable statistical patterns and present a level-wise item-set mining algorithm. With both positive and negative utilities, the antimonotone pruning strategy in Apriori algorithm no longer holds. In response, we develop a new pruning strategy based on utilities that allow pruning of low utility itemsets to be done by means of a weaker but antimonotonic condition. Our experimental results show that our algorithm does not require a user specified minimum utility and hence is effective in practice.
[high utility itemset, top-K objective-directed data mining, Itemsets, statistical pattern, antimonotone pruning strategy, very large databases, data mining, probability, Apriori algorithm, Data mining, association rule mining]
Mining significant pairs of patterns from graph structures with class labels
Third IEEE International Conference on Data Mining
None
2003
In recent years, the problem of mining association rules over frequent itemsets in transactional data has been frequently studied and yielded several algorithms that can find association rules within a limited amount of time. Also more complex patterns have been considered such as ordered trees, unordered trees, or labeled graphs. Although some approaches can efficiently derive all frequent subgraphs from a massive dataset of graphs, a subgraph or subtree that is mathematically defined is not necessarily a better knowledge representation. We propose an efficient approach to discover significant rules to classify positive and negative graph examples by estimating a tight upper bound on the statistical metric. This approach abandons unimportant rules earlier in the computations, and thereby accelerates the overall performance. The performance has been evaluated using real world datasets, and the efficiency and effect of our approach has been confirmed with respect to the amount of data and the computation time.
[significant pair mining, pattern mining, Laboratories, graph theory, data mining, statistical metric, Knowledge representation, unordered trees, Association rules, Data mining, transactional data, Chemical compounds, association rule mining, ordered trees, Upper bound, Tree graphs, Itemsets, knowledge representation, class labels, Acceleration, graph structure, Bonding, labeled graph, computational complexity]
Unsupervised link discovery in multi-relational data via rarity analysis
Third IEEE International Conference on Data Mining
None
2003
A significant portion of knowledge discovery and data mining research focuses on finding patterns of interest in data. Once a pattern is found, it can be used to recognize satisfying instances. The new area of link discovery requires a complementary approach, since patterns of interest might not yet be known or might have too few examples to be learnable. We present an unsupervised link discovery method aimed at discovering unusual, interestingly linked entities in multi-relational datasets. Various notions of rarity are introduced to measure the "interestingness" of sets of paths and entities. These measurements have been implemented and applied to a real-world bibliographic dataset where they give very promising results.
[Performance evaluation, Data analysis, Event detection, data analysis, data mining, knowledge discovery, Pattern recognition, Data mining, Association rules, relational databases, multirelational datasets, unsupervised learning, Computer science, Databases, distributed databases, unsupervised link discovery method, data patterns, real-world bibliographic dataset, Pattern matching, Contracts, pattern recognition]
Segmenting customer transactions using a pattern-based clustering approach
Third IEEE International Conference on Data Mining
None
2003
Grouping customer transactions into categories helps understand customers better. The marketing literature has concentrated on identifying important segmentation variables (e.g. customer loyalty) and on using clustering and mixture models for segmentation. The data mining literature has provided various clustering algorithms for segmentation. We investigate using "pattern-based" clustering approaches to grouping customer transactions. We argue that there are clusters in transaction data based on natural behavioral patterns, and present a new technique, YACA, that groups transactions such that itemsets generated from each cluster, while similar to each other, are different from ones generated from others. We present experimental results from user-centric Web usage data that demonstrates that YACA generates a highly effective clustering of transactions.
[transaction processing, YACA technique, Data analysis, data mining, Credit cards, Information management, Data mining, customer relationship management, Postal services, marketing, user-centric Web usage data, pattern-based clustering, Itemsets, Cellular phones, pattern clustering, Clustering algorithms, Pricing, Internet, Advertising, customer transactions segmentation]
Using discriminant analysis for multi-class classification
Third IEEE International Conference on Data Mining
None
2003
Discriminant analysis is known to learn discriminative feature transformations. We study its use in multiclass classification problems. The performance is tested on a large collection of benchmark datasets.
[pattern classification, support vector machines, discriminant analysis, Face recognition, Scattering, Covariance matrix, SVM, machine learning problem, Computer science, Support vector machines, support vector machine, multiclass classification, benchmark dataset collection, discriminative feature transformation, Character generation, Machine learning, Chromium, Benchmark testing, statistical databases, Linear discriminant analysis, learning (artificial intelligence), pattern recognition, computational complexity]
Pattern discovery based on rule induction and taxonomy generation
Third IEEE International Conference on Data Mining
None
2003
One of the most important problems with rule induction methods is that they cannot extract rules, which plausibly represent expert's decision processes. Here, the characteristics of expert's rules are closely examined and a new approach to extract plausible rules is introduced, which consists of the following three procedures. First, the characterization of decision attributes (given classes) is extracted from databases and the concept hierarchy for given classes is calculated. Second, based on the hierarchy, rules for each hierarchical level are induced from data. Then, for each given class, rules for all the hierarchical levels are integrated into one rule.
[Induction generators, Taxonomy, Biomedical informatics, data mining, Muscles, pattern discovery, medical information systems, Data mining, Diseases, rule induction, Databases, medical expert systems, decision attribute characterization, expert decision process, Cities and towns, Neck, taxonomy generation, rough set theory, Medical diagnostic imaging]
On the privacy preserving properties of random data perturbation techniques
Third IEEE International Conference on Data Mining
None
2003
Privacy is becoming an increasingly important issue in many data mining applications. This has triggered the development of many privacy-preserving data mining techniques. A large fraction of them use randomized data distortion techniques to mask the data for preserving the privacy of sensitive data. This methodology attempts to hide the sensitive data by randomly modifying the data values often using additive noise. We question the utility of the random value distortion technique in privacy preservation. We note that random objects (particularly random matrices) have "predictable" structures in the spectral domain and it develops a random matrix-based spectral filtering technique to retrieve original data from the dataset distorted by adding random values. We present the theoretical foundation of this filtering method and extensive experimental results to demonstrate that in many cases random data distortion preserve very little data privacy. We also point out possible avenues for the development of new privacy-preserving data mining techniques like exploiting multiplicative and colored noise for preserving privacy in data mining applications.
[Additive noise, Data privacy, perturbation techniques, data perturbation technique, Filtering, data mining, Telecommunication traffic, Information retrieval, random noise, Data mining, Application software, multiplicative noise, Computer science, randomized data distortion technique, privacy-preserving data mining technique, Perturbation methods, matrix-based spectral filtering technique, data privacy, Colored noise, colored noise, random objects]
OP-cluster: clustering by tendency in high dimensional space
Third IEEE International Conference on Data Mining
None
2003
Clustering is the process of grouping a set of objects into classes of similar objects. Because of unknownness of the hidden patterns in the data sets, the definition of similarity is very subtle. Until recently, similarity measures are typically based on distances, e.g Euclidean distance and cosine distance. We propose a flexible yet powerful clustering model, namely OP-cluster (Order Preserving Cluster). Under this new model, two objects are similar on a subset of dimensions if the values of these two objects induce the same relative order of those dimensions. Such a cluster might arise when the expression levels of (coregulated) genes can rise or fall synchronously in response to a sequence of environment stimuli. Hence, discovery of OP-Cluster is essential in revealing significant gene regulatory networks. A deterministic algorithm is designed and implemented to discover all the significant OP-Clusters. A set of extensive experiments has been done on several real biological data sets to demonstrate its effectiveness and efficiency in detecting coregulated patterns.
[Algorithm design and analysis, Data analysis, high dimensional space, environment stimuli, Statistical analysis, data mining, cosine distance, Pattern recognition, deterministic algorithms, gene regulatory network, Computer science, Databases, pattern clustering, biology computing, Clustering algorithms, Euclidean distance, Machine learning, OP cluster model, statistical analysis, Pattern analysis]
Clustering of time series subsequences is meaningless: implications for previous and future research
Third IEEE International Conference on Data Mining
None
2003
Time series data is perhaps the most frequently encountered type of data examined by the data mining community. Clustering is perhaps the most frequently used data mining algorithm, being useful in it's own right as an exploratory technique, and also as a subroutine in more complex data mining algorithms such as rule discovery, indexing, summarization, anomaly detection, and classification. Given these two facts, it is hardly surprising that time series clustering has attracted much attention. The data to be clustered can be in one of two formats: many individual time series, or a single time series, from which individual time series are extracted with a sliding window. Given the recent explosion of interest in streaming data and online algorithms, the latter case has received much attention. We make an amazing claim. Clustering of streaming time series is completely meaningless. More concretely, clusters extracted from streaming time series are forced to obey a certain constraint that is pathologically unlikely to be satisfied by any dataset, and because of this, the clusters extracted by any clustering algorithm are essentially random. While this constraint can be intuitively demonstrated with a simple illustration and is simple to prove, it has never appeared in the literature. We can justify calling our claim surprising, since it invalidates the contribution of dozens of previously published papers. We will justify our claim with a theorem, illustrative examples, and a comprehensive set of experiments on reimplementations of previous work.
[streaming time series, streaming data, clustering algorithm, data mining, time series, rule discovery, anomaly detection, many individual time series, Data mining, data mining algorithm, pattern clustering, single time series, time series clustering, computational complexity]
Mining production data with neural network &amp; CART
Third IEEE International Conference on Data Mining
None
2003
We present the preliminary results of a data mining study of a production line involving hundreds of variables related to mechanical, chemical, electrical and magnetic processes involved in manufacturing coated glass. The study was performed using two nonlinear, nonparametric approaches, namely neural network and CART, to model the relationship between the qualities of the coating and machine readings. Furthermore, neural network sensitivity analysis and CART variable rankings were used to gain insight into the coating process. Our initial results show the promise of data mining techniques to improve the production.
[machine reading, production data mining, sensitivity analysis, regression tree modelling, data mining, regression analysis, production engineering computing, variable analysi, Chemical processes, Machinery production industries, Data mining, Coatings, glass industry, neural network, coated glass manufacturing, Mechanical variables control, Neural networks, Chemical products, Electric variables control, Glass manufacturing, neural nets, Testing]
Cost-sensitive learning by cost-proportionate example weighting
Third IEEE International Conference on Data Mining
None
2003
We propose and evaluate a family of methods for converting classifier learning algorithms and classification theory into cost-sensitive algorithms and theory. The proposed conversion is based on cost-proportionate weighting of the training examples, which can be realized either by feeding the weights to the classification algorithm (as often done in boosting), or by careful subsampling. We give some theoretical performance guarantees on the proposed methods, as well as empirical evidence that they are practical alternatives to existing approaches. In particular, we propose costing, a method based on cost-proportionate rejection sampling and ensemble aggregation, which achieves excellent predictive performance on two publicly available datasets, while drastically reducing the computation required by other methods.
[pattern classification, cost-sensitive learning algorithms, Costs, sampling methods, Costing, support vector machines, classification algorithm, Boosting, Classification algorithms, Data mining, Medical diagnosis, Computer crime, High performance computing, cost-proportionate example weighting, cost-proportionate rejection sampling, cost-benefit analysis, Machine learning, Sampling methods, learning (artificial intelligence)]
Privacy-preserving distributed clustering using generative models
Third IEEE International Conference on Data Mining
None
2003
We present a framework for clustering distributed data in unsupervised and semisupervised scenarios, taking into account privacy requirements and communication costs. Rather than sharing parts of the original or perturbed data, we instead transmit the parameters of suitable generative models built at each local data site to a central location. We mathematically show that the best representative of all the data is a certain "mean" model, and empirically show that this model can be approximated quite well by generating artificial samples from the underlying distributions using Markov Chain Monte Carlo techniques, and then fitting a combined global model with a chosen parametric form to these samples. We also propose a new measure that quantifies privacy based on information theoretic concepts, and show that decreasing privacy leads to a higher quality of the combined model and vice versa. We provide empirical results on different data types to highlight the generality of our framework. The results show that high quality distributed clustering can be achieved with little privacy loss and low communication cost.
[Data privacy, Costs, perturbed data, Law, Fitting, distributed clustering, data mining, communication cost, Distributed power generation, Data mining, Monte Carlo techniques, unsupervised scenarios, Monte Carlo methods, Clustering algorithms, Distributed databases, distributed databases, local data site, Markov Chain, Markov processes, data privacy, generative model, Mathematical model, statistical analysis, semisupervised scenarios]
Clustering item data sets with association-taxonomy similarity
Third IEEE International Conference on Data Mining
None
2003
We explore here the efficient clustering of item data. Different from those of the traditional data, the features of item data are known to be of high dimensionality and sparsity. In view of the features of item data, we devise here a novel measurement, called the association-taxonomy similarity, and utilize this measurement to perform the clustering. With this association-taxonomy similarity measurement, we develop an efficient clustering algorithm, called algorithm AT (standing for association-taxonomy), for item data. Two validation indexes based on association and taxonomy properties are also devised to assess the quality of clustering for item data. As validated by the real dataset, it is shown by our experimental results that algorithm AT devised here significantly outperforms the prior works in the clustering quality as measured by the validation indexes, indicating the usefulness of association-taxonomy similarity in item data clustering.
[Performance evaluation, Data analysis, real dataset, association-taxonomy similarity, Taxonomy, data mining, Data engineering, Transaction databases, Association rules, Data mining, Electronic commerce, association-taxonomy algorithm, validation index, Itemsets, Clustering algorithms, item data clustering, statistical analysis]
PixelMaps: a new visual data mining approach for analyzing large spatial data sets
Third IEEE International Conference on Data Mining
None
2003
PixelMaps are a new pixel-oriented visual data mining technique for large spatial datasets. They combine kernel-density-based clustering with pixel-oriented displays to emphasize clusters while avoiding overlap in locally dense point sets on maps. Because a full evaluation of density functions is prohibitively expensive, we also propose an efficient approximation, Fast-PixelMap, based on a synthesis of the quadtree and gridfile data structures.
[spatial data set analysis, Laboratories, data mining, visual databases, Data mining, PixelMap algorithm, spatial data structures, quadtree synthesis, Clustering algorithms, data visualisation, gridfile data structure, visual data mining, kernel-density-based clustering, Density functional theory, Grid computing, approximation theory, Data analysis, pixel-oriented display, Credit cards, Data structures, fast-PixelMap approximation, Computer displays, Data visualization, pixel-oriented visual data mining technique, quadtrees]
Sequence modeling with mixtures of conditional maximum entropy distributions
Third IEEE International Conference on Data Mining
None
2003
We present a novel approach to modeling sequences using mixtures of conditional maximum entropy (maxent) distributions. Our method generalizes the mixture of first-order Markov models by including the "long-term" dependencies in model components. The "long-term" dependencies are represented by the frequently used in the natural language processing (NLP) domain probabilistic triggers or rules (such as "A occurred k positions back"/spl rarr/"the current symbol is B" with probability P). The maxent framework is then used to create a coherent global probabilistic model from all selected triggers. We enhance this formalism by using probabilistic mixtures with maxent models as components, thus representing hidden or unobserved effects in the data. We demonstrate how our mixture of conditional maxent models can be learned from data using the generalized EM algorithm that scales linearly in the dimensions of the data and the number of mixture components. We present empirical results on the simulated and real-world data sets and demonstrate that the proposed approach enables us to create better quality models than the mixtures of first-order Markov models and resist overfitting and curse of dimensionality that would inevitably present themselves for the higher order Markov models.
[Sequences, natural language processing, Entropy, Markov model, maxent model, Data mining, History, Proteins, Analytical models, hidden Markov models, conditional maximum entropy distribution, optimisation, sequence modelling, NLP, global probabilistic model, Hidden Markov models, DNA, Resists, maximum entropy methods, natural languages, Natural language processing, learning (artificial intelligence), EM algorithm]
Postprocessing decision trees to extract actionable knowledge
Third IEEE International Conference on Data Mining
None
2003
Most data mining algorithms and tools stop at discovered customer models, producing distribution information on customer profiles. Such techniques, when applied to industrial problems such as customer relationship management (CRM), are useful in pointing out customers who are likely attritors and customers who are loyal, but they require human experts to postprocess the mined information manually. Most of the postprocessing techniques have been limited to producing visualization results and interestingness ranking, but they do not directly suggest actions that would lead to an increase the objective function such as profit. Here, we present a novel algorithm that suggest actions to change customers from an undesired status (such as attritors) to a desired one (such as loyal) while maximizing objective function: the expected net profit. We develop these algorithms under resource constraints that are abound in reality. The contribution of the work is in taking the output from an existing mature technique (decision trees, for example), and producing novel, actionable knowledge through automatic postprocessing.
[decision trees postprocessing, Visualization, profit-based objective function, Industrial relations, Heuristic algorithms, Humans, data mining, human expert, Data mining, Customer profiles, customer relationship management, Computer science, data mining algorithm, optimisation, Customer relationship management, decision trees, customer profile, customer model, Decision trees, Mining industry, actionable knowledge extraction]
Mining semantic networks for knowledge discovery
Third IEEE International Conference on Data Mining
None
2003
We address the problem of mining a class of semantic networks, called concept frame graphs (CFG's), for knowledge discovery from text. This new representation is motivated by the need to capture richer text content so that nontrivial mining tasks can be performed. We first define the CFG representation and then describe a rule-based algorithm for constructing a CFG from text documents. Treating the CFG as a networked knowledge base, we propose new methods for text mining. On a specific task of discovering the top companies in an area, we observe that our approach leads to simpler content mining algorithms, once the CFG has been constructed. Moreover, exploiting the network structure of CFG results in significant improvements in precision and recall.
[Text mining, Knowledge engineering, Algorithm design and analysis, text analysis, Computer worms, rule-based algorithm, Knowledge based systems, frame based representation, data mining, text documents, knowledge discovery, Data mining, content mining algorithms, Organizing, networked knowledge base, Computer hacking, knowledge based systems, semantic networks, User interfaces, semantic networks mining, Computer networks, text mining, concept frame graphs]
Comparing naive Bayes, decision trees, and SVM with AUC and accuracy
Third IEEE International Conference on Data Mining
None
2003
Predictive accuracy has often been used as the main and often only evaluation criterion for the predictive performance of classification or data mining algorithms. In recent years, the area under the ROC (receiver operating characteristics) curve, or simply AUC, has been proposed as an alternative single-number measure for evaluating performance of learning algorithms. We proved that AUC is, in general, a better measure (defined precisely) than accuracy. Many popular data mining algorithms should then be reevaluated in terms of AUC. For example, it is well accepted that Naive Bayes and decision trees are very similar in accuracy. How do they compare in AUC? Also, how does the recently developed SVM (support vector machine) compare to traditional learning algorithms in accuracy and AUC? We will answer these questions. Our conclusions will provide important guidelines in data mining applications on real-world datasets.
[Machine learning algorithms, support vector machines, receiver operating characteristics, Area measurement, sensitivity analysis, data mining, ROC, performance evaluation, SVM, Data mining, Guidelines, Support vector machines, Computer science, data mining algorithm, Accuracy, Naive Bayes method, support vector machine, accuracy prediction, Support vector machine classification, Machine learning, decision trees, Bayes methods, Decision trees, learning (artificial intelligence)]
Understanding Helicoverpa armigera pest population dynamics related to chickpea crop using neural networks
Third IEEE International Conference on Data Mining
None
2003
Insect pests are a major cause of crop loss globally. Pest management will be effective and efficient if we can predict the occurrence of peak activities of a given pest. Research efforts are going on to understand the pest dynamics by applying analytical and other techniques on pest surveillance data sets. We make an effort to understand pest population dynamics using neural networks by analyzing pest surveillance data set of Helicoverpa armigera or Pod borer on chickpea (Cicer arietinum L.) crop. The results show that neural network method successfully predicts the pest attack incidences for one week in advance.
[Data analysis, pest surveillance data set, Biological system modeling, chickpea crop, Crops, Weather forecasting, data mining, Helicoverpa armigera pest population dynamics, neural networks, Predictive models, crops, Data mining, Insects, Surveillance, Neural networks, Production, pest attack incidence, pest control, statistical analysis, neural nets, Pod borer]
Detecting interesting exceptions from medical test data with visual summarization
Third IEEE International Conference on Data Mining
None
2003
We propose a method which visualizes irregular multidimensional time-series data as a sequence of probabilistic prototypes for detecting exceptions from medical test data. Conventional visualization methods often require iterative analysis and considerable skill thus are not totally supported by a wide range of medical experts. Our PrototypeLines displays summarized information based on a probabilistic mixture model by using hue only thus is considered to exhibit novelty. The effectiveness of the summarization is pursued mainly through use of a novel information criterion. We report our endeavor with chronic hepatitis data, especially discoveries of interesting exceptions by a nonexpert and an untrained expert.
[probabilistic mixture model, chronic hepatitis data, Liver diseases, irregular multidimensional time-series data, data mining, Data engineering, time series, Data mining, probabilistic prototype sequence, PrototypeLines display summarized information, interesting exception detection, medical test data, medical expert systems, Antibiotics, visual summarization, Data visualization, Prototypes, data visualisation, iterative analysis, Medical tests, Belts, Iterative methods, Biomedical engineering]
T-trees, vertical partitioning and distributed association rule mining
Third IEEE International Conference on Data Mining
None
2003
We consider a technique (DATA-VP) for distributed (and parallel) association rule mining that makes use of a vertical partitioning technique to distribute the input data, amongst processors. The proposed vertical partitioning is facilitated by a novel compressed set enumeration tree data structure (the T-tree), and an associated mining algorithm (Apriori-T), that allows for computationally effective distributed/parallel ARM when compared with existing approaches.
[Tree data structures, DATA-VP technique, distributed Apriori-T algorithm vertical partitioning, data mining, distributed processing, Partitioning algorithms, Association rules, Data mining, Distributed computing, distributed association rule mining, Computer science, Concurrent computing, Itemsets, parallel association rule mining, Computational efficiency, tree data structures, tree data structure, Indexing]
Integrating fuzziness into OLAP for multidimensional fuzzy association rules mining
Third IEEE International Conference on Data Mining
None
2003
We contribute to the ongoing research on multidimensional online association rules mining by proposing a general architecture that utilizes a fuzzy data cube for knowledge discovery. Three different methods are introduced to mine fuzzy association rules in the constructed fuzzy data cube, namely single dimension, multidimensional and hybrid association rules mining. Experimental results obtained for each of the three methods on the adult data of the United States census in 2000 show their effectiveness and applicability.
[Knowledge engineering, Multidimensional systems, data mining, fuzzy set theory, Data engineering, OLAP, knowledge discovery, Association rules, Data mining, Computer science, multidimensional fuzzy association rules mining, Fuzzy sets, Databases, Computer architecture, fuzzy data cube, data warehouses, Fuzzy systems]
TSP: mining top-K closed sequential patterns
Third IEEE International Conference on Data Mining
None
2003
Sequential pattern mining has been studied extensively in data mining community. Most previous studies require the specification of a minimum support threshold to perform the mining. However, it is difficult for users to provide an appropriate threshold in practice. To overcome this difficulty, we propose an alternative task: mining top-k frequent closed sequential patterns of length no less than min-l, where k is the desired number of closed sequential patterns to be mined, and minl, is the minimum length of each pattern. We mine closed patterns since they are compact representations of frequent patterns. We developed an efficient algorithm, called TSP, which makes use of the length constraint and the properties of top-k closed sequential patterns to perform dynamic support-raising and projected database-pruning. Our extensive performance study shows that TSP outperforms the closed sequential pattern mining algorithm even when the latter is running with the best tuned minimum support threshold.
[minimum support threshold specification, data mining, top-k frequent closed sequential pattern, Data mining, sequences, dynamic support-raising, TSP algorithm, Computer science, Databases, Itemsets, very large databases, Frequency, sequential pattern mining, minimisation, projected database-pruning, Testing]
A new optimization criterion for generalized discriminant analysis on undersampled problems
Third IEEE International Conference on Data Mining
None
2003
A new optimization criterion for discriminant analysis is presented. The new criterion extends the optimization criteria of the classical linear discriminant analysis (LDA) by introducing the pseudo-inverse when the scatter matrices are singular. It is applicable regardless of the relative sizes of the data dimension and sample size, overcoming a limitation of the classical LDA. Recently, a new algorithm called LDA/GSVD for structure-preserving dimension reduction has been introduced, which extends the classical LDA to very high-dimensional undersampled problems by using the generalized singular value decomposition (GSVD). The solution from the LDA/GSVD algorithm is a special case of the solution for our generalized criterion, which is also based on GSVD. We also present an approximate solution for our GSVD-based solution, which reduces computational complexity by finding subclusters of each cluster, and using their centroids to capture the structure of each cluster. This reduced problem yields much smaller matrices of which the GSVD can be applied efficiently. Experiments on text data, with up to 7000 dimensions, show that the approximation algorithm produces results that are close to those produced by the exact algorithm.
[linear generalized discriminant analysis, data mining, optimization criterion, Data mining, optimisation, Clustering algorithms, approximation algorithm, Linear discriminant analysis, generalized singular value decomposition, singular value decomposition, LDA algorithm, Singular value decomposition, GSVD algorithm, approximation theory, Scattering, undersampled problems, structure-preserving dimension reduction, Matrix decomposition, Computational complexity, Computer science, High performance computing, pattern clustering, Approximation algorithms, statistical analysis, computational complexity]
Active sampling for feature selection
Third IEEE International Conference on Data Mining
None
2003
In knowledge discovery applications, where new features are to be added, an acquisition policy can help select the features to be acquired based on their relevance and the cost of extraction. This can be posed as a feature selection problem where the feature values are not known in advance. We propose a technique to actively sample the feature values with the ultimate goal of choosing between alternative candidate features with minimum sampling cost. Our heuristic algorithm is based on extracting candidate features in a region of the instance space where the feature value is likely to alter our knowledge the most. An experimental evaluation on a standard database shows that it is possible outperform a random subsampling policy in terms of the accuracy in feature selection.
[Costs, Data analysis, sampling methods, Heuristic algorithms, Data acquisition, data mining, knowledge acquisition, Spatial databases, knowledge discovery, Electronic mail, Diseases, heuristic algorithm, feature extraction, active sampling, Sampling methods, Feature extraction, Agriculture, feature selection]
Parsing without a grammar: making sense of unknown file formats
Third IEEE International Conference on Data Mining
None
2003
The thousands of specialized structured file formats in use today present a substantial barrier to freely exchanging information between applications programs. We consider the problem of deducing such basic features as the whitespace characters, bracketing delimiter symbols, and self-delimiter characters of a given file format from one or more example files. We demonstrate that for sufficiently large example files, we can typically identify the basic features of interest.
[text analysis, whitespace character, Poles and towers, HTML, Spatial databases, Data mining, Application software, self delimiter character, Text processing, word processing, Computer science, application program, Markup languages, XML, bracketing delimiter symbol, symbol manipulation, structured file format, Page description languages, hypermedia markup languages]
Dynamic weighted majority: a new ensemble method for tracking concept drift
Third IEEE International Conference on Data Mining
None
2003
Algorithms for tracking concept drift are important for many applications. We present a general method based on the weighted majority algorithm for using any online learner for concept drift. Dynamic weighted majority (DWM) maintains an ensemble of base learners, predicts using a weighted-majority vote of these "experts\
[Algorithm design and analysis, expert systems, Data mining, STAGGER concepts, base algorithm, Voting, Training data, ensemble method, Noise robustness, tree data structures, DWM, learning (artificial intelligence), Computer security, Target tracking, concept drift tracking, incremental tree inducer, Blum implementation, dynamic weighted majority, Application software, SEA concepts, Computer science, incremental naive Bayes, User interfaces, Bayes methods, computational complexity]
Change profiles
Third IEEE International Conference on Data Mining
None
2003
We introduce a generalization of association rules: change profiles. We analyze their properties, describe their relationship to other structures in pattern discovery and sketch their possible applications. We study how the frequent patterns can be clustered based on their change profiles and propose methods for approximating the frequencies of the patterns from the approximate change profiles and bounding the intervals where the frequencies of the patterns are guaranteed to be. We evaluate empirically the methods for estimating the frequencies and the stability of their frequency estimates under different kinds of noise.
[pattern classification, Pattern classification, data mining, pattern discovery, Frequency estimation, stability estimation, change profile, frequency estimation]
Dimensionality reduction using kernel pooled local discriminant information
Third IEEE International Conference on Data Mining
None
2003
We study the use of kernel subspace methods for learning low-dimensional representations for classification. We propose a kernel pooled local discriminant subspace method and compare it against several competing techniques: generalized Fisher discriminant analysis (GDA) and kernel principal components analysis (KPCA) in classification problems. We evaluate the classification performance of the nearest-neighbor rule with each subspace representation. The experimental results demonstrate the efficacy of the kernel pooled local subspace method and the potential for substantial improvements over competing methods such as KPCA in some classification problems.
[Gold, pattern classification, subspace representation, Data preprocessing, nearest-neighbor rule, Null space, Fisher discriminant analysis, Data mining, kernel pooled local discriminant information, dimensionality reduction, knowledge representation, Data visualization, Feature extraction, Linear discriminant analysis, Computational efficiency, learning (artificial intelligence), Kernel, kernel principal component analysis, principal component analysis, Principal component analysis]
Effectiveness of information extraction, multi-relational, and semi-supervised learning for predicting functional properties of genes
Third IEEE International Conference on Data Mining
None
2003
We focus on the problem of predicting functional properties of the proteins corresponding to genes in the yeast genome. Our goal is to study the effectiveness of approaches that utilize all data sources that are available in this problem setting, including unlabeled and relational data, and abstracts of research papers. We study transduction and co-training for using unlabeled data. We investigate a propositionalization approach which uses relational gene interaction data. We study the benefit of information extraction for utilizing a collection of scientific abstracts. The studied tasks are KDD Cup tasks of 2001 and 2002. The solutions which we describe achieved the highest score for task 2 in 2001, the fourth rank for task 3 in 2001, the highest score for one of the two subtasks and the third place for the overall task 2 in 2002.
[semisupervised learning, Genomics, data mining, information retrieval, Data mining, relational databases, Proteins, Fungi, Computer science, information extraction, Hidden Markov models, Abstracts, Semisupervised learning, multirelational data, yeast genome, Genetics, propositionalization approach, relational gene interaction data, gene functional property prediction, unlabeled data, co-training, learning (artificial intelligence), Bioinformatics]
Frequent-pattern based iterative projected clustering
Third IEEE International Conference on Data Mining
None
2003
Irrelevant attributes add noise to high dimensional clusters and make traditional clustering techniques inappropriate. Projected clustering algorithms have been proposed to find the clusters in hidden subspaces. We realize the analogy between mining frequent itemsets and discovering the relevant subspace for a given cluster. We propose a methodology for finding projected clusters by mining frequent itemsets and present heuristics that improve its quality. Our techniques are evaluated with synthetic and real data; they are scalable and discover projected clusters accurately.
[real data, data mining, Partitioning algorithms, projected clustering algorithm, Data mining, frequent itemset mining, Information systems, Computer science, Itemsets, Databases, Lungs, pattern clustering, hidden subspace, Clustering algorithms, Character generation, synthetic data, projected cluster discovery, Iterative algorithms, statistical analysis]
Impact studies and sensitivity analysis in medical data mining with ROC-based genetic learning
Third IEEE International Conference on Data Mining
None
2003
ROC curves have been used for a fair comparison of machine learning algorithms since the late 90's. Accordingly, the area under the ROC curve (AUC) is nowadays considered a relevant learning criterion, accommodating imbalanced data, misclassification costs and noisy data. We show how a genetic algorithm-based optimization of the AUC criterion can be exploited for impact studies and sensitivity analysis. The approach is illustrated on the Atherosclerosis Identification problem, PKDD 2002 Challenge.
[Machine learning algorithms, Costs, Sensitivity analysis, Atherosclerosis Identification problem, sensitivity analysis, data mining, medical information systems, genetic algorithms, Data mining, Support vector machines, medical expert systems, Support vector machine classification, Character generation, Atherosclerosis, Machine learning, Genetics, ROC-based genetic learning, learning (artificial intelligence), machine learning algorithms, medical data mining]
SVM based models for predicting foreign currency exchange rates
Third IEEE International Conference on Data Mining
None
2003
Support vector machine (SVM) has appeared as a powerful tool for forecasting forex market and demonstrated better performance over other methods, e.g., neural network or ARIMA based model. SVM-based forecasting model necessitates the selection of appropriate kernel function and values of free parameters: regularization parameter and /spl epsiv/-insensitive loss function. We investigate the effect of different kernel functions, namely, linear, polynomial, radial basis and spline on prediction error measured by several widely used performance metrics. The effect of regularization parameter is also studied. The prediction of six different foreign currency exchange rates against Australian dollar has been performed and analyzed. Some interesting results are presented.
[Measurement, splines (mathematics), support vector machines, ARIMA based model, Predictive models, time series, neural network, Spline, Support vector machines, Exchange rates, support vector machine, SVM based model, Neural networks, foreign currency exchange rate prediction, Economic forecasting, Polynomials, Australia, economic forecasting, Kernel, neural nets]
Analyzing high-dimensional data by subspace validity
Third IEEE International Conference on Data Mining
None
2003
We are proposing a novel method that makes it possible to analyze high-dimensional data with arbitrary shaped projected clusters and high noise levels. At the core of our method lies the idea of subspace validity. We map the data in a way that allows us to test the quality of subspaces using statistical tests. Experimental results, both on synthetic and real data sets, demonstrate the potential of our method.
[Data analysis, Automation, arbitrary shaped projected clusters, Humans, visual databases, real data sets, Topology, high-dimensional data analysis, Noise level, Information analysis, Computer science, statistical tests, subspace validity, Space technology, feature extraction, image segmentation, Clustering algorithms, statistical testing, noise levels, Testing]
Interactive visualization and navigation in large data collections using the hyperbolic space
Third IEEE International Conference on Data Mining
None
2003
We propose the combination of two recently introduced methods for the interactive visual data mining of large collections of data. Both hyperbolic multidimensional scaling (HMDS) and hyperbolic self-organizing maps (HSOM) employ the extraordinary advantages of the hyperbolic plane (H2): (i) the underlying space grows exponentially with its radius around each point deal for embedding high-dimensional (or hierarchical) data; (ii) the Poincare model of the IH/sup 2/ exhibits a fish-eye perspective with a focus area and a context preserving surrounding; (in) the mouse binding of focus-transfer allows intuitive interactive navigation. The HMDS approach extends multidimensional scaling and generates a spatial embedding of the data representing their dissimilarity structure as faithfully as possible. It is very suitable for interactive browsing of data object collections, but calls for batch precomputation for larger collection sizes. The HSOM is an extension of Kohonen's self-organizing map and generates a partitioning of the data collection assigned to an IH/sup 2/ tessellating grid. While the algorithm's complexity is linear in the collection size, the data browsing is rigidly bound to the underlying grid. By integrating the two approaches, we gain the synergetic effect of adding advantages of both. And the hybrid architecture uses consistently the IH/sup 2/ visualization and navigation concept. We present the successfully application to a text mining example involving the Reuters-21578 text corpus.
[tessellating grid, Hydrogen, data mining, visual databases, interactive navigation, hyperbolic space, Data mining, hyperbolic self-organizing maps, Self organizing feature maps, very large databases, self-organising feature maps, data visualisation, interactive systems, text mining, interactive visualization, Mesh generation, Multidimensional systems, Navigation, large data collections, interactive visual data mining, Partitioning algorithms, Data visualization, batch precomputation, Mice, data handling, multidimensional scaling, Kohonen self-organizing map, hyperbolic multidimensional scaling, Context modeling, computational complexity]
Combining the Web content and usage mining to understand the visitor behavior in a Web site
Third IEEE International Conference on Data Mining
None
2003
A Web site is a semi structured collection of different kinds of data, whose motivation is to show relevant information to a visitor and in this way capture her/his attention. Understanding the specific preferences that define the visitor behavior in a Web site is a complex task. An approximation is supposed that depends on the content, navigation sequence and time spent in each page visited. These variables can be extracted from the Web log files and the Web site itself, using Web usage and content mining respectively. Combining the described variables, a similarity measure among visitor sessions is introduced and used in a clustering algorithm, which identifies groups of similar sessions, allowing the analysis of visitor behavior. In order to prove the methodology's effectiveness, it was applied in a certain Web site, showing the benefits of the described approach.
[Algorithm design and analysis, content mining, Navigation, clustering algorithm, visitor behavior analysis, data mining, information retrieval, Web log file, Web usage, Time measurement, Electronic mail, Data mining, Web content, navigation sequence, content management, self-organising feature maps, Clustering algorithms, Web pages, Web mining, Internet, Web sites, Web site]
Complex spatial relationships
Third IEEE International Conference on Data Mining
None
2003
We describe the need for mining complex relationships in spatial data. Complex relationships are defined as those involving two or more of: multifeature colocation, self-colocation, one-to-many relationships, self-exclusion and multifeature exclusion. We demonstrate that even in the mining of simple relationships, knowledge of complex relationships is necessary to accurately calculate the significance of results. We implement a representation of spatial data such that it contains known 'weak-monotonic' properties, which are exploited for the efficient mining of complex relationships, and discuss the strengths and limitations of this representation.
[spatial data, data mining, visual databases, Frequency measurement, Data mining, Association rules, weak monotonic property, Sun, Information technology, self colocation, complex relationship, multifeature colocation, Animals, Fires, one-to-many relationship, statistical analysis]
Zigzag: a new algorithm for mining large inclusion dependencies in databases
Third IEEE International Conference on Data Mining
None
2003
In the relational model, inclusion dependencies (INDs) convey many information on data semantics. They generalize foreign keys, which are very popular constraints in practice. However, one seldom knows the set of satisfied INDs in a database. The IND discovery problem in existing databases can be formulated as a data-mining problem. We underline that the exploration of IND expressions from most general (smallest) INDs to most specific (largest) INDs does not succeed whenever large INDs have to be discovered. To cope with this problem, we introduce a new algorithm, called Zigzag, which combines the strength of levelwise algorithms (to find out some smallest INDs) with an optimistic criteria to jump more or less to largest INDs. Preliminary tests, on synthetic databases, are presented and commented on. It is worth noting that the main result is general enough to be applied to other data-mining problems, such as maximal frequent itemsets mining.
[data mining, Relational databases, Data structures, Transaction databases, Data mining, Proposals, relational databases, maximal frequent itemsets mining, query processing, Zigzag algorithm, Itemsets, Query processing, data semantics, Space exploration, inclusion dependencies, Testing]
K-d decision tree: an accelerated and memory efficient nearest neighbor classifier
Third IEEE International Conference on Data Mining
None
2003
Most nearest neighbor (NN) classifiers employ NN search algorithms for the acceleration. However, NN classification does not always require the NN search. Based on this idea, we propose a novel algorithm named k-d decision tree (KDDT). Since KDDT uses Voronoi condensed prototypes, it is less memory consuming than naive NN classifiers. We have confirmed that KDDT is much faster than NN search based classifiers through the comparative experiment (from 9 to 369 times faster).
[Error probability, K-d decision tree, nearest neighbor classifier, tree searching, Voronoi condensed prototypes, Nearest neighbor searches, memory consuming, Support vector machines, storage management, Neural networks, Prototypes, Support vector machine classification, NN search algorithms, decision trees, Search engines, Decision trees, Acceleration, learning (artificial intelligence), Classification tree analysis]
Objective and subjective algorithms for grouping association rules
Third IEEE International Conference on Data Mining
None
2003
We propose two algorithms for grouping and summarizing association rules. The first algorithm recursively groups rules according to the structure of the rules and generates a tree of clusters as a result. The second algorithm groups the rules according to the semantic distance between the rules by making use of an automatically tagged semantic tree-structured network of items. We provide a case study in which the proposed algorithms are evaluated. The results show that our grouping methods are effective and produce good grouping results.
[automatically tagged semantic tree-structured network, data mining, semantic networks, association rules, subjective grouping algorithm, tree data structures, Association rules, Data mining, objective grouping algorithm, computational complexity, semantic distance, association rule mining]
Association rule mining in peer-to-peer systems
Third IEEE International Conference on Data Mining
None
2003
We extend the problem of association rule mining - a key data mining problem - to systems in which the database is partitioned among a very large number of computers that are dispersed over a wide area. Such computing systems include GRID computing platforms, federated database systems, and peer-to-peer computing environments. The scale of these systems poses several difficulties, such as the impracticality of global communications and global synchronization, dynamic topology changes of the network, on-the-fly data updates, the need to share resources with other applications, and the frequent failure and recovery of resources. We present an algorithm by which every node in the system can reach the exact solution, as if it were given the combined database. The algorithm is entirely asynchronous, imposes very little communication overhead, transparently tolerates network topology changes and node failures, and quickly adjusts to changes in the data as they occur. Simulation of up to 10000 nodes show that the algorithm is local: all rules, except for those whose confidence is about equal to the confidence threshold, are discovered using information gathered from a very small vicinity, whose size is independent of the size of the system.
[federated database systems, wide area networks, data mining, grid computing, Data mining, on-the-fly data updates, Network topology, asynchronous algorithm, very large databases, Distributed databases, distributed databases, Grid computing, Computer networks, Database systems, data mining problem, global synchronization, Peer to peer computing, dynamic network topology changes, Transaction databases, Association rules, GRID computing, association rule mining, Image databases, peer-to-peer systems, confidence threshold]
Class decomposition via clustering: a new framework for low-variance classifiers
Third IEEE International Conference on Data Mining
None
2003
We propose a preprocessing step to classification that applies a clustering algorithm to the training set to discover local patterns in the attribute or input space. We demonstrate how this knowledge can be exploited to enhance the predictive accuracy of simple classifiers. Our focus is mainly on classifiers characterized by high bias but low variance (e.g., linear classifiers); these classifiers experience difficulty in delineating class boundaries over the input space when a class distributes in complex ways. Decomposing classes into clusters makes the new class distribution easier to approximate and provides a viable way to reduce bias while limiting the growth in variance. Experimental results on real-world domains show an advantage in predictive accuracy when clustering is used as a preprocessing step to classification.
[training set, pattern classification, class decomposition, support vector machines, clustering algorithm, data mining, low-variance classifier, pattern discovery, Classification algorithms, Computer science, Support vector machines, Accuracy, optimisation, Clustering algorithms, Support vector machine classification, predictive accuracy, Polynomials, Space exploration, Bayes methods, learning (artificial intelligence), statistical analysis, Kernel, Testing]
TECNO-STREAMS: tracking evolving clusters in noisy data streams with a scalable immune system learning model
Third IEEE International Conference on Data Mining
None
2003
Artificial immune system (AIS) models hold many promises in the field of unsupervised learning. However, existing models are not scalable, which makes them of limited use in data mining. We propose a new AIS based clustering approach (TECNO-STREAMS) that addresses the weaknesses of current AIS models. Compared to existing AIS based techniques, our approach exhibits superior learning abilities, while at the same time, requiring low memory and computational costs. Like the natural immune system, the strongest advantage of immune based learning compared to other approaches is expected to be its ease of adaptation to the dynamic environment that characterizes several applications, particularly in mining data streams. We illustrate the ability of the proposed approach in detecting clusters in noisy data sets, and in mining evolving user profiles from Web clickstream data in a single pass. TECNO-STREAMS adheres to all the requirements of clustering data streams: compactness of representation, fast incremental processing of new data points, and clear and fast identification of outliers.
[user profile, Pathogens, TECNO-STREAMS approach, Web clickstream data, Cloning, data mining, AIS, Data mining, Unsupervised learning, unsupervised learning, Proteins, pattern clustering, Artificial immune systems, Euclidean distance, artificial immune system model, cluster detection, Computational efficiency, dynamic environment, noisy data set, Power generation, Immune system]
Efficient subsequence matching in time series databases under time and amplitude transformations
Third IEEE International Conference on Data Mining
None
2003
Subsequence matching in large time series databases has attracted a lot of interest and many methods have been proposed that cope with this problem in an adequate extend. However, locating subsequence matches of arbitrary length, under time and amplitude transformations, has received far less attention and is still an open problem. We present an efficient algorithm for variable-length subsequence matching under transformations that guarantees no false dismissals. Further, this algorithm uses a novel similarity criterion for determining similarity under amplitude transformations in a most efficient way. Finally, our algorithm has been tested in various experiments on real data, resulting in a running time improvement of one order of magnitude compared to the naive approach.
[Algorithm design and analysis, time transformations, Humans, Data engineering, time series, Approximation methods, Data mining, query processing, similarity criterion, amplitude transformations, Databases, temporal databases, very large databases, variable-length subsequence matching, Speech, Approximation algorithms, time series databases, Testing]
Bootstrapping rule induction
Third IEEE International Conference on Data Mining
None
2003
Most rule learning systems posit hard decision boundaries for continuous attributes and point estimates of rule accuracy, with no measures of variance, which may seem arbitrary to a domain expert. These hard boundaries/points change with small perturbations to the training data. Moreover, rule induction typically produces a large number of rules that must be filtered and interpreted by an analyst. We describe a method of combining rules over multiple bootstrap replications of rule induction so as to reduce the total number of rules presented to an analyst and to provide measures of variance to continuous attribute decision boundaries and accuracy-point estimates. The method is illustrated with perioperative data.
[Hypertension, data mining, domain expert, Biomedical measurements, perioperative data, rule induction, Learning systems, Filters, Pain, computer bootstrapping, accuracy-point estimate, Electric variables measurement, Training data, training data, continuous attribute decision boundary, Anesthesia, rule learning system, learning (artificial intelligence), multiple bootstrap replication, Analysis of variance, Biomedical engineering]
Third IEEE International Conference on Data Mining
Third IEEE International Conference on Data Mining
None
2003
The following topics are dealt with: data mining; pattern clustering; statistical analysis; pattern classification; machine learning; Bayesian network; very large databases; relational databases; pattern matching; association rules; knowledge discovery.
[pattern classification, pattern matching, data mining, association rules, knowledge discovery, Pattern recognition, database management systems, machine learning, relational databases, Statistics, Learning systems, Database management systems, pattern clustering, very large databases, statistical analysis, learning (artificial intelligence), pattern recognition]
Welcome to ICDM 2004
Fourth IEEE International Conference on Data Mining
None
2004
Presents the welcome message from the conference proceedings.
[]
Conference organization
Fourth IEEE International Conference on Data Mining
None
2004
Provides a listing of current committee members and society officers.
[]
Steering Committee
Fourth IEEE International Conference on Data Mining
None
2004
Provides a listing of current committee members.
[]
Program Committee
Fourth IEEE International Conference on Data Mining
None
2004
Provides a listing of current committee members.
[]
Tutorials
Fourth IEEE International Conference on Data Mining
None
2004
Provides an abstract for each of the tutorial presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
Subspace selection for clustering high-dimensional data
Fourth IEEE International Conference on Data Mining
None
2004
In high-dimensional feature spaces traditional clustering algorithms tend to break down in terms of efficiency and quality. Nevertheless, the data sets often contain clusters which are hidden in various subspaces of the original feature space. In this paper, we present a feature selection technique called SURFING (subspaces relevant for clustering) that finds all subspaces interesting for clustering and sorts them by relevance. The sorting is based on a quality criterion for the interestingness of a subspace using the k-nearest neighbor distances of the objects. As our method is more or less parameterless, it addresses the unsupervised notion of the data mining task "clustering" in a best possible way. A broad evaluation based on synthetic and real-world data sets demonstrates that SURFING is suitable to find all relevant sub-spaces in high dimensional, sparse data sets and produces better results than comparative methods.
[k-nearest neighbor distances, Navigation, Density measurement, clustering algorithm, Clustering methods, Biomedical informatics, data mining, subspace selection, subspaces relevant for clustering, feature spaces, Data mining, Sorting, Computer science, Space technology, pattern clustering, Clustering algorithms, sorting, SURFING, high-dimensional data clustering, sparse data sets, feature selection, Principal component analysis]
Multi-view clustering
Fourth IEEE International Conference on Data Mining
None
2004
We consider clustering problems in which the available attributes can be split into two independent subsets, such that either subset suffices for learning. Example applications of this multi-view setting include clustering of Web pages which have an intrinsic view (the pages themselves) and an extrinsic view (e.g., anchor texts of inbound hyperlinks); multi-view learning has so far been studied in the context of classification. We develop and study partitioning and agglomerative, hierarchical multi-view clustering algorithms for text data. We find empirically that the multi-view versions of k-means and EM greatly improve on their single-view counterparts. By contrast, we obtain negative results for agglomerative hierarchical multi-view clustering. Our analysis explains this surprising phenomenon.
[pattern classification, text analysis, independent subsets, clustering algorithm, data mining, agglomerative hierarchical multiview clustering, set theory, Data mining, multiview learning, pattern clustering, text data, Web pages, partitioning, learning (artificial intelligence)]
Density connected clustering with local subspace preferences
Fourth IEEE International Conference on Data Mining
None
2004
Many clustering algorithms tend to break down in high-dimensional feature spaces, because the clusters often exist only in specific subspaces (attribute subsets) of the original feature space. Therefore, the task of projected clustering (or subspace clustering) has been defined recently. As a solution to tackle this problem, we propose the concept of local subspace preferences, which captures the main directions of high point density. Using this concept, we adopt density-based clustering to cope with high-dimensional data. In particular, we achieve the following advantages over existing approaches: Our proposed method has a determinate result, does not depend on the order of processing, is robust against noise, performs only one single scan over the database, and is linear in the number of dimensions. A broad experimental evaluation shows that our approach yields results of significantly better quality than recent work on clustering high-dimensional data.
[Visualization, clustering algorithm, density-based clustering, Clustering methods, data mining, high-dimensional data, data clustering, feature spaces, Partitioning algorithms, Data mining, Nearest neighbor searches, Computer science, local subspace preferences, Databases, pattern clustering, subspace clustering, Clustering algorithms, projected clustering, Noise robustness, high point density, Principal component analysis]
On closed constrained frequent pattern mining
Fourth IEEE International Conference on Data Mining
None
2004
Constrained frequent patterns and closed frequent patterns are two paradigms aimed at reducing the set of extracted patterns to a smaller, more interesting, subset. Although a lot of work has been done with both these paradigms, there is still confusion around the mining problem obtained by joining closed and constrained frequent patterns in a unique framework. In this paper, we shed light on this problem by providing a formal definition and a thorough characterization. We also study computational issues and show how to combine the most recent results in both paradigms, providing a very efficient algorithm which exploits the two requirements (satisfying constraints and being closed) together at mining time in order to reduce the computation as much as possible.
[Itemsets, Databases, Laboratories, data mining, Frequency, Data mining, Association rules, closed frequent patterns, formal definition, pattern recognition, frequent pattern mining, constrained frequent patterns]
Efficient density-based clustering of complex objects
Fourth IEEE International Conference on Data Mining
None
2004
Nowadays, data mining in large databases of complex objects from scientific, engineering or multimedia applications is getting more and more important. In many different application domains, complex object representations along with complex distance functions are used for measuring the similarity between objects. Often, not only these complex distance measures are available but also simpler distance functions which can be computed much more efficiently. Traditionally, the well known concept of multi-step query processing which is based on exact and lower-bounding approximative distance functions are used independently of data mining algorithms. In this paper, we demonstrate how the paradigm of multi-step query processing can be integrated into the two density-based clustering algorithms DBSCAN and OPTICS resulting in a considerable efficiency boost. Our approach tries to confine itself to /spl epsiv/-range queries on the simple distance functions and carries out complex distance computations only at that stage of the clustering algorithm where they are compulsory to compute the correct clustering result. In a broad experimental evaluation based on real-world test data sets, we demonstrate that our approach accelerates the generation of flat and hierarchical density-based clusterings by more than one order of magnitude.
[Integrated optics, multistep query processing, complex distance function, data mining, Multimedia databases, Data engineering, Data mining, query processing, very large databases, test data sets, Clustering algorithms, OPTICS, complex distance measures, density-based clustering, clustering algorithm, DBSCAN, Application software, complex distance computations, Computer science, Image databases, complex object representation, Query processing, pattern clustering, Acceleration, approximative distance function, large databases]
Test-cost sensitive naive Bayes classification
Fourth IEEE International Conference on Data Mining
None
2004
Inductive learning techniques such as the naive Bayes and decision tree algorithms have been extended in the past to handle different types of costs mainly by distinguishing different costs of classification errors. However, it is an equally important issue to consider how to handle the test costs associated with querying the missing values in a test case. When the value of an attribute is missing in a test case, it may or may not be worthwhile to take the effort to obtain its missing value, depending on how much the value results in a potential gain in the classification accuracy. In this paper, we show how to obtain a test-cost sensitive naive Bayes classifier (csNB) by including a test strategy which determines how unknown attributes are selected to perform test on in order to minimize the sum of the mis-classification costs and test costs. We propose and evaluate several potential test strategies including one that allows several tests to be done at once. We empirically evaluate the csNB method, and show that it compares favorably with its decision tree counterpart.
[Performance evaluation, pattern classification, Costs, decision tree algorithm, misclassification costs, Blood, Computer science, inductive learning, test-cost sensitive naive Bayes classification, classification errors, decision trees, test case, Computer errors, Medical tests, Bayes methods, Decision trees, Medical diagnostic imaging, learning by example, csNB method, Testing, Classification tree analysis]
Moment: maintaining closed frequent itemsets over a stream sliding window
Fourth IEEE International Conference on Data Mining
None
2004
This paper considers the problem of mining closed frequent itemsets over a sliding window using limited memory space. We design a synopsis data structure to monitor transactions in the sliding window so that we can output the current closed frequent itemsets at any time. Due to time and memory constraints, the synopsis data structure cannot monitor all possible itemsets. However, monitoring only frequent itemsets make it impossible to detect new itemsets when they become frequent. In this paper, we introduce a compact data structure, the closed enumeration tree (CET), to maintain a dynamically selected set of item-sets over a sliding-window. The selected itemsets consist of a boundary between closed frequent itemsets and the rest of the itemsets. Concept drifts in a data stream are reflected by boundary movements in the CET. In other words, a status change of any itemset (e.g., from non-frequent to frequent) must occur through the boundary. Because the boundary is relatively stable, the cost of mining closed frequent item-sets over a sliding window is dramatically reduced to that of mining transactions that can possibly cause boundary movements in the CET. Our experiments show that our algorithm performs much better than previous approaches.
[Tree data structures, transaction processing, Costs, data mining, trees (mathematics), limited memory space, closed enumeration tree, Data structures, Data mining, Association rules, closed frequent itemset mining, compact data structure, stream sliding window, Computer science, time constraint, Itemsets, Moment, Memory management, mining transactions, memory constraint, synopsis data structure, data structures, Time factors, Monitoring]
Communication efficient construction of decision trees over heterogeneously distributed data
Fourth IEEE International Conference on Data Mining
None
2004
We present an algorithm designed to efficiently construct a decision tree over heterogeneously distributed data without centralizing. We compare our algorithm against a standard centralized decision tree implementation in terms of accuracy as well as the communication complexity. Our experimental results show that by using only 20% of the communication cost necessary to centralize the data we can achieve trees with accuracy at least 80% of the trees produced by the centralized version.
[Algorithm design and analysis, Costs, heterogeneously distributed data, Random Projection, data mining, distributed processing, Distributed decision making, Complexity theory, Data mining, communication complexity, Communication standards, Computer science, Message passing, distributed data mining, Communication channels, decision trees, communication efficient construction, Decision Trees, Decision trees, random projection, Distributed Data Mining]
Non-redundant data clustering
Fourth IEEE International Conference on Data Mining
None
2004
Data clustering is a popular approach for automatically finding classes, concepts, or groups of patterns. In practice, this discovery process should avoid redundancies with existing knowledge about class structures or groupings, and reveal novel, previously unknown aspects of the data. In order to deal with this problem, we present an extension of the information bottleneck framework, called coordinated conditional information bottleneck, which takes negative relevance information into account by maximizing a conditional mutual information score subject to constraints. Algorithmically, one can apply an alternating optimization scheme that can be used in conjunction with different types of numeric and non-numeric attributes. We present experimental results for applications in text mining and computer vision.
[Text mining, Geography, Computer vision, text analysis, Demography, class structures, optimization scheme, class groupings, data mining, knowledge discovery, Data mining, Application software, Face detection, nonredundant data clustering, Computer science, conditional mutual information, pattern clustering, computer vision, Cities and towns, text mining, Mutual information, coordinated conditional information bottleneck]
Fast and exact out-of-core k-means clustering
Fourth IEEE International Conference on Data Mining
None
2004
Clustering has been one of the most widely studied topics in data mining and k-means clustering has been one of the popular clustering algorithms. K-means requires several passes on the entire dataset, which can make it very expensive for large disk-resident datasets. In view of this, a lot of work has been done on various approximate versions of k-means, which require only one or a small number of passes on the entire dataset. In this paper, we present a new algorithm which typically requires only one or a small number of passes on the entire dataset, and provably produces the same cluster centers as reported by the original k-means algorithm. The algorithm uses sampling to create initial cluster centers, and then takes one or more passes over the entire dataset to adjust these cluster centers. We provide theoretical analysis to show that the cluster centers thus reported are the same as the ones computed by the original k-means algorithm. Experimental results from a number of real and synthetic datasets show speedup between a factor of 2 and 4.5, as compared to k-means.
[Algorithm design and analysis, cluster centers, data mining, Data engineering, Pattern recognition, Data mining, real datasets, Statistics, Convergence, Computer science, k-means clustering, Databases, pattern clustering, Clustering algorithms, Sampling methods, k-means algorithm, large disk-resident datasets, synthetic datasets]
Mining frequent itemsets from secondary memory
Fourth IEEE International Conference on Data Mining
None
2004
Mining frequent itemsets is at the core of mining association rules, and is by now quite well understood algorithmically for main memory databases. In this paper, we investigate approaches to mining frequent itemsets when the database or the data structures used in the mining are too large to fit in main memory. Experimental results show that our techniques reduce the required disk accesses by orders of magnitude, and enable truly scalable data mining.
[association rule, disk accesses, Conferences, data mining, Companies, Data structures, Transaction databases, Data mining, Association rules, storage management, secondary memory, Itemsets, Sampling methods, data structures, Testing, Business, frequent itemsets, memory databases]
A Bayesian framework for regularized SVM parameter estimation
Fourth IEEE International Conference on Data Mining
None
2004
The support vector machine (SVM) is considered here in the context of pattern classification. The emphasis is on the soft margin classifier which uses regularization to handle non-separable learning samples. We present an SVM parameter estimation algorithm that first identifies a subset of the learning samples that we call the support set and then determines not only the weights of the classifier but also the hyperparameter that controls the influence of the regularizing penalty term on basis thereof. We provide numerical results using several data sets from the public domain.
[soft margin classifier, pattern classification, support set, Parameter estimation, Costs, Error analysis, support vector machines, Bayesian framework, Least squares methods, Support vector machines, Computer science, Constraint optimization, regularized SVM, support vector machine, nonseparable learning samples, Bayesian methods, Support vector machine classification, Pattern classification, parameter estimation, Bayes methods]
Unimodal segmentation of sequences
Fourth IEEE International Conference on Data Mining
None
2004
We study the problem of segmenting a sequence into k pieces so that the resulting segmentation satisfies monotonicity or unimodality constraints. Unimodal functions can be used to model phenomena in which a measured variable first increases to a certain level and then decreases. We combine a well-known unimodal regression algorithm with a simple dynamic-programming approach to obtain an optimal quadratic-time algorithm for the problem of unimodal k-segmentation. In addition, we describe a more efficient greedy-merging heuristic that is experimentally shown to give solutions very close to the optimal. As a concrete application of our algorithms, we describe two methods for testing if a sequence behaves unimodally or not. Our experimental evaluation shows that our algorithms and the proposed unimodality tests give very intuitive results.
[unimodality constraints, greedy-merging heuristic, Heuristic algorithms, regression analysis, Data mining, sequences, unimodal sequence segmentation, heuristic programming, monotonicity constraints, quadratic-time algorithm, unimodal regression algorithm, Statistical distributions, Polynomials, Dynamic programming, Testing, merging, Statistical analysis, unimodal functions, greedy algorithms, unimodality test, dynamic programming, unimodal k-segmentation, Information technology, Computer science, Concrete, computational complexity]
Dependencies between transcription factor binding sites: comparison between ICA, NMF, PLSA and frequent sets
Fourth IEEE International Conference on Data Mining
None
2004
Gene expression of eucaryotes is regulated through transcription factors, which are molecules able to attach to the binding sites in the DNA sequence. These binding sites are small pieces of DNA usually found upstream from the gene they regulate. As the binding sites play an important role in the gene expression, it is of interest to find out their characteristics. In this paper, we look for dependencies and independencies between these binding sites using independent component analysis (ICA), non-negative matrix factorization (NMF), probabilistic latent semantic analysis (PLSA) and the method of frequent sets. The data used are human gene upstream regions and possible binding sites listed in a biological database. Also, results on the baker's yeast (S. Cerevisiae) upstream regions are briefly discussed for comparison. ICA, NMF and PLSA are latent variable methods that decompose the observed data into smaller components. Of these, ICA and NMF were originally aimed for continuous data. We show that these methods can be successfully used on discrete DNA data as well. PLSA and the method of frequent sets were created for discrete data sets. The above methods reveal partially overlapping sets of possible binding sites such that the binding sites within a set are dependent of each other. The methods of frequent sets and NMF give a good overview of the most common data structures, whereas using ICA and PLSA we find large sets that are surprisingly frequent. That is, sets of very frequently occurring possible binding sites can be found near hundreds or thousands of genes; also interesting but less frequent ones co-occur surprisingly often.
[transcription factor binding sites, baker yeast, Laboratories, transcription factors, Humans, data mining, probabilistic latent semantic analysis, frequent sets, DNA sequence, matrix decomposition, S. Cerevisiae, Fungi, biological database, independent component analysis, Databases, genetics, biology computing, Computer networks, data structures, human gene upstream regions, eucaryotes, Sequences, probability, Independent component analysis, Gene expression, nonnegative matrix factorization, Neural networks, DNA, gene expression]
Mass spectrum labeling: theory and practice
Fourth IEEE International Conference on Data Mining
None
2004
We introduce the problem of labeling a particle's mass spectrum with the substances it contains, and develop several formal representations of the problem, taking into account practical complications such as unknown compounds and noise. This task is currently a bottle-neck in analyzing data from a new generation of instruments for real-time environmental monitoring.
[Data analysis, Machine learning algorithms, data analysis, Instruments, data mining, Mass spectroscopy, mass spectra, Iron, monitoring, mass spectroscopy, physics computing, mass spectrum labeling, real-time environmental monitoring, Working environment noise, Aerosols, formal representations, Labeling, Chemical elements, Monitoring, environmental management]
Generation of attribute value taxonomies from data for data-driven construction of accurate and compact classifiers
Fourth IEEE International Conference on Data Mining
None
2004
Attribute value taxonomies (AVT) have been shown to be useful in constructing compact, robust, and comprehensible classifiers. However, in many application domains, human-designed AVTs are unavailable. We introduce AVT-learner, an algorithm for automated construction of attribute value taxonomies from data. AVT-learner uses hierarchical agglomerative clustering (HAC) to cluster attribute values based on the distribution of classes that co-occur with the values. We describe experiments on UCI data sets that compare the performance of AVT-NBL (an AVT-guided naive Bayes learner) with that of the standard naive Bayes learner (NBL) applied to the original data set. Our results show that the AVTs generated by AVT-learner are competitive with human-gene rated AVTs (in cases where such AVTs are available). AVT-NBL using AVTs generated by AVT-learner achieves classification accuracies that are comparable to or higher than those obtained by NBL; and the resulting classifiers are significantly more compact than those generated by NBL.
[Instruction sets, human-designed AVT, Taxonomy, Laboratories, data mining, Ontologies, Data mining, Clustering algorithms, Robustness, compact classifiers, learning (artificial intelligence), naive Bayes learner, pattern classification, UCI data sets, AVT-NBL, cluster attribute values, Application software, AVT-learner, Computer science, data-driven construction, automated construction, Bayes methods, Artificial intelligence, attribute value taxonomies, hierarchical agglomerative clustering]
Semi-supervised mixture-of-experts classification
Fourth IEEE International Conference on Data Mining
None
2004
We introduce a mixture-of-experts technique that is a generalization of mixture modeling techniques previously suggested for semi-supervised learning. We apply the bias-variance decomposition to semi-supervised classification and use the decomposition to study the effects from adding unlabeled data when learning a mixture model. Our empirical results indicate that the biggest gain from adding unlabeled data comes from the reduction of the model variance, whereas the behavior of the bias error term heavily depends on the correctness of the underlying model assumptions.
[Drugs, pattern classification, Costs, Machine learning algorithms, semisupervised learning, model variance, bias-variance decomposition, bias error term, Information filtering, mixture model, Computer science, Degradation, mixture modeling, semisupervised mixture-of-experts classification, Machine learning, Semisupervised learning, Performance analysis, Error correction, unlabeled data, learning (artificial intelligence)]
Transduction and typicalness for quality assessment of individual classifications in machine learning and data mining
Fourth IEEE International Conference on Data Mining
None
2004
In the past, machine learning algorithms have been successfully used in many problems, and are emerging as valuable data analysis tools. However, their serious practical use is affected by the fact, that more often than not, they cannot produce reliable and unbiased assessments of their predictions' quality. In last years, several approaches for estimating reliability or confidence of individual classifiers have emerged, many of them building upon the algorithmic theory of randomness, such as (historically ordered) transduction-based confidence estimation, typicalness-based confidence estimation, and transductive reliability estimation. Unfortunately, they all have weaknesses: either they are tightly bound with particular learning algorithms, or the interpretation of reliability estimations is not always consistent with statistical confidence levels. In the paper, we propose a joint approach that compensates the mentioned weaknesses by integrating typicalness-based confidence estimation and transductive reliability estimation into joint confidence machine. The resulting confidence machine produces confidence values in the statistical sense (e.g., a confidence level of 95% means that in 95% the predicted class is also a true class), as well as provides us with a general principle that is independent of to the particular underlying classifier. We perform a series of tests with several different machine learning algorithms in several problem domains. We compare our results with that of a proprietary TCM-NN method as well as with kernel density estimation. We show that the proposed method significantly outperforms density estimation methods, and how it may be used to improve their performance.
[Machine learning algorithms, joint confidence machine, TCM-NN method, data mining, Probability distribution, typicalness-based confidence estimation, reliability theory, Data mining, quality assessment, learning (artificial intelligence), Kernel, Classification tree analysis, pattern classification, learning algorithm, confidence estimation, kernel density estimation, transductive reliability estimation, transduction, Reliability theory, density estimation method, machine learning, typicalness, individual classifications, Bayesian methods, Neural networks, Machine learning, data analysis tool, Quality assessment, transduction-based confidence estimation, statistical analysis]
Mining associations by linear inequalities
Fourth IEEE International Conference on Data Mining
None
2004
The main theorem is: generalized associations of a relational table can be found by a finite set of linear inequalities within polynomial time. It is derived from the following three results, which were established in ICDMO'02 and are re-developed here. They are: (1) isomorphic theorem: isomorphic relations have isomorphic patterns. Such an isomorphism classifies relational tables into isomorphic classes. (2) A variant of the classical bitmaps indexes uniquely exists in each isomorphic class. We take it as the canonical model of the class. (3) All possible attributes/features can be generated by a generalized procedure of the classical AOG (attribute oriented generalization). Then, (4) the main theorem for canonical model is established. By isomorphism theorem, we had the final result (5).
[attribute oriented generalization, bitmaps, Humans, data mining, association, set theory, Data mining, association mining, linear inequalities, isomorphic classes, feature, Logic functions, Polynomials, polynomial time, theorem proving, pattern classification, finite set, isomorphic theorem, Data processing, canonical model, isomorphic relations, Association rules, Anatomy, Computer science, relational table, deduction, granules, Frequency, Software systems, generalized associations, bitmaps indexes, isomorphic patterns]
Improving text classification using local latent semantic indexing
Fourth IEEE International Conference on Data Mining
None
2004
Latent semantic indexing (LSI) has been shown to be extremely useful in information retrieval, but it is not an optimal representation for text classification. It always drops the text classification performance when being applied to the whole training set (global LSI) because this completely unsupervised method ignores class discrimination while only concentrating on representation. Some local LSI methods have been proposed to improve the classification by utilizing class discrimination information. However, their performance improvements over original term vectors are still very limited. In this paper, we propose a new local LSI method called "local relevancy weighted LSI" to improve text classification by performing a separate single value decomposition (SVD) on the transformed local region of each class. Experimental results show that our method is much better than global LSI and traditional local LSI methods on classification within a much smaller LSI dimension.
[Text mining, training set, text analysis, pattern classification, single value decomposition, indexing, term vectors, class discrimination information, information retrieval, Information retrieval, Large scale integration, Classification algorithms, text classification, local LSI method, Support vector machines, local relevancy weighted LSI, Text categorization, Asia, Support vector machine classification, Feature extraction, global LSI, singular value decomposition, local latent semantic indexing, Indexing]
Dependency networks for relational data
Fourth IEEE International Conference on Data Mining
None
2004
Instance independence is a critical assumption of traditional machine learning methods contradicted by many relational datasets. For example, in scientific literature datasets, there are dependencies among the references of a paper. Recent work on graphical models for relational data has demonstrated significant performance gains for models that exploit the dependencies among instances. In this paper, we present relational dependency networks (RDNs), a new form of graphical model capable of reasoning with such dependencies in a relational setting. We describe the details of RDN models and outline their strengths, most notably the ability to learn and reason with cyclic relational dependencies. We present RDN models learned on a number of real-world datasets, and evaluate the models in a classification context, showing significant performance improvements. In addition, we use synthetic data to evaluate the quality of model learning and inference procedures.
[instance independence, Performance gain, relational databases, inference mechanisms, classification, machine learning, RDN models, real-world datasets, model learning, Markov random fields, relational dependency networks, Proteins, Computer science, Learning systems, relational data, Graphical models, Bayesian methods, graphical models, cyclic relational dependencies, inference procedures, Autocorrelation, Web sites, learning (artificial intelligence), Context modeling]
Hybrid pre-query term expansion using latent semantic analysis
Fourth IEEE International Conference on Data Mining
None
2004
Latent semantic retrieval methods (unlike vector space methods) take the document and query vectors and map them into a topic space to cluster related terms and documents. This produces a more precise retrieval but also a long query time. We present a new method of document retrieval which allows us to process the latent semantic information into a hybrid latent semantic-vector space query mapping. This mapping automatically expands the users query based on the latent semantic information in the document set. This expanded query is processed using a fast vector space method. Since we have the latent semantic data in a mapping, we are able to store and retrieve vector information in the same fast manner that the vector space method offers. Multiple mappings are combined to produce hybrid latent semantic retrieval which provide precision results 5% greater than the vector space method and fast query times.
[text analysis, hybrid latent semantic, user query, information retrieval, Information retrieval, query mapping, Vectors, latent semantic retrieval, vector space method, latent semantic analysis, Computational complexity, query vectors, Machine intelligence, Computer science, document retrieval, Writing, Hybrid power systems, hybrid prequery term expansion]
SCHISM: a new approach for interesting subspace mining
Fourth IEEE International Conference on Data Mining
None
2004
High-dimensional data pose challenges to traditional clustering algorithms due to their inherent sparsity and data tend to cluster in different and possibly overlapping subspaces of the entire feature space. Finding such subspaces is called subspace mining. We present SCHISM, a new algorithm for mining interesting subspaces, using the notions of support and Chernoff-Hoeffding bounds. We use a vertical representation of the dataset, and use a depth-first search with backtracking to find maximal interesting subspaces. We test our algorithm on a number of high-dimensional synthetic and real datasets to test its effectiveness.
[US Department of Energy, Multidimensional systems, Engineering profession, clustering algorithm, feature space, data mining, Partitioning algorithms, vertical dataset representation, interesting subspaces, tree searching, Unsupervised learning, Computer science, depth-first search, pattern clustering, Clustering algorithms, Karhunen-Loeve transforms, backtracking, subspace mining, Chernoff-Hoeffding bound, Testing, Singular value decomposition, SCHISM]
A transaction-based neighbourhood-driven approach to quantifying interestingness of association rules
Fourth IEEE International Conference on Data Mining
None
2004
In this paper, we present a data-driven approach for ranking association rules (ARs) based on interestingness. The occurrence of unrelated or weakly related item-pairs in an AR is interesting. In the retail market-basket context, items may be related through various relationships arising due to mutual interaction, 'substitutability' and 'complementarity.' Item-relatedness is a composite of these relationships. We introduce three relatedness measures for capturing relatedness between item-pairs. These measures use the concept of junction embedding to appropriately weigh the relatedness contributions due to complementarity and substitutability between items. We propose an interestingness coefficient by combining the three relatedness measures. We compare this with two objective measures of interestingness and show the intuitiveness of the proposed interestingness coefficient.
[transaction processing, Industrial relations, relatedness contribution, data mining, association rules, interestingness coefficient, Association rules, Data mining, Information technology, transaction-based neighbourhood-driven approach, retail market-basket, Technology management, Management information systems, item relatedness, Manufacturing, item pairs, junction embedding]
Probabilistic principal surfaces for yeast gene microarray data mining
Fourth IEEE International Conference on Data Mining
None
2004
The recent technological advances are producing huge data sets in almost all fields of scientific research, from astronomy to genetics. Although each research field often requires ad-hoc, fine tuned, procedures to properly exploit all the available information inherently present in the data, there is an urgent need for a new generation of general computational theories and tools capable to boost most human activities of data analysis. Here, we propose probabilistic principal surfaces (PPS) as an effective high-D data visualization and clustering tool for data mining applications, emphasizing its flexibility and generality of use in data-rich field. In order to better illustrate the potentialities of the method, we also provide a real world case-study by discussing the use of PPS for the analysis of yeast gene expression levels from microarray chips.
[Humans, data mining, Data mining, yeast gene microarray, Fungi, general computational tools, genetics, Space technology, biology computing, data visualisation, Genetics, data visualization, Data analysis, probabilistic principal surfaces, data analysis, clustering tool, probability, microarray chip, Topology, Gene expression, astronomy, pattern clustering, Data visualization, Astronomy, general computational theories]
On local spatial outliers
Fourth IEEE International Conference on Data Mining
None
2004
We propose a measure, spatial local outlier measure (SLOM) which captures the local behaviour of datum in their spatial neighborhood. With the help of SLOM, we are able to discern local spatial outliers which are usually missed by global techniques like "three standard deviations away from the mean". Furthermore, the measure takes into account the local stability around a data point and supresses the reporting of outliers in highly unstable areas, where data is too heterogeneous and the notion of outliers is not meaningful. We prove several properties of SLOM and report experiments on synthetic and real data sets which show that our approach is scalable to large data sets.
[Sea surface, Stability, Area measurement, Sea measurements, data mining, visual databases, Data mining, Information technology, Sun, Ocean temperature, SLOM, very large databases, spatial neighborhood, large data sets, Chebyshev approximation, local spatial outliers, Australia, spatial local outlier measure]
MMAC: a new multi-class, multi-label associative classification approach
Fourth IEEE International Conference on Data Mining
None
2004
Building fast and accurate classifiers for large-scale databases is an important task in data mining. There is growing evidence that integrating classification and association rule mining together can produce more efficient and accurate classifiers than traditional classification techniques. In this paper, the problem of producing rules with multiple labels is investigated. We propose a new associative classification approach called multi-class, multi-label associative classification (MMAC). This paper also presents three measures for evaluating the accuracy of data mining classification approaches to a wide range of traditional and multi-label classification problems. Results for 28 different datasets show that the MMAC approach is an accurate and effective classification technique, highly competitive and scalable in comparison with other classification approaches.
[pattern classification, multiple labels, data mining, MMAC, Data mining, Association rules, Intelligent structures, association rule mining, classification technique, large-scale databases, Processor scheduling, multiclass multilabel associative classification, Text categorization, very large databases, Training data, Large-scale systems, Decision trees, Deductive databases, Testing]
Analysis of consensus partition in cluster ensemble
Fourth IEEE International Conference on Data Mining
None
2004
In combination of multiple partitions, one is usually interested in deriving a consensus solution with a quality better than that of given partitions. Several recent studies have empirically demonstrated improved accuracy of clustering ensembles on a number of artificial and real-world data sets. Unlike certain multiple supervised classifier systems, convergence properties of unsupervised clustering ensembles remain unknown for conventional combination schemes. In this paper, we present formal arguments on the effectiveness of cluster ensemble from two perspectives. The first is based on a stochastic partition generation model related to re-labeling and consensus function with plurality voting. The second is to study the property of the "mean" partition of an ensemble with respect to a metric on the space of all possible partitions. In both the cases, the consensus solution can be shown to converge to a true underlying clustering solution as the number of partitions in the ensemble increases. This paper provides a rigorous justification for the use of cluster ensemble.
[Algorithm design and analysis, convergence properties, Stochastic processes, data mining, real-world data set, artificial data set, supervised classifier systems, consensus function, Partitioning algorithms, Data mining, stochastic partition generation model, Computer science, mean partition, Voting, pattern clustering, plurality voting, Clustering algorithms, consensus partition, Robustness, relabeling function, Labeling, cluster ensemble, Mutual information, unsupervised clustering ensembles]
Privacy-preserving outlier detection
Fourth IEEE International Conference on Data Mining
None
2004
Outlier detection can lead to the discovery of truly unexpected knowledge in many areas such as electronic commerce, credit card fraud and especially national security. We look at the problem of finding outliers in large distributed databases where privacy/security concerns restrict the sharing of data. Both homogeneous and heterogeneous distribution of data is considered. We propose techniques to detect outliers in such scenarios while giving formal guarantees on the amount of information disclosed.
[Data privacy, Protocols, Terrorism, Data security, data mining, Credit cards, knowledge discovery, homogeneous data distribution, Data mining, Electronic commerce, security of data, very large databases, large distributed databases, Distributed databases, Information security, distributed databases, data privacy, privacy-preserving outlier detection, heterogeneous data distribution, National security, data sharing]
SUMMARY: efficiently summarizing transactions for clustering
Fourth IEEE International Conference on Data Mining
None
2004
Frequent itemset mining was initially proposed and has been studied extensively in the context of association rule mining. In recent years, several studies have also extended its application to the transaction (or document) classification and clustering. However, most of the frequent-itemset based clustering algorithms need to first mine a large intermediate set of frequent itemsets in order to identify a subset of the most promising ones that can be used for clustering. In this paper, we study how to directly find a subset of high quality frequent itemsets that can be used as a concise summary of the transaction database and to cluster the categorical data. By exploring some properties of the subset of itemsets that we are interested in, we proposed several search space pruning methods and designed an efficient algorithm called SUMMARY. Our empirical results have shown that SUMMARY runs very fast even when the minimum support is extremely low and scales very well with respect to the database size, and surprisingly, as a pure frequent itemset mining algorithm, it is very effective in clustering the categorical data and summarizing the dense transaction databases.
[Algorithm design and analysis, transaction processing, Design methodology, data mining, document clustering, Data mining, Itemsets, SUMMARY, transaction clustering, Clustering algorithms, categorical data, search space pruning, clustering algorithm, Transaction databases, summarizing transactions, document classification, Association rules, Application software, frequent itemset mining, association rule mining, transaction database, Computer science, High performance computing, pattern clustering, transaction classification]
Bottom-up generalization: a data mining solution to privacy protection
Fourth IEEE International Conference on Data Mining
None
2004
The well-known privacy-preserved data mining modifies existing data mining techniques to randomized data. In this paper, we investigate data mining as a technique for masking data, therefore, termed data mining based privacy protection. This approach incorporates partially the requirement of a targeted data mining task into the process of masking data so that essential structure is preserved in the masked data. The idea is simple but novel: we explore the data generalization concept from data mining as a way to hide detailed information, rather than discover trends and patterns. Once the data is masked, standard data mining techniques can be applied without modification. Our work demonstrated another positive use of data mining technology: not only can it discover useful patterns, but also mask private information. We consider the following privacy problem: a data holder wants to release a version of data for building classification models, but wants to protect against linking the released data to an external source for inferring sensitive information. We adapt an iterative bottom-up generalization from data mining to generalize the data. The generalized data remains useful to classification but becomes difficult to link to other sources. The generalization space is specified by a hierarchical structure of generalizations. A key is identifying the best generalization to climb up the hierarchy at each iteration. Enumerating all candidate generalizations is impractical. We present a scalable solution that examines at most one generalization in each iteration for each attribute involved in the linking.
[Data privacy, Chemical engineering, data mining, Data mining, generalisation (artificial intelligence), data holder, classification model, Couplings, Iris, Councils, pattern clustering, privacy protection, bottom-up generalization, data privacy, data masking, data generalization, randomized data, Protection, Joining processes, Biomedical engineering]
A probabilistic approach for adapting information extraction wrappers and discovering new attributes
Fourth IEEE International Conference on Data Mining
None
2004
We develop a probabilistic framework for adapting information extraction wrappers with new attribute discovery. Wrapper adaptation aims at automatically adapting a previously learned wrapper from the source Web site to a new unseen site for information extraction. One unique characteristic of our framework is that it can discover new or previously unseen attributes as well as headers from the new site. It is based on a generative model for the generation of text fragments related to attribute items and formatting data in a Web page. To solve the wrapper adaptation problem, we consider two kinds of information from the source Web site. The first kind of information is the extraction knowledge contained in the previously learned wrapper from the source Web site. The second kind of information is the previously extracted or collected items. We employ a Bayesian learning approach to automatically select a set of training examples for adapting a wrapper for the new unseen site. To solve the new attribute discovery problem, we develop a model which analyzes the surrounding text fragments of the attributes in the new unseen site. A Bayesian learning method is developed to discover the new attributes and their headers. EM technique is employed in both Bayesian learning models. We conducted extensive experiments from a number of real-world Web sites to demonstrate the effectiveness of our framework.
[attribute discovery, Humans, data mining, probability, Bayesian learning, EM technique, Data mining, text fragments, Learning systems, information extraction wrappers, Bayesian methods, Web pages, Web page, knowledge extraction, Systems engineering and theory, Bayes methods, Books, Web sites, learning (artificial intelligence), Web site, Research and development management, learned wrapper, Web search, wrapper adaptation]
Aligning boundary in kernel space for learning imbalanced dataset
Fourth IEEE International Conference on Data Mining
None
2004
An imbalanced training dataset poses serious problem for many real-world supervised learning tasks. In this paper, we propose a kernel-boundary-alignment algorithm, which considers training-data imbalance as prior information to augment SVMs to improve class-prediction accuracy. Using a simple example, we first show that SVMs can suffer from high incidences of false negatives when the training instances of the target class are heavily outnumbered by the training instances of a nontarget class. The remedy we propose is to adjust the class boundary by modifying the kernel matrix, according to the imbalanced data distribution. Through theoretical analysis backed by empirical study, we show that our kernel-boundary-alignment algorithm works effectively on several datasets.
[Algorithm design and analysis, Machine learning algorithms, support vector machines, supervised learning, Data engineering, training-data imbalance, SVM, class prediction, matrix algebra, Support vector machines, kernel boundary alignment, imbalanced dataset learning, Bayesian methods, Surveillance, Supervised learning, Training data, Support vector machine classification, kernel matrix, learning (artificial intelligence), Kernel]
IRC: an iterative reinforcement categorization algorithm for interrelated Web objects
Fourth IEEE International Conference on Data Mining
None
2004
Most existing categorization algorithms deal with homogeneous Web data objects, and consider interrelated objects as additional features when taking the interrelationships with other types of objects into account. However, focusing on any single aspects of these interrelationships and objects does not fully reveal their true categories. In this paper, we propose a categorization algorithm, the iterative reinforcement categorization algorithm (IRC), to exploit the full interrelationships between the heterogeneous objects on the Web. IRC attempts to classify the interrelated Web objects by iterative reinforcement between individual classification results of different types via the interrelationships. Experiments on a clickthrough log dataset from MSN search engine show that, with the Fl measures, IRC achieves a 26.4% improvement over a pure content-based classification method, a 21% improvement over a query metadata-based method, and a 16.4% improvement over a virtual document-based method. Furthermore, our experiments show that IRC converges rapidly.
[Head, iterative reinforcement categorization, Data engineering, Classification algorithms, Data mining, classification, Statistics, Asia, Text categorization, Web pages, interrelated Web objects, Search engines, Iterative algorithms, Web sites]
A polygonal line algorithm based nonlinear feature extraction method
Fourth IEEE International Conference on Data Mining
None
2004
We propose a polygonal line based principal curve algorithm for nonlinear feature extraction, in which the nonlinearities among the multivariable data can be described by a set of local linear models. The proposed algorithm integrates the linear PCA approach with the polygonal line algorithm to represent complicated nonlinear data structure. Statistical redundancy elimination for high dimensional data is also discussed for describing the underlying principal curves without much loss of information among the original data sets. The polygonal line algorithm can produce robust and accurate nonlinear curve estimation for different multivariate data types, and it is helpful in reducing the computation complexity for existing principal curve approaches when the sample size is large.
[nonlinear data structure, principal curve algorithm, computational geometry, Data structures, Vectors, Covariance matrix, polygonal line algorithm based nonlinear feature extraction, Convergence, nonlinear curve estimation, Nonlinear distortion, feature extraction, polynomial approximation, Euclidean distance, Feature extraction, Robustness, Eigenvalues and eigenfunctions, curve fitting, statistical redundancy elimination, principal component analysis, Principal component analysis, computational complexity]
AVT-NBL: an algorithm for learning compact and accurate naive Bayes classifiers from attribute value taxonomies and data
Fourth IEEE International Conference on Data Mining
None
2004
In many application domains, there is a need for learning algorithms that can effectively exploit attribute value taxonomies (AVT) - hierarchical groupings of attribute values - to learn compact, comprehensible, and accurate classifiers from data - including data that are partially specified. This paper describes AVT-NBL, a natural generalization of the naive Bayes learner (NBL), for learning classifiers from AVT and data. Our experimental results show that AVT-NBL is able to generate classifiers that are substantially more compact and more accurate than those produced by NBL on a broad range of data sets with different percentages of partially specified values. We also show that AVT-NBL is more efficient in its use of training data: AVT-NBL produces classifiers that outperform those produced by NBL using substantially fewer training examples.
[AVT-NBL algorithm, pattern classification, classifier learning, Taxonomy, Laboratories, Ontologies, Data mining, Application software, Learning, Computer science, Training data, naive Bayes classification, Robustness, Bayes methods, learning (artificial intelligence), Artificial intelligence, attribute value taxonomies]
Cost-guided class noise handling for effective cost-sensitive learning
Fourth IEEE International Conference on Data Mining
None
2004
Research in machine learning, data mining and related areas has produced a wide variety of algorithms for cost-sensitive (CS) classification, where instead of maximizing the classification accuracy, minimizing the misclassification cost becomes the objective. However, these methods assume that training sets do not contain significant noise, which is rarely the case in real-world environments. In this paper, we systematically study the impacts of class noise on CS learning, and propose a cost-guided class noise handling algorithm to identify noise for effective CS learning. We call it cost-guided iterative classification filter (CICF), because it seamlessly integrates costs and an existing classification filter (C. Brodley and M. Friedl, 1999) for noise identification. Instead of putting equal weights to handle noise in all classes in existing efforts, CICF puts more emphasis on expensive classes, which makes it especially successful in dealing with datasets with a large cost-ratio. Experimental results and comparative studies from real-world datasets indicate that the existence of noise may seriously corrupt the performance of CS classifiers, and by adopting the proposed CICF algorithm, we can significantly reduce the misclassification cost of a CS classifier in noisy environments.
[pattern classification, Costs, Machine learning algorithms, effective cost-sensitive learning, Noise reduction, cost-sensitive classification, cost-guided class noise handling, Data mining, Computer science, cost-guided iterative classification filter, Filters, noise identification, Working environment noise, Machine learning, noise, Decision trees, learning (artificial intelligence), Classification tree analysis]
Dynamic classifier selection for effective mining from noisy data streams
Fourth IEEE International Conference on Data Mining
None
2004
Mining from data streams has become an important and challenging task for many real-world applications such as credit card fraud protection and sensor networking. One popular solution is to separate stream data into chunks, learn a base classifier from each chunk, and then integrate all base classifiers for effective classification. In this paper, we propose a dynamic classifier selection (DCS) mechanism to integrate base classifiers for effective mining from data streams. The proposed algorithm dynamically selects a single "best" classifier to classify each test instance at run time. Our scheme uses statistical information from attribute values, and uses each attribute to partition the evaluation set into disjoint subsets, followed by a procedure that evaluates the classification accuracy of each base classifier on these subsets. Given a test instance, its attribute values determine the subsets that the similar instances in the evaluation set have constructed, and the classifier with the highest classification accuracy on those subsets is selected to classify the test instance. Experimental results and comparative studies demonstrate the efficiency and efficacy of our method. Such a DCS scheme appears to be promising in mining data streams with dramatic concept drifting or with a significant amount of noise, where the base classifiers are likely conflictive or have low confidence.
[pattern classification, Heuristic algorithms, data mining, Credit cards, Partitioning algorithms, Data mining, Application software, dynamic classifier selection, Computer science, Distributed control, noise, base classifier, Protection, noisy data stream mining, Testing]
Using emerging patterns and decision trees in rare-class classification
Fourth IEEE International Conference on Data Mining
None
2004
The problem of classifying rarely occurring cases is faced in many real life applications. The scarcity of the rare cases makes it difficult to classify them correctly using traditional classifiers. In this paper, we propose an approach to use emerging patterns (EPs) (G. Dong and J. Li, 1999) and decision trees (DTs) in rare-class classification (EPDT). EPs are those itemsets whose supports in one class are significantly higher than their supports in the other classes. EPDT employs the power of EPs to improve the quality of rare-case classification. To achieve this aim, we first introduce the idea of generating nonexisting rare-class instances, and then we over-sample the most important rare-class instances. Our experiments show that EPDT outperforms many classification methods.
[pattern classification, Law, Encoding, Application software, Data mining, Computer science, Itemsets, decision trees, rare-class classification, Decision trees, Legal factors, Classification tree analysis, Software engineering, emerging patterns]
Discovery of functional relationships in multi-relational data using inductive logic programming
Fourth IEEE International Conference on Data Mining
None
2004
ILP systems have been largely applied to data mining classification tasks with a considerable success. The use of ILP systems in regression tasks has been far less successful. Current systems have very limited numerical reasoning capabilities, which limits the application of ILP to discovery of functional relationships of numeric nature. This paper proposes improvements in numerical reasoning capabilities of ILP systems for dealing with regression tasks. It proposes the use of statistical-based techniques like model validation and model selection to improve noise handling and it introduces a search stopping criterium based on the PAC method to evaluate learning performance. We have found these extensions essential to improve on results over machine learning and statistical-based algorithms used in the empirical evaluation study.
[PAC method, Machine learning algorithms, statistical-based techniques, data mining, search stopping, regression analysis, Biochemistry, Data mining, Proposals, model validation, regression tasks, Cost function, numerical reasoning, learning (artificial intelligence), inductive logic programming, Logic programming, model selection, relational databases, inference mechanisms, machine learning, Root mean square, functional relationship discovery, Machine learning, noise handling, multirelational data, Iterative algorithms, Impedance]
Attribute measurement policies for time and cost sensitive classification
Fourth IEEE International Conference on Data Mining
None
2004
Attribute measurement is an important component of classification algorithms, which could limit their applicability in realtime settings. The time taken to assign a value to an unknown attribute may reduce the overall utility of the final result. We identify three different costs that must be considered, including a time sensitive utility function. We model this attribute measurement problem as a Markov decision process (MDP), and build a policy to control this process using AO* heuristic search. The results offer a cost-effective approach to attribute measurement and classification for a variety of realtime applications.
[attribute measurement, pattern classification, Information resources, cost sensitive classification, time sensitive classification, AO* heuristic search, Process control, Quality of service, Time measurement, Classification algorithms, Computer science, realtime applications, Current measurement, Training data, real-time systems, Machine learning, Markov processes, Cost function, Markov decision process, search problems]
Detecting patterns of appliances from total load data using a dynamic programming approach
Fourth IEEE International Conference on Data Mining
None
2004
Nonintrusive appliance load monitoring (NIALM) systems require sufficient accurate total load data to separate the load into its major appliances. The most available solutions separate the whole electric energy consumption based on the measurement of all three voltages and currents. Aside from the cost for special measuring devices, the intrusion into the local installation is the main problem for reaching a high market distribution. The use of standard digital electricity meters could avoid this problem but the loss of information of the measured data has to be compensated by more intelligent algorithms and implemented rules to disaggregate the total load trace of only the active power measurements. The paper presents a NIALM approach to analyse data, collected from a standard digital electricity meter. To disaggregate the consumption of the entire active power into its major electrical end uses, an algorithm consisting of clustering methods, a genetic algorithm and a dynamic programming approach is presented. The genetic algorithm is used to combine frequently occurring events to create hypothetical finite state machines to model detectable appliances. The time series of each finite state machine is optimized using a dynamic programming method similar to the viterbi algorithm.
[Energy consumption, nonintrusive appliance load monitoring, clustering method, appliance pattern detection, digital electricity meter, finite state machines, Watthour meters, power consumption, domestic appliances, time series optimization, Genetic algorithms, genetic algorithm, Home appliances, Power measurement, Electric variables measurement, Dynamic programming, Monitoring, dynamic programming, time series, genetic algorithms, power engineering computing, load management, electric energy consumption, Current measurement, pattern clustering, Automata, load forecasting, viterbi algorithm]
Text classification by boosting weak learners based on terms and concepts
Fourth IEEE International Conference on Data Mining
None
2004
Document representations for text classification are typically based on the classical bag-of-words paradigm. This approach comes with deficiencies that motivate the integration of features on a higher semantic level than single words. In this paper we propose an enhancement of the classical document representation through concepts extracted from background knowledge. Boosting is used for actual classification. Experimental evaluations on two well known text corpora support our approach through consistent improvement of the results.
[Tree data structures, text analysis, Ontologies, Boosting, Data engineering, Information retrieval, Knowledge management, document representations, text classification, Data mining, classification, bag-of-words paradigm, Learning systems, weak learner boosting, Text categorization, Frequency, ontologies (artificial intelligence)]
Matching in frequent tree discovery
Fourth IEEE International Conference on Data Mining
None
2004
Various definitions and frameworks for discovering frequent trees in forests have been developed. At the heart of these frameworks lies the notion of matching, which determines when a pattern tree matches a tree in a data set. We introduce a notion of tree matching for use in frequent tree mining and we show that it generalizes the framework of Zaki while still being more specific than that of Termier et al. Furthermore, we show how Zaki's TreeMinerV algorithm can be adapted towards our notion of tree matching. Experiments show the promise of the approach.
[Heart, pattern matching, Formal languages, trees (mathematics), data mining, frequent tree discovery, tree matching, Tree graphs, Databases, TreeMinerV algorithm, Machine learning, Web mining, Frequency, tree mining, Pattern matching]
A biobjective model to select features with good classification quality and low cost
Fourth IEEE International Conference on Data Mining
None
2004
In this paper we address a multigroup classification problem in which we want to take into account, together with the generalization ability, costs associated with the features. This cost is not limited to an economical payment, but can also refer to risk, computational effort, space requirements, etc. In order to get a good generalization ability, we use support vector machines (SVM) as the basic mechanism by considering the maximization of the margin. We formulate the problem as a biobjective mixed integer problem, for which Pareto optimal solutions can be obtained.
[Pareto optimal solutions, pattern classification, Pareto optimisation, Costs, support vector machines, generalization ability, SVM, Medical diagnosis, Data mining, generalisation (artificial intelligence), biobjective mixed integer problem, Support vector machines, multigroup classification, Support vector machine classification, Economic forecasting, Medical tests, feature selection, Testing]
Incremental mining of frequent XML query patterns
Fourth IEEE International Conference on Data Mining
None
2004
The discovering of frequent XML query patterns gains its focus due to its many applications in XML data management, and several algorithms have been proposed to discover frequent query patterns using the frequent structure mining techniques. In this paper we consider the problem of incremental mining of frequent XML query patterns. We propose a method to minimize the I/O and computation requirements for handling incremental updates.
[Software algorithms, data mining, trees (mathematics), XML data management, frequent XML query patterns, Encoding, Knowledge management, Transaction databases, Application software, Data mining, database management systems, Computer science, query processing, XML, Frequency, Internet, incremental mining]
Spam filtering using a Markov random field model with variable weighting schemas
Fourth IEEE International Conference on Data Mining
None
2004
In this paper we present a Markov random field model based approach to filter spam. Our approach examines the importance of the neighborhood relationship (MRF cliques) among words in an email message for the purpose of spam classification. We propose and test several different theoretical bases for weighting schemes among corresponding neighborhood windows. Our results demonstrate that unexpected side effects depending on the neighborhood window size may have larger accuracy impact than the neighborhood relationship effects of the Markov random field.
[pattern classification, Filtering, Unsolicited electronic mail, variable weighting schemas, unsolicited e-mail, information filtering, Markov random field, Markov random fields, spam classification, Filters, Computer hacking, Bayesian methods, Text categorization, Markov processes, Polynomials, spam filtering, Random variables, email, Testing]
An adaptive learning approach for noisy data streams
Fourth IEEE International Conference on Data Mining
None
2004
Two critical challenges typically associated with mining data streams are concept drift and data contamination. To address these challenges, we seek learning techniques and models that are robust to noise and can adapt to changes in timely fashion. We approach the stream-mining problem using a statistical estimation framework, and propose a fast and robust discriminative model for learning noisy data streams. We build an ensemble of classifiers to achieve timely adaptation by weighting classifiers in a way that maximizes the likelihood of the data. We further employ robust statistical techniques to alleviate the problem of noise sensitivity. Experimental results on both synthetic and real-life data sets demonstrate the effectiveness of this model learning approach.
[robust statistical techniques, pattern classification, statistical estimation, noisy data streams, data mining, Telecommunication traffic, Data mining, adaptive learning, stream mining, Contamination, Computer science, Working environment noise, Voting, concept drift, data contamination, Traffic control, noise, Noise robustness, learning (artificial intelligence), statistical analysis, Bagging, Monitoring, noise sensitivity]
Scalable multi-relational association mining
Fourth IEEE International Conference on Data Mining
None
2004
We propose the RADAR technique for multirelational data mining. This permits the mining of very large collections and provides a technique for discovering multirelational associations. Results show that RADAR is reliable and scalable for mining a large yeast homology collection, and that it does not have the main-memory scalability constraints of the Farmer and Warmr tools.
[Vocabulary, yeast homology collection mining, Costs, Scalability, data mining, Information retrieval, Data mining, relational databases, Information technology, scalable multirelational association mining, Computer science, multirelational data mining, multirelational association discovery, Radar, Search engines, Bioinformatics, RADAR technique]
An evaluation of approaches to classification rule selection
Fourth IEEE International Conference on Data Mining
None
2004
In this paper a number of classification rule evaluation measures are considered. In particular the authors review the use of a variety of selection techniques used to order classification rules contained in a classifier, and a number of mechanisms used to classify unseen data. The authors demonstrate that rule ordering founded on the size of antecedent works well given certain conditions.
[Computer science, classification rule ordering, System testing, pattern classification, data mining, Chromium, Association rules, Data mining, Classification rule evaluation measures, classification rule selection, classification rule evaluation measures]
Mining frequent closed patterns in microarray data
Fourth IEEE International Conference on Data Mining
None
2004
Microarray data typically contains a large number of columns and a small number of rows, which poses a great challenge for existing frequent (closed) pattern mining algorithms that discover patterns in item enumeration space. In this paper, we propose two algorithms that explore the row enumeration space to mine frequent closed patterns. Several experiments on real-life gene expression data show that the algorithms are faster than existing algorithms, including CLOSET, CHARM, CLOSET+ and CARPENTER.
[System testing, microarray data, row enumeration space, data mining, frequent closed pattern mining, Drives, pattern discovery, Gene expression, Association rules, Data mining, real-life gene expression data, Databases, spatial data structures, item enumeration space]
Clustering on demand for multiple data streams
Fourth IEEE International Conference on Data Mining
None
2004
In the data stream environment, the patterns generated by the mining techniques are usually distinct at different time because of the evolution of data. In order to deal with various types of multiple data streams and to support flexible mining requirements, we devise in this paper a clustering on demand framework, abbreviated as COD framework, to dynamically cluster multiple data streams. While providing a general framework of clustering on multiple data streams, the COD framework has two major features, namely one data scan for online statistics collection and compact multiresolution approximations, which are designed to address, respectively, the time and the space constraints in a data stream environment. Furthermore, with the multiresolution approximations of data streams, flexible clustering demands can be supported.
[time constraints, data mining, data evolution, compact multiresolution approximations, Data mining, Association rules, Statistics, flexible mining requirements, Multiresolution analysis, multiple data streams, Aggregates, pattern clustering, online statistics collection, Investments, space constraints, Clustering algorithms, Frequency, pattern generation, computational complexity, clustering-on-demand]
Extensible Markov model
Fourth IEEE International Conference on Data Mining
None
2004
A Markov chain is a popular data modeling tool. This paper presents a variation of Markov chain, namely extensible Markov model (EMM). By providing a dynamically adjustable structure, EMM overcomes the problems caused by the static nature of the traditional Markov chain. Therefore, EMMs are particularly well suited to model spatiotemporal data such as network traffic, environmental data, weather data, and automobile traffic. Performance studies using EMMs for spatiotemporal prediction problems show the advantages of this approach.
[Weather forecasting, Telecommunication traffic, Automobiles, Power system modeling, Computer science, data models, Markov chain, extensible Markov model, spatiotemporal data modeling, Training data, Traffic control, Cities and towns, Markov processes, Spatiotemporal phenomena, data modeling tool, spatiotemporal prediction problems, Biomedical engineering]
Using representative-based clustering for nearest neighbor dataset editing
Fourth IEEE International Conference on Data Mining
None
2004
The goal of dataset editing in instance-based learning is to remove objects from a training set in order to increase the accuracy of a classifier. For example, Wilson editing removes training examples that are misclassified by a nearest neighbor classifier so as to smooth the shape of the resulting decision boundaries. This paper revolves around the use of representative-based clustering algorithms for nearest neighbor dataset editing. We term this approach supervised clustering editing. The main idea is to replace a dataset by a set of cluster prototypes. A clustering approach called supervised clustering is introduced for this purpose. Our empirical evaluation using eight UCI datasets shows that both Wilson and supervised clustering editing improve accuracy on more than 50% of the datasets tested. However, supervised clustering editing achieves four times higher compression rates than Wilson editing.
[Algorithm design and analysis, pattern classification, Shape, combinatorial mathematics, H infinity control, Data mining, representative-based clustering, instance-based learning, Nearest neighbor searches, Computer science, nearest neighbor dataset editing, Impurities, pattern clustering, Clustering algorithms, Prototypes, Wilson editing, learning (artificial intelligence), supervised clustering editing, Testing]
Decision tree evolution using limited number of labeled data items from drifting data streams
Fourth IEEE International Conference on Data Mining
None
2004
Most previously proposed mining methods on data streams make an unrealistic assumption that "labelled" data stream is readily available and can be mined at anytime. However, in most real-world problems, labelled data streams are rarely immediately available. Due to this reason, models are reconstructed only when labelled data become available periodically. This passive stream mining model has several drawbacks. We propose a concept of demand-driven active data mining. In active mining, the loss of the model is either continuously guessed without using any true class labels or estimated, whenever necessary, from a small number of instances whose actual class labels are verified by paying an affordable cost. When the estimated loss is more than a tolerable threshold, the model evolves by using a small number of instances with verified true class labels. Previous work on active mining concentrates on error guess and estimation. In this paper, we discuss several approaches on decision tree evolution.
[Estimation error, Costs, Change detection algorithms, drifting data streams, data mining, Data warehouses, Educational institutions, Credit cards, Data mining, Engines, decision tree evolution, data stream mining, demand-driven active data mining, labeled data, Production, decision trees, Decision trees]
A machine learning approach to improve congestion control over wireless computer networks
Fourth IEEE International Conference on Data Mining
None
2004
In this paper, we present the application of machine learning techniques to the improvement of the congestion control of TCP in wired/wireless networks. TCP is sub-optimal in hybrid wired/wireless networks because it reacts in the same way to losses due to congestion and losses due to link errors. We thus propose to use machine learning techniques to build automatically a loss classifier from a database obtained by simulations of random network topologies. Several machine learning algorithms are compared for this task and the best method for this application turns out to be decision tree boosting. It outperforms ad hoc classifiers proposed in the networking literature.
[TCP, Machine learning algorithms, telecommunication congestion control, random network topologies, wireless computer networks, loss classifier, Databases, Network topology, Wireless networks, wired network, Automatic control, Computer networks, Decision trees, learning (artificial intelligence), pattern classification, computer networks, decision tree boosting, telecommunication network topology, congestion control, Boosting, Application software, machine learning, link errors, Machine learning, decision trees]
LOADED: link-based outlier and anomaly detection in evolving data sets
Fourth IEEE International Conference on Data Mining
None
2004
In this paper, we present LOADED, an algorithm for outlier detection in evolving data sets containing both continuous and categorical attributes. LOADED is a tunable algorithm, wherein one can trade off computation for accuracy so that domain-specific response times are achieved. Experimental results show that LOADED provides very good detection and false positive rates, which are several times better than those of existing distance-based schemes.
[Dictionaries, Engineering profession, data analysis, evolving data sets, Data engineering, Extraterrestrial measurements, Cleaning, anomaly detection, Biomedical measurements, LOADED, link-based outlier, Delay, Computer science, Couplings, Intrusion detection]
SVD based term suggestion and ranking system
Fourth IEEE International Conference on Data Mining
None
2004
In this paper, we consider the application of the singular value decomposition (SVD) to a search term suggestion system in a pay-for-performance search market. We propose a positive and negative refinement method based on orthogonal subspace projections. We demonstrate that SVD subspace-based methods: 1) expand coverage by reordering the results, and 2) enhance the clustered structure of the data. The numerical experiments reported in this paper were performed on Overture's pay-per-performance search market data.
[ranking system, clustered data structure, orthogonal subspace projections, search term suggestion, Educational institutions, Information retrieval, Large scale integration, Spatial databases, advertising, Sparse matrices, Search engines, Frequency, Bipartite graph, pay-for-performance search market, query formulation, singular value decomposition, search problems, Singular value decomposition, Indexing]
The anatomy of a hierarchical clustering engine for Web-page, news and book snippets
Fourth IEEE International Conference on Data Mining
None
2004
In this paper, we investigate the Web snippet hierarchical clustering problem in its full extent by devising an algorithmic solution, and a software prototype called SnakeT (accessible at http://roquefort.di.unipi.it/), that: (1) draws the snippets from 16 Web search engines, the Amazon collection of books a9.com, the news of Google News and the blogs of Blogline; (2) builds the clusters on-the-fly (ephemeral clustering (Maarek et al., 2000)) in response to a user query without adopting any predefined organization in categories; (3) labels the clusters with sentences of variable length, drawn from the snippets and possibly missing some terms, provided they are not too many; (4) uses some ranking functions which exploit two knowledge bases properly built by our engine at preprocessing time for the sentences selection and cluster-assignment process; (5) organizes the clusters into a hierarchy, and assigns to the nodes intelligible sentences in order to allow post-navigation for query refinement. Our clustering algorithm possibly let the clusters overlap at different levels of the hierarchy.
[Blogline, query refinement, search engines, Data mining, knowledge bases, Software architecture, hierarchical clustering engine, Clustering algorithms, knowledge based systems, ephemeral clustering, Search engines, Amazon collection, Books, Software algorithms, Blogs, information retrieval, SnakeT, Anatomy, sentences selection, Surges, cluster assignment, Web search engines, pattern clustering, book snippets, Web pages, Web page, clusters on-the-fly, Google News, ranking functions, Web sites]
Query-driven support pattern discovery for classification learning
Fourth IEEE International Conference on Data Mining
None
2004
We propose a query-driven lazy learning algorithm which attempts to discover useful local patterns, called support patterns, for classifying a given query. The learning is customized to the query to avoid the horizon effect. We show that this query-driven learning algorithm can guarantee to discover all support patterns with perfect expected accuracy in polynomial time. The experimental results on benchmark data sets also demonstrate that our learning algorithm really has prominent learning performance.
[pattern classification, classification learning, data mining, Partitioning algorithms, Classification algorithms, query-driven lazy learning, query processing, query-driven support pattern discovery, Training data, Systems engineering and theory, Polynomials, Iterative algorithms, Decision trees, learning (artificial intelligence), Research and development management, Classification tree analysis, Testing]
Evolutionary algorithms for clustering gene-expression data
Fourth IEEE International Conference on Data Mining
None
2004
This work deals with the problem of automatically finding optimal partitions in bioinformatics datasets. We propose incremental improvements for a clustering genetic algorithm (CGA) culminating in the evolutionary algorithm for clustering (EAC). The CGA and its modified versions are evaluated in five gene-expression datasets, showing that the proposed EAC is a promising tool for clustering gene-expression data.
[Algorithm design and analysis, biology, Evolutionary computation, evolutionary algorithms, Encoding, Pattern recognition, Partitioning algorithms, genetic algorithms, Gene expression, Genetic algorithms, Design optimization, pattern clustering, Clustering algorithms, gene-expression data clustering, bioinformatics, clustering genetic algorithm, Bioinformatics]
Mining ratio rules via principal sparse non-negative matrix factorization
Fourth IEEE International Conference on Data Mining
None
2004
Association rules are traditionally designed to capture statistical relationship among itemsets in a given database. To additionally capture the quantitative association knowledge, Korn et al. (1998) proposed a paradigm named ratio rules for quantifiable data mining. However, their approach is mainly based on principle component analysis (PCA) and as a result, it cannot guarantee that the ratio coefficient is nonnegative. This may lead to serious problems in the rules' application. In this paper, we propose a method, called principal sparse nonnegative matrix factorization (PSNMF), for learning the associations between itemsets in the form of ratio rules. In addition, we provide a support measurement to weigh the importance of each rule for the entire dataset.
[Weight measurement, Dairy products, data mining, association rules, Transaction databases, matrix decomposition, Sparse matrices, Association rules, Data mining, quantitative association knowledge, support measurement, Convergence, Bridges, Itemsets, ratio rules mining, quantifiable data mining, principle component analysis, principal sparse nonnegative matrix factorization, sparse matrices, principal component analysis, Principal component analysis]
Feature selection via supervised model construction
Fourth IEEE International Conference on Data Mining
None
2004
ReliefF is a feature mining technique, which has been successfully used in data mining applications. However, ReliefF is sensitive to the definition of relevance that is used in its implementation and when handling a large data set, it is computationally expensive. This paper presents an optimisation (feature selection via supervised model construction) for data transformation and starter selection, and evaluates its effectiveness with C4.5. Experiments indicate that the proposed method gave improvement of computation efficiency whilst maintaining classification accuracy of trial data sets.
[Costs, Neodymium, data mining, Context awareness, supervised model construction, Data engineering, Mathematics, Electronic mail, Data mining, classification, ReliefF, Feature Selection, Training data, starter selection, Computational efficiency, Noise robustness, data transformation, feature selection, feature mining]
Mining generalized substructures from a set of labeled graphs
Fourth IEEE International Conference on Data Mining
None
2004
The problem of mining frequent itemsets in transactional data has been studied frequently and has yielded several algorithms that can find the itemsets within a limited amount of time. Some of them can derive "generalized" frequent itemsets consisting of items at any level of a taxonomy (Srikant and Agrawal, 1995). Several approaches have been proposed to mine frequent substructures (patterns) from a set of labeled graphs. The graph mining approaches are easily extended to mine generalized patterns where some vertices and/or edges have labels at any level of a taxonomy of the labels by extending the definition of "subgraph". However, the extended method outputs a massive set of the patterns most of which are over-generalized, which causes computation explosion. In this paper, an efficient method is proposed to discover all frequent patterns which are not over-generalized from labeled graphs, when taxonomies on vertex and edge labels are available.
[pattern mining, Taxonomy, Laboratories, graph theory, data mining, Explosions, Data mining, Nitrogen, frequent substructure mining, Chemical compounds, generalized substructure mining, frequent itemset mining, labeled graphs, graph mining, Itemsets, Tree graphs, Bonding, Carbon compounds]
Divide and prosper: comparing models of customer behavior from populations to individuals
Fourth IEEE International Conference on Data Mining
None
2004
This paper compares customer segmentation, 1-to-1, and aggregate marketing approaches across a broad range of experimental settings, including multiple segmentation levels, marketing datasets, dependent variables, and different types of classifiers, segmentation techniques, and predictive measures. Our experimental results show that, overall, 1-to-1 modeling significantly outperforms the aggregate approach among high-volume customers and is never worse than aggregate approach among low-volume customers. Moreover, the best segmentation techniques tend to outperform 1-to-l modeling among low-volume customers.
[Demography, aggregate marketing, Predictive models, consumer behaviour, Data mining, History, Statistics, customer segmentation, Aggregates, pattern clustering, 1-to-1 modeling, Machine learning, customer behavior, Context modeling]
Filling-in missing objects in orders
Fourth IEEE International Conference on Data Mining
None
2004
Filling-in techniques are important, since missing values frequently appear in real data. Such techniques have been established for categorical or numerical values. Though lists of ordered objects are widely used as representational forms (e.g., Web search results, best-seller lists), filling-in techniques for orders have received little attention. We therefore propose a simple but effective technique to fill-in missing objects in orders. We built this technique into our collaborative filtering system.
[Filtering, collaborative filtering system, Collaboration, Search engines, Marketing and sales, information filtering, data handling, Electronic commerce, order filling-in, Web search]
Orthogonal decision trees
Fourth IEEE International Conference on Data Mining
None
2004
This paper introduces orthogonal decision trees that offer an effective way to construct a redundancy-free, accurate, and meaningful representation of large decision-tree-ensembles often created by popular techniques such as bagging, boosting, random forests and many distributed and data stream mining algorithms. Orthogonal decision trees are functionally orthogonal to each other and they correspond to the principal components of the underlying function space. This paper offers a technique to construct such trees based on eigen-analysis of the ensemble and offers experimental results to document the performance of orthogonal trees on grounds of accuracy and model complexity.
[Stacking, data mining, Boosting, Covariance matrix, Data mining, eigen analysis, eigenvalues and eigenfunctions, model complexity, Computer science, orthogonal decision trees, Machine learning, decision trees, Decision trees, Time factors, Bagging, Monitoring, computational complexity]
Integrating multi-objective genetic algorithms into clustering for fuzzy association rules mining
Fourth IEEE International Conference on Data Mining
None
2004
In this paper, we propose an automated method to decide on the number of fuzzy sets and for the autonomous mining of both fuzzy sets and fuzzy association rules. We compare the proposed multiobjective GA based approach with: 1) CURE based approach; 2) Chien et al. (2001) clustering approach. Experimental results on JOOK transactions extracted from the adult data of United States census in year 2000 show that the proposed method exhibits good performance over the other two approaches in terms of runtime, number of large itemsets and number of association rules.
[multiobjective genetic algorithms, Humans, fuzzy set theory, data mining, genetic algorithms, Association rules, Data mining, Genetic algorithms, Computer science, Fuzzy sets, Runtime, Itemsets, pattern clustering, Clustering algorithms, fuzzy association rules mining, Field-flow fractionation, clustering, fuzzy set mining]
Feature-based prediction of unknown preferences for nearest-neighbor collaborative filtering
Fourth IEEE International Conference on Data Mining
None
2004
Recommendation systems analyze user preferences and recommend items to a user by predicting the user's preference for those items. Among various kinds of recommendation methods, collaborative filtering (CF) has been widely used and successfully applied to practical applications. However, collaborative filtering has two inherent problems: data sparseness and the cold-start problems. In this paper, we propose a method of integrating additional feature information of users and items into CF to overcome the difficulties caused by sparseness and improve the accuracy of recommendation. Several experimental results that show the effectiveness of the proposed method are also presented.
[cold-start problem, unknown preferences, recommendation systems, Filtering, nearest-neighbor collaborative filtering, data sparseness, feature-based prediction, International collaboration, information filtering, Data mining]
GREW - a scalable frequent subgraph discovery algorithm
Fourth IEEE International Conference on Data Mining
None
2004
Existing algorithms that mine graph datasets to discover patterns corresponding to frequently occurring subgraphs can operate efficiently on graphs that are sparse, contain a large number of relatively small connected components, have vertices with low and bounded degrees, and contain well-labeled vertices and edges. However, for graphs that do not share these characteristics, these algorithms become highly unscalable. In this paper we present a heuristic algorithm called GREW to overcome the limitations of existing complete or heuristic frequent subgraph discovery algorithms. GREW is designed to operate on a large graph and to find patterns corresponding to connected subgraphs that have a large number of vertex-disjoint embeddings. Our experimental evaluation shows that GREW is efficient, can scale to very large graphs, and find non-trivial patterns.
[Algorithm design and analysis, Military computing, Heuristic algorithms, Laboratories, Government, graph theory, vertex-disjoint embedding, data mining, frequent subgraph, Data engineering, heuristic frequent subgraph discovery, graph datasets, Computer science, graph mining, Runtime, High performance computing, Performance analysis, frequent pattern discovery, GREW]
Predicting density-based spatial clusters over time
Fourth IEEE International Conference on Data Mining
None
2004
Most of existing clustering algorithms are designed to discover snapshot clusters that reflect only the current status of a database. Snapshot clusters do not reveal the fact that clusters may either persist over a period of time, or slowly fade away as other clusters may gradually develop. Predicting dynamic cluster evolutions and their occurring periods are important because this information can guide users to prepare appropriate actions toward the right areas during the right time for the most effective results. In this paper we developed a simple but effective approach in predicting the future distance among object pairs. Objects that will be close in distance over different periods of time are then processed to discover density-based clusters that may occur or change over time.
[Algorithm design and analysis, Missiles, clustering algorithm, Software algorithms, data mining, Spatial databases, density-based spatial cluster, Air traffic control, snapshot cluster discovery, Weapons, pattern clustering, cluster evolution prediction, Clustering algorithms, Cities and towns, Computational efficiency, statistical analysis, Software engineering]
Dynamic daily-living patterns and association analyses in tele-care systems
Fourth IEEE International Conference on Data Mining
None
2004
Tele-care systems aim to carry out intelligent analyses of a person's wellbeing using data about their daily activities. This is a very challenging task because the massive dataset is likely to be erroneous, possibly with misleading sections due to noise or missing values. Furthermore, the interpretation of the data is highly sensitive to the lifestyle of the monitored person and the environment in which they interact. In our tele-care project, sensor-network domain knowledge is used to overcome the difficulties of monitoring long-term wellbeing with an imperfect data source. In addition, a fuzzy association analysis is leveraged to implement a dynamic and flexible analysis over individual- and environment-dependent data.
[Infrared detectors, telemedicine, Data analysis, data analysis, Data engineering, Mathematics, telecare systems, sensor-network domain knowledge, Intelligent sensors, Information analysis, Working environment noise, environment-dependent data, Infrared sensors, Pattern analysis, daily-living patterns, individual-dependent data, Monitoring, health care, fuzzy association analysis]
Mining temporal patterns without predefined time windows
Fourth IEEE International Conference on Data Mining
None
2004
This paper proposes algorithms for discovering temporal patterns without predefined time windows. The problem of discovering temporal patterns is divided into two sub-tasks: (1) using "cheap statistics" for dependence testing and candidates removal, (2) identifying the temporal relationships between dependent event types. The dependence problem is formulated as the problem of comparing two probability distributions and is solved using a technique reminiscent of the distance methods used in spatial point process, while the latter problem is solved using an approach based on chi-squared tests. Experiments are conducted to evaluate the effectiveness and scalability of the proposed methods.
[pattern classification, temporal pattern mining, dependence problem, data mining, cheap statistics, Data mining, statistical distributions, candidates removal, temporal pattern discovery, probability distributions, distance method, predefined time windows, chi-squared test, dependence testing]
Classifying biomedical citations without labeled training examples
Fourth IEEE International Conference on Data Mining
None
2004
In this paper we introduce a novel technique for classifying text citations without labeled training examples. We first utilize the search results of a general search engine as original training data. We then proposed a mutually reinforcing learning algorithm (MRL) to mine the classification knowledge and to "clean" the training data. With the help of a set of established domain-specific ontological terms or keywords, the MRL mining step derives the relevant classification knowledge. The MRL cleaning step then builds a naive Bayes classifier based on the mined classification knowledge and tries to clean the training set. The MRL algorithm is iteratively applied until a clean training set is obtained. We show the effectiveness of the proposed technique in the classification of biomedical citations from a large medical literature database.
[domain-specific ontologies, search engine, mutually reinforcing learning algorithm, Ontologies, Biomedical computing, Cleaning, medical information systems, biomedical citation classification, classification knowledge, classification, Diseases, labeled training examples, naive Bayes classifier, Text categorization, Training data, training data, Search engines, citation analysis, Iterative algorithms, Bayes methods, Labeling, learning (artificial intelligence), Cancer]
Improving the reliability of decision tree and naive Bayes learners
Fourth IEEE International Conference on Data Mining
None
2004
The C4.5 decision tree and naive Bayes learners are known to produce unreliable probability forecasts. We have used simple binning (Zadrozny and Elkan, 2001) and Laplace transform (Cestnik, 2001) techniques to improve the reliability of these learners and compare their effectiveness with that of the newly developed Venn probability machine (VPM) meta-learner (Vovk et al., 2003). We assess improvements in reliability using loss functions, receiver operator characteristic (ROC) curves and empirical reliability curves (ERC). The VPM outperforms the simple techniques to improve reliability, although at the cost of increased computational intensity and slight increase in error rate. These trade-offs are discussed.
[Error analysis, reliability theory, simple binning, receiver operator characteristic curves, naive Bayes learners, decision tree, Venn probability machine meta-learner, unreliable probability forecast, empirical reliability curves, Computational efficiency, Decision trees, learning (artificial intelligence), Laplace transform, Testing, Laplace equations, probability, Pattern recognition, Laplace transforms, Supervised learning, forecasting theory, Machine learning, decision trees, Biology computing, Frequency, Bayes methods]
Revealing true subspace clusters in high dimensions
Fourth IEEE International Conference on Data Mining
None
2004
Subspace clustering is one of the best approaches for discovering meaningful clusters in high dimensional space. One cluster in high dimensional space may be transcribed into multiple distinct maximal clusters by projecting onto different subspaces. A direct consequence of clustering independently in each subspace is an overwhelmingly large set of overlapping clusters which may be significantly similar. To reveal the true underlying clusters, we propose a similarity measurement of the overlapping clusters. We adopt the model of Gaussian tailed hyper-rectangles to capture the distribution of any subspace cluster. A set of experiments on a synthetic dataset demonstrates the effectiveness of our approach. Application to real gene expression data also reveals impressive meta-clusters expected by biologists.
[Algorithm design and analysis, Cluster Intersection, high dimensional space, Merging, Entropy, Data mining, overlapping cluster, Clustering algorithms, Tail, Local Grid, Gene Expression, Biological system modeling, Gaussian tailed hyperrectangles, Gene expression, Adhesion, Computer science, cluster intersection, Gaussian Tails, Adhesives, pattern clustering, subspace clustering, similarity measurement, Gaussian processes, statistical analysis, Overlapping Cluster, gene expression, Subspace Clustering]
An adaptive density-based clustering algorithm for spatial database with noise
Fourth IEEE International Conference on Data Mining
None
2004
Clustering spatial data has various applications. Several clustering algorithms have been proposed to cluster objects in spatial databases. Spatial object distribution has significant effect on the results of clustering. Few of current algorithms consider the distribution of objects while processing clusters. In this paper, we propose an adaptive density-based clustering algorithm, ADBC, which uses a novel adaptive strategy for neighbor selection based on spatial object distribution to improve clustering accuracy. We perform a series of experiments on simulated data sets and real data sets. A comparison with DBSCAN and OPTICS shows the superiority of our new approach.
[Algorithm design and analysis, adaptive density-based clustering, visual databases, neighbor selection, Data engineering, Spatial databases, Partitioning algorithms, Application software, Military satellites, spatial object distribution, Information analysis, Computer science, spatial database, pattern clustering, Clustering algorithms, Optical noise]
Finding constrained frequent episodes using minimal occurrences
Fourth IEEE International Conference on Data Mining
None
2004
Recurrent combinations of events within an event sequence, known as episodes, often reveal useful information. Most of the proposed episode mining algorithms adopt an apriori-like approach that generates candidates and then calculates their support levels. Obviously, such an approach is computationally expensive. Moreover, those algorithms are capable of handling only a limited range of constraints. In this paper, we introduce two mining algorithms - episode prefix tree (EPT) and position pairs set (PPS) - based on a prefix-growth approach to overcome the above limitations. Both algorithms push constraints systematically into the mining process. Performance study shows that the proposed algorithms run considerably faster than MINEPI (Mannila and Toivonen, 1996).
[Scalability, Taxonomy, minimal occurrences, data mining, trees (mathematics), prefix-growth approach, Data mining, episode mining, constrained frequent episode, Computer science, position pairs set, episode prefix tree, Frequency, Iterative algorithms, Explosives, Time factors]
Estimation of false negatives in classification
Fourth IEEE International Conference on Data Mining
None
2004
In many classification problems such as spam detection and network intrusion, a large number of unlabeled test instances are predicted negative by the classifier However, the high costs as well as time constraints on an expert's time prevent further analysis of the "predicted false" class instances in order to segregate the false negatives from the true negatives. A systematic method is thus required to obtain an estimate of the number of false negatives. A capture-recapture based method can be used to obtain an ML-estimate of false negatives when two or more independent classifiers are available. In the case for which independence does not hold, we can apply log-linear models to obtain an estimate of false negatives. However, as shown in this paper, lesser the dependencies among the classifiers, better is the estimate obtained for false negatives. Thus, ideally independent classifiers should be used to estimate the false negatives in an unlabeled dataset. Experimental results on the spam dataset from the UCI machine learning repository are presented.
[Costs, false negative estimation, capture-recapture based method, Humans, network intrusion, unsolicited e-mail, Information management, classification, Computer science, classification problem, Intelligent networks, log-linear model, security of data, spam detection, Intrusion detection, Machine learning, Marketing and sales, Time factors, Testing]
Correlation preserving discretization
Fourth IEEE International Conference on Data Mining
None
2004
Discretization is a crucial preprocessing primitive for a variety of data warehousing and mining tasks. In this article we present a novel PCA-based unsupervised algorithm for the discretization of continuous attributes in multivariate datasets. The algorithm leverages the underlying correlation structure in the dataset to obtain the discrete intervals, and ensures that the inherent correlations are preserved. The approach also extends easily to datasets containing missing values. We demonstrate the efficacy of the approach on real datasets and as a preprocessing step for both classification and frequent item set mining tasks. We also show that the intervals are meaningful and can uncover hidden patterns in data.
[Missing Data, Data preprocessing, data mining, frequent item set mining, Data engineering, data warehousing, Classification algorithms, Data mining, PCA-based unsupervised algorithm, classification, missing data, Computer science, Discrete transforms, Itemsets, multivariate dataset, Warehousing, unsupervised discretization, correlation structure, Decision trees, correlation preserving discretization, data warehouses, principal component analysis, Unsupervised Discretization, Classification tree analysis]
Active feature-value acquisition for classifier induction
Fourth IEEE International Conference on Data Mining
None
2004
Many induction problems include missing data that can be acquired at a cost. For building accurate predictive models, acquiring complete information for all instances is often expensive or unnecessary, while acquiring information for a random subset of instances may not be most effective. Active feature-value acquisition tries to reduce the cost of achieving a desired model accuracy by identifying instances for which obtaining complete information is most informative. We present an approach in which instances are selected for acquisition based on the current model's accuracy and its confidence in the prediction. Experimental results demonstrate that our approach can induce accurate models using substantially fewer feature-value acquisitions as compared to alternative policies.
[Costs, predictive model, Current measurement, Predictive models, Sampling methods, Design for experiments, data acquisition, Data mining, classification, classifier induction, missing data, active feature-value acquisition]
Privacy-sensitive Bayesian network parameter learning
Fourth IEEE International Conference on Data Mining
None
2004
This paper considers the problem of learning the parameters of a Bayesian network, assuming the structure of the network is given, from a privacy-sensitive dataset that is distributed between multiple parties. For a binary-valued dataset, we show that the count information required to estimate the conditional probabilities in a Bayesian network can be obtained as a solution to a set of linear equations involving some inner product between the relevant different feature vectors. We consider a random projection-based method that was proposed elsewhere to securely compute the inner product (with a modified implementation of that method).
[Computers, Data privacy, Terrorism, privacy-sensitive Bayesian network parameter learning, data mining, random projection-based method, Medical services, linear equation, Vectors, Data mining, Sliding mode control, privacy-sensitive dataset, Bayesian methods, Differential equations, conditional probability, Explosives, data privacy, binary-valued dataset, belief networks, learning (artificial intelligence)]
MMSS: multi-modal story-oriented video summarization
Fourth IEEE International Conference on Data Mining
None
2004
We propose multi-modal story-oriented video summarization (MMSS) which, unlike previous works that use fine-tuned, domain-specific heuristics, provides a domain-independent, graph-based framework. MMSS uncovers correlation between information of different modalities which gives meaningful story-oriented news video summaries. MMSS can also be applied for video retrieval, giving performance that matches the best traditional retrieval techniques (OKAPI and LSI), with no fine-tuned heuristics such as tf/idf.
[Content based retrieval, MMSS, graph theory, Information retrieval, Large scale integration, fine-tuned domain-specific heuristics, Data mining, Multimedia communication, domain-independent graph-based framework, Computer science, multi-modal story-oriented video summarization, video retrieval, Broadcasting, image retrieval, Motion pictures, Robustness, Libraries, video signal processing]
A comparative study of linear and nonlinear feature extraction methods
Fourth IEEE International Conference on Data Mining
None
2004
This paper presents theoretical relationships among several generalized LDA algorithms and proposes computationally efficient approaches for them utilizing the relationships. Generalized LDA algorithms are extended nonlinearly by kernel methods resulting in nonlinear discriminant analysis. Performances and computational complexities of these linear and nonlinear discriminant analysis algorithms are compared.
[Algorithm design and analysis, linear feature extraction, Scattering, kernel method, generalized LDA algorithm, Computational complexity, matrix algebra, feature extraction, Chromium, nonlinear discriminant analysis, Feature extraction, nonlinear feature extraction, Eigenvalues and eigenfunctions, Linear discriminant analysis, Performance analysis, linear discriminant analysis, Kernel, Singular value decomposition, computational complexity]
SVM and graphical algorithms: a cooperative approach
Fourth IEEE International Conference on Data Mining
None
2004
We present a cooperative approach using both support vector machine (SVM) algorithms and visualization methods. SVM are widely used today and often give high quality results, but they are used as "black-box" (it is very difficult to explain the obtained results) and cannot treat easily very large datasets. We have developed graphical methods to help the user to evaluate and explain the SVM results. The first method is a graphical representation of the separating frontier quality, it is then linked with other visualization tools to help the user explaining SVM results. The information provided by these graphical methods is also used for SVM parameter tuning, they are then used together with automatic algorithms to deal with very large datasets on standard computers. We present an evaluation of our approach with the UCI and the Kent Ridge Bio-medical data sets.
[graphical algorithm, visualization tool, support vector machines, Displays, Classification algorithms, Data mining, Visual databases, frontier quality, Distributed computing, Support vector machines, Histograms, visualization method, support vector machine, Support vector machine classification, Data visualization, data visualisation, SVM parameter tuning, cooperative approach, Bioinformatics]
RDF: a density-based outlier detection method using vertical data representation
Fourth IEEE International Conference on Data Mining
None
2004
Outlier detection can lead to discovering unexpected and interesting knowledge, which is critical important to some areas such as monitoring of criminal activities in electronic commerce, credit card fraud, etc. In this paper, we developed an efficient density-based outlier detection method for large datasets. Our contributions are: a) we introduce a relative density factor (RDF); b) based on RDF, we propose an RDF-based outlier detection method which can efficiently prune the data points which are deep in clusters, and detect outliers only within the remaining small subset of the data; c) the performance of our method is further improved by means of a vertical data representation, P-trees. We tested our method with NHL and NBA data. Our method shows an order of magnitude speed improvement compared to the contemporary approaches.
[Costs, Neodymium, Computerized monitoring, density-based outlier detection, data mining, trees (mathematics), vertical data representation, relative density factor, RDF-based outlier detection, Data structures, Credit cards, Resource description framework, Electronic commerce, P-trees, credit transactions, Computer science, Surveillance, very large databases, fraud, credit card fraud, Testing, electronic commerce]
Quantitative association rules based on half-spaces: an optimization approach
Fourth IEEE International Conference on Data Mining
None
2004
We tackle the problem of finding association rules for quantitative data. Whereas most of the previous approaches operate on hyper rectangles, we propose a representation based on half-spaces. Consequently, the left-hand side and right-hand side of an association rule does not contain a conjunction of items or intervals, but a weighted sum of variables tested against a threshold. Since the downward closure property does not hold for such rules, we propose an optimization setting for finding locally optimal rules. A simple gradient descent algorithm optimizes a parameterized score function, where iterations optimizing the first separating hyperplane alternate with iterations optimizing the second. Experiments with two real-world data sets show that the approach finds non-random patterns and scales up well. We therefore propose quantitative association rules based on half-spaces as an interesting new class of patterns with a high potential for applications.
[Temperature, gradient descent algorithm, data mining, locally optimal rules, Probability, Association rules, Data mining, downward closure property, quantitative association rules, optimisation, Itemsets, optimization, hyper rectangles, gradient methods, Testing]
Evaluating attraction in spatial point patterns with an application in the field of cultural history
Fourth IEEE International Conference on Data Mining
None
2004
Spatial collocation rules are often useful for describing dependencies between spatial features. Still, the commonly used criteria for the interestingness of the rules and the selected neighbourhood constraints for spatial objects may be too rough for capturing the essentials of such dependencies. We demonstrate the difficulties with concrete examples on a large place-name data set. We propose a technique based on simple density estimation for assessing the interesting-ness with different neighbouring constraints.
[data mining, Rivers, history, Cultural differences, History, Information technology, Couplings, neighbourhood constraint, Character generation, cultural history, Euclidean distance, Lakes, Concrete, spatial point pattern, Artificial intelligence, density estimation, spatial collocation rules]
Efficient relationship pattern mining using multi-relational iceberg-cubes
Fourth IEEE International Conference on Data Mining
None
2004
Multi-relational data mining (MRDM) is concerned with data that contains heterogeneous and semantically rich relationships among various entity types. In this paper, we introduce multi-relational iceberg-cubes (MRI-Cubes) as a scalable approach to efficiently compute data cubes (aggregations) over multiple database relations and, in particular, as mechanisms to compute frequent multi-relational patterns ("item sets"). We also present a summary of performance results of our algorithm.
[Algorithm design and analysis, multirelational iceberg-cubes, Data analysis, data mining, Relational databases, Data mining, Association rules, relational databases, Computer science, Intelligent networks, relationship pattern mining, multiple database relation, multirelational data mining, Database systems, Performance analysis, Materials requirements planning]
Cluster cores-based clustering for high dimensional data
Fourth IEEE International Conference on Data Mining
None
2004
We propose a new approach to clustering high dimensional data based on a novel notion of cluster cores, instead of on nearest neighbors. A cluster core is a fairly dense group with a maximal number of pairwise similar objects. It represents the core of a cluster, as all objects in a cluster are with a great degree attracted to it. As a result, building clusters from cluster cores achieves high accuracy. Other major characteristics of the approach include: (1) It uses a semantics-based similarity measure. (2) It does not incur the curse of dimensionality and is scalable linearly with the dimensionality of data. (3) It outperforms the well-known clustering algorithm, ROCK, with both lower time complexity and higher accuracy.
[semantics-based similarity measure, clustering algorithm, Clustering methods, Laboratories, curse of dimensionality, data mining, time complexity, Time measurement, Data mining, Nearest neighbor searches, Computer science, high dimensional data, Clustering algorithms, Euclidean distance, cluster cores-based clustering, Robustness, computational complexity]
Metric incremental clustering of nominal data
Fourth IEEE International Conference on Data Mining
None
2004
We present an algorithm/or clustering nominal data that is based on a metric on the set of partitions of a finite set of objects; this metric is defined starting from a lower valuation of the lattice of partitions. The proposed algorithm seeks to determine a clustering partition such that the total distance between this partition and the partitions determined by the attributes of the objects has a local minimum. The resulting clustering is quite stable relative to the ordering of the objects.
[Terminology, Shape measurement, Buildings, Lattices, data mining, metric incremental clustering, Partitioning algorithms, Data mining, Cost accounting, Unsupervised learning, clustering partition, Computer science, pattern clustering, Clustering algorithms, nominal data]
On ranking refinements in the step-by-step searching through a product catalogue
Fourth IEEE International Conference on Data Mining
None
2004
In our previous work we have developed a logic-based approach for the refinement of ontology-based queries that enables a user to search through a repository in a step-by-step fashion. Since the set of refinements in a step can be large, they should be ranked according to their relevance for fulfilling a user's need. In this paper we present such a ranking model, which takes into account the information content (informativeness) of a refinement as well as the preferences of the user.
[cataloguing, ontology-based queries, Information resources, Terminology, Navigation, Logic programming, ranking refinements, Taxonomy, Lattices, Ontologies, Data mining, product catalogue, formal logic, logic-based approach, information content, Refining, Chromium, ontologies (artificial intelligence), step-by-step searching, search problems]
Learning conditional independence tree for ranking
Fourth IEEE International Conference on Data Mining
None
2004
Accurate ranking is desired in many real-world data mining applications. Traditional learning algorithms, however, aim only at high classification accuracy. It has been observed that both traditional decision trees and naive Bayes produce good classification accuracy but poor probability estimates. In this paper, we use a new model, conditional independence tree (CITree), which is a combination of decision tree and naive Bayes and more suitable for ranking and more learnable in practice. We propose a novel algorithm for learning CITree for ranking, and the experiments show that the CITree algorithm outperforms the state-of-the-art decision tree learning algorithm C4.4 and naive Bayes significantly in yielding accurate rankings. Our work provides an effective data mining algorithm for applications in which an accurate ranking is required.
[learning algorithm, Error analysis, conditional independence tree, data mining, Probability, accurate ranking, Frequency estimation, Data mining, Application software, Niobium, Computer science, decision trees, naive Bayes method, Chromium, Bayes methods, Decision trees, learning (artificial intelligence), Classification tree analysis]
Supervised latent semantic indexing for document categorization
Fourth IEEE International Conference on Data Mining
None
2004
Latent semantic indexing (LSI) is a successful technology in information retrieval (IR) which attempts to explore the latent semantics implied by a query or a document through representing them in a dimension-reduced space. However, LSI is not optimal for document categorization tasks because it aims to find the most representative features for document representation rather than the most discriminative ones. In this paper, we propose supervised LSI (SLSI) which selects the most discriminative basis vectors using the training data iteratively. The extracted vectors are then used to project the documents into a reduced dimensional space for better classification. Experimental evaluations show that the SLSI approach leads to dramatic dimension reduction while achieving good classification results.
[document handling, indexing, supervised latent semantic indexing, document representation, discriminative basis vectors, information retrieval, Information retrieval, Large scale integration, document categorization, dimension-reduced space, Data mining, Sun, Computer science, Space technology, Asia, Training data, Indexing, Singular value decomposition]
Sparse kernel least squares classifier
Fourth IEEE International Conference on Data Mining
None
2004
In this paper, we propose a new learning algorithm for constructing kernel least squares classifier. The new algorithm adopts a recursive learning way and a novel two-step sparsification procedure is incorporated into learning phase. These two most important features not only provide a feasible approach for large-scale problems as it is not necessary to store the entire kernel matrix, but also produce a very sparse model with fast training and testing time. Experimental results on a number of data classification problems are presented to demonstrate the competitiveness of new proposed algorithm.
[pattern classification, learning algorithm, least squares approximations, recursive learning, Sparse matrices, Sun, Least squares methods, Unsupervised learning, Computer science, Support vector machines, sparse kernel least squares classifier, Support vector machine classification, kernel matrix, large-scale problem, Large-scale systems, learning (artificial intelligence), Kernel, data classification problem, Testing]
Dryade: a new approach for discovering closed frequent trees in heterogeneous tree databases
Fourth IEEE International Conference on Data Mining
None
2004
In this paper we present a novel algorithm for discovering tree patterns in a tree database. This algorithm uses a relaxed tree inclusion definition, making the problem more complex (checking tree inclusion is NP-complete), but allowing to mine highly heterogeneous databases. To obtain good performances, our DRYADE algorithm, discovers only closed frequent tree patterns.
[Tree data structures, relaxed tree inclusion, data mining, trees (mathematics), Data mining, NP-complete problem, tree pattern discovery, Tree graphs, Image databases, XML, closed frequent tree discovery, Frequency, heterogeneous tree databases, Labeling, computational complexity, Dryade]
A greedy algorithm for selecting models in ensembles
Fourth IEEE International Conference on Data Mining
None
2004
We are interested in ensembles of models built over k data sets. Common approaches are either to combine models by vote averaging, or to build a meta-model on the outputs of the local models. In this paper, we consider the model assignment approach, in which a meta-model selects one of the local statistical models for scoring. We introduce an algorithm called greedy data labeling (GDL) that improves the initial data partition by reallocating some data, so that when each model is built on its local data subset, the resulting hierarchical system has minimal error. We present evidence that model assignment may in certain situations be more natural than traditional ensemble learning, and if enhanced by GDL, it often outperforms traditional ensembles.
[Greedy algorithms, greedy algorithms, Cardiac disease, Hierarchical systems, data mining, Predictive models, Boosting, statistical model, Partitioning algorithms, Data mining, ensemble learning, Voting, vote averaging, model assignment approach, greedy data labeling, Sampling methods, Labeling, statistical analysis, greedy algorithm]
Mining Web data to create online navigation recommendations
Fourth IEEE International Conference on Data Mining
None
2004
A system to provide online navigation recommendation for Web visitors is introduced. We call visitor the anonymous user, i.e., when only data about her/his browsing behavior (Web logs) are available. We first apply clustering techniques over a large sample of Web data. Next, from the significant patterns that are discovered, a set of rules about how to use them is created. Finally, comparing the current Web visitor session with the patterns, online navigation recommendations are proposed using the mentioned rules. The system was tested using data from a real Web site, showing its effectiveness.
[System testing, Web page design, Navigation, clustering technique, data mining, Industrial engineering, Web visitors, Research initiatives, Data mining, information filters, anonymous user, Web data mining, online navigation recommendation, browsing behavior, Web pages, Web logs, Internet, Online Communities/Technical Collaboration, Web server]
Alpha Galois lattices
Fourth IEEE International Conference on Data Mining
None
2004
In many applications there is a need to represent a large number of data by clustering them in a hierarchy of classes. Our basic representation is a Galois lattice, a structure that exhaustively represents the whole set of concepts that are distinguishable given the instance set and the representation language. What we propose here is a method to reduce the size of the lattice, and thus simplify our view of the data, while conserving its formal structure and exhaustivity. For that purpose we use a preliminary partition of the instance set, representing the association of a "type" to each instance. By redefining the notion of extent of a term in order to cope, to a certain degree (denoted as /spl alpha/), with this partition, we define a particular family of Galois lattices denoted as alpha Galois lattices. We also discuss the related implication rules defined as inclusion of such /spl alpha/-extents.
[clustering algorithm, Electronic catalog, Lattices, data mining, Frequency, lattice theory, representation language, alpha Galois lattices, Data mining, Association rules, Galois fields, instance set partition]
AGILE: a general approach to detect transitions in evolving data streams
Fourth IEEE International Conference on Data Mining
None
2004
In many applications such as e-commerce, system diagnosis and telecommunication services, data arrives in streams at a high speed. It is common that the underlying process generating the stream may change over time, either as a result of the fundamental evolution or in response to some external stimulus. Detecting these changes is a very challenging problem of great practical importance. The overall volume of the stream usually far exceeds the available main memory and access to the data stream is typically performed via a linear scan in ascending order of the indices of the records. In this paper, we propose a novel approach, AGILE, to monitor streaming data and to detect distinguishable transitions of the underlying processes. AGILE has many advantages over the traditional Hidden Markov Model, e.g., AGILE only requires one scan of the data.
[Process design, hidden Markov model, telecommunication services, Data mining, stream processing, Intrusion detection, data structures, e-commerce, Monitoring, Telecommunication services, Data analysis, Change detection algorithms, data analysis, transition detection, AGILE, streaming data monitoring, Application software, emission tree, Emission tree, Computer science, Hidden Markov models, system diagnosis, Stream processing, Transition detection, evolving data streams, Variable memory Markov model, variable memory Markov model]
Scalable construction of topic directory with nonparametric closed termset mining
Fourth IEEE International Conference on Data Mining
None
2004
A topic directory, e.g., Yahoo directory, provides a view of a document set at different levels of abstraction and is ideal for the interactive exploration and visualization of the document set. We present a method that dynamically generates a topic directory from a document set using a frequent closed termset mining algorithm. Our method shows experimental results of equal quality to recent document clustering methods and has additional benefits such as automatic generation of topic labels and determination of a clustering parameter.
[document handling, Visualization, Clustering methods, Taxonomy, nonparametric closed termset mining, data mining, document clustering, Yahoo directory, Data mining, Organizing, Computer science, Itemsets, Tree graphs, pattern clustering, Clustering algorithms, Permission, topic directory, hierarchical clustering, automatic generation]
Learning weighted naive Bayes with accurate ranking
Fourth IEEE International Conference on Data Mining
None
2004
Naive Bayes is one of most effective classification algorithms. In many applications, however, a ranking of examples are more desirable than just classification. How to extend naive Bayes to improve its ranking performance is an interesting and useful question in practice. Weighted naive Bayes is an extension of naive Bayes, in which attributes have different weights. This paper investigates how to learn a weighted naive Bayes with accurate ranking from data, or more precisely, how to learn the weights of a weighted naive Bayes to produce accurate ranking. We explore various methods: the gain ratio method, the hill climbing method, and the Markov chain Monte Carlo method, the hill climbing method combined with the gain ratio method, and the Markov chain Monte Carlo method combined with the gain ratio method. Our experiments show that a weighted naive Bayes trained to produce accurate ranking outperforms naive Bayes.
[pattern classification, Machine learning algorithms, gain ratio method, hill climbing method, Error analysis, classification algorithm, Classification algorithms, Data mining, Application software, Niobium, Computer science, Monte Carlo methods, Markov chain Monte Carlo method, Bayesian methods, Machine learning, Chromium, Markov processes, Bayes methods, learning (artificial intelligence), weighted naive Bayes learning]
Learning rules from highly unbalanced data sets
Fourth IEEE International Conference on Data Mining
None
2004
This paper presents a simple and effective rule learning algorithm for highly unbalanced data sets. By using the small size of the minority class to its advantage this algorithm can conduct an almost exhaustive search for patterns within the known fraudulent cases. This algorithm was designed for and successfully applied to a law enforcement problem, which involves discovering common patterns of fraudulent transactions.
[Algorithm design and analysis, law administration, Costs, Humans, data mining, Inspection, unbalanced data sets, pattern discovery, Spatial databases, Classification algorithms, Transaction databases, Data mining, law enforcement problem, Law enforcement, learning (artificial intelligence), exhaustive search, fraudulent transaction, Testing, rule learning algorithm]
Relational peculiarity oriented data mining
Fourth IEEE International Conference on Data Mining
None
2004
Peculiarity rules are a new type of interesting rules which can be discovered by searching the relevance among peculiar data. A main task of mining peculiarity rules is the identification of peculiarity. Traditional methods of finding peculiar data are attribute-based approaches. This paper extends peculiarity oriented mining to relational peculiarity oriented mining. Peculiar data are identified on record level, and peculiar rules are mined and explained in a relational mining framework. The results from preliminary experiments show that relational peculiarity oriented mining is very effective.
[Logic programming, Laboratories, data mining, Relational databases, peculiarity rules, Data engineering, Educational institutions, Data mining, relational databases, Computer science, Learning systems, Image databases, Brain modeling, relational peculiarity oriented data mining, attribute-based approach, peculiarity identification]
Welcome Message from the Conference Chairs
Fifth IEEE International Conference on Data Mining
None
2005
Presents the welcome message from the conference proceedings.
[]
Welcome to ICDM 2005
Fifth IEEE International Conference on Data Mining
None
2005
Presents the welcome message from the conference proceedings.
[]
Conference organization
Fifth IEEE International Conference on Data Mining
None
2005
Provides a listing of current committee members and society officers.
[]
Top 10 data mining mistakes
Fifth IEEE International Conference on Data Mining
None
2005
Summary form only given. Data mining is still as much it is an art as a science, and fancy new tools make it easy to do wrong things with one's data even faster. We'll examine the major "cracks in the crystal ball" through case studies, both simple and complex, of (often personal) errors - drawn from real-world consulting engagements. Best practices for data mining will be (accidentally) illuminated by their (rarely described) opposites. These common errors range from allowing anachronistic variables into the pool of candidate inputs, to subtly inflating results through early up-sampling. You'll hear cautionary tales of endangered projects and embarrassed teams-but also the keys to avoiding such a fate yourself.
[complex errors, simple errors, data mining, real-world consulting, anachronistic variables]
Tutorials
Fifth IEEE International Conference on Data Mining
None
2005
Provides an abstract for each of the tutorial presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
Workshops
Fifth IEEE International Conference on Data Mining
None
2005
false
[]
Online hierarchical clustering in a data warehouse environment
Fifth IEEE International Conference on Data Mining
None
2005
Many important industrial applications rely on data mining methods to uncover patterns and trends in large data warehouse environments. Since a data warehouse is typically updated periodically in a batch mode, the mined patterns have to be updated as well. This requires not only accuracy from data mining methods but also fast availability of up-to-date knowledge, particularly in the presence of a heavy update load. To cope with this problem, we propose the use of online data mining algorithms which permanently store the discovered knowledge in suitable data structures and enable an efficient adaptation of these structures after insertions and deletions on the raw data. In this paper, we demonstrate how hierarchical clustering methods can be reformulated as online algorithms based on the hierarchical clustering method OPTICS, using a density estimator for data grouping. We also discuss how this algorithmic schema can be specialized for efficient online single-link clustering. A broad experimental evaluation demonstrates that the efficiency is superior with significant speed-up factors even for large bulk insertions and deletions.
[density estimator, online data mining, Clustering methods, pattern mining, data mining, Data warehouses, Data structures, knowledge discovery, Data mining, Application software, industrial application, Computer science, single-link clustering, pattern clustering, data warehouse environment, Clustering algorithms, Data visualization, data grouping, online hierarchical clustering, Computer industry, data structures, Mining industry, data warehouses, OPTICS method]
eMailSift: eMail classification based on structure and content
Fifth IEEE International Conference on Data Mining
None
2005
In this paper we propose a novel approach that uses structure as well as the content of emails in a folder for email classification. Our approach is based on the premise that representative - common and recurring -structures/patterns can be extracted from a pre-classified email folder and the same can be used effectively for classifying incoming emails. A number of factors that influence representative structure extraction and the classification are analyzed conceptually and validated experimentally. In our approach, the notion of inexact graph match is leveraged for deriving structures that provide coverage for characterizing folder contents. Extensive experimentation validate the selection of parameters and the effectiveness of our approach for email classification.
[eMail structure, pattern classification, inexact graph match, Laboratories, electronic mail, eMail classification, Electronic mail, Data mining, Degradation, structure extraction, Text categorization, Training data, eMail content, Internet, eMailSift]
An empirical Bayes approach to detect anomalies in dynamic multidimensional arrays
Fifth IEEE International Conference on Data Mining
None
2005
We consider the problem of detecting anomalies in data that arise as multidimensional arrays with each dimension corresponding to the levels of a categorical variable. In typical data mining applications, the number of cells in such arrays is usually large. Our primary focus is detecting anomalies by comparing information at the current time to historical data. Naive approaches advocated in the process control literature do not work well in this scenario due to the multiple testing problems - performing multiple statistical tests on the same data produce excessive number of false positives. We use an empirical Bayes method which works by fitting a two component Gaussian mixture to deviations at current time. The approach is scalable to problems that involve monitoring massive number of cells and fast enough to be potentially useful in many streaming scenarios. We show the superiority of the method relative to a naive "per component error rate" procedure through simulation. A novel feature of our technique is the ability to suppress deviations that are merely the consequence of sharp changes in the marginal distributions. This research was motivated by the need to extract critical application information and business intelligence from the daily logs that accompany large-scale spoken dialog systems deployed by AT&T. We illustrate our method on one such system.
[Performance evaluation, Multidimensional systems, Process control, data mining, large-scale spoken dialog systems, anomaly detection, empirical Bayes approach, Data mining, Statistics, business intelligence, Databases, Gaussian processes, Gaussian mixture, Large-scale systems, Bayes methods, Monitoring, Computational intelligence, Testing, dynamic multidimensional arrays]
Classifier fusion using shared sampling distribution for boosting
Fifth IEEE International Conference on Data Mining
None
2005
We present a new framework for classifier fusion that uses a shared sampling distribution for obtaining a weighted classifier ensemble. The weight update process is self regularizing as subsequent classifiers trained on the disjoint views rectify the bias introduced by any classifier in preceding iterations. We provide theoretical guarantees that our approach indeed provides results which are better than the case when boosting is performed separately on different views. The results are shown to outperform other classifier fusion strategies on a well known texture image database.
[Biometrics, pattern classification, weighted classifier ensemble, sampling methods, weight update process, Optimization methods, Color, Boosting, Image databases, Voting, shared sampling distribution, Prototypes, Sampling methods, classifier fusion, Biosensors, Kernel]
Improving automatic query classification via semi-supervised learning
Fifth IEEE International Conference on Data Mining
None
2005
Accurate topical classification of user queries allows for increased effectiveness and efficiency in general-purpose Web search systems. Such classification becomes critical if the system is to return results not just from a general Web collection but from topic-specific back-end databases as well. Maintaining sufficient classification recall is very difficult as Web queries are typically short, yielding few features per query. This feature sparseness coupled with the high query volumes typical for a large-scale search service makes manual and supervised learning approaches alone insufficient. We use an application of computational linguistics to develop an approach for mining the vast amount of unlabeled data in Web query logs to improve automatic topical Web query classification. We show that our approach in combination with manual matching and supervised learning allows us to classify a substantially larger proportion of queries than any single technique. We examine the performance of each approach on a real Web query stream and show that our combined method accurately classifies 46% of queries, outperforming the recall of best single approach by nearly 20%, with a 7% improvement in overall effectiveness.
[manual matching, search engines, automatic query classification, semisupervised learning, Laboratories, computational linguistics, data mining, Manuals, information retrieval, Information retrieval, classification, topical classification, Computer science, Databases, Web query logs, Supervised learning, Semisupervised learning, Search engines, Web query classification, Web search systems, Large-scale systems, Internet, learning (artificial intelligence), Web search]
ViVo: visual vocabulary construction for mining biomedical images
Fifth IEEE International Conference on Data Mining
None
2005
Given a large collection of medical images of several conditions and treatments, how can we succinctly describe the characteristics of each setting? For example, given a large collection of retinal images from several different experimental conditions (normal, detached, reattached, etc.), how can data mining help biologists focus on important regions in the images or on the differences between different experimental conditions? If the images were text documents, we could find the main terms and concepts for each condition by existing IR methods (e.g., tf/idf and LSI). We propose something analogous, but for the much more challenging case of an image collection: We propose to automatically develop a visual vocabulary by breaking images into n /spl times/ n tiles and deriving key tiles ("ViVos") for each image and condition. We experiment with numerous domain-independent ways of extracting features from tiles (color histograms, textures, etc.), and several ways of choosing characteristic tiles (PCA, ICA). We perform experiments on two disparate biomedical datasets. The quantitative measure of success is classification accuracy: Our "ViVos" achieve high classification accuracy (up to 83 %for a nine-class problem on feline retinal images). More importantly, qualitatively, our "ViVos" do an excellent job as "visual vocabulary terms": they have biological meaning, as corroborated by domain experts; they help spot characteristic regions of images, exactly like text vocabulary terms do for documents; and they highlight the differences between pairs of images.
[image collection, Vocabulary, text document images, Medical treatment, data mining, document image processing, Retina, Large scale integration, medical information systems, classification accuracy, Data mining, biomedical image mining, Histograms, Tiles, Feature extraction, visual vocabulary construction, medical image processing, Biomedical imaging, Principal component analysis]
Adaptive product normalization: using online learning for record linkage in comparison shopping
Fifth IEEE International Conference on Data Mining
None
2005
The problem of record linkage focuses on determining whether two object descriptions refer to the same underlying entity. Addressing this problem effectively has many practical applications, e.g., elimination of duplicate records in databases and citation matching for scholarly articles. In this paper, we consider a new domain where the record linkage problem is manifested: Internet comparison shopping. We address the resulting linkage setting that requires learning a similarity function between record pairs from streaming data. The learned similarity function is subsequently used in clustering to determine which records are co-referent and should be linked. We present an online machine learning method for addressing this problem, where a composite similarity function based on a linear combination of basis functions is learned incrementally. We illustrate the efficacy of this approach on several real-world datasets from an Internet comparison shopping site, and show that our method is able to effectively learn various distance functions for product data with differing characteristics. We also provide experimental results that show the importance of considering multiple performance measures in record linkage evaluation.
[record linkage, Displays, home shopping, Cleaning, adaptive product normalization, Data mining, Application software, Couplings, Learning systems, Databases, Search engines, online machine learning, Natural language processing, Internet, learning (artificial intelligence), similarity function, online learning, Internet comparison shopping]
Using information-theoretic measures to assess association rule interestingness
Fifth IEEE International Conference on Data Mining
None
2005
Assessing rules with interestingness measures is the cornerstone of successful applications of association rule discovery. However, there exists no information-theoretic measure which is adapted to the semantics of association rules. In this article, we present the directed information ratio (DIE), a new rule interestingness measure which is based on information theory. DIR is specially designed for association rules, and in particular it differentiates two opposite rules a /spl rarr/ b and a /spl rarr/ b~. Moreover, to our knowledge, DIR is the only rule interestingness measure which rejects both independence and (what we call) equilibrium, i.e. it discards both the rules whose antecedent and consequent are negatively correlated, and the rules which have more counter-examples than examples. Experimental studies show that DIR is a very filtering measure, which is useful for association rule post-processing.
[Communication systems, Filtering, Induction generators, data mining, Knowledge representation, information-theoretic measures, Association rules, Data mining, directed information ratio, association rule discovery, rule interestingness measure, Particle measurements, association rule interestingness, information theory, Artificial intelligence, Expert systems, Information theory]
Shortest-path kernels on graphs
Fifth IEEE International Conference on Data Mining
None
2005
Data mining algorithms are facing the challenge to deal with an increasing number of complex objects. For graph data, a whole toolbox of data mining algorithms becomes available by defining a kernel function on instances of graphs. Graph kernels based on walks, subtrees and cycles in graphs have been proposed so far. As a general problem, these kernels are either computationally expensive or limited in their expressiveness. We try to overcome this problem by defining expressive graph kernels which are based on paths. As the computation of all paths and longest paths in a graph is NP-hard, we propose graph kernels based on shortest paths. These kernels are computable in polynomial time, retain expressivity and are still positive definite. In experiments on classification of graph models of proteins, our shortest-path kernels show significantly higher classification accuracy than walk-based kernels.
[shortest-path kernel, Statistical learning, graph theory, data mining, graph data, Time measurement, Graph theory, Data mining, polynomial time algorithm, Proteins, Computer science, Support vector machines, NP-hard problem, Support vector machine classification, graph kernels, Polynomials, Kernel, computational complexity]
Mining frequent spatio-temporal sequential patterns
Fifth IEEE International Conference on Data Mining
None
2005
Many applications track the movement of mobile objects, which can be represented as sequences of timestamped locations. Given such a spatiotemporal series, we study the problem of discovering sequential patterns, which are routes frequently followed by the object. Sequential pattern mining algorithms for transaction data are not directly applicable for this setting. The challenges to address are: (i) the fuzziness of locations in patterns, and (ii) the identification of non-explicit pattern instances. In this paper, we define pattern elements as spatial regions around frequent line segments. Our method first transforms the original sequence into a list of sequence segments, and detects frequent regions in a heuristic way. Then, we propose algorithms to find patterns by employing a newly proposed substring tree structure and improving a priori technique. A performance evaluation demonstrates the effectiveness and efficiency of our approach.
[Tree data structures, frequent spatiotemporal sequential pattern, sequential pattern discovery, Tracking, spatiotemporal series, data mining, Transaction databases, Application software, History, Global Positioning System, Computer science, pattern identification, Frequency, sequential pattern mining, tree data structures, Pattern analysis, Mobile computing, substring tree structure]
Modeling multiple time series for anomaly detection
Fifth IEEE International Conference on Data Mining
None
2005
Our goal is to generate comprehensible and accurate models from multiple time series for anomaly detection. The models need to produce anomaly scores in an online manner for real-life monitoring tasks. We introduce three algorithms that work in a constructed feature space and evaluate them with a real data set from the NASA shuttle program. Our offline and online evaluations indicate that our algorithms can be more accurate than two existing algorithms.
[Computerized monitoring, NASA, Humans, Space shuttles, NASA shuttle program, time series, anomaly detection, monitoring task, Data mining, Condition monitoring, security of data, Space technology, Neural networks, Detectors, Detection algorithms, multiple time series modeling]
Summarization - compressing data into an informative representation
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we formulate the problem of summarization of a dataset of transactions with categorical attributes as an optimization problem involving two objective functions - compaction gain and information loss. We propose metrics to characterize the output of any summarization algorithm. We investigate two approaches to address this problem. The first approach is an adaptation of clustering and the second approach makes use of frequent item sets from the association analysis domain. We illustrate one application of summarization in the field of network data where we show how our technique can be effectively used to summarize network traffic into a compact but meaningful representation. Specifically, we evaluate our proposed algorithms on the 1998 DARPA Off-line Intrusion Detection Evaluation data and network data generated by SKAION Corp for the ARDA information assurance program.
[transaction processing, data compression, optimization problem, Data analysis, information loss, data summarization, informative representation, data mining, Telecommunication traffic, frequent item sets, Compaction, Data mining, Computer science, optimisation, Itemsets, compaction gain, transaction data, association analysis, Clustering algorithms, Intrusion detection, Data visualization, Monitoring, objective function]
Labeling unclustered categorical data into clusters based on the important attribute values
Fifth IEEE International Conference on Data Mining
None
2005
Sampling has been recognized as an important technique to improve the efficiency of clustering. However, with sampling applied, those points which are not sampled will not have their labels. Although there is a straightforward approach in the numerical domain, the problem of how to allocate those unlabeled data points into proper clusters remains as a challenging issue in the categorical domain. In this paper, a mechanism named MAximal Resemblance Data Labeling (abbreviated as MARDL) is proposed to allocate each unlabeled data point into the corresponding appropriate cluster based on the novel categorical clustering representative, namely, Node Importance Representative (abbreviated as NIR), which represents clusters by the importance of attribute values. MARDL has two advantages: (1) MARDL exhibits high execution efficiency; (2) after each unlabeled data is allocated into the proper cluster, MARDL preserves clustering characteristics, i.e., high intra-cluster similarity and low inter-cluster similarity. MARDL is empirically validated via real and synthetic data sets, and is shown to be not only more efficient than prior methods but also attaining results of better quality.
[maximal resemblance data labeling, sampling methods, categorical clustering, unclustered categorical data, data mining, Information retrieval, Pattern recognition, Data mining, database management systems, important attribute values, node importance representative, Databases, NP-hard problem, pattern clustering, Clustering algorithms, Machine learning, Sampling methods, sampling technique, Labeling, data labeling]
Making subsequence time series clustering meaningful
Fifth IEEE International Conference on Data Mining
None
2005
The startling claim was made that sequential time series clustering is meaningless. This has important consequences for a significant amount of work in the literature, since such a claim invalidates this work's contribution. In this paper, we show that sequential time series clustering is not meaningless, and that the problem highlighted in these works stem from their use of the Euclidean distance metric as the distance measure in the subsequence vector space. As a solution, we consider quite a general class of time series, and propose a regime based on two types of similarity that can exist between subsequence vectors, which give rise naturally to an alternative distance measure to Euclidean distance in the subsequence vector space. We show that, using this alternative distance measure, sequential time series clustering can indeed be meaningful. We repeat a key experiment in the work on which the "meaningless" claim was based, and show that our method leads to a successful clustering outcome.
[sequential time series clustering, Euclidean distance metric, time series, subsequence vector space, Time measurement, Data mining, Mobile robots, Information science, pattern clustering, Clustering algorithms, Euclidean distance, Feature extraction, distance measure, statistical analysis, Stock markets, Indexing]
Kernel-density-based clustering of time series subsequences using a continuous random-walk noise model
Fifth IEEE International Conference on Data Mining
None
2005
Noise levels in time series subsequence data are typically very high, and properties of the noise differ front those of white noise. The proposed algorithm incorporates a continuous random-walk noise model into kernel-density-based clustering. Evaluation is done by testing to what extent the resulting clusters are predictive of the process that generated the time series. It is shown that the new algorithm not only outperforms partitioning techniques that lead to trivial and unsatisfactory results under the given quality measure, but also improves upon other density-based algorithms. The results suggest that the noise elimination properties of kernel-density-based clustering algorithms can be of significant value for the use of clustering in preprocessing of data.
[Density measurement, noise elimination, Time measurement, Partitioning algorithms, Data mining, Noise level, Computer science, white noise, continuous random-walk noise model, pattern clustering, Clustering algorithms, White noise, kernel-density-based clustering, time series subsequence data, statistical analysis, stochastic processes, Testing, Signal to noise ratio]
Usage-based PageRank for Web personalization
Fifth IEEE International Conference on Data Mining
None
2005
Recommendation algorithms aim at proposing "next" pages to a user based on her current visit and the past users' navigational patterns. In the vast majority of related algorithms, only the usage data are used to produce recommendations, whereas the structural properties of the Web graph are ignored. We claim that taking also into account the Web structure and using link analysis algorithms ameliorates the quality of recommendations. In this paper we present UPR, a novel personalization algorithm which combines usage data and link analysis techniques for ranking and recommending Web pages to the end user. Using the Web site's structure and its usage data we produce personalized navigational graph synopsis (prNG) to be used for applying UPR and produce personalized recommendations. Experimental results show that the accuracy of the recommendations is superior to pure usage-based approaches.
[Algorithm design and analysis, Performance evaluation, Data analysis, Navigation, usage data, personalized recommendation, information filters, usage-based PageRank, Web structure, Tree graphs, Web graph, Web personalization, Web pages, Web mining, Search engines, personalized navigational graph synopsis, recommendation algorithm, link analysis, Internet, Web sites, Web search, navigational pattern]
WARP: time warping for periodicity detection
Fifth IEEE International Conference on Data Mining
None
2005
Periodicity mining is used for predicting trends in time series data. Periodicity detection is an essential process in periodicity mining to discover potential periodicity rates. Existing periodicity detection algorithms do not take into account the presence of noise, which is inevitable in almost every real-world time series data. In this paper, we tackle the problem of periodicity detection in the presence of noise. We propose a new periodicity detection algorithm that deals efficiently with all types of noise. Based on time warping, the proposed algorithm warps (extends or shrinks) the time axis at various locations to optimally remove the noise. Experimental results show that the proposed algorithm outperforms the existing periodicity detection algorithms in terms of noise resiliency.
[Noise figure, Energy consumption, Energy measurement, data mining, time series, Data mining, Noise level, Temperature measurement, periodicity detection, Working environment noise, noise, periodicity mining, time warping, time series data, Detection algorithms, noise removal, Signal to noise ratio, Meteorology]
Bifold constraint-based mining by simultaneous monotone and anti-monotone checking
Fifth IEEE International Conference on Data Mining
None
2005
Mining for frequent item sets can generate an overwhelming number of patterns, often exceeding the size of the original transactional database. One way to deal with this issue is to set filters and interestingness measures. Others advocate the use of constraints to apply to the patterns, either on the form of the patterns or on descriptors of the items in the patterns. However, typically the filtering of patterns based on these constraints is done as a post-processing phase. Filtering the patterns post-mining adds a significant overhead, still suffers from the sheer size of the pattern set and loses the opportunity to exploit those constraints. In this paper we propose an approach that allows the efficient mining of frequent item sets patterns, while pushing simultaneously both monotone and anti-monotone constraints during and at different strategic stages of the mining process. Our implementation shows a significant improvement when considering the constraints early and a better performance over Dualminer which also considers both types of constraints.
[interestingness measure, Costs, Filtering, pattern filtering, Buildings, Lattices, data mining, frequent item set mining, Transaction databases, Data mining, Association rules, Filters, Itemsets, Clustering algorithms, bifold constraint-based mining, antimonotone checking, simultaneous monotone checking]
Effective estimation of posterior probabilities: explaining the accuracy of randomized decision tree approaches
Fifth IEEE International Conference on Data Mining
None
2005
There has been increasing number of independently proposed randomization methods in different stages of decision tree construction to build multiple trees. Randomized decision tree methods have been reported to be significantly more accurate than widely-accepted single decision trees, although the training procedure of some methods incorporates a surprisingly random factor and therefore opposes the generally accepted idea of employing gain functions to choose optimum features at each node and compute a single tree that fits the data. One important question that is not well understood yet is the reason behind the high accuracy. We provide an insight based on posterior probability estimations. We first establish the relationship between effective posterior probability estimation and effective loss reduction. We argue that randomized decision tree methods effectively approximate the true probability distribution using the decision tree hypothesis space. We conduct experiments using both synthetic and real-world datasets under both 0-1 and cost-sensitive loss functions.
[Algorithm design and analysis, estimation theory, probability, posterior probability estimation, randomization method, Probability distribution, randomized decision tree, decision tree construction, decision trees, probability distribution, gain function, Decision trees, loss reduction, Classification tree analysis]
A thorough experimental study of datasets for frequent itemsets
Fifth IEEE International Conference on Data Mining
None
2005
The discovery of frequent patterns is a famous problem in data mining. While plenty of algorithms have been proposed during the last decade, only a few contributions have tried to understand the influence of datasets on the algorithms behavior. Being able to explain why certain algorithms are likely to perform very well or very poorly on some datasets is still an open question. In this setting, we describe a thorough experimental study of datasets with respect to frequent item sets. We study the distribution of frequent item sets with respect to item sets size together with the distribution of three concise representations: frequent closed, frequent free and frequent essential item sets. For each of them, we also study the distribution of their positive and negative borders whenever possible. From this analysis, we exhibit a new characterization of datasets and some invariants allowing to better predict the behavior of well known algorithms. The main perspective of this work is to devise adaptive algorithms with respect to dataset characteristics.
[Algorithm design and analysis, frequent item set, Adaptive algorithm, Conferences, frequent patterns, data mining, Data structures, Classification algorithms, Data mining, Association rules, Itemsets, frequent free item set, Statistical distributions, frequent essential item set, frequent closed item set]
AMIOT: induced ordered tree mining in tree-structured databases
Fifth IEEE International Conference on Data Mining
None
2005
Frequent subtree mining has become increasingly important in recent years. In this paper, we present AMIOT algorithm to discover all frequent ordered subtrees in a tree-structured database. In order to avoid the generation of infrequent candidate trees, we propose the techniques such as right-and-left tree join and serial tree extension. Proposed methods enumerate only the candidate trees with high probability of being frequent without any duplication. The experiments on synthetic dataset and XML database show that AMIOT reduces redundant candidate trees and outperforms FREQT algorithm by up to five times in execution time.
[Algorithm design and analysis, Machine learning algorithms, right-and-left tree join, serial tree extension, data mining, Data mining, induced ordered tree mining, Chemistry, tree-structured databases, Databases, Tree graphs, Text categorization, XML, Machine learning, frequent subtree mining, AMIOT, tree data structures, Informatics, frequent ordered subtrees]
Hierarchy-regularized latent semantic indexing
Fifth IEEE International Conference on Data Mining
None
2005
Organizing textual documents into a hierarchical taxonomy is a common practice in knowledge management. Beside textual features, the hierarchical structure of directories reflect additional and important knowledge annotated by experts. It is generally desired to incorporate this information into text mining processes. In this paper, we propose hierarchy-regularized latent semantic indexing, which encodes the hierarchy into a similarity graph of documents and then formulates an optimization problem mapping each document into a low dimensional vector space. The new feature space preserves the intrinsic structure of the original taxonomy and thus provides a meaningful basis for various learning tasks like visualization and classification. Our approach employs the information about class proximity and class specificity, and can naturally cope with multi-labeled documents. Our empirical studies show very encouraging results on two real-world data sets, the new Reuters (RCVI) benchmark and the Swissprot protein database.
[Text mining, text analysis, optimization problem, Navigation, indexing, Taxonomy, data mining, hierarchical structure, Knowledge management, Visual databases, knowledge management, document mapping, Organizing, Proteins, Computer science, Data visualization, similarity graph, hierarchy-regularized latent semantic indexing, text mining, textual documents, hierarchical taxonomy, Indexing]
Extracting frequent subsequences from a single long data sequence a novel anti-monotonic measure and a simple on-line algorithm
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we study frequent subsequence extraction from a single very-long data-sequence. First we propose a novel frequency measure, called the total frequency, for counting multiple occurrences of a sequential pattern in a single data sequence. The total frequency is anti-monotonic, and makes it possible to count up pattern occurrences without duplication. Moreover the total frequency has a good property for implementation based on the dynamic programming strategy. Second we give a simple on-line algorithm for a specialized subsequence extraction problem, i.e., a problem with the infinite window-length. This specialized problem is considered to be a relaxation of the general-case problem, thus this fast on-line algorithm is important from the view of practical applications.
[Computer science, total frequency measure, Databases, Itemsets, antimonotonic measure, data mining, online algorithm, frequent subsequence extraction, Data engineering, Frequency measurement, Dynamic programming, Data mining]
Mining minimal distinguishing subsequence patterns with gap constraints
Fifth IEEE International Conference on Data Mining
None
2005
Discovering contrasts between collections of data is an important task in data mining. In this paper, we introduce a new type of contrast pattern, called a minimal distinguishing subsequence (MDS). An MDS is a minimal subsequence that occurs frequently in one class of sequences and infrequently in sequences of another class. It is a natural way of representing strong and succinct contrast information between two sequential datasets and can be useful in applications such as protein comparison, document comparison and building sequential classification models. Mining MDS patterns is a challenging task and is significantly different from mining contrasts between relational/transactional data. One particularly important type of constraint that can be integrated into the mining process is the maximum gap constraint. We present an efficient algorithm called ConSGapMiner, to mine all MDSs according to a maximum gap constraint. It employs highly efficient bitset and Boolean operations, for powerful gap based pruning within a prefix growth framework. A performance evaluation with both sparse and dense datasets, demonstrates the scalability of ConSGapMiner and shows its ability to mine patterns from high dimensional datasets at low supports.
[Protein engineering, Sequences, Biochemical analysis, Scalability, pattern mining, maximum gap constraint, data mining, sequential classification model, document comparison, Data mining, transactional data, contrast information, Computer science, minimal distinguishing subsequence patterns, relational data, protein comparison, ConSGapMiner, Biomembranes, Books, Pattern analysis, Bioinformatics]
Learning instance greedily cloning naive Bayes for ranking
Fifth IEEE International Conference on Data Mining
None
2005
Naive Bayes (simply NB) (Langley et al., 1992) has been widely used in machine learning and data mining as a simple and effective classification algorithm. Since its conditional independence assumption is rarely true, researchers have made a substantial amount of effort to improve naive Bayes. The related research work can be broadly divided into two approaches: eager learning and lazy learning, depending on when the major computation occurs. Different from eager approach, the key idea for extending naive Bayes from the lazy approach is to learn a naive Bayes for each testing example. In recent years, some lazy extensions of naive Bayes have been proposed. For example, SNNB, LWNB, and LBR. All are aiming at improving the classification accuracy of naive Bayes. In many real-world machine learning and data mining applications, however, an accurate ranking is more desirable than an accurate classification. Responding to this fact, we present a lazy learning algorithm called instance greedily cloning naive Bayes (simply IGCNB) in this paper. Our motivation is to improve naive Bayes' ranking performance measured by AUC (Bradley, 1997; Provost and Fawcett, 1997). We experimentally tested our algorithm, using the whole 36 UCI datasets recommended by Weka, and compared it to C4.4 (Provost and Domingos, 2003), NB (Langley et al., 1992), SNNB (Xie, 2002) and LWNB (Frank, 2003). The experimental results show that our algorithm outperforms all the other algorithms used to compare significantly in yielding accurate ranking.
[Machine learning algorithms, Geology, Cloning, data mining, lazy learning, classification algorithm, Classification algorithms, Data mining, machine learning, Niobium, Computer science, Bayesian methods, eager learning, Machine learning, Bayes methods, learning (artificial intelligence), instance greedily cloning naive Bayes, Testing]
An algorithm for in-core frequent itemset mining on streaming data
Fifth IEEE International Conference on Data Mining
None
2005
Frequent item set mining is a core data mining operation and has been extensively studied over the last decade. This paper takes a new approach for this problem and makes two major contributions. First, we present a one pass algorithm for frequent item set mining, which has deterministic bounds on the accuracy, and does not require any out-of-core summary structure. Second, because our one pass algorithm does not produce any false negatives, it can be easily extended to a two pass accurate algorithm. Our two pass algorithm is very memory efficient, and allows mining of datasets with large number of distinct items and/or very low support levels. Our detailed experimental evaluation on synthetic and real datasets shows the following. First, our one pass algorithm is very accurate in practice. Second, our algorithm requires significantly lower memory than Manku and Motwani's one pass algorithm and the multi-pass Apriori algorithm. Our two pass algorithm outperforms Apriori and FP-tree when the number of distinct items is large and/or support levels are very low. In other cases, it is quite competitive, with possible exception of cases where the average length of frequent item sets is quite high.
[Computer science, incore frequent item set mining, multipass Apriori algorithm, Itemsets, streaming data, one pass algorithm, data mining, Data engineering, Frequency, Data structures, Data mining]
Stability of feature selection algorithms
Fifth IEEE International Conference on Data Mining
None
2005
With the proliferation of extremely high-dimensional data, feature selection algorithms have become indispensable components of the learning process. Strangely, despite extensive work on the stability of learning algorithms, the stability of feature selection algorithms has been relatively neglected. This study is an attempt to fill that gap by quantifying the sensitivity of feature selection algorithms to variations in the training set. We assess the stability of feature selection algorithms based on the stability of the feature preferences that they express in the form of weights-scores, ranks, or a selected feature subset. We examine a number of measures to quantify the stability of feature preferences and propose an empirical way to estimate them. We perform a series of experiments with several feature selection algorithms on a set of proteomics datasets. The experiments allow us to explore the merits of each stability measure and create stability profiles of the feature selection algorithms. Finally we show how stability profiles can support the choice of a feature selection algorithm.
[pattern classification, Error analysis, learning algorithm stability, stability measure, Predictive models, Probability distribution, Stability analysis, Classification algorithms, Data mining, Computer science, stability profile, feature selection algorithm, Proteomics, Sampling methods, learning (artificial intelligence), Testing]
HOT SAX: efficiently finding the most unusual time series subsequence
Fifth IEEE International Conference on Data Mining
None
2005
In this work, we introduce the new problem of finding time series discords. Time series discords are subsequences of a longer time series that are maximally different to all the rest of the time series subsequences. They thus capture the sense of the most unusual subsequence within a time series. Time series discords have many uses for data mining, including improving the quality of clustering, data cleaning, summarization, and anomaly detection. Discords are particularly attractive as anomaly detectors because they only require one intuitive parameter (the length of the subsequence) unlike most anomaly detection algorithms that typically require many parameters. We evaluate our work with a comprehensive set of experiments. In particular, we demonstrate the utility of discords with objective experiments on domains as diverse as Space Shuttle telemetry monitoring, medicine, surveillance, and industry, and we demonstrate the effectiveness of our discord discovery algorithm with more than one million experiments, on 82 different datasets from diverse domains.
[summarization, Space shuttles, data mining, HOT SAX, time series, Cleaning, anomaly detection, Telemetry, Data mining, Clustering, Aerospace industry, Computer science, Anomaly Detection, clustering quality, Surveillance, data cleaning, Detectors, time series discords, Time Series Data Mining, Detection algorithms, Monitoring]
Orthogonal neighborhood preserving projections
Fifth IEEE International Conference on Data Mining
None
2005
Orthogonal neighborhood preserving projections (ONPP) is a linear dimensionality reduction technique which attempts to preserve both the intrinsic neighborhood geometry of the data samples and the global geometry. The proposed technique constructs a weighted data graph where the weights are constructed in a data-driven fashion, similarly to locally linear embedding (LLE). A major difference with the standard LLE where the mapping between the input and the reduced spaces is implicit, is that ONPP employs an explicit linear mapping between the two. As a result, and in contrast with LLE, handling new data samples becomes straightforward, as this amounts to a simple linear transformation. ONPP shares some of the properties of locality preserving projections (LPP). Both ONPP and LPP rely on a k-nearest neighbor graph in order to capture the data topology. However, our algorithm inherits the characteristics of LLE in preserving the structure of local neighborhoods, while LPP aims at preserving only locality without specifically aiming at preserving the geometric structure. This feature makes ONPP an effective method for data visualization. We provide ample experimental evidence to demonstrate the advantageous characteristics of ONPP, using well known synthetic test cases as well as real life data from computational biology and computer vision.
[locality preserving projection, Computer vision, intrinsic neighborhood geometry, graph theory, Life testing, computational geometry, Data engineering, orthogonal neighborhood preserving projection, weighted data graph, k-nearest neighbor graph, Topology, Data mining, global geometry, data topology, Computer science, Computational geometry, data reduction, locally linear embedding, Data visualization, linear dimensionality reduction, data visualization, data samples, Principal component analysis, Computational biology]
Higher-order Web link analysis using multilinear algebra
Fifth IEEE International Conference on Data Mining
None
2005
Linear algebra is a powerful and proven tool in Web search. Techniques, such as the PageRank algorithm of Brin and Page and the HITS algorithm of Kleinberg, score Web pages based on the principal eigenvector (or singular vector) of a particular non-negative matrix that captures the hyperlink structure of the Web graph. We propose and test a new methodology that uses multilinear algebra to elicit more information from a higher-order representation of the hyperlink graph. We start by labeling the edges in our graph with the anchor text of the hyperlinks so that the associated linear algebra representation is a sparse, three-way tensor. The first two dimensions of the tensor represent the Web pages while the third dimension adds the anchor text. We then use the rank-1 factors of a multilinear PARAFAC tensor decomposition, which are akin to singular vectors of the SVD, to automatically identify topics in the collection along with the associated authoritative Web pages.
[text analysis, HITS algorithm, anchor text, Laboratories, singular vector, multilinear algebra, higher-order representation, higher-order Web link analysis, Web graph, nonnegative matrix, Search engines, Labeling, score Web pages, linear algebra, Testing, sparse three-way tensor, PageRank algorithm, hyperlink graph, Vectors, multilinear PARAFAC tensor decomposition, Topology, principal eigenvector, Tensile stress, Web pages, Linear algebra, Internet, hyperlink structure, Web search]
A generic framework for efficient subspace clustering of high-dimensional data
Fifth IEEE International Conference on Data Mining
None
2005
Subspace clustering has been investigated extensively since traditional clustering algorithms often fail to detect meaningful clusters in high-dimensional data spaces. Many recently proposed subspace clustering methods suffer from two severe problems: First, the algorithms typically scale exponentially with the data dimensionality and/or the subspace dimensionality of the clusters. Second, for performance reasons, many algorithms use a global density threshold for clustering, which is quite questionable since clusters in subspaces of significantly different dimensionality will most likely exhibit significantly varying densities. In this paper, we propose a generic framework to overcome these limitations. Our framework is based on an efficient filter-refinement architecture that scales at most quadratic w.r.t. the data dimensionality and the dimensionality of the subspace clusters. It can be applied to any clustering notions including notions that are based on a local density threshold. A broad experimental evaluation on synthetic and real-world data empirically shows that our method achieves a significant gain of runtime and quality in comparison to state-of-the-art subspace clustering algorithms.
[Clustering methods, Scalability, data mining, high-dimensional data, Partitioning algorithms, data dimensionality, subspace dimensionality, Data mining, Gene expression, Diseases, Computer science, Runtime, pattern clustering, subspace clustering, filter-refinement architecture, Clustering algorithms, Principal component analysis]
Effective and efficient distributed model-based clustering
Fifth IEEE International Conference on Data Mining
None
2005
In many companies data is distributed among several sites, i.e. each site generates its own data and manages its own data repository. Analyzing and mining these distributed sources requires distributed data mining techniques to find global patterns representing the complete information. The transmission of the entire local data set is often unacceptable because of performance considerations, privacy and security aspects, and bandwidth constraints. Traditional data mining algorithms, demanding access to complete data, are not appropriate for distributed applications. Thus, there is a need for distributed data mining algorithms in order to analyze and discover new knowledge in distributed environments. One of the most important data mining tasks is clustering which aims at detecting groups of similar data objects. In this paper, we propose a distributed model-based clustering algorithm that uses EM for detecting local models in terms of mixtures of Gaussian distributions. We propose an efficient and effective algorithm for deriving and merging these local Gaussian distributions to generate a meaningful global model. In a broad experimental evaluation we show that our framework is scalable in a highly distributed environment.
[Algorithm design and analysis, Data privacy, Data security, distributed model-based clustering, data mining, Gaussian distribution, data repository, Data mining, local Gaussian distributions, Information analysis, pattern clustering, distributed data mining, Clustering algorithms, Information security, Bandwidth, distributed databases, Pattern analysis, global patterns]
Finding maximal frequent itemsets over online data streams adaptively
Fifth IEEE International Conference on Data Mining
None
2005
Due to the characteristics of a data stream, it is very important to confine the memory usage of a data mining process regardless of the amount of information generated in the data stream. For this purpose, this paper proposes a CP-tree (compressed-prefix tree) that can be effectively used in finding either frequent or maximal frequent itemsets over an online data stream. Unlike a prefix tree, a node of a CP-tree can maintain the information of several item-sets together. Based on this characteristic, the size of a CP-tree can be flexibly controlled by merging or splitting nodes. In this paper, a mining method employing a CP-tree is proposed and an adaptive memory utilization scheme is also presented in order to maximize the mining accuracy of the proposed method for confined memory space at all times. Finally, the performance of the proposed method is analyzed by a series of experiments to identify its various characteristics.
[Data analysis, Buffer storage, Merging, data mining, trees (mathematics), mining method, CP-tree, Data mining, adaptive memory utilization, Computer science, maximal frequent itemsets, Itemsets, Space technology, Character generation, online data streams, Performance analysis, Size control, compressed-prefix tree]
CanTree: a tree structure for efficient incremental mining of frequent patterns
Fifth IEEE International Conference on Data Mining
None
2005
Since its introduction, frequent-pattern mining has been the subject of numerous studies, including incremental updating. Many existing incremental mining algorithms are Apriori-based, which are not easily adoptable to FP-tree based frequent-pattern mining. In this paper, we propose a novel tree structure, called CanTree (canonical-order tree), that captures the content of the transaction database and orders tree nodes according to some canonical order. By exploiting its nice properties, the CanTree can be easily maintained when database transactions are inserted, deleted, and/or modified. For example, the CanTree does not require adjustment, merging, and/or splitting of tree nodes during maintenance. No rescan of the entire updated database or reconstruction of a new tree is needed for incremental updating. Experimental results show the effectiveness of our CanTree.
[Tree data structures, transaction processing, Cats, Merging, tree structure, Humans, data mining, tree nodes, Transaction databases, Data mining, Test pattern generators, FP-tree based frequent-pattern mining, transaction database, CanTree, Frequency, Database systems, tree data structures, incremental mining, canonical-order tree, incremental updating, Testing]
Combining multiple clusterings by soft correspondence
Fifth IEEE International Conference on Data Mining
None
2005
Combining multiple clusterings arises in various important data mining scenarios. However, finding a consensus clustering from multiple clusterings is a challenging task because there is no explicit correspondence between the classes from different clusterings. We present a new framework based on soft correspondence to directly address the correspondence problem in combining multiple clusterings. Under this framework, we propose a novel algorithm that iteratively computes the consensus clustering and correspondence matrices using multiplicative updating rules. This algorithm provides a final consensus clustering as well as correspondence matrices that gives intuitive interpretation of the relations between the consensus clustering and each clustering from clustering ensembles. Extensive experimental evaluations also demonstrate the effectiveness and potential of this framework as well as the algorithm for discovering a consensus clustering from multiple clusterings.
[Data analysis, Robust stability, Shape, data mining, multiple clusterings, Partitioning algorithms, soft correspondence, Data mining, Unsupervised learning, Information analysis, Uniform resource locators, consensus clustering, pattern clustering, multiplicative updating rule, Clustering algorithms, Iterative algorithms, correspondence matrices]
A new algorithm for finding minimal sample uniques for use in statistical disclosure assessment
Fifth IEEE International Conference on Data Mining
None
2005
We present SUDA2, a recursive algorithm for finding minimal sample uniques (MSUs). SUDA2 uses a novel method for representing the search space for MSUs and new observations about the properties of MSUs to prune and traverse this space. Experimental comparisons with previous work demonstrate that SUDA2 is not only several orders of magnitude faster but is also capable of identifying the boundaries of the search space, enabling datasets of larger numbers of columns than before to be addressed.
[Algorithm design and analysis, Technological innovation, Data analysis, Sequences, minimal sample uniques, Extraterrestrial measurements, SUDA2, database management systems, Computer science, DNA, Lead, recursive algorithm, Computer networks, search space, statistical analysis, special unique detection algorithm, Protection, statistical disclosure assessment]
Alternate representation of distance matrices for characterization of protein structure
Fifth IEEE International Conference on Data Mining
None
2005
The most suitable method for the automated classification of protein structures remains an open problem in computational biology. In order to classify a protein structure with any accuracy, an effective representation must be chosen. Here we present two methods of representing protein structure. One involves representing the distances between the C/sub a/ atoms of a protein as a two-dimensional matrix and creating a model of the resulting surface with Zernike polynomials. The second uses a wavelet-based approach. We convert the distances between a protein's C/sub a/ atoms into a one-dimensional signal which is then decomposed using a discrete wavelet transformation. Using the Zernike coefficients and the approximation coefficients of the wavelet decomposition as feature vectors, we test the effectiveness of our representation with two different classifiers on a dataset of more than 600 proteins taken from the 27 most-populated SCOP folds. We find that the wavelet decomposition greatly outperforms the Zernike model. With the wavelet representation, we achieve an accuracy of approximately 56%, roughly 12% higher than results reported on a similar, but less-challenging dataset. In addition, we can couple our structure-based feature vectors with several sequence-based properties to increase accuracy another 5-7%. Finally, we use a multi-stage classification strategy on the combined features to increase performance to 78%, an improvement in accuracy of more than 15-20% and 34% over the highest reported sequence-based and structure-based classification results, respectively.
[computational biology, Zernike polynomials, wavelet-based approach, discrete wavelet transformation, approximation coefficients, wavelet transforms, Zernike model, Zernike coefficients, wavelet decomposition, 2D matrix, data reduction, multistage classification, Databases, 1D signal, biology computing, proteins, Polynomials, Nuclear magnetic resonance, Matrix converters, distance matrices, Computational biology, Testing, Protein engineering, pattern classification, Biological system modeling, structure-based feature vectors, Matrix decomposition, wavelet representation, matrix algebra, Computer science, automated protein structure classification]
Training support vector machines using Gilbert's algorithm
Fifth IEEE International Conference on Data Mining
None
2005
Support vector machines are classifiers designed around the computation of an optimal separating hyperplane. This hyperplane is typically obtained by solving a constrained quadratic programming problem, but may also be located by solving a nearest point problem. Gilbert's algorithm can be used to solve this nearest point problem but is unreasonably slow. In this paper we present a modified version of Gilbert's algorithm for the fast computation of the support vector machine hyperplane. We then compare our algorithm with the nearest point algorithm and with sequential minimal optimization.
[support vector machine hyperplane, constrained quadratic programming, support vector machines, Laboratories, Nearest Point Algorithm, optimal separating hyperplane, sequential minimal optimization, Gilbert algorithm, Quadratic programming, Data mining, Support Vector Machines, Support vector machines, Sequential Minimal Optimization, optimisation, Neural networks, Support vector machine classification, Prototypes, Gilbert&amp;#146;s Algorithm, Polynomials, learning (artificial intelligence), Kernel, nearest point problem]
A heterogeneous field matching method for record linkage
Fifth IEEE International Conference on Data Mining
None
2005
Record linkage is the process of determining that two records refer to the same entity. A key subprocess is evaluating how well the individual fields, or attributes, of the records match each other. One approach to matching fields is to use hand-written domain-specific rules. This "expert systems" approach may result in good performance for specific applications, but it is not scalable. This paper describes a new machine learning approach that creates expert-like rules for field matching. In our approach, the relationship between two field values is described by a set of heterogeneous transformations. Previous machine learning methods used simple models to evaluate the distance between two fields. However, our approach enables more sophisticated relationships to be modeled, which better capture the complex domain specific, common-sense phenomena that humans use to judge similarity. We compare our approach to methods that rely on simpler homogeneous models in several domains. By modeling more complex relationships we produce more accurate results.
[Marine technology, pattern matching, record linkage, heterogeneous transformations, Humans, Switches, database management systems, machine learning, Couplings, Learning systems, Databases, Animals, expert-like rules, Machine learning, heterogeneous field matching, Manufacturing, learning (artificial intelligence), Business]
Leveraging relational autocorrelation with latent group models
Fifth IEEE International Conference on Data Mining
None
2005
The presence of autocorrelation provides a strong motivation for using relational learning and inference techniques. Autocorrelation is a statistical dependence between the values of the same variable on related entities and is a nearly ubiquitous characteristic of relational data sets. Recent research has explored the use of collective inference techniques to exploit this phenomenon. These techniques achieve significant performance gains by modeling observed correlations among class labels of related instances, but the models fail to capture a frequent cause of autocorrelation - the presence of underlying groups that influence the attributes on a set of entities. We propose a latent group model (LGM) for relational data, which discovers and exploits the hidden structures responsible for the observed autocorrelation among class labels. Modeling the latent group structure improves model performance, increases inference efficiency, and enhances our understanding of the datasets. We evaluate performance on three relational classification tasks and show that LGM outperforms models that ignore latent group structure, particularly when there is little information with which to seed inference.
[latent group models, Performance gain, Predictive models, latent group structure, relational classification, Data mining, relational databases, inference mechanisms, Computer science, collective inference, Graphical models, relational autocorrelation, Web pages, relational data sets, Motion pictures, statistical dependence, Autocorrelation, Web sites, learning (artificial intelligence), Advertising, relational learning, latent group model]
Balancing exploration and exploitation: a new algorithm for active machine learning
Fifth IEEE International Conference on Data Mining
None
2005
Active machine learning algorithms are used when large numbers of unlabeled examples are available and getting labels for them is costly (e.g. requiring consulting a human expert). Many conventional active learning algorithms focus on refining the decision boundary, at the expense of exploring new regions that the current hypothesis misclassifies. We propose a new active learning algorithm that balances such exploration with refining of the decision boundary by dynamically adjusting the probability to explore at each step. Our experimental results demonstrate improved performance on data sets that require extensive exploration while remaining competitive on data sets that do not. Our algorithm also shows significant tolerance of noise.
[Machine learning algorithms, Humans, Application software, decision boundary, Computer science, active machine learning, Feedback, Text categorization, Machine learning, Sampling methods, unlabeled examples, Labeling, Region 4, learning (artificial intelligence)]
Finding representative set from massive data
Fifth IEEE International Conference on Data Mining
None
2005
In the information age, data is pervasive. In some applications, data explosion is a significant phenomenon. The massive data volume poses challenges to both human users and computers. In this project, we propose a new model for identifying representative set from a large database. A representative set is a special subset of the original dataset, which has three main characteristics: It is significantly smaller in size compared to the original dataset. It captures the most information from the original dataset compared to other subsets of the same size. It has low redundancy among the representatives it contains. We use information-theoretic measures such as mutual information and relative entropy to measure the representativeness of the representative set. We first design a greedy algorithm and then present a heuristic algorithm that delivers much better performance. We run experiments on two real datasets and evaluate the effectiveness of our representative set in terms of coverage and accuracy. The experiments show that our representative set attains expected characteristics and captures information more efficiently.
[Algorithm design and analysis, Greedy algorithms, massive data volume, greedy algorithms, Humans, large database, Interference, information-theoretic measures, Entropy, Explosions, Application software, heuristic algorithm, Databases, entropy, very large databases, Clustering algorithms, relative entropy, mutual information, greedy algorithm, Mutual information, data explosion]
Parameter-free spatial data mining using MDL
Fifth IEEE International Conference on Data Mining
None
2005
Consider spatial data consisting of a set of binary features taking values over a collection of spatial extents (grid cells). We propose a method that simultaneously finds spatial correlation and feature co-occurrence patterns, without any parameters. In particular, we employ the minimum description length (MDL) principle coupled with a natural way of compressing regions. This defines what "good" means: a feature co-occurrence pattern is good, if it helps us better compress the set of locations for these features. Conversely, a spatial correlation is good, if it helps us better compress the set of features in the corresponding region. Our approach is scalable for large datasets (both number of locations and of features). We evaluate our method on both real and synthetic datasets.
[parameter-free spatial data mining, NASA, data mining, visual databases, Hurricanes, spatial correlation, Data mining, large datasets, Biological materials, Hospitals, Storms, Space technology, Character generation, minimum description length principle, Cities and towns, Bioinformatics, feature cooccurrence patterns]
Discovering frequent arrangements of temporal intervals
Fifth IEEE International Conference on Data Mining
None
2005
In this paper we study a new problem in temporal pattern mining: discovering frequent arrangements of temporal intervals. We assume that the database consists of sequences of events, where an event occurs during a time-interval. The goal is to mine arrangements of event intervals that appear frequently in the database. There are many applications where these type of patterns can be useful, including data network, scientific, and financial applications. Efficient methods to find frequent arrangements of temporal intervals using both breadth first and depth first search techniques are described. The performance of the proposed algorithms is evaluated and compared with other approaches on real datasets (American sign language streams and network data) and large synthetic datasets.
[temporal intervals, Event detection, temporal pattern mining, Handicapped aids, data mining, Transaction databases, Data mining, Computer science, breadth first and depth first search, temporal databases, Intrusion detection, frequent arrangement discovery, Monitoring]
Mining patterns of change in remote sensing image databases
Fifth IEEE International Conference on Data Mining
None
2005
Remote sensing image databases are the fastest growing archives of spatial information. However, we still have a limited capacity for extracting information from large remote sensing image databases. There are currently very few techniques for image data mining and information extraction in large image data sets, and thus we are failing to exploit our large remote sensing data archives. This paper proposes a methodology to provide guidance for mining remote sensing image databases. The basic idea is to use domain concepts to build generic description of patterns in remote sensing images, and then use structural approaches to identify such patterns in images. We illustrate our proposal with a case study for detecting land use patterns in Amazonia from INPE's remote sensing image database.
[Image resolution, large image data sets, Government, data mining, visual databases, remote sensing, change pattern mining, Data mining, Proposals, Remote sensing, image data mining, remote sensing image databases, Satellites, information extraction, Image databases, Planets, spatial information, large remote sensing image database, Spatial resolution, remote sensing data archives, Remote monitoring]
Ranking-based evaluation of regression models
Fifth IEEE International Conference on Data Mining
None
2005
We suggest the use of ranking-based evaluation measures for regression models, as a complement to the commonly used residual-based evaluation. We argue that in some cases, such as the case study we present, ranking can be the main underlying goal in building a regression model, and ranking performance is the correct evaluation metric. However, even when ranking is not the contextually correct performance metric, the measures we explore still have significant advantages: They are robust against extreme outliers in the evaluation set; and they are interpretable. The two measures we consider correspond closely to non-parametric correlation coefficients commonly used in data analysis (Spearman's p and Kendall's r); and they both have interesting graphical representations, which, similarly to ROC curves, offer useful "partial" model performance views, in addition to a one-number summary in the area under the curve. We illustrate our methods on a case study of evaluating IT wallet size estimation models for IBM's customers.
[Pediatrics, Data analysis, Costs, data analysis, Area measurement, regression analysis, Predictive models, Data mining, Statistics, regression model, residual-based evaluation, ranking performance, nonparametric correlation coefficients, evaluation metric, graphical representation, model performance views, Robustness, Marketing and sales, ranking-based evaluation, Testing, correlation methods]
Compound classification models for recommender systems
Fifth IEEE International Conference on Data Mining
None
2005
Recommender systems recommend products to customers based on ratings or past customer behavior. Without any information about attributes of the products or customers involved, the problem has been tackled most successfully by a nearest neighbor method called collaborative filtering in the context, while additional efforts invested in building classification models did not pay off and did not increase the quality. Therefore, classification methods have mainly been used in conjunction with product or customer attributes. Starting from a view on the plain recommendation task without attributes as a multi-class classification problem, we investigate two particularities, its autocorrelation structure as well as the absence of re-occurring items (repeat buying). We adapt the standard generic reductions 1-vs-rest and 1-vs-l of multi-class problems to a set of binary classification problems to these particularities and thereby provide a generic compound classifier for recommender systems. We evaluate a particular specialization thereof using linear support vector machines as member classifiers on MovieLens data and show that it outperforms state-of-the-art methods, i.e., item-based collaborative filtering.
[pattern classification, collaborative filtering, autocorrelation structure, binary classification problem, Buildings, multiclass classification problem, consumer behaviour, Information filtering, information filters, Nearest neighbor searches, linear support vector machines, Support vector machines, recommender systems, Collaboration, Support vector machine classification, compound classification model, Information filters, Autocorrelation, Recommender systems, customer behavior, nearest neighbor method, Context modeling]
Multi-stage classification
Fifth IEEE International Conference on Data Mining
None
2005
While much research has focused on methods for evaluating and maximizing the accuracy of classifiers either individually or in ensembles, little effort has been devoted to analyzing how classifiers are typically deployed in practice. In many domains, classifiers are used as part of a multi-stage process that increases accuracy at the expense of more data collection and/or more processing resources as the likelihood of a positive class label increases. This paper systematically explores the tradeoffs inherent in constructing these multi-stage classifiers from a series of increasingly accurate and expensive individual classifiers, considering a variety of metrics such as accuracy, cost/benefit ratio, and lift. It suggests architectures appropriate for both independent instances and for highly linked data.
[pattern classification, cost-benefit ratio, Event detection, data linking, Data mining, Proposals, data collection, Couplings, multistage classification, Databases, resources processing, US Government, Books]
Learning functional dependency networks based on genetic programming
Fifth IEEE International Conference on Data Mining
None
2005
Bayesian Network (BN) is a powerful network model, which represents a set of variables in the domain and provides the probabilistic relationships among them. But BN can handle discrete values only; it cannot handle continuous, interval and ordinal ones, which must be converted to discrete values and the order information is lost. Thus, BN tends to have higher network complexity and lower understandability. In this paper, we present a novel dependency network which can handle discrete, continuous, interval and ordinal values through functions; it has lower network complexity and stronger expressive power; it can represent any kind of relationships; and it can incorporate a-priori knowledge though user-defined functions. We also propose a novel Genetic Programming (GP) to learn dependency networks. The novel GP does not use any knowledge-guided nor application-oriented operator, thus it is robust and easy to replicate. The experimental results demonstrate that the novel GP can successfully discover the target novel dependency networks, which have the highest accuracy and the lowest network complexity.
[genetic programming, Educational institutions, Biology, genetic algorithms, History, Computer science, Power engineering computing, Bayesian methods, functional dependency network, network complexity, Genetic programming, Computer networks, Robustness, user-defined function, belief networks, Bayesian network, Power engineering and energy]
Generalizing the notion of confidence [Mining association rules]
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we explore extending association analysis to non-traditional types of patterns and nonbinary data by generalizing the notion of confidence. The key idea is to regard confidence as a measure of the extent to which the strength of one association pattern provides information about the strength of another. This approach provides a framework that encompasses the traditional concept of confidence as a special case and can be used as the basis for designing a variety of new confidence measures. Besides discussing such confidence measures, we provide examples that illustrate the potential usefulness of a generalized notion of confidence. In particular, we describe an approach to defining confidence for error tolerant itemsets that preserves the interpretation of confidence as a conditional probability and derive a confidence measure for continuous data that agrees with the standard confidence measure when applied to binary transaction data.
[Pediatrics, binary transaction data, data mining, Data engineering, Association rules, Data mining, confidence measure, association rule mining, error tolerant itemsets, Computer science, Itemsets, Measurement standards, association analysis, association pattern, conditional probability, Particle measurements, Pattern analysis]
SVM feature selection for classification of SPECT images of Alzheimer's disease using spatial information
Fifth IEEE International Conference on Data Mining
None
2005
Alzheimer's disease is the most frequent type of dementia for elderly patients. Due to aging populations the occurrence of this disease will increase in the next years. Early diagnosis is crucial to be able to develop more powerful treatments. Brain perfusion changes can be a marker for Alzheimer's disease. In this article we study the use of SPECT perfusion imaging for the diagnosis of Alzheimer's disease differentiating between images from healthy subjects and images from Alzheimer's disease patients. Our classification approach is based on a linear programming formulation similar to the 1-norm support vector machines. In contrast with other linear hyperplane-based methods that perform simultaneous feature selection and classification, our proposed formulation incorporates proximity information about the features and generates a classifier that does not just select the most relevant voxels but the most relevant "areas" for classification resulting in more robust classifiers that are better suitable for interpretation. This approach is compared with the classical Fisher linear discriminant (FLD) classifier as well as with statistical parametric mapping (SPM). We tested our method on data from four European institutions. Our method achieved sensitivity of 84.4% at 90.9% specificity, this is considerable better the human experts. Our method also outperformed the ELD and SPM techniques. We conclude that our approach has the potential to be a useful help for clinicians.
[linear hyperplane, image classification, Alzheimer disease, linear programming, SPECT perfusion imaging, feature extraction, Aging, Robustness, Alzheimer's disease, medical image processing, feature selection, Testing, support vector machines, Scanning probe microscopy, single photon emission computed tomography, Linear programming, diseases, feature classification, haemorheology, Fisher linear discriminant classifier, Support vector machines, support vector machine, Senior citizens, Support vector machine classification, disease diagnosis, spatial information, Dementia, statistical parametric mapping]
Neighborhood formation and anomaly detection in bipartite graphs
Fifth IEEE International Conference on Data Mining
None
2005
Many real applications can be modeled using bipartite graphs, such as users vs. files in a P2P system, traders vs. stocks in a financial trading system, conferences vs. authors in a scientific publication network, and so on. We introduce two operations on bipartite graphs: 1) identifying similar nodes (Neighborhood formation), and 2) finding abnormal nodes (Anomaly detection). And we propose algorithms to compute the neighborhood for each node using random walk with restarts and graph partitioning; we also propose algorithms to identify abnormal nodes, using neighborhood information. We evaluate the quality of neighborhoods based on semantics of the datasets, and we also measure the performance of the anomaly detection algorithm with manually injected anomalies. Both effectiveness and efficiency of the methods are confirmed by experiments on several real datasets.
[Peer to peer computing, NASA, graph theory, anomaly detection, Partitioning algorithms, Data mining, Noise measurement, bipartite graph, Space technology, neighborhood formation, random walk method, Bipartite graph, graph partitioning, Stock markets, Detection algorithms]
A border-based approach for hiding sensitive frequent itemsets
Fifth IEEE International Conference on Data Mining
None
2005
Sharing data among organizations often leads to mutual benefit. Recent technology in data mining has enabled efficient extraction of knowledge from large databases. This, however, increases risks of disclosing the sensitive knowledge when the database is released to other parties. To address this privacy issue, one may sanitize the original database so that the sensitive knowledge is hidden. The challenge is to minimize the side effect on the quality of the sanitized database so that nonsensitive knowledge can still be mined. In this paper, we study such a problem in the context of hiding sensitive frequent itemsets by judiciously modifying the transactions in the database. To preserve the non-sensitive frequent itemsets, we propose a border-based approach to efficiently evaluate the impact of any modification to the database during the hiding process. The quality of database can be well maintained by greedily selecting the modifications with minimal side effect. Experiments results are also reported to show the effectiveness of the proposed approach.
[Data privacy, Data analysis, privacy issue, data mining, Transaction databases, Data mining, Association rules, Sun, border-based approach, Diseases, sensitive knowledge, Itemsets, knowledge extraction, Marketing and sales, data privacy, Australia, sensitive frequent itemset hiding, data sharing]
X-mHMM: an efficient algorithm for training mixtures of HMMs when the number of mixtures is unknown
Fifth IEEE International Conference on Data Mining
None
2005
In this paper we consider sequence clustering problems and propose an algorithm for the estimation of the number of clusters based on the X-means algorithm. The sequences are modeled using mixtures of Hidden Markov Models. By means of experiments with synthetic data we analyze the proposed algorithm. This algorithm proved to be both computationally efficient and capable of providing accurate estimates of the number of clusters. Some results of experiments with real-world Web-log data are also given.
[Algorithm design and analysis, Sequences, Automation, Data analysis, sequence clustering, X-means algorithm, hidden Markov model, X-mHMM algorithm, Partitioning algorithms, Application software, hidden Markov models, Chemistry, pattern clustering, Hidden Markov models, Clustering algorithms, Biology computing]
A random walk through human associations
Fifth IEEE International Conference on Data Mining
None
2005
Letting one's thoughts wander is not simply an arbitrary or rambling process. It can better be described as "associative thinking\
[Chaos, human associations, interestingness measure, user profile, Force measurement, Humans, data mining, Steady-state, Association rules, Data mining, Diseases, local confidence gain, random walk, association rule discovery, directed graphs, weighted directed graph, user associations, associative thinking, Gain measurement, association graph, Joining processes, Testing]
Supervised tensor learning
Fifth IEEE International Conference on Data Mining
None
2005
This paper aims to take general tensors as inputs for supervised learning. A supervised tensor learning (STL) framework is established for convex optimization based learning techniques such as support vector machines (SVM) and minimax probability machines (MPM). Within the STL framework, many conventional learning machines can be generalized to take n/sup th/-order tensors as inputs. We also study the applications of tensors to learning machine design and feature extraction by linear discriminant analysis (LDA). Our method for tensor based feature extraction is named the tenor rank-one discriminant analysis (TR1DA). These generalized algorithms have several advantages: 1) reduce the curse of dimension problem in machine learning and data mining; 2) avoid the failure to converge; and 3) achieve better separation between the different categories of samples. As an example, we generalize MPM to its STL version, which is named the tensor MPM (TMPM). TMPM learns a series of tensor projections iteratively. It is then evaluated against the original MPM. Our experiments on a binary classification problem show that TMPM significantly outperforms the original MPM.
[data mining, supervised learning, tensors, Data mining, minimax techniques, minimax probability machines, supervised tensor learning, feature extraction, Linear discriminant analysis, learning (artificial intelligence), linear discriminant analysis, learning machine design, support vector machines, Minimax techniques, machine learning, Support vector machines, Computer science, Tensile stress, Supervised learning, Support vector machine classification, Machine learning, Feature extraction, convex optimization, statistical analysis, tenor rank-one discriminant analysis]
A Bernoulli relational model for nonlinear embedding
Fifth IEEE International Conference on Data Mining
None
2005
The notion of relations is extremely important in mathematics. In this paper, we use relations to describe the embedding problem and propose a novel stochastic relational model for nonlinear embedding. Given some relation among points in a high-dimensional space, we start from preserving the same relation in a low embedded space and model the relation as probabilistic distributions over these two spaces, respectively. We illustrate that the stochastic neighbor embedding and the Gaussian process latent variable model can be derived from our relational model. Moreover we devise a new stochastic embedding model and refer to it as Bernoulli relational embedding (BRE). BRE's ability in nonlinear dimensionality reduction is illustrated on a set of synthetic data and collections of bitmaps of handwritten digits and face images.
[Automation, Stochastic processes, Mathematics, Pattern recognition, statistical distributions, stochastic neighbor embedding, high-dimensional space, Computer science, stochastic embedding model, Bernoulli relational model, Space technology, nonlinear embedding, Gaussian process latent variable model, Gaussian processes, Machine learning, nonlinear dimensionality reduction, stochastic relational model, probabilistic distribution, Kernel, Principal component analysis]
Template-based privacy preservation in classification problems
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we present a template-based privacy preservation to protect against the threats caused by data mining abilities. The problem has dual goals: preserve the information for a wanted classification analysis and limit the usefulness of unwanted sensitive inferences that may be derived from the data. Sensitive inferences are specified by a set of "privacy templates". Each template specifies the sensitive information to be protected, a set of identifying attributes, and the maximum association between the two. We show that suppressing the domain values is an effective way to eliminate sensitive inferences. For a large data set, finding an optimal suppression is hard, since it requires optimization over all suppressions. We present an approximate but scalable solution. We demonstrate the effectiveness of this approach on real life data sets.
[Data privacy, pattern classification, Data security, data mining, template-based privacy preservation, Transaction databases, sensitive information, Data mining, classification analysis, Information analysis, sensitive inference, classification problem, Iris, Aggregates, Councils, Frequency, data privacy, privacy templates, Protection]
On reducing classifier granularity in mining concept-drifting data streams
Fifth IEEE International Conference on Data Mining
None
2005
Many applications use classification models on streaming data to detect actionable alerts. Due to concept drifts in the underlying data, how to maintain a model's up-to-dateness has become one of the most challenging tasks in mining data streams. State of the art approaches, including both the incrementally updated classifiers and the ensemble classifiers, have proved that model update is a very costly process. In this paper, we introduce the concept of model granularity. We show that reducing model granularity will reduce model update cost. Indeed, models of fine granularity enable us to efficiently pinpoint local components in the model that are affected by the concept drift. It also enables us to derive new components that can easily integrate with the model to reflect the current data distribution, thus avoiding expensive updates on a global scale. Experiments on real and synthetic data show that our approach is able to maintain good prediction accuracy at a fraction of model updating cost of state of the art approaches.
[pattern classification, Costs, ensemble classifiers, data mining, Predictive models, Ubiquitous computing, Data mining, Accuracy, concept-drifting data stream mining, model granularity reduction, Training data, Decision trees, classifier granularity, model update cost reduction, incrementally updated classifier]
Approximate inverse frequent itemset mining: privacy, complexity, and approximation
Fifth IEEE International Conference on Data Mining
None
2005
In order to generate synthetic basket datasets for better benchmark testing, it is important to integrate characteristics from real-life databases into the synthetic basket datasets. The characteristics that could be used for this purpose include the frequent itemsets and association rules. The problem of generating synthetic basket datasets from frequent itemsets is generally referred to as inverse frequent itemset mining. In this paper, we show that the problem of approximate inverse frequent itemset mining is NP-complete. Then we propose and analyze an approximate algorithm for approximate inverse frequent itemset mining, and discuss privacy issues related to the synthetic basket dataset. In particular, we propose an approximate algorithm to determine the privacy leakage in a synthetic basket dataset.
[Algorithm design and analysis, Data privacy, approximation theory, complexity, data mining, association rules, Linear programming, privacy, Data mining, Association rules, NP-complete problem, inverse frequent itemset mining, approximate inverse frequent itemset mining, synthetic basket dataset, Itemsets, Databases, Character generation, Benchmark testing, approximation algorithm, Frequency, data privacy, benchmark testing, real-life databases, computational complexity]
Atomic wedgie: efficient query filtering for streaming time series
Fifth IEEE International Conference on Data Mining
None
2005
In many applications, it is desirable to monitor a streaming time series for predefined patterns. In domains as diverse as the monitoring of space telemetry, patient intensive care data, and insect populations, where data streams at a high rate and the number of predefined patterns is large, it may be impossible for the comparison algorithm to keep up. We propose a novel technique that exploits the commonality among the predefined patterns to allow monitoring at higher bandwidths, while maintaining a guarantee of no false dismissals. Our approach is based on the widely used envelope-based lower bounding technique. Extensive experiments demonstrate that our approach achieves tremendous improvements in performance in the offline case, and significant improvements in the fastest possible arrival rate of the data stream that can be processed with guaranteed no false dismissal.
[envelope-based lower bounding, Costs, Filtering, time series streaming, Computerized monitoring, data mining, time series, atomic wedgie, Computer science, query processing, query filtering, Patient monitoring, Matched filters, Insects, Space technology, XML, Cardiology]
Discriminatively trained Markov model for sequence classification
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we propose a discriminative counterpart of the directed Markov Models of order k - 1, or MM(k - 1) for sequence classification. MM(k - 1) models capture dependencies among neighboring elements of a sequence. The parameters of the classifiers are initialized to based on the maximum likelihood estimates for their generative counterparts. We derive gradient based update equations for the parameters of the sequence classifiers in order to maximize the conditional likelihood function. Results of our experiments with data sets drawn from biological sequence classification (specifically protein function and subcellular localization) and text classification applications show that the discriminatively trained sequence classifiers outperform their generative counterparts, confirming the benefits of discriminative training when the primary objective is classification. Our experiments also show that the discriminatively trained MM(k - 1) sequence classifiers are competitive with the computationally much more expensive Support Vector Machines trained using k-gram representations of sequences.
[Maximum likelihood detection, Maximum likelihood estimation, pattern classification, Laboratories, discriminative training, text classification, maximum likelihood estimation, biological sequence classification, Equations, gradient based update equation, Proteins, Learning, Computer science, Text categorization, directed Markov model, Markov processes, conditional likelihood function, discriminatively trained Markov model, Artificial intelligence, Computational intelligence]
Integrating hidden Markov models and spectral analysis for sensory time series clustering
Fifth IEEE International Conference on Data Mining
None
2005
We present a novel approach for clustering sequences of multi-dimensional trajectory data obtained from a sensor network. The sensory time-series data present new challenges to data mining, including uneven sequence lengths, multi-dimensionality and high levels of noise. We adopt a principled approach, by first transforming all the data into an equal-length vector form while keeping as much temporal information as we can, and then applying dimensionality and noise reduction techniques such as spectral clustering to the transformed data. Experimental evaluation on synthetic and real data shows that our proposed approach outperforms standard model-based clustering algorithms for time series data.
[Machine learning algorithms, wireless sensor networks, hidden Markov model, Noise reduction, data mining, multidimensional trajectory data, Sensor phenomena and characterization, spectral analysis, Data mining, hidden Markov models, data reduction, dimensionality reduction, Clustering algorithms, noise reduction, spectral clustering, sequence clustering, time series, Time measurement, Spectral analysis, Wireless sensor networks, Working environment noise, pattern clustering, sensor network, Hidden Markov models, equal-length vector, sensory time series clustering]
Discriminant analysis: a unified approach
Fifth IEEE International Conference on Data Mining
None
2005
Linear discriminant analysis (LDA) as a dimension reduction method is widely used in data mining and machine learning. It however suffers from the small sample size (SSS) problem when data dimensionality is greater than the sample size. Many modified methods have been proposed to address some aspect of this difficulty from a particular viewpoint. A comprehensive framework that provides a complete solution to the SSS problem is still missing. In this paper, we provide a unified approach to LDA, and investigate the SSS problem in the framework of statistical learning theory. In such a unified approach, our analysis results in a deeper understanding of LDA. We demonstrate that LDA (and its nonlinear extension) belongs to the same framework where powerful classifiers such as support vector machines (SVMs) are formulated. In addition, this approach allows us to establish an error bound for LDA. Finally our experiments validate our theoretical analysis results.
[Statistical learning, data mining, statistical learning theory, Mathematics, Data mining, machine learning, dimension reduction, Equations, Computer science, Support vector machines, data reduction, small sample size problem, Support vector machine classification, Machine learning, Linear discriminant analysis, learning (artificial intelligence), statistical analysis, linear discriminant analysis, Kernel]
Sharing classifiers among ensembles from related problem domains
Fifth IEEE International Conference on Data Mining
None
2005
A classification ensemble is a group of classifiers that all solve the same prediction problem in different ways. It is well-known that combining the predictions of classifiers within the same problem domain using techniques like bagging or boosting often improves the performance. This research shows that sharing classifiers among different but closely related problem domains can also be helpful. In addition, a semi-definite programming based ensemble pruning method is implemented in order to optimize the selection of a subset of classifiers for each problem domain. Computational results on a catalog dataset indicate that the ensembles resulting from sharing classifiers among different product categories generally have larger AUCs than those ensembles trained only on their own categories. The pruning algorithm not only prevents the occasional decrease of effectiveness caused by conflicting concepts among the problem domains, but also provides a better understanding of the problem domains and their relationships.
[pattern classification, Optimization methods, Boosting, Credit cards, Data mining, classification ensemble, ensemble pruning, semidefinite programming, Voting, Neural networks, prediction problem, Cities and towns, Decision trees, Bagging, Classification tree analysis]
A visual data mining framework for convenient identification of useful knowledge
Fifth IEEE International Conference on Data Mining
None
2005
Data mining algorithms usually generate a large number of rules, which may not always be useful to human users. In this project, we propose a novel visual data-mining framework, called Opportunity Map, to identify useful and actionable knowledge quickly and easily from the discovered rules. The framework is inspired by the House of Quality from Quality Function Deployment (QFD) in Quality Engineering. It associates discovered rules, related summarized data and data distributions with the application objective using an interactive matrix. Combined with drill down visualization, integrated visualization of data distribution bars and rules, visualization of trend behaviors, and comparative analysis, the Opportunity Map allows users to analyze rules and data at different levels of detail and quickly identify the actionable knowledge and opportunities. The proposed framework represents a systematic and flexible approach to rule analysis. Applications of the system to large-scale data sets from our industrial partner have yielded promising results.
[Data analysis, Humans, data mining, Data mining, trend behavior visualization, visual data mining framework, interactive matrix, Computer science, comparative analysis, drill down visualization, Data visualization, data visualisation, Opportunity Map, Quality function deployment, Manufacturing, Large-scale systems, quality function deployment, data visualization, Bars, Quality management]
Efficient text classification by weighted proximal SVM
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we present an algorithm that can classify large-scale text data with high classification quality and fast training speed. Our method is based on a novel extension of the proximal SVM mode (Fung and Mangasarian, 2001). Previous studies on proximal SVM have focused on classification for low dimensional data and did not consider the unbalanced data cases. Such methods will meet difficulties when classifying unbalanced and high dimensional data sets such as text documents. In this work, we extend the original proximal SVM by learning a weight for each training error. We show that the classification algorithm based on this model is capable of handling high dimensional and unbalanced data. In the experiments, we compare our method with the original proximal SVM (as a special case of our algorithm) and the standard SVM (such as SVM light) on the recently published RCV1-v2 dataset. The results show that our proposed method had comparable classification quality with the standard SVM. At the same time, both the time and memory consumption of our method are less than that of the standard SVM.
[text analysis, support vector machines, Statistical learning, large-scale text data, text documents, unbalanced data, Classification algorithms, text classification, Standards publication, weight learning, Support vector machines, Computer science, Information science, high dimensional data, Text categorization, Asia, Support vector machine classification, Large-scale systems, weighted proximal support vector maching]
A rule evaluation support method with learning models based on objective rule evaluation indexes
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we present a novel rule evaluation support method for post-processing of mined results with rule evaluation models based on objective indexes. Post-processing of mined results is one of the key issues to make a data mining process successfully. However, it is difficult for human experts to evaluate many thousands of rules from a large dataset with noises completely. To reduce the costs of rule evaluation procedures, we have developed the rule evaluation support method with rule evaluation models, which are obtained with objective rule evaluation indexes and evaluations of a human expert for each rule. Since the method is needed more accurate rule evaluation models, we have compared learning algorithms to construct rule evaluation models with the actual meningitis data mining result and actual rule sets from UCI datasets. Then we show the availability of our adaptive rule evaluation support method.
[Availability, learning algorithm, Costs, objective rule evaluation index, Humans, Biomedical informatics, data mining, rule evaluation model, Predictive models, Data mining, Information systems, adaptive rule evaluation support, Training data, Prediction algorithms, learning model, learning (artificial intelligence), Biomedical engineering]
Mining chains of relations
Fifth IEEE International Conference on Data Mining
None
2005
Traditional data mining applications consider the problem of mining a single relation between two attributes. For example, in a scientific bibliography database, authors are related to papers, and we may be interested in discovering association rules between authors. However, in real life, we often have multiple attributes related though chains of relations. For example, authors write papers, and papers concern one or more topics. Mining such relational chains poses additional challenges. In this paper we consider the following problem: given a chain of two relations R/sub 1/ (A, P) and R/sub 2/(P, T) we want to find selectors for the objects in T such that the projected relation between A and P satisfies a specific property. The motivation for our approach is that a given property might not hold on the whole dataset, but it might hold when projecting the data on a selector set. We discuss various algorithms and we examine the conditions under which the a priori technique can be used. We experimentally demonstrate the effectiveness of our methods.
[relational chain, association rule discovery, Bibliographies, data mining, Relational databases, Writing, Motion pictures, Data models, Data mining, Association rules, Joining processes]
A preference model for structured supervised learning tasks
Fifth IEEE International Conference on Data Mining
None
2005
The preference model introduced in this paper gives a natural framework and a principled solution for a broad class of supervised learning problems with structured predictions, such as predicting orders (label and instance ranking), and predicting rates (classification and ordinal regression). We show how all these problems can be cast as linear problems in an augmented space, and we propose an on-line method to efficiently solve them. Experiments on an ordinal regression task confirm the effectiveness of the approach.
[Algorithm design and analysis, ordinal regression, Minimization methods, instance ranking, regression analysis, preference model, label ranking, Predictive models, classification regression, Data mining, rate prediction, order prediction, structured supervised learning, structured prediction, Supervised learning, Cost function, learning (artificial intelligence), Plugs]
Blocking anonymity threats raised by frequent itemset mining
Fifth IEEE International Conference on Data Mining
None
2005
In this paper we study when the disclosure of data mining results represents, per se, a threat to the anonymity of the individuals recorded in the analyzed database. The novelty of our approach is that we focus on an objective definition of privacy compliance of patterns without any reference to a preconceived knowledge of what is sensitive and what is not, on the basis of the rather intuitive and realistic constraint that the anonymity of individuals should be guaranteed. In particular, the problem addressed here arises from the possibility of inferring from the output of frequent itemset mining (i.e., a set of item-sets with support larger than a threshold a), the existence of patterns with very low support (smaller than an anonymity threshold k)[M. Atzori et. al, 2005]. In the following we develop a simple methodology to block such inference opportunities by introducing distortion on the dangerous patterns.
[database analysis, Data privacy, pattern classification, Data analysis, Laboratories, data mining, pattern privacy compliance, Data mining, Association rules, frequent itemset mining, Computer science, pattern distortion, Itemsets, Databases, anonymity threat blocking, data privacy, Distortion measurement, Protection]
Adaptive clustering: obtaining better clusters using feedback and past experience
Fifth IEEE International Conference on Data Mining
None
2005
Adaptive clustering uses external feedback to improve cluster quality; past experience serves to speed up execution time. An adaptive clustering environment is proposed that uses Q-learning to learn the reward values of successive data clusterings. Adaptive clustering supports the reuse of clusterings by memorizing what worked well in the past. It has the capability of exploring multiple paths in parallel when searching for good clusters. In a case study, we apply adaptive clustering to instance-based learning relying on a distance function modification approach. A distance function adaptation scheme that uses external feedback is proposed and compared with other distance function learning approaches. Experimental results indicate that the use of adaptive clustering leads to significant improvements of instance-based learning techniques, such as k-nearest neighbor classifiers. Moreover, as a by-product a new instance-based learning technique is introduced that classifies examples by solely using cluster representatives; this technique shows high promise in our experimental evaluation.
[Performance evaluation, Q-learning, data analysis, k-nearest neighbor classifier, data clustering, State-space methods, Data mining, Unsupervised learning, instance-based learning, Computer science, adaptive clustering, pattern clustering, Feedback, Clustering algorithms, distance function learning, learning (artificial intelligence), external feedback]
Semi-supervised mixture of kernels via LPBoost methods
Fifth IEEE International Conference on Data Mining
None
2005
We propose an algorithm to construct classification models with a mixture of kernels from labeled and unlabeled data. The derived classifier is a mixture of models, each based on one kernel choice from a library of kernels. The sparse-favoring 1-norm regularization method is employed to restrict the complexity of mixture models and to achieve the sparsity of solutions. By modifying the column generation boosting algorithm LPBoost to a more general linear programming formulation, we are able to efficiently solve mixture-of-kernel problems and automatically select kernel basis functions centered at labeled data as well as unlabeled data. The effectiveness of the proposed approach is proved by experimental results on benchmark datasets.
[semisupervised mixture-of-kernel problem, Medical treatment, Predictive models, Boosting, Linear programming, linear programming, boosting algorithm, Classification algorithms, classification model, LPBoost methods, Bismuth, Libraries, learning (artificial intelligence), sparse-favoring 1-norm regularization, Kernel, Medical diagnostic imaging, mixture model complexity, Testing, computational complexity]
A levelwise search algorithm for interesting subspace clusters
Fifth IEEE International Conference on Data Mining
None
2005
We present a levelwise search algorithm for finding subspace clusters in high dimensional data satisfying various properties besides the commonly used minimum density property. A set of such properties are summarized and a user can choose any of these properties. A lattice is built with all the discovered clusters which enables further analysis and discovery of useful knowledge about the clusters and their inter-relationships.
[Algorithm design and analysis, Machine learning algorithms, Lattices, minimum density property, Data mining, Gene expression, Association rules, interesting subspace clusters, pattern clustering, high dimensional data, Clustering algorithms, Machine learning, levelwise search algorithm, search problems]
Segment-based injection attacks against collaborative filtering recommender systems
Fifth IEEE International Conference on Data Mining
None
2005
Significant vulnerabilities have recently been identified in collaborative filtering recommender systems. Researchers have shown that attackers can manipulate a system's recommendations by injecting biased profiles into it. In this paper, we examine attacks that concentrate on a targeted set of users with similar tastes, biasing the system's responses to these users. We show that such attacks are both pragmatically reasonable and also highly effective against both user-based and item-based algorithms. As a result, an attacker can mount such a "segmented" attack with little knowledge of the specific system being targeted and with strong likelihood of success.
[item-based algorithm, collaborative filtering recommender systems, user-based algorithm, information filtering, Information filtering, segment-based injection attack, Information systems, Computer science, Databases, security of data, Collaboration, Filtering algorithms, biased profile injection, Information filters, Robustness, Books, Recommender systems]
On feature selection through clustering
Fifth IEEE International Conference on Data Mining
None
2005
We study an algorithm for feature selection that clusters attributes using a special metric and then makes use of the dendrogram of the resulting cluster hierarchy to choose the most relevant attributes. The main interest of our technique resides in the improved understanding of the structure of the analyzed data and of the relative importance of the attributes for the selection process.
[Algorithm design and analysis, Data analysis, Terminology, Relational databases, classification algorithm, Data mining, Computer science, pattern clustering, Clustering algorithms, Bismuth, Robustness, Performance analysis, feature selection, attribute clustering]
Sequential pattern mining in multiple streams
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we deal with mining sequential patterns in multiple data streams. Building on a state-of-the-art sequential pattern mining algorithm PrefixSpan for mining transaction databases, we propose MILE, an efficient algorithm to facilitate the mining process. MILE recursively utilizes the knowledge of existing patterns to avoid redundant data scanning, and can therefore effectively speed up the new patterns' discovery process. Another unique feature of MILE is that it can incorporate some prior knowledge of the data distribution in data streams into the mining process to further improve the performance. Extensive empirical results show that MILE is significantly faster than PrefixSpan. As MILE consumes more memory than PrefixSpan, we also present a solution to balance the memory usage and time efficiency in memory constrained environments.
[transaction processing, PrefixSpan algorithm, pattern classification, data distribution, data mining, pattern discovery, Transaction databases, Steady-state, History, Heart rate, Computer science, multiple data streams, MILE algorithm, sequential pattern mining, Pattern matching, transaction database mining]
Privacy preserving data classification with rotation perturbation
Fifth IEEE International Conference on Data Mining
None
2005
Data perturbation techniques are one of the most popular models for privacy preserving data mining (Agrawal and Srikant, 2000; Aggarwal and Yu, 2004). It is especially convenient for applications where the data owners need to export/publish the privacy-sensitive data. A data perturbation procedure can be simply described as follows. Before the data owner publishes the data, they randomly change the data in certain way to disguise the sensitive information while preserving the particular data property that is critical for building the data models. Several perturbation techniques have been proposed recently, among which the most typical ones are randomization approach (Agrawal and Srikant, 2000) and condensation approach (Aggarwal and Yu, 2004).
[Data privacy, pattern classification, data model, condensation approach, randomization approach, data mining, Educational institutions, data perturbation, rotation perturbation, Data mining, Association rules, Resilience, Noise level, privacy preserving data classification, Perturbation methods, Data models, data privacy, Kernel, Protection, privacy preserving data mining]
A computational framework for taxonomic research: diagnosing body shape within fish species complexes
Fifth IEEE International Conference on Data Mining
None
2005
It is estimated that ninety percent of the world's species have yet to be discovered and described. The main reason for the slow pace of new species description is that the science of taxonomy, as traditionally practiced, can be very laborious. To formally describe a new species, taxonomists have to manually gather and analyze data from large numbers of specimens, often from broad geographic areas, and identify the smallest subset of external body characters that uniquely diagnoses the new species as distinct from all its known relatives. In this paper, we use an automated feature selection and classification approach to address the taxonomic impediment in new species discovery. The experiments on a taxonomic problem involving species of suckers in the genus Carpiodes demonstrate promising results.
[pattern classification, automated feature selection, Data analysis, Sequences, Shape, data analysis, Taxonomy, Rivers, taxonomic research, History, Computer science, new species discovery, fish species complexes, computational framework, body shape diagnosis, DNA, automated feature classification, Marine animals, Software tools, zoology]
Obtaining best parameter values for accurate classification
Fifth IEEE International Conference on Data Mining
None
2005
In this paper we examine the effect that the choice of support and confidence thresholds has on the accuracy of classifiers obtained by classification association rule mining. We show that accuracy can almost always be improved by a suitable choice of threshold values, and we describe a method for finding the best values. We present results that demonstrate this approach can obtain higher accuracy without the need for coverage analysis of the training data.
[Software testing, pattern classification, Costs, Smoothing methods, best parameter value, data mining, classification association rule mining, Data mining, Association rules, Computer science, accurate classification, Training data, Machine learning, confidence threshold]
Process diagnosis via electrical-wafer-sorting maps classification
Fifth IEEE International Conference on Data Mining
None
2005
The commonality analysis is a proven tool for fault detection in semiconductor manufacturing. This methodology extracts subsets of production lots from all the available data. Then, data mining techniques are used only on the selected data. This approach loses part of the available information and does not discriminate among the lots. The new methodology performance the automatic classification of the electrical wafer test maps in order to identify the classes of failure present in the production lots. Subsequently, the proposed procedure uses the process history of each wafer to create a list of the root cause candidates. This methodology is the core of the software tool ACID which is currently used for process diagnosis at the Agrate site of the ST Microelectronics. A real analysis is presented.
[pattern classification, fault diagnosis, data mining, fault detection, process planning, Microelectronics, Data mining, History, electrical wafer test map, Research and development, Information analysis, process diagnosis, semiconductor device manufacture, automatic classification, Fault detection, Automatic testing, commonality analysis, Production, Semiconductor device manufacture, semiconductor manufacturing, Software tools, electrical-wafer-sorting maps classification]
An improved categorization of classifier's sensitivity on sample selection bias
Fifth IEEE International Conference on Data Mining
None
2005
A recent paper categorizes classifier learning algorithms according to their sensitivity to a common type of sample selection bias where the chance of an example being selected into the training sample depends on its feature vector x but not (directly) on its class label y. A classifier learner is categorized as "local" if it is insensitive to this type of sample selection bias, otherwise, it is considered "global". In that paper, the true model is not clearly distinguished from the model that the algorithm outputs. In their discussion of Bayesian classifiers, logistic regression and hard-margin SVMs, the true model (or the model that generates the true class label for every example) is implicitly assumed to be contained in the model space of the learner, and the true class probabilities and model estimated class probabilities are assumed to asymptotically converge as the training data set size increases. However, in the discussion of naive Bayes, decision trees and soft-margin SVMs, the model space is assumed not to contain the true model, and these three algorithms are instead argued to be "global learners". We argue that most classifier learners may or may not be affected by sample selection bias; this depends on the dataset as well as the heuristics or inductive bias implied by the learning algorithm and their appropriateness to the particular dataset.
[pattern classification, support vector machines, classifier sensitivity categorization, classifier learning, regression analysis, hard-margin support vector machine, sample selection bias, Data mining, Bayesian classifier, Computer science, Support vector machines, Bayesian methods, Training data, decision trees, naive Bayes, Bayes methods, Decision trees, learning (artificial intelligence), logistic regression, Regression tree analysis, Logistics, Testing, Classification tree analysis]
Fast frequent string mining using suffix arrays
Fifth IEEE International Conference on Data Mining
None
2005
We present a method to mine strings that are frequent in one database and infrequent in another. The method uses suffix- and lcp-arrays that can be computed extremely fast and space efficiently, and further exhibit a good locality behavior. Experiments with several biologically relevant data sets show that our approach outperforms existing methods in terms of time and space.
[Sequences, Lattices, data mining, Spatial databases, Data mining, Biology computing, Frequency, string matching, fast frequent string mining, lcp arrays, Computational biology, computational complexity, suffix arrays]
Privacy-preserving frequent pattern mining across private databases
Fifth IEEE International Conference on Data Mining
None
2005
Privacy consideration has much significance in the application of data mining. It is very important that the privacy of individual parties is not exposed when data mining techniques are applied to a large collection of data about the parties. In many scenarios such as data warehousing or data integration, data from the different parties form a many-to-many schema. This paper addresses the problem of privacy-preserving frequent pattern mining in such a schema across two dimension sites. We assume that sites are not trusted and they are semi-honest. Our method is based on the concept of semi-join and does not involve data encryption which is used in most previous work. Experiments are conducted to study the efficiency of the proposed models.
[Data privacy, data mining, Relational databases, Data engineering, Data mining, Application software, database management systems, Computer science, Road accidents, Itemsets, privacy-preserving frequent pattern mining, Warehousing, Tires, data privacy, private database]
CoLe: a cooperative data mining approach and its application to early diabetes detection
Fifth IEEE International Conference on Data Mining
None
2005
We present CoLe, a cooperative data mining approach for discovering hybrid knowledge. It employs multiple different data mining algorithms, and combines results from them to enhance the mined knowledge. For our medical application area, we analyse several focusing strategies that allowed us to gain medically significant results.
[CoLe, Biomedical informatics, data mining, Medical services, diseases, Data mining, Application software, hybrid knowledge discovery, Computer science, Feedback, Back, Diabetes, Feeds, medical computing, Medical diagnostic imaging, cooperative data mining, early diabetes detection]
Feature selection for building cost-effective data stream classifiers
Fifth IEEE International Conference on Data Mining
None
2005
A stream classifier is a decision model that assigns a class label to a data stream, based on its arriving data. Various features of the stream can be used in the classifier, each of which may have different relevance to the classification task and different cost in obtaining its value. As time passes by, some less costly features may become more relevant, but the time needed for decision may be considered as a cost. A challenge is how to balance the different costs when building a cost-effective classifier. This paper proposes a new feature selection strategy that extends the traditional relief algorithm in two aspects: (1) estimate the classification cost associated with each feature, and (2) order all the features with a score that combines both cost estimation and classification relevance. A classifier is then built with the selected features using a traditional classification method. Experimental results show that classifiers constructed with this strategy are indeed cost effective.
[Computer science, pattern classification, Costs, Temperature, Buildings, Training data, classification cost estimation, data handling, classification relevance, Data mining, feature selection, cost-effective data stream classifier]
A scalable collaborative filtering framework based on co-clustering
Fifth IEEE International Conference on Data Mining
None
2005
Collaborative filtering-based recommender systems have become extremely popular due to the increase in Web-based activities such as e-commerce and online content distribution. Current collaborative filtering (CF) techniques such as correlation and SVD based methods provide good accuracy, but are computationally expensive and can be deployed only in static off-line settings. However, a number of practical scenarios require dynamic real-time collaborative filtering that can allow new users, items and ratings to enter the system at a rapid rate. In this paper, we consider a novel CF approach based on a proposed weighted co-clustering algorithm (Banerjee et al., 2004) that involves simultaneous clustering of users and items. We design incremental and parallel versions of the co-clustering algorithm and use it to build an efficient real-time CF framework. Empirical evaluation demonstrates that our approach provides an accuracy comparable to that of the correlation and matrix factorization based approaches at a much lower computational cost.
[Real time systems, Algorithm design and analysis, parallel algorithms, parallel algorithm, information filtering, Information filtering, real-time collaborative filtering, information filters, collaborative filtering-based recommender system, Computer science, Clustering algorithms, real-time systems, weighted coclustering, Information filters, Computational efficiency, Internet, Online Communities/Technical Collaboration, Recommender systems]
Text classification with evolving label-sets
Fifth IEEE International Conference on Data Mining
None
2005
We introduce the evolving label-set problem encountered in building real-world text classification systems. This problem arises when a text classification system trained on a label-set encounters documents of unseen classes at deployment time. We design a class-detector module that monitors unlabeled data, detects new classes, and suggests them to the administrator for inclusion in the label-set. We propose abstractions that group together tokens under human understandable concepts and provide a mechanism of assigning importance to unseen terms. We present generative algorithms leveraging the notion of support of documents in a model for (1) selecting documents of proposed new classes, and (2) automatically triggering detection of new classes. Experiments on three real world taxonomies show that our methods select new class documents with high precision, and trigger emergence of new classes with low false-positive and false-negative rates.
[Algorithm design and analysis, text analysis, Error analysis, Taxonomy, Buildings, Humans, text classification, Data mining, classification, generative algorithm, class-detector module, Text categorization, document selection, Robustness, Australia, evolving label-sets, Constitution]
CloseMiner: discovering frequent closed itemsets using frequent closed tidsets
Fifth IEEE International Conference on Data Mining
None
2005
Complete set of itemsets can be grouped into non-overlapping clusters identified by closed tidsets. Each cluster has only one closed itemset and is the superset of all itemsets with the same support. Number of closed itemsets is identical to the number of clusters. Therefore, the problem of discovering closed itemsets can be considered as the problem of clustering the complete set of itemsets by closed tidsets. In this paper, we present CloseMiner, a new algorithm for discovering all frequent closed itemsets by grouping the complete set of itemsets into non-overlapping clusters identified by closed tidsets. An extensive experimental evaluation on a number of real and synthetic databases shows that CloseMiner outperforms Apriori and CHARM.
[frequent closed itemset discovery, data mining, CloseMiner, Transaction databases, set theory, Data mining, Association rules, Computer science, Itemsets, real database, Clustering algorithms, frequent closed tidsets, synthetic database]
A framework for semi-supervised learning based on subjective and objective clustering criteria
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we propose a semi-supervised framework for learning a weighted Euclidean subspace, where the best clustering can be achieved. Our approach capitalizes on user-constraints and the quality of intermediate clustering results in terms of its structural properties. It uses the clustering algorithm and the validity measure as parameters.
[semisupervised learning, Partitioning algorithms, Data mining, objective clustering criteria, Organizing, subjective clustering criteria, Constraint optimization, pattern clustering, Clustering algorithms, Euclidean distance, Semisupervised learning, weighted Euclidean subspace, learning (artificial intelligence)]
Focused community discovery
Fifth IEEE International Conference on Data Mining
None
2005
We present a new approach to community discovery. Community discovery usually partitions the graph into communities or clusters. Focused community discovery allows the searcher to specify start points of interest, and find the community of those points. Focused search allows for a much more scalable algorithm in which the time depends only on the size of the community, and not on the number of nodes in the graph, and so is scalable to arbitrarily large graphs. Furthermore, our algorithm is robust to imperfect data, such as extra or missing edges in the graph. We show the effectiveness of our algorithm using both synthetic graphs and on the real-life Livejournal friends graph, a publicly-available social network consisting of over two million users and 13 million edges.
[Costs, Social network services, graph theory, Telecommunication traffic, Size measurement, publicly-available social network, Partitioning algorithms, Data mining, focused community discovery, Livejournal friends graph, Clustering algorithms, social sciences, Robustness]
Suppressing data sets to prevent discovery of association rules
Fifth IEEE International Conference on Data Mining
None
2005
Enterprises have been collecting data for many reasons including better customer relationship management, and high-level decision making. Public safety was another motivation for large-scale data collection efforts initiated by government agencies. However, such widespread data collection efforts coupled with powerful data analysis tools raised concerns about privacy. This is due to the fact that collected data may contain confidential information. One method to ensure privacy is to selectively hide confidential information from the data sets to be disclosed. In this paper, we focus on hiding confidential correlations. We introduce a heuristic to reduce the information loss and propose a blocking method that prevents discovery of confidential correlations while preserving the usefulness of the data set.
[Data privacy, confidential information hiding, Decision making, Government, data mining, association rule discovery prevention, Data engineering, Association rules, Data mining, data set suppression, Customer relationship management, data privacy, Safety, Large-scale systems, Power engineering and energy]
Triple jump acceleration for the EM algorithm
Fifth IEEE International Conference on Data Mining
None
2005
This paper presents the triple jump framework for accelerating the EM algorithm and other bound optimization methods. The idea is to extrapolate the third search point based on the previous two search points found by regular EM. As the convergence rate of regular EM becomes slower, the distance of the triple jump is longer, and thus provide higher speedup for data sets where EM converges slowly. Experimental results show that the triple jump framework significantly outperforms EM and other acceleration methods of EM for a variety of probabilistic models, especially when the data set is sparse. The results also show that the triple jump framework is particularly effective for cluster models.
[bound optimization, triple jump acceleration, Optimization methods, Switches, third search point extrapolation, Convergence, Extrapolation, Information science, optimisation, extrapolation, Bayesian methods, pattern clustering, Clustering algorithms, Hidden Markov models, Gaussian processes, expectation-maximisation algorithm, Acceleration, EM algorithm, search problems]
Partial ensemble classifiers selection for better ranking
Fifth IEEE International Conference on Data Mining
None
2005
Ranking is an important task in data mining and knowledge discovery. We propose a novel approach called PECS algorithm to improve the overall ranking performance of a given ensemble. We formally analyse the sufficient and necessary condition under which PECS algorithm can effectively improve ensemble ranking performance. The experiments with real-world data sets show that this new approach achieves significant improvements in ranking over the original bagging and Adaboost ensembles.
[Algorithm design and analysis, pattern classification, data mining, Boosting, Data engineering, knowledge discovery, Classification algorithms, Data mining, Computer science, PECS algorithm, ensemble ranking performance, Performance analysis, Internet, partial ensemble classifiers selection, Bagging, Testing]
Pairwise symmetry decomposition method for generalized covariance analysis
Fifth IEEE International Conference on Data Mining
None
2005
We propose a new theoretical framework for generalizing the traditional notion of covariance. First, we discuss the role of pairwise cross-cumulants by introducing a cluster expansion technique for the cumulant generating function. Next, we introduce a novel concept of symmetry decomposition of probability density functions according to the C/sub 4V/ group. By utilizing the irreducible representations, generalized covariances are explicitly defined, and their utility is demonstrated using an analytically solvable model.
[Laboratories, probability, generalized covariance analysis, Gaussian distribution, Taylor series, Covariance matrix, Pattern recognition, pairwise cross-cumulants, Data mining, pairwise symmetry decomposition, pattern clustering, cluster expansion, Probability density function, covariance analysis, Kernel, probability density function]
FS/sup 3/: a random walk based free-form spatial scan statistic for anomalous window detection
Fifth IEEE International Conference on Data Mining
None
2005
Often, it is required to identify anomalous windows over a spatial region that reflect unusual rate of occurrence of a specific event of interest. A spatial scan statistic essentially considers a scan window, and identifies anomalous windows by moving the scan window in the region. While spatial scan statistic has been successful, earlier proposals suffer from two limitations: (i) They restrict the scan window to be of a regular shape (e.g., circle, rectangle, cylinder). However, the region of anomaly, in general, is not necessarily of a regular shape. (ii) They take into account autocorrelation among spatial data, but not spatial heterogeneity. As a result, they often result in inaccurate anomalous windows. To address these limitations, we propose a random walk based free-form spatial scan statistic (FS/sup 3/). Application of FS/sup 3/ on real datasets has shown that it can identify more refined anomalous windows with better likelihood ratio of it being an anomaly, than those identified by earlier spatial scan statistic approaches.
[Shape, Event detection, data analysis, anomalous window detection, random processes, Proposals, Statistics, FS/sup 3/, random walk based free-form spatial scan statistic, Diseases, Road transportation, Road accidents, Surveillance, Statistical distributions, scan window, Autocorrelation, statistical analysis]
Mining ontological knowledge from domain-specific text documents
Fifth IEEE International Conference on Data Mining
None
2005
Traditional text mining systems employ shallow parsing techniques and focus on concept extraction and taxonomic relation extraction. This paper presents a novel system called CRCTOL for mining rich semantic knowledge in the form of ontology from domain-specific text documents. By using a full text parsing technique and incorporating both statistical and lexico-syntactic methods, the knowledge extracted by our system is more concise and contains a richer semantics compared with alternative systems. We conduct a case study wherein CRCTOL extracts ontological knowledge, specifically key concepts and semantic relations, from a terrorism domain text collection. Quantitative evaluation, by comparing with a state-of-the-art ontology learning system known as text-to-onto, has shown that CRCTOL produces much better precision and recall for both concept and relation extraction, especially from sentences with complex structures.
[domain-specific text document, text analysis, Spine, data mining, Ontologies, full text parsing, Data mining, relation extraction, Learning systems, ontological knowledge mining, concept extraction, statistical method, Libraries, Natural language processing, Software agents, text mining, ontology learning, Text mining, concept relation concept tuple, Terrorism, learning systems, Semantic Web, grammars, ontologies (artificial intelligence), statistical analysis, lexico-syntactic method]
Mining patterns that respond to actions
Fifth IEEE International Conference on Data Mining
None
2005
Data mining focuses on patterns that summarize the data. In this paper, we focus on mining patterns that could change the state by responding to opportunities of actions.
[pattern mining, Education, data mining, Microeconomics, Turning, Concrete, Spatial databases, Data mining, Decision trees, pattern recognition]
Supervised ordering - an empirical survey
Fifth IEEE International Conference on Data Mining
None
2005
Ordered lists of objects are widely used as representational forms. Such ordered objects include Web search results or bestseller lists. In spite of their importance, methods of processing orders have received little attention. However, research concerning orders has become common; in particular, researchers have developed various methods for the task of supervised ordering to acquire functions for object sorting from example orders. Here, we give a unified view of these methods and our new one, and empirically survey their merits and demerits.
[Laboratories, ordered object, supervised ordering, Data mining, Sorting, Telegraphy, Communication industry, sorting, Telephony, Search engines, Marketing and sales, ordered lists, Random variables, learning (artificial intelligence), Web search, object sorting]
Categorization and keyword identification of unlabeled documents
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we first propose a global unsupervised feature selection approach for text, based on frequent itemset mining. As a result, each document is represented as a set of words that co-occur frequently in the given corpus of documents. We then introduce a locally adaptive clustering algorithm, designed to estimate (local) word relevance and, simultaneously, to group the documents. We present experimental results to demonstrate the feasibility of our approach. Furthermore, the analysis of the weights credited to terms provides evidence that the identified keywords can guide the process of label assignment to clusters. We take into consideration both spam email filtering and general classification datasets. Our analysis of the distribution of weights in the two cases provides insights on how the spam problem distinguishes from the general classification case.
[Algorithm design and analysis, label assignment, text analysis, Dictionaries, data mining, global unsupervised feature selection, Predictive models, unsolicited e-mail, information filtering, Data mining, Itemsets, adaptive clustering, feature extraction, Clustering algorithms, text mining, unlabeled document categorization, Data analysis, Filtering, Functional analysis, classification, frequent itemset mining, pattern clustering, spam email filtering, word relevance, general classification dataset, Indexing, keyword identification]
Gradual model generator for single-pass clustering
Fifth IEEE International Conference on Data Mining
None
2005
We present an algorithm for generating a mixture model from data set by performing a single pass over the data. The method is applicable when the entire data is not available at the same time in the main memory. We use Gaussian mixture model but the algorithm can be adapted to other types of models, too. We also outline a post processing method, which can iteratively reduce the size of the model obtained by the single-pass algorithm. This results in a model with fewer components, but with approximately the same representation accuracy than the result of the original model from the single-pass algorithm.
[Algorithm design and analysis, post processing method, single-pass clustering, mixture model generation, Data mining, Gaussian mixture model, Computer science, pattern clustering, Clustering algorithms, Gaussian processes, Iterative algorithms, Computer buffers, gradual model generation]
Making logistic regression a core data mining tool with TR-IRLS
Fifth IEEE International Conference on Data Mining
None
2005
Binary classification is a core data mining task. For large datasets or real-time applications, desirable classifiers are accurate, fast, and need no parameter tuning. We present a simple implementation of logistic regression that meets these requirements. A combination of regularization, truncated Newton methods, and iteratively re-weighted least squares make it faster and more accurate than modern SVM implementations, and relatively insensitive to parameters. It is robust to linear dependencies and some scaling problems, making most data preprocessing unnecessary.
[pattern classification, least squares approximations, Data preprocessing, data mining, truncated Newton method, iteratively reweighted least squares, regression analysis, Data mining, Sparse matrices, Least squares methods, Computer science, Support vector machines, data mining tool, regularization method, Character generation, Support vector machine classification, logistic regression, binary classification, Newton method, Logistics]
Hierarchical density-based clustering of uncertain data
Fifth IEEE International Conference on Data Mining
None
2005
The hierarchical density-based clustering algorithm OPTICS has proven to help the user to get an overview over large data sets. When using OPTICS for analyzing uncertain data which naturally occur in many emerging application areas, e.g. location based services, or sensor databases, the similarity between uncertain objects has to be expressed by one numerical distance value. Based on such single-valued distance functions OPTICS, like other standard data mining algorithms, can work without any changes. In this paper, we propose to express the similarity between two fuzzy objects by distance probability functions which assign a probability value to each possible distance value. Contrary to the traditional approach, we do not extract aggregated values from the fuzzy distance functions but enhance OPTICS so that it can exploit the full information provided by these functions. The resulting algorithm FOPTICS helps the user to get an overview over a large set of fuzzy objects.
[Algorithm design and analysis, Data analysis, data mining, fuzzy set theory, probability, single-valued distance function, distance probability function, Data mining, Fuzzy sets, uncertain data clustering, Databases, pattern clustering, Clustering algorithms, fuzzy object similarity, Probability density function, Density functional theory, Optical sensors, hierarchical density-based clustering, Distribution functions]
Semi-supervised clustering with metric learning using relative comparisons
Fifth IEEE International Conference on Data Mining
None
2005
Semi-supervised clustering algorithms partition a given data set using limited supervision from the user. In this paper, we propose a clustering algorithm that uses supervision in terms of relative comparisons, viz., x is closer to y than to z. The success of a clustering algorithm also depends on the kind of dissimilarity measure. The proposed clustering algorithm learns the underlying dissimilarity measure while finding compact clusters in the given data set. Through our experimental studies on high-dimensional textual data sets, we demonstrate that the proposed algorithm achieves higher accuracy than the algorithms using pair-wise constraints for supervision.
[relative comparison, metric learning, clustering algorithm, pattern clustering, dissimilarity measure, Feedback, Clustering algorithms, semisupervised clustering, Partitioning algorithms, Data mining, learning (artificial intelligence), Nearest neighbor searches]
On learning asymmetric dissimilarity measures
Fifth IEEE International Conference on Data Mining
None
2005
Many practical applications require that distance measures to be asymmetric and context-sensitive. We introduce context-sensitive learnable asymmetric dissimilarity (CLAD) measures, which are defined to be a weighted sum of a fixed number of dissimilarity measures where the associated weights depend on the point from which the dissimilarity is measured. The parameters used in defining the measure capture the global relationships among the features. We provide an algorithm to learn the dissimilarity measure automatically from a set of user specified comparisons in the form "x is closer to y than to z" and study its performance. The experimental results show that the proposed algorithm outperforms other approaches due to the context sensitive nature of the CLAD measures.
[Constraint optimization, Computer aided instruction, Clustering algorithms, Zoology, Information retrieval, Extraterrestrial measurements, Proposals, Data mining, learning (artificial intelligence), asymmetric dissimilarity measure learning, context-sensitive learnable asymmetric dissimilarity measures, Unsupervised learning]
Partial elastic matching of time series
Fifth IEEE International Conference on Data Mining
None
2005
We consider the problem of elastic matching of time series. We propose an algorithm that determines a subsequence of a target time series that best matches a query series. In the proposed algorithm, we map the problem of the best matching subsequence to the problem of a cheapest path in a DAG (directed acyclic graph). The proposed approach allows us to also compute the optimal scale and translation of time series values, which is a nontrivial problem in the case of subsequence matching.
[Pervasive computing, pattern matching, partial elastic matching, time series matching, directed acyclic graph, time series, Time measurement, Data mining, Computer science, Information science, subsequence matching, directed graphs, Clustering algorithms, Euclidean distance, Dynamic programming, query series matching]
CLUGO: a clustering algorithm for automated functional annotations based on gene ontology
Fifth IEEE International Conference on Data Mining
None
2005
We address the issue of providing highly informative and comprehensive annotations using information revealed by the structured vocabularies of gene ontology (GO). For a target, a set of candidate terms for inferring target properties is collected and form a unique distribution on the GO directed acyclic graph (DAG). We propose a novel ontology-based clustering algorithm $CLUGO, which considers GO hierarchical characteristics and the clustering of term distributions. By identifying significant groups in the distributions, CLUGO assigns comprehensive and correct annotations for a target. According to the results of experiments with automated sequence functional annotations, CLUGO represents a considerable improvement over our previous work - GOMIT in terms of recall while maintaining a similar level of precision. We conclude that given a GO candidate term distribution, CLUGO is an efficient ontology-based clustering algorithm for selecting comprehensive and correct annotations.
[Vocabulary, ontology-based clustering algorithm, CLUGO, Ontologies, Performance gain, directed acyclic graph, Information retrieval, term distribution, Data mining, Information science, gene ontology, Accuracy, Databases, genetics, vocabulary, biology computing, pattern clustering, directed graphs, automated sequence functional annotation, Clustering algorithms, Filtering algorithms, structured vocabulary, ontologies (artificial intelligence)]
An optimal linear time algorithm for quasi-monotonic segmentation
Fifth IEEE International Conference on Data Mining
None
2005
Monotonicity is a simple yet significant qualitative characteristic. We consider the problem of segmenting an array in up to K segments. We want segments to be as monotonic as possible and to alternate signs. We propose a quality metric for this problem, present an optimal linear time algorithm based on novel formalism, and compare experimentally its performance to a linear time top-down regression algorithm. We show that our algorithm is faster and more accurate. Applications include pattern recognition and qualitative modeling.
[linear time algorithm, Councils, Aggregates, quasimonotonic segmentation, array segmentation, Pattern recognition, Labeling, Data mining, qualitative modeling, computational complexity, pattern recognition, linear time top-down regression]
Average number of frequent (closed) patterns in Bernoulli and Markovian databases
Fifth IEEE International Conference on Data Mining
None
2005
In data mining, enumerate the frequent or the closed patterns is often the first difficult task leading to the association rules discovery. The number of these patterns represents a great interest. The lower bound is known to be constant whereas the upper bound is exponential, but both situations correspond to pathological cases. For the first time, we give an average analysis of the number of frequent or closed patterns. Average analysis is often closer to real situations and gives more information about the role of the parameters. In this paper, two probabilistic models are studied: a Bernoulli and a Markovian. In both models and for large databases, we prove that the number of frequent patterns, for a fixed frequency threshold, is exponential in the number of items and polynomial in the number of transactions. On the other hand, for a proportional frequency threshold, the number of frequent patterns is polynomial in the number of items and does not involve the number of transactions. Finally, we prove in the Bernoulli model that the number of closed patterns, for a proportional frequency threshold, is polynomial in the number of items.
[frequent patterns, data mining, Transaction databases, fixed frequency threshold, Data mining, Association rules, association rules discovery, Markovian model, Information analysis, frequent enumeration, probabilistic models, Pathology, Upper bound, closed patterns, average analysis, proportional frequency threshold, Bernoulli model, Frequency, Sampling methods, Polynomials, Pattern analysis]
Predicting software escalations with maximum ROI
Fifth IEEE International Conference on Data Mining
None
2005
Enterprise software vendors often have to release software products before all reported defects are corrected, and a small number of these reported defects will be escalated by customers whose businesses are seriously impacted. Escalated defects must be quickly resolved at a high cost by the software vendors. The total costs can be even greater, including loss of reputation, satisfaction, loyalty, and repeat revenue. In this paper, we develop an Escalation Prediction (EP) system to mine historic defect report data and predict the escalation risk of current defect reports for maximum ROI (Return On Investment). More specifically, we first describe a simple and general framework to convert the maximum ROI problem to cost-sensitive learning. We then apply and compare several best-known cost-sensitive learning approaches for EP. The EP system has produced promising results, and has been deployed in the product group of an enterprise software vendor. Conclusions drawn from this study also provide guidelines for mining imbalanced datasets and cost-sensitive learning.
[enterprise software, escalation prediction, Costs, cost-sensitive learning, Humans, data mining, Predictive models, Programming, DP management, Data mining, Sun, software products defects, Guidelines, dataset mining, Computer science, defect report data, Investments, Computer architecture]
Mining approximate frequent itemsets from noisy data
Fifth IEEE International Conference on Data Mining
None
2005
Frequent itemset mining is a popular and important first step in analyzing data sets across a broad range of applications. The traditional, "exact" approach for finding frequent itemsets requires that every item in the itemset occurs in each supporting transaction. However, real data is typically subject to noise, and in the presence of such noise, traditional itemset mining may fail to detect relevant itemsets, particularly those large itemsets that are more vulnerable to noise. In this paper we propose approximate frequent itemsets (AFI), as a noise-tolerant itemset model. In addition to the usual requirement for sufficiently many supporting transactions, the AFI model places constraints on the fraction of errors permitted in each item column and the fraction of errors permitted in a supporting transaction. Taken together, these constraints winnow out the approximate itemsets that exhibit systematic errors. In the context of a simple noise model, we demonstrate that AFI is better at recovering underlying data patterns, while identifying fewer spurious patterns than either the exact frequent itemset approach or the existing error tolerant itemset approach of Yang et al.
[Data analysis, Operations research, Statistical analysis, data analysis, data mining, Relational databases, data sets analysis, Data mining, Application software, Association rules, frequent itemset mining, approximate frequent itemset, exact frequent itemset, error tolerant itemset, Computer science, noise-tolerant itemset model, Itemsets, noisy data, data patterns, Context modeling]
Text representation: from vector to tensor
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we propose a text representation model, Tensor Space Model (TSM), which models the text by multilinear algebraic high-order tensor instead of the traditional vector. Supported by techniques of multilinear algebra, TSM offers a potent mathematical framework for analyzing the multifactor structures. TSM is further supported by certain introduced particular operations and presented tools, such as the High-Order Singular Value Decomposition (HOSVD) for dimension reduction and other applications. Experimental results on the 20 Newsgroups dataset show that TSM is constantly better than VSM for text classification.
[text analysis, high-order singular value decomposition, vector space model, text representation, multilinear algebraic high-order tensor, Information retrieval, Large scale integration, tensors, Data mining, Matrix decomposition, dimension reduction, Computer science, tensor space model, Tensile stress, vectors, Asia, multifactor structures, singular value decomposition, Principal component analysis, Singular value decomposition, Indexing]
Parallel algorithms for distance-based and density-based outliers
Fifth IEEE International Conference on Data Mining
None
2005
An outlier is an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism. Outlier detection has many applications, such as data cleaning, fraud detection and network intrusion. The existence of outliers can indicate individuals or groups that exhibit a behavior that is very different from most of the individuals of the dataset. In this paper we design two parallel algorithms, the first one is for finding out distance-based outliers based on nested loops along with randomization and the use of a pruning rule. The second parallel algorithm is for detecting density-based local outliers. In both cases data parallelism is used. We show that both algorithms reach near linear speedup. Our algorithms are tested on four real-world datasets coming from the Machine Learning Database Repository at the UCI.
[Algorithm design and analysis, parallel algorithms, Machine learning algorithms, data analysis, network intrusion, Mathematics, Cleaning, Data mining, Parallel algorithms, outlier detection, Nearest neighbor searches, nested loops, Databases, distance-based outliers, data cleaning, data parallelism, Intrusion detection, fraud detection, density-based local outliers, pruning rule, Testing]
Bit reduction support vector machine
Fifth IEEE International Conference on Data Mining
None
2005
Support vector machines are very accurate classifiers and have been widely used in many applications. However, the training and to a lesser extent prediction time of support vector machines on very large data sets can be very long. This paper presents a fast compression method to scale up support vector machines to large data sets. A simple bit reduction method is applied to reduce the cardinality of the data by weighting representative examples. We then develop support vector machines which may be trained on weighted data. Experiments indicate that the bit reduction support vector machine produces a significant reduction in the time required for both training and prediction with minimum loss in accuracy. It is also shown to be more accurate than random sampling, when the data is not over-compressed.
[data cardinality, data compression, support vector machines, Marine vegetation, Educational institutions, Application software, Quadratic programming, Support vector machines, Computer science, data reduction, prediction time, fast compression method, Support vector machine classification, bit reduction support vector machine, large data sets, Sampling methods, Libraries, Marine vehicles]
Spatial clustering of chimpanzee locations for neighborhood identification
Fifth IEEE International Conference on Data Mining
None
2005
Since 1960, the chimpanzees (Pan troglodytes) of Gombe National Park, Tanzania, have been studied by behavioral ecologists, including Jane Goodall. Data have been collected for more than 40 years and are being analyzed by researchers in order to increase our understanding of the social structure of chimpanzees. In this paper, we consider the following question of interest to behavioral ecologists - "Does clustering exist among female chimpanzees in terms of their spatial locations ?" The analysis of this question will help behavioral ecologists to learn about the space use and the social interactions between female chimpanzees. The data collected for this analysis are marked spatial point patterns over the park. Current spatial clustering methods lack the ability to handle such marked point patterns directly. This paper presents a novel application of spatial point pattern analysis and data mining techniques to the ecological problem of clustering female chimpanzees. We found that Ripley's K-function provides a powerful statistical tool for evaluating clustering behavior among spatial point patterns. We then proposed two clustering approaches for marked point patterns using the K-function. Experimental results using the proposed clustering methods provide significant insight into the dynamics of female chimpanzee space use and into the overall social stucture of the species. In addition, the proposed methods can be extended to also include temporal information.
[clustering behavior evaluation, neighborhood identification, Clustering methods, data mining, spatial clustering, Data mining, ecology, chimpanzees social structure, Animal structures, Clustering algorithms, spatial point pattern analysis, female chimpanzees clustering, Pattern analysis, spatial locations, temporal information, social interactions, chimpanzee locations, Extraterrestrial measurements, Environmental factors, Pan troglodytes, behavioral ecologists, marked point patterns, Computer science, pattern clustering, Ripley's K-function, zoology]
A graph-ranking algorithm for geo-referencing documents
Fifth IEEE International Conference on Data Mining
None
2005
This paper presents an application of PageRank for assigning documents with a corresponding geographical scope. We describe the technique in detail, together with its theoretical formulation. Experimental results are promising, comparing favorably with previous proposals.
[document handling, Vocabulary, Merging, Ontologies, PageRank, geographic information systems, Data mining, Statistics, Text recognition, Training data, Frequency, georeferencing documents, geographical scope, graph-ranking algorithm, Testing]
An expected utility approach to active feature-value acquisition
Fifth IEEE International Conference on Data Mining
None
2005
In many classification tasks, training data have missing feature values that can be acquired at a cost. For building accurate predictive models, acquiring all missing values is often prohibitively expensive or unnecessary, while acquiring a random subset of feature values may not be most effective. The goal of active feature-value acquisition is to incrementally select feature values that are most cost-effective for improving the model's accuracy. We present an approach that acquires feature values for inducing a classification model based on an estimation of the expected improvement in model accuracy per unit cost. Experimental results demonstrate that our approach consistently reduces the cost of producing a model of a desired accuracy compared to random feature acquisitions.
[Measurement, data training, pattern classification, Costs, random feature acquisitions, Demography, data analysis, Medical treatment, Predictive models, classification tasks, active feature-value acquisition, classification model, Utility theory, predictive models, Training data, Decision trees, Testing, Classification tree analysis]
Automatically mining result records from search engine response pages
Fifth IEEE International Conference on Data Mining
None
2005
Usually, Web applications such as deep Web crawlers, metasearch engines, and other Web mining systems need to extract information displayed in the form of result records on response pages returned by search engines in response to submitted queries. Extracting such records is challenging as search engines are heterogeneous in displaying their records. In addition, response pages returned by many search engines include other noisy content such as advertisements, suggestion links, etc., which make the extraction task even more complicated. In this paper, we propose a highly effective and efficient algorithm for automatically mining result records from search engine response pages.
[search engines, query submission, Crawlers, Humans, data mining, metasearch engines, Web applications, HTML, Data mining, Computer displays, information extraction, XML, Web pages, Web mining, Search engines, automatically mining result records, deep Web crawlers, search engine response pages, Web mining systems, Metasearch, Internet]
Efficiently mining frequent closed partial orders
Fifth IEEE International Conference on Data Mining
None
2005
Mining ordering information from sequence data is an important data mining task. Sequential pattern mining (Agrawal and Srikant, 1995) can be regarded as mining frequent segments of total orders from sequence data. However, sequential patterns are often insufficient to concisely capture the general ordering information.
[Sequences, data mining, data sequence, total orders frequent segments, frequent closed partial orders mining, mining ordering information, Data mining, Gene expression, Biological information theory, Information analysis, Diseases, Loans and mortgages, Databases, DNA, sequential pattern mining, Retirement]
CLUMP: a scalable and robust framework for structure discovery
Fifth IEEE International Conference on Data Mining
None
2005
We introduce a robust and efficient framework called CLUMP (CLustering Using Multiple Prototypes) for unsupervised discovery of structure in data. CLUMP relies on finding multiple prototypes that summarize the data. Clustering the prototypes enables our algorithm to scale up to extremely large and high-dimensional domains such as text data. Other desirable properties include robustness to noise and parameter choices. In this paper, we describe the approach in detail, characterize its performance on a variety of datasets, and compare it to some existing model selection approaches.
[Tree data structures, Knee, unsupervised discovery, text analysis, Scalability, Merging, CLustering Using Multiple Prototypes, data mining, model selection, Noise shaping, Data mining, Statistics, pattern clustering, Prototypes, Clustering algorithms, scalable robust framework, structure discovery, Noise robustness, CLUMP]
On the tractability of rule discovery from distributed data
Fifth IEEE International Conference on Data Mining
None
2005
This paper analyses the tractability of rule selection for supervised learning in distributed scenarios. The selection of rules is usually guided by a utility measure such as predictive accuracy or weighted relative accuracy. A common strategy to tackle rule selection from distributed data is to evaluate rules locally on each dataset. While this works well for homogeneously distributed data, this work proves limitations of this strategy if distributions are allowed to deviate. The identification of those subsets for which local and global distributions deviate, poses a learning task of its own, which is shown to be at least as complex as discovering the globally best rules from local data.
[Costs, data mining, supervised learning, distributed processing, rule discovery, Computer science, rule selection, Privacy, Accuracy, Databases, Supervised learning, Machine learning, predictive accuracy, weighted relative accuracy, Logic, learning (artificial intelligence), utility measure, Artificial intelligence, distributed data, Testing]
Face recognition using landmark-based bidimensional regression
Fifth IEEE International Conference on Data Mining
None
2005
This paper studies how biologically meaningful landmarks extracted from face images can be exploited for face recognition using the bidimensional regression. Incorporating the correlation statistics of landmarks, this paper also proposes a new approach called eigenvalue weighted bidimensional regression. Complex principal component analysis is used for computing eigenvalues and removing correlation among landmarks. We evaluate our approach using two standard face databases: the Purdue AR and the NIST FERET. Experimental results show that the bidimensional regression is an efficient method to exploit geometry information of face images.
[Shape, Face recognition, Biological system modeling, regression analysis, Biology, Biological information theory, Statistics, eigenvalues and eigenfunctions, biologically meaningful landmarks, Computer science, Information geometry, correlation statistics, face recognition, landmark-based bidimensional regression, face images, eigenvalue weighted bidimensional regression, Eigenvalues and eigenfunctions, principal component analysis, Principal component analysis]
Instability of classifiers on categorical data
Fifth IEEE International Conference on Data Mining
None
2005
In this paper we study the local behaviour of arbitrary classifiers using the instability of that classifier in a data point. Moreover, we introduce two algorithms. The first to find highly unstable points, the second to find islands of stability.
[Performance evaluation, pattern classification, Stability, data mining, classifier instability, Data mining, Computer science, Support vector machines, Information geometry, Support vector machine classification, categorical data, Books, Classification tree analysis, Testing]
Pruning social networks using structural properties and descriptive attributes
Fifth IEEE International Conference on Data Mining
None
2005
Scale is often an issue with understanding and making sense of large social networks. Here we investigate methods for pruning social networks by determining the most relevant relationships. We measure importance in terms of predictive accuracy on a set of target attributes of the social network. Our goal is to create a pruned network that models only the most informative affiliations and relationships. We present methods for pruning networks based on both structural properties and descriptive attributes demonstrate it on a network of NASDAQ and NYSE businesses and on a bibliographic network.
[NASDAQ, structural property, bibliographic network, graph theory, NYSE, Graph theory, social network pruning, descriptive attribute, information networks]
Optimizing constraint-based mining by automatically relaxing constraints
Fifth IEEE International Conference on Data Mining
None
2005
In constraint-based mining, the monotone and anti-monotone properties are exploited to reduce the search space. Even if a constraint has not such suitable properties, existing algorithms can be re-used thanks to an approximation, called relaxation. In this paper, we automatically compute monotone relaxations of primitive-based constraints. First, we show that the latter are a superclass of combinations of both kinds of monotone constraints. Second, we add two operators to detect the properties of monotonicity of such constraints. Finally, we define relaxing operators to obtain monotone relaxations of them.
[approximation theory, Filtering, monotone property, data mining, automatic constraint relaxation, monotone relaxation, Data mining, constraint-based mining, Constraint optimization, antimonotone property, relaxation theory, approximation algorithm, Approximation algorithms, Constraint theory, Testing]
Bias analysis in text classification for highly skewed data
Fifth IEEE International Conference on Data Mining
None
2005
Feature selection is often applied to high-dimensional data as a preprocessing step in text classification. When dealing with highly skewed data, we observe that typical feature selection metrics like information gain or chi-squared are biased toward selecting features for the minor class, and the metric of bi-normal separation can select features for both minor and major classes. In this work, we investigate how these feature selection metrics impact on the performance of frequently used classifiers such as decision trees, naive bayes, and support vector machines via bias analysis for highly skewed data. Three types of biases are metric bias, class bias, and classifier bias. Extensive experiments are designed to understand how these biases can be employed in concert and efficiently to achieve good classification performance. We report our findings and present recommended approaches to text classification based on bias analysis and the empirical study.
[information gain, text analysis, chi squared method, binormal separation, Data engineering, Classification algorithms, bias analysis, text classification, class bias, metric bias, Performance analysis, Decision trees, Classification tree analysis, support vector machines, highly skewed data, Support vector machines, Computer science, classifier bias, Text categorization, Niobium compounds, feature selection metrics, Support vector machine classification, decision trees, naive Bayes, Bayes methods]
Efficient mining of high branching factor attribute trees
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we present a new tree mining algorithm, DryadeParent, based on the hooking principle first introduced in Dryade (Termier et al, 2004). In the experiments, we demonstrate that the branching factor and depth of the frequent patterns to find are key factor of complexity for tree mining algorithms. We show that DryadeParent outperforms the current fastest algorithm, CMTreeMiner, by orders of magnitude on datasets where the frequent patterns have a high branching factor.
[frequent pattern depth, hooking principle, efficient tree mining, data mining, trees (mathematics), high branching factor attribute trees, DryadeParent, Data mining, Itemsets, Tree graphs, Labeling, algorithm complexity, CMTreeMiner, computational complexity]
Anomaly intrusion detection using multi-objective genetic fuzzy system and agent-based evolutionary computation framework
Fifth IEEE International Conference on Data Mining
None
2005
In this paper, we present a multi-objective genetic fuzzy system for anomaly intrusion detection. The proposed system extracts accurate and interpret able fuzzy rule-based knowledge from network data using an agent-based evolutionary computation framework. The experimental results on KDD-Cup99 intrusion detection benchmark data demonstrate that our system can achieve high detection rate for intrusion attacks and low false positive rate for normal network traffic.
[multi-agent systems, Distribution strategy, Evolutionary computation, agent-based evolutionary computation, genetic algorithms, Data mining, Biological cells, Computer science, Fuzzy sets, security of data, fuzzy rule-based knowledge, Intrusion detection, knowledge based systems, Gaussian processes, anomaly intrusion detection, Genetics, multiobjective genetic fuzzy system, fuzzy systems, Fuzzy systems]
Mining quantitative frequent itemsets using adaptive density-based subspace clustering
Fifth IEEE International Conference on Data Mining
None
2005
A novel approach to subspace clustering is proposed to exhaustively and efficiently mine quantitative frequent item-sets (QFIs) from massive transaction data. For the computational tractability, our approach introduces adaptive density-based and Apriori-like algorithm. Its outstanding performance is shown through numerical experiments.
[massive transaction data, Shape, Merging, data mining, Data mining, Association rules, computational tractability, Computational complexity, adaptive density-based subspace clustering, Itemsets, pattern clustering, Clustering algorithms, Cities and towns, Hypercubes, quantitative frequent item set mining, Mining industry]
Hot item mining and summarization from multiple auction Web sites
Fifth IEEE International Conference on Data Mining
None
2005
Online auction Web sites are fast changing, highly dynamic, and complex as they involve tremendous sellers and potential buyers, as well as a huge amount of items listed for bidding. We develop a two-phase framework which aims at mining and summarizing hot items from multiple auction Web sites to assist decision making. The objective of the first phase is to automatically extract the product features and product feature values of the items from the descriptions provided by the sellers. We design a HMM-based learning method to train an extended HMM model which can adapt to the unseen Web page from which the information is extracted. The goal of the second phase is to discover and summarize the hot items based on the extracted information. We formulate the hot item mining task as a semi-supervised learning problem and employ the graph mincuts algorithm to accomplish this task. The summary of the hot items is then generated by considering the frequency and the position of the product features being mentioned in the descriptions. We have conducted extensive experiments from several real-world auction Web sites to demonstrate the effectiveness of our framework.
[multiple auction Web sites, hidden Markov model, semisupervised learning, graph theory, data mining, HMM-based learning, Data mining, Learning systems, hidden Markov models, learning (artificial intelligence), electronic commerce, online auction Web sites, Decision making, graph mincuts algorithm, product feature extraction, hot item summarization, hot item mining, Hidden Markov models, Potential well, Web pages, Semisupervised learning, Feature extraction, Systems engineering and theory, Web sites, Research and development management]
Merging interface schemas on the deep Web via clustering aggregation
Fifth IEEE International Conference on Data Mining
None
2005
We consider the problem of integrating a large number of interface schemas over the deep Web, The scale of the problem and the diversity of the sources present serious challenges to the conventional manual or rule-based approaches to schema integration. To address these challenges, we propose a novel formulation of schema integration as an optimization problem, with the objective of maximally satisfying the constraints given by individual schemas. Since the optimization problem can be shown to be NP-complete, we develop a novel approximation algorithm LMax, which builds the unified schema via recursive applications of clustering aggregation. We further extend LMax to handle the irregularities frequently occurring among the interface schemas. Extensive evaluation on real-world data sets shows the effectiveness of our approach.
[Vocabulary, approximation theory, optimization problem, LMax algorithm, Merging, NP-complete problem, Constraint optimization, optimisation, Databases, schema integration, Clustering algorithms, approximation algorithm, Approximation algorithms, interface schema, clustering aggregation, Internet, deep Web, computational complexity]
On the stationarity of multivariate time series for correlation-based data analysis
Fifth IEEE International Conference on Data Mining
None
2005
Multivariate time series (MTS) data sets are common in-various multimedia, medical and financial application domains. These applications perform several data-analysis operations on large number of MTS data sets such as similarity searches, feature-subset-selection, clustering and classifications. Correlation-based techniques, such as principal component analysis (PCA), have proven to improve the efficiency of many of the above-mentioned data-analysis operations on MTS, which implies that the correlation coefficients concisely represent the original MTS data. However, if the statistical properties (e.g., variance) of MTS data change over time dimension, i.e., MTS data is non-stationary, the correlation coefficients are not stable. In this paper, we propose to utilize the stationarity of the MTS data sets, in order to represent the original MTS data more stably, as well as concisely with the correlation coefficients. That is, before performing any correlation-based data analysis, we first executes the stationarity test to decide whether the MTS data is stationary or not, i.e., whether the correlation is stable or not. Subsequently, for a non-stationary MTS data set, we difference it to render the data set stationary. Even though our approach is general, to focus the discussion we describe our approach within the context of our previously proposed technique for MTS similarity search. In order to show the validity of our approach, we performed several experiments on four real-world data sets. The results show that the performance of our similarity search technique have significantly improved in terms of precision/recall.
[Performance evaluation, Data analysis, data analysis, similarity search, Time series analysis, multivariate time series, time series, Time measurement, stationarity test, Application software, Data mining, correlation-based data analysis, Computer science, Euclidean distance, search problems, Principal component analysis, Testing]
Speculative Markov blanket discovery for optimal feature selection
Fifth IEEE International Conference on Data Mining
None
2005
In this paper we address the problem of learning the Markov blanket of a quantity from data in an efficient manner Markov blanket discovery can be used in the feature selection problem to find an optimal set of features for classification tasks, and is a frequently-used preprocessing phase in data mining, especially for high-dimensional domains. Our contribution is a novel algorithm for the induction of Markov blankets from data, called Fast-IAMB, that employs a heuristic to quickly recover the Markov blanket. Empirical results show that Fast-IAMB performs in many cases faster and more reliably than existing algorithms without adversely affecting the accuracy of the recovered Markov blankets.
[pattern classification, Calcium, data mining, speculative Markov blanket discovery, classification tasks, Reliability engineering, Data mining, Equations, Computer science, Graphical models, Bayesian methods, learning problem, Markov processes, optimal feature selection, learning (artificial intelligence), Fast-IAMB, Cancer, Lung neoplasms]
A join-less approach for co-location pattern mining: a summary of results
Fifth IEEE International Conference on Data Mining
None
2005
Spatial co-location patterns represent the subsets of features whose instances are frequently located together in geographic space. Co-location pattern discovery presents challenges since the instances of spatial features are embedded in a continuous space and share a variety of spatial relationships. A large fraction of the computation time is devoted to identifying the instances of co-location patterns. We propose a novel join-less approach for co-location pattern mining, which materializes spatial neighbor relationships with no loss of co-location instances and reduces the computational cost of identifying the instances. The join-less co-location mining algorithm is efficient since it uses an instance-lookup scheme instead of an expensive spatial or instance join operation for identifying co-location instances. The experimental evaluations show the join-less algorithm performs more efficiently than a current join-based algorithm and is scalable in dense spatial datasets.
[Performance evaluation, geographic space, instance-lookup scheme, Transportation, data mining, Geoscience, visual databases, Birds, Biology, Data mining, Association rules, Computer science, colocation pattern mining, spatial colocation patterns, Computational efficiency, colocation pattern discovery, Public healthcare, join-less approach, spatial relationship]
Learning through changes: an empirical study of dynamic behaviors of probability estimation trees
Fifth IEEE International Conference on Data Mining
None
2005
In practice, learning from data is often hampered by the limited training examples. In this paper, as the size of training data varies, we empirically investigate several probability estimation tree algorithms over eighteen binary classification problems. Nine metrics are used to evaluate their performances. Our aggregated results show that ensemble trees consistently outperform single trees. Confusion factor trees(CFT) register poor calibration even as training size increases, which shows that CFTs are potentially biased if data sets have small noise. We also provide analysis on the observed performance of the tree algorithms.
[Performance evaluation, binary classification problem, Error analysis, trees (mathematics), probability, Calibration, probability estimation trees, learning through changes, Computer science, confusion factor trees, Training data, Performance analysis, Decision trees, learning (artificial intelligence), Positron emission tomography, Testing, Classification tree analysis]
Visualizing global manifold based on distributed local data abstractions
Fifth IEEE International Conference on Data Mining
None
2005
Mining distributed data for global knowledge is getting more attention recently. The problem is especially challenging when data sharing is prohibited due to local constraints like limited bandwidth and data privacy. In this paper, we investigate how to derive the embedded manifold (as a 2-D map) for a horizontally partitioned data set, where data cannot be shared among the partitions directly. We propose a model-based approach which computes hierarchical local data abstractions, aggregates the abstractions, and finally learns a global generative model - generative topographic mapping (GTM) based on the aggregated data abstraction. We applied the proposed method to two benchmarking data sets and demonstrated that the accuracy of the derived manifold can effectively be controlled by adjusting the data granularity level of the adopted local abstraction.
[Data privacy, abstraction aggregation, Memory, data mining, generative topographic mapping, distributed processing, global generative model, Parametric statistics, Covariance matrix, Data mining, Computer science, hierarchical local data abstraction, Aggregates, distributed data mining, Data visualization, Bandwidth, data granularity, Automatic control, data privacy, global manifold visualization, distributed local data abstraction]
Bagging with adaptive costs
Fifth IEEE International Conference on Data Mining
None
2005
Ensemble methods have proved to be highly effective in improving the performance of base learners under most circumstances. In this paper, we propose a new algorithm that combines the merits of some existing techniques, namely bagging, arcing and stacking. The basic structure of the algorithm resembles bagging, using a linear support vector machine (SVM). However, the misclassification cost of each training point is repeatedly adjusted according to its observed out-of-bag vote margin. In this way, the method gains the advantage of arcing - building the classifier the ensemble needs - without fixating on potentially noisy points. Computational experiments show that this algorithm performs consistently better than bagging and arcing.
[Costs, support vector machines, Stacking, Buildings, Boosting, linear support vector machine, adaptive cost, Support vector machines, Voting, Support vector machine classification, Training data, ensemble method, Cities and towns, stacking technique, learning (artificial intelligence), bagging technique, arcing technique, Bagging]
Example-based robust outlier detection in high dimensional datasets
Fifth IEEE International Conference on Data Mining
None
2005
Detecting outliers is an important problem. Most of its applications typically possess high dimensional datasets. In high dimensional space, the data becomes sparse which implies that every object can be regarded as an outlier from the point of view of similarity. Furthermore, a fundamental issue is that the notion of which objects are outliers typically varies between users, problem domains or, even, datasets. In this paper, we present a novel robust solution which detects high dimensional outliers based on user examples and tolerates incorrect inputs. It studies the behavior of projections of such a few examples, to discover further objects that are outstanding in the projection where many examples are outlying. Our experiments on both real and synthetic datasets demonstrate the ability of the proposed method to detect outliers corresponding to the user examples.
[data analysis, Design methodology, Application software, Data mining, Unsupervised learning, high dimensional outlier, Computer science, example-based robust outlier detection, Clustering algorithms, Object detection, Systems engineering and theory, Robustness, Monitoring, high dimensional data sets]
CTC - correlating tree patterns for classification
Fifth IEEE International Conference on Data Mining
None
2005
We present CTC, a new approach to structural classification. It uses the predictive power of tree patterns correlating with the class values, combining state-of-the-art tree mining with sophisticated pruning techniques to find the k most discriminative pattern in a dataset. In contrast to existing methods, CTC uses no heuristics and the only parameters to be chosen by the user are the maximum size of the rule set and a single, statistically well founded cut-off value. The experiments show that CTC classifiers achieve good accuracies while the induced models are smaller than those of existing approaches, facilitating comprehensibility.
[Drugs, tree pattern correlation, pattern classification, structural classification, data mining, trees (mathematics), Electronic mail, Frequency measurement, Data mining, Association rules, pruning technique, Support vector machines, Tree graphs, k most discriminative pattern, XML, Machine learning, tree mining, Classification tree analysis]
Welcome from Conference Chairs
Sixth International Conference on Data Mining
None
2006
Presents the welcome message from the conference proceedings.
[]
Preface
Sixth International Conference on Data Mining
None
2006
Presents the welcome message from the conference proceedings.
[]
Conference organization
Sixth International Conference on Data Mining
None
2006
Provides a listing of current committee members and society officers.
[]
Program Committee
Sixth International Conference on Data Mining
None
2006
Provides a listing of current committee members.
[]
Program Committee
Sixth International Conference on Data Mining
None
2006
Provides a listing of current committee members.
[]
Invited speakers
Sixth International Conference on Data Mining
None
2006
Provides an abstract for each of the invited presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
Tutorials
Sixth International Conference on Data Mining
None
2006
Provides an abstract for each of the presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
Neuroscience: New Insights for AI?
Sixth International Conference on Data Mining
None
2006
Understanding the processing of information in our cortex is a significant part of understanding how the brain works and of understanding intelligence itself, arguably one of the greatest problems in science today. In particular, our visual abilities are computationally amazing and we are still far from imitating them with computers. Thus, visual cortex may well be a good proxy for the rest of the cortex and indeed for intelligence itself. But despite enormous progress in the physiology and anatomy of the visual cortex, our understanding of the underlying computations remains fragmentary. This position paper is based on the very recent, surprising realization that we may be on the verge of developing an initial quantitative theory of visual cortex, faithful to known physiology and able to mimic human performance in difficult recognition tasks, outperforming current computer vision systems. The proof of principle was provided by a preliminary model that, spanning several levels from biophysics to circuitry to the highest system level, describes information processing in the feedforward pathway of the ventral stream of primate visual cortex. The thesis of this paper is that - finally - neurally plausible computational models are beginning to provide powerful new insights into the key problem of how the brain works, and how to implement learning and intelligence in machines.
[Computer vision, visual cortex, Humans, neuroscience, learning, Biophysics, brain works, Anatomy, brain, Power system modeling, artificial intelligence, vision, Neuroscience, computer vision systems, cortex information processing, Brain modeling, Physiology, neurophysiology, Artificial intelligence, biophysics, Computational intelligence]
Exploratory Mining in Cube Space
Sixth International Conference on Data Mining
None
2006
Data Mining has evolved as a new discipline at the intersection of several existing areas, including Database Systems, Machine Learning, Optimization, and Statistics. An important question is whether the field has matured to the point where it has originated substantial new problems and techniques that distinguish it from its parent disciplines. In this paper, we discuss a class of new problems and techniques that show great promise for exploratory mining, while synthesizing and generalizing ideas from the parent disciplines. While the class of problems we discuss is broad, there is a common underlying objective-to look beyond a single data mining step (e.g., data summarization or model construction) and address the combined process of data selection and transformation, parameter and algorithm selection, and model construction. The fundamental difficulty lies in the large space of alternative choices at each step, and good solutions must provide a natural framework for managing this complexity. We regard this as a grand challenge for Data Mining, and see the ideas in this paper as promising initial steps towards a rigorous exploratory framework that supports the entire process. This is joint work with several people, in particular, Beechung Chen.
[cube space, data selection, data mining, Machine learning, exploratory mining, Database systems, data transformation, Data mining, machine learning, Statistics, database systems]
An Information Theoretic Approach to Detection of Minority Subsets in Database
Sixth International Conference on Data Mining
None
2006
Detection of rare and exceptional occurrences in large- scale databases have become an important practice in the field of knowledge discovery and information retrieval. Many databases include large amount of noise or irrelevant data, whose distribution often overlaps with the subsets of exceptional data containing useful knowledge. This paper addresses the problem of finding a small subset of "minority" data whose distribution overlaps with, but are exceptional to or inconsistent with that of the majority of the database. In such a case, conventional distance-based or density-based approaches in Outlier Detection are ineffective due to their dependence on the structure of the majority or the prerequisite of critical parameters. We formalize the task as an estimation of a model of the minority subset which provides a simple description of the subset and yet maintains divergence from that of the majority. This estimation is formalized as a minimization problem using an information theoretic framework of Rate Distortion theory. We further introduce conditions of the majority to derive an objective function which factorizes the property of the minority and dependence to the structure of the majority. The proposed method shows improvements from conventional approaches in artificial data and a promising result in document retrieval problem.
[Knowledge engineering, data mining, Rate distortion theory, information retrieval, Data engineering, Information retrieval, knowledge discovery, Data mining, Gene expression, outlier detection, Unsupervised learning, rate distortion theory, Information science, large-scale databases, Databases, very large databases, Rate-distortion, minority subsets, information theory]
Bayesian State Space Modeling Approach for Measuring the Effectiveness of Marketing Activities and Baseline Sales from POS Data
Sixth International Conference on Data Mining
None
2006
Analysis of point of sales (POS) data is an important research area of marketing science and knowledge discovery, which may enable marketing managers to attain the effective marketing activities. To measure the effectiveness of marketing activities and baseline sales, we develop the multivariate time series modeling method in the framework of a general state space model. A multivariate Poisson model and a multivariate correlated auto-regressive model are used for a system model and an observation model. The Bayesian approach via Markov Chain Monte Carlo (MCMC) algorithm is employed for estimating model parameters. To evaluate the goodness of the estimated models, the Bayesian predictive information criterion is utilized. The proposed model is evaluated with its application to actual POS data.
[Parameter estimation, marketing activity, Predictive models, knowledge discovery, multivariate time series modeling, Monte Carlo methods, model parameter estimation, multivariate Poisson model, multivariate correlated auto-regressive model, Marketing and sales, Bayesian state space modeling, Markov Chain Monte Carlo algorithm, Data analysis, point of sale systems, baseline sales, marketing science, POS data, general state space model, autoregressive processes, time series, Knowledge management, Time measurement, State-space methods, Marketing management, marketing data processing, point of sales data, Bayesian methods, Markov processes, Bayes methods, state-space methods, Bayesian predictive information criterion]
Learning to Use a Learned Model: A Two-Stage Approach to Classification
Sixth International Conference on Data Mining
None
2006
Association rule-based classifiers have recently emerged as competitive classification systems. However, there are still deficiencies that hinder their performance. One deficiency is the use of rules in the classification stage. Current systems assign classes to new objects based on the best rule applied or on some predefined scoring of multiple rules. In this paper we propose a new technique where the system automatically learns how to use the rules. We achieve this by developing a two-stage classification model. First, we use association rule mining to discover classification rules. Second, we employ another learning algorithm to learn how to use these rules in the prediction process. Our two-stage approach outperforms C4.5 and RIPPER on the UCI datasets in our study, and outperforms other rule- learning methods on more than half the datasets. The versatility of our method is also demonstrated by applying it to text classification, where it equals the performance of the best known systems for this task, SVMs.
[pattern classification, two-stage classification model, learning algorithm, data mining, text classification, Association rules, Data mining, association rule-based classifiers, Voting, Councils, rule mining, Text categorization, Neural networks, learned model, Computer networks, learning (artificial intelligence), Informatics]
Hierarchical Classification by Expected Utility Maximization
Sixth International Conference on Data Mining
None
2006
Hierarchical classification refers to an extension of the standard classification problem, in which labels must be chosen from a class hierarchy. In this paper, we look at hierarchical classification from an information retrieval point of view. More specifically, we consider a scenario in which a user searches a document in a topic hierarchy. This scenario gives rise to the problem of predicting an optimal entry point, that is, a topic node in which the user starts searching. The usefulness of a corresponding prediction strongly depends on the search behavior of the user, which becomes relevant if the document is not immediately found in the predicted node. Typically, users tend to browse the hierarchy in a top-down manner, i.e., they look at a few more specific subcategories but usually refuse exploring completely different branches of the search tree. From a classification point of view, this means that a prediction should be evaluated, not solely on the basis of its correctness, but rather by judging its usefulness against the background of the user behavior. The idea of this paper is to formalize hierarchical classification within a decision-theoretic framework which allows for modeling this usefulness in terms of a user-specific utility function. The prediction problem thus becomes a problem of expected utility maximization. Apart from its theoretical appeal, we provide first empirical results showing that the approach performs well in practice.
[document handling, expected utility maximization, decision-theoretic framework, information retrieval, hierarchical classification, Ontologies, Information retrieval, optimal entry point, Data mining, classification, Computer science, Waste materials, search tree, Utility theory, Web pages, user-specific utility function, user behavior, search problems]
COALA: A Novel Approach for the Extraction of an Alternate Clustering of High Quality and High Dissimilarity
Sixth International Conference on Data Mining
None
2006
Cluster analysis has long been a fundamental task in data mining and machine learning. However, traditional clustering methods concentrate on producing a single solution, even though multiple alternative clusterings may exist. It is thus difficult for the user to validate whether the given solution is in fact appropriate, particularly for large and complex datasets. In this paper we explore the critical requirements for systematically finding a new clustering, given that an already known clustering is available and we also propose a novel algorithm, COALA, to discover this new clustering. Our approach is driven by two important factors; dissimilarity and quality. These are especially important for finding a new clustering which is highly informative about the underlying structure of data, but is at the same time distinctively different from the provided clustering. We undertake an experimental analysis and show that our method is able to outperform existing techniques, for both synthetic and real datasets.
[Clustering methods, Merging, Laboratories, data mining, Data mining, machine learning, Computer science, Proteins, multiple alternative clustering, pattern clustering, Clustering algorithms, Machine learning, Search engines, cluster analysis, Software engineering, COALA]
Cluster Ranking with an Application to Mining Mailbox Networks
Sixth International Conference on Data Mining
None
2006
We initiate the study of a new clustering framework, called cluster ranking. Rather than simply partitioning a network into clusters, a cluster ranking algorithm also orders the clusters by their strength. To this end, we introduce a novel strength measure for clusters - the integrated cohesion - which is applicable to arbitrary weighted networks. We then present C-Rank: a new cluster ranking algorithm. Given a network with arbitrary pairwise similarity weights, C-Rank creates a list of overlapping clusters and ranks them by their integrated cohesion. We provide extensive theoretical and empirical analysis of C-Rank and show that it is likely to have high precision and recall. Our experiments focus on mining mailbox networks. A mailbox network is an egocentric social network, consisting of contacts with whom an individual exchanges email. Ties among contacts are represented by the frequency of their co-occurrence on message headers. C-Rank is well suited to mine such networks, since they are abundant with overlapping communities of highly variable strengths. We demonstrate the effectiveness of C-Rank on the Enron data set, consisting of 130 mailbox networks.
[Particle separators, Social network services, Clustering methods, Enron data set, data mining, electronic mail, egocentric social network, Information retrieval, arbitrary weighted network, Partitioning algorithms, Application software, cluster ranking algorithm, Computer science, pattern clustering, Clustering algorithms, Frequency, Needles, mailbox network mining]
Large Scale Detection of Irregularities in Accounting Data
Sixth International Conference on Data Mining
None
2006
In recent years, there have been several large accounting frauds where a company's financial results have been intentionally misrepresented by billions of dollars. In response, regulatory bodies have mandated that auditors perform analytics on detailed financial data with the intent of discovering such misstatements. For a large auditing firm, this may mean analyzing millions of records from thousands of clients. This paper proposes techniques for automatic analysis of company general ledgers on such a large scale, identifying irregularities - which may indicate fraud or just honest errors - for additional review by auditors. These techniques have been implemented in a prototype system, called Sherlock, which combines aspects of both outlier detection and classification. In developing Sherlock, we faced three major challenges: developing an efficient process for obtaining data from many heterogeneous sources, training classifiers with only positive and unlabeled examples, and presenting information to auditors in an easily interpretable manner. In this paper, we describe how we addressed these challenges over the past two years and report on experiments evaluating Sherlock.
[Heart, accounts data processing, accounting data irregularities, large scale detection, auditing, Risk analysis, Face detection, heterogeneous sources, Computer crime, Synthetic aperture sonar, Sherlock, financial data, Prototypes, Marketing and sales, training classifiers, Large-scale systems, Performance analysis, Testing]
Adaptive Blocking: Learning to Scale Up Record Linkage
Sixth International Conference on Data Mining
None
2006
Many data mining tasks require computing similarity between pairs of objects. Pairwise similarity computations are particularly important in record linkage systems, as well as in clustering and schema mapping algorithms. Because the number of object pairs grows quadratically with the size of the dataset, computing similarity between all pairs is impractical and becomes prohibitive for large datasets and complex similarity functions. Blocking methods alleviate this problem by efficiently selecting approximately similar object pairs for subsequent distance computations, leaving out the remaining pairs as dissimilar. Previously proposed blocking methods require manually constructing an index- based similarity function or selecting a set of predicates, followed by hand-tuning of parameters. In this paper, we introduce an adaptive framework for automatically learning blocking functions that are efficient and accurate. We describe two predicate-based formulations of learnable blocking functions and provide learning algorithms for training them. The effectiveness of the proposed techniques is demonstrated on real and simulated datasets, on which they prove to be more accurate than non-adaptive blocking methods.
[Machine learning algorithms, Uncertainty, Computational modeling, learning algorithms, data mining, pairwise similarity computations, nonadaptive blocking methods, adaptive blocking, Data mining, Sparse matrices, schema mapping algorithms, Sorting, Couplings, record linkage systems, subsequent distance computations, Clustering algorithms, Machine learning, learning (artificial intelligence), Indexing]
Adaptive Parallel Graph Mining for CMP Architectures
Sixth International Conference on Data Mining
None
2006
Mining graph data is an increasingly popular challenge, which has practical applications in many areas, including molecular substructure discovery, Web link analysis, fraud detection, and social network analysis. The problem statement is to enumerate all subgraphs occurring in at least sigma graphs of a database, where sigma is a user specified parameter. Chip multiprocessors (CMPs) provide true parallel processing, and are expected to become the de facto standard for commodity computing. In this work, building on the state-of-the-art, we propose an efficient approach to parallelize such algorithms for CMPs. We show that an algorithm which adapts its behavior based on the runtime state of the system can improve system utilization and lower execution times. Most notably, we incorporate dynamic state management to allow memory consumption to vary based on availability. We evaluate our techniques on current day shared memory systems (SMPs) and expect similar performance for CMPs. We demonstrate excellent speedup, 27-fold on 32 processors for several real world datasets. Additionally, we show our dynamic techniques afford this scalability while consuming up to 35% less memory than static techniques.
[Social network services, data mining, CMP architectures, microprocessor chips, Data mining, Yarn, parallel processing, Chemicals, Concurrent computing, chip multiprocessors, Runtime, adaptive parallel graph mining, dynamic state management, Memory management, Computer architecture, Parallel processing, shared memory systems, Personal communication networks]
Meta Clustering
Sixth International Conference on Data Mining
None
2006
Clustering is ill-defined. Unlike supervised learning where labels lead to crisp performance criteria such as accuracy and squared error, clustering quality depends on how the clusters will be used. Devising clustering criteria that capture what users need is difficult. Most clustering algorithms search for optimal clusterings based on a pre-specified clustering criterion. Our approach differs. We search for many alternate clusterings of the data, and then allow users to select the clustering(s) that best fit their needs. Meta clustering first finds a variety of clusterings and then clusters this diverse set of clusterings so that users must only examine a small number of qualitatively different clusterings. We present methods for automatically generating a diverse set of alternate clusterings, as well as methods for grouping clusterings into meta clusters. We evaluate meta clustering on four test problems and two case studies. Surprisingly, clusterings that would be of most interest to users often are not very compact clusterings.
[clustering criteria, Cardiac disease, meta clustering, Predictive models, Partitioning algorithms, History, Unsupervised learning, clustering quality, Databases, pattern clustering, Supervised learning, Clustering algorithms, Space exploration, optimal clustering, Testing]
Mixed-Drove Spatio-Temporal Co-occurence Pattern Mining: A Summary of Results
Sixth International Conference on Data Mining
None
2006
Mixed-drove spatio-temporal co-occurrence patterns (MDCOPs) represent subsets of object-types that are located together in space and time. Discovering MDCOPs is an important problem with many applications such as identifying tactics in battlefields, games, and predator-prey interactions. However, mining MDCOPs is computationally very expensive because the interest measures are computationally complex, datasets are larger due to the archival history, and the set of candidate patterns is exponential in the number of object-types. We propose a monotonic composite interest measure for discovering MDCOPs and a novel MDCOP mining algorithm. Analytical and experimental results show that the proposed algorithm is correct and complete. Results also show the proposed method is computationally more efficient than naive alternatives.
[Rabbits, Military computing, monotonic composite interest measure, data mining, computationally complex, Strategic planning, MDCOP mining algorithm, Application software, spatiotemporal phenomena, candidate patterns, Computer science, Current measurement, very large databases, Clustering algorithms, Pollution measurement, Spatiotemporal phenomena, archival history, mixed-drove spatiotemporal co-occurrence pattern mining, Contracts, computational complexity]
An Interactive Semantic Video Mining and Retrieval Platform--Application in Transportation Surveillance Video for Incident Detection
Sixth International Conference on Data Mining
None
2006
Understanding and retrieving videos based on their semantic contents is an important research topic in multimedia data mining and has found various real- world applications. Most existing video analysis techniques focus on the low level visual features of video data. However, there is a "semantic gap" between the machine-readable features and the high level human concepts i.e. human understanding of the video content. In this paper, an interactive platform for semantic video mining and retrieval is proposed using relevance feedback (RF), a popular technique in the area of content-based image retrieval (CBIR). By tracking semantic objects in a video and then modeling spatio-temporal events based on object trajectories and object interactions, the proposed interactive learning algorithm in the platform is able to mine the spatio-temporal data extracted from the video. An iterative learning process is involved in the proposed platform, which is guided by the user's response to the retrieved results. Although the proposed video retrieval platform is intended for general use and can be tailored to many applications, we focus on its application in traffic surveillance video database retrieval to demonstrate the design details. The effectiveness of the algorithm is demonstrated by our experiments on real-life traffic surveillance videos.
[Content based retrieval, video databases, semantic gap, Transportation, Humans, data mining, Multimedia data mining, Data mining, multimedia computing, video analysis, video database retrieval, Radio frequency, interactive semantic video retrieval, spatio-temporal data mining, transportation surveillance video, Feedback, video retrieval, interactive systems, Trajectory, learning (artificial intelligence), content-based image retrieval, video surveillance, data mining applications, multimedia data mining, Image retrieval, Information retrieval, iterative learning, content-based retrieval, incident detection, transportation, interactive semantic video mining, Surveillance, relevance feedback, semantic contents, machine-readable features, high level human concepts]
\\delta-Tolerance Closed Frequent Itemsets
Sixth International Conference on Data Mining
None
2006
In this paper, we study an inherent problem of mining frequent itemsets (FIs): the number of FIs mined is often too large. The large number of FIs not only affects the mining performance, but also severely thwarts the application of FI mining. In the literature, Closed FIs (CFIs) and Maximal FIs (MFIs) are proposed as concise representations of FIs. However, the number of CFIs is still too large in many cases, while MFIs lose information about the frequency of the FIs. To address this problem, we relax the restrictive definition of CFIs and propose the (delta-Tolerance CFIs delta- TCFIs). Mining delta-TCFIs recursively removes all subsets of a delta-TCFI that fall within a frequency distance bounded by delta. We propose two algorithms, CFI2TCFI and MineTCFI, to mine delta-TCFIs. CFI2TCFI achieves very high accuracy on the estimated frequency of the recovered FIs but is less efficient when the number of CFIs is large, since it is based on CFI mining. MineTCFI is significantly faster and consumes less memory than the algorithms of the state-of-the-art concise representations of FIs, while the accuracy of MineTCFI is only slightly lower than that of CFI2TCFI.
[maximal frequent itemset mining, Data analysis, Error analysis, data mining, delta-tolerance, Frequency estimation, Transaction databases, Data mining, Association rules, closed frequent itemset mining, Computer science, Itemsets, Pattern analysis, Indexing]
Active Learning to Maximize Area Under the ROC Curve
Sixth International Conference on Data Mining
None
2006
In active learning, a machine learning algorithm is given an unlabeled set of examples U, and is allowed to request labels for a relatively small subset of U to use for training. The goal is then to judiciously choose which examples in U to have labeled in order to optimize some performance criterion, e.g. classification accuracy. We study how active learning affects AUC. We examine two existing algorithms from the literature and present our own active learning algorithms designed to maximize the AUC of the hypothesis. One of our algorithms was consistently the top performer, and Closest Sampling from the literature often came in second behind it. When good posterior probability estimates were available, our heuristics were by far the best.
[Algorithm design and analysis, active learning algorithms, Machine learning algorithms, closest sampling from, Support vector machines, Computer science, performance criterion, Support vector machine classification, Web pages, Machine learning, Sampling methods, Robustness, receiver operating curve analysis, Labeling, learning (artificial intelligence), machine learning algorithm]
Rapid Identification of Column Heterogeneity
Sixth International Conference on Data Mining
None
2006
Data quality is a serious concern in every data management application, and a variety of quality measures have been proposed, e.g., accuracy, freshness and completeness, to capture common sources of data quality degradation. We identify and focus attention on a novel measure, column heterogeneity, that seeks to quantify the data quality problems that can arise when merging data from different sources. We identify desiderata that a column heterogeneity measure should intuitively satisfy, and describe our technique to quantify database column heterogeneity based on using a novel combination of cluster entropy and soft clustering. Finally, we present detailed experimental results, using diverse data sets of different types, to demonstrate that our approach provides a robust mechanism for identifying and quantifying database column heterogeneity.
[Data analysis, data analysis, Data security, Merging, soft clustering, Entropy, Large scale integration, data quality degradation, Cultural differences, database management systems, Degradation, database column heterogeneity, Databases, quality measures, rapid identification, Robustness, column heterogeneity measure, cluster entropy, Quality management, data management application]
Data Mining Approaches to Criminal Career Analysis
Sixth International Conference on Data Mining
None
2006
Narrative reports and criminal records are stored digitally across individual police departments, enabling the collection of this data to compile a nation-wide database of criminals and the crimes they committed. The compilation of this data through the last years presents new possibilities of analyzing criminal activity through time. Augmenting the traditional, more socially oriented, approach of behavioral study of these criminals and traditional statistics, data mining methods like clustering and prediction enable police forces to get a clearer picture of criminal careers. This allows officers to recognize crucial spots in changing criminal behaviour and deploy resources to prevent these careers from unfolding. Four important factors play a role in the analysis of criminal careers: crime nature, frequency, duration and severity. We describe a tool that extracts these from the database and creates digital profiles for all offenders. It compares all individuals on these profiles by a new distance measure and clusters them accordingly. This method yields a visual clustering of these criminal careers and enables the identification of classes of criminals. The proposed method allows for several user-defined parameters.
[Engineering profession, data compilation, criminal activity analysis, criminal behavioral study, Humans, data mining, visual clustering, police department, Data mining, Visual databases, criminal career analysis, Statistics, Computer science, police data processing, Law enforcement, nationwide criminal database, pattern clustering, Prototypes, Frequency, criminal record, offender digital profile, distance measure, behavioural sciences computing, Testing]
Biclustering Protein Complex Interactions with a Biclique Finding Algorithm
Sixth International Conference on Data Mining
None
2006
Biclustering has many applications in text mining, Web clickstream mining, and bioinformatics. When data entries are binary, the tightest biclusters become bicliques. We propose a flexible and highly efficient algorithm to compute bicliques. We first generalize the Motzkin-Straus formalism for computing the maximal clique from L<sub>1</sub> constraint to L<sub>p</sub> constraint, which enables us to provide a generalized Motzkin-Straus formalism for computing maximal-edge bicliques. By adjusting parameters, the algorithm can favor biclusters with more rows less columns, or vice verse, thus increasing the flexibility of the targeted biclusters. We then propose an algorithm to solve the generalized Motzkin-Straus optimization problem. The algorithm is provably convergent and has a computational complexity of O(/E/) where /E/ is the number of edges. Using this algorithm, we bicluster the yeast protein complex interaction network. We find that biclustering protein complexes at the protein level does not clearly reflect the functional linkage among protein complexes in many cases, while biclustering at protein domain level can reveal many underlying linkages. We show several new biologically significant results.
[Text mining, generalized Motzkin-Straus optimization problem, biclustering protein complex interactions, bioinformaticMotzkin-Straus formalisms, data mining, Web clickstream mining, Phylogeny, Data mining, Computational complexity, Proteins, Couplings, Fungi, Itemsets, biology computing, pattern clustering, text mining, Bipartite graph, yeast protein complex interaction network, Bioinformatics, biclique finding, computational complexity]
STAGGER: Periodicity Mining of Data Streams Using Expanding Sliding Windows
Sixth International Conference on Data Mining
None
2006
Sensor devices are becoming ubiquitous, especially in measurement and monitoring applications. Because of the real-time, append-only and semi-infinite natures of the generated sensor data streams, an online incremental approach is a necessity for mining stream data types. In this paper, we propose STAGGER: a one-pass, online and incremental algorithm for mining periodic patterns in data streams. STAGGER does not require that the user pre-specify the periodicity rate of the data. Instead, STAGGER discovers the potential periodicity rates. STAGGER maintains multiple expanding sliding windows staggered over the stream, where computations are shared among the multiple overlapping windows. Small-length sliding windows are imperative for early and real-time output, yet are limited to discover short periodicity rates. As streamed data arrives continuously, the sliding windows expand in length in order to cover the whole stream. Larger-length sliding windows are able to discover longer periodicity rates. STAGGER incrementally maintains a tree-like data structure for the frequent periodic patterns of each discovered potential periodicity rate. In contrast to the Fourier/Wavelet-based approaches used for discovering periodicity rates, STAGGER not only discovers a wider, more accurate set of periodicities, but also discovers the periodic patterns themselves. In fact, experimental results with real and synthetic data sets show that STAGGER outperforms Fourier/Wavelet-based approaches by an order of magnitude in terms of the accuracy of the discovered periodicity rates. Moreover, real-data experiments demonstrate the practicality of the discovered periodic patterns.
[Tree data structures, Pervasive computing, periodicity rates discovering, sensor devices, Computerized monitoring, data streams mining, expanding sliding windows, online incremental approach, data mining, Inspection, Windows, Data mining, History, tree-like data structure, Telephony, Frequency, periodicity mining, tree data structures, STAGGER, Hysteresis]
Turning Clusters into Patterns: Rectangle-Based Discriminative Data Description
Sixth International Conference on Data Mining
None
2006
The ultimate goal of data mining is to extract knowledge from massive data. Knowledge is ideally represented as human-comprehensible patterns from which end-users can gain intuitions and insights. Yet not all data mining methods produce such readily understandable knowledge, e.g., most clustering algorithms output sets of points as clusters. In this paper, we perform a systematic study of cluster description that generates interpretable patterns from clusters. We introduce and analyze novel description formats leading to more expressive power, motivate and define novel description problems specifying different trade-offs between interpretability and accuracy. We also present effective heuristic algorithms together with their empirical evaluations.
[Content based retrieval, Shape, empirical evaluations, Heuristic algorithms, Clustering methods, data mining, rectangle-based discriminative data description, data mining methods, Pareto optimization, human-comprehensible patterns, Turning, Data mining, pattern clustering, clustering algorithms, Clustering algorithms, knowledge extraction, Database systems, Iterative algorithms, cluster description]
Converting Output Scores from Outlier Detection Algorithms into Probability Estimates
Sixth International Conference on Data Mining
None
2006
Current outlier detection schemes typically output a numeric score representing the degree to which a given observation is an outlier. We argue that converting the scores into well-calibrated probability estimates is more favorable for several reasons. First, the probability estimates allow us to select the appropriate threshold for declaring outliers using a Bayesian risk model. Second, the probability estimates obtained from individual models can be aggregated to build an ensemble outlier detection framework. In this paper, we present two methods for transforming outlier scores into probabilities. The first approach assumes that the posterior probabilities follow a logistic sigmoid function and learns the parameters of the function from the distribution of outlier scores. The second approach models the score distributions as a mixture of exponential and Gaussian probability functions and calculates the posterior probabilites via the Bayes' rule. We evaluated the efficacy of both methods in the context of threshold selection and ensemble outlier detection. We also show that the calibration accuracy improves with the aid of some labeled examples.
[outlier detection algorithms, Costs, posterior probabilities, exponential probability functions, Probability, Gaussian distribution, Calibration, Gaussian probability functions, Computer science, Support vector machines, Bayesian risk model, probability estimates, exponential distribution, Bayesian methods, Support vector machine classification, logistic sigmoid function, outlier scores distribution, Bayes methods, data handling, Detection algorithms, State estimation, Logistics]
Personalization in Context: Does Context Matter When Building Personalized Customer Models?
Sixth International Conference on Data Mining
None
2006
The idea that context is important when predicting customer behavior has been maintained by scholars in marketing and data mining. However, no systematic study measuring how much the contextual information really matters in building customer models in personalization applications have been done before. In this paper, we address this problem. To this aim, we collected data containing rich contextual information by developing a special-purpose browser to help users to navigate a well- known e-commerce retail portal and purchase products on its site. The experimental results show that context does matter for the case of modeling behavior of individual customers. The granularity of contextual information also matters, and the effect of contextual information gets diluted during the process of aggregating customers' data.
[Context-aware services, Costs, Dictionaries, Navigation, purchasing, data mining, portals, Predictive models, consumer behaviour, e-commerce retail portal, Data mining, contextual information, purchase products, personalization, marketing, special-purpose browser, personalized customer models, online front-ends, Economic forecasting, Marketing and sales, Portals, customer behavior, Context modeling, electronic commerce]
Bregman Bubble Clustering: A Robust, Scalable Framework for Locating Multiple, Dense Regions in Data
Sixth International Conference on Data Mining
None
2006
In traditional clustering, every data point is assigned to at least one cluster. On the other extreme, one class clustering algorithms proposed recently identify a single dense cluster and consider the rest of the data as irrelevant. However, in many problems, the relevant data forms multiple natural clusters. In this paper, we introduce the notion of Bregman bubbles and propose Bregman bubble clustering (BBC) that seeks k dense Bregman bubbles in the data. We also present a corresponding generative model, soft BBC, and show several connections with Bregman clustering, and with a one class clustering algorithm. Empirical results on various datasets show the effectiveness of our method.
[Bregman bubble clustering, Data engineering, Mass spectroscopy, Partitioning algorithms, Phylogeny, datasets, Unsupervised learning, Proteins, pattern clustering, Clustering algorithms, Euclidean distance, Robustness, scalable framework, data handling, Bioinformatics, data dense regions]
Optimal Segmentation Using Tree Models
Sixth International Conference on Data Mining
None
2006
Sequence data are abundant in application areas such as computational biology, environmental sciences, and telecommunication. Many real-life sequences have a strong segmental structure, with segments of different complexities. In this paper we study the description of sequence segments using variable length Markov chains (VLMCs), also known as tree models. We discover the segment boundaries of a sequence and at the same time we obtain a VLMC for each segment. Such a context tree contains the probability distribution vectors that capture the essential features of the corresponding segment. We use the Bayesian information criterion (BIC) and the Krichevsky-Trofimov probability (KT) to select the number of segments of a sequence. On DNA data the method selects segments that closely correspond to the annotated regions of the genes.
[Sequences, data analysis, Biological system modeling, probability, trees (mathematics), variable length Markov chain, probability distribution vector, Probability distribution, optimal sequence data segmentation, Data mining, sequences, Krichevsky-Trofimov probability, Bayesian methods, DNA, Markov processes, Dynamic programming, Bayes methods, tree model, Bayesian information criterion, Mobile computing, Context modeling, Computational biology]
Mining for Tree-Query Associations in a Graph
Sixth International Conference on Data Mining
None
2006
New applications of data mining, such as in biology, bioinformatics, or sociology, are faced with large datasets structured as graphs. We present an efficient algorithm for mining associations between tree queries in a large graph. Tree queries are powerful tree-shaped patterns featuring existential variables and data constants. Our algorithm applies the theory of conjunctive database queries to make the generation of association rules efficient. We propose a practical, database-oriented implementation in SQL, and show that the approach works in practice through experiments on data about food webs, protein interactions, and citation analysis.
[association rule, Citation analysis, graph theory, data mining, tree-query association, Spatial databases, Organisms, Data mining, Association rules, conjunctive database query, SQL, Proteins, Tree graphs, graph-structured data, Sociology, tree-shaped pattern, tree data structures, Bioinformatics, Pattern matching]
Global and Componentwise Extrapolation for Accelerating Data Mining from Large Incomplete Data Sets with the EM Algorithm
Sixth International Conference on Data Mining
None
2006
The expectation-maximization (EM) algorithm is one of the most popular algorithms for data mining from incomplete data. However, when applied to large data sets with a large proportion of missing data, the EM algorithm may converge slowly. The triple jump extrapolation method can effectively accelerate the EM algorithm by substantially reducing the number of iterations required for EM to converge. There are two options for the triple jump method, global extrapolation (TJEM) and componentwise extrapolation (CTJEM). We tried these two methods for a variety of probabilistic models and found that in general, global extraplolation yields a better performance, but there are cases where componentwise extrapolation yields very high speedup. In this paper, we investigate when componentwise extrapolation should be preferred. We conclude that, when the Jacobian of the EM mapping is diagonal or block diagonal, CTJEM should be preferred. We show how to determine whether a Jacobian is diagonal or block diagonal and experimentally confirm our claim. In particular, we show that CTJEM is especially effective for the semi-supervised Bayesian classifier model given a highly sparse data set.
[Parameter estimation, componentwise extrapolation, large incomplete data sets, data mining, triple jump extrapolation, Data mining, probabilistic model, Convergence, Jacobian matrices, Information science, global extrapolation, pattern classification, data analysis, expectation-maximization algorithm, EM mapping, highly sparse data set, probability, semisupervised Bayesian classifier model, Extrapolation, extrapolation, Bayesian methods, Packaging, expectation-maximisation algorithm, Iterative algorithms, Bayes methods, Acceleration, EM algorithm]
Keyphrase Extraction Using Semantic Networks Structure Analysis
Sixth International Conference on Data Mining
None
2006
Keyphrases play a key role in text indexing, summarization and categorization. However, most of the existing keyphrase extraction approaches require human-labeled training sets. In this paper, we propose an automatic keyphrase extraction algorithm, which can be used in both supervised and unsupervised tasks. This algorithm treats each document as a semantic network. Structural dynamics of the network are used to extract keyphrases (key nodes) unsupervised. Experiments demonstrate the proposed algorithm averagely improves 50% in effectiveness and 30% in efficiency in unsupervised tasks and performs comparatively with supervised extractors. Moreover, by applying this algorithm to supervised tasks, we develop a classifier with an overall accuracy up to 80%.
[semantic networks structure analysis, text analysis, Data mining, classification, unsupervised task, Unsupervised learning, Computer science, Software libraries, Supervised learning, feature extraction, structural dynamics, Training data, Web pages, Frequency, automatic keyphrase extraction, Books, Indexing]
Subjectivity Categorization of Weblog with Part-of-Speech Based Smoothing
Sixth International Conference on Data Mining
None
2006
Experts from different domains try to mine users' comments on Weblogs for different reasons such as politics or commerce. All these needs necessitate automatically distinguishing subjective Weblog contents from objective ones, namely subjectivity categorization. Since Weblogs contain various topics from different domains, limited training data can hardly cover all the topics and "unseen words" becomes a serious problem for categorization tasks. In this paper, part-of-speech (POS) based smoothing is proposed to alleviate the "unseen words" problem. In conjunction with a naive Bayes model constructed from limited training data, the probability of an unseen word in a new domain can be well smoothed by the probability of its POS result. Empirical studies on five datasets show that our approach consistently outperforms the basic naive Bayes with Laplace smoothing. In a cross-domain experiment, our approach achieves 22.0% improvement in Macro Fl and 24.4% in Micro Fl over basic naive Bayes. These verify that POS based smoothing can indeed benefit subjectivity categorization, especially in the cases with a large number of unseen words.
[naive Bayes model, Smoothing methods, subjective Weblog contents, smoothing methods, part-of-speech based smoothing, subjectivity categorization, Data mining, classification, Weblogs, unseen words problem, Mood, Laplace smoothing, Neural networks, Asia, Training data, Information services, training data, Cameras, speech processing, Internet, Bayes methods, Web sites]
Applying Data Mining to Pseudo-Relevance Feedback for High Performance Text Retrieval
Sixth International Conference on Data Mining
None
2006
In this paper, we investigate the use of data mining, in particular the text classification and co-training techniques, to identify more relevant passages based on a small set of labeled passages obtained from the blind feedback of a retrieval system. The data mining results are used to expand query terms and to re-estimate some of the parameters used in a probabilistic weighting function. We evaluate the data mining based feedback method on the TREC HARD data set. The results show that data mining can be successfully applied to improve the text retrieval performance. We report our experimental findings in detail.
[pattern classification, text analysis, data mining, pseudo-relevance feedback, Information retrieval, text classification, Data mining, Information technology, Computer science, high performance text retrieval system, blind feedback, probabilistic weighting function, Feedback, Text categorization, Supervised learning, relevance feedback, Training data, Labeling, labeled passages, Testing]
Improving Personalization Solutions through Optimal Segmentation of Customer Bases
Sixth International Conference on Data Mining
None
2006
On the Web, where the search costs are low and the competition is just a mouse click away, it is crucial to segment the customers intelligently in order to offer more targeted and personalized products and services to them. Traditionally, customer segmentation is achieved using statistics-based methods that compute a set of statistics from the customer data and group customers into segments by applying distance-based clustering algorithms in the space of these statistics. In this paper, we present a direct grouping based approach to computing customer segments that groups customers not based on computed statistics, but in terms of optimally combining transactional data of several customers to build a data mining model of customer behavior for each group. Then building customer segments becomes a combinatorial optimization problem of finding the best partitioning of the customer base into disjoint groups. The paper shows that finding an optimal customer partition is NP-hard, proposes a suboptimal direct grouping segmentation method and empirically compares it against traditional statistics-based segmentation and 1-to-1 methods across multiple experimental conditions. We show that the direct grouping method significantly dominates the statistics-based and 1-to-1 approaches across all the experimental conditions, while still being computationally tractable. We also show that there are very few size-one customer segments generated by the best direct grouping method and that micro-segmentation provides the best approach to personalization.
[Demography, data mining, combinatorial optimization problem, Predictive models, consumer behaviour, World Wide Web, Data mining, customer grouping, transactional data, customer segmentation, personalization, marketing application, 1-to-1 marketing, Clustering algorithms, Cost function, customer bases, direct grouping segmentation, direct grouping based approach, statistics-based segmentation, optimal segmentation, optimal customer partition, Statistics, Customer profiles, customer profiles, NP-hard problem, Aggregates, customer segments computing, data mining model, microsegmentation, Mice, personalization solutions, customer behavior, Context modeling]
Secure Distributed k-Anonymous Pattern Mining
Sixth International Conference on Data Mining
None
2006
Privacy-preserving data mining is an important area that studies privacy issues of data mining. When the goal is to share data mining results, two privacy-related problems may arise. The first one is how to compute the data-mining results among several parties without sharing the data. Cryptography-based primitives are the basic tool used to develop ad-hoc secure multi-party computation protocols that share information as less as possible during the computation under different adversary models. The second one is how to produce data mining results that provably do not contain threats to the anonymity of individuals. The concept of k-anonymity has been used to discover anonymity-preserving frequent patterns, and centralized algorithms have been developed. In this paper and for the first time, we study how to produce anonymity-preserving data mining results in a distributed environment. We present two privacy-preserving strategies and show their feasibility through experimental analysis.
[Data privacy, cryptographic protocols, distributed k-anonymous pattern mining security, Decision making, privacy preservation, data mining, distributed environment, Data mining, Association rules, History, Cryptographic protocols, ad-hoc secure multiparty computation protocols, Itemsets, cryptography-based primitives, data privacy, Cryptography, Protection, k-anonymity, anonymity preservation]
Dimension Reduction for Supervised Ordering
Sixth International Conference on Data Mining
None
2006
Ordered lists of objects are widely used as representational forms. Such ordered objects include Web search results and best-seller lists. Techniques for processing such ordinal data are being developed, particularly methods for a supervised ordering task: i.e., learning functions used to sort objects from sample orders. In this article, we propose two dimension reduction methods specifically designed to improve prediction performance in a supervised ordering task.
[Performance evaluation, Design methodology, Information retrieval, dimension reduction, Sorting, learning function, Degradation, Search engines, supervised ordering task, Marketing and sales, learning (artificial intelligence), statistical analysis, Web search, Principal component analysis, Testing]
A Parameterized Probabilistic Model of Network Evolution for Supervised Link Prediction
Sixth International Conference on Data Mining
None
2006
We introduce a new approach to the problem of link prediction for network structured domains, such as the Web, social networks, and biological networks. Our approach is based on the topological features of network structures, not on the node features. We present a novel parameterized probabilistic model of network evolution and derive an efficient incremental learning algorithm for such models, which is then used to predict links among the nodes. We show some promising experimental results using biological network data sets.
[Biological system modeling, Social network services, Stationary state, probability, network evolution, network structured domain, social networks, Predictive models, network theory (graphs), World Wide Web, topological features, Data mining, supervised link prediction, biological networks, Proteins, evolutionary computation, Evolution (biology), incremental learning, Prediction algorithms, Inference algorithms, parameterized probabilistic model, learning (artificial intelligence), Bioinformatics, network structures]
Incremental Mining of Frequent Query Patterns from XML Queries for Caching
Sixth International Conference on Data Mining
None
2006
Existing studies for mining frequent XML query patterns mainly introduce a straightforward candidate generate-and-test strategy and compute frequencies of candidate query patterns from scratch periodically by checking the entire transaction database, which consists of XML query patterns transformed from user queries. However, it is nontrivial to maintain such discovered frequent patterns in real XML databases because there may incur frequent updates that may not only invalidate some existing frequent query patterns but also generate some new frequent ones. Accordingly, existing proposals are inefficient for the evolution of the transaction database. To address these problems, this paper presents an efficient algorithm IPS-FXQPMiner for mining frequent XML query patterns without candidate maintenance and costly tree-containment checking. We transform XML queries into sequences through a one- to-one mapping and then mine the frequent sequences to generate frequent XML query patterns. More importantly, based on IPS-FXQPMiner, an efficient incremental algorithm, Incre-FXQPMiner is proposed to incrementally mine frequent XML query patterns, which can minimize the I/O and computation requirements for handling incremental updates. Our experimental study on various real-life datasets demonstrates the efficiency and scalability of our algorithms over previous known alternatives.
[Incre-FXQPMiner, Scalability, data mining, Information representation, Transaction databases, Proposals, Test pattern generators, Data mining, database management systems, transaction database, Computer science, query processing, generate-and-test strategy, XML query patterns, XML, tree-containment checking, Frequency, incremental mining, Testing]
The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering
Sixth International Conference on Data Mining
None
2006
The nonnegative matrix factorization (NMF) has been shown recently to be useful for clustering and various extensions and variations of NMF have been proposed recently. Despite significant research progress in this area, few attempts have been made to establish the connections between various factorization methods while highlighting their differences. In this paper we aim to provide a comprehensive study on matrix factorization for clustering. In particular, we present an overview and summary on various matrix factorization algorithms and theoretically analyze the relationships among them. Experiments are also conducted to empirically evaluate and compare various factorization methods. In addition, our study also answers several previously unaddressed yet important questions for matrix factorizations including the interpretation and normalization of cluster posterior and the benefits and evaluation of simultaneous clustering. We expect our study would provide good insights on matrix factorization research for clustering.
[Algorithm design and analysis, Text mining, matrix factorization, Clustering methods, Pattern recognition, matrix decomposition, Matrix decomposition, Data mining, relation, Computer science, pattern clustering, interpretation, normalization, Clustering algorithms, DNA, nonnegative matrix factorization methods, clustering, Principal component analysis, simultaneous clustering]
Integrating Features from Different Sources for Music Information Retrieval
Sixth International Conference on Data Mining
None
2006
Efficient and intelligent music information retrieval is a very important topic of the 21st century. With the ultimate goal of building personal music information retrieval systems, this paper studies the problem of identifying "similar" artists using both lyrics and acoustic data. In this paper, we present a clustering algorithm that integrates features from both sources to perform bimodal learning. The algorithm is tested on a data set consisting of 570 songs from 53 albums of 41 artists using artist similarity provided by All Music Guide. Experimental results show that the accuracy of artist similarity classifiers can be significantly improved and that artist similarity can be efficiently identified.
[Algorithm design and analysis, clustering algorithm, bimodal learning, information retrieval, data set, multimedia systems, Boosting, Personnel, Unsupervised learning, acoustic data, Computer science, music, pattern clustering, Supervised learning, Clustering algorithms, Semisupervised learning, Motion pictures, Music information retrieval, music information retrieval]
How Bayesians Debug
Sixth International Conference on Data Mining
None
2006
Manual debugging is expensive. And the high cost has motivated extensive research on automated fault localization in both software engineering and data mining communities. Fault localization aims at automatically locating likely fault locations, and hence assists manual debugging. A number of fault localization algorithms have been developed in recent years, which prove effective when multiple failing and passing cases are available. However, we notice what is more commonly encountered in practice is the two-sample debugging problem, where only one failing and one passing cases are available. This problem has been either overlooked or insufficiently tackled in previous studies. In this paper, we develop a new fault localization algorithm, named BayesDebug, which simulates some manual debugging principles through a Bayesian approach. Different from existing approaches that base fault analysis on multiple passing and failing cases, BayesDebug only requires one passing and one failing cases. We reason about why BayesDebug fits the two- sample debugging problem and why other approaches do not. Finally, an experiment with a real-world program grep-2.2 is conducted, which exemplifies the effectiveness of BayesDebug.
[Chaos, BayesDebug, program debugging, Costs, Instruments, software reliability, data mining, Debugging, automated debugging, Fault location, Data mining, automated fault localization, fault analysis, Computer science, Bayesian methods, NIST, software engineering, Bayes methods, Software engineering]
On the Use of Structure and Sequence-Based Features for Protein Classification and Retrieval
Sixth International Conference on Data Mining
None
2006
The need to retrieve or classify protein molecules using structure or sequence-based similarity measures underlies a wide range of biomedical applications. In drug discovery, researchers search for proteins that share specific chemical properties as possible sources for new treatment. With folding simulations, similar intermediate structures might be indicative of a common folding pathway. To derive any type of similarity, however, one must have an effective model of the protein that allows for easy comparison. In this work, we present two normalized, stand-alone representations of proteins that enable fast and efficient object retrieval based on sequence or structure. To create our sequence-based representation, we take the frequency and scoring matrices returned by the PSTBIAST alignment algorithm and create a normalized summary using a discrete wavelet transform. Our structural descriptor is constructed using an algorithm we developed previously. First, we transform each 3D structure into a 2D distance matrix by calculating the pair-wise distance between the amino acids of a protein. We normalize this matrix and apply a 2D wavelet decomposition to generate a set of approximation coefficients, which serve as our feature vector. We also concatenate the sequence and structural descriptors together to create a hybrid solution. We evaluate the generality of our models by using them as database indices for nearest-neighbor and range-based retrieval experiments as well as feature vectors for classification using support vector machines. We find that our methods provide excellent performance when compared with the current state-of-the-art techniques of each task. Our results show that the sequence-based representation is on par with, or out-performs, the structure-based representation. Moreover, we find that in the classification context, the hybrid strategy affords a significant improvement over sequence or structure.
[Drugs, protein molecules, wavelet transforms, chemical properties, Amino acids, structure-based representation, folding simulations, Proteins, sequence-based similarity measures, biology computing, drug discovery, proteins, protein retrieval, sequence-based representation, stand-alone representations, pattern classification, support vector machines, information retrieval, discrete wavelet transform, Spatial databases, sequence-based features, Discrete wavelet transforms, Biomedical measurements, Matrix decomposition, scoring matrices, Chemicals, matrix algebra, Discrete transforms, Frequency, medical computing, protein classification]
Accelerating Newton Optimization for Log-Linear Models through Feature Redundancy
Sixth International Conference on Data Mining
None
2006
Log-linear models are widely used for labeling feature vectors and graphical models, typically to estimate robust conditional distributions in presence of a large number of potentially redundant features. Limited-memory quasi-Newton methods like LBFGS or BLMVM are optimization workhorses for such applications, and most of the training time is spent computing the objective and gradient for the optimizer. We propose a simple technique to speed up the training optimization by clustering features dynamically, and interleaving the standard optimizer with another, coarse-grained, faster optimizer that uses far fewer variables. Experiments with logistic regression training for text classification and conditional random field (CRF) training for information extraction show promising speed-ups between 2times and 9times without any systematic or significant degradation in the quality of the estimated models.
[Optimization methods, regression analysis, text classification, Data mining, Degradation, Graphical models, optimisation, feature extraction, Newton optimization, feature clustering, Robustness, Labeling, gradient methods, Newton method, logistic regression training, pattern classification, optimizer gradient, conditional random field, log-linear model, information extraction, Text categorization, feature vectors, graphical models, feature redundancy, Interleaved codes, limited-memory quasiNewton method, Acceleration, Logistics]
P3C: A Robust Projected Clustering Algorithm
Sixth International Conference on Data Mining
None
2006
Projected clustering has emerged as a possible solution to the challenges associated with clustering in high dimensional data. A projected cluster is a subset of points together with a subset of attributes, such that the cluster points project onto a small range of values in each of these attributes, and are uniformly distributed in the remaining attributes. Existing algorithms for projected clustering rely on parameters whose appropriate values are difficult to set by the user, or are unable to identify projected clusters with few relevant attributes. In this paper, we present a robust algorithm for projected clustering that can effectively discover projected clusters in the data while minimizing the number of parameters required as input. In contrast to all previous approaches, our algorithm can discover, under very general conditions, the true number of projected clusters. We show through an extensive experimental evaluation that our algorithm: (1) significantly outperforms existing algorithms for projected clustering in terms of accuracy; (2) is effective in detecting very low-dimensional projected clusters embedded in high dimensional spaces; (3) is effective in detecting clusters with varying orientation in their relevant subspaces; (4) is scalable with respect to large data sets and high number of dimensions.
[robust algorithm, Nearest neighbor searches, Databases, pattern clustering, high dimensional data, very large databases, Clustering algorithms, large data sets, robust projected clustering algorithm, Robustness, Data models, extensive experimental evaluation, Principal component analysis]
Frequent Closed Itemset Mining Using Prefix Graphs with an Efficient Flow-Based Pruning Strategy
Sixth International Conference on Data Mining
None
2006
This paper presents PGMiner, a novel graph-based algorithm for mining frequent closed itemsets. Our approach consists of constructing a prefix graph structure and decomposing the database to variable length bit vectors, which are assigned to nodes of the graph. The main advantage of this representation is that the bit vectors at each node are relatively shorter than those produced by existing vertical mining methods. This facilitates fast frequency counting of itemsets via intersection operations. We also devise several inter- node and intra-node pruning strategies to substantially reduce the combinatorial search space. Unlike other existing approaches, we do not need to store in memory the entire set of closed itemsets that have been mined so far in order to check whether a candidate itemset is closed. This dramatically reduces the memory usage of our algorithm, especially for low support thresholds. Our experiments using synthetic and real-world data sets show that PGMiner outperforms existing mining algorithms by as much as an order of magnitude and is scalable to very large databases.
[PGMiner, Data analysis, Merging, graph theory, data mining, Transaction databases, Data mining, intranode pruning, Computer science, Degradation, Runtime, Itemsets, frequent closed itemset mining, Frequency, prefix graph structure, flow-based pruning strategy, internode pruning]
Efficient Clustering of Uncertain Data
Sixth International Conference on Data Mining
None
2006
We study the problem of clustering data objects whose locations are uncertain. A data object is represented by an uncertainty region over which a probability density function (pdf) is defined. One method to cluster uncertain objects of this sort is to apply the UK-means algorithm, which is based on the traditional K-means algorithm. In UK-means, an object is assigned to the cluster whose representative has the smallest expected distance to the object. For arbitrary pdf, calculating the expected distance between an object and a cluster representative requires expensive integration computation. We study various pruning methods to avoid such expensive expected distance calculation.
[Measurement errors, Uncertainty, Costs, probability, Gaussian distribution, Vehicles, Computer science, Histograms, uncertain data clustering, Animals, data object, pattern clustering, Clustering algorithms, Probability density function, pruning method, data handling, UK-means algorithm, probability density function]
A Data Mining Approach for Capacity Building of Stakeholders in Integrated Flood Management
Sixth International Conference on Data Mining
None
2006
New approaches to managing flood events are increasingly of more relevance due to recent widespread floods and the presumed changes in the climate. These approaches fall under the integrated flood management (IFM) banner and focus not only on flood prevention, but on flood resilience. This paper introduces an application (FLORETO) for IFM that utilizes the data mining approach, in a web based three tier system, devoted to the capacity building of stakeholders as a micro-scale resilience strategy of IFM. The intelligent models, which constitute the business logic in FLORETO, are used to match the input parameters or design criteria, describing properties prone to flooding, to technically justifiable flood mitigation measures. Datasets from the German city of Kellinghusen were collected and intelligent models were built. Satisfactory results have been obtained, which shows the promise of this data mining approach and opens the door for its application for IFM in other regions.
[floods, micro-scale resilience strategy, data mining, Strategic planning, Financial management, stakeholders, integrated flood management, Data mining, Floods, Resilience, Web based three tier system, Technology management, flood mitigation measures, FLORETO, Engineering management, public administration, Land use planning, business logic, Kellinghusen, Internet, Risk management, Protection, capacity building]
Local Correlation Tracking in Time Series
Sixth International Conference on Data Mining
None
2006
We address the problem of capturing and tracking local correlations among time evolving time series. Our approach is based on comparing the local auto-covariance matrices (via their spectral decompositions) of each series and generalizes the notion of linear cross-correlation. In this way, it is possible to concisely capture a wide variety of local patterns or trends. Our method produces a general similarity score, which evolves over time, and accurately reflects the changing relationships. Finally, it can also be estimated incrementally, in a streaming setting. We demonstrate its usefulness, robustness and efficiency on a wide range of real datasets.
[Biomedical equipment, local autocovariance matrices, Telecommunication traffic, Medical services, Throughput, time series, local correlation tracking, Matrix decomposition, Sun, Time varying systems, System performance, Robustness, Biomedical monitoring, correlation methods]
Who Thinks Who Knows Who? Socio-cognitive Analysis of Email Networks
Sixth International Conference on Data Mining
None
2006
Interpersonal interaction plays an important role in organizational dynamics, and understanding these interaction networks is a key issue for any organization, since these can be tapped to facilitate various organizational processes. However, the approaches of collecting data about them using surveys/interviews are fraught with problems of scalability, logistics and reporting biases, especially since such surveys may be perceived to be intrusive. Widespread use of computer networks for organizational communication provides a unique opportunity to overcome these difficulties and automatically map the organizational networks with a high degree of detail and accuracy. This paper describes an effective and scalable approach for modeling organizational networks by tapping into an organization's email communication. The approach models communication between actors as non-stationary Bernoulli trials and Bayesian inference is used for estimating model parameters over time. This approach is useful for socio-cognitive analysis (who knows who knows who) of organizational communication networks. Using this approach, novel measures for analysis of (i) closeness between actors' perceptions about such organizational networks (agreement), (ii) divergence of an actor's perceptions about organizational network from reality (misperception) are explained. Using the Enron email data, we show that these techniques provide sociologists with a new tool to understand organizational networks.
[Scalability, electronic mail, business communication, Electronic mail, nonstationary Bernoulli trials, model parameter estimation, Enron email data, interpersonal interaction, parameter estimation, sociologists, Computer networks, social sciences computing, Communication networks, Bayesian inference, Social network services, socio-cognitive analysis, computer networks, organizational dynamics, logistics problems, organizational communication, Computer science, scalability problems, email networks, Bayesian methods, reporting biases problems, Bayes methods, Resource management, organisational aspects, Professional communication, Logistics]
An Efficient Reference-Based Approach to Outlier Detection in Large Datasets
Sixth International Conference on Data Mining
None
2006
A bottleneck to detecting distance and density based outliers is that a nearest-neighbor search is required for each of the data points, resulting in a quadratic number of pairwise distance evaluations. In this paper, we propose a new method that uses the relative degree of density with respect to a fixed set of reference points to approximate the degree of density defined in terms of nearest neighbors of a data point. The running time of our algorithm based on this approximation is 0(Rn log n) where n is the size of dataset and R is the number of reference points. Candidate outliers are ranked based on the outlier score assigned to each data point. Theoretical analysis and empirical studies show that our method is effective, efficient, and highly scalable to very large datasets.
[Tree data structures, approximation theory, density degree approximation, Art, nearest-neighbor search, Data mining, Distributed computing, Spatial indexes, outlier detection, large datasets, distance detection, Nearest neighbor searches, Computer science, reference-based approach, Statistical distributions, Clustering algorithms, Approximation algorithms, pairwise distance evaluations, density based outliers, search problems, computational complexity, pattern recognition]
Using an Ensemble of One-Class SVM Classifiers to Harden Payload-based Anomaly Detection Systems
Sixth International Conference on Data Mining
None
2006
Unsupervised or unlabeled learning approaches for network anomaly detection have been recently proposed. In particular, recent work on unlabeled anomaly detection focused on high speed classification based on simple payload statistics. For example, PAYL, an anomaly IDS, measures the occurrence frequency in the payload of n-grams. A simple model of normal traffic is then constructed according to this description of the packets' content. It has been demonstrated that anomaly detectors based on payload statistics can be "evaded" by mimicry attacks using byte substitution and padding techniques. In this paper we propose a new approach to construct high speed payload-based anomaly IDS intended to be accurate and hard to evade. We propose a new technique to extract the features from the payload. We use a feature clustering algorithm originally proposed for text classification problems to reduce the dimensionality of the feature space. Accuracy and hardness of evasion are obtained by constructing our anomaly-based IDS using an ensemble of one-class SVM classifiers that work on different feature spaces.
[text analysis, payload-based anomaly detection, Telecommunication traffic, Frequency measurement, network anomaly detection, telecommunication computing, padding technique, feature extraction, Intrusion detection, Detectors, Traffic control, pattern classification, text classification problem, support vector machines, feature clustering algorithm, computer networks, unsupervised learning approach, Statistics, payload statistics, Support vector machines, security of data, pattern clustering, Support vector machine classification, unlabeled learning approach, Feature extraction, Payloads]
Relational Ensemble Classification
Sixth International Conference on Data Mining
None
2006
Relational classification aims at including relations among entities, for example taking relations between documents such as a common author or citations into account. However, considering more than one relation can further improve classification accuracy. In this paper we introduce a new approach to make use of several relations as well as both relations and attributes for classification using ensemble methods. To accomplish this, we present a generic relational ensemble model, that can use different relational and local classifiers as components. Furthermore, we discuss solutions for several problems concerning relational data such as heterogeneity, sparsity, and multiple relations. Finally, we provide empirical evidence, that our relational ensemble methods outperform existing relational classification methods, even rather complex models such as relational probability trees (RPTs), relational dependency networks (RDNs) and relational Bayesian classifiers (RBCs).
[text analysis, Merging, relational probability tree, Information retrieval, text classification, classification, machine learning, relational dependency network, Computer science, Publishing, Bayesian methods, Text categorization, Web pages, Iterative algorithms, relational Bayesian classifier, relational ensemble classification, Autocorrelation, learning (artificial intelligence), Classification tree analysis]
Discovering Partial Orders in Binary Data
Sixth International Conference on Data Mining
None
2006
We approach the problem of discovering interesting orders in data. In many applications, it is more important to find interesting partial orders since there is often no clear ordering between certain sets of elements. Furthermore, a partial order is more robust against partially erroneous data. We present the notion of fundamental partial orders (FPO), and argue that any partial order that satisfies this property is an interesting partial order. To mine such partial orders, we present a two-stage methodology that first finds an interesting total order, and then discovers a partial order satisfying FPO using this total order. To illustrate, we focus on {0,1} data. This is an important problem with many applications, e.g., in paleontology, where we chronologically order fossil sites by minimizing Lazarus counts. We present the experimental results of our method on paleontological data, and show that it outperforms existing approaches. The techniques developed here are general and can be abstracted for mining partial orders in any setting.
[partially erroneous data, fundamental partial orders, Cloning, Humans, Genomics, data mining, Frequency estimation, Lazarus counts, Data mining, palaeontology, Biological cells, paleontological data, binary data, Chromosome mapping, discovering partial orders, Databases, Robustness, Bioinformatics, partial orders mining]
Stability Region Based Expectation Maximization for Model-based Clustering
Sixth International Conference on Data Mining
None
2006
In spite of the initialization problem, the expectation-maximization (EM) algorithm is widely used for estimating the parameters in several data mining related tasks. Most popular model-based clustering techniques might yield poor clusters if the parameters are not initialized properly. To reduce the sensitivity of initial points, a novel algorithm for learning mixture models from multivariate data is introduced in this paper. The proposed algorithm takes advantage of TRUST-TECH (TRansformation Under STability- reTaining Equilibra CHaracterization) to compute neighborhood local maxima on likelihood surface using stability regions. Basically, our method coalesces the advantages of the traditional EM with that of the dynamic and geometric characteristics of the stability regions of the corresponding nonlinear dynamical system of the log-likelihood function. Two phases namely, the EM phase and the stability region phase, are repeated alternatively in the parameter space to achieve improvements in the maximum likelihood. Though applied to Gaussian mixtures in this paper, our technique can be easily generalized to any other parametric finite mixture model. The algorithm has been tested on both synthetic and real datasets and the improvements in the performance compared to other approaches are demonstrated. The robustness with respect to initialization is also illustrated experimentally.
[Parameter estimation, Stochastic processes, data mining, Nonlinear dynamical systems, Data mining, stability-retaining equilibria characterization, log-likelihood function, Clustering algorithms, multivariate data, parameter estimation, Robustness, likelihood surface, learning (artificial intelligence), Newton method, Testing, Maximum likelihood estimation, neighborhood local maxima, Stability, expectation-maximization algorithm, model-based clustering, Gaussian mixture model learning algorithm, nonlinear dynamical system, pattern clustering, Gaussian processes, expectation-maximisation algorithm, stability region phase]
Co-clustering Documents and Words Using Bipartite Isoperimetric Graph Partitioning
Sixth International Conference on Data Mining
None
2006
In this paper, we present a novel graph theoretic approach to the problem of document-word co-clustering. In our approach, documents and words are modeled as the two vertices of a bipartite graph. We then propose isoperimetric co-clustering algorithm (ICA) - a new method for partitioning the document-word bipartite graph. ICA requires a simple solution to a sparse system of linear equations instead of the eigenvalue or SVD problem in the popular spectral co-clustering approach. Our extensive experiments performed on publicly available datasets demonstrate the advantages of ICA over spectral approach in terms of the quality, efficiency and stability in partitioning the document-word bipartite graph.
[text analysis, isoperimetric coclustering, Stability, graph theory, Independent component analysis, bipartite isoperimetric graph partitioning, sparse system, linear equation, Partitioning algorithms, Data mining, Equations, eigenvalues and eigenfunctions, Machine vision, pattern clustering, Eigenvalues and eigenfunctions, Bipartite graph, Random variables, document-word coclustering, Mutual information, eigenvalue]
Latent Dirichlet Co-Clustering
Sixth International Conference on Data Mining
None
2006
We present a generative model for simultaneously clustering documents and terms. Our model is a four-level hierarchical Bayesian model, in which each document is modeled as a random mixture of document topics , where each topic is a distribution over some segments of the text. Each of these segments in the document can be modeled as a mixture of word topics where each topic is a distribution over words. We present efficient approximate inference techniques based on Markov Chain Monte Carlo method and a moment-matching algorithm for empirical Bayes parameter estimation. We report results in document modeling, document and term clustering, comparing to other topic models, Clustering and Co-Clustering algorithms including latent Dirichlet allocation (LDA), model-based overlapping clustering (MOC), model-based overlapping co-clustering (MOCC) and information-theoretic co-clustering (ITCC).
[Markov Chain Monte Carlo method, latent Dirichlet allocation, text analysis, Parameter estimation, empirical Bayes parameter estimation, latent Dirichlet co-clustering, Data mining, document topics random mixture, Monte Carlo methods, Clustering algorithms, approximate inference techniques, Linear discriminant analysis, document modeling, text segments, model-based overlapping coclustering, clustering documents, Natural languages, moment-matching algorithm, Large scale integration, Computer science, Bayesian methods, pattern clustering, information-theoretic coclustering, Markov processes, Frequency, Inference algorithms, Bayes methods, four-level hierarchical Bayesian model]
Latent Friend Mining from Blog Data
Sixth International Conference on Data Mining
None
2006
The rapid growth of blog (also known as "weblog") data provides a rich resource for social community mining. In this paper, we put forward a novel research problem of mining the latent friends of bloggers based on the contents of their blog entries. Latent friends are defined in this paper as people who share the similar topic distribution in their blogs. These people may not actually know each other, but they have the interest and potential to find each other out. Three approaches are designed for latent friend detection. The first one, called cosine similarity-based method, determines the similarity between bloggers by calculating the cosine similarity between the contents of the blogs. The second approach, known as topic-based method, is based on the discovery of latent topics using a latent topic model and then calculating the similarity at the topic level. The third one is two-level similarity-based, which is conducted in two stages. In the first stage, an existing topic hierarchy is exploited to build a topic distribution for a blogger. Then, in the second stage, a detailed similarity comparison is conducted for bloggers that are close in interest to each other which are discovered in the first stage. Our experimental results show that both the topic-based and two-level similarity-based methods work well, and the last approach performs much better than the first two. In this paper, we give a detailed analysis of the advantages and disadvantages of different approaches.
[latent friend mining, Data privacy, Social network services, data mining, Data engineering, Electronic mail, topic-based method, latent topic model, Data mining, Computer science, social community mining, Asia, Information services, Weblog data, cosine similarity-based method, Internet, Web sites]
The PDD Framework for Detecting Categories of Peculiar Data
Sixth International Conference on Data Mining
None
2006
Peculiar data are objects that are relatively few in number and significantly different from the other objects in a data set. In this paper, we propose the PDD framework for detecting multiple categories of peculiar data. This framework provides an extensible set of perspectives for viewing data, currently including viewing data as a set of records, attributes, frequencies, intervals, sequences, or sequences of changes. By using these six views of the data, multiple categories of peculiar data can be detected to reveal different aspects of the data. For each view, the framework provides an extensible set of peculiarity measures to detect outliers and other kinds of peculiar data. The PDD framework has been implemented for Oracle and Access. Experiments are reported for data sets concerning Regina weather and NHL hockey.
[Event detection, Density measurement, data analysis, data mining, Access, peculiarity measure, Cleaning, Medical diagnosis, database management systems, outlier detection, Regina weather, Computer science, NHL hockey, PDD framework, Intrusion detection, peculiar data category detection, Object detection, Frequency, Oracle, Cancer, data aspects]
Entity Resolution with Markov Logic
Sixth International Conference on Data Mining
None
2006
Entity resolution is the problem of determining which records in a database refer to the same entities, and is a crucial and expensive step in the data mining process. Interest in it has grown rapidly, and many approaches have been proposed. However, they tend to address only isolated aspects of the problem, and are often ad hoc. This paper proposes a well-founded, integrated solution to the entity resolution problem based on Markov logic. Markov logic combines first-order logic and probabilistic graphical models by attaching weights to first-order formulas, and viewing them as templates for features of Markov networks. We show how a number of previous approaches can be formulated and seamlessly combined in Markov logic, and how the resulting learning and inference problems can be solved efficiently. Experiments on two citation databases show the utility of this approach, and evaluate the contribution of the different components.
[graph theory, data mining, first-order logic, learning problems, Data engineering, inference problems, Data mining, database management systems, formal logic, probabilistic graphical models, Graphical models, entity-relationship modelling, learning (artificial intelligence), citation databases, probability, Probabilistic logic, Spatial databases, entity resolution, inference mechanisms, Markov networks, Markov random fields, Couplings, Computer science, Markov processes, Markov logic, Joining processes, Logistics]
Boosting Kernel Models for Regression
Sixth International Conference on Data Mining
None
2006
This paper proposes a general boosting framework for combining multiple kernel models in the context of both classification and regression problems. Our main approach is built on the idea of gradient boosting together with a new regularization scheme and aims at reducing the cubic complexity of training kernel models. We focus mainly on using the proposed boosting framework to combine kernel ridge regression (KRR) models for regression tasks. Numerical experiments on four large-scale data sets have shown that boosting multiple small KRR models is superior to training a single large KRR model on both improving generalization performance and reducing computational requirements.
[pattern classification, gradient boosting method, support vector machines, regression analysis, Boosting, cubic complexity, Pattern recognition, Quadratic programming, machine learning, Sun, Support vector machines, Computer science, support vector machine, kernel ridge regression model, Large-scale systems, learning (artificial intelligence), Kernel, gradient methods, regularization scheme, Context modeling, Logistics, computational complexity]
Boosting for Learning Multiple Classes with Imbalanced Class Distribution
Sixth International Conference on Data Mining
None
2006
Classification of data with imbalanced class distribution has posed a significant drawback of the performance attainable by most standard classifier learning algorithms, which assume a relatively balanced class distribution and equal misclassification costs. This learning difficulty attracts a lot of research interests. Most efforts concentrate on bi-class problems. However, bi-class is not the only scenario where the class imbalance problem prevails. Reported solutions for bi-class applications are not applicable to multi-class problems. In this paper, we develop a cost-sensitive boosting algorithm to improve the classification performance of imbalanced data involving multiple classes. One barrier of applying the cost-sensitive boosting algorithm to the imbalanced data is that the cost matrix is often unavailable for a problem domain. To solve this problem, we apply Genetic Algorithm to search the optimum cost setup of each class. Empirical tests show that the proposed cost-sensitive boosting algorithm improves the classification performances of imbalanced data sets significantly.
[pattern classification, multiple classes imbalance learning, data mining, Drives, Boosting, boosting algorithm, Classification algorithms, genetic algorithms, Data mining, Sun, classifier learning algorithm, genetic algorithm, cost-sensitive boosting algorithm, imbalanced class distribution, data classification, Cost function, Software systems, Iterative algorithms, Software standards, learning (artificial intelligence), Testing]
What is the Dimension of Your Binary Data?
Sixth International Conference on Data Mining
None
2006
Many 0/1 datasets have a very large number of variables; however, they are sparse and the dependency structure of the variables is simpler than the number of variables would suggest. Defining the effective dimensionality of such a dataset is a nontrivial problem. We consider the problem of defining a robust measure of dimension for 0/1 datasets, and show that the basic idea of fractal dimension can be adapted for binary data. However, as such the fractal dimension is difficult to interpret. Hence we introduce the concept of normalized fractal dimension. For a dataset D, its normalized fractal dimension counts the number of independent columns needed to achieve the unnormalized fractal dimension of D. The normalized fractal dimension measures the degree of dependency structure of the data. We study the properties of the normalized fractal dimension and discuss its computation. We give empirical results on the normalized fractal dimension, comparing it against PCA.
[Data analysis, data mining, fractal dimension, Fractals, datasets, Matrix decomposition, Data mining, Computer science, dependency structure, binary data, Robustness, Linear discriminant analysis, Random variables, data handling, principal component analysis, Principal component analysis]
Fast Random Walk with Restart and Its Applications
Sixth International Conference on Data Mining
None
2006
How closely related are two nodes in a graph? How to compute this score quickly, on huge, disk-resident, real graphs? Random walk with restart (RWR) provides a good relevance score between two nodes in a weighted graph, and it has been successfully used in numerous settings, like automatic captioning of images, generalizations to the "connection subgraphs\
[Heart, weighted graph, Costs, connection subgraphs, Error analysis, random walk with restart, graph theory, random processes, matrix inversion, low-rank matrix approximation, Sparse matrices, Delay, Niobium, Design optimization, Linearity, Image storage, Time factors, Sherman-Morrison lemma, graph partitioning]
Anytime Classification Using the Nearest Neighbor Algorithm with Applications to Stream Mining
Sixth International Conference on Data Mining
None
2006
For many real world problems we must perform classification under widely varying amounts of computational resources. For example, if asked to classify an instance taken from a bursty stream, we may have from milliseconds to minutes to return a class prediction. For such problems an anytime algorithm may be especially useful. In this work we show how we can convert the ubiquitous nearest neighbor classifier into an anytime algorithm that can produce an instant classification, or if given the luxury of additional time, can utilize the extra time to increase classification accuracy. We demonstrate the utility of our approach with a comprehensive set of experiments on data from diverse domains.
[Embedded computing, pattern classification, Temperature, nearest neighbor algorithm, data mining, anytime classification, Data mining, Application software, stream mining, Nearest neighbor searches, Manufacturing industries, Computer science, ubiquitous nearest neighbor classifier, Insects, computational resources, Humidity, Frequency, bursty stream]
Dirichlet Aspect Weighting: A Generalized EM Algorithm for Integrating External Data Fields with Semantically Structured Queries by Using Gradient Projection Method
Sixth International Conference on Data Mining
None
2006
In this paper we address the problem of document retrieval with semantically structured queries - queries where each term has a tagged field label. We introduce Dirichlet Aspect Weighting model which integrates terms from external databases into the query language model in a bayesian learning framework. For this model, the Dirichlet prior distribution is governed by parameters which depend on the number of fields in the external databases. This model needs additional examples to be augmented to the semantically structured query. These examples are obtained using pseudo relevance feedback. We formulate a loglikelihood function for the Dirichlet Aspect Weighting model and maximize it using a novel Generalized EM algorithm. Comparison of the results of Dirichlet Aspect Weighting model on TREC 2005 Genomics Track dataset with baseline methods using pseudo relevance feedback, while incorporating terms from external databases shows an improvement.
[TREC 2005 Genomics Track dataset, semantically structured queries, Genetic mutations, Genomics, Data engineering, Data mining, Database languages, query processing, document retrieval, Feedback, query language model, gradient projection method, pseudo relevance feedback, external databases, loglikelihood function, belief networks, Bioinformatics, bayesian learning, generalized EM algorithm, Natural languages, information retrieval, Information retrieval, dirichlet aspect weighting, SQL, Bayesian methods, relevance feedback, expectation-maximisation algorithm, external data fields]
Lazy Associative Classification
Sixth International Conference on Data Mining
None
2006
Decision tree classifiers perform a greedy search for rules by heuristically selecting the most promising features. Such greedy (local) search may discard important rules. Associative classifiers, on the other hand, perform a global search for rules satisfying some quality constraints (i.e., minimum support). This global search, however, may generate a large number of rules. Further, many of these rules may be useless during classification, and worst, important rules may never be mined. Lazy (non-eager) associative classification overcomes this problem by focusing on the features of the given test instance, increasing the chance of generating more rules that are useful for classifying the test instance. In this paper we assess the performance of lazy associative classification. First we demonstrate that an associative classifier performs no worse than the corresponding decision tree classifier. Also we demonstrate that lazy classifiers outperform the corresponding eager ones. Our claims are empirically confirmed by an extensive set of experimental results. We show that our proposed lazy associative classifier is responsible for an error rate reduction of approximately 10% when compared against its eager counterpart, and for a reduction of 20% when compared against a decision tree classifier. A simple caching mechanism makes lazy associative classification fast, and thus improvements in the execution time are also observed.
[pattern classification, lazy associative classification, Error analysis, greedy algorithms, data mining, lazy learning, Predictive models, decision tree classifier, associative rule mining, Data mining, tree searching, caching mechanism, Genetic algorithms, Computer science, Neural networks, Training data, decision trees, greedy search, Decision trees, learning (artificial intelligence), Classification tree analysis, Testing]
Geometrically Inspired Itemset Mining
Sixth International Conference on Data Mining
None
2006
In our geometric view, an itemset is a vector (itemvector) in the space of transactions. Linear and potentially non-linear transformations can be applied to the itemvectors before mining patterns. Aggregation functions and interestingness measures can be applied to the transformed vectors and pushed inside the mining process. We show that interesting itemset mining can be carried out by instantiating four abstract functions: a transformation (g), an algebraic aggregation operator (o) and measures (f and F). For frequent itemset mining (FIM), g and F are identity transformations, o is intersection and f is the cardinality. Based on this geometric view we present a novel algorithm that uses space linear in the number of 1-itemsets to mine all interesting itemsets in a single pass over the data, with no candidate generation. It scales (roughly) linearly in running time with the number of interesting item- sets. FIM experiments show that it outperforms FP-growth on realistic datasets above a small support threshold (0.29% and 1.2% in our experiments).
[Performance evaluation, linear transformations, Australia Council, data mining, Extraterrestrial measurements, Size measurement, Vectors, FIM, Data mining, Association rules, Information technology, geometrically inspired itemset mining, frequent itemset mining, algebraic aggregation operator, nonlinear transformations, vectors, Itemsets, Space technology]
Finding "Who Is Talking to Whom" in VoIP Networks via Progressive Stream Clustering
Sixth International Conference on Data Mining
None
2006
Technologies that use the Internet network to deliver voice communications have the potential to reduce costs and improve access to communications services around the world. However, these new technologies pose several challenges in terms of confidentiality of the conversations and anonymity of the conversing parties. Call authentication and encryption techniques provide a way to protect confidentiality, while anonymity is typically preserved by an anonymizing service (anonymous call). This work studies the feasibility of revealing pairs of anonymous and encrypted conversing parties (caller/callee pair of streams) by exploiting the vulnerabilities inherent to VoIP systems. In particular, by exploiting the aperiodic inter-departure time of VoIP packets, we can trivialize each VoIP stream into a binary time-series. We first define a simple yet intuitive metric to gauge the correlation between two VoIP binary streams. Then we propose an effective technique that progressively pairs conversing parties with high accuracy and in a limited amount of time. Our metric and method are justified analytically and validated by experiments on a very large standard corpus of conversational speech. We obtain impressively high pairing accuracy that reaches 97% after 5 minutes of voice conversations.
[telecommunication security, progressive stream clustering, Speech analysis, Costs, Watermarking, call authentication, conversation confidentiality protection, conversing party encryption, Web and internet services, voice communication, Internet network, Cryptography, IP networks, binary time-series, Routing, cryptography, time series, encryption technique, VoIP binary stream, pattern clustering, Authentication, message authentication, Streaming media, Internet telephony, VoIP network]
Comparison of Descriptor Spaces for Chemical Compound Retrieval and Classification
Sixth International Conference on Data Mining
None
2006
In recent years the development of computational techniques that build models to correctly assign chemical compounds to various classes or to retrieve potential drug-like compounds has been an active area of research. Many of the best-performing techniques for these tasks utilize a descriptor-based representation of the compound that captures various aspects of the underlying molecular graph's topology. In this paper we compare different set of descriptors that are currently used for chemical compound classification. In this process, we also introduce four different descriptors derived from all connected fragments present in the molecular graphs. In addition, we introduce an extension to existing vector-based kernel functions to take into account the length of the fragments present in the descriptors. We experimentally evaluate the performance of the previously introduced and the new descriptors in the context of SVM-based classification and ranked-retrieval on 28 classification and retrieval problems derived from 18 datasets. Our experiments show that for both these tasks, the new descriptors consistently and statistically outperform previously developed schemes based on the widely used fingerprint- and Maces keys-based descriptors, as well as recently introduced descriptors obtained by mining and analyzing the structure of the molecular graphs.
[Drugs, graph theory, Fingerprint recognition, Data mining, Cities and towns, Kernel, chemical compound classification, SVM-based classification, pattern classification, support vector machines, drugs, information retrieval, Information retrieval, vector-based kernel function, Topology, chemical compound retrieval, Chemical compounds, drug-like compound retrieval, Diseases, Computer science, descriptor fragment length, chemistry computing, ranked retrieval, descriptor-based representation, descriptor space comparison, molecular graph topology]
Regularized Least Absolute Deviations Regression and an Efficient Algorithm for Parameter Tuning
Sixth International Conference on Data Mining
None
2006
Linear regression is one of the most important and widely used techniques for data analysis. However, sometimes people are not satisfied with it because of the following two limitations: 1) its results are sensitive to outliers, so when the error terms are not normally distributed, especially when they have heavy-tailed distributions, linear regression often works badly; 2) its estimated coefficients tend to have high variance, although their bias is low. To reduce the influence of outliers, robust regression models were developed. Least absolute deviation (LAD) regression is one of them. LAD minimizes the mean absolute errors, instead of mean squared errors, so its results are more robust. To address the second limitation, shrinkage methods were proposed, which add a penalty on the size of the coefficients. The LASSO is one of these methods and it uses the L1-norm penalty, which not only reduces the prediction error and the variance of estimated coefficients, but also provides an automatic feature selection function. In this paper, we propose the regularized least absolute deviation (RLAD) regression model, which combines the nice features of the LAD and the LASSO together. The RLAD is a regularization method, whose objective function has the form of "loss + penalty." The "loss" is the sum of the absolute deviations and the "penalty" is the L1-norm of the coefficient vector. Furthermore, to facilitate parameter tuning, we develop an efficient algorithm which can solve the entire regularization path in one pass. Simulations with various settings are performed to demonstrate its performance. Finally, we apply the algorithm to solve the image reconstruction problem and find interesting results.
[heavy-tailed distributions, Data analysis, Statistical analysis, data analysis, Linear regression, regression analysis, regularized least absolute deviations regression, L1-norm penalty, Gaussian distribution, linear regression, automatic feature selection function, Vectors, parameter tuning, image reconstruction, Data mining, statistical distributions, Least squares approximation, Image reconstruction, image reconstruction problem, shrinkage methods, feature extraction, LASSO, Noise robustness]
LOCI: Load Shedding through Class-Preserving Data Acquisition
Sixth International Conference on Data Mining
None
2006
An avalanche of data available in the stream form is overstretching our data analyzing ability. In this paper, we propose a novel load shedding method that enables fast and accurate stream data classification. We transform input data so that its class information concentrates on a few features, and we introduce a progressive classifier that makes prediction with partial input. We take advantage of stream data's temporal locality -for example, readings from a temperature sensor usually do not change dramatically over a short period of time -for load shedding. We first show that temporal locality of the original data is preserved by our transform, then we utilize positive and negative knowledge about the data (which is of much smaller size than the data itself) for classification. We employ both analytical and empirical analysis to demonstrate the advantage of our approach.
[Algorithm design and analysis, load shedding, pattern classification, Data analysis, Costs, Event detection, Data acquisition, stream data classification, Data mining, Intelligent sensors, Temperature sensors, Machine learning, class-preserving data acquisition, data acquisition, progressive classifier, temporal data locality, Testing]
SAXually Explicit Images: Finding Unusual Shapes
Sixth International Conference on Data Mining
None
2006
Over the past three decades, there has been a great deal of research on shape analysis, focusing mostly on shape indexing, clustering, and classification. In this work, we introduce the new problem of finding shape discords, the most unusual shapes in a collection. We motivate the problem by considering the utility of shape discords in diverse domains including zoology, anthropology, and medicine. While the brute force search algorithm has quadratic time complexity, we avoid this by using locality-sensitive hashing to estimate similarity between shapes which enables us to reorder the search more efficiently. An extensive experimental evaluation demonstrates that our approach can speed up computation by three to four orders of magnitude.
[pattern classification, anthropology, Shape, image classification, brute force search algorithm, shape analysis, medicine, shape discords, locality-sensitive hashing, saxually explicit images, pattern clustering, shape indexing, quadratic time complexity, zoology, computational complexity]
A Novel Scalable Algorithm for Supervised Subspace Learning
Sixth International Conference on Data Mining
None
2006
Subspace learning approaches aim to discover important statistical distribution on lower dimensions for high dimensional data. Methods such as principal component analysis (PCA) do not make use of the class information, and linear discriminant analysis (LDA) could not be performed efficiently in a scalable way. In this paper, we propose a novel highly scalable supervised subspace learning algorithm called as supervised Kampong measure (SKM). It assigns data points as close as possible to their corresponding class mean, simultaneously assigns data points to be as far as possible from the other class means in the transformed lower dimensional subspace. Theoretical derivation shows that our algorithm is not limited by the number of classes or the singularity problem faced by LDA. Furthermore, our algorithm can be executed in an incremental manner in which learning is done in an online fashion as data streams are received. Experimental results on several datasets, including a very large text data set RCV1, show the outstanding performance of our proposed algorithm on classification problems as compared to PCA, LDA and a popular feature selection approach, information gain (IG).
[Machine learning algorithms, scalable algorithm, supervised Kampong measure, Classification algorithms, statistical distributions, Computational complexity, Computer science, singularity problem, Asia, Clustering algorithms, Statistical distributions, statistical distribution, Machine learning, Linear discriminant analysis, learning (artificial intelligence), linear discriminant analysis, supervised subspace learning, principal component analysis, Principal component analysis]
A Novel Method for Detecting Outlying Subspaces in High-dimensional Databases Using Genetic Algorithm
Sixth International Conference on Data Mining
None
2006
Detecting outlying subspaces is a relatively new research problem in outlier-ness analysis for high-dimensional data. An outlying subspace for a given data point p is the sub- space in which p is an outlier. Outlying subspace detection can facilitate a better characterization process for the detected outliers. It can also enable outlier mining for high- dimensional data to be performed more accurately and efficiently. In this paper, we proposed a new method using genetic algorithm paradigm for searching outlying subspaces efficiently. We developed a technique for efficiently computing the lower and upper bounds of the distance between a given point and its kth nearest neighbor in each possible subspace. These bounds are used to speed up the fitness evaluation of the designed genetic algorithm for outlying subspace detection. We also proposed a random sampling technique to further reduce the computation of the genetic algorithm. The optimal number of sampling data is specified to ensure the accuracy of the result. We show that the proposed method is efficient and effective in handling outlying subspace detection problem by a set of experiments conducted on both synthetic and real-life datasets.
[Algorithm design and analysis, Data analysis, data mining, high-dimensional data mining, subspace detection, random sampling technique, Credit cards, Spatial databases, genetic algorithms, Data mining, Genetic algorithms, Nearest neighbor searches, Computer science, genetic algorithm, Upper bound, high-dimensional databases, Sampling methods, outlierness analysis]
Discovering Unrevealed Properties of Probability Estimation Trees: On Algorithm Selection and Performance Explanation
Sixth International Conference on Data Mining
None
2006
There has been increasing interest to design better probability estimation trees, or PETs, for ranking and probability estimation. Capable of generating class membership probabilities, PETs have been shown to be highly accurate and flexible for many difficult problems, such as cost-sensitive learning and matching skewed distributions. There are a large number of PET algorithms available, and about ten of them are well- known. This large number provides an advantage, but it also creates confusion in practice. One would ask "given a new dataset, which algorithm to choose and what performance to expect and not to expect? What are the reasons to explain either good or bad performance under different situations? " In this paper, we systematically, for the first time, answer these important questions by conducting a large-scale empirical comparison of five popular PETs by examining their AUC, MSE and error rate "learning curves" (instead of training-test split based cross-validation). Using the maximum AUC achieved by any of the evaluated probability estimation tree algorithms, we demonstrate that the preference of a probability estimation tree on different evaluation metrics can be accurately characterized by the "signal-noise separability" of the dataset, as well as some other observable statistics of the dataset explained further in the paper. Moreover, in order to understand their relative performance, many important and previously unrevealed properties of each PET's mechanism and heuristics are analyzed and evaluated. Importantly, a practical guide for choosing the most appropriate PET algorithm given a new data mining problem is provided.
[Error analysis, large-scale empirical comparison, cost-sensitive learning, data mining, probability, trees (mathematics), Probability distribution, Mechanical factors, Parametric statistics, probability estimation trees, Data mining, performance explanation, observable statistics, Large-scale systems, Performance analysis, Decision trees, Positron emission tomography, signal-noise separability, data mining problem, membership probabilities, Classification tree analysis]
Forecasting Skewed Biased Stochastic Ozone Days: Analyses and Solutions
Sixth International Conference on Data Mining
None
2006
Much work on skewed, stochastic, high dimensional, and biased datasets usually implicitly solve each problem separately. Recently, we have been approached by Texas Commission on Environmental Quality (TCEQ) to help them build highly accurate ozone level alarm forecasting models for the Houston area, where these technical difficulties come together in one single problem. Key characteristics of this problem that are challenging and interesting include: 1) the dataset is sparse (72 features, and 2% or 5% positives depending on the criteria of "ozone days"), 2) evolving over time from year to year, 3) limited in collected data size (7 years or around 2500 data entries), 4) contains a large number of irrelevant features, 5) is biased in terms of "sample selection bias\
[Texas Commission on Environmental Quality, Stochastic processes, Geophysical measurements, data mining, Predictive models, Data engineering, Size measurement, Time measurement, sample selection bias, Data mining, Accuracy, environmental science computing, data mining techniques, Quality control, Technology forecasting, forecasting models, skewed biased stochastic ozone days]
Identifying Follow-Correlation Itemset-Pairs
Sixth International Conference on Data Mining
None
2006
An association rule ArarrB is useful to predict that B will likely occur when A occurs. This is a classical association rule. In real world applications, such as bioinformatics and medical research, there are many follow correlations between itemsets A and B: B likely occurs n times after A occurred m times, wrote to &lt;Am, BN&gt;. We refer to this follow-correlation as P3.1 itemset-pairs because &lt;A3, B1&gt; like that in the example ( Example 2) should be uninterested in association analysis. This paper designs an efficient algorithm for identifying P3.1 itemset-pairs in sequential data. We experimentally evaluate our approach, and demonstrate that the proposed approach is efficient and promising.
[Algorithm design and analysis, association rule, Automation, data analysis, data mining, Transaction databases, Association rules, Data mining, Application software, Computer science, Itemsets, follow-correlation itemset-pairs, Marketing and sales, Bioinformatics]
On the Lower Bound of Local Optimums in K-Means Algorithm
Sixth International Conference on Data Mining
None
2006
The k-means algorithm is a popular clustering method used in many different fields of computer science, such as data mining, machine learning and information retrieval. However, the k-means algorithm is very likely to converge to some local optimum which is much worse than the desired global optimal solution. To overcome this problem, current k-means algorithm and its variants usually run many times with different initial centers to avoid being trapped in local optimums that are of unacceptable quality. In this paper, we propose an efficient method to compute a lower bound on the cost of the local optimum from the current center set. After every k-means iteration, k-means algorithm can halt the procedure if the lower bound of the cost at the future local optimum is worse than the best solution that has already been computed so far. Although such a lower bound computation incurs some extra time consumption in the iterations, extensive experiments on both synthetic and real data sets show that this method can greatly prune the unnecessary iterations and improve the efficiency of the algorithm in most of the data sets, especially with high dimensionality and large k.
[iterative methods, Machine learning algorithms, Clustering methods, clustering method, data mining, information retrieval, Information retrieval, Data mining, machine learning, Computer science, k-means iteration, pattern clustering, computer science, Clustering algorithms, Machine learning, Euclidean distance, local optimums, Cost function, Acceleration]
Fast On-line Kernel Learning for Trees
Sixth International Conference on Data Mining
None
2006
Kernel methods have been shown to be very effective for applications requiring the modeling of structured objects. However kernels for structures usually are too computational demanding to be applied to complex learning algorithms, e.g. Support Vector Machines. Consequently, in order to apply kernels to large amount of structured data, we need fast on-line algorithms along with an efficiency optimization of kernel-based computations. In this paper, we optimize this computation by representing set of trees by minimal Direct Acyclic Graphs (DAGs) allowing us i) to reduce the storage requirements and ii) to speed up the evaluation on large number of trees as it can be done 'one-shot' by computing kernels over DAGs. The experiments on predicate argument subtrees from PropBank data show that substantial computational savings can be obtained for the perceptron algorithm.
[complex learning algorithms, support vector machines, minimal direct acyclic graphs, mathematics computing, trees (mathematics), perceptron algorithm, Phylogeny, DAG, Data mining, PropBank data, Support vector machines, Proteins, Tree graphs, directed graphs, XML, kernel methods, Natural language processing, Kernel, Bioinformatics, online kernel learning, structured objects, Classification tree analysis]
bitSPADE: A Lattice-based Sequential Pattern Mining Algorithm Using Bitmap Representation
Sixth International Conference on Data Mining
None
2006
Sequential pattern mining allows to discover temporal relationship between items within a database. The patterns can then be used to generate association rules. When the databases are very large, the execution speed and the memory usage of the mining algorithm become critical parameters. Previous research has focused on either one of the two parameters. In this paper, we present bitSPADE, a novel algorithm that combines the best features of SPAM, one of the fastest algorithm, and SPADE, one of the most memory efficient algorithm. Moreover, we introduce a new pruning strategy that enables bitSPADE to reach high performances. Experimental evaluations showed that bitSPADE ensures an efficient tradeoff between speed and memory usage by outperforming SPADE by both speed and memory usage factors more than 3.4 and SPAM by a memory consumption factor up to more than an order of magnitude.
[TV, lattice-based sequential pattern mining algorithm, bitmap representation, Unsolicited electronic mail, Lattices, data mining, association rules, Spatial databases, memory usage, temporal relationship, Data mining, Association rules, bitSPADE, DVD, Customer satisfaction, very large databases, pruning strategy, Genetics, Frequency, memory consumption factor]
Decision Trees for Functional Variables
Sixth International Conference on Data Mining
None
2006
Classification problems with functionally structured input variables arise naturally in many applications. In a clinical domain, for example, input variables could include a time series of blood pressure measurements. In a financial setting, different time series of stock returns might serve as predictors. In an archaeological application, the 2D profile of an artifact may serve as a key input variable. In such domains, accuracy of the classifier is not the only reasonable goal to strive for; classifiers that provide easily interpretable results are also of value. In this work, we present an intuitive scheme for extending decision trees to handle functional input variables. Our results show that such decision trees are both accurate and readily interpretable.
[pattern classification, functional structured input variable, Input variables, Shape measurement, Application software, Statistics, functional variable, Computer science, classification problem, decision tree, Animals, decision trees, Blood pressure, intuitive scheme, Decision trees, Immune system, Classification tree analysis]
Mining Latent Associations of Objects Using a Typed Mixture Model--A Case Study on Expert/Expertise Mining
Sixth International Conference on Data Mining
None
2006
This paper studies the problem of discovering latent associations among objects in text documents. Specifically, given two sets of objects and various types of co-occurrence data concerning the objects existing in texts, we aim to discover the hidden or latent associative relationships between the two sets of objects. Existing methods are not directly applicable as they are unable to consider all this information. For example, the probabilistic mixture model called Separable Mixture Model (SMM) proposed by Hofmann can use only one type of co-occurrences to mine latent associations. This paper proposes a more general probabilistic mixture model called the Typed Separable Mixture Model (TSMM), which is able to use all types of co-occurrences within a single framework. Experimental results based on the expert/expertise mining task show that TSMM outperforms SMM significantly.
[document handling, probabilistic mixture model, latent associations discovery, co-occurrence data, latent associative relationships, typed mixture model, expert mining, data mining, probability, text documents, Information retrieval, Information filtering, Data mining, expertise mining, typed separable mixture model, Computer science, Asia, Collaboration, Information filters, Natural language processing, latent associations mining]
Semantic Kernels for Text Classification Based on Topological Measures of Feature Similarity
Sixth International Conference on Data Mining
None
2006
In this paper we propose a new approach to the design of semantic smoothing kernels for text classification. These kernels implicitly encode a superconcept expansion in a semantic network using well-known measures of term similarity. The experimental evaluation on two different datasets indicates that our approach consistently improves performance in situations of little training data and data sparseness.
[feature similarity, pattern classification, text analysis, Smoothing methods, Machine learning algorithms, Document handling, topological measures, data sparseness, Knowledge management, text classification, Learning systems, Support vector machines, superconcept expansion, semantic smoothing kernels, Text categorization, Training data, Support vector machine classification, training data, learning (artificial intelligence), Kernel]
Mining Maximal Generalized Frequent Geographic Patterns with Knowledge Constraints
Sixth International Conference on Data Mining
None
2006
In frequent geographic pattern mining a large amount of patterns is well known a priori. This paper presents a novel approach for mining frequent geographic patterns without associations that are previously known as non- interesting. Geographic dependences are eliminated during the frequent set generation using prior knowledge. After the dependence elimination maximal generalized frequent sets are computed to remove redundant frequent sets. Experimental results show a significant reduction of both the number of frequent sets and the computational time for mining maximal frequent geographic patterns.
[frequent set generation, dependence elimination, data mining, knowledge constraints, maximal generalized frequent sets, Spatial databases, Transaction databases, Association rules, Data mining, redundant frequent sets, Pollution, Itemsets, geography, maximal generalized frequent geographic patterns, Computational efficiency, frequent geographic pattern mining]
Pattern Mining in Frequent Dynamic Subgraphs
Sixth International Conference on Data Mining
None
2006
Graph-structured data is becoming increasingly abundant in many application domains. Graph mining aims at finding interesting patterns within this data that represent novel knowledge. While current data mining deals with static graphs that do not change over time, coming years will see the advent of an increasing number of time series of graphs. In this article, we investigate how pattern mining on static graphs can be extended to time series of graphs. In particular, we are considering dynamic graphs with edge insertions and edge deletions over time. We define frequency in this setting and provide algorithmic solutions for finding frequent dynamic subgraph patterns. Existing subgraph mining algorithms can be easily integrated into our framework to make them handle dynamic graphs. Experimental results on real-world data confirm the practical feasibility of our approach.
[frequent dynamic subgraphs, Social network services, pattern mining, graph theory, data mining, Data structures, Graph theory, Pattern recognition, Data mining, Application software, Computer science, Databases, graph-structured data, Frequency, Bioinformatics]
Discovery of Collocation Episodes in Spatiotemporal Data
Sixth International Conference on Data Mining
None
2006
Given a collection of trajectories of moving objects with different types (e.g., pumas, deers, vultures, etc.), we introduce the problem of discovering collocation episodes in them (e.g., if a puma is moving near a deer, then a vulture is also going to move close to the same deer with high probability within the next 3 minutes). Collocation episodes catch the inter-movement regularities among different types of objects. We formally define the problem of mining collocation episodes and propose two scaleable algorithms for its efficient solution. We empirically evaluate the performance of the proposed methods using synthetically generated data that emulate real-world object movements.
[Algorithm design and analysis, real-world object movements, Data analysis, data mining, intermovement regularities, Data mining, spatiotemporal data, moving objects trajectories, Computer science, Animals, Databases, collocation episodes, Computer applications, Humidity, scaleable algorithms, Frequency, Spatiotemporal phenomena]
Getting the Most Out of Ensemble Selection
Sixth International Conference on Data Mining
None
2006
We investigate four previously unexplored aspects of ensemble selection, a procedure for building ensembles of classifiers. First we test whether adjusting model predictions to put them on a canonical scale makes the ensembles more effective. Second, we explore the performance of ensemble selection when different amounts of data are available for ensemble hillclimbing. Third, we quantify the benefit of ensemble selection's ability to optimize to arbitrary metrics. Fourth, we study the performance impact of pruning the number of models available for ensemble selection. Based on our results we present improved ensemble selection methods that double the benefit of the original method.
[arbitrary metrics, pattern classification, Art, model predictions, Optimization methods, Predictive models, Calibration, Wrapping, Computer science, Training data, ensemble selection, Particle measurements, Natural language processing, learning (artificial intelligence), canonical scale, Testing]
Diverse Topic Phrase Extraction through Latent Semantic Analysis
Sixth International Conference on Data Mining
None
2006
We propose a novel algorithm for extracting diverse topic phrases in order to provide summary for large corpora. Previous works often ignore the importance of diversity and thus extract phrases crowded on some hot topics while failing to cover other less obvious but important topics. We solve this problem through document re-weighting and phrase diversification by using latent semantic analysis (LSA). Experiments on various datasets show that our new algorithm can improve relevance as well as diversity over different topics for topic phrase extraction problems.
[Computer science, text analysis, diverse topic phrase extraction, phrase diversification, Asia, Supervised learning, Frequency, Data mining, latent semantic analysis, document re-weighting]
AC-Close: Efficiently Mining Approximate Closed Itemsets by Core Pattern Recovery
Sixth International Conference on Data Mining
None
2006
Recent studies have proposed methods to discover approximate frequent itemsets in the presence of random noise. By relaxing the rigid requirement of exact frequent pattern mining, some interesting patterns, which would previously be fragmented by exact pattern mining methods due to the random noise or measurement error, are successfully recovered. Unfortunately, a large number of "uninteresting" candidates are explored as well during the mining process, as a result of the relaxed pattern mining methodology. This severely slows down the mining process. Even worse, it is hard for an end user to distinguish the recovered interesting patterns from these uninteresting ones. In this paper, we propose an efficient algorithm AC-Close to recover the approximate closed itemsets from "core patterns". By focusing on the so-called core patterns, integrated with a top-down mining and several effective pruning strategies, the algorithm narrows down the search space to those potentially interesting ones. Experimental results show that AC-Close substantially outperforms the previously proposed method in terms of efficiency, while delivers a similar set of interesting recovered patterns.
[Algorithm design and analysis, Measurement errors, Uncertainty, data mining, approximate closed itemset mining, random noise, transaction databases, Transaction databases, AC-Close algorithm, Data mining, core pattern recovery, measurement error, Itemsets, top-down mining, interesting patterns, pruning strategies, uninteresting candidates, search space, search problems, frequent pattern mining]
Belief Propagation in Large, Highly Connected Graphs for 3D Part-Based Object Recognition
Sixth International Conference on Data Mining
None
2006
We describe a part-based object-recognition framework, specialized to mining complex 3D objects from detailed 3D images. Objects are modeled as a collection of parts together with a pairwise potential function. An efficient inference algorithm - based on belief propagation (BP) -finds the optimal layout of parts, given some input image. We introduce AggBP, a message aggregation scheme for BP, in which groups of messages are approximated as a single message. For objects consisting of N parts, we reduce CPU time and memory requirements from O(N2) to O(N). We apply AggBP on synthetic data as well as a real-world task identifying protein fragments in three-dimensional images. These experiments show that our improvements result in minimal loss in accuracy in significantly less time.
[object recognition, Image recognition, belief propagation, graph theory, protein fragments, data mining, Topology, AggBP, Object recognition, Crystallography, 3D object mining, X-ray imaging, Proteins, Graphical models, inference algorithm, connected graphs, message aggregation, part-based object-recognition, three-dimensional images, Inference algorithms, belief networks, Belief propagation, Testing]
A Framework for Regional Association Rule Mining in Spatial Datasets
Sixth International Conference on Data Mining
None
2006
The immense explosion of geographically referenced data calls for efficient discovery of spatial knowledge. One of the special challenges for spatial data mining is that information is usually not uniformly distributed in spatial datasets. Consequently, the discovery of regional knowledge is of fundamental importance for spatial data mining. This paper centers on discovering regional association rules in spatial datasets. In particular, we introduce a novel framework to mine regional association rules relying on a given class structure. A reward-based regional discovery methodology is introduced, and a divisive, grid-based supervised clustering algorithm is presented that identifies interesting subregions in spatial datasets. Then, an integrated approach is discussed to systematically mine regional rules. The proposed framework is evaluated in a real-world case study that identifies spatial risk patterns from arsenic in the Texas water supply.
[Knowledge engineering, Phase measurement, geographically referenced data, data mining, Data engineering, Explosions, Association rules, Data mining, Computer science, regional knowledge discovery, Itemsets, pattern clustering, spatial dataset, reward-based regional discovery, Clustering algorithms, Statistical distributions, geography, grid-based supervised clustering, efficient discovery, spatial knowledge, regional association rule mining, spatial data mining]
Mining Generalized Graph Patterns Based on User Examples
Sixth International Conference on Data Mining
None
2006
There has been a lot of recent interest in mining patterns from graphs. Often, the exact structure of the patterns of interest is not known. This happens, for example, when molecular structures are mined to discover fragments useful as features in chemical compound classification task, or when web sites are mined to discover sets of web pages representing logical documents. Such patterns are often generated from a few small subgraphs (cores), according to certain generalization rules (GRs). We call such patterns "generalized patterns "(GPs). While being structurally different, GPs often perform the same function in the network. Previously proposed approaches to mining GPs either assumed that the cores and the GRs are given, or that all interesting GPs are frequent. These are strong assumptions, which often do not hold in practical applications. In this paper, we propose an approach to mining GPs that is free from the above assumptions. Given a small number of GPs selected by the user, our algorithm discovers all GPs similar to the user examples. First, a machine learning-style approach is used to find the cores. Second, generalizations of the cores in the graph are computed to identify GPs. Evaluation on synthetic data, generated using real cores and GRs from biological and web domains, demonstrates effectiveness of our approach.
[Machine learning algorithms, graph theory, data mining, HTML, logical documents, Data mining, machine learning, Chemical compounds, Chemical technology, Computer science, Proteins, generalization rules, Evolution (biology), Web pages, chemical compound classification task, Biology computing, graph patterns mining, learning (artificial intelligence), patterns structure, Web sites, generalized patterns]
An Experimental Investigation of Graph Kernels on a Collaborative Recommendation Task
Sixth International Conference on Data Mining
None
2006
This work presents a systematic comparison between seven kernels (or similarity matrices) on a graph, namely the exponential diffusion kernel, the Laplacian diffusion kernel, the von Neumann kernel, the regularized Laplacian kernel, the commute time kernel, and finally the Markov diffusion kernel and the cross-entropy diffusion matrix - both introduced in this paper - on a collaborative recommendation task involving a database. The database is viewed as a graph where elements are represented as nodes and relations as links between nodes. From this graph, seven kernels are computed, leading to a set of meaningful proximity measures between nodes, allowing to answer questions about the structure of the graph under investigation; in particular, recommend items to users. Cross- validation results indicate that a simple nearest-neighbours rule based on the similarity measure provided by the regularized Laplacian, the Markov diffusion and the commute time kernels performs best. We therefore recommend the use of the commute time kernel for computing similarities between elements of a database, for two reasons: (1) it has a nice appealing interpretation in terms of random walks and (2) no parameter needs to be adjusted.
[von Neumann kernel, graph theory, data mining, Relational databases, Data mining, proximity measures, Information systems, cross-entropy diffusion matrix, graph kernels, Kernel, Markov diffusion kernel, Laplace equations, Time measurement, exponential diffusion kernel, exponential distribution, commute time kernel, similarity matrices, Collaboration, Laplacian diffusion kernel, Markov processes, Collaborative work, Particle measurements, collaborative recommendation task, regularized Laplacian kernel, Joining processes]
A Balanced Ensemble Approach to Weighting Classifiers for Text Classification
Sixth International Conference on Data Mining
None
2006
This paper studies the problem of constructing an effective heterogeneous ensemble classifier for text classification. One major challenge of this problem is to formulate a good combination function, which combines the decisions of the individual classifiers in the ensemble. We show that the classification performance is affected by three weight components and they should be included in deriving an effective combination function. They are: (1) Global effectiveness, which measures the effectiveness of a member classifier in classifying a set of unseen documents; (2) Local effectiveness, which measures the effectiveness of a member classifier in classifying the particular domain of an unseen document; and (3) Decision confidence, which describes how confident a classifier is when making a decision when classifying a specific unseen document. We propose a new balanced combination function, called dynamic classifier weighting (DCW), that incorporates the aforementioned three components. The empirical study demonstrates that the new combination function is highly effective for text classification.
[text analysis, dynamic classifier weighting, Stacking, Boosting, specific unseen document, text classification, classification, balanced ensemble approach, heterogeneous ensemble classifier, balanced combination function, Support vector machines, Voting, Text categorization, Support vector machine classification, weighting classifiers, Particle measurements, Robustness, Labeling, Bagging]
Detection of Interdomain Routing Anomalies Based on Higher-Order Path Analysis
Sixth International Conference on Data Mining
None
2006
Anomalous interdomain border gateway protocol (BGP) events including misconfigurations, attacks and large-scale power failures often affect the global routing infrastructure. Thus, the ability to detect and categorize such events is extremely useful. In this article we present a novel anomaly detection technique for BGP that distinguishes between different anomalies in BGP traffic. This technique is termed higher order path analysis (HOPA) and focuses on the discovery of patterns in higher order paths in supervised learning datasets. Our results demonstrate that not only worm events but also different types of worms as well as blackout events are cleanly separable and can be classified in real time based on our incremental approach. This novel approach to supervised learning has potential applications in cybersecurity/forensics and text/data mining in general.
[telecommunication security, Event detection, data mining, internetworking, interdomain routing anomaly detection, supervised learning dataset, Data mining, telecommunication computing, Failure analysis, Routing protocols, learning (artificial intelligence), protocols, Pattern analysis, Computer security, data analysis, Forensics, pattern discovery, Surges, higher-order path pattern analysis, anomalous interdomain border gateway protocol, Supervised learning, telecommunication network routing, BGP traffic, Internet, telecommunication traffic]
Star-Structured High-Order Heterogeneous Data Co-clustering Based on Consistent Information Theory
Sixth International Conference on Data Mining
None
2006
Heterogeneous object co-clustering has become an important research topic in data mining. In early years of this research, people mainly worked on two types of heterogeneous data (denoted by pair-wise co-clustering); while recently more and more attention was paid to multiple types of heterogeneous data (denoted by high- order co-clustering). In this paper, we studied the high- order co-clustering of objects with star-structured interrelationship, i.e., there is a central type of objects that connects the other types of objects. Actually, this case could be a very good model for many real-world applications, such as the co-clustering of Web images, their low-level visual features, and the surrounding text. We used a tripartite graph to represent the interrelationships among different objects, and proposed a consistent information theory which generates an effective algorithm to obtain the co-clusters of different types of objects. Experiments on a Web image show that our proposed algorithm is a better choice compared with previous work on heterogeneous object co-clustering.
[graph theory, data mining, Probability distribution, Data mining, Constraint optimization, star-structured high-order heterogeneous data co-clustering, pattern clustering, Asia, Clustering algorithms, Search engines, Constraint theory, Random variables, tripartite graph, Mutual information, consistent information theory, Information theory]
GraphRank: Statistical Modeling and Mining of Significant Subgraphs in the Feature Space
Sixth International Conference on Data Mining
None
2006
We propose a technique for evaluating the statistical significance of frequent subgraphs in a database. A graph is represented by a feature vector that is a histogram over a set of basis elements. The set of basis elements is chosen based on domain knowledge and consists generally of vertices, edges, or small graphs. A given subgraph is transformed to a feature vector and the significance of the subgraph is computed by considering the significance of occurrence of the corresponding vector. The probability of occurrence of the vector in a random vector is computed based on the prior probability of the basis elements. This is then used to obtain a probability distribution on the support of the vector in a database of random vectors. The statistical significance of the vector/subgraph is then defined as the p-value of its observed support. We develop efficient methods for computing p-values and lower bounds. A simplified model is further proposed to improve the efficiency. We also address the problem of feature vector mining, a generalization of item- set mining where counts are associated with items and the goal is to find significant sub-vectors. We present an algorithm that explores closed frequent sub-vectors to find significant ones. Experimental results show that the proposed techniques are effective, efficient, and useful for ranking frequent subgraphs by their statistical significance.
[Chemical analysis, graph theory, data mining, GraphRank, item-set mining, Multimedia databases, random vectors. database, Probability distribution, Data mining, database management systems, statistical distributions, Proteins, Histograms, Itemsets, probability distribution, data structures, frequent subgraphs statistical significance, feature vector mining, significant subgraphs mining, feature space, occurrence probability, Spatial databases, Helium, Computer science, histogram, significant subgraphs modeling]
A Feature Selection and Evaluation Scheme for Computer Virus Detection
Sixth International Conference on Data Mining
None
2006
Anti-virus systems traditionally use signatures to detect malicious executables, but signatures are over-fitted features that are of little use in machine learning. Other more heuristic methods seek to utilize more general features, with some degree of success. In this paper, we present a data mining approach that conducts an exhaustive feature search on a set of computer viruses and strives to obviate over-fitting. We also evaluate the predictive power of a classifier by taking into account dependence relationships that exist between viruses, and we show that our classifier yields high detection rates and can be expected to perform as well in real-world conditions.
[Performance evaluation, pattern classification, Computer viruses, data mining, computer viruses, data mining approach, antivirus system, heuristic method, Data mining, feature selection scheme, machine learning, Information technology, Learning systems, digital signature, feature extraction, computer virus detection, Machine learning, Writing, Feature extraction, classification method, learning (artificial intelligence), Viruses (medical), Testing]
Cluster Analysis of Time-Series Medical Data Based on the Trajectory Representation and Multiscale Comparison Techniques
Sixth International Conference on Data Mining
None
2006
This paper presents a cluster analysis method for multidimensional time-series data on clinical laboratory examinations. Our method represents the time series of test results as trajectories in multidimensional space, and compares their structural similarity by using the multiscale comparison technique. It enables us to find the part-to-part correspondences between two trajectories, taking into account the relationships between different tests. The resultant dissimilarity can be further used with clustering algorithms for finding the groups of similar cases. The method was applied to the cluster analysis of Albumin-Platelet data in the chronic hepatitis dataset. The results denonstrated that it could form interesting groups of cases that have high correspondence to the fibrotic stages.
[trajectory representation, Multidimensional systems, Data analysis, Liver diseases, multiscale comparison technique, Time series analysis, Laboratories, Biomedical informatics, clinical laboratory examination, time series, Convolution, pattern clustering, Clustering algorithms, time-series medical data, cluster analysis, multidimensional space, Kernel, medical computing, Testing]
Constructing Ensembles for Better Ranking
Sixth International Conference on Data Mining
None
2006
We propose a novel algorithm, RankDE, to build an ensemble using an extra artificial dataset. RankDE aims at improving the overall ranking performance, which is crucial in many machine learning applications. This algorithm constructs artificial datasets that are diverse with the current training dataset in terms of ranking. We conduct experiments with real-world data sets to compare RankDE with some traditional and state-of-the-art ensembling algorithms of Bagging, Adaboost, DECORATE and Rankboost in terms of ranking. The experiments show that RankDE outperforms Bagging, DECORATE, Adaboost, and Rankboost when limited data is available. When enough training data is available, it is competitive with DECORATE and Adaboost.
[pattern classification, Machine learning algorithms, Boosting, Data engineering, Data mining, Application software, machine learning, Information technology, artificial dataset, Computer science, ranking performance, Training data, ensembles construction, Machine learning, RankDE, learning (artificial intelligence), Bagging]
TRIAS--An Algorithm for Mining Iceberg Tri-Lattices
Sixth International Conference on Data Mining
None
2006
In this paper, we present the foundations for mining frequent tri-concepts, which extend the notion of closed item-sets to three-dimensional data to allow for mining folk-sonomies. We provide a formal definition of the problem, and present an efficient algorithm for its solution as well as experimental results on a large real-world example.
[Algorithm design and analysis, 3D data, triadic formal concept analysis, data analysis, Lattices, data mining, iceberg tri-lattice mining, Data structures, Data engineering, TRIAS algorithm, Data mining, Association rules, frequent tri-concept mining, Itemsets, Algebra, Clustering algorithms, folksonomy mining, Resource management]
Intelligent Icons: Integrating Lite-Weight Data Mining and Visualization into GUI Operating Systems
Sixth International Conference on Data Mining
None
2006
The vast majority of visualization tools introduced so far are specialized pieces of software that run explicitly on a particular dataset at a particular time for a particular purpose. In this work we introduce a novel framework for allowing visualization to take place in the background of normal day-to-day operation of any GUI based operation system. Our system works by replacing the standard file icons with automatically created icons that reflect the contents of the files in a principled way. We call such icons Intelligent Icons. The utility of Intelligent Icons is further enhanced by arranging them in a way that reflects their similarity/differences. We demonstrate the utility of our approach on diverse applications.
[Sequences, graphical user interfaces, data mining, GUI operating system, Displays, Data mining, intelligent icon, integrating lite-weight data mining, Operating systems, Data visualization, DNA, data visualisation, Frequency, Feature extraction, operating systems (computers), standard file icon, data visualization, Software tools, Graphical user interfaces]
COSMIC: Conceptually Specified Multi-Instance Clusters
Sixth International Conference on Data Mining
None
2006
Recently, more and more applications represent data objects as sets of feature vectors or multi-instance objects. In this paper, we propose COSMIC, a method for deriving concept lattices from multi-instance data based on hierarchical density-based clustering. The found concepts correspond to groups or clusters of multi-instance objects having similar instances in common. We demonstrate that COSMIC outperforms compared methods with respect to efficiency and cluster quality and is capable to extract interesting patterns in multi-instance data sets.
[multiinstance object, formal concept lattice, Shape, data analysis, Lattices, data mining, Displays, Partitioning algorithms, set theory, Data mining, feature vector, Histograms, Content addressable storage, pattern extraction, conceptually specified multiinstance cluster, pattern clustering, Clustering algorithms, COSMIC method, data object representation, Informatics, Kernel, hierarchical density-based clustering]
Direct Marketing When There Are Voluntary Buyers
Sixth International Conference on Data Mining
None
2006
In traditional direct marketing, the implicit assumption is that customers will only purchase the product if they are contacted. In real business environments, however, there are "voluntary buyers, " who will still make the purchase in the absence of a contact. While no direct promotion is needed for voluntary buyers, the traditional response-driven paradigm tends to target such customers. This paper presents "influential marketing, " targeting only those whose purchase decisions can be positively influenced, i.e. buyers who are non-voluntary. Our novel, practical solution to this problem gives promising results.
[influential marketing, Costs, direct marketing, response-driven paradigm, consumer behaviour, business environment, Data mining, Postal services, voluntary buyers, Loans and mortgages, Telephony, learning (artificial intelligence), Business, purchase decisions]
DSTree: A Tree Structure for the Mining of Frequent Sets from Data Streams
Sixth International Conference on Data Mining
None
2006
With advances in technology, a flood of data can be produced in many applications such as sensor networks and Web click streams. This calls for efficient techniques for extracting useful information from streams of data. In this paper, we propose a novel tree structure, called DSTree (Data Stream Tree), that captures important data from the streams. By exploiting its nice properties, the DSTree can be easily maintained and mined for frequent itemsets as well as various other patterns like constrained itemsets.
[Tree data structures, Automation, Area measurement, tree structure, data mining, data streams, Data structures, data stream tree, Data mining, Floods, frequent sets mining, DSTree, Hoses, Itemsets, Fires, Frequency, constrained itemsets, tree data structures, frequent itemsets]
Searching for Pattern Rules
Sixth International Conference on Data Mining
None
2006
We address the problem of finding a set of pattern rules, from a transaction dataset given a statistical metric. A new data structure, called an incrementally counting suffix tree (ICST), is proposed for online computation of estimates of the support of any pattern or itemset. Using an ICST, our approach directly generates a set of pattern rules by a single scan of the whole dataset in partitions without the generation of frequent itemsets. Non-redundant rules can be found by removing redundancies from the pattern rules. The PPMCR algorithm first finds pattern rules and then non-redundant rules by generating valid candidates while traversing the ICST. Experimental results show that the PPMCR algorithm can be used for efficiently mining fewer non-redundant rules.
[Tree data structures, Data analysis, data mining, statistical metric, data structure, incrementally counting suffix tree, Partitioning algorithms, Data mining, Association rules, PPMCR algorithm, Computer science, pattern rules, Itemsets, tree data structures, statistical analysis, Testing]
Adding Semantics to Email Clustering
Sixth International Conference on Data Mining
None
2006
This paper presents a novel algorithm to cluster emails according to their contents and the sentence styles of their subject lines. In our algorithm, natural language processing techniques and frequent itemset mining techniques are utilized to automatically generate meaningful generalized sentence patterns (GSPs) from subjects of emails. Then we put forward a novel unsupervised approach which treats GSPs as pseudo class labels and conduct email clustering in a supervised manner, although no human labeling is involved. Our proposed algorithm is not only expected to improve the clustering performance, it can also provide meaningful descriptions of the resulted clusters by the GSPs. Experimental results on open dataset (Enron email dataset) and a personal email dataset collected by ourselves demonstrate that the proposed algorithm outperforms the K-means algorithm in terms of the popular measurement Fl. Furthermore, the cluster naming readability is improved by 68.5% on the personal email dataset.
[open dataset, natural language processing, Taxonomy, Humans, electronic mail, generalized sentence patterns, Data mining, Seminars, Itemsets, pattern clustering, Asia, Clustering algorithms, Training data, email clustering, Natural language processing, Enron email dataset, Labeling, learning (artificial intelligence), itemset mining techniques]
Gradual Cube: Customize Profile on Mobile OLAP
Sixth International Conference on Data Mining
None
2006
OLAP is supported by more and more environment as a powerful analysis tool. With the rapid development of mobile and wireless technologies, users wish to enjoy the OLAP service on these devices. However, there are many issues on mobile OLAP against the traditional ones, e.g. the transmission bottleneck, unstable network connection, etc. Moreover, the mobile device owners have raised increasing requirements to customize the service such as transmitting the data on demand or ASAP to support their activities. All these challenges provide new chances for OLAP. In this paper, a new mechanism Gradual Cube is proposed to face such challenges. It can reduce the transmission data size, provide customized transmission strategy and enable users to conduct off-line browsing. We assume the users' precision requirement follows some distribution so that three methods, namely random, optimal and heuristic, are developed to customize the transmission plan. The experiments show that such methods are both effective and efficient.
[Decision support systems, wireless technologies, mobile OLAP, Gradual Cube, data mining, OLAP service, Data mining, Information technology, Information analysis, mobile device, Network servers, Histograms, off-line browsing, mobile computing, Aggregates, unstable network connection, transmission bottleneck, Data communication, mobile technologies, Mobile computing]
CoMiner: An Effective Algorithm for Mining Competitors from the Web
Sixth International Conference on Data Mining
None
2006
This paper attempts to accomplish a novel task of mining competitive information with respect to an entity (such as a company, product, person) from the web. An algorithm called "CoMiner" is proposed, which first extracts a set of comparative candidates of the input entity and then ranks them according to the comparability, and finally extracts the competitive fields. The experimental results show that the proposed algorithm drafts a complete picture of competitive relation of a given entity effectively.
[Text mining, data mining, World Wide Web, CoMiner, Data mining, Sun, Computer science, Databases, Asia, Prototypes, Web pages, Search engines, competitor mining, Internet, competitive information, Web search]
Multi-Tier Granule Mining for Representations of Multidimensional Association Rules
Sixth International Conference on Data Mining
None
2006
It is a big challenge to promise the quality of multidimensional association mining. The essential issue is how to represent meaningful multidimensional association rules efficiently. Currently we have not found satisfactory approaches for solving this challenge because of the complicated correlation between attributes. Multi-tier granule mining is an initiative for solving this challenging issue. It divides attributes into some tiers and then compresses the large multidimensional database into granules at each tier. It also builds association mappings to illustrate the correlation between tiers. In this way, the meaningful association rules can be justified according to these association mappings.
[data compression, Multidimensional systems, Costs, data mining, Transaction databases, Data mining, Association rules, multitier granule mining, Itemsets, very large databases, association mapping, knowledge representation, Frequency, Data communication, Australia, multidimensional association rule mining, Software engineering, large multidimensional database compression]
Social Capital in Friendship-Event Networks
Sixth International Conference on Data Mining
None
2006
In this paper, we examine a particular form of social network which we call a friendship-event network. A friendship-event network captures both the friendship relationship among a set of actors, and also the organizer and participation relationships of actors in a series of events. Within these networks, we formulate the notion of social capital based on the actor-organizer friendship relationship and the notion of benefit, based on event participation. We investigate appropriate definitions for the social capital of both a single actor and a collection of actors. We ground these definitions in a real-world example of academic collaboration networks, where the actors are researchers, the friendships are collaborations, the events are conferences, the organizers are program committee members and the participants are conference authors. We show that our definitions of capital and benefit capture interesting qualitative properties of event series. In addition, we show that social capital is a better publication predictor than publication history.
[social capital, actor-organizer friendship, academic collaboration network, Density measurement, Social network services, Buildings, Predictive models, Educational institutions, International collaboration, friendship-event network, History, Data mining, Computer science, Fluid flow measurement, knowledge based systems, groupware, Internet]
Exploratory Under-Sampling for Class-Imbalance Learning
Sixth International Conference on Data Mining
None
2006
Under-sampling is a class-imbalance learning method which uses only a subset of major class examples and thus is very efficient. The main deficiency is that many major class examples are ignored. We propose two algorithms to overcome the deficiency. EasyEnsemble samples several subsets from the major class, trains a learner using each of them, and combines the outputs of those learners. BalanceCascade is similar to EasyEnsemble except that it removes correctly classified major class examples of trained learners from further consideration. Experiments show that both of the proposed algorithms have better AUC scores than many existing class-imbalance learning methods. Moreover, they have approximately the same training time as that of under-sampling, which trains significantly faster than other methods.
[Learning systems, class-imbalance learning, Laboratories, EasyEnsemble, Sampling methods, Educational institutions, BalanceCascade, Data mining, learning (artificial intelligence), exploratory undersampling]
The Influence of Class Imbalance on Cost-Sensitive Learning: An Empirical Study
Sixth International Conference on Data Mining
None
2006
In real-world applications the number of examples in one class may overwhelm the other class, but the primary interest is usually on the minor class. Cost-sensitive learning has been deeded as a good solution to these class-imbalanced tasks, yet it is not clear how does the class-imbalance affect cost-sensitive classifiers. This paper presents an empirical study using 38 data sets, which discloses that class-imbalance often affects the performance of cost-sensitive classifiers: When the misclassification costs are not seriously unequal, cost-sensitive classifiers generally favor natural class distribution although it might be imbalanced; while when misclassification costs are seriously unequal, a balanced class distribution is more favorable.
[pattern classification, cost-sensitive classifier, Design methodology, class imbalance task, Laboratories, cost-sensitive learning, Application software, Medical diagnosis, Data mining, Learning systems, Intrusion detection, Cost function, misclassification cost, natural class distribution, Decision trees, learning (artificial intelligence), Biomedical monitoring]
Similarity of Temporal Query Logs Based on ARIMA Model
Sixth International Conference on Data Mining
None
2006
A challenging issue faced by modern information retrieval is that of determining and satisfying users' requirements relying only on very short text queries. In this paper, we propose an algorithm to find out related queries based on Auto-Regressive Integrated Moving Average (ARIMA) Model. First, we select and estimate ARIMA model of the temporal query logs. And then each query is denoted by a sequence of coefficients. We use the correlation of ARIMA coefficients as the similarity measurement. We call it as the ARIMA Temporal Similarity (ARIMA TS). This similarity describes how strongly two time series are linearly related. On the other hand, the ARIMA model could also be treated as a dimensionality reduction procedure. It can save storage space for a large database of the query logs. In addition, ARIMA model could be used as a tool to predict the trend of a query. The experimental results on two query logs of MSN search engine 1 demonstrate that the proposed approach can achieve better similarity measurement efficiently.
[Content based retrieval, Databases, Asia, Stochastic processes, Euclidean distance, Predictive models, Search engines, Information retrieval, Frequency, Data mining]
Probabilistic Segmentation and Analysis of Horizontal Cells
Sixth International Conference on Data Mining
None
2006
Because images of neurons show interweaved processes from multiple cells, it is hard to determine which pixels belong to each cell, and consequently to analyze the images automatically. To manage these difficulties, we introduce probabilistic segmentation, in which each pixel is assigned a probability of belonging to each cell instead of being categorically assigned to one cell. We propose a randomized algorithm for probabilistic segmentation. The algorithm is based on repeated, intensity-weighted random walks on the image, and leads to improved segmentation quality. Analysis and mining techniques can utilize the more nuanced and complete information that the probabilistic segmentation yields about an image. Such techniques can then compute probabilistic values, which indicate the level of confidence that can be placed in them.
[cellular biophysics, multiple cells, Neurons, probability, Fluorescence, Retina, randomized algorithm, segmentation quality, randomised algorithms, probabilistic segmentation, horizontal cells, Proteins, Image segmentation, Image analysis, biology computing, intensity-weighted random walks, image segmentation, Morphology, Cells (biology), Photoreceptors, interweaved process, Injuries]
Mining Correlation between Motifs and Gene Expression
Sixth International Conference on Data Mining
None
2006
One of the major challenges in the post-genomic era is to determine all DNA-binding transcription factors (TFs) and their regulatory binding sites (motifs) within the genomes. To discover the relationship between the motifs and changes in gene expression, we propose a new algorithm, co-miner (correlation miner). Correlation rules are generated based on the expression profiles of genes with significant expression change through the time course of gene expression. Thus, we may consider the change in gene expression to be causatively associated with the transcription binding sites in the upstream sequences. In addition, we introduce partition and constraint pushing techniques to improve the performance and demonstrate their effectiveness by our experiments. By applying co-miner to a yeast dataset, the relationships between motifs and gene expression revealed by co-miner are confirmed in the literature.
[Sequences, Co-Miner, Genomics, data mining, correlation miner, regulatory binding sites, Partitioning algorithms, Gene expression, Data mining, Association rules, Fungi, genetics, biology computing, DNA, motifs, mining correlation, Bioinformatics, Bonding, DNA-binding transcription factors, gene expression]
High Quality, Efficient Hierarchical Document Clustering Using Closed Interesting Itemsets
Sixth International Conference on Data Mining
None
2006
High dimensionality remains a significant challenge for document clustering. Recent approaches used frequent itemsets and closed frequent itemsets to reduce dimensionality, and to improve the efficiency of hierarchical document clustering. In this paper, we introduce the notion of "closed interesting" itemsets (i.e. closed itemsets with high interestingness). We provide heuristics such as "super item" to efficiently mine these itemsets and show that they provide significant dimensionality reduction over closed frequent itemsets. Using "closed interesting" itemsets, we propose a new, sub-linearly scalable, hierarchical document clustering method that outperforms state of the art agglomerative, partitioning and frequent-itemset based methods both in terms of clustering quality and runtime performance, without requiring dataset specific parameter tuning. We evaluate twenty interestingness measures and show that when used to generate "closed interesting" itemsets, and to select parent nodes, mutual information, added value, Yule's Q and Chi- Square offer best clustering performance.
[document handling, interestingness measure, Scalability, Clustering methods, Merging, closed frequent itemsets, data mining, Data mining, Association rules, closed interesting itemsets, Computer science, runtime performance, clustering quality, added value, Runtime, dimensionality reduction, Itemsets, pattern clustering, itemset mining, Clustering algorithms, Frequency, mutual information, hierarchical document clustering, parent nodes]
On Trajectory Representation for Scientific Features
Sixth International Conference on Data Mining
None
2006
In this article, we present trajectory representation algorithms for tangible features found in temporally varying scientific datasets. Rather than modeling the features as points, we take attributes like shape and extent of the feature into account. Our contention is that these attributes play an important role in understanding the temporal evolution and interactions among features. The proposed representation scheme is based on motion and shape parameters including linear velocity, angular velocity, etc. We use these parameters to segment the trajectory instead of relying on the geometry of the trajectory. We evaluate our algorithms on real datasets originating from different domains. We show the accuracy of the motion and shape parameter estimation by reconstructing the trajectories with high accuracy. Finally, we present performance and scalability results.
[trajectory representation algorithms, Parameter estimation, Shape, shape parameters, Scalability, Data engineering, Data mining, image motion analysis, Computer science, Geometry, Databases, feature extraction, image representation, Angular velocity, geometry, scientific features, Motion analysis, motion parameters]
NewsCATS: A News Categorization and Trading System
Sixth International Conference on Data Mining
None
2006
NewsCATS is an automated text categorization (ATC) prototype using a hand-made thesaurus to forecast intraday stock price trends from information contained in press releases. Due to a unique labeling approach and by carefully selecting the appropriate training data NewsCATS achieves a performance which is clearly superior to other ATC prototypes used for stock price trend forecasting. In this paper we describe the architecture, training, and testing of NewsCATS as well as the results of an extensive robustness analysis.
[text analysis, thesauri, NewsCATS, Vectors, Thesauri, hand-made thesaurus, news categorization, Engines, trading system, automated text categorization, Text categorization, Prototypes, Training data, Frequency, Economic forecasting, intraday stock price forecast, Labeling, stock markets, pricing, Testing]
Improving Grouped-Entity Resolution Using Quasi-Cliques
Sixth International Conference on Data Mining
None
2006
The entity resolution (ER) problem, which identifies duplicate entities that refer to the same real world entity, is essential in many applications. In this paper, in particular, we focus on resolving entities that contain a group of related elements in them (e.g., an author entity with a list of citations, a singer entity with song list, or an intermediate result by GROUP BY SQL query). Such entities, named as grouped-entities, frequently occur in many applications. The previous approaches toward grouped-entity resolution often rely on textual similarity, and produce a large number of false positives. As a complementing technique, in this paper, we present our experience of applying a recently proposed graph mining technique, Quasi-Clique, atop conventional ER solutions. Our approach exploits contextual information mined from the group of elements per entity in addition to syntactic similarity. Extensive experiments verify that our proposal improves precision and recall up to 83% when used together with a variety of existing ER solutions, but never worsens them.
[Technological innovation, text analysis, grouped-entity resolution, SQL query, data mining, Data structures, Proposals, Data mining, Degradation, Software libraries, Computer errors, Motion pictures, Large-scale systems, quasi-cliques, textual similarity, Erbium, graph mining technique]
Fast Relevance Discovery in Time Series
Sixth International Conference on Data Mining
None
2006
In this paper, we propose to model time series from a new angle: state transition points. When fluctuation of values in a time series crosses a certain point, it may trigger state transition in the system, which may lead to abrupt changes in many other time series. The concept of state transition points is essential in understanding the behavior of the time series and the behavior of the system. The new measure is robust and is capable of discovering correlations that Pearson's coefficient cannot reveal. We propose efficient algorithms to identify state transition points and to compute correlation between two time series. We also introduce some triangular inequalities to efficiently find highly correlated time series among many time series.
[Fluctuations, data analysis, fast relevance discovery, Time series analysis, Scattering, Bifurcation, state transition points, time series, Pearson coefficient, Time measurement, Application software, Condition monitoring, triangular inequalities, Robustness, binary sequences, Mutual information, Binary sequences]
Probabilistic Enhanced Mapping with the Generative Tabular Model
Sixth International Conference on Data Mining
None
2006
Visualization of the massive datasets needs new methods which are able to quickly and easily reveal their contents. The projection of the data cloud is an interesting paradigm in spite of its difficulty to be explored when data plots are too numerous. So we study a new way to show a bidimensional projection from a multidimensional data cloud: our generative model constructs a tabular view of the projected cloud. We are able to show the high densities areas by their non equidistributed discretization. This approach is an alternative to the self-organizing map when a projection does already exist. The resulting pixel views of a dataset are illustrated by projecting a data sample of real images: it becomes possible to observe how are laid out the class labels or the frequencies of a group of modalities without being lost because of a zoom enlarging change for instance. The conclusion gives perspectives to this original promising point of view to get a readable projection for a statistical data analysis of large data samples.
[Multidimensional systems, Data analysis, Shape, data analysis, Clouds, probabilistic enhanced mapping, probability, massive dataset visualization, bidimensional data cloud projection, Topology, generative tabular model, Self organizing feature maps, Clustering algorithms, Data visualization, data visualisation, multidimensional data cloud, Frequency, statistical data analysis, statistical analysis, Pixel]
Object Identification with Constraints
Sixth International Conference on Data Mining
None
2006
Object identification aims at identifying different representations of the same object based on noisy attributes such as descriptions of the same product in different online shops or references to the same paper in different publications. Numerous solutions have been proposed for solving this task, almost all of them based on similarity functions of a pair of objects. Although today the similarity functions are learned from a set of labeled training data, the structural information given by the labeled data is not used. By formulating a generic model for object identification we show how almost any proposed identification model can easily be extended for satisfying structural constraints. Therefore we propose a model that uses structural information given as pairwise constraints to guide collective decisions about object identification in addition to a learned similarity measure. We show with empirical experiments on public and on real-life data that combining both structural information and attribute-based similarity enormously increases the overall performance for object identification tasks.
[Merging, object-oriented databases, Predictive models, Data mining, similarity functions, Computer science, Couplings, database, Databases, pattern clustering, Training data, Object detection, structural information, object identification, Manufacturing, semi-supervised clustering, Testing]
High-Performance Unsupervised Relation Extraction from Large Corpora
Sixth International Conference on Data Mining
None
2006
We present URIES - an unsupervised relation identification and extraction system. The system automatically identifies interesting binary relations between entities in the input corpus, and then proceeds to extract a large number of instances of these relations. The system discovers relations by clustering frequently co- occuring pairs of entities, based on the contexts in which they appear. Its complex pattern-based representation of the contexts allows the clustering step to achieve very high precision, sufficient for the clusters to perform as sets of seeds for bootstrapping a high-recall relation extraction process. In a series of experiments we demonstrate the successful performance of URIES and compare it to the two existing systems - a weakly supervised high-recall Web relation extraction system called SRES, and an unsupervised relation identification system that uses a simpler bag-ofwords representation of contexts. The experiments show that URIES performs comparably to SRES, but without any supervision, and that such performance is due to the power of its complex contexts representation and to its novel candidate selection method.
[Knowledge engineering, Web relation extraction system, Humans, knowledge acquisition, Data mining, Gallium nitride, Relays, unsupervised learning, unsupervised relation identification, Strontium, bag-of-words representation, Machine learning, unsupervised relation extraction, Internet, pattern-based representation]
Cluster Based Core Vector Machine
Sixth International Conference on Data Mining
None
2006
Core vector machine(CVM) is suitable for efficient large-scale pattern classification. In this paper, a method for improving the performance of CVM with Gaussian kernel function irrespective of the orderings of patterns belonging to different classes within the data set is proposed. This method employs a selective sampling based training of CVM using a novel kernel based scalable hierarchical clustering algorithm. Empirical studies made on synthetic and real world data sets show that the proposed strategy performs well on large data sets.
[pattern classification, Automation, support vector machines, kernel based scalable hierarchical clustering algorithm, cluster based core vector machine, Data mining, Gaussian kernel function, Support vector machines, Computer science, pattern clustering, Clustering algorithms, Training data, Pattern classification, Gaussian processes, Sampling methods, selective sampling based training, Large-scale systems, Kernel]
Enhancing Text Clustering Using Concept-based Mining Model
Sixth International Conference on Data Mining
None
2006
Most of text mining techniques are based on word and/or phrase analysis of the text. The statistical analysis of a term (word or phrase) frequency captures the importance of the term within a document. However, to achieve a more accurate analysis, the underlying mining technique should indicate terms that capture the semantics of the text from which the importance of a term in a sentence and in the document can be derived. A new concept-based mining model that relies on the analysis of both the sentence and the document, rather than, the traditional analysis of the document dataset only is introduced. The proposed mining model consists of a concept-based analysis of terms and a concept-based similarity measure. The term which contributes to the sentence semantics is analyzed with respect to its importance at the sentence and document levels. The model can efficiently find significant matching terms, either words or phrases, of the documents according to the semantics of the text. The similarity between documents relies on a new concept-based similarity measure which is applied to the matching terms between documents. Experiments using the proposed concept-based term analysis and similarity measure in text clustering are conducted. Experimental results demonstrate that the newly developed concept-based mining model enhances the clustering quality of sets of documents substantially.
[text analysis, Text analysis, pattern matching, Clustering methods, text clustering, document dataset analysis, computational linguistics, sentence semantics, Humans, data mining, word analysis, Entropy, Frequency measurement, concept-based mining model, Data mining, phrase analysis, Natural language processing, Text mining, Statistical analysis, term matching, natural language processing, Unsupervised learning, pattern clustering, concept-based similarity measure, statistical analysis]
Detecting Link Spam Using Temporal Information
Sixth International Conference on Data Mining
None
2006
How to effectively protect against spam on search ranking results is an important issue for contemporary web search engines. This paper addresses the problem of combating one major type of web spam: 'link spam.' Most of the previous work on anti link spam managed to make use of one snapshot of web data to detect spam, and thus it did not take advantage of the fact that link spam tends to result in drastic changes of links in a short time period. To overcome the shortcoming, this paper proposes using temporal information on links in detection of link spam, as well as other information. Specifically, it defines temporal features such as in-link growth rate (IGR) and in-link death rate (IDR) in a spam classification model (i.e., SVM). Experimental results on web domain graph data show that link spam can be successfully detected with the proposed method.
[search engines, temporal information, Unsolicited electronic mail, link spam detection, Data engineering, unsolicited e-mail, Data mining, in-link death rate, Support vector machines, Asia, contemporary Web search engines, IGR, Support vector machine classification, Web pages, in-link growth rate, IDR, Search engines, Robustness, Web spam, Web search]
Minimum Enclosing Spheres Formulations for Support Vector Ordinal Regression
Sixth International Conference on Data Mining
None
2006
We present two new support vector approaches for ordinal regression. These approaches find the concentric spheres with minimum volume that contain most of the training samples. Both approaches guarantee that the radii of the spheres are properly ordered at the optimal solution. The size of the optimization problem is linear in the number of training samples. The popular SMO algorithm is adapted to solve the resulting optimization problem. Numerical experiments on some real-world data sets verify the usefulness of our approaches for data mining.
[optimization problem, Automation, support vector machines, data mining, regression analysis, Information retrieval, Vectors, Data mining, Computer science, Learning systems, optimisation, Supervised learning, Training data, minimum enclosing spheres, Hilbert space, learning (artificial intelligence), Kernel, support vector ordinal regression]
Mining Maximal Quasi-Bicliques to Co-Cluster Stocks and Financial Ratios for Value Investment
Sixth International Conference on Data Mining
None
2006
We introduce an unsupervised process to co-cluster groups of stocks and financial ratios, so that investors can gain more insight on how they are correlated. Our idea for the co-clustering is based on a graph concept called maximal quasi-bicliques, which can tolerate erroneous or/and missing information that are common in the stock and financial ratio data. Compared to previous works, our maximal quasi-bicliques require the errors to be evenly distributed, which enable us to capture more meaningful co-clusters. We develop a new algorithm that can efficiently enumerate maximal quasi-bicliques from an undirected graph. The concept of maximal quasi-bicliques is domain-independent; it can be extended to perform co-clustering on any set of data that are modeled by graphs.
[data mining, investment, financial ratio, Data mining, stocks, stock ratio, co-clustering, Kelvin, Investments, directed graphs, maximal quasi-biclique mining, Bipartite graph, value investment, stock markets, undirected graph]
Boosting the Feature Space: Text Classification for Unstructured Data on the Web
Sixth International Conference on Data Mining
None
2006
The issue of seeking efficient and effective methods for classifying unstructured text in large document corpora has received much attention in recent years. Traditional document representation like bag-of-words encodes documents as feature vectors, which usually leads to sparse feature spaces with large dimensionality, thus making it hard to achieve high classification accuracies. This paper addresses the problem of classifying unstructured documents on the Web. A classification approach is proposed that utilizes traditional feature reduction techniques along with a collaborative filtering method for augmenting document feature spaces. The method produces feature spaces with an order of magnitude less features compared with a baseline bag-of-words feature selection method. Experiments on both real-world data and benchmark corpus indicate that our approach improves classification accuracy over the traditional methods for both support vector machines and AdaBoost classifiers.
[text analysis, Filtering, unstructured text classification, document corpora, Boosting, information filtering, Data mining, classification, feature reduction technique, Support vector machines, support vector machine, AdaBoost classifier, Space technology, Text categorization, Neural networks, feature extraction, bag-of-words feature selection method, Web, Support vector machine classification, Collaboration, Bismuth, Internet, document feature space augmentation, collaborative filtering method]
Plagiarism Detection in arXiv
Sixth International Conference on Data Mining
None
2006
We describe a large-scale application of methods for finding plagiarism in research document collections. The methods are applied to a collection of 284,834 documents collected by arXiv.org over a 14 year period, covering a few different research disciplines. The methodology efficiently detects a variety of problematic author behaviors, and heuristics are developed to reduce the number of false positives. The methods are also efficient enough to implement as a real-time submission screen for a collection many times larger.
[text analysis, Sequences, Displays, Application software, History, Computer science, research document collections, Information science, Plagiarism, Physics computing, research and development, arXiv, plagiarism detection, Large-scale systems, problematic author behaviors, Testing]
Window-based Tensor Analysis on High-dimensional and Multi-aspect Streams
Sixth International Conference on Data Mining
None
2006
Data stream values are often associated with multiple aspects. For example, each value from environmental sensors may have an associated type (e.g., temperature, humidity, etc) as well as location. Aside from timestamp, type and location are the two additional aspects. How to model such streams? How to simultaneously find patterns within and across the multiple aspects? How to do it incrementally in a streaming fashion? In this paper, all these problems are addressed through a general data model, tensor streams, and an effective algorithmic framework, window-based tensor analysis (WTA). Two variations of WTA, independent- window tensor analysis (IW) and moving-window tensor analysis (MW), are presented and evaluated extensively on real datasets. Finally, we illustrate one important application, multi-aspect correlation analysis (MACA), which uses WTA and we demonstrate its effectiveness on an environmental monitoring application.
[Algorithm design and analysis, window-based tensor analysis, data mining, multi-aspect correlation analysis, Data mining, Temperature sensors, Tensile stress, multi-aspect streams, environmental science computing, Humidity, high-dimensional streams, Data models, Computational efficiency, Pattern analysis, Monitoring, environmental monitoring application]
Automatic Single-Organ Segmentation in Computed Tomography Images
Sixth International Conference on Data Mining
None
2006
In this paper, we propose a hybrid approach for automatic single-organ segmentation in computed tomography (CT) data. The approach consists of three stages: first, a probability image of the organ of interest is obtained by applying a binary classification model obtained using pixel-based texture features; second, an adaptive split-and-merge segmentation algorithm is applied on the organ probability image to remove the noise introduced by the misclassified pixels; and third, the segmented organ's boundaries from the previous stage are iteratively refined using a region growing algorithm. While we applied our approach for liver segmentation in 2-D CT images, a challenging and important task in many medical applications, the proposed approach can be applied for the segmentation of any other organ in CT images. Moreover, the proposed approach can be extended to perform automatic multiple organ segmentation and to build context-sensitive reporting tools for computer-aided diagnosis applications.
[binary classification model, Anatomical structure, Application software, computed tomography images, Active contours, pixel-based texture features, Image segmentation, probability image, adaptive split-and-merge segmentation algorithm, Image analysis, computerised tomography, Computed tomography, image segmentation, region growing algorithm, Iterative algorithms, automatic single-organ segmentation, Pixel, Medical diagnostic imaging, image resolution, medical image processing, Biomedical imaging]
Improving Nearest Neighbor Classifier Using Tabu Search and Ensemble Distance Metrics
Sixth International Conference on Data Mining
None
2006
The nearest-neighbor (NN) classifier has long been used in pattern recognition, exploratory data analysis, and data mining problems. A vital consideration in obtaining good results with this technique is the choice of distance function, and correspondingly which features to consider when computing distances between samples. In this paper, a new ensemble technique is proposed to improve the performance of NN classifier. The proposed approach combines multiple NN classifiers, where each classifier uses a different distance function and potentially a different set of features (feature vector). These feature vectors are determined for each distance metric using Simple Voting Scheme incorporated in Tabu Search (TS). The proposed ensemble classifier with different distance metrics and different feature vectors (TS-DF/NN) is evaluated using various benchmark data sets from UCI Machine Learning Repository. Results have indicated a significant increase in the performance when compared with various well-known classifiers. Furthermore, the proposed ensemble method is also compared with ensemble classifier using different distance metrics but with same feature vector (with or without Feature Selection (FS)).
[Algorithm design and analysis, Costs, data mining, Data mining, Voting, UCI Machine Learning Repository, feature extraction, ensemble distance metrics, tabu search, search problems, feature selection, pattern recognition, pattern classification, Data analysis, distance function, data analysis, nearest neighbor classifier, Pattern recognition, Nearest neighbor searches, feature vector, Computer science, exploratory data analysis, Neural networks, Machine learning, simple voting scheme]
Comparisons of K-Anonymization and Randomization Schemes under Linking Attacks
Sixth International Conference on Data Mining
None
2006
Recently K-anonymity has gained popularity as a privacy quantification against linking attacks, in which attackers try to identify a record with values of some identifying attributes. If attacks succeed, the identity of the record will be revealed and potential confidential information contained in other attributes of the record will be disclosed. K-anonymity counters this attack by requiring that each record must be indistinguishable from at least K-1 other records with respect to the identifying attributes. Randomization can also be used for protection against linking attacks. In this paper, we compare the performance of K-anonymization and randomization schemes under linking attacks. We present a new privacy definition that can be applied to both k-anonymization and randomization. We compare these two schemes in terms of both utility and risks of privacy disclosure, and we promote to use R-U confidentiality map for such comparisons. We also compare various randomization schemes.
[Data privacy, R-U confidentiality map, privacy disclosure, random processes, privacy quantification, Data mining, Association rules, Information technology, linking attacks, Counting circuits, K-anonymity, Databases, Aggregates, data privacy, K-anonymization schemes, Decision trees, Joining processes, Protection, randomization schemes]
MARGIN: Maximal Frequent Subgraph Mining
Sixth International Conference on Data Mining
None
2006
The exponential number of possible subgraphs makes the problem of frequent subgraph mining a challenge. The set of maximal frequent subgraphs is much smaller to that of the set of frequent subgraphs, thus providing ample scope for pruning. MARGIN is a maximal subgraph mining algorithm that moves among promising nodes of the search space along the "border" of the infrequent and frequent subgraphs. This drastically reduces the number of candidate patterns considered in the search space. Experimental results validate the efficiency and utility of the technique proposed.
[maximal frequent subgraph mining, graph theory, Lattices, data mining, Data engineering, Data mining, MARGIN, Databases, Web pages, Computer applications, Writing, Space exploration, search space, search problems]
Resource Management for Networked Classifiers in Distributed Stream Mining Systems
Sixth International Conference on Data Mining
None
2006
Networks of classifiers are capturing the attention of system and algorithmic researchers because they offer improved accuracy over single model classifiers, can be distributed over a network of servers for improved scalability, and can be adapted to available system resources. This work provides a principled approach for the optimized allocation of system resources across a networked chain of classifiers. We begin with an illustrative example of how complex classification tasks can be decomposed into a network of binary classifiers. We formally define a global performance metric by recursively collapsing the chain of classifiers into one combined classifier. The performance metric trades off the end-to-end probabilities of detection and false alarm, both of which depend on the resources allocated to each individual classifier. We formulate the optimization problem and present optimal resource allocation results for both simulated and state-of-the-art classifier chains operating on telephony data.
[Measurement, resource management, recursive classifier chain collapse, pattern classification, Scalability, data mining, probability, Quality of service, system resource allocation optimization, Topology, Data mining, distributed stream mining system, Telegraphy, Network servers, optimisation, resource allocation, distributed databases, Streaming media, Telephony, detection probability, Resource management, networked classifier chain]
A Simple Yet Effective Data Clustering Algorithm
Sixth International Conference on Data Mining
None
2006
In this paper, we use a simple concept based on k-reverse nearest neighbor digraphs, to develop a framework RECORD for clustering and outlier detection. We developed three algorithms - (i) RECORD algorithm (requires one parameter), (ii) Agglomerative RECORD algorithm (no parameters required) and (iii) Stability-based RECORD algorithm (no parameters required). Our experimental results with published datasets, synthetic and real-life datasets show that RECORD not only handles noisy data, but also identifies the relevant clusters. Our results are as good as (if not better than) the results got from other algorithms.
[Stability, Merging, synthetic dataset, data clustering, Data engineering, Noise shaping, Partitioning algorithms, stability-based RECORD algorithm, Data mining, outlier detection, Nearest neighbor searches, pattern clustering, directed graphs, Clustering algorithms, real-life dataset, data handling, agglomerative RECORD algorithm, Optical sensors, Detection algorithms, k-reverse nearest neighbor digraph]
Entropy-based Concept Shift Detection
Sixth International Conference on Data Mining
None
2006
When monitoring sensory data (e.g., from a wearable device) the context oftentimes changes abruptly: people move from one situation (e.g., working quietly in their office) to another (e.g., being interrupted by one's manager). These context changes can be treated like concept shifts, since the underlying data generator (the concept) changes while moving from one context situation to another. We present an entropy based measure for data streams that is suitable to detect concept shifts in a reliable, noise-resistant, fast, and computationally efficient way. We assess the entropy measure under different concept shift conditions. To support our claims we illustrate the concept shift behavior of the stream entropy. We also present a simple algorithm control approach to show how useful and reliable the information obtained by the entropy measure is compared to a ensemble learner as well as an experimentally inferred upper limit. Our analysis is based on three large synthetic data sets representing real, virtual, and a combination of both concept drifts under different noise conditions (up to 50%). Last but not least, we demonstrate the usefulness of the entropy based measure context switch indication in a real world application in the context-awareness/wearable computing domain.
[entropy-based concept shift detection, data analysis, data streams, Switches, Multimedia computing, Entropy, Noise measurement, Data mining, ubiquitous computing, context switch indication, stream entropy behavior, Wearable computers, Streaming media, Biomedical monitoring, Informatics, Wearable sensors]
Recommendation on Item Graphs
Sixth International Conference on Data Mining
None
2006
A novel scheme for item-based recommendation is proposed in this paper. In our framework, the items are described by an undirected weighted graph Q = (V,epsiv). V is the node set which is identical to the item set, and epsiv is the edge set. Associate with each edge eij isin epsiv is a weight omegaij ges 0, which represents similarity between items i and j. Without the loss of generality, we assume that any user's ratings to the items should be sufficiently smooth with respect to the intrinsic structure of the items, i.e., a user should give similar ratings to similar items. A simple algorithm is presented to achieve such a smooth solution. Encouraging experimental results are provided to show the effectiveness of our method.
[Automation, Demography, smooth solution, graph theory, information filtering, Information filtering, Data mining, Computer science, item-based recommendation, Collaboration, Motion pictures, Explosives, undirected weighted graph, Books, Recommender systems]
Solution Path for Semi-Supervised Classification with Manifold Regularization
Sixth International Conference on Data Mining
None
2006
With very low extra computational cost, the entire solution path can be computed for various learning algorithms like support vector classification (SVC) and support vector regression (SVR). In this paper, we extend this promising approach to semi-supervised learning algorithms. In particular, we consider finding the solution path for the Laplacian support vector machine (LapSVM) which is a semi-supervised classification model based on manifold regularization. One advantage of the this algorithm is that the coefficient path is piecewise linear with respect to the regularization parameter, hence its computational complexity is quadratic in the number of labeled examples.
[various learning algorithms, SVC, regression analysis, SVR, Manifolds, piecewise linear, Laplacian support vector machine, LapSVM, Static VAr compensators, Computational efficiency, learning (artificial intelligence), Kernel, pattern classification, Laplace equations, Piecewise linear techniques, support vector machines, support vector classification, Support vector machines, support vector regression, manifold regularization, Supervised learning, Support vector machine classification, Semisupervised learning, semi-supervised classification, computational complexity]
Semi-Supervised Kernel Regression
Sixth International Conference on Data Mining
None
2006
Insufficiency of training data is a major obstacle in machine learning and data mining applications. Many different semi-supervised learning algorithms have been proposed to tackle this difficulty by leveraging a large amount of unlabeled data. However, most of them focus on semi-supervised classification. In this paper we propose a semi-supervised regression algorithm named semi-supervised kernel regression (SSKR). While classical kernel regression is only based on labeled examples, our approach extends it to all observed examples using a weighting factor to modulate the effect of unlabeled examples. Experimental results prove that SSKR significantly outperforms traditional kernel regression and graph-based semi-supervised regression methods.
[pattern classification, Machine learning algorithms, semisupervised learning, Humans, data mining, regression analysis, Data mining, machine learning, semisupervised classification, semisupervised kernel regression, Asia, Supervised learning, Clustering algorithms, Training data, Machine learning, training data, Semisupervised learning, learning (artificial intelligence), Kernel]
Mining Complex Time-Series Data by Learning Markovian Models
Sixth International Conference on Data Mining
None
2006
In this paper, we propose a novel and general approach for time-series data mining. As an alternative to traditional ways of designing specific algorithm to mine certain kind of pattern directly from the data, our approach extracts the temporal structure of the time-series data by learning Markovian models, and then uses well established methods to efficiently mine a wide variety of patterns from the topology graph of the learned models. We consolidate the approach by explaining the use of some well-known Markovian models on mining several kinds of patterns. We then present a novel high-order hidden Markov model, the variable-length hidden Markov model (VLHMM), which combines the advantages of well- known Markovian models and has the superiority in both efficiency and accuracy. Therefore, it can mine a much wider variety of patterns than each of prior Markovian models. We demonstrate the power of VLHMM by mining four kinds of interesting patterns from 3D motion capture data, which is typical for the high-dimensionality and complex dynamics.
[Algorithm design and analysis, graph topology, Uncertainty, pattern mining, Statistical learning, graph theory, data mining, temporal structure, time series, Graph theory, Topology, Data mining, high-order hidden Markov model, Computer science, hidden Markov models, Viterbi algorithm, Hidden Markov models, complex time-series data mining, variable-length hidden Markov model, learning Markovian models, Periodic structures]
Temporal Data Mining in Dynamic Feature Spaces
Sixth International Conference on Data Mining
None
2006
Many interesting real-world applications for temporal data mining are hindered by concept drift. One particular form of concept drift is characterized by changes to the underlying feature space. Seemingly little has been done in this area. This paper presents FAE, an incremental ensemble approach to mining data subject to such concept drift. Empirical results on large data streams demonstrate promise.
[data mining, Predictive models, feature adaptive ensemble, Data mining, Application software, Niobium, dynamic feature space, Computer science, Degradation, incremental ensemble approach, concept drift, feature extraction, Cities and towns, Marketing and sales, Decision trees, learning (artificial intelligence), temporal data mining, Testing]
Discover Bayesian Networks from Incomplete Data Using a Hybrid Evolutionary Algorithm
Sixth International Conference on Data Mining
None
2006
This paper proposes a novel hybrid approach for learning Bayesian networks from incomplete data in the presence of missing values, which combines an evolutionary algorithm with the traditional expectation-maximization (EM) algorithm. The new algorithm can overcome the problem of getting stuck in sub-optimal solutions which occurs in most existing learning algorithms. The experimental results on the data sets generated from several benchmark networks illustrate that the new algorithm has better performance than some state-of-the-art algorithms. We also apply the approach to a data set of direct marketing and compare the performance of the discovered Bayesian networks obtained by the new algorithm with the networks generated by other methods. In the comparison, the Bayesian networks learned by the new algorithm outperform other networks.
[hybrid evolutionary algorithm, expectation-maximization algorithm, Evolutionary computation, direct marketing, Probability distribution, learning Bayesian networks, Data mining, Statistics, evolutionary computation, Bayesian methods, discover Bayesian networks, expectation-maximisation algorithm, Sampling methods, Computer networks, suboptimal solutions, Random variables, belief networks, learning (artificial intelligence), incomplete data]
Distances and (Indefinite) Kernels for Sets of Objects
Sixth International Conference on Data Mining
None
2006
The main disadvantage of most existing set kernels is that they are based on averaging, which might be inappropriate for problems where only specific elements of the two sets should determine the overall similarity. In this paper we propose a class of kernels for sets of vectors directly exploiting set distance measures and, hence, incorporating various semantics into set kernels and lending the power of regularization to learning in structural domains where natural distance functions exist. These kernels belong to two groups: (i) kernels in the proximity space induced by set distances and (ii) set distance substitution kernels (non-PSD in general). We report experimental results which show that our kernels compare favorably with kernels based on averaging and achieve results similar to other state-of-the-art methods. At the same time our kernels systematically improve over the naive way of exploiting distances.
[Density measurement, Buildings, set theory, machine learning, Support vector machines, Computer science, Power measurement, vectors, set kernels, Support vector machine classification, Gaussian processes, Machine learning, Probability density function, learning (artificial intelligence), Kernel]
Deploying Approaches for Pattern Refinement in Text Mining
Sixth International Conference on Data Mining
None
2006
Text mining is the technique that helps users find useful information from a large amount of digital text documents on the Web or databases. Instead of the keyword-based approach which is typically used in this field, the pattern-based model containing frequent sequential patterns is employed to perform the same concept of tasks. However, how to effectively use these discovered patterns is still a big challenge. In this study, we propose two approaches based on the use of pattern deploying strategies. The performance of the pattern deploying algorithms for text mining is investigated on the Reuters dataset RCVI and the results show that the effectiveness is improved by using our proposed pattern refinement approaches.
[Text mining, text analysis, databases, data mining, digital text documents, Information retrieval, World Wide Web, Data mining, Reuters dataset, pattern-based model, pattern refinement, pattern deploying algorithms, Databases, frequent sequential patterns, Text categorization, Frequency, text mining, Internet, Data communication, Australia, Indexing, Software engineering]
TOP-COP: Mining TOP-K Strongly Correlated Pairs in Large Databases
Sixth International Conference on Data Mining
None
2006
Recently, there has been considerable interest in computing strongly correlated pairs in large databases. Most previous studies require the specification of a minimum correlation threshold to perform the computation. However, it may be difficult for users to provide an appropriate threshold in practice, since different data sets typically have different characteristics. To this end, we propose an alternative task: mining the top-k strongly correlated pairs. In this paper, we identify a 2-D monotone property of an upper bound of Pearson's correlation coefficient and develop an efficient algorithm, called TOP-COP to exploit this property to effectively prune many pairs even without computing their correlation coefficients. Our experimental results show that the TOP-COP algorithm can be orders of magnitude faster than brute-force alternatives for mining the top-k strongly correlated pairs.
[Algorithm design and analysis, data mining, Promotion - marketing, TOP-K strongly correlated pair mining, Transaction databases, Data mining, database management systems, 2D monotone property, Upper bound, minimum correlation threshold, Marketing and sales, Computational efficiency, TOP-COP, Books, Public healthcare, Bioinformatics, large databases, Pearson correlation coefficient]
Manifold Clustering of Shapes
Sixth International Conference on Data Mining
None
2006
Shape clustering can significantly facilitate the automatic labeling of objects present in image collections. For example, it could outline the existing groups of pathological cells in a bank of cyto-images; the groups of species on photographs collected from certain aerials; or the groups of objects observed on surveillance scenes from an office building. Here we demonstrate that a nonlinear projection algorithm such as Isomap can attract together shapes of similar objects, suggesting the existence of isometry between the shape space and a low dimensional nonlinear embedding. Whenever there is a relatively small amount of noise in the data, the projection forms compact, convex clusters that can easily be learned by a subsequent partitioning scheme. We further propose a modification of the Isomap projection based on the concept of degree-bounded minimum spanning trees. The proposed approach is demonstrated to move apart bridged clusters and to alleviate the effect of noise in the data.
[object recognition, degree-bounded minimum spanning trees, Shape measurement, data noise, shapes manifold clustering, objects automatic labeling, nonlinear projection algorithm, Noise shaping, Projection algorithms, surveillance scenes, Pathology, Surveillance, pattern clustering, Layout, Clustering algorithms, image retrieval, subsequent partitioning scheme, Noise robustness, Circuit noise, Labeling, image collections, office building, low dimensional nonlinear embedding]
Linear and Non-Linear Dimensional Reduction via Class Representatives for Text Classification
Sixth International Conference on Data Mining
None
2006
We address the problem of building fast and effective text classification tools. We describe a "representatives methodology" related to feature extraction and illustrate its performance using as vehicles a centroid based method and a method based on clustered LSI that were recently proposed as useful tools for low rank matrix approximation and cost effective alternatives to LSI. The methodology is very flexible, providing the means for accelerating existing algorithms. It is also combined with kernel techniques to enable the analysis of data for which linear techniques are insufficient. Numerous classification examples indicate that the proposed technique is effective and efficient with an overall performance superior than existing linear and nonlinear LSI-based approaches.
[approximation theory, text analysis, Costs, Large scale integration, matrix approximation, text classification, classification, Vehicles, Automotive engineering, nonlinear dimensional reduction, representatives methodology, class representatives, Text categorization, feature extraction, Clustering algorithms, Feature extraction, Kernel, Informatics, Testing]
Adaptive Kernel Principal Component Analysis with Unsupervised Learning of Kernels
Sixth International Conference on Data Mining
None
2006
Choosing an appropriate kernel is one of the key problems in kernel-based methods. Most existing kernel selection methods require that the class labels of the training examples are known. In this paper, we propose an adaptive kernel selection method for kernel principal component analysis, which can effectively learn the kernels when the class labels of the training examples are not available. By iteratively optimizing a novel criterion, the proposed method can achieve nonlinear feature extraction and unsupervised kernel learning simultaneously. Moreover, a non-iterative approximate algorithm is developed. The effectiveness of the proposed algorithms are validated on UCI datasets and the COIL-20 object recognition database.
[approximation theory, Laboratories, class label, Optimization methods, Appropriate technology, Unsupervised learning, unsupervised learning, Support vector machines, Computer science, feature extraction, unsupervised kernel learning, noniterative approximate algorithm, Feature extraction, adaptive kernel selection, nonlinear feature extraction, Iterative algorithms, Kernel, principal component analysis, Principal component analysis]
Rule-Based Platform for Web User Profiling
Sixth International Conference on Data Mining
None
2006
This paper discusses a research project: rule-based Web user profiling platform. In this platform, usage data are encoded as a sequence of events, each of which represents an action performed by a user on a Web service at a given time. An event template is proposed to define event models for different Web services. The platform is rule-based. Rules define profile metrics and determine how to compute profile metrics from usage events. A prototype of the platform was implemented and was applied to generate profiles from page view events. The major contribution of the work is the rule-based approach to user profiling. It is the rules and the event template that provide the flexibility to allow the platform to be configured for different Web services.
[data compression, Engineering profession, Industrial relations, Companies, Web service, Data mining, encoding, user modelling, profile metrics, event template, rule-based platform, Web services, usage data encoding, Web and internet services, Prototypes, Web pages, Customer relationship management, Web mining, Web user profiling]
Opening the Black Box of Feature Extraction: Incorporating Visualization into High-Dimensional Data Mining Processes
Sixth International Conference on Data Mining
None
2006
Feature extraction techniques have been used to handle high-dimensional data and experimental studies often show improved classification accuracies. Unfortunately very few studies provide concrete evidences on the effectiveness of these feature extraction techniques and they largely remain to be black boxes. In this study, we design and implement a visualization prototype system that allows users to look into the classification processes, explore the links among the original and extracted features in different classifiers, examine why and how an instance is correctly or incorrectly classified. We demonstrate the prototype's capabilities by combining a feature extraction method based on hierarchical feature space clustering with J48 decision tree classifiers and perform experiments on a real hyperspectral remote sensing image dataset.
[pattern classification, Hyperspectral sensors, data mining, classification accuracy, Data mining, Remote sensing, hyperspectral remote sensing image dataset, feature extraction, Data visualization, Prototypes, data visualisation, decision trees, Feature extraction, high-dimensional data mining process, hierarchical feature space clustering, Concrete, data visualization, Decision trees, J48 decision tree classifier, Classification tree analysis, Hyperspectral imaging]
Semantic Smoothing for Model-based Document Clustering
Sixth International Conference on Data Mining
None
2006
A document is often full of class-independent "general" words and short of class-specific "core " words, which leads to the difficulty of document clustering. We argue that both problems will be relieved after suitable smoothing of document models in agglomerative approaches and of cluster models in partitional approaches, and hence improve clustering quality. To the best of our knowledge, most model-based clustering approaches use Laplacian smoothing to prevent zero probability while most similarity-based approaches employ the heuristic TF*IDF scheme to discount the effect of "general" words. Inspired by a series of statistical translation language model for text retrieval, we propose in this paper a novel smoothing method referred to as context-sensitive semantic smoothing for document clustering purpose. The comparative experiment on three datasets shows that model-based clustering approaches with semantic smoothing is effective in improving cluster quality.
[Vocabulary, smoothing methods, model-based document clustering, zero probability, Information science, Clustering algorithms, document models, Laplacian smoothing, document handling, Smoothing methods, Laplace equations, probability, information retrieval, Probability, Educational institutions, Information retrieval, text retrieval, statistical translation language model, model-based clustering, Nearest neighbor searches, clustering quality, pattern clustering, context-sensitive semantic smoothing, cluster quality, Context modeling]
Corrective Classification: Classifier Ensembling with Corrective and Diverse Base Learners
Sixth International Conference on Data Mining
None
2006
Empirical studies on supervised learning have shown that ensembling methods lead to a model superior to the one built from a single learner under many circumstances especially when learning from imperfect, such as biased or noise infected, information sources. In this paper, we provide a novel corrective classification (C2) design, which incorporates error detection, data cleansing and Bootstrap sampling to construct base learners that constitute the classifier ensemble. The essential goal is to reduce noise impacts and eventually enhance the learners built from noise corrupted data. We further analyze the importance of both the accuracy and diversity of base learners in ensembling, in order to shed some light on the mechanism under which C2 works. Experimental comparisons will demonstrate that C2 is not only superior to the learner built from the original noisy sources, but also more reliable than bagging or the aggressive classifier ensemble (ACE), which are two degenerate components/variants of C2.
[classifier ensembling, pattern classification, Data analysis, Data preprocessing, Noise reduction, data mining, supervised learning, bootstrap sampling, Data mining, error detection, noise impacts, Computer science, data cleansing, Supervised learning, Sampling methods, Robustness, Error correction, learning (artificial intelligence), Bagging, corrective classification, corrective base learners, diverse base learners]
Speedup Clustering with Hierarchical Ranking
Sixth International Conference on Data Mining
None
2006
Many clustering algorithms in particular hierarchical clustering algorithms do not scale-up well for large data-sets especially when using an expensive distance function. In this paper, we propose a novel approach to perform approximate clustering with high accuracy. We introduce the concept of a pairwise hierarchical ranking to efficiently determine close neighbors for every data object. Empirical results on synthetic and real-life data show a speedup of up to two orders of magnitude over OPTICS while maintaining a high accuracy and up to one order of magnitude over the previously proposed DATA BUBBLES method, which also tries to speedup OPTICS by trading accuracy for speed.
[data mining, expensive distance function, Optical distortion, Optical computing, Extraterrestrial measurements, Statistics, Proteins, pairwise hierarchical ranking, Runtime, pattern clustering, hierarchical clustering algorithm, Clustering algorithms, data bubbles method, Computer applications, Sampling methods, approximate clustering, speedup clustering, OPTICS, Indexing]
Query-Sensitive Similarity Measure for Content-Based Image Retrieval
Sixth International Conference on Data Mining
None
2006
Similarity measure is one of the keys of a high- performance content-based image retrieval (CBIR) system. Given a pair of images, existing similarity measures usually produce a static and constant similarity score. However, an image can usually be perceived with different meanings and therefore, the similarity between the same pair of images may change when the concept being queried changes. This paper proposes a query-sensitive similarity measure, Qsim, which takes the concept being queried into account in measuring image similarities, by exploiting the query image as well as the images labeled by user in the relevance feedback process. Experimental comparisons to state-of-the-art techniques show that Qsim has superior performance.
[relevance feedback process, Qsim, Content based retrieval, Image retrieval, Laboratories, Information retrieval, Spatial databases, image similarities, content-based retrieval, Engines, Image databases, Feedback, image retrieval, Software measurement, Labeling, content-based image retrieval, query-sensitive similarity measure]
Welcome Message from the Conference Chairs
Seventh IEEE International Conference on Data Mining
None
2007
Presents the welcome message from the conference proceedings.
[]
Preface
Seventh IEEE International Conference on Data Mining
None
2007
Presents the introductory welcome message from the conference proceedings.
[]
Conference organization
Seventh IEEE International Conference on Data Mining
None
2007
Provides a listing of current committee members and society officers.
[]
Program Committee
Seventh IEEE International Conference on Data Mining
None
2007
Provides a listing of current committee members.
[]
Invited Speakers and Their Talk Descriptions
Seventh IEEE International Conference on Data Mining
None
2007
Provides an abstract for each of the invited presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[Computer science, Tensile stress, Software libraries, Image coding, Data visualization, Machine learning, Search engines, Information retrieval, Data mining, Open source software]
Tutorials and Their Descriptions
Seventh IEEE International Conference on Data Mining
None
2007
Provides an abstract for each of the tutorial presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
How Much Noise Is Too Much: A Study in Automatic Text Classification
Seventh IEEE International Conference on Data Mining
None
2007
Noise is a stark reality in real life data. Especially in the domain of text analytics, it has a significant impact as data cleaning forms a very large part of the data processing cycle. Noisy unstructured text is common in informal settings such as on-line chat, SMS, email, newsgroups and blogs, automatically transcribed text from speech, and automatically recognized text from printed or handwritten material. Gigabytes of such data is being generated everyday on the Internet, in contact centers, and on mobile phones. Researchers have looked at various text mining issues such as pre-processing and cleaning noisy text, information extraction, rule learning, and classification for noisy text. This paper focuses on the issues faced by automatic text classifiers in analyzing noisy documents coming from various sources. The goal of this paper is to bring out and study the effect of different kinds of noise on automatic text classification. Does the nature of such text warrant moving beyond traditional text classification techniques? We present detailed experimental results with simulated noise on the Reuters- 21578 and 20-newsgroups benchmark datasets. We present interesting results on real-life noisy datasets from various CRM domains.
[Text mining, pattern classification, text analysis, Blogs, data mining, Data processing, Cleaning, Mobile handsets, Automatic speech recognition, noisy text analytics, Handwriting recognition, Text recognition, Text categorization, data cleaning, noisy text document analysis, automatic text classification, text mining, Internet, data processing cycle]
Clustering Needles in a Haystack: An Information Theoretic Analysis of Minority and Outlier Detection
Seventh IEEE International Conference on Data Mining
None
2007
Identifying atypical objects is one of the traditional topics in machine learning. Recently, novel approaches, e.g., Minority Detection and One-class clustering, have explored further to identify clusters of atypical objects which strongly contrast from the rest of the data in terms of their distribution or density. This paper analyzes such tasks from an information theoretic perspective. Based on Information Bottleneck formalization, these tasks interpret to increasing the averaged atypicalness of the clusters while reducing the complexity of the clustering. This formalization yields a unifying view of the new approaches as well as the classic outlier detection. We also present a scalable minimization algorithm which exploits the localized form of the cost function over individual clusters. The proposed algorithm is evaluated using simulated datasets and a text classification benchmark, in comparison with an existing method.
[information theoretic analysis, pattern classification, Machine learning algorithms, Rate distortion theory, information bottleneck formalization, object detection, needles clustering, text classification, Data mining, machine learning, simulated datasets, Unsupervised learning, Information analysis, scalable minimization algorithm, Clustering algorithms, minority detection, one-class clustering, Object detection, Machine learning, Needles, Cost function, learning (artificial intelligence)]
Temporal Analysis of Semantic Graphs Using ASALSAN
Seventh IEEE International Conference on Data Mining
None
2007
ASALSAN is a new algorithm for computing three-way DEDICOM, which is a linear algebra model for analyzing intrinsically asymmetric relationships, such as trade among nations or the exchange of emails among individuals, that incorporates a third mode of the data, such as time. ASALSAN is unique because it enables computing the three-way DEDICOM model on large, sparse data. A nonnegative version of ASALSAN is described as well. When we apply these techniques to adjacency arrays arising from directed graphs with edges labeled by time, we obtain a smaller graph on latent semantic dimensions and gain additional information about their changing relationships over time. We demonstrate these techniques on international trade data and the Enron email corpus to uncover latent components and their transient behavior. The mixture of roles assigned to individuals by ASALSAN showed strong correspondence with known job classifications and revealed the patterns of communication between these roles. Changes in the communication pattern over time, e.g., between top executives and the legal department, were also apparent in the solutions.
[Algorithm design and analysis, US Department of Energy, Data analysis, Law, Laboratories, mathematics computing, Data mining, adjacency arrays, matrix algebra, ASALSAN algorithm, International trade, semantic graph temporal analysis, USA Councils, directed graphs, three-way DEDICOM computing, large sparse data, Linear algebra, Legal factors, linear algebra model]
Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights
Seventh IEEE International Conference on Data Mining
None
2007
Recommender systems based on collaborative filtering predict user preferences for products or services by learning past user-item relationships. A predominant approach to collaborative filtering is neighborhood based ("k-nearest neighbors"), where a user-item preference rating is interpolated from ratings of similar items and/or users. We enhance the neighborhood-based approach leading to substantial improvement of prediction accuracy, without a meaningful increase in running time. First, we remove certain so-called "global effects" from the data to make the ratings more comparable, thereby improving interpolation accuracy. Second, we show how to simultaneously derive interpolation weights for all nearest neighbors, unlike previous approaches where each weight is computed separately. By globally solving a suitable optimization problem, this simultaneous interpolation accounts for the many interactions between neighbors leading to improved accuracy. Our method is very fast in practice, generating a prediction in about 0.2 milliseconds. Importantly, it does not require training many parameters or a lengthy preprocessing, making it very practical for large scale applications. Finally, we show how to apply these methods to the perceivably much slower user-oriented approach. To this end, we suggest a novel scheme for low dimensional embedding of the users. We evaluate these methods on the netflix dataset, where they deliver significantly better results than the commercial netflix cinematch recommender system.
[Filtering, Demography, scalable collaborative filtering, International collaboration, Data mining, information filters, Nearest neighbor searches, Interpolation, Accuracy, interpolation, Netflix Cinematch recommender system, Motion pictures, Large-scale systems, Recommender systems, interpolation weights]
Rule Cubes for Causal Investigations
Seventh IEEE International Conference on Data Mining
None
2007
With the complexity of modern vehicles tremendously increasing, quality engineers play a key role within today's automotive industry. Field data analysis supports corrective actions in development, production and after sales support. We decompose the requirements and show that association rules, being a popular approach to generating ex- planative models, still exhibit shortcomings. Recently proposed interactive rule cubes are a promising alternative. We extend this work by introducing a way of intuitively visualizing and meaningfully ranking them. Moreover, we present methods to interactively factorize a problem and validate hypotheses by ranking patterns based on expectations, and by browsing a cube-based network of related influences. All this is currently in use as an interactive tool for warranty data analysis in the automotive industry. A real-world case study shows how engineers successfully use it in identifying root causes of quality issues.
[Warranties, Data analysis, automotive industry, data analysis, ranking patterns, warranty data analysis, causal investigations, cube-based network, Data engineering, field data analysis, automobile industry, Data mining, Information technology, Automotive engineering, Vehicles, interactive tool, Information processing, Production, interactive systems, vehicles complexity, Mining industry, computational complexity]
The Chosen Few: On Identifying Valuable Patterns
Seventh IEEE International Conference on Data Mining
None
2007
Constrained pattern mining extracts patterns based on their individual merit. Usually this results in far more patterns than a human expert or a machine learning technique could make use of. Often different patterns or combinations of patterns cover a similar subset of the examples, thus being redundant and not carrying any new information. To remove the redundant information contained in such pattern sets, we propose a general heuristic approach for selecting a small subset of patterns. We identify several selection techniques for use in this general algorithm and evaluate those on several data sets. The results show that the technique succeeds in severely reducing the number of patterns, while at the same time apparently retaining much of the original information. Additionally the experiments show that reducing the pattern set indeed improves the quality of classification results. Both results show that the approach is very well suited for the goals we aim at.
[Machine learning algorithms, Humans, data mining, Spatial databases, Encoding, valuable pattern identification, Data mining, machine learning, constrained pattern mining, pattern extraction, Machine learning, redundant information, pattern set]
Spectral Regression: A Unified Approach for Sparse Subspace Learning
Seventh IEEE International Conference on Data Mining
None
2007
Recently the problem of dimensionality reduction (or, subspace learning) has received a lot of interests in many fields of information processing, including data mining, information retrieval, and pattern recognition. Some popular methods include principal component analysis (PCA), linear discriminant analysis (LDA) and locality preserving projection (LPP). However, a disadvantage of all these approaches is that the learned projective functions are linear combinations of all the original features, thus it is often difficult to interpret the results. In this paper, we propose a novel dimensionality reduction framework, called Unified Sparse Subspace Learning (USSL), for learning sparse projections. USSL casts the problem of learning the projective functions into a regression framework, which facilitates the use of different kinds of regularizes. By using a L<sub>1</sub>-norm regularizer (lasso), the sparse projections can be efficiently computed. Experimental results on real world classification and clustering problems demonstrate the effectiveness of our method.
[information processing, locality preserving projection, spectral regression, Scattering, data mining, unified sparse subspace learning, regression analysis, information retrieval, Information retrieval, Pattern recognition, Covariance matrix, Data mining, Helium, data reduction, dimensionality reduction, Clustering algorithms, Information processing, learned projective functions, Linear discriminant analysis, learning (artificial intelligence), linear discriminant analysis, principal component analysis, Principal component analysis, pattern recognition]
Mining Frequent Itemsets in a Stream
Seventh IEEE International Conference on Data Mining
None
2007
We study the problem of finding frequent itemsets in a continuous stream of transactions. The current frequency of an itemset in a stream is defined as its maximal frequency over all possible windows in the stream from any point in the past until the current state that satisfy a minimal length constraint. Properties of this new measure are studied and an incremental algorithm that allows, at any time, to immediately produce the current frequencies of all frequent itemsets is proposed. Experimental and theoretical analysis show that the space requirements for the algorithm are extremely small for many realistic data distributions.
[Algorithm design and analysis, continuous stream, data mining, Time measurement, Ice, Frequency measurement, Data mining, History, incremental algorithm, Itemsets, Databases, Current measurement, data distributions, minimal length constraint, Marketing and sales, frequent itemsets mining]
A Cascaded Approach to Biomedical Named Entity Recognition Using a Unified Model
Seventh IEEE International Conference on Data Mining
None
2007
We propose a cascaded approach for extracting biomedical named entities from text documents using a unified model. Previous works often ignore the high computational cost incurred by a single-phase approach. We alleviate this problem by dividing the named entity extraction task into a segmentation task and a classification task, reducing the computational cost by an order of magnitude. A unified model, which we term "maximum-entropy margin-based" (MEMB), is used in both tasks. The MEMB model considers the error between a correct and an incorrect output during training and helps improve the performance of extracting sparse entity types that occur in biomedical literature. We report experimental evaluations on the GENIA corpus available from the BioNLP/NLPBA (2004) shared task, which demonstrate the state-of-the-art performance achieved by the proposed approach.
[document handling, pattern classification, RNA, supervised learning, information retrieval, text documents, Data engineering, biomedical named entity recognition, Data mining, classification task, segmentation task, maximum-entropy margin-based model, Databases, Text recognition, entity extraction problem, Systems engineering and theory, Computational efficiency, Error correction, learning (artificial intelligence), Research and development management, medical computing, Biomedical engineering]
Incorporating User Provided Constraints into Document Clustering
Seventh IEEE International Conference on Data Mining
None
2007
Document clustering without any prior knowledge or background information is a challenging problem. In this paper, we propose SS-NMF: a semi-supervised non- negative matrix factorization framework for document clustering. In SS-NMF, users are able to provide supervision for document clustering in terms of pairwise constraints on a few documents specifying whether they "must" or "cannot" be clustered together. Through an iterative algorithm, we perform symmetric tri-factorization of the document- document similarity matrix to infer the document clusters. Theoretically, we show that SS-NMF provides a general framework for semi-supervised clustering and that existing approaches can be considered as special cases of SS-NMF. Through extensive experiments conducted on publicly available data sets, we demonstrate the superior performance of SS-NMF for clustering documents.
[document handling, Symmetric matrices, Clustering methods, document clustering, user provided constraints, Pattern recognition, matrix decomposition, Data mining, Couplings, Databases, pairwise constraints, Machine vision, pattern clustering, Clustering algorithms, Computer graphics, Iterative algorithms, semisupervised nonnegative matrix factorization]
Depth-Based Novelty Detection and Its Application to Taxonomic Research
Seventh IEEE International Conference on Data Mining
None
2007
It is estimated that less than 10 percent of the world's species have been described, yet species are being lost daily due to human destruction of natural habitats. The job of describing the earth's remaining species is exacerbated by the shrinking number of practicing taxonomists and the very slow pace of traditional taxonomic research. In this article, we tackle, from a novelty detection perspective, one of the most important and challenging research objectives in taxonomy - new species identification. We propose a unique and efficient novelty detection framework based on statistical depth functions. Statistical depth functions provide from the "deepest" point a "center-outward ordering" of multidimensional data. In this sense, they can detect observations that appear extreme relative to the rest of the observations, i.e., novelty. Of the various statistical depths, the spatial depth is especially appealing because of its computational efficiency and mathematical tractability. We propose a novel statistical depth, the kernelized spatial depth (KSD) that generalizes the spatial depth via positive definite kernels. By choosing a proper kernel, the KSD can capture the local structure of a data set while the spatial depth fails. Observations with depth values less than a threshold are declared as novel. The proposed algorithm is simple in structure: the threshold is the only one parameter for a given kernel. We give an upper bound on the false alarm probability of a depth-based detector, which can be used to determine the threshold. Experimental study demonstrates its excellent potential in new species discovery.
[depth-based detector, Taxonomy, Humans, data mining, kernelized spatial depth, Data mining, Earth, center-outward ordering, USA Councils, biology computing, Detectors, statistical depth functions, false alarm probability, learning (artificial intelligence), Kernel, multidimensional data, probability, taxonomic research, mathematical tractability, Character recognition, machine learning, Support vector machines, new species identification, depth-based novelty detection, Machine learning, zoology]
Detecting Fractures in Classifier Performance
Seventh IEEE International Conference on Data Mining
None
2007
A fundamental tenet assumed by many classification algorithms is the presumption that both training and testing samples are drawn from the same distribution of data - this is the stationary distribution assumption. This entails that the past is strongly indicative of the future. However, in real world applications, many factors may alter the One True Model responsible for generating the data distribution both significantly and subtly. In circumstances violating the stationary distribution assumption, traditional validation schemes such as ten-folds and hold-out become poor performance predictors and classifier rankers. Thus, it becomes critical to discover the fracture points in classifier performance by discovering the divergence between populations. In this paper, we implement a comprehensive evaluation framework to identify bias, enabling selection of a "correct" classifier given the sample bias. To thoroughly evaluate the performance of classifiers within biased distributions, we consider the following three scenarios: missing completely at random (akin to stationary); missing at random; and missing not at random. The latter reflects the canonical sample selection bias problem.
[Measurement, classification algorithms, pattern classification, Virtual colonoscopy, data distribution, data mining, Data engineering, stationary distribution assumption, Classification algorithms, Data mining, classifier performance, biased distributions, Computer science, Machine learning, Risk management, Decision trees, Testing, fracture points]
Non-redundant Multi-view Clustering via Orthogonalization
Seventh IEEE International Conference on Data Mining
None
2007
Typical clustering algorithms output a single clustering of the data. However, in real world applications, data can often be interpreted in many different ways; data can have different groupings that are reasonable and interesting from different perspectives. This is especially true for high-dimensional data, where different feature subspaces may reveal different structures of the data. Why commit to one clustering solution while all these alternative clustering views might be interesting to the user. In this paper, we propose a new clustering paradigm for explorative data analysis: find all non-redundant clustering views of the data, where data points of one cluster can belong to different clusters in other views. We present a framework to solve this problem and suggest two approaches within this framework: (1) orthogonal clustering, and (2) clustering in orthogonal subspaces. In essence, both approaches find alternative ways to partition the data by projecting it to a space that is orthogonal to our current solution. The first approach seeks orthogonality in the cluster space, while the second approach seeks orthogonality in the feature space. We test our framework on both synthetic and high-dimensional benchmark data sets, and the results show that indeed our approaches were able to discover varied solutions that are interesting and meaningful.
[Data analysis, nonredundant multiview data clustering, data analysis, Clustering methods, Scattering, Data structures, Data mining, orthogonal clustering, pattern clustering, Clustering algorithms, Insurance, Machine learning, Benchmark testing, feature subspace, Biomedical imaging]
On Appropriate Assumptions to Mine Data Streams: Analysis and Practice
Seventh IEEE International Conference on Data Mining
None
2007
Recent years have witnessed an increasing number of studies in stream mining, which aim at building an accurate model for continuously arriving data. Somehow most existing work makes the implicit assumption that the training data and the yet-to-come testing data are always sampled from the "same distribution\
[model averaging, Data analysis, baseline models, data mining, data streams, random guessing, Data mining, stream mining, voting-based framework, Sequential analysis, Training data, Robustness, Testing]
ORIGAMI: Mining Representative Orthogonal Graph Patterns
Seventh IEEE International Conference on Data Mining
None
2007
In this paper, we introduce the concept of alpha-orthogonal patterns to mine a representative set of graph patterns. Intuitively, two graph patterns are alpha-orthogonal if their similarity is bounded above by alpha. Each alpha-orthogonal pattern is also a representative for those patterns that are at least beta similar to it. Given user defined alpha, beta isin [0,1], the goal is to mine an alpha-orthogonal, beta-representative set that minimizes the set of unrepresented patterns. We present ORIGAMI, an effective algorithm for mining the set of representative orthogonal patterns. ORIGAMI first uses a randomized algorithm to randomly traverse the pattern space, seeking previously unexplored regions, to return a set of maximal patterns. ORIGAMI then extracts an alpha-orthogonal, beta-representative set from the mined maximal patterns. We show the effectiveness of our algorithm on a number of real and synthetic datasets. In particular, we show that our method is able to extract high quality patterns even in cases where existing enumerative graph mining methods fail to do so.
[Chaos, ORIGAMI, Social network services, Blogs, graph theory, data mining, randomized algorithm, Data mining, Proteins, Computer science, Databases, orthogonal graph pattern, IP networks, Web sites, Pattern analysis]
Efficient Algorithms for Mining Significant Substructures in Graphs with Quality Guarantees
Seventh IEEE International Conference on Data Mining
None
2007
Graphs have become popular for modeling scientific data in recent years. As a result, techniques for mining graphs are extremely important for understanding inherent data and domain characteristics. One such exploratory mining paradigm is the k-MST (minimum spanning tree over k vertices) problem that can be used to discover significant local substructures. In this paper, we present an efficient approximation algorithm for the k-MST problem in large graphs. The algorithm has an O(radic/k) approximation ratio and O(n log n + in log m log k + nk2 log k) running time, where n and m are the number of vertices and edges respectively. Experimental results on synthetic graphs and protein interaction networks show that the algorithm is scalable to large graphs and useful for discovering biological pathways. The highlight of the algorithm is that it offers both analytical guarantees and empirical evidence of good running time and quality.
[Algorithm design and analysis, approximation theory, scientific data modeling, Biological system modeling, Social network services, large graph theory, data mining, trees (mathematics), minimum spanning tree, Proteins, Tree graphs, Layout, Clustering algorithms, approximation algorithm, Approximation algorithms, Frequency, Dynamic programming, natural sciences computing, computational complexity]
Dynamic Micro Targeting: Fitness-Based Approach to Predicting Individual Preferences
Seventh IEEE International Conference on Data Mining
None
2007
It is crucial to segment customers intelligently in order to offer more targeted and personalized products and services. Traditionally, customer segmentation is achieved using statistics-based methods that compute a set of statistics from the customer data and group customers into segments by applying clustering algorithms. Recent research proposed a direct grouping-based approach that combines customers into segments by optimally combining transactional data of several customers and building a data mining model of customer behavior for each group. This paper proposes a new micro targeting method that builds predictive models of customer behavior not on the segments of customers but rather on the customer-product groups. This micro-targeting method is more general than the previously considered direct grouping method. We empirically show that it significantly outperforms the direct grouping and statistics-based segmentation methods across multiple experimental conditions and that it generates predominately small-sized segments, thus providing additional support for the micro-targeting approach to personalization.
[Demography, direct grouping-based approach, Clustering methods, data mining, fitness-based approach, Predictive models, consumer behaviour, Data mining, Clustering algorithms, clustering algorithm, Partitioning algorithms, Statistics, Customer profiles, statistics-based segmentation method, customer services, Aggregates, pattern clustering, data mining model, personalized product, dynamic microtargeting method, statistical analysis, intelligent customer segmentation, personalized service, customer behavior, Context modeling]
Data Discretization Unification
Seventh IEEE International Conference on Data Mining
None
2007
Data discretization is defined as a process of converting continuous data attribute values into a finite set of intervals with minimal loss of information. In this paper, we prove that discretization methods based on informational theoretical complexity and the methods based on statistical measures of data dependency are asymptotically equivalent. Furthermore, we define a notion of generalized entropy and prove that discretization methods based on MDLP, Gini Index, AIC, BIC, and Pearson's X2 and G2 statistics are all derivable from the generalized entropy function. We design a dynamic programming algorithm that guarantees the best discretization based on the generalized entropy notion. Furthermore, we conducted an extensive performance evaluation of our method for several publicly available data sets. Our results show that our method delivers on the average 31% less classification errors than many previously known discretization methods.
[Algorithm design and analysis, generalized entropy function, Error analysis, Heuristic algorithms, data mining, dynamic programming, Entropy, information minimal loss, Data mining, Association rules, Statistics, Computer science, continuous data attribute values, data dependency, Bayesian methods, dynamic programming algorithm, data discretization unification, Dynamic programming, informational theoretical complexity]
Improving Knowledge Discovery in Document Collections through Combining Text Retrieval and Link Analysis Techniques
Seventh IEEE International Conference on Data Mining
None
2007
In this paper, we present Concept Chain Queries (CCQ), a special case of text mining in document collections focusing on detecting links between two topics across text documents. We interpret such a query as finding the most meaningful evidence trails across documents that connect these two topics. We propose to use link-analysis techniques over the extracted features provided by Information Extraction Engine for finding new knowledge. A graphical text representation and mining model is proposed which combines information retrieval, association mining and link analysis techniques. We present experiments on different datasets that demonstrate the effectiveness of our algorithm. Specifically, the algorithm generates ranked concept chains and evidence trails where the key terms representing significant relationships between topics are ranked high.
[Text mining, Knowledge engineering, document handling, document collections, natural language processing, information extraction engine, data mining, information retrieval, text documents, Information retrieval, Data engineering, text retrieval, knowledge discovery, concept chain queries, Data mining, Engines, Information analysis, Computer science, link analysis techniques, USA Councils, Feature extraction, text mining, graphical text representation]
Finding Cohesive Clusters for Analyzing Knowledge Communities
Seventh IEEE International Conference on Data Mining
None
2007
Documents and authors can be clustered into "knowledge communities" based on the overlap in the papers they cite. We introduce a new clustering algorithm, Streemer, which finds cohesive foreground clusters embedded in a diffuse background, and use it to identify knowledge communities as foreground clusters of papers which share common citations. To analyze the evolution of these communities over time, we build predictive models with features based on the citation structure, the vocabulary of the papers, and the affiliations and prestige of the authors. Findings include that scientific knowledge communities tend to grow more rapidly if their publications build on diverse information and if they use a narrow vocabulary.
[Text mining, Vocabulary, text analysis, clustering algorithm, Communities, Citation analysis, data mining, cohesive foreground clusters, Predictive models, Time measurement, Computational Intelligence Society, Data mining, information networks, Streemer, predictive models, Clustering algorithms, Rhetoric, text mining, scientific knowledge communities]
Succinct Matrix Approximation and Efficient k-NN Classification
Seventh IEEE International Conference on Data Mining
None
2007
This work reveals that instead of the polynomial bounds in previous literatures there exists a sharper bound of exponential form for the L<sub>2</sub> norm of an arbitrary shaped random matrix. Based on the newly elaborated bound, a nonuniform sampling method is presented to succinctly approximate a matrix with a sparse binary one, and thus relieves the computation loads of k-NN classifier in both time and storage. The method is also pass-efficient because sampling and quantizing are combined together in a single step and the whole process can be completed within one pass over the input matrix. In the evaluations on compression ratio and reconstruction error, the sampling method exhibits impressive capability in providing succinct and tight approximations for the input matrices. The most significant finding in the classification experiment is that the k-NN classifier based on the approximation can even outperform the standard one. This provides another strong evidence for the claim that our method is especially capable in capturing intrinsic characteristics.
[Computer vision, pattern classification, Machine learning algorithms, Symmetric matrices, sampling methods, k-NN classification, nonuniform sampling method, data mining, Information retrieval, matrix approximation, polynomial bounds, Sparse matrices, Data mining, compression ratio, arbitrary shaped random matrix, reconstruction error, polynomial matrices, Content addressable storage, Machine learning, sparse matrix, Sampling methods, Approximation algorithms, sparse matrices]
A Pairwise Covariance-Preserving Projection Method for Dimension Reduction
Seventh IEEE International Conference on Data Mining
None
2007
Dimension reduction is critical in many areas of pattern classification and machine learning and many discriminant analysis algorithms have been proposed. In this paper, a Pairwise Covariance-preserving Projection Method (PCPM) is proposed for dimension reduction. PCPM maximizes the class discrimination and also preserves approximately the pairwise class covariances. The optimization involved in PCPM can be solved directly by eigenvalues decomposition. Our theoretical and empirical analysis reveals the relationship between PCPM and Linear Discriminant Analysis (LDA), Sliced Average Variance Estimator (SAVE), Heteroscedastic Discriminant Analysis (HDA) and Covariance preserving Projection Method (CPM). PCPM can utilize class mean and class covariance information at the same time. Furthermore, pairwise weight scheme can be incorporated naturally with the pairwise summarization form. The proposed methods are evaluated by both synthetic and real-world datasets.
[Maximum likelihood estimation, pattern classification, heteroscedastic discriminant analysis, pairwise covariance-preserving projection method, eigenvalues decomposition, Educational institutions, Covariance matrix, Data mining, machine learning, dimension reduction, Computer science, sliced average variance estimator, USA Councils, Pattern classification, Machine learning, Linear discriminant analysis, learning (artificial intelligence), linear discriminant analysis, Analysis of variance, principal component analysis]
Community Learning by Graph Approximation
Seventh IEEE International Conference on Data Mining
None
2007
Learning communities from a graph is an important problem in many domains. Different types of communities can be generalized as link-pattern based communities. In this paper, we propose a general model based on graph approximation to learn link-pattern based community structures from a graph. The model generalizes the traditional graph partitioning approaches and is applicable to learning various community structures. Under this model, we derive a family of algorithms which are flexible to learn various community structures and easy to incorporate the prior knowledge of the community structures. Experimental evaluation and theoretical analysis show the effectiveness and great potential of the proposed model and algorithms.
[Algorithm design and analysis, link-pattern based community structure learning, Social network services, Communities, graph theory, graph partitioning approaches, graph approximation, Partitioning algorithms, Data mining, Scheduling algorithm, unsupervised learning algorithms, Vehicles, unsupervised learning, USA Councils, Web pages, Web mining]
Parallel Mining of Frequent Closed Patterns: Harnessing Modern Computer Architectures
Seventh IEEE International Conference on Data Mining
None
2007
Inspired by emerging multi-core computer architectures, in this paper we present MT_CLOSED, a multi-threaded algorithm for frequent closed itemset mining (FCIM). To the best of our knowledge, this is the first FCIM parallel algorithm proposed so far. We studied how different duplicate checking techniques, typical of FCIM algorithms, may affect this parallelization. We showed that only one of them allows to decompose the global FCIM problem into independent tasks that can be executed in any order, and thus in parallel. Finally we show how MT_Closed efficiently harness modern CPUs. We designed and tested several parallelization paradigms by investigating static/dynamic decomposition and scheduling of tasks, thus showing its scalability w.r.t. to the number of CPUs. We analyzed the cache friendliness of the algorithm. Finally, we provided additional speed-up by introducing SIMD extensions.
[Algorithm design and analysis, data mining, parallel algorithm, central processing unit, Data mining, Yarn, Parallel algorithms, multithreaded algorithm, Itemsets, MT_CLOSED, computer architecture, Computer architecture, Parallel processing, scheduling, parallel mining, parallel algorithms, multi-threading, frequent closed pattern, multicore computer architecture, Dynamic scheduling, Data structures, parallelization, frequent closed itemset mining, duplicate checking, task scheduling, Coprocessors]
Supervised Learning by Training on Aggregate Outputs
Seventh IEEE International Conference on Data Mining
None
2007
Supervised learning is a classic data mining problem where one wishes to be be able to predict an output value associated with a particular input vector. We present a new twist on this classic problem where, instead of having the training set contain an individual output value for each input vector, the output values in the training set are only given in aggregate over a number of input vectors. This new problem arose from a particular need in learning on mass spectrometry data, but could easily apply to situations when data has been aggregated in order to maintain privacy. We provide a formal description of this new problem for both classification and regression. We then examine how k-nearest neighbor, neural networks, and support vector machines can be adapted for this problem.
[training set, Data privacy, support vector machines, Scanning probe microscopy, data mining, supervised learning, neural networks, Mass spectroscopy, Educational institutions, mass spectrometry data, formal description, input vector, k-nearest neighbor, Data mining, Computer science, Aggregates, Supervised learning, Neural networks, Aerosols, learning (artificial intelligence), data mining problem, neural nets]
Sample Selection for Maximal Diversity
Seventh IEEE International Conference on Data Mining
None
2007
The problem of selecting a sample subset sufficient to preserve diversity arises in many applications. One example is in the design of recombinant inbred lines (RIL) for genetic association studies. In this context, genetic diversity is measured by how many alleles are retained in the resulting inbred strains. RIL panels that are derived from more than two parental strains, such as the collaborative cross (Churchill et al., 2004), present a particular challenge with regard to which of the many existing lab mouse strains should be included in the initial breeding funnel in order to maximize allele retention. A similar problem occurs in the study of customer reviews when selecting a subset of products with a maximal diversity in reviews. Diversity in this case implies the presence of a set of products having both positive and negative ranks for each customer. In this paper, we demonstrate that selecting an optimal diversity subset is an NP-complete problem via reduction to set cover. This reduction is sufficiently tight that greedy approximations to the set cover problem directly apply to maximizing diversity. We then suggest a slightly modified subset selection problem in which an initial greedy diversity solution is used to effectively prune an exhaustive search for all diversity subsets bounded from below by a specified coverage threshold. Extensive experiments on real datasets are performed to demonstrate the effectiveness and efficiency of our approach.
[genetic diversity, data mining, Capacitive sensors, Data mining, recombinant inbred lines, breeding funnel, genetics, biology computing, maximal diversity, greedy diversity, inbred strains, Genetics, greedy algorithms, sample selection, allele retention, Discrete wavelet transforms, Application software, NP-complete problem, greedy approximations, Computer science, Collaboration, sample subset selection, Frequency, Strain measurement, Mice, genetic association studies, computational complexity]
Mining Statistical Information of Frequent Fault-Tolerant Patterns in Transactional Databases
Seventh IEEE International Conference on Data Mining
None
2007
Constraints applied on classic frequent patterns are too strict and may cause interesting patterns to be missed. Hence, researchers have proposed to mine a more relaxed version of frequent patterns, where transactions are allowed to miss some items in the itemset they support. Patterns exhibiting such "faults" are called frequent fault-tolerant patterns (FFT-patterns) if they are significant in number. In this paper, the term "pattern" is distinguished from "item- set" as referring to a pair (tidset times itemset). Unlike classical frequent patterns, the number of FFT- patterns grows exponentially not only with the number of items, but also with the number of transactions. Since the latter may reach millions, mining FFT-patterns by enumerating them becomes infeasible. Hence, the challenge is to represent FFT-patterns concisely without losing any useful information. To address this, we draw on the observation that, in transactional databases, the transactions themselves are not important from the data mining point-of- view; i.e. researchers are interested in finding itemsets contained in lots of transactions, rather than in the transactions per se. Therefore, we propose to mine only the frequent itemsets along with the statistical information of the supporting transaction sets, rather than enumerate entire FFT- patterns. Then we present our approach - the BIAS framework, consisting of Backtracking algorithm, Integer Linear Programming (ILP) constraints, and aggregation statistics to solve this problem. Algorithms under this framework not only increase the efficiency of the FFT-patterns mining process by more than an order of magnitude, but also provide a more comprehensive analysis of FFT-Patterns.
[Algorithm design and analysis, transaction processing, integer linear programming, integer programming, data mining, linear programming, Transaction databases, Data mining, database management systems, Statistics, Combinatorial mathematics, Constraint optimization, statistical information, Fault tolerance, Itemsets, backtracking algorithm, Integer linear programming, backtracking, aggregation statistics, statistical analysis, Pattern analysis, transactional database, frequent fault-tolerant pattern mining]
Lightweight Distributed Trust Propagation
Seventh IEEE International Conference on Data Mining
None
2007
Using mobile devices, such as smart phones, people may create and distribute different types of digital content (e.g., photos, videos). One of the problems is that digital content, being easy to create and replicate, may likely swamp users rather than informing them. To avoid that, users may organize content producers that they know and trust in a web of trust. Users may then reason about this web of trust to form opinions about content producers with whom they have never interacted before. These opinions will then determine whether content is accepted. The process of forming opinions is called trust propagation. We design a mechanism for mobile devices that effectively propagates trust and that is lightweight and distributed (as opposed to previous work that focuses on centralized propagation). This mechanism uses a graph-based learning technique. We evaluate the effectiveness (predictive accuracy) of this mechanism against a large real-world data set. We also evaluate the computational cost of a J2ME implementation on a mobile phone.
[content producers, Java, mobile radio, digital content, lightweight distributed trust propagation, graph-based learning, centralized propagation, Educational institutions, Mobile handsets, Data mining, Videos, Computer science, Accuracy, mobile computing, security of data, Optical propagation, mobile phone, mobile devices, Computational efficiency, J2ME implementation, learning (artificial intelligence), Mobile computing, Smart phones]
Social Network Extraction of Academic Researchers
Seventh IEEE International Conference on Data Mining
None
2007
This paper addresses the issue of extraction of an academic researcher social network. By researcher social network extraction, we are aimed at finding, extracting, and fusing the 'semantic '-based profiling information of a researcher from the Web. Previously, social network extraction was often undertaken separately in an ad-hoc fashion. This paper first gives a formalization of the entire problem. Specifically, it identifies the 'relevant documents' from the Web by a classifier. It then proposes a unified approach to perform the researcher profiling using conditional random fields (CRF). It integrates publications from the existing bibliography datasets. In the integration, it proposes a constraints-based probabilistic model to name disambiguation. Experimental results on an online system show that the unified approach to researcher profiling significantly outperforms the baseline methods of using rule learning or classification. Experimental results also indicate that our method to name disambiguation performs better than the baseline method using unsupervised learning. The methods have been applied to expert finding. Experiments show that the accuracy of expert finding can be significantly improved by using the proposed methods.
[Biometrics, Computer vision, conditional random fields, Social network services, constraints-based probabilistic model, Data mining, Visual databases, Application software, unsupervised learning, Computer science, Image databases, social network extraction, USA Councils, Internet, learning (artificial intelligence), online system, Indexing, bibliography datasets]
General Averaged Divergence Analysis
Seventh IEEE International Conference on Data Mining
None
2007
Subspace selection is a powerful tool in data mining. An important subspace method is the Fisher-Rao linear discriminant analysis (LDA), which has been successfully applied in many fields such as biometrics, bioinformatics, and multimedia retrieval. However, LDA has a critical drawback: the projection to a subspace tends to merge those classes that are close together in the original feature space. If the separated classes are sampled from Gaussian distributions, all with identical covariance matrices, then LDA maximizes the mean value of the Kullback-Leibler (KL) divergences between the different classes. We generalize this point of view to obtain a framework for choosing a subspace by 1) generalizing the KL divergence to the Bregman divergence and 2) generalizing the arithmetic mean to a general mean. The framework is named the general averaged divergence analysis (GADA). Under this GADA framework, a geometric mean divergence analysis (GMDA) method based on the geometric mean is studied. A large number of experiments based on synthetic data show that our method significantly outperforms LDA and several representative LDA extensions.
[Biometrics, covariance matrices, Gaussian distributions, Merging, Fisher-Rao linear discriminant analysis, data mining, subspace selection, Gaussian distribution, geometric mean divergence analysis, arithmetic, Covariance matrix, Data mining, Information systems, Computer science, general averaged divergence analysis, USA Councils, Kullback-Leibler divergences, Bregman divergence, arithmetic mean, synthetic data, Linear discriminant analysis, general mean, statistical analysis, Arithmetic]
Maximum Entropy Based Significance of Itemsets
Seventh IEEE International Conference on Data Mining
None
2007
We consider the problem of defining the significance of an itemset. We say that the itemset is significant if we are surprised by its frequency when compared to the frequencies of its sub-itemsets. In other words, we estimate the frequency of the itemset from the frequencies of its sub-itemsets and compute the deviation between the real value and the estimate. For the estimation we use Maximum Entropy and for measuring the deviation we use Kullback-Leibler divergence. A major advantage compared to the previous methods is that we are able to use richer models whereas the previous approaches only measure the deviation from the independence model. We show that our measure of significance goes to zero for derivable itemsets and that we can use the rank as a statistical test. Our empirical results demonstrate that for our real datasets the independence assumption is too strong but applying more flexible models leads to good results.
[data mining, Predictive models, maximum entropy, Entropy, Frequency estimation, Kullback-Leibler divergence, Frequency measurement, Electronic mail, Data mining, Proposals, independence assumption, Computer science, flexible model, Itemsets, independence model, maximum entropy methods, itemsets significance, Testing]
Local Probabilistic Models for Link Prediction
Seventh IEEE International Conference on Data Mining
None
2007
One of the core tasks in social network analysis is to predict the formation of links (i.e. various types of relationships) over time. Previous research has generally represented the social network in the form of a graph and has leveraged topological and semantic measures of similarity between two nodes to evaluate the probability of link formation. Here we introduce a novel local probabilistic graphical model method that can scale to large graphs to estimate the joint co-occurrence probability of two nodes. Such a probability measure captures information that is not captured by either topological measures or measures of semantic similarity, which are the dominant measures used for link prediction. We demonstrate the effectiveness of the co-occurrence probability feature by using it both in isolation and in combination with other topological and semantic features for predicting co-authorship collaborations on real datasets.
[Chaos, Social network services, data mining, Predictive models, Probability, Data engineering, topological features, Data mining, Statistics, Computer science, local probabilistic graphical model method, very large databases, Venus, link prediction, social network analysis, Prediction algorithms, semantic features]
Improving Text Classification by Using Encyclopedia Knowledge
Seventh IEEE International Conference on Data Mining
None
2007
The exponential growth of text documents available on the Internet has created an urgent need for accurate, fast, and general purpose text classification algorithms. However, the "bag of words" representation used for these classification methods is often unsatisfactory as it ignores relationships between important terms that do not co-occur literally. In order to deal with this problem, we integrate background knowledge - in our application: Wikipedia - into the process of classifying text documents. The experimental evaluation on Reuters newsfeeds and several other corpus shows that our classification results with encyclopedia knowledge are much better than the baseline "bag of words " methods.
[text analysis, Encyclopedias, Ontologies, Wikipedia, Classification algorithms, text classification, Data mining, classification, Computer science, encyclopedia knowledge, text document, Reuters newsfeeds, Text categorization, Asia, Frequency, Internet, encyclopaedias, bag of words representation]
Language-Independent Set Expansion of Named Entities Using the Web
Seventh IEEE International Conference on Data Mining
None
2007
Set expansion refers to expanding a given partial set of objects into a more complete set. A well-known example system that does set expansion using the web is Google Sets. In this paper, we propose a novel method for expanding sets of named entities. The approach can be applied to semi-structured documents written in any markup language and in any human language. We present experimental results on 36 benchmark sets in three languages, showing that our system is superior to Google Sets in terms of mean average precision.
[document handling, search engines, semistructured documents, natural language processing, Humans, language-independent set expansion, Google sets, markup language, World Wide Web, Information filtering, Data mining, Markup languages, USA Councils, Web pages, named entity, Seals, Collaborative work, Information filters, Internet, human language, Cancer, hypermedia markup languages]
Structure-Based Statistical Features and Multivariate Time Series Clustering
Seventh IEEE International Conference on Data Mining
None
2007
We propose a new method for clustering multivariate time series. A univariate time series can be represented by a fixed-length vector whose components are statistical features of the time series, capturing the global structure. These descriptive vectors, one for each component of the multivariate time series, are concatenated, before being clustered using a standard fast clustering algorithm such as k-means or hierarchical clustering. Such statistical feature extraction also serves as a dimension-reduction procedure for multivariate time series. We demonstrate the effectiveness and simplicity of our proposed method by clustering human motion sequences: dynamic and high-dimensional multivariate time series. The proposed method based on univariate time series structure and statistical metrics provides a novel, yet simple and flexible way to cluster multivariate time series data efficiently with promising accuracy. The success of our method on the case study suggests that clustering may be a valuable addition to the tools available for human motion pattern recognition research.
[Discrete Fourier transforms, Humans, fixed-length vector, time series, Time measurement, Pattern recognition, Discrete wavelet transforms, Data mining, Character recognition, multivariate time series clustering, Computer science, k-means clustering, pattern clustering, structure-based statistical feature extraction, feature extraction, Clustering algorithms, human motion sequence, Feature extraction, statistical analysis, dimension-reduction procedure]
A Generalization of Proximity Functions for K-Means
Seventh IEEE International Conference on Data Mining
None
2007
K-means is a widely used partitional clustering method. A large amount of effort has been made on finding better proximity (distance) functions for k-means. However, the common characteristics of proximity functions remain unknown. To this end, in this paper, we show that all proximity functions that fit k-means clustering can be generalized as k-means distance, which can be derived by a differentiable convex function. A general proof of sufficient and necessary conditions for k-means distance functions is also provided. In addition, we reveal that k-means has a general uniformization effect; that is, k-means tends to produce clusters with relatively balanced cluster sizes. This uniformization effect of k-means exists regardless of proximity functions. Finally, we have conducted extensive experiments on various real-world data sets, and the results show the evidence of the uniformization effect. Also, we observed that external clustering validation measures, such as entropy and variance of information (VI), have difficulty in measuring clustering quality if data have skewed distributions on class sizes.
[proximity functions, Statistical analysis, differentiable convex function, Conference management, Size measurement, Educational institutions, convex programming, Entropy, information variance, Electronic mail, Information management, Data mining, k-means clustering, entropy, pattern clustering, Clustering algorithms, Statistical distributions, partitional clustering method, k-means distance functions]
Multilevel Belief Propagation for Fast Inference on Markov Random Fields
Seventh IEEE International Conference on Data Mining
None
2007
Graph-based inference plays an important role in many mining and learning tasks. Among all the solvers for this problem, belief propagation (BP) provides a general and efficient way to derive approximate solutions. However, for large scale graphs the computational cost of BP is still demanding. In this paper, we propose a multilevel algorithm to accelerate belief propagation on Markov Random Fields (MRF). First, we coarsen the original graph to get a smaller one. Then, BP is applied on the new graph to get a coarse result. Finally the coarse solution is efficiently refined back to derive the original solution. Unlike traditional multi- resolution approaches, our method features adaptive coarsening and efficient refinement. The above process can be recursively applied to reduce the computational cost remarkably. We theoretically justify the feasibility of our method on Gaussian MRFs, and empirically show that it is also effectual on discrete MRFs. The effectiveness of our method is verified in experiments on various inference tasks.
[Computer vision, Automation, multilevel belief propagation, discrete Markov random field, graph theory, random processes, Gaussian Markov random field, belief maintenance, Data mining, Surges, graph-based inference, Markov random fields, Gaussian processes, Markov processes, Inference algorithms, Large-scale systems, Computational efficiency, Acceleration, Belief propagation]
Disk Aware Discord Discovery: Finding Unusual Time Series in Terabyte Sized Datasets
Seventh IEEE International Conference on Data Mining
None
2007
The problem of finding unusual time series has recently attracted much attention, and several promising methods are now in the literature. However, virtually all proposed methods assume that the data reside in main memory. For many real-world problems this is not be the case. For example, in astronomy, multi-terabyte time series datasets are the norm. Most current algorithms faced with data which cannot fit in main memory resort to multiple scans of the disk/tape and are thus intractable. In this work we show how one particular definition of unusual time series, the time series discord, can be discovered with a disk aware algorithm. The proposed algorithm is exact and requires only two linear scans of the disk with a tiny buffer of main memory. Furthermore, it is very simple to implement. We use the algorithm to provide further evidence of the effectiveness of the discord definition in areas as diverse as astronomy, Web query mining, video surveillance, etc., and show the efficiency of our method on datasets which are many orders of magnitude larger than anything else attempted in the literature.
[buffer storage, multiple scans, disk aware discord discovery, memory resort, data mining, Data engineering, time series, Data mining, terabyte sized datasets, Computer science, USA Councils, unusual time series, Investments, Intrusion detection, Search engines, linear scans, main memory, Video surveillance, tiny buffer, Astronomy, Portfolios, real-world problems, disk aware algorithm]
Binary Matrix Factorization with Applications
Seventh IEEE International Conference on Data Mining
None
2007
An interesting problem in nonnegative matrix factorization (NMF) is to factorize the matrix X which is of some specific class, for example, binary matrix. In this paper, we extend the standard NMF to binary matrix factorization (BMF for short): given a binary matrix X, we want to factorize X into two binary matrices W, H (thus conserving the most important integer property of the objective matrix X) satisfying X ap WH. Two algorithms are studied and compared. These methods rely on a fundamental boundedness property of NMF which we propose and prove. This new property also provides a natural normalization scheme that eliminates the bias of factor matrices. Experiments on both synthetic and real world datasets are conducted to show the competency and effectiveness of BMF.
[Data analysis, integer property, matrix decomposition, natural normalization scheme, Matrix decomposition, Data mining, Application software, nonnegative matrix factorization, Computer science, Proteins, binary matrix factorization, USA Councils, fundamental boundedness property, Clustering algorithms, DNA, Machine learning, bias elimination]
A Semantic Kernel for Semi-structured DocumentS
Seventh IEEE International Conference on Data Mining
None
2007
Natural Language Processing has emerged as an active field of research in the machine learning community. Several methods based on statistical information have been proposed. However, with the linguistic complexity of the texts, semantic-based approaches have been investigated. In this paper, we propose a Semantic Kernel for semi- structured biomedical documents. The semantic meanings of words are extracted using the UMLS framework. The kernel, with a SVM classifier, has been applied to a text categorization task on a medical corpus of free text documents. The results have shown that the Semantic Kernel outperforms the Linear Kernel and the Naive Bayes classifier. Moreover, this kernel was ranked in the top ten of the best algorithms among 44 classification methods at the 2007 CMC Medical NLP International Challenge.
[text analysis, Unified modeling language, Humans, Data mining, statistical information, UMLS framework, medical corpus, Natural language processing, learning (artificial intelligence), Kernel, semantic-based approaches, SVM classifier, Tree data structures, support vector machines, natural language processing, linguistic complexity, 2007 CMC Medical NLP International Challenge, semantic kernel, medical information systems, machine learning, text categorization task, Support vector machines, Text categorization, Machine learning, semantic networks, Feature extraction, semi- structured biomedical documents]
DUSC: Dimensionality Unbiased Subspace Clustering
Seventh IEEE International Conference on Data Mining
None
2007
To gain insight into today's large data resources, data mining provides automatic aggregation techniques. Clustering aims at grouping data such that objects within groups are similar while objects in different groups are dissimilar. In scenarios with many attributes or with noise, clusters are often hidden in subspaces of the data and do not show up in the full dimensional space. For these applications, subspace clustering methods aim at detecting clusters in any sub- space. Existing subspace clustering approaches fall prey to an effect we call dimensionality bias. As dimensionality of subspaces varies, approaches which do not take this effect into account fail to separate clusters from noise. We give a formal definition of dimensionality bias and analyze consequences for subspace clustering. A dimensionality unbiased subspace clustering (DUSC) definition based on statistical foundations is proposed. In thorough experiments on synthetic and real world data, we show that our approach outperforms existing subspace clustering algorithms.
[Multi-stage noise shaping, Density measurement, Clustering methods, data mining, Conference management, Mobile communication, automatic aggregation, Data mining, Computational complexity, dimensionality unbiased subspace clustering, pattern clustering, very large databases, Clustering algorithms, Fires, statistical foundation, Resource management, statistical analysis, large data resources, dimensionality bias]
Finding Predictive Runs with LAPS
Seventh IEEE International Conference on Data Mining
None
2007
We present an extension to the Lasso [6] for binary classification problems with ordered attributes. Inspired by the Fused Lasso [5] and the Group Lasso [7, 3] models, we aim to both discover and model runs (contiguous subgroups of the variables) that are highly predictive. We call the extended model LAPS (the Lasso with Attribute Partition Search). Such problems commonly arise in financial and medical domains, where predictors are time series variables, for example. This paper outlines the formulation of the problem, an algorithm to obtain the model coefficients and experiments showing applicability to practical problems of this type.
[pattern classification, linear logistic regression model, binary classification problem, regression analysis, Predictive models, Time measurement, Partitioning algorithms, Data mining, Statistics, Computer science, optimisation, Animals, USA Councils, ordered attribute partition search, LAPS optimization problem, Protection, search problems, Logistics, predictive run]
Latent Dirichlet Conditional Naive-Bayes Models
Seventh IEEE International Conference on Data Mining
None
2007
In spite of the popularity of probabilistic mixture models for latent structure discovery from data, mixture models do not have a natural mechanism for handling sparsity, where each data point only has a few non-zero observations. In this paper, we introduce conditional naive-Bayes (CNB) models, which generalize naive-Bayes mixture models to naturally handle sparsity by conditioning the model on observed features. Further, we present latent Dirichlet conditional naive-Bayes (LD-CNB) models, which constitute a family of powerful hierarchical Bayesian models for latent structure discovery from sparse data. The proposed family of models are quite general and can work with arbitrary regular exponential family conditional distributions. We present a variational inference based EM algorithm for learning along with special case analyses for Gaussian and discrete distributions. The efficacy of the proposed models are demonstrated by extensive experiments on a wide variety of different datasets.
[Gaussian distributions, latent sparse data structure discovery, data mining, probabilistic mixture models, Gaussian distribution, Data engineering, naive-Bayes mixture models, Data mining, Niobium, Computer science, Bayesian methods, Cities and towns, expectation-maximisation algorithm, sparsity handling, nonzero observations, discrete distributions, Motion pictures, Inference algorithms, data structures, latent Dirichlet conditional naive-Bayes models, Linear discriminant analysis, Bayes methods, EM algorithm, Recommender systems]
Efficient Kernel Discriminant Analysis via Spectral Regression
Seventh IEEE International Conference on Data Mining
None
2007
Linear discriminant analysis (LDA) has been a popular method for extracting features which preserve class separability. The projection vectors are commonly obtained by maximizing the between class covariance and simultaneously minimizing the within class covariance. LDA can be performed either in the original input space or in the reproducing kernel Hilbert space (RKHS) into which data points are mapped, which leads to Kernel Discriminant Analysis (KDA). When the data are highly nonlinear distributed, KDA can achieve better performance than LDA. However, computing the projective functions in KDA involves eigen-decomposition of kernel matrix, which is very expensive when a large number of training samples exist. In this paper, we present a new algorithm for kernel discriminant analysis, called spectral regression kernel discriminant analysis (SRKDA). By using spectral graph analysis, SRKDA casts discriminant analysis into a regression framework which facilitates both efficient computation and the use of regularization techniques. Specifically, SRKDA only needs to solve a set of regularized regression problems and there is no eigenvector computation involved, which is a huge save of computational cost. Our computational analysis shows that SRKDA is 27 times faster than the ordinary KDA. Moreover, the new formulation makes it very easy to develop incremental version of the algorithm which can fully utilize the computational results of the existing training samples. Experiments on face recognition demonstrate the effectiveness and efficiency of the proposed algorithm.
[Algorithm design and analysis, graph theory, regression analysis, spectral analysis, Data mining, eigenvalues and eigenfunctions, eigen-decomposition, spectral graph analysis, feature extraction, kernel matrix, face recognition, spectral regression kernel discriminant analysis, Hilbert space, reproducing kernel Hilbert space, computational analysis, Linear discriminant analysis, Performance analysis, Computational efficiency, linear discriminant analysis, Kernel, class covariance, Hilbert spaces, Vectors, Spectral analysis, matrix algebra, projection vectors, vectors, Feature extraction]
Zonal Co-location Pattern Discovery with Dynamic Parameters
Seventh IEEE International Conference on Data Mining
None
2007
Zonal co-location patterns represent subsets of feature- types that are frequently located in a subset of space (i.e., zone). Discovering zonal spatial co-location patterns is an important problem with many applications in areas such as ecology, public health, and homeland defense. However, discovering these patterns with dynamic parameters (i.e., repeated specification of zone and interest measure values according to user preferences) is computationally complex due to the repetitive mining process. Also, the set of candidate patterns is exponential in the number of feature types, and spatial datasets are huge. Previous studies have focused on discovering global spatial co-location patterns with a fixed interest measure threshold. In this paper, we propose an indexing structure for co-location patterns and propose algorithms (Zoloc-Miner) to discover zonal co- location patterns efficiently for dynamic parameters. Extensive experimental evaluation shows our proposed approaches are scalable, efficient, and outperform naive alternatives.
[Symbiosis, pattern classification, data analysis, zonal co-location pattern discovery, data mining, visual databases, Birds, Environmental factors, Data mining, Association rules, Application software, Computer science, spatial analysis techniques, USA Councils, spatial datasets, Public healthcare, Indexing]
Predicting Blogging Behavior Using Temporal and Social Networks
Seventh IEEE International Conference on Data Mining
None
2007
Modeling the behavior of bloggers is an important problem with various applications in recommender systems, targeted advertising, and event detection. In this paper, we propose three models by combining content, temporal, social dimensions: the general blogging-behavior model, the profile-based blogging-behavior model and the social- network and profile-based blogging-behavior model. The models are based on two regression techniques: Extreme Learning Machine (ELM), and Modified General Regression Neural Network (MGRNN). We choose one of the largest blogs, a political blog, DailyKos1, for our empirical evaluation. Experiments show that the social network and profile-based blogging behavior model with ELM regression techniques produce good results for the most active bloggers and can be used to predict blogging behavior.
[recommender systems, temporal networks, profile-based blogging-behavior model, social-network based blogging-behavior model, event detection, extreme learning machine, regression analysis, social sciences computing, targeted advertising, modified general regression neural network, learning (artificial intelligence), Web sites]
gApprox: Mining Frequent Approximate Patterns from a Massive Network
Seventh IEEE International Conference on Data Mining
None
2007
Recently, there arise a large number of graphs with massive sizes and complex structures in many new applications, such as biological networks, social networks, and the Web, demanding powerful data mining methods. Due to inherent noise or data diversity, it is crucial to address the issue of approximation, if one wants to mine patterns that are potentially interesting with tolerable variations. In this paper, we investigate the problem of mining frequent approximate patterns from a massive network and propose a method called gApprox. gApprox not only finds approximate network patterns, which is the key for many knowledge discovery applications on structural data, but also enriches the library of graph mining methodologies by introducing several novel techniques such as: (1) a complete and redundancy-free strategy to explore the new pattern space faced by gApprox; and (2) transform "frequent in an approximate sense" into an anti-monotonic constraint so that it can be pushed deep into the mining process. Systematic empirical studies on both real and synthetic data sets show that frequent approximate patterns mined from the worm protein-protein interaction network are biologically interesting and gApprox is both effective and efficient.
[approximate network patterns, worm protein-protein interaction network, Social network services, graph mining methodologies, graph theory, data mining, Switches, data mining methods, frequent approximate patterns mining, Amino acids, gApprox, knowledge discovery, Partitioning algorithms, Data mining, Cultural differences, Proteins, Systematics, Complex networks, Libraries, massive network]
Document Transformation for Multi-label Feature Selection in Text Categorization
Seventh IEEE International Conference on Data Mining
None
2007
Feature selection on multi-label documents for automatic text categorization is an under-explored research area. This paper presents a systematic document transformation framework, whereby the multi-label documents are transformed into single-label documents before applying standard feature selection algorithms, to solve the multi-label feature selection problem. Under this framework, we undertake a comparative study on four intuitive document transformation approaches and propose a novel approach called entropy-based label assignment (ELA), which assigns the labels weights to a multi-label document based on label entropy. Three standard feature selection algorithms are utilized for evaluating the document transformation approaches in order to verify its impact on multi-class text categorization problems. Using a SVM classifier and two multi-label evaluation benchmark text collections, we show that the choice of document transformation approaches can significantly influence the performance of multi-class categorization and that our proposed document transformation approach ELA can achieve better performance than all other approaches.
[Algorithm design and analysis, text analysis, multilabel feature selection, support vector machines, label entropy, Entropy, multiclass text categorization, Data mining, document transformation evaluation, multilabel document transformation, Computer science, Support vector machines, single-label document, support vector machine, intuitive document transformation, Text categorization, Asia, multilabel evaluation benchmark text collection, Support vector machine classification, Explosives, entropy-based label assignment, Web sites, multiclass categorization, SVM classifier]
Recommendation via Query Centered Random Walk on K-Partite Graph
Seventh IEEE International Conference on Data Mining
None
2007
This paper presents an algorithm for recommending items using a diverse set of features. The items are recommended by performing a random walk on the k-partite graph constructed from the heterogenous features. To support personalized recommendation, the random walk must be initiated separately for each user, which is computationally demanding given the massive size of the graph. To overcome this problem, we apply multi-way clustering to group together the highly correlated nodes. A recommendation is then made by traversing the subgraph induced by clusters associated with a user's interest. Our experimental results on real data sets demonstrate the efficacy of the proposed algorithm.
[K-partite graph, multiway clustering, graph theory, random processes, query centered random walk, information filtering, Information filtering, Matrix decomposition, Data mining, query processing, recommender system, pattern clustering, Clustering algorithms, Collaboration, Motion pictures, Information filters, Bipartite graph, Books, Recommender systems]
Bandit-Based Algorithms for Budgeted Learning
Seventh IEEE International Conference on Data Mining
None
2007
We explore the problem of budgeted machine learning, in which the learning algorithm has free access to the training examples' labels but has to pay for each attribute that is specified. This learning model is appropriate in many areas, including medical applications. We present new algorithms for choosing which attributes to purchase of which examples in the budgeted learning model based on algorithms for the multi-armed bandit problem. All of our approaches outperformed the current state of the art. Furthermore, we present a new means for selecting an example to purchase after the attribute is selected, instead of selecting an example uniformly at random, which is typically done. Our new example selection method improved performance of all the algorithms we tested, both ours and those in the literature.
[Biomedical equipment, Machine learning algorithms, Costs, Medical services, Data engineering, Data mining, Computer science, USA Councils, bandit-based algorithm, Machine learning, multi armed bandit problem, budgeted machine learning problem, learning (artificial intelligence), Testing]
Extracting Product Comparisons from Discussion Boards
Seventh IEEE International Conference on Data Mining
None
2007
In recent years, product discussion forums have become a rich environment in which consumers and potential adopters exchange views and information. Researchers and practitioners are starting to extract user sentiment about products from user product reviews. Users often compare different products, stating which they like better and why. Extracting information about product comparisons offers a number of challenges; recognizing and normalizing entities (products) in the informal language of blogs and discussion groups require different techniques than those used for entity extraction in the more formal text of newspapers and scientific articles. We present a case study in extracting information about comparisons between running shoes and between cars, describe an effective methodology, and show how it produces insight into how consumers view the running shoe and car markets.
[Visualization, Discussion forums, Text analysis, car markets, Blogs, blogs, Footwear, information retrieval, running shoe market, Marketing management, automobile industry, Data mining, product comparisons, Jacobian matrices, Information science, marketing, information extraction, Text recognition, Web sites, footwear industry, product discussion forums, user product reviews]
Mining Interpretable Human Strategies: A Case Study
Seventh IEEE International Conference on Data Mining
None
2007
This paper focuses on mining human strategies by observing their actions. Our application domain is an HCI study aimed at discovering general strategies used by software users and understanding how such strategies relate to gender and success. We cast this as a sequential pattern discovery problem, where user strategies are manifested as sequential patterns. Problematically, we found that the patterns discovered by standard algorithms were difficult to interpret and provided limited information about high-level strategies. To help interpret the patterns and extract general strategies, we examined multiple ways of clustering the patterns into meaningful groups, which collectively led to interesting findings about user behavior both in terms of gender differences and problem-solving success. As a real-world application of data mining techniques, our work led to the discovery of new strategic patterns that are linked to user success and had not been revealed in more than nine years of manual empirical work. As a case study, our work highlights important research directions for making data mining more accessible to non-experts.
[problem-solving success, Data analysis, sequential pattern discovery, data mining, Manuals, Data engineering, Data mining, Application software, gender differences, HCI, Human computer interaction, Software design, Clustering algorithms, data mining techniques, human computer interaction, Problem-solving, behavioural sciences computing, Pattern analysis, mining interpretable human strategies]
Cross-Mining Binary and Numerical Attributes
Seventh IEEE International Conference on Data Mining
None
2007
We consider the problem of relating itemsets mined on binary attributes of a data set to numerical attributes of the same data. An example is biogeographical data, where the numerical attributes correspond to environmental variables and the binary attributes encode the presence or absence of species in different environments. From the viewpoint of itemset mining, the task is to select a small collection of interesting itemsets using the numerical attributes; from the viewpoint of the numerical attributes, the task is to constrain the search for local patterns (e.g. clusters) using the binary attributes. We give a formal definition of the problem, discuss it theoretically, give a simple constant-factor approximation algorithm, and show by experiments on biogeographical data that the algorithm can capture interesting patterns that would not have been found using either itemset mining or clustering alone.
[approximation theory, Temperature, Demography, data mining, data set, biogeographical data, binary attributes encode, Birds, Data mining, Information science, Itemsets, itemset mining, Clustering algorithms, Motion pictures, Approximation algorithms, cross-mining binary, Bioinformatics, constant-factor approximation algorithm]
Prism: A Primal-Encoding Approach for Frequent Sequence Mining
Seventh IEEE International Conference on Data Mining
None
2007
Sequence mining is one of the fundamental data mining tasks. In this paper we present a novel approach called Prism, for mining frequent sequences. Prism utilizes a vertical approach for enumeration and support counting, based on the novel notion o/prime block encoding, which in turn is based on prime factorization theory. Via an extensive evaluation on both synthetic and real datasets, we show that Prism outperforms popular sequence mining methods like SPADE [10], PrefixSpan [6] and SPAM [2], by an order of magnitude or more.
[Economic indicators, Unsolicited electronic mail, data mining, Encoding, Mathematics, Data mining, Computer science, prime block encoding, Itemsets, Databases, USA Councils, prime factorization theory, block codes, Bioinformatics, frequent sequence mining]
Using Burstiness to Improve Clustering of Topics in News Streams
Seventh IEEE International Conference on Data Mining
None
2007
Specialists who analyze online news have a hard time separating the wheat from the chaff. Moreover, automatic data-mining techniques like clustering of news streams into topical groups can fully recover the underlying true class labels of data if and only if all classes are well separated. In reality, especially for news streams, this is clearly not the case. The question to ask is thus this: if we cannot recover the full C classes by clustering, what is the largest K &lt; C clusters we can find that best resemble the K underlying classes? Using the intuition that bursty topics are more likely to correspond to important events that are of interest to analysts, we propose several new bursty vector space models (B-VSM)for representing a news document. B-VSM takes into account the burstiness (across the full corpus and whole duration) of each constituent word in a document at the time of publication. We benchmarked our B-VSM against the classical TFIDF-VSM on the task of clustering a collection of news stream articles with known topic labels. Experimental results show that B-VSM was able to find the burstiest clusters/topics. Further, it also significantly improved the recall and precision for the top K clusters/topics.
[document handling, information resources, online news analysis, bursty topics, burstiness, Clustering methods, news stream clustering, Nominations and elections, data mining, Telecommunication traffic, news document representation, Data engineering, Functional analysis, topics clustering, topic label, Data mining, Helium, Organizing, automatic data mining, bursty vector space model, media streaming, news stream article]
Bayesian Folding-In with Dirichlet Kernels for PLSI
Seventh IEEE International Conference on Data Mining
None
2007
Probabilistic latent semantic indexing (PLSI) represents documents of a collection as mixture proportions of latent topics, which are learned from the collection by an expectation maximization (EM) algorithm. New documents or queries need to be folded into the latent topic space by a simplified version of the EM-algorithm. During PLSI- Folding-in of a new document, the topic mixtures of the known documents are ignored. This may lead to a suboptimal model of the extended collection. Our new approach incorporates the topic mixtures of the known documents in a Bayesian way during folding- in. That knowledge is modeled as prior distribution over the topic simplex using a kernel density estimate of Dirichlet kernels. We demonstrate the advantages of the new Bayesian folding-in using real text data.
[Text mining, document handling, Costs, indexing, probability, Bayesian folding-in, Biochemistry, PLSI-folding-in, Data mining, known documents, Dirichlet kernels, Graphical models, Runtime, latent topics, Bayesian methods, expectation-maximisation algorithm, Linear discriminant analysis, Bayes methods, expectation maximization algorithm, Kernel, probabilistic latent semantic indexing, Indexing]
Confident Identification of Relevant Objects Based on Nonlinear Rescaling Method and Transductive Inference
Seventh IEEE International Conference on Data Mining
None
2007
We present a novel machine learning algorithm to identify relevant objects from a large amount of data. This approach is driven by linear discrimination based on nonlinear rescaling (NR) method and transductive inference. The NR algorithm for linear discrimination (NRLD) computes both the primal and the dual approximation at each step. The dual variables associated with the given labeled data-set provide important information about the objects in the data-set and play the key role in ordering these objects. A confidence score based on a transductive inference procedure using NRLD is used to rank and identify the relevant objects from a pool of unlabeled data. Experimental results on an unbalanced protein data-set for the drug target prioritization and identification problem are used to illustrate the feasibility of the proposed identification algorithm.
[Drugs, Machine learning algorithms, confident identification, Data mining, relevant objects, Proteins, dual approximation, confidence score, proteins, drug target prioritization, learning (artificial intelligence), nonlinear rescaling method, drug identification problem, approximation theory, drugs, unbalanced protein data-set, linear discrimination, transductive inference, Lagrangian functions, Support vector machines, Computer science, Support vector machine classification, Approximation algorithms, Inference algorithms, data handling, machine learning algorithm]
Training Conditional Random Fields by Periodic Step Size Adaptation for Large-Scale Text Mining
Seventh IEEE International Conference on Data Mining
None
2007
For applications with consecutive incoming training examples, on-line learning has the potential to achieve a likelihood as high as off-line learning without scanning all available training examples and usually has a much smaller memory footprint. To train CRFson-line, this paper presents the Periodic Step size Adaptation (PSA) method to dynamically adjust the learning rates in stochastic gradient descent. We applied our method to three large scale text mining tasks. Experimental results show that PSA outperforms the best off-line algorithm, L-BFGS, by many hundred times, and outperforms the best on-line algorithm, SMD, by an order of magnitude in terms of the number of passes required to scan the training data set.
[Algorithm design and analysis, text analysis, Geographic Information Systems, periodic step size adaptation, Stochastic processes, data mining, large-scale text mining, Data mining, Information science, sequential data classification, Training data, Large-scale systems, Labeling, learning (artificial intelligence), stochastic processes, gradient methods, Text mining, pattern classification, probability, random processes, machine learning, stochastic gradient descent, conditional random field training, conditional probability, Iterative algorithms]
Semi-supervised Document Clustering via Active Learning with Pairwise Constraints
Seventh IEEE International Conference on Data Mining
None
2007
This paper investigates a framework that discovers pair-wise constraints for semi-supervised text document clustering. An active learning approach is proposed to select informative document pairs for obtaining user feedbacks. A gain directed document pair selection method that measures how much we can learn by revealing the relationships between pairs of documents is designed. Three different models, namely, uncertainty model, generation error model, and objective function model are proposed. Language modeling is investigated for representing clusters in the semi-supervised document clustering approach.
[text analysis, language modeling, Data engineering, semi supervised text document clustering, generation error model, Probability distribution, Parametric statistics, Data mining, uncertainty model, objective function model, active learning, pairwise constraints, informative document pairs, pattern clustering, Feedback, Text categorization, Machine learning, gain directed document pair selection method, Gain measurement, Systems engineering and theory, user feedback, learning (artificial intelligence), Research and development management]
Computing Correlation Anomaly Scores Using Stochastic Nearest Neighbors
Seventh IEEE International Conference on Data Mining
None
2007
This paper addresses the task of change analysis of correlated multi-sensor systems. The goal of change analysis is to compute the anomaly score of each sensor when we know that the system has some potential difference from a reference state. Examples include validating the proper performance of various car sensors in the automobile industry. We solve this problem based on a neighborhood preservation principle - If the system is working normally, the neighborhood graph of each sensor is almost invariant against the fluctuations of experimental conditions. Here a neighborhood graph is defined based on the correlation between sensor signals. With the notion of stochastic neighborhood, our method is capable of robustly computing the anomaly score of each sensor under conditions that are hard to be detected by other naive methods.
[Fluctuations, car sensors, neighborhood graph, Laboratories, Stochastic processes, data mining, change analysis, sensor fusion, Sensor systems, automobile industry, Data mining, Automobiles, Unsupervised learning, Nearest neighbor searches, correlation anomaly scores computing, USA Councils, correlated multisensor systems, stochastic nearest neighbors, stochastic processes, Signal analysis]
On Meta-Learning Rule Learning Heuristics
Seventh IEEE International Conference on Data Mining
None
2007
The goal of this paper is to investigate to what extent a rule learning heuristic can be learned from experience. To that end, we let a rule learner learn a large number of rules and record their performance on the test set. Subsequently, we train regression algorithms on predicting the test set performance of a rule from its training set characteristics. We investigate several variations of this basic scenario, including the question whether it is better to predict the performance of the candidate rule itself or of the resulting final rule. Our experiments on a number of independent evaluation sets show that the learned heuristics outperform standard rule learning heuristics. We also analyze their behavior in coverage space.
[Knowledge engineering, Algorithm design and analysis, Shape, rule learner, regression analysis, Search problems, Electronic mail, Data mining, regression algorithms, Machine learning, Prediction algorithms, meta-learning rule learning heuristics, Performance analysis, learning (artificial intelligence), Testing]
Web Site Recommendation Using HTTP Traffic
Seventh IEEE International Conference on Data Mining
None
2007
Collaborative Filtering (CF) is widely used in web recommender systems, while most existing CF applications focus on transactions or page views within a single site. In this paper, we build a recommender system prototype, which suggests web sites to users, by collecting browsing events at routers without neither user nor website effort. 100 million HTTP flows, involving 11, 327 websites, are converted to user-site ratings using access frequency as the implicit rating metric. With this rating dataset, we evaluate six CF algorithms including one proposed algorithm based on IP address locality. Our experiments show that the recommendation from K nearest neighbors (Runn) performs the best by 50% p@10 (precision of top 10) and 53% p@5 (precision of top 5). Although the precision is far from ideal, our preliminary results suggest the potential value of such a centralized web site recommender system.
[HTTP traffic, collaborative filtering, Web recommender systems, information filtering, Information filtering, History, Data mining, Web site recommendation, Nearest neighbor searches, Web pages, Clustering algorithms, Prototypes, K nearest neighbors, Information filters, Web sites, Recommender systems, Portals]
Trend Motif: A Graph Mining Approach for Analysis of Dynamic Complex Networks
Seventh IEEE International Conference on Data Mining
None
2007
Complex networks have been used successfully in scientific disciplines ranging from sociology to microbiology to describe systems of interacting units. Until recently, studies of complex networks have mainly focused on their network topology. However, in many real world applications, the edges and vertices have associated attributes that are frequently represented as vertex or edge weights. Furthermore, these weights are often not static, instead changing with time and forming a time series. Hence, to fully understand the dynamics of the complex network, we have to consider both network topology and related time series data. In this work, we propose a motif mining approach to identify trend motifs for such purposes. Simply stated, a trend motif describes a recurring subgraph where each of its vertices or edges displays similar dynamics over a user- defined period. Given this, each trend motif occurrence can help reveal significant events in a complex system; frequent trend motifs may aid in uncovering dynamic rules of change for the system, and the distribution of trend motifs may characterize the global dynamics of the system. Here, we have developed efficient mining algorithms to extract trend motifs. Our experimental validation using three disparate empirical datasets, ranging from the stock market, world trade, to a protein interaction network, has demonstrated the efficiency and effectiveness of our approach.
[Biotechnology, empirical datasets, Laboratories, data mining, Displays, stock market, Data mining, network topology, Proteins, graph mining, dynamic complex networks, Network topology, protein interaction network, Sociology, Complex networks, world trade, Biology computing, Computer networks]
Analyzing and Detecting Review Spam
Seventh IEEE International Conference on Data Mining
None
2007
Mining of opinions from product reviews, forum posts and blogs is an important research topic with many applications. However, existing research has been focused on extraction, classification and summarization of opinions from these sources. An important issue that has not been studied so far is the opinion spam or the trustworthiness of online opinions. In this paper, we study this issue in the context of product reviews. To our knowledge, there is still no published study on this topic, although Web page spam and email spam have been investigated extensively. We will see that review spam is quite different from Web page spam and email spam, and thus requires different detection techniques. Based on the analysis of 5.8 million reviews and 2.14 million reviewers from amazon.com, we show that review spam is widespread. In this paper, we first present a categorization of spam reviews and then propose several techniques to detect them.
[Web page spam, Data analysis, Blogs, User-generated content, Unsolicited electronic mail, data mining, product review spam detection, spam review categorization, Data mining, Application software, email spam, Computer science, Web pages, Search engines, product reviews opinion mining, Manufacturing, Internet, electronic commerce]
A Computational Approach to Style in American Poetry
Seventh IEEE International Conference on Data Mining
None
2007
We develop a quantitative method to assess the style of American poems and to visualize a collection of poems in relation to one another. Qualitative poetry criticism helped guide our development of metrics that analyze various orthographic, syntactic, and phonemic features. These features are used to discover comprehensive stylistic information from a poem's multi-layered latent structure, and to compute distances between poems in this space. Visualizations provide ready access to the analytical components. We demonstrate our method on several collections of poetry, showing that it better delineates poetry style than the traditional word-occurrence features that are used in typical text analysis algorithms. Our method has potential applications to academic research of texts, to research of the intuitive personal response to poetry, and to making recommendations to readers based on their favorite poems.
[text analysis, Text analysis, Statistical analysis, qualitative poetry analysis, Scholarships, Functional analysis, Data mining, Computer science, Employee welfare, literature, American poetry, Data visualization, poem multilayered latent structure, Natural language processing, poetry style, Principal component analysis]
Change-Point Detection in Time-Series Data Based on Subspace Identification
Seventh IEEE International Conference on Data Mining
None
2007
In this paper, we propose series of algorithms for detecting change points in time-series data based on subspace identification, meaning a geometric approach for estimating linear state-space models behind time-series data. Our algorithms are derived from the principle that the subspace spanned by the columns of an observability matrix and the one spanned by the subsequences of time-series data are approximately equivalent. In this paper, we derive a batch-type algorithm applicable to ordinary time-series data, i.e. consisting of only output series, and then introduce the online version of the algorithm and the extension to be available with input-output time-series data. We illustrate the effectiveness of our algorithms with comparative experiments using some artificial and real datasets.
[Change detection algorithms, geometric approach, linear state-space model estimation, Time series analysis, data mining, change-point detection, subspace identification, time series, Data mining, time-series data, Fault detection, Stochastic systems, Signal processing algorithms, observability matrix, Observability, State estimation, Detection algorithms, Signal analysis, batch-type algorithm]
Optimal Subsequence Bijection
Seventh IEEE International Conference on Data Mining
None
2007
We consider the problem of elastic matching of sequences of real numbers. Since both a query and a target sequence may be noisy, i.e., contain some outlier elements, it is desirable to exclude the outlier elements from matching in order to obtain a robust matching performance. Moreover, in many applications like shape alignment or stereo correspondence it is also desirable to have a one-to-one and onto correspondence (bijection) between the remaining elements. We propose an algorithm that determines the optimal subsequence bijection (OSB) of a query and target sequence. The OSB is efficiently computed since we map the problem's solution to a cheapest path in a DAG (directed acyclic graph). We obtained excellent results on standard benchmark time series datasets. We compared OSB to Dynamic Time Warping (DTW) with and without warping window. We do not claim that OSB is always superior to DTW. However, our results demonstrate that skipping outlier elements as done by OSB can significantly improve matching results for many real datasets. Moreover, OSB is particularly suitable for partial matching. We applied it to the object recognition problem when only parts of contours are given. We obtained sequences representing shapes by representing object contours as sequences of curvatures.
[object recognition, dynamic time warping, Shape, time series datasets, data mining, directed acyclic graph, Time measurement, Noise shaping, Data mining, Object recognition, sequences, query processing, query sequence, USA Councils, directed graphs, Clustering algorithms, optimal subsequence bijection, Euclidean distance, target sequence, Robustness, Dynamic programming, elastic matching]
Connections between Mining Frequent Itemsets and Learning Generative Models
Seventh IEEE International Conference on Data Mining
None
2007
Frequent itemsets mining is a popular framework for pattern discovery. In this framework, given a database of customer transactions, the task is to unearth all patterns in the form of sets of items appearing in a sizable number of transactions. We present a class of models called Itemset Generating Models (or IGMs) that can be used to formally connect the process of frequent item- sets discovery with the learning of generative models. IGMs are specified using simple probability mass functions (over the space of transactions), peaked at specific sets of items and uniform everywhere else. Under such a connection, it is possible to rigorously associate higher frequency patterns with generative models that have greater data likelihoods. This enables a generative model-learning interpretation of frequent itemsets mining. More importantly, it facilitates a statistical significance test which prescribes the minimum frequency needed for a pattern to be considered interesting. We illustrate the effectiveness of our analysis through experiments on standard benchmark data sets.
[transaction processing, customer transactions, data mining, pattern discovery, Probability distribution, Transaction databases, generative model-learning interpretation, Data mining, Association rules, probability mass functions, frequency patterns, Itemsets, data likelihoods, itemset generating models, Benchmark testing, Frequency, Pattern analysis, Joining processes, statistical testing, frequent itemsets mining, statistical significance test, Context modeling]
Solving Consensus and Semi-supervised Clustering Problems Using Nonnegative Matrix Factorization
Seventh IEEE International Conference on Data Mining
None
2007
Consensus clustering and semi-supervised clustering are important extensions of the standard clustering paradigm. Consensus clustering (also known as aggregation of clustering) can improve clustering robustness, deal with distributed and heterogeneous data sources and make use of multiple clustering criteria. Semi-supervised clustering can integrate various forms of background knowledge into clustering. In this paper, we show how consensus and semi-supervised clustering can be formulated within the framework of nonnegative matrix factorization (NMF). We show that this framework yields NMF-based algorithms that are: (1) extremely simple to implement; (2) provably correct and provably convergent. We conduct a wide range of comparative experiments that demonstrate the effectiveness of this NMF-based approach.
[Data analysis, Engineering profession, Partitioning algorithms, matrix decomposition, Matrix decomposition, Data mining, Statistics, nonnegative matrix factorization, consensus clustering, USA Councils, pattern clustering, Clustering algorithms, Robustness, semisupervised clustering problems]
Failure Prediction in IBM BlueGene/L Event Logs
Seventh IEEE International Conference on Data Mining
None
2007
Frequent failures are becoming a serious concern to the community of high-end computing, especially when the applications and the underlying systems rapidly grow in size and complexity. In order to develop effective fault-tolerant strategies, there is a critical need to predict failure events. To this end, we have collected detailed event logs from IBM BlueGene/L, which has 128 K processors, and is currently the fastest supercomputer in the world. In this study, we first show how the event records can be converted into a data set that is appropriate for running classification techniques. Then we apply classifiers on the data, including RIPPER (a rule-based classifier), Support Vector Machines (SVMs), a traditional Nearest Neighbor method, and a customized Nearest Neighbor method. We show that the customized nearest neighbor approach can outperform RIPPER and SVMs in terms of both coverage and precision. The results suggest that the customized nearest neighbor approach can be used to alleviate the impact of failures.
[pattern classification, fault diagnosis, support vector machines, Predictive models, IBM BlueGene/L event logs, Supercomputers, Data mining, parallel machines, frequent failures, Nearest neighbor searches, Support vector machines, Fault tolerance, Runtime, Accuracy, fault-tolerant strategies, RIPPER, System performance, high-end computing, Support vector machine classification, fault tolerant computing, failure prediction, rule-based classifier, nearest neighbor method]
A Text Classification Framework with a Local Feature Ranking for Learning Social Networks
Seventh IEEE International Conference on Data Mining
None
2007
In this paper, a text classifier framework with a feature ranking scheme is proposed to extract social structures from text data. It is assumed that only a small subset of relations between the individuals in a community is known. With this assumption, the social network extraction is translated into a classification problem. The relations between two individuals are represented by merging their document vectors and the given relations are used as labels of training data. By this transformation, a text classifier such as Rocchio is used for learning the unknown relations. We show that there is a link between the intrinsic sparsity of social networks and class imbalance. Furthermore, we show that feature ranking methods usually fail in problem with unbalanced data. In order to deal with this deficiency and re-balance the unbalanced social data, a local feature ranking method, which is called reverse discrimination, is proposed.
[Vocabulary, text analysis, reverse discrimination, Social network services, social structures, Frequency estimation, text classification, Data mining, classification, document vectors, social network extraction, Text categorization, feature extraction, Training data, Web pages, Machine learning, local feature ranking, Search engines, social sciences computing, learning social network, Pattern analysis]
Optimizing Frequency Queries for Data Mining Applications
Seventh IEEE International Conference on Data Mining
None
2007
Data mining algorithms use various Trie and bitmap-based representations to optimize the support (i.e., frequency) counting performance. In this paper, we compare the memory requirements and support counting performance of FP Tree, and Compressed Patricia Trie against several novel variants of vertical bit vectors. First, borrowing ideas from the VLDB domain, we compress vertical bit vectors using WAH encoding. Second, we evaluate the Gray code rank- based transaction reordering scheme, and show that in practice, simple lexicographic ordering, obtained by applying LSB Radix sort, outperforms this scheme. Led by these results, we propose HDO, a novel Hamming-distance-based greedy transaction reordering scheme, and aHDO, a linear-time approximation to HDO. We present results of experiments performed on 15 common datasets with varying degrees of sparseness, and show that HDO- reordered, WAH encoded bit vectors can take as little as 5% of the uncompressed space, while aHDO achieves similar compression on sparse datasets. Finally, with results from over a billion database and data mining style frequency query executions, we show that bitmap-based approaches result in up to hundreds of times faster support counting, and HDO-WAH encoded bitmaps offer the best space-time tradeoff.
[transaction processing, Hamming-distance-based greedy transaction reordering scheme, vertical bit vector compression, data mining, Encoding, Transaction databases, Data mining, Application software, Trie-based representation, simple lexicographic ordering, Computer science, query processing, data mining algorithm, frequency query optimization, Itemsets, Gray code rank-based transaction reordering scheme, Clustering algorithms, Linear approximation, bitmap-based representation, Frequency, Reflective binary codes, linear-time approximation]
Detecting Subdimensional Motifs: An Efficient Algorithm for Generalized Multivariate Pattern Discovery
Seventh IEEE International Conference on Data Mining
None
2007
Discovering recurring patterns in time series data is a fundamental problem for temporal data mining. This paper addresses the problem of locating subdimensional motifs in real-valued, multivariate time series, which requires the simultaneous discovery of sets of recurring patterns along with the corresponding relevant dimensions. While many approaches to motif discovery have been developed, most are restricted to categorical data, univariate time series, or multivariate data in which the temporal patterns span all of the dimensions. In this paper, we present an expected linear-time algorithm that addresses a generalization of multivariate pattern discovery in which each motif may span only a subset of the dimensions. To validate our algorithm, we discuss its theoretical properties and empirically evaluate it using several data sets including synthetic data and motion capture data collected by an on-body iner- tial sensor.
[Multidimensional systems, Multimedia systems, temporal patterns span, multivariate time series, data mining, on-body inertial sensor, univariate time series, Sensor phenomena and characterization, Educational institutions, time series, generalized multivariate pattern discovery, Sensor systems, Data mining, Sparse matrices, recurring patterns, linear-time algorithm, subdimensional motifs, USA Councils, Feature extraction, motif discovery, time series data, Motion analysis, temporal data mining]
Consensus Clusterings
Seventh IEEE International Conference on Data Mining
None
2007
In this paper we address the problem of combining multiple clusterings without access to the underlying features of the data. This process is known in the literature as clustering ensembles, clustering aggregation, or consensus clustering. Consensus clustering yields a stable and robust final clustering that is in agreement with multiple clusterings. We find that an iterative EM-like method is remarkably effective for this problem. We present an iterative algorithm and its variations for finding clustering consensus. An extensive empirical study compares our proposed algorithms with eleven other consensus clustering methods on four data sets using three different clustering performance metrics. The experimental results show that the new ensemble clustering methods produce clusterings that are as good as, and often better than, these other methods.
[Measurement, iterative methods, consensus clusterings, Clustering methods, data mining, multiple clusterings, Partitioning algorithms, clustering ensembles, Data mining, iterative method, iterative algorithm, Computer science, pattern clustering, Clustering algorithms, Motion pictures, clustering aggregation, Iterative algorithms, Robustness, Iterative methods]
High-Speed Function Approximation
Seventh IEEE International Conference on Data Mining
None
2007
We address a new learning problem where the goal is to build a predictive model that minimizes prediction time (the time taken to make a prediction) subject to a constraint on model accuracy. Our solution is a generic framework that leverages existing data mining algorithms without requiring any modifications to these algorithms. We show a first application of our framework to a combustion simulation problem. Our experimental evaluation shows significant improvements over existing methods; prediction time typically is improved by a factor between 2 and 6.
[Costs, predictive model, Computational modeling, Area measurement, data mining algorithms, data mining, prediction time minimisation, Predictive models, high-speed function approximation, Combustion, Function approximation, Data mining, prediction theory, Computer science, combustion, function approximation, learning problem, Prediction algorithms, Polynomials, learning (artificial intelligence), combustion simulation problem]
Weighted Additive Criterion for Linear Dimension Reduction
Seventh IEEE International Conference on Data Mining
None
2007
Linear discriminant analysis (LDA) for dimension reduction has been applied to a wide variety of face recognition tasks. However, it has two major problems. First, it suffers from the small sample size problem when dimensionality is greater than the sample size. Second, it creates subspaces that favor well separated classes over those that are not. In this paper, we propose a simple weighted criterion for linear dimension reduction that addresses the above two problems associated with LDA. In addition, there are well established numerical procedures such as semi-definite programming for efficiently computing the proposed criterion. We demonstrate the efficacy of our proposal and compare it against other competing techniques using a number of examples.
[pattern classification, Face recognition, weighted additive criterion, Data mining, Proposals, Computational complexity, linear dimension reduction, Computer science, Degradation, small sample size problem, Pattern classification, Linear discriminant analysis, statistical analysis, linear discriminant analysis, Principal component analysis]
Local Word Bag Model for Text Categorization
Seventh IEEE International Conference on Data Mining
None
2007
Many text processing applications adopted the bag of words (BOW) model representation of documents, in which each document is represented as a vector of weighted terms or n-grams, and then the cosine distance between two vectors is used as the similarity measurement. Although the great success in information retrieval and text categorization, the conventional BOW model ignores the detailed local text information, i.e. the co-occurrence pattern of words at sentence or paragraph level. In this paper, we propose a novel approach to represent a document as a set of local tf-idf vectors, or what we called local word bags (LWB). By encapsulating local information distributed around a document into multiple LWBs, we can measure the similarity of two documents via the partial match of their corresponding local bags. To perform the matching efficiently, we introduce the local word bag kernel (LWB kernel), a variant of VG-Pyramid match kernel. The new kernel enables the discriminative machine learning methods like SVM to compute the partial matching between two sets of LWBs in linear time after an one time hierarchical clustering procedure over all local bags at the initialization stage. Experiments on real world datasets demonstrate the effectiveness of our new approach.
[text analysis, Laboratories, information retrieval, cosine distance, Extraterrestrial measurements, Data engineering, text categorization, Data mining, documents representation, Machine intelligence, Text processing, word processing, Support vector machines, text processing, local word bag model, VG-Pyramid match kernel, Text categorization, Asia, learning (artificial intelligence), Kernel, discriminative machine learning]
Sampling for Sequential Pattern Mining: From Static Databases to Data Streams
Seventh IEEE International Conference on Data Mining
None
2007
Sequential pattern mining is an active field in the domain of knowledge discovery. Recently, with the constant progress in hardware technologies, real-world databases tend to grow larger and the hypothesis that a database can be loaded into main-memory for sequential pattern mining purpose is no longer valid. Furthermore, the new model of data as a continuous and potentially infinite flow, known as data stream model, call for a pre-processing step to ease the mining operations. Since the database size is the most influential factor for mining algorithms we examine the use of sampling over static databases to get approximate mining results with an upper bound on the error rate. Moreover, we extend these sampling analysis and present an algorithm based on reservoir sampling to cope with sequential pattern mining over data streams. We demonstrate with empirical results that our sampling methods are efficient and that sequence mining remains accurate over static databases and data streams.
[sampling methods, Error analysis, data mining, data streams, knowledge discovery, Transaction databases, Data mining, database management systems, Upper bound, Itemsets, Space technology, static databases, Sampling methods, Reservoirs, sequential pattern mining, Hardware, data sampling, Pattern analysis]
Can the Content of Public News Be Used to Forecast Abnormal Stock Market Behaviour?
Seventh IEEE International Conference on Data Mining
None
2007
A popular theory of markets is that they are efficient: all available information is deemed to provide an accurate valuation of an asset at any time. In this paper, we consider how the content of market- related news articles contributes to such information. Specifically, we mine news articles for terms of interest, and quantify this degree of interest. We then incorporate this measure into traditional models for market index volatility with a view to forecasting whether the incidence of interesting news is correlated with a shock in the index, and thus if the information can be captured to value the underlying asset. We illustrate the methodology on stock market indices for the USA, the UK, and Australia.
[Job shop scheduling, Industrial economics, market index volatility, Finance, data mining, Predictive models, Data mining, Investments, Economic forecasting, public news content, Macroeconomics, Australia, stock markets, abnormal stock market behaviour forecasting, Stock markets, publishing]
An Efficient Spectral Algorithm for Network Community Discovery and Its Applications to Biological and Social Networks
Seventh IEEE International Conference on Data Mining
None
2007
Automatic discovery of community structures in complex networks is a fundamental task in many disciplines, including social science, engineering, and biology. A quantitative measure called modularity (Q) has been proposed to effectively assess the quality of community structures. Several community discovery algorithms have since been developed based on the optimization of Q. However, this optimization problem is NP-hard, and the existing algorithms have a low accuracy or are computationally expensive. In this paper, we present an efficient spectral algorithm for modularity optimization. When tested on a large number of synthetic or real-world networks, and compared to the existing algorithms, our method is efficient and and has a high accuracy. In addition, we have successfully applied our algorithm to detect interesting and meaningful community structures from real-world networks in different domains, including biology, medicine and social science. Due to space limitation, results of these applications are presented in a complete version of the paper available on our Website (http://cse .wustl.edu/ ~jruan/).
[NP-hard, Data engineering, Biology, biological networks, optimisation, quantitative measure, Space technology, biology computing, Clustering algorithms, Complex networks, social sciences computing, Samarium, network community discovery, Laplace equations, modularity optimization, biology, Social network services, community structure discovery algorithms, social networks, medicine, Partitioning algorithms, engineering, social science, Q measurement, spectral algorithm, computational complexity]
Exploration of Link Structure and Community-Based Node Roles in Network Analysis
Seventh IEEE International Conference on Data Mining
None
2007
Communities are nodes in a network that are grouped together based on a common set of properties. While the communities and link structures are often thought to be in alignment, it may not be the case when the communities are defined using other external criterion. In this paper we provide a new way to measure the alignment. We also provide a new metric that can be used to estimate the number of communities to which a node is attached. This metric, along with degree, is used to assign a community-based role to nodes. We demonstrate the usefulness of the community-based node roles by applying them to the influence maximization problem.
[telecommunication links, Data analysis, Social network services, Communities, Data engineering, Data mining, Statistics, maximization problem, Computer science, Q measurement, telecommunication networks, community-based node roles, Writing, Computer networks, network analysis, link structure exploration]
A Support Vector Approach to Censored Targets
Seventh IEEE International Conference on Data Mining
None
2007
Censored targets, such as the time to events in survival analysis, can generally be represented by intervals on the real line. In this paper, we propose a novel support vector technique (named SVCR) for regression on censored targets. SVCR inherits the strengths of support vector methods, such as a globally optimal solution by convex programming, fast training speed and strong generalization capacity. In contrast to ranking approaches to survival analysis, our approach is able not only to achieve superior ordering performance, but also to predict the survival time very well. Experiments show a significant performance improvement when the majority of the training data is censored. Experimental results on several survival analysis datasets demonstrate that SVCR is very competitive against classical survival analysis models.
[Data analysis, Statistical analysis, support vector machines, Oncological surgery, censored target, regression analysis, support vector technique, Data mining, Support vector machines, Training data, Clustering algorithms, Support vector machine classification, survival analysis model, Performance analysis, Kernel]
Understanding Discrete Classifiers with a Case Study in Gene Prediction
Seventh IEEE International Conference on Data Mining
None
2007
The requirement that the models resulting from data mining should be understandable is an uncontroversial requirement. In the data mining literature, however, it plays hardly any role, if at all. In practice, though, understandability is often even more important than, e.g., accuracy. Understandability does not mean that models should be simple. It means that one should be able to understand the predictions of models. In this paper we introduce tools to understand arbitrary classifiers defined on discrete data. More in particular, we introduce Explanations that provide insight at a local level. They explain why a classifier classifies a data point as it does. For global insight, we introduce attribute weights. The higher the weight of an attribute, the more often it is decisive in the classification of a data point. To illustrate our tools, we describe a case study in the prediction of small genes. This is a notoriously hard problem in bioinformatics.
[Data analysis, gene prediction, discrete classifiers, Biological system modeling, Area measurement, Laboratories, data mining, Predictive models, Data mining, Databases, arbitrary classifiers, biology computing, bioinformatics, Bioinformatics, Classification tree analysis, Testing]
Statistical Learning Algorithm for Tree Similarity
Seventh IEEE International Conference on Data Mining
None
2007
Tree edit distance is one of the most frequently used distance measures for comparing trees. When using the tree edit distance, we need to determine the cost of each operation, but this is a labor-intensive and highly skilled task. This paper proposes an algorithm for learning the costs of tree edit operations from training data consisting of pairs of similar trees. To formalize the cost learning problem, we define a probabilistic model for tree alignment that is a variant of tree edit distance. Then, the parameters of the model are estimated using the expectation maximization (EM) technique. In this paper, we develop an algorithm for parameter learning that is polynomial in time (O{mn2d6)) and space (O{n2d4)) where n, d, and m represent the size of the trees, the maximum degree of trees, and the number of training pairs of trees, respectively.
[Costs, Statistical learning, trees (mathematics), tree similarity, tree edit distance, cost learning problem, Data mining, expectation maximization technique, probabilistic model, Filters, statistical learning algorithm, XML, Training data, Filtering algorithms, expectation-maximisation algorithm, distance measures, Polynomials, learning (artificial intelligence), Informatics, Classification tree analysis, computational complexity]
A Novel Criterion for Onset Detection: Differential Information Redundancy with Application to Human Movement Initiation
Seventh IEEE International Conference on Data Mining
None
2007
A new detection criterion based on the change in the marginal information redundancy is presented. By establishing a link with information theory we are able to give an intuitive interpretation of our criterion. The usefulness of the new criterion is demonstrated for a case study of human movement initiation detection from force and torque signals in activity of daily living tasks. Using the new criterion, we achieve a performance that is more in agreement with expert decisions compared with traditional thresholding techniques and the advanced wavelet-based detector and energy detectors.
[Wavelet transforms, energy detectors, force signals, Torque, Shape, information management, Muscles, Predictive models, wavelet-based detector, differential information redundancy, Data mining, Background noise, human movement initiation, onset detection, Continuous wavelet transforms, Detectors, torque signals, expert decisions, information theory, medical administrative data processing, redundancy, Information theory]
Using Significant, Positively Associated and Relatively Class Correlated Rules for Associative Classification of Imbalanced Datasets
Seventh IEEE International Conference on Data Mining
None
2007
The application of association rule mining to classification has led to a new family of classifiers which are often referred to as "associative classifiers (ACs)". An advantage of ACs is that they are rule-based and thus lend themselves to an easier interpretation. Rule-based classifiers can play a very important role in applications such as medical diagnosis and fraud detection where "imbalanced data sets" are the norm and not the exception. The focus of this paper is to extend and modify ACs for classification on imbalanced data sets using only statistical techniques. We combine the use of statistically significant rules with a new measure, the Class Correlation Ratio (CCR), to build an AC which we call SPARCCC. Experiments show that in terms of classification quality, SPARCCC performs comparably on balanced datasets and outperforms other AC techniques on imbalanced data sets. It also has a significantly smaller rule base and is much more computationally efficient.
[pattern classification, imbalanced datasets, Law, data mining, Data mining, Association rules, Medical diagnosis, Information technology, relatively class correlated rules, association rule mining, imbalanced data sets, associative classification, class correlation ratio, rule-based classifiers, statistically significant rules, associative classifiers, Legal factors, Testing]
Preserving Privacy through Data Generation
Seventh IEEE International Conference on Data Mining
None
2007
Many databases will not or can not be disclosed without strong guarantees that no sensitive information can be extracted. To address this concern several data perturbation techniques have been proposed. However, it has been shown that either sensitive information can still be extracted from the perturbed data with little prior knowledge, or that many patterns are lost. In this paper we show that generating new data is an inherently safer alternative. We present a data generator based on the models obtained by the MDL-based KRIMP (Siebes et al., 2006) algorithm. These are accurate representations of the data distributions and can thus be used to generate data with the same characteristics as the original data. Experimental results show a very large pattern-similarity between the generated and the original data, ensuring that viable conclusions can be drawn from the anonymised data. Furthermore, anonymity is guaranteed for suited databases and the quality-privacy trade-off can be balanced explicitly.
[Heart, Data privacy, databases, Multidimensional systems, Eyes, privacy preservation, data mining, data anonymity, data perturbation, data generator, quality-privacy trade-off, Transaction databases, Data mining, database management systems, very large pattern-similarity, data generation, Perturbation methods, data distribution representation, sensitive information extraction, Character generation, Concrete, data privacy, Protection, MDL-based KRIMP algorithm]
Transitional Patterns and Their Significant Milestones
Seventh IEEE International Conference on Data Mining
None
2007
Mining frequent patterns in transaction databases has been studied extensively in data mining research. However, most of the existing frequent pattern mining algorithms do not consider the time stamps associated with the transactions. In this paper, we extend the existing frequent pattern mining framework to take into account the time stamp of each transaction and discover patterns whose frequency dramatically changes over time. We define a new type of patterns, called transitional patterns, to capture the dynamic behavior of frequent patterns in a transaction database. Transitional patterns include both positive and negative transitional patterns. Their frequencies increase/decrease dramatically at some time points of a transaction database. We introduce the concept of significant milestones for a transitional pattern, which are time points at which the frequency of the pattern changes most significantly. Moreover, we develop an algorithm to mine from a transaction database the set of transitional patterns along with their significant milestones. Our experimental studies on real-world databases illustrate that mining positive and negative transitional patterns is highly promising as a practical and useful approach to discovering novel and interesting knowledge from large databases.
[Computer science, transaction processing, Itemsets, data mining, transitional pattern, Frequency, Data engineering, Transaction databases, Data mining, Association rules, frequent pattern mining, transaction database]
Topical N-Grams: Phrase and Topic Discovery, with an Application to Information Retrieval
Seventh IEEE International Conference on Data Mining
None
2007
Most topic models, such as latent Dirichlet allocation, rely on the bag-of-words assumption. However, word order and phrases are often critical to capturing the meaning of text in many text mining tasks. This paper presents topical n-grams, a topic model that discovers topics as well as topical phrases. The probabilistic model generates words in their textual order by, for each word, first sampling a topic, then sampling its status as a unigram or bigram, and then sampling the word from a topic-specific unigram or bigram distribution. Thus our model can model "white house" as a special meaning phrase in the 'politics' topic, but not in the 'real estate' topic. Successive bigrams form longer phrases. We present experiments showing meaningful phrases and more interpretable topics from the NIPS data and improved information retrieval performance on a TREC collection.
[Text mining, topic-specific unigram distribution, Vocabulary, text analysis, sampling methods, topic model, data mining, probability, Artificial neural networks, information retrieval, Information retrieval, Data mining, Biological neural networks, probabilistic model, phrase/topic discovery, topic-specific bigram distribution, topical n-grams, Neuroscience, word sampling, Sampling methods, Natural language processing, text mining, Context modeling]
Mechanism Design for Clustering Aggregation by Selfish Systems
Seventh IEEE International Conference on Data Mining
None
2007
We propose a market mechanism that can be implemented on clustering aggregation problem among selfish systems, which tend to lie about their correct clustering during aggregation process. Our study is the preliminary step toward the development of robust distributed data mining among selfish systems.
[market mechanism design, Multiagent systems, data mining, game theory, marketing data processing, Data mining, Game theory, banking, Computer science, Couplings, Privacy, robust distributed data mining, Aggregates, Clustering algorithms, Robustness, selfish system clustering aggregation, Protection]
estMax: Tracing Maximal Frequent Itemsets over Online Data Streams
Seventh IEEE International Conference on Data Mining
None
2007
In general, the number of frequent itemsets in a data set is very large. In order to represent them in more compact notation, closed or maximal frequent itemsets (MFIs) are used. However, the characteristics of a data stream make such a task be more difficult. For this purpose, this paper proposes a method called estMax that can trace the set of MFIs over a data stream. The proposed method maintains the set of frequent itemsets by a prefix tree and extracts all of MFIs without any additional superset/subset checking mechanism. Upon processing a newly generated transaction, its longest matched frequent itemsets are marked in a prefix tree as candidates for MFIs. At the same time, if any subset of these newly marked itemsets has been already marked as a candidate MFI, it is cleared as well. By employing this additional step, it is possible to extract the set of MFIs at any moment. The performance of the proposed method is comparatively analyzed by a series of experiments to identify its various characteristics.
[Data analysis, data mining, trees (mathematics), superset-subset checking mechanism, set theory, prefix tree, Data mining, Computer science, maximal frequent itemsets, Itemsets, online data streams, Performance analysis, estMax method]
Locally Constrained Support Vector Clustering
Seventh IEEE International Conference on Data Mining
None
2007
Support vector clustering transforms the data into a high dimensional feature space, where a decision function is computed. In the original space, the function outlines the boundaries of higher density regions, naturally splitting the data into individual clusters. The method, however, though theoretically sound, has certain drawbacks which make it not so appealing to the practitioner. Namely, it is unstable in the presence of outliers and it is hard to control the number of clusters that it identifies. Parametrizing the algorithm incorrectly in noisy settings, can either disguise some objectively present clusters in the data, or can identify a large number of small and nonintuitive clusters. Here, we explore the properties of the data in small regions building a mixture of factor analyzers. The obtained information is used to regularize the complexity of the outlined cluster boundaries, by assigning suitable weighting to each example. The approach is demonstrated to be less susceptible to noise and to outline better interpretable clusters than support vector clustering alone.
[data analysis, support vector machines, nonintuitive clusters, data cluster, Data engineering, cluster boundary, Data mining, Support vector machines, Computer science, high dimensional feature space, Acoustic noise, USA Councils, pattern clustering, Static VAr compensators, Clustering algorithms, decision function, Labeling, Kernel, constrained support vector clustering]
Cocktail Ensemble for Regression
Seventh IEEE International Conference on Data Mining
None
2007
This paper is motivated to improve the performance of individual ensembles using a hybrid mechanism in the regression setting. Based on an error-ambiguity decomposition, we formally analyze the optimal linear combination of two base ensembles, which is then extended to multiple individual ensembles via pairwise combinations. The Cocktail ensemble approach is proposed based on this analysis. Experiments over a broad range of data sets show that the proposed approach outperforms the individual ensembles, two other methods of ensemble combination, and two state-of-the-art regression approaches.
[error-ambiguity decomposition, Laboratories, Stochastic processes, data mining, Software performance, regression analysis, Boosting, Data mining, Information technology, Cocktail ensemble, pairwise combination, regression, Computer errors, data sets, Computational efficiency, Bagging, Analysis of variance]
Incremental Subspace Clustering over Multiple Data Streams
Seventh IEEE International Conference on Data Mining
None
2007
Data streams are often locally correlated, with a subset of streams exhibiting coherent patterns over a subset of time points. Subspace clustering can discover clusters of objects in different subspaces. However, traditional sub- space clustering algorithms for static data sets are not readily used for incremental clustering, and is very expensive for frequent re-clustering over dynamically changing stream data. In this paper, we present an efficient incremental sub- space clustering algorithm for multiple streams over sliding windows. Our algorithm detects all the delta-CC-Clusters, which capture the coherent changing patterns among a set of streams over a set of time points. delta-CC'-Cluster s are incrementally generated by traversing a directed acyclic graph pDAG. We propose efficient insertion and deletion operations to update the pDAG dynamically. In addition, effective pruning techniques are applied to reduce the search space. Experiments on real data sets demonstrate the performance of our algorithm.
[Algorithm design and analysis, Data analysis, delta-CC-Clusters, data analysis, static data sets, Clustering methods, Subspace constraints, effective pruning technique, Telecommunication traffic, Data structures, Data mining, sliding windows, Boolean functions, multiple data streams, pattern clustering, Clustering algorithms, Monitoring, incremental subspace clustering]
Noise Modeling with Associative Corruption Rules
Seventh IEEE International Conference on Data Mining
None
2007
This paper presents an active learning approach to the problem of systematic noise inference and noise elimination, specifically the inference of Associated Corruption (AC) rules. AC rules are defined to simulate a common noise formation process in real-world data, in which the occurrence of an error on one attribute is dependent on several other attribute values. Our approach consists of two algorithms, Associative Corruption Forward (ACF) and Associative Corruption Backward (ACB). Algorithm ACF is proposed for noise inference, and ACB is designed for noise elimination. The experimental results show that the ACF algorithm can infer the noise formation correctly, and ACB indeed enhances the data quality for supervised learning.
[Noise figure, Data privacy, active learning approach, noise elimination, Data preprocessing, associative corruption forward, data mining, supervised learning, noise modeling, Data mining, inference mechanisms, Computer science, data quality, USA Councils, Active noise reduction, Supervised learning, associative corruption rules, Inference algorithms, associative corruption backward, Noise robustness, learning (artificial intelligence), noise formation process, systematic noise inference]
Co-ranking Authors and Documents in a Heterogeneous Network
Seventh IEEE International Conference on Data Mining
None
2007
Recent graph-theoretic approaches have demonstrated remarkable successes for ranking networked entities, but most of their applications are limited to homogeneous networks such as the network of citations between publications. This paper proposes a novel method for co-ranking authors and their publications using several networks: the social network connecting the authors, the citation network connecting the publications, as well as the authorship network that ties the previous two together. The new co-ranking framework is based on coupling two random walks, that separately rank authors and documents following the PageRankparadigm. As a result, improved rankings of documents and their authors depend on each other in a mutually reinforcing way, thus taking advantage of the additional information implicit in the heterogeneous network of authors and documents.
[Performance evaluation, document handling, authorship network, random walks, Social network services, information retrieval, random processes, PageRank, Data engineering, citation network, Data mining, Application software, Computer science, social network, coranking author, document ranking, Bibliometrics, heterogeneous network, Web pages, publication, Computer networks, Joining processes]
Discovering Temporal Communities from Social Network Documents
Seventh IEEE International Conference on Data Mining
None
2007
This paper studies the discovery of communities from social network documents produced over time, addressing the discovery of temporal trends in community memberships. We first formulate static community discovery at a single time period as a tripartite graph partitioning problem. Then we propose to discover the temporal communities by threading the statically derived communities in different time periods using a new constrained partitioning algorithm, which partitions graphs based on topology as well as prior information regarding vertex membership. We evaluate the proposed approach on synthetic datasets and a real-world dataset prepared from the CiteSeer.
[document handling, social network document, Social network services, Communities, graph theory, synthetic dataset, static community discovery, Data engineering, vertex membership, Partitioning algorithms, Data mining, real-world dataset, Computer science, Information science, temporal community, tripartite graph partitioning, Clustering algorithms, community membership, Cost function, constrained partitioning algorithm, Computer networks, social sciences computing, constraint handling]
Efficient Discovery of Frequent Approximate Sequential Patterns
Seventh IEEE International Conference on Data Mining
None
2007
We propose an efficient algorithm for mining frequent approximate sequential patterns under the Hamming distance model. Our algorithm gains its efficiency by adopting a "break-down-and-build-up" methodology. The "breakdown" is based on the observation that all occurrences of a frequent pattern can be classified into groups, which we call strands. We developed efficient algorithms to quickly mine out all strands by iterative growth. In the "build-up" stage, these strands are grouped up to form the support sets from which all approximate patterns would be identified. A salient feature of our algorithm is its ability to grow the frequent patterns by iteratively assembling building blocks of significant sizes in a local search fashion. By avoiding incremental growth and global search, we achieve greater efficiency without losing the completeness of the mining result. Our experimental studies demonstrate that our algorithm is efficient in mining globally repeating approximate sequential patterns that would have been missed by existing methods.
[Sequences, Data analysis, Hamming distance, Genomics, data mining, break-down-and-build-up methodology, Data mining, frequent approximate sequential patterns, Hamming distance model, approximate sequential patterns, DNA, Iterative algorithms, global search, Bioinformatics, Pattern analysis, incremental growth, Assembly, search problems]
Active Learning from Data Streams
Seventh IEEE International Conference on Data Mining
None
2007
In this paper, we address a new research problem on active learning from data streams where data volumes grow continuously and labeling all data is considered expensive and impractical. The objective is to label a small portion of stream data from which a model is derived to predict newly arrived instances as accurate as possible. In order to tackle the challenges raised by data streams' dynamic nature, we propose a classifier ensembling based active learning framework which selectively labels instances from data streams to build an accurate classifier. A minimal variance principle is introduced to guide instance labeling from data streams. In addition, a weight updating rule is derived to ensure that our instance labeling process can adaptively adjust to dynamic drifting concepts in the data. Experimental results on synthetic and real-world data demonstrate the performances of the proposed efforts in comparison with other simple approaches.
[pattern classification, Uncertainty, Decision making, data streams, Predictive models, Data engineering, Data mining, Association rules, instance labeling, Computer science, Accuracy, USA Councils, data handling, classifier ensembling based active learning framework, Labeling, minimal variance principle]
Lazy Bagging for Classifying Imbalanced Data
Seventh IEEE International Conference on Data Mining
None
2007
In this paper, we propose a lazy bagging (LB) design, which builds bootstrap replicate bags based on the characteristics of the test instances. Upon receiving a test instance Ik, LB will trim bootstrap bags by taking Ik's nearest neighbors in the training set into consideration. Our hypothesis is that an unlabeled instance's nearest neighbors provide valuable information for learners to refine their local decision boundaries for classifying this instance. By taking full advantage of I<sub>k</sub>'s nearest neighbors, the base learners are able to receive less bias and variance in classifying I<sub>k</sub>. This strategy is beneficial for classifying imbalanced data because refining local decision boundaries can help a learner reduce its inherent bias towards the majority class and improve its performance on minority class examples. Our experimental results will confirm that LB outperforms C4.5 and TB in terms of reducing classification error, and most importantly this error reduction is largely contributed from LB's improvement on minority class examples.
[pattern classification, supervised classification learning, Data engineering, Data mining, Nearest neighbor searches, Computer science, Design engineering, lazy bagging design, Accuracy, Decision theory, imbalanced data classification, local decision boundary, Decision trees, learning (artificial intelligence), Bagging, test instance, Testing]
Welcome Message from the Conference Chairs
2008 Eighth IEEE International Conference on Data Mining
None
2008
Presents the introductory welcome message from the conference proceedings.
[]
Welcome to ICDM 2008
2008 Eighth IEEE International Conference on Data Mining
None
2008
Presents the introductory welcome message from the conference proceedings.
[]
Conference organization
2008 Eighth IEEE International Conference on Data Mining
None
2008
Provides a listing of current committee members and society officers.
[]
Steering Committee
2008 Eighth IEEE International Conference on Data Mining
None
2008
Provides a listing of current committee members.
[]
Program Committee
2008 Eighth IEEE International Conference on Data Mining
None
2008
Provides a listing of current committee members.
[]
Invited talks
2008 Eighth IEEE International Conference on Data Mining
None
2008
Provides an abstract for each of the invited presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
Workshops
2008 Eighth IEEE International Conference on Data Mining
None
2008
Provides an abstract for each of the workshop presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
Panel session ICDM 2008: Introduction to ICDM'08 Panel Session Social Networks and Data Mining: Where's the Beef?
2008 Eighth IEEE International Conference on Data Mining
None
2008
Provides an abstract of the panel presentation and a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.
[]
On-line LDA: Adaptive Topic Models for Mining Text Streams with Applications to Topic Detection and Tracking
2008 Eighth IEEE International Conference on Data Mining
None
2008
This paper presents online topic model (OLDA), a topic model that automatically captures the thematic patterns and identifies emerging topics of text streams and their changes over time. Our approach allows the topic modeling framework, specifically the latent Dirichlet allocation (LDA) model, to work in an online fashion such that it incrementally builds an up-to-date model (mixture of topics per document and mixture of words per topic) when a new document (or a set of documents) appears. A solution based on the empirical Bayes method is proposed. The idea is to incrementally update the current model according to the information inferred from the new stream of data with no need to access previous data. The dynamics of the proposed approach also provide an efficient mean to track the topics over time and detect the emerging topics in real time. Our method is evaluated both qualitatively and quantitatively using benchmark datasets. In our experiments, the OLDA has discovered interesting patterns by just analyzing a fraction of data at a time. Our tests also prove the ability of OLDA to align the topics across the epochs with which the evolution of the topics over time is captured. The OLDA is also comparable to, and sometimes better than, the original LDA in predicting the likelihood of unseen documents.
[latent Dirichlet allocation, text analysis, adaptive topic model, data mining, pattern discovery, topic tracking, Data mining, Application software, Yarn, Organizing, Computer science, empirical Bayes method, Software libraries, USA Councils, Benchmark testing, topic detection, Linear discriminant analysis, Bayes methods, text stream mining, Pattern analysis, online LDA]
Paired Learners for Concept Drift
2008 Eighth IEEE International Conference on Data Mining
None
2008
To cope with concept drift, we paired a stable online learner with a reactive one. A stable learner predicts based on all of its experience, whereas are active learner predicts based on its experience over a short, recent window of time. The method of paired learning uses differences in accuracy between the two learners over this window to determine when to replace the current stable learner, since the stable learner performs worse than does there active learner when the target concept changes. While the method uses the reactive learner as an indicator of drift, it uses the stable learner to predict, since the stable learner performs better than does the reactive learner when acquiring target concept. Experimental results support these assertions. We evaluated the method by making direct comparisons to dynamic weighted majority, accuracy weighted ensemble, and streaming ensemble algorithm (SEA) using two synthetic problems, the Stagger concepts and the SEA concepts, and three real-world data sets: meeting scheduling, electricity prediction, and malware detection. Results suggest that, on these problems, paired learners outperformed or performed comparably to methods more costly in time and space.
[Algorithm design and analysis, invasive software, Heuristic algorithms, paired learners, reactive learner, Data mining, electricity prediction, active learner, accuracy weighted ensemble, Stagger concepts, USA Councils, online learner, scheduling, learning (artificial intelligence), Stability, time-changing data streams, Dynamic scheduling, malware detection, dynamic weighted majority, Scheduling algorithm, Computer science, concept drift, streaming ensemble algorithm, real-world data sets, online learning]
Predicting Future Decision Trees from Evolving Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
Recognizing and analyzing change is an important human virtue because it enables us to anticipate future scenarios and thus allows us to act pro-actively. One approach to understand change within a domain is to analyze how models and patterns evolve. Knowing how a model changes over time is suggesting to ask: Can we use this knowledge to learn a model in anticipation, such that it better reflects the near-future characteristics of an evolving domain? In this paper we provide an answer to this question by presenting an algorithm which predicts future decision trees based on a model of change. In particular, this algorithm encompasses a novel approach to change mining which is based on analyzing the changes of the decisions made during model learning. The proposed approach can also be applied to other types of classifiers and thus provides a basis for future research. We present our first experimental results which show that anticipated decision trees have the potential to outperform trees learned on the most recent data.
[Algorithm design and analysis, Evolving Data, Humans, Predictive models, Data warehouses, Data mining, model learning, Computer science, evolving data, Change Mining, future decision tree prediction, decision trees, Prediction algorithms, data handling, Decision Trees, Decision trees, Pattern analysis, Intelligent systems]
A Randomized Approach for Approximating the Number of Frequent Sets
2008 Eighth IEEE International Conference on Data Mining
None
2008
We investigate the problem of counting the number of frequent (item)sets - a problem known to be intractable in terms of an exact polynomial time computation. In this paper, we show that it is in general also hard to approximate. Subsequently, a randomized counting algorithm is developed using the Markov chain Monte Carlo method. While for general inputs an exponential running time is needed in order to guarantee a certain approximation bound, we empirically show that the algorithm still has the desired accuracy on real-world datasets when its running time is capped polynomially.
[counting, approximation theory, randomized approach, approximation, polynomials, polynomial time computation, Explosions, set theory, randomized counting algorithm, Data mining, Association rules, frequent itemset mining, Monte Carlo methods, Markov chain Monte Carlo method, approximation bound, randomized algorithms, Markov processes, Frequency, Approximation algorithms, frequent sets number, Polynomials]
A Non-parametric Semi-supervised Discretization Method
2008 Eighth IEEE International Conference on Data Mining
None
2008
Semi-supervised classification methods aim to exploit labelled and unlabelled examples to train a predictive model. Most of these approaches make assumptions on the distribution of classes. This article first proposes a new semi-supervised discretization method which adopts very low informative prior on data. This method discretizes the numerical domain of a continuous input variable, while keeping the information relative to the prediction of classes. Then, an in-depth comparison of this semi-supervised method with the original supervised MODL approach is presented. We demonstrate that the semi-supervised approach is asymptotically equivalent to the supervised approach, improved with a post-optimization of the intervals bounds location.
[Maximum likelihood estimation, pattern classification, minimal optimized description length, predictive model, Input variables, supervised MODL, semisupervised learning, Optimization methods, data mining, Predictive models, nonparametric semisupervised discretization, Classification algorithms, Data mining, semisupervised classification, Non-parametric, Bayesian methods, Discretization, Semi-supervised, Semisupervised learning, Iterative algorithms, learning (artificial intelligence), Bonding]
Non-negative Matrix Factorization on Manifold
2008 Eighth IEEE International Conference on Data Mining
None
2008
Recently non-negative matrix factorization (NMF) has received a lot of attentions in information retrieval, computer vision and pattern recognition. NMF aims to find two non-negative matrices whose product can well approximate the original matrix. The sizes of these two matrices are usually smaller than the original matrix. This results in a compressed version of the original data matrix. The solution of NMF yields a natural parts-based representation for the data. When NMF is applied for data representation, a major disadvantage is that it fails to consider the geometric structure in the data. In this paper, we develop a graph based approach for parts-based data representation in order to overcome this limitation. We construct an affinity graph to encode the geometrical information and seek a matrix factorization which respects the graph structure. We demonstrate the success of this novel algorithm by applying it on real world problems.
[Computer vision, Face recognition, graph theory, original data matrix, Humans, information retrieval, Information retrieval, Pattern recognition, matrix decomposition, Matrix decomposition, Data mining, parts-based data representation, Convergence, Clustering algorithms, computer vision, non negative matrix factorization, data structures, data handling, Singular value decomposition, pattern recognition, affinity graph]
Anti-monotonic Overlap-Graph Support Measures
2008 Eighth IEEE International Conference on Data Mining
None
2008
In graph mining, a frequency measure is anti-monotonic if the frequency of a pattern never exceeds the frequency of a subpattern. The efficiency and correctness of most graph pattern miners relies critically on this property. We study the case where the dataset is a single graph. Vanetik, Gudes and Shimony already gave sufficient and necessary conditions for anti-monotonicity of measures depending only on the edge-overlaps between the instances of the pattern in a labeled graph. We extend these results to homomorphisms, isomorphisms and homeomorphisms on both labeled and unlabeled, directed and undirected graphs, for vertex and edge overlap. We show a set of reductions between the different morphisms that preserve overlap. We also prove that the popular maximum independent set measure assigns the minimal possible meaningful frequency, introduce a new measure based on the minimum clique partition that assigns the maximum possible meaningful frequency and introduce a new measure sandwiched between the former two based on the poly-time computable Lovasz thetas-function.
[Logic programming, Social network services, data mining, anti-monotinicity, Frequency measurement, anti-monotonic overlap-graph support measures, Data mining, Sufficient conditions, overlap graph, frequency measure, minimum clique partition, graph support measure, directed graphs, graph pattern mining, undirected graphs, labeled graph, Lovasz thetas-function, Pattern matching]
SeqStream: Mining Closed Sequential Patterns over Stream Sliding Windows
2008 Eighth IEEE International Conference on Data Mining
None
2008
Previous studies have shown mining closed patterns provides more benefits than mining the complete set of frequent patterns, since closed pattern mining leads to more compact results and more efficient algorithms. It is quite useful in a data stream environment where memory and computation power are major concerns. This paper studies the problem of mining closed sequential patterns over data stream sliding windows. A synopsis structure IST (Inverse Closed Sequence Tree) is designed to keep inverse closed sequential patterns in current window. An efficient algorithm SeqStream is developed to mine closed sequential patterns in stream windows incrementally, and various novel strategies are adopted in SeqStream to prune search space aggressively. Extensive experiments on both real and synthetic data sets show that SeqStream outperforms PrefixSpan, CloSpan and BIDE by a factor of about one to two orders of magnitude.
[CloSpan, Stream Sliding Windows, inverse closed sequence tree, Laboratories, Software algorithms, data mining, Educational technology, Data engineering, Electromagnetic compatibility, closed sequential pattern, Data mining, data stream, sliding window, Computer science, PrefixSpan, Databases, BIDE, data structures, closed sequential patterns, Computer science education, Monitoring, SeqStream]
SPARCL: Efficient and Effective Shape-Based Clustering
2008 Eighth IEEE International Conference on Data Mining
None
2008
Clustering is one of the fundamental data mining tasks. Many different clustering paradigms have been developed over the years, which include partitional, hierarchical, mixture model based, density-based, spectral, subspace, and so on. The focus of this paper is on full-dimensional, arbitrary shaped clusters. Existing methods for this problem suffer either in terms of the memory or time complexity (quadratic or even cubic). This shortcoming has restricted these algorithms to datasets of moderate sizes. In this paper we propose SPARCL, a simple and scalable algorithm for finding clusters with arbitrary shapes and sizes, and it has linear space and time complexity. SPARCL consists of two stages - the first stage runs a carefully initialized version of the K-means algorithm to generate many small seed clusters. The second stage iteratively merges the generated clusters to obtain the final shape-based clusters. Experiments were conducted on a variety of datasets to highlight the effectiveness, efficiency, and scalability of our approach. On the large datasets SPARCL is an order of magnitude faster than the best existing approaches.
[Chaos, SPARCL algorithm, iterative methods, memory complexity, Shape, Scalability, Clustering methods, data mining, time complexity, Partitioning algorithms, Data mining, iterative method, Computer science, K-means algorithm, Image analysis, linear space complexity, pattern clustering, Clustering algorithms, full-dimensional arbitrary shaped cluster, shape-based clustering, Iterative algorithms, computational complexity]
Graph OLAP: Towards Online Analytical Processing on Graphs
2008 Eighth IEEE International Conference on Data Mining
None
2008
OLAP (On-Line Analytical Processing) is an important notion in data analysis. Recently, more and more graph or networked data sources come into being. There exists a similar need to deploy graph analysis from different perspectives and with multiple granularities. However, traditional OLAP technology cannot handle such demands because it does not consider the links among individual data tuples. In this paper, we develop a novel graph OLAP framework, which presents a multi-dimensional and multi-level view over graphs. The contributions of this work are two-fold. First, starting from basic definitions, i.e., what are dimensions and measures in the graph OLAP scenario, we develop a conceptual framework for data cubes on graphs. We also look into different semantics of OLAP operations, and classify the framework into two major subcases: informational OLAP and topological OLAP. Then, with more emphasis on informational OLAP (topological OLAP will be covered in a future study due to the lack of space), we show how a graph cube can be materialized by calculating a special kind of measure called aggregated graph and how to implement it efficiently. This includes both full materialization and partial materialization where constraints are enforced to obtain an iceberg cube. We can see that the aggregated graphs, which depend on the graph properties of underlying networks, are much harder to compute than their traditional OLAP counterparts, due to the increased structural complexity of data. Empirical studies show insightful results on real datasets and demonstrate the efficiency of our proposed optimizations.
[aggregated graph, Data analysis, on-line analytical processing, Navigation, data analysis, NASA, Circuits, graph theory, data mining, graph analysis, OLAP, Pattern recognition, Data mining, Chemical compounds, multilevel view over graph, Proteins, Aggregates, Computer networks]
Exploiting Local and Global Invariants for the Management of Large Scale Information Systems
2008 Eighth IEEE International Conference on Data Mining
None
2008
This paper presents a data oriented approach to modeling the complex computing systems, in which an ensemble of correlation models are discovered to represent the system status. If the discovered correlations can continually hold under different user scenarios and workloads, they are regarded as invariants of the information system. In our previous work, we have developed an algorithm to automatically search the invariants between any pair of system attributes, which we call local invariants. However that method is unable to deal with the high order dependency models due to the combinatorial explosion of search space. In this paper we use Bayesian regression technique to discover those high order correlation models, called global invariants. We treat each attribute as a response variable in turn and express its dependency with the other attributes in a regression model. By adding the prior constraint of Laplacian distribution to the regression coefficients, we can find the solution in which only the correlated attributes with respect to the response have nonzero regression coefficients. After that we further consider the temporal dependencies of those extracted attributes by incorporating their past observations. We also provide a confidence metric and a validation procedure to measure the reliability of learned models. If the model does not break down in the validation, it is regarded as a true invariant of the system. Experimental results on a real wireless networking system show that the discovered invariants can be used to effectively detect system failures as well as provide valuable information about the failure source.
[regression analysis, Data mining, Information systems, Network servers, Management information systems, failure diagnosis, confidence metric, Large-scale systems, Bayesian regression technique, Laplace equations, dependency, system management, Bayesian regression, Explosions, Laplace transforms, Power system modeling, large scale information systems management, wireless networking system, Bayesian methods, Frequency, Bayes methods, data handling, Laplacian distribution, failure detection, complex computing systems]
DECK: Detecting Events from Web Click-Through Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
In the past few years, there has been increased research interest in detecting previously unidentified events from Web resources. Our focus in this paper is to detect events from the click-through data generated by Web search engines. Existing event detection algorithms, which mainly study the news archive data, cannot be employed directly because of the following two unique features of click-through data: 1) the information provided by click-through data is quite limited; 2) not every query issued to a Web search engine corresponds to an event in the real world. In this paper, we address this problem by proposing an effective algorithm which Detects Events from ClicK-through data DECK. We firstly transform click-through data to the 2D polar space by considering the semantic dimension and temporal dimension of queries. Robust subspace estimation is performed to detect subspaces such that each subspace consists of queries of similar semantics. Next, we prune uninteresting subspaces which do not contain queries corresponding to real events by simultaneously considering the respective distribution of queries along the semantic dimension and the temporal dimension in each subspace. Finally, events are detected from interesting subspaces using a nonparametric clustering technique. Compared with an existing approach, our experimental results based on real-life data have shown that the proposed approach is more accurate and effective in detecting real events from click-through data.
[nonparametric clustering technique, search engines, semantic dimension, Event detection, 2D polar space, Data preprocessing, data mining, Data engineering, Data mining, Uniform resource locators, Web click-through data, Web search engine, Publishing activities, pattern clustering, DECK, Search engines, temporal dimension, detects event from click-through data, Robustness, subspace estimation, Internet, Web search]
Mining Order-Preserving Submatrices from Data with Repeated Measurements
2008 Eighth IEEE International Conference on Data Mining
None
2008
Order-preserving submatrices (OPSM's) have been shown useful in capturing concurrent patterns in data when the relative magnitudes of data items are more important than their absolute values. To cope with data noise, repeated experiments are often conducted to collect multiple measurements. We propose and study a more robust version of OPSM, where each data item is represented by a set of values obtained from replicated experiments. We call the new problem OPSM-RM (OPSM with repeated measurements). We define OPSM-RM based on a number of practical requirements. We discuss the computational challenges of OPSM-RM and propose a generic mining algorithm. We further propose a series of techniques to speed up two time-dominating components of the algorithm. We clearly show the effectiveness of our methods through a series of experiments conducted on real microarray data.
[Data analysis, data noise, order-preserving submatrices, data mining, Data mining, Noise measurement, Gene expression, Noise level, Computer science, data item, real microarray data, Intrusion detection, Noise robustness, Bioinformatics, generic mining algorithm, OPSM, sequence mining, gene expression]
Start Globally, Optimize Locally, Predict Globally: Improving Performance on Imbalanced Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
Class imbalance is a ubiquitous problem in supervised learning and has gained wide-scale attention in the literature. Perhaps the most prevalent solution is to apply sampling to training data in order improve classifier performance. The typical approach will apply uniform levels of sampling globally. However, we believe that data is typically multi-modal, which suggests sampling should be treated locally rather than globally. It is the purpose of this paper to propose a framework which first identifies meaningful regions of data and then proceeds to find optimal sampling levels within each. This paper demonstrates that a global classifier trained on data locally sampled produces superior rank-orderings on a wide range of real-world and artificial datasets as compared to contemporary global sampling methods.
[pattern classification, ubiquitous problem, Class imbalance, data mining, supervised learning, ubiquitous computing, class imbalance, SMOTE, Supervised learning, Training data, training data, Sampling methods, local sampling, learning (artificial intelligence)]
Generalized Framework for Syntax-Based Relation Mining
2008 Eighth IEEE International Conference on Data Mining
None
2008
Supervised approaches to data mining are particularly appealing as they allow for the extraction of complex relations from data objects. In order to facilitate their application in different areas, ranging from protein to protein interaction in bioinformatics to text mining in computational linguistics research, a modular and general mining framework is needed. The major constraint to the generalization process concerns the feature design for the description of relational data. In this paper, we present a machine learning framework for the automatic mining of relations, where the target objects are structurally organized in a tree. Object types are generalized by means of the use of roles, whereas the relation properties are described by means of the underlying tree structure. The latter is encoded in the learning algorithm thanks to kernel methods for structured data, which represent structures in terms of their all possible subparts. This approach can be applied to any kind of data disregarding their very nature. Experiments with support vector machines on two text mining datasets for relation extraction, i.e. the PropBank and FrameNet corpora, show both that our approach is general, and that it reaches state-of-the-art accuracy.
[Machine learning algorithms, data mining, supervised learning, roles, PropBank, relation mining, Data mining, machine learning framework, Proteins, structured data, text mining datasets, Computational linguistics, learning (artificial intelligence), Bioinformatics, Kernel, FrameNet, frame recognition, Text mining, Tree data structures, automatic relation mining, support vector machines, tree, Support vector machines, relational data, semantic role labeling, generalization, Machine learning, kernel methods, syntax-based relation mining]
Formal Models for Expert Finding on DBLP Bibliography Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
Finding relevant experts in a specific field is often crucial for consulting, both in industry and in academia. The aim of this paper is to address the expert-finding task in a real world academic field. We present three models for expert finding based on the large-scale DBLP bibliography and Google scholar for data supplementation. The first, a novel weighted language model, models an expert candidate based on the relevance and importance of associated documents by introducing a document prior probability, and achieves much better results than the basic language model. The second, a topic-based model, represents each candidate as a weighted sum of multiple topics, whilst the third, a hybrid model, combines the language model and the topic-based model. We evaluate our system using a benchmark dataset based on human relevance judgments of how well the expertise of proposed experts matches a query topic. Evaluation results show that our hybrid model outperforms other models in nearly all metrics.
[expert systems, DBLP bibliography data, Humans, expert-finding task, Data engineering, Information retrieval, data supplementation, bibliographic systems, Data mining, Application software, formal models, Computer science, Bibliographies, Expert finding, Google scholar, topic-based model, Computer industry, Large-scale systems, Mining industry, DBLP, language models, novel weighted language model]
ReDSOM: Relative Density Visualization of Temporal Changes in Cluster Structures Using Self-Organizing Maps
2008 Eighth IEEE International Conference on Data Mining
None
2008
We introduce a self-organizing map (SOM) based visualization method that compares cluster structures in temporal datasets using relative density SOM (ReDSOM) visualization. Our method, combined with a distance matrix-based visualization, is capable of visually identifying emerging clusters, disappearing clusters, enlarging clusters, contracting clusters, the shifting of cluster centroids, and changes in cluster density. For example, when a region in a SOM becomes significantly more dense compared to an earlier SOM, and well separated from other regions, then the new region can be said to represent a new cluster. The capabilities of ReDSOM are demonstrated using synthetic datasets, as well as real-life datasets from the World Bank and the Australian Taxation Office. The results on the real-life datasets demonstrate that changes identified interactively can be related to actual changes. The identification of such cluster changes is important in many contexts, including the exploration of changes in population behavior in the context of compliance and fraud in taxation.
[Self-Organizing Map, data mining, Data mining, Degradation, Self organizing feature maps, relative density visualization, self-organising feature maps, data visualisation, distance matrix-based visualization, self-organizing map, learning (artificial intelligence), ReDSOM, temporal change visualization, temporal dataset, Data analysis, Change detection algorithms, Government, change analysis, visual analytics, Temporal Cluster Analysis, Computer science, pattern clustering, Supervised learning, Data visualization, cluster structure, Australia]
Nonnegative Matrix Factorization for Combinatorial Optimization: Spectral Clustering, Graph Matching, and Clique Finding
2008 Eighth IEEE International Conference on Data Mining
None
2008
Nonnegative matrix factorization (NMF) is a versatile model for data clustering. In this paper, we propose several NMF inspired algorithms to solve different data mining problems. They include (1) multi-way normalized cut spectral clustering, (2) graph matching of both undirected and directed graphs, and (3) maximal clique finding on both graphs and bipartite graphs. Key features of these algorithms are (a) they are extremely simple to implement; and (b) they are provably convergent. We conduct experiments to demonstrate the effectiveness of these new algorithms. We also derive a new spectral bound for the size of maximal edge bicliques as a byproduct of our approach.
[pattern matching, Nonnegative matrix factorization, data mining, graph matching, data clustering, matrix decomposition, Data mining, bipartite graphs, Clustering algorithms, Cost function, Linear discriminant analysis, maximal clique finding, undirected graphs, spectral clustering, Data analysis, nonnegative matrix factorization, Unsupervised learning, Lagrangian functions, Support vector machines, maximal edge bicliques, combinatorial optimization, pattern clustering, directed graphs, Support vector machine classification, DH-HEMTs, clustering, clique finding]
Space Efficient String Mining under Frequency Constraints
2008 Eighth IEEE International Conference on Data Mining
None
2008
Let D<sub>1</sub> and D<sub>2</sub> be two databases (i.e. multisets) of d strings, over an alphabet Sigma, with overall length n. We study the problem of mining discriminative patterns between D<sub>1</sub> and D<sub>2</sub> - e.g., patterns that are frequent in one database but not in the other, emerging patterns, or patterns satisfying other frequency-related constraints. Using the algorithmic framework by Hui (CPM 1992), one can solve several variants of this problem in the optimal linear time with the aid of suffix trees or suffix arrays. This stands in high contrast to other pattern domains such as item-sets or subgraphs, where super-linear lower bounds are known. However, the space requirement of existing solutions is O(n log n) bits, which is not optimal for |Sigma| Lt n (in particular for constant |Sigma|), as the databases themselves occupy only n log |Sigma| bits. Because in many real-life applications space is a more critical resource than time, the aim of this article is to reduce the space, at the cost of an increased running time. In particular, we give a solution for the above problems that uses O(n log |Sigma| + d log n) bits, while the time requirement is increased from the optimal linear time to O(n log n). Our new method is tested extensively on a biologically relevant datasets and shown to be usable even on a genome-scale data.
[Data analysis, biologically relevant datasets, data mining, frequency constraints, space efficient string mining, Data mining, Unsupervised learning, Lagrangian functions, Support vector machines, d strings, Clustering algorithms, Support vector machine classification, mining discriminative patterns, Frequency, Cost function, |constraint based string mining, Linear discriminant analysis, string matching, real-life applications]
Efficient Discovery of Statistically Significant Association Rules
2008 Eighth IEEE International Conference on Data Mining
None
2008
Searching statistically significant association rules is an important but neglected problem. Traditional association rules do not capture the idea of statistical dependence and the resulting rules can be spurious, while the most significant rules may be missing. This leads to erroneous models and predictions which often become expensive.The problem is computationally very difficult, because the significance is not a monotonic property. However, in this paper we prove several other properties, which can be used for pruning the search space. The properties are implemented in the StatApriori algorithm, which searches statistically significant, non-redundant association rules. Based on both theoretical and empirical observations, the resulting rules are very accurate compared to traditional association rules. In addition, StatApriori can work with extremely low frequencies, thus finding new interesting rules.
[association rule, Data analysis, statistical significance, data mining, nonredundant association rules, efficient statistically significant association rule discovery, Association rules, Data mining, Unsupervised learning, Lagrangian functions, Support vector machines, Clustering algorithms, Support vector machine classification, statistically significant association rule searching, StatApriori algorithm, Cost function, Linear discriminant analysis]
Interpreting PET Scans by Structured Patient Data: A Data Mining Case Study in Dementia Research
2008 Eighth IEEE International Conference on Data Mining
None
2008
One of the goals of medical research in the area of dementia is to correlate images of the brain with other variables, for instance, demographic information or outcomes of clinical tests. The usual approach is to select a subset of patients based on such variables and analyze the images associated with those patients. In this paper, we apply data mining techniques to take the opposite approach: We start with the images and explain the differences and commonalities in terms of the other variables. In the first step, we cluster PET scans of patients to form groups sharing similar features in brain metabolism. To the best of our knowledge, it is the first time ever that clustering is applied to whole PET scans. In the second step, we explain the clusters by relating them to non-image variables. To do so, we employ RSD, an algorithm for relational subgroup discovery, with the cluster membership of patients as target variable. Our results enable interesting interpretations of differences in brain metabolism in terms of demographic and clinical variables. The approach was implemented and tested on an exceptionally large pre-existing data collection of patients with different types of dementia. It comprises 10 GB of image data from 454 PET scans, and 42 variables from psychological and demographical data organized in 11 relations of a relational database. We believe that explaining medical images in terms of other variables (patient records, demographic information, etc.) is a challenging new and rewarding area for data mining research.
[dementia research, brain metabolism, alzheimer's disease, Demography, PET scan, data mining, Biochemistry, Data mining, structured data, Clustering algorithms, medical image processing, Biomedical imaging, Testing, subgroup discovery, medical information systems, relational subgroup discovery algorithm, structured patient data, brain, Image analysis, pattern clustering, neuro imaging, Medical tests, dementia, clustering, neurophysiology, positron emission tomography, brain image, Positron emission tomography, PET, Dementia]
Inlier-Based Outlier Detection via Direct Density Ratio Estimation
2008 Eighth IEEE International Conference on Data Mining
None
2008
We propose a new statistical approach to the problem of inlier-based outlier detection, i.e.,finding outliers in the test set based on the training set consisting only of inliers. Our key idea is to use the ratio of training and test data densities as an outlier score; we estimate the ratio directly in a semi-parametric fashion without going through density estimation. Thus our approach is expected to have better performance in high-dimensional problems. Furthermore, the applied algorithm for density ratio estimation is equipped with a natural cross-validation procedure, allowing us to objectively optimize the value of tuning parameters such as the regularization parameter and the kernel width. The algorithm offers a closed-form solution as well as a closed-form formula for the leave-one-out error. Thanks to this, the proposed outlier detection method is computationally very efficient and is scalable to massive datasets. Simulations with benchmark and real-world datasets illustrate the usefulness of the proposed approach.
[data analysis, kernel width, importance, Data mining, closed-form solution, machine learning, closed-form formula, outlier detection, semiparametric fashion, density ratio, natural cross-validation procedure, direct density ratio estimation, learning (artificial intelligence), statistical analysis, inlier-based outlier detection, high-dimensional problem, regularization parameter, statistical approach, leave-one-out error]
Supervised Inductive Learning with Lotka-Volterra Derived Models
2008 Eighth IEEE International Conference on Data Mining
None
2008
We present a classification algorithm built on our adaptation of the Generalized Lotka-Volterra model, well-known in mathematical ecology. The training algorithm itself consists only of computing several scalars, per each training vector, using a single global user parameter and then solving a linear system of equations. Construction of the system matrix is driven by our model and based on kernel functions. The model allows an interesting point of view of kernels' role in the inductive learning process. We describe the model through axiomatic postulates. Finally, we present the results of the preliminary validation experiments.
[Machine learning algorithms, supervised inductive learning process, data mining, classification algorithm, Classification algorithms, Data mining, linear equation system matrix, ecology, biology computing, axiomatic postulate, Mathematical model, pattern classification, mathematical ecology, Biological system modeling, Volterra equations, supervised inductive machine-learning, generalized Lotka-Volterra derived model, training algorithm, kernel function, classification, Equations, Support vector machines, Computer science, Support vector machine classification, model-driven algorithm, Machine learning, learning by example]
A Novel Language-Model-Based Approach for Image Object Mining and Re-ranking
2008 Eighth IEEE International Conference on Data Mining
None
2008
One leading framework for image object mining is the bag-of-words (BOW) approach. The idea is to encode an image as a collection of visual words of the quantized local patches. Objects in the image can then be retrieved through inferring the semantic topics associated with the set of visual words. However, the visual BOW mining framework is apt to suffer from the so-called term-mismatch problem (a.k.a. vocabulary problem). This is caused by the poverty of query information, and consequently becomes an obstacle to deal with synonymy (i.e., different visual words for describing the same object). In this paper, we propose a novel language-model-based approach with pseudo-relevance feedback for addressing the vocabulary problem in visual BOW mining. We employ the pseudo positive images produced in response to the original query as a set of "cues" to gradually refine the query language model. Unlike traditional approaches that only ruggedly append feedback information into the original query, the proposed approach reconstructs the query language model with finer granularities so that the query concepts can be captured more accurately. The proposed approach is experimentally evaluated using two different types of image object databases. Our algorithms are shown to bring significant improvement in the retrieval accuracy over a non-feedback baseline, and achieve better performance than conventional feedback approaches.
[Vocabulary, Content based retrieval, bag-of-words approach, data mining, Multimedia databases, visual words, Data mining, Database languages, Image object retrieval, query processing, Histograms, Bag of words, term-mismatch problem, Language model, Feedback, query language model, quantized local patches, semantic topics, image object mining, image object reranking, language-model-based approach, Pseudo relevance feedback, Image retrieval, pseudo-relevance feedback, query information, Information retrieval, Image databases, relevance feedback, vocabulary problem, image retrieval, image object databases]
Maximum Margin Clustering with Pairwise Constraints
2008 Eighth IEEE International Conference on Data Mining
None
2008
Maximum margin clustering (MMC), which extends the theory of support vector machine to unsupervised learning, has been attracting considerable attention recently. The existing approaches mainly focus on reducing the computational complexity of MMC. The accuracy of these methods, however, has not always been guaranteed. In this paper, we propose to incorporate additional side-information, which is in the form of pairwise constraints, into MMC to further improve its performance. A set of pairwise loss functions are introduced into the clustering objective function which effectively penalize the violation of the given constraints. We show that the resulting optimization problem can be easily solved via constrained concave-convex procedure (CCCP). Moreover, for constrained multi-class MMC, we present an efficient cutting-plane algorithm to solve the sub-problem in each iteration of CCCP. The experiments demonstrate that the pairwise constrained MMC algorithms considerably outperform the unconstrained MMC algorithms and two other clustering algorithms that exploit the same type of side-information.
[clustering objective function, support vector machines, Optimization methods, pairwise constraint, Data mining, Computational complexity, Unsupervised learning, unsupervised learning, Support vector machines, Constraint optimization, optimisation, support vector machine, pattern clustering, Asia, Clustering algorithms, Constraint theory, pairwise loss function, Iterative methods, maximum margin clustering, computational complexity]
Collaborative Filtering for Implicit Feedback Datasets
2008 Eighth IEEE International Conference on Data Mining
None
2008
A common task of recommender systems is to improve customer experience through personalized recommendations based on prior implicit feedback. These systems passively track different sorts of user behavior, such as purchase history, watching habits and browsing activity, in order to model user preferences. Unlike the much more extensively researched explicit feedback, we do not have any direct input from the users regarding their preferences. In particular, we lack substantial evidence on which products consumer dislike. In this work we identify unique properties of implicit feedback datasets. We propose treating the data as indication of positive and negative preference associated with vastly varying confidence levels. This leads to a factor model which is especially tailored for implicit feedback recommenders. We also suggest a scalable optimization procedure, which scales linearly with the data size. The algorithm is used successfully within a recommender system for television shows. It compares favorably with well tuned implementations of other known methods. In addition, we offer a novel way to give explanations to recommendations given by this factor model.
[collaborative filtering, TV, Demography, user preferences, Watches, purchase history, History, Data mining, feedback, Negative feedback, implicit feedback datasets, browsing activity, Collaborative filtering, Motion pictures, Recommender systems, electronic commerce, implicit feedback, personalized recommendations, Filtering, International collaboration, watching habits, scalable optimization procedure, recommender systems, recommender system, customer experience]
Semi-supervised Learning from General Unlabeled Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
We consider the problem of semi-supervised learning (SSL) from general unlabeled data, which may contain irrelevant samples. Within the binary setting, our model manages to better utilize the information from unlabeled data by formulating them as a three-class (-1,+1, 0) mixture, where class 0 represents the irrelevant data. This distinguishes our work from the traditional SSL problem where unlabeled data are assumed to contain relevant samples only, either +1 or -1, which are forced to be the same as the given labeled samples. This work is also different from another family of popular models, universum learning (universum means "irrelevant" data), in that the universum need not to be specified beforehand. One significant contribution of our proposed framework is that such irrelevant samples can be automatically detected from the available unlabeled data, even though they are mixed with relevant data. This hence presents a general SSL framework that does not force "clean" unlabeled data.More importantly, we formulate this general learning framework as a Semi-definite Programming problem, making it solvable in polynomial time. A series of experiments demonstrate that the proposed framework can outperform the traditional SSL on both synthetic and real data.
[semisupervised learning, data mining, General Unlabeled Data, Data engineering, Mathematics, Data mining, Management training, Support vector machines, Computer science, semidefinite programming, universum learning, Semi-supervised Learning, Training data, Machine learning, general unlabeled data, Semisupervised learning, Polynomials, data handling, learning (artificial intelligence), SDP]
Metropolis Algorithms for Representative Subgraph Sampling
2008 Eighth IEEE International Conference on Data Mining
None
2008
While data mining in chemoinformatics studied graph data with dozens of nodes, systems biology and the Internet are now generating graph data with thousands and millions of nodes. Hence data mining faces the algorithmic challenge of coping with this significant increase in graph size: Classic algorithms for data analysis are often too expensive and too slow on large graphs. While one strategy to overcome this problem is to design novel efficient algorithms, the other is to 'reduce' the size of the large graph by sampling. This is the scope of this paper: We will present novel Metropolis algorithms for sampling a 'representative' small subgraph from the original large graph, with 'representative' describing the requirement that the sample shall preserve crucial graph properties of the original graph. In our experiments, we improve over the pioneering work of Leskovec and Faloutsos (KDD 2006), by producing representative subgraph samples that are both smaller and of higher quality than those produced by other methods from the literature.
[Algorithm design and analysis, systems biology, Data analysis, data analysis, graph theory, data mining, chemoinformatics, Markov Chain Monte Carlo, Data mining, Application software, Computer science, graph mining, Runtime, representative subgraph sampling, Metropolis, Systems biology, Sampling methods, Internet, Bioinformatics, metropolis algorithms]
Learning on Weighted Hypergraphs to Integrate Protein Interactions and Gene Expressions for Cancer Outcome Prediction
2008 Eighth IEEE International Conference on Data Mining
None
2008
Building reliable predictive models from multiple complementary genomic data for cancer study is a crucial step towards successful cancer treatment and a full understanding of the underlying biological principles. To tackle this challenging data integration problem, we propose a hypergraph-based learning algorithm called HyperGene to integrate microarray gene expressions and protein-protein interactions for cancer outcome prediction and biomarker identification. HyperGene is a robust two-step iterative method that alternatively finds the optimal outcome prediction and the optimal weighting of the marker genes guided by a protein-protein interaction network. Under the hypothesis that cancer-related genes tend to interact with each other, the HyperGene algorithm uses a protein-protein interaction network as prior knowledge by imposing a consistent weighting of interacting genes. Our experimental results on two large-scale breast cancer gene expression datasets show that HyperGene utilizing a curated protein-protein interaction network achieves significantly improved cancer outcome prediction. Moreover, HyperGene can also retrieve many known cancer genes as highly weighted marker genes.
[hypergraph-based learning algorithm, Genomics, multiple complementary genomic data, Predictive models, challenging data integration problem, cancer outcome prediction, spectral graph learning, Proteins, protein interactions, biomarker identification, cancer genomics, Robustness, Iterative methods, learning (artificial intelligence), reliable predictive models, Bioinformatics, weighted hypergraphs, HyperGene, Gene expression, protein-protein interactions, Biomarkers, Iterative algorithms, gene expressions, semi-supervised learning, medical computing, Cancer]
A Fast Method to Mine Frequent Subsequences from Graph Sequence Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
In recent years, the mining of a complete set of frequent subgraphs from labeled graph data has been extensively studied.However, to our best knowledge, almost no methods have been proposed to find frequent subsequences of graphs from a set of graph sequences. In this paper, we define a novel class of graph subsequences by introducing axiomatic rules of graph transformation, their admissibility constraints and a union graph. Then we propose an efficient approach named "GTRACE'' to enumerate frequent transformation subsequences (FTSs) of graphs from a given set of graph sequences. Its fundamental performance has been evaluated by using artificial datasets, and its practicality has been confirmed through the experiments using real world datasets.
[labeled graph sequence data, Admissibility, graph theory, Humans, data mining, frequent subgraph mining, Probability distribution, Data mining, History, graph transformation, Itemsets, Network topology, axiomatic rule, Character generation, Graph Sequence, Transformation Rule, Mining industry, frequent transformation subsequence mining, Frequent Pattern]
Overlapping Matrix Pattern Visualization: A Hypergraph Approach
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this work, we study a visual data mining problem: Given a set of discovered overlapping submatrices of interest, how can we order the rows and columns of the data matrix to best display these submatrices and their relationships? We find this problem can be converted to the hypergraph ordering problem, which generalizes the traditional minimal linear arrangement (or graph ordering) problem and then we are able to prove the NP-hardness of this problem. We propose a novel iterative algorithm which utilize the existing graph ordering algorithm to solve the optimal visualization problem. This algorithm can always converge to a local minimum. The detailed experimental evaluation using a set of publicly available transactional datasets demonstrates the effectiveness and efficiency of the proposed algorithm.
[transaction processing, Hypergraph, iterative methods, graph theory, visual data mining problem, transactional datasets, data mining, Data mining, Sparse matrices, Matrix Pattern Visualization, optimal visualization problem, Minimum Linear Arrangement Problem, Itemsets, minimal linear arrangement, data visualisation, hypergraph approach, Matrix converters, Symmetric matrices, discovered overlapping submatrices, data matrix, Hyperrectangle, overlapping matrix pattern visualization, Matrix decomposition, iterative algorithm, hypergraph ordering problem, matrix algebra, Computer science, NP-hardness, Computer displays, Data visualization, graph ordering algorithm, Iterative algorithms, computational complexity]
A Robust Discriminative Term Weighting Based Linear Discriminant Method for Text Classification
2008 Eighth IEEE International Conference on Data Mining
None
2008
Text classification is widely used in applications ranging from e-mail filtering to review classification. Many of these applications demand that the classification method be efficient and robust, yet produce accurate categorizations by using the terms in the documents only. We present a supervised text classification method based on discriminative term weighting, discrimination information pooling, and linear discrimination. Terms in the documents are assigned weights according to the discrimination information they provide for one category over the others. These weights also serve to partition the terms into two sets. A linear opinion pool is adopted for combining the discrimination information provided by each set of terms yielding a two-dimensional feature space. Subsequently, a linear discriminant function is learned to categorize the documents in the feature space. We provide intuitive and empirical evidence of the robustness of our method with three term weighting strategies. Experimental results are presented for data sets from three different application areas. The results show that our method's accuracy is higher than other popular methods, especially when there is a distribution shift from training to testing sets. Moreover, our method is simple yet robust to different application domains and small training set sizes.
[text analysis, documents, supervised text classification method, Data engineering, Electronic mail, Information filtering, text classification, Application software, Data mining, classification, robust discriminative term weighting based linear discriminant method, Computer science, term weighting, Text categorization, linear opinion pool, Web pages, generative-discriminative algorithm, discrimination information pooling, Robustness, Hybrid power systems, two-dimensional feature space]
Clustering Uncertain Data Using Voronoi Diagrams
2008 Eighth IEEE International Conference on Data Mining
None
2008
We study the problem of clustering uncertain objects whose locations are described by probability density functions (pdf). We show that the UK-means algorithm, which generalises the k-means algorithm to handle uncertain objects, is very inefficient. The inefficiency comes from the fact that UK-means computes expected distances (ED) between objects and cluster representatives. For arbitrary pdf's, expected distances are computed by numerical integrations, which are costly operations. We propose pruning techniques that are based on Voronoi diagrams to reduce the number of expected distance calculation. These techniques are analytically proven to be more effective than the basic bounding-box-based technique previous known in the literature. We conduct experiments to evaluate the effectiveness of our pruning techniques and to show that our techniques significantly outperform previous methods.
[bounding-box-based technique, computational geometry, Mobile communication, Data mining, Network servers, Voronoi diagram, Energy conservation, Clustering algorithms, Bandwidth, Probability density function, k-means, Computational efficiency, k-means algorithm, probability density function, UK-mean algorithm, expected distance calculation, probability, UK-means, classification, pruning technique, Computer science, uncertain data clustering, pattern clustering, uncertain data, Measurement uncertainty]
SCS: A New Similarity Measure for Categorical Sequences
2008 Eighth IEEE International Conference on Data Mining
None
2008
Measuring the similarity between categorical sequences is a fundamental process in many data mining applications. A key issue is to extract and make use of significant features hidden behind the chronological and structural dependencies found in these sequences. Almost all existing algorithms designed to perform this task are based on the matching of patterns in chronological order, but such sequences often have similar structural features in chronologically different positions. In this paper we propose SCS, a novel method for measuring the similarity between categorical sequences, based on an original pattern matching scheme that makes it possible to capture chronological and non-chronological dependencies. SCS captures significant patterns that represent the natural structure of sequences, and reduces the influence of those representing noise. It constitutes an effective approach for measuring the similarity of data such as biological sequences, natural language texts and financial transactions. To show its effectiveness, we have tested SCS extensively on a range of datasets, and compared the results with those obtained by various mainstream algorithms.
[Algorithm design and analysis, similarity measure, pattern classification, Sequences, Costs, pattern matching, Laboratories, Natural languages, Noise reduction, data mining, structural dependency, chronological dependency, Data mining, Application software, sequences, categorical sequence, Proteins, feature extraction, Pattern matching]
Toward Faster Nonnegative Matrix Factorization: A New Algorithm and Comparisons
2008 Eighth IEEE International Conference on Data Mining
None
2008
Nonnegative matrix factorization (NMF) is a dimension reduction method that has been widely used for various tasks including text mining, pattern analysis, clustering, and cancer class discovery. The mathematical formulation for NMF appears as a non-convex optimization problem, and various types of algorithms have been devised to solve the problem. The alternating nonnegative least squares (ANLS) framework is a block coordinate descent approach for solving NMF, which was recently shown to be theoretically sound and empirically efficient. In this paper, we present a novel algorithm for NMF based on the ANLS framework. Our new algorithm builds upon the block principal pivoting method for the nonnegativity constrained least squares problem that overcomes some limitations of active set methods. We introduce ideas to efficiently extend the block principal pivoting method within the context of NMF computation. Our algorithm inherits the convergence theory of the ANLS framework and can easily be extended to other constrained NMF formulations. Comparisons of algorithms using datasets that are from real life applications as well as those artificially generated show that the proposed new algorithm outperforms existing ones in computational speed.
[data mining, matrix decomposition, nonconvex optimization, Data mining, Convergence, Constraint optimization, Clustering algorithms, block principal pivoting method, nonnegativity constrained least squares, text mining, convergence theory, learning (artificial intelligence), alternating nonnegative least squares framework, concave programming, Text mining, pattern analysis, least squares approximations, dimension reduction method, Educational institutions, convergence of numerical methods, Application software, machine learning, Least squares approximation, nonnegative matrix factorization, Least squares methods, pattern clustering, block coordinate descent approach, active set method, block principal pivoting, Cancer, cancer class discovery]
Scalable Tensor Decompositions for Multi-aspect Data Mining
2008 Eighth IEEE International Conference on Data Mining
None
2008
Modern applications such as Internet traffic, telecommunication records, and large-scale social networks generate massive amounts of data with multiple aspects and high dimensionalities. Tensors (i.e., multi-way arrays) provide a natural representation for such data. Consequently, tensor decompositions such as Tucker become important tools for summarization and analysis. One major challenge is how to deal with high-dimensional, sparse data. In other words, how do we compute decompositions of tensors where most of the entries of the tensor are zero. Specialized techniques are needed for computing the Tucker decompositions for sparse tensors because standard algorithms do not account for the sparsity of the data. As a result, a surprising phenomenon is observed by practitioners: Despite the fact that there is enough memory to store both the input tensors and the factorized output tensors, memory overflows occur during the tensor factorization process. To address this intermediate blowup problem, we propose Memory-Efficient Tucker (MET). Based on the available memory, MET adaptively selects the right execution strategy during the decomposition. We provide quantitative and qualitative evaluation of MET on real tensors. It achieves over 1000X space reduction without sacrificing speed; it also allows us to work with much larger tensors that were too big to handle before. Finally, we demonstrate a data mining case-study using MET.
[Portable computers, Laboratories, data mining, Telecommunication traffic, tensors, Internet traffic, matrix decomposition, Memory-Efficient Tucker, Data mining, telecommunication records, sparse tensors, tensor decompositions, Tucker decompositions, large-scale social networks, intermediate blowup problem, Hardware, Large-scale systems, IP networks, Tensor Decomposition, Social network services, Sun, scalable tensor decompositions, Tensile stress, social networking (online), Sparse data, Internet, multiaspect data mining, tensor factorization, sparse matrices, telecommunication traffic, Tucker Decomposition]
Mining Periodic Behavior in Dynamic Social Networks
2008 Eighth IEEE International Conference on Data Mining
None
2008
Social interactions that occur regularly typically correspond to significant yet often infrequent and hard to detect interaction patterns. To identify such regular behavior, we propose a new mining problem of finding periodic or near periodic subgraphs in dynamic social networks. We analyze the computational complexity of the problem, showing that, unlike any of the related subgraph mining problems, it is polynomial. We propose a practical, efficient and scalable algorithm to find such subgraphs that takes imperfect periodicity into account. We demonstrate the applicability of our approach on several real-world networks and extract meaningful and interesting periodic interaction patterns.
[Heuristic algorithms, Social network services, pattern mining, graph theory, data mining, social networks, social interactions, periodic interaction patterns, Complexity theory, Data mining, Upper bound, periodic behavior mining, Probability density function, social networking (online), Polynomials, dynamic social networks, computational complexity]
Unsupervised Face Annotation by Mining the Web
2008 Eighth IEEE International Conference on Data Mining
None
2008
Searching for images of people is an essential task for image and video search engines. However, current search engines have limited capabilities for this task since they rely on text associated with images and video, and such text is likely to return many irrelevant results. We propose a method for retrieving relevant faces of one person by learning the visual consistency among results retrieved from text correlation-based search engines. The method consists of two steps. In the first step, each candidate face obtained from a text-based search engine is ranked with a score that measures the distribution of visual similarities among the faces. Faces that are possibly very relevant or irrelevant are ranked at the top or bottom of the list, respectively. The second step improves this ranking by treating this problem as a classification problem in which input faces are classified as psilaperson-Xpsila or psilanon-person-Xpsila; and the faces are re-ranked according to their relevant score inferred from the classifierpsilas probability output. To train this classifier, we use a bagging-based framework to combine results from multiple weak classifiers trained using different subsets. These training subsets are extracted and labeled automatically from the rank list produced from the classifier trained from the previous step. In this way, the accuracy of the ranked list increases after a number of iterations. Experimental results on various face sets retrieved from captions of news photos show that the retrieval performance improved after each iteration, with the final performance being higher than those of the existing algorithms.
[search engines, Density measurement, data mining, visual databases, Data mining, Videoconference, face annotation, Lighting, multiple weak classifiers, Web mining, Search engines, face recognition, Informatics, video search engines, image search engines, text-correlation-based search engines, pattern classification, unsupervised face annotation, Face recognition, Information retrieval, visual consistency, Unsupervised learning, unsupervised learning, ensemble learning, Image databases, face retrieval]
Border Sampling through Coupling Markov Chain Monte Carlo
2008 Eighth IEEE International Conference on Data Mining
None
2008
Recently, progressive border sampling (PBS) was proposed for sample selection in supervised learning by progressively learning an augmented full border from small labeled datasets. However, this quadratic learning algorithm is inapplicable to large datasets. In this paper, we incorporate the PBS to a state of the art technique called coupling Markov chain Monte Carlo (CMCMC) in an attempt to scale the original algorithm up on large labeled datasets. The CMCMC can produce an exact sample while a naive strategy for Markov chain Monte Carlo cannot guarantee the convergence to a stationary distribution. The resulting CMCMC-PBS algorithm is thus proposed for border sampling on large datasets. CMCMC-PBS exhibits several remarkable characteristics: linear time complexity, learner-independence, and a consistent convergence to an optimal sample from the original training sets by learning from their subsamples. Our experimental results on the 33 either small or large labeled datasets from the UCIKDD repository and a nuclear security application show that our new approach outperforms many previous sampling techniques for sample selection.
[Costs, markov chain monte carlo, convergence, supervised learning, original training set, Data mining, Convergence, progressive border sampling, Monte Carlo methods, border identification, learning (artificial intelligence), Protection, linear time complexity, labeled dataset, sampling methods, Data security, sample selection, coupling Markov chain Monte Carlo, Computer science, stationary distribution, Supervised learning, Machine learning, Markov processes, Sampling methods, sampling technique, quadratic learning algorithm, learner-independence, computational complexity]
Computationally Efficient Estimators for Dimension Reductions Using Stable Random Projections
2008 Eighth IEEE International Conference on Data Mining
None
2008
The method of stable random projections is an efficient tool for computing the l<sub>alpha</sub> distances using low memory, where 0 &lt; alpha les 2 may be viewed as a tuning parameter. This method boils down to a statistical estimation task and various estimators have been proposed, based on the geometric mean, harmonic mean, and fractional power etc. This study proposes the optimal quantile estimator, whose main operation is selecting, which is considerably less expensive than taking fractional power, the main operation in previous estimators. Our experiments report that this estimator is nearly one order of magnitude more computationally efficient than previous estimators. For large-scale tasks in which storing and computing pairwise distances is a serious bottleneck, this estimator should be desirable. In addition to its computational advantage, the optimal quantile estimator exhibits nice theoretical properties. It is more accurate than previous estimators when alpha &gt; 1. We derive its theoretical error bound and establish the explicit (i.e., no hidden constants) sample complexity bound.
[Machine learning algorithms, Costs, statistical estimation, estimation theory, random processes, Power system harmonics, Data mining, dimension reduction, tuning parameter, geometric mean, harmonic mean, Information science, data reduction, Image color analysis, stable random projection, fractional power, USA Councils, Machine learning, Streaming media, Large-scale systems, statistical analysis, optimal quantile estimator]
Isolation Forest
2008 Eighth IEEE International Conference on Data Mining
None
2008
Most existing model-based approaches to anomaly detection construct a profile of normal instances, then identify instances that do not conform to the normal profile as anomalies. This paper proposes a fundamentally different model-based method that explicitly isolates anomalies instead of profiles normal points. To our best knowledge, the concept of isolation has not been explored in current literature. The use of isolation enables the proposed method, iForest, to exploit sub-sampling to an extent that is not feasible in existing methods, creating an algorithm which has a linear time complexity with a low constant and a low memory requirement. Our empirical evaluation shows that iForest performs favourably to ORCA, a near-linear time complexity distance-based method, LOF and random forests in terms of AUC and processing time, and especially in large data sets. iForest also works well in high dimensional problems which have a large number of irrelevant attributes, and in situations where training set does not contain any anomalies.
[Performance evaluation, Laboratories, data mining, anomaly detection, isolation forest method, random forest, Data mining, outlier detection, Constraint optimization, isolation forest, Detectors, training data, Isolation technology, learning (artificial intelligence), linear time complexity, binary trees, LOF, ORCA, model based, trees (mathematics), Credit cards, Application software, Information technology, iForest method, model-based anomaly detection approach, novelty detection, Astronomy, computational complexity]
TEFE: A Time-Efficient Approach to Feature Extraction
2008 Eighth IEEE International Conference on Data Mining
None
2008
With the rapid evolution of Internet applications, people all over the world are sharing pictures, videos and audios online, and thus, content-based analysis is often demanded. Test efficiency is crucial to the success of online information processing. One obstacle to high-speed testing is the time cost of feature extraction for test objects, particularly for objects with complex representation such as images, videos and audios. In this paper, we study the problem of reducing test time cost by extracting cheap but sufficient features. We propose the TEFE (time-efficient feature extraction) approach, which balances between the test accuracy and test time cost by extracting a proper subset of features for each test object. In the implementation, TEFE trains a sequence of support vector machines and classifies each test object cascadingly. Empirical study shows that TEFE is time efficient while holding a classification accuracy close to that of using all features. It also shows that the test time is linearly adjustable in TEFE.
[Costs, Discussion forums, support vector machines, Information retrieval, Data mining, Videos, online information processing, feature extraction, time-efficient approach, Information processing, computer vision, Feature extraction, Internet, Acceleration, content-based analysis, Testing]
Transductive Component Analysis
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this paper, we study semisupervised linear dimensionality reduction. Beyond conventional supervised methods which merely consider labeled instances, the semisupervised scheme allows to leverage abundant and ample unlabeled instances into learning so as to achieve better generalization performance. Under semisupervised settings, our objective is to learn a smooth as well as discriminative subspace and linear dimensionality reduction is thus achieved by mapping all samples into the subspace. Specifically, we present the transductive component analysis (TCA) algorithm to generate such a subspace founded on a graph-theoretic framework. Considering TCA is nonorthogonal, we further present the orthogonal transductive component analysis (OTCA) algorithm to iteratively produce a series of orthogonal basis vectors. OTCA has better discriminating power than TCA. Experiments carried out on synthetic and real-world datasets by OTCA show a clear improvement over the results of representative dimensionality reduction algorithms.
[Algorithm design and analysis, Machine learning algorithms, graph-theoretic framework, graph theory, Humans, Data engineering, Graph theory, Data mining, machine learning, Information analysis, orthogonal basis vector, orthogonal transductive component analysis algorithm, Machine learning, semisupervised linear dimensionality reduction, Semisupervised learning, Iterative algorithms, learning (artificial intelligence)]
Modeling and Predicting the Helpfulness of Online Reviews
2008 Eighth IEEE International Conference on Data Mining
None
2008
Online reviews provide a valuable resource for potential customers to make purchase decisions. However, the sheer volume of available reviews as well as the large variations in the review quality present a big impediment to the effective use of the reviews, as the most helpful reviews may be buried in the large amount of low quality reviews. The goal of this paper is to develop models and algorithms for predicting the helpfulness of reviews, which provides the basis for discovering the most helpful reviews for given products. We first show that the helpfulness of a review depends on three important factors: the reviewerpsilas expertise, the writing style of the review, and the timeliness of the review. Based on the analysis of those factors, we present a nonlinear regression model for helpfulness prediction. Our empirical study on the IMDB movie reviews dataset demonstrates that the proposed approach is highly effective.
[reviewer expertise, regression analysis, Predictive models, Data engineering, Data mining, nonlinear regression model, Information technology, Helpfulness prediction, Computer science, IMDB movie reviews dataset, review quality, Voting, online reviews, Writing, Motion pictures, Prediction algorithms, Impedance, Review mining, electronic commerce, purchase decisions]
LBF: A Labeled-Based Forecasting Algorithm and Its Application to Electricity Price Time Series
2008 Eighth IEEE International Conference on Data Mining
None
2008
A new approach is presented in this work with the aim of predicting time series behaviors. A previous labeling of the samples is obtained utilizing clustering techniques and the forecasting is applied using the information provided by the clustering. Thus, the whole data set is discretized with the labels assigned to each data point and the main novelty is that only these labels are used to predict the future behavior of the time series, avoiding using the real values of the time series until the process ends. The results returned by the algorithm, however, are not labels but the nominal value of the point that is required to be predicted. The algorithm based on labeled (LBF) has been tested in several energy-related time series and a notable improvement in the prediction has been achieved.
[Wavelet transforms, electricity price time series prediction, energy-related time series prediction, Time series analysis, data mining, forecasting, Artificial neural networks, Predictive models, time series, Data mining, Application software, Clustering, power engineering computing, Computer science, pattern clustering, neighbourhood, data clustering technique, Prediction algorithms, power markets, Labeling, economic forecasting, Testing, labeled-based forecasting algorithm]
Enhancing the Stability of Spectral Ordering with Sparsification and Partial Supervision: Application to Paleontological Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
Recent studies have demonstrated the prospects of data mining algorithms for addressing the task of seriation in paleontological data (i.e. the age-based ordering of the sites of excavation). A prominent approach is spectral ordering that computes a similarity measure between the sites and orders them such that similar sites become adjacent and dissimilar sites are placed far apart. In the paleontological domain, the similarity measure is based on the mammal genera whose remains are retrieved at each site of excavation. Although spectral ordering achieves good performance in the seriation task, it ignores the background knowledge that is naturally present in the domain, as paleontologists can derive the ages of the sites of excavation within some accuracy. On the other hand, the age information is uncertain, so the best approach would be to combine the background knowledge with the information on mammal co-occurrences. Motivated by this kind of partial supervision we propose a novel semi-supervised spectral ordering algorithm. Our algorithm modifies the Laplacian matrix used in spectral ordering, such that domain knowledge of the ordering is taken into account. Also, it performs feature selection (sparsification) by discarding features that contribute most to the unwanted variability of the data in bootstrap sampling. The theoretical properties of the proposed algorithm are thoroughly analyzed and it is demonstrated that the proposed framework enhances the stability of the spectral ordering output and induces computational gains.
[Algorithm design and analysis, similarity measure, semisupervised spectral ordering algorithm, Uncertainty, data mining algorithms, data mining, bootstrap sampling, Data mining, palaeontology, age-based ordering, supervision, excavation sites, Eigenvalues and eigenfunctions, Laplacian matrix, mammal genera, Informatics, feature selection, eigengap, Laplace equations, spectral ordering, Reliability theory, geophysics computing, Stability analysis, Information technology, Laplacian, matrix algebra, paleontological data, bootstrapping, Sampling methods, spectral ordering stability, mammal cooccurrences]
Scaling up Classifiers to Cloud Computers
2008 Eighth IEEE International Conference on Data Mining
None
2008
As the size of available datasets has grown from Megabytes to Gigabytes and now into Terabytes, machine learning algorithms and computing infrastructures have continuously evolved in an effort to keep pace. But at large scales, mining for useful patterns still presents challenges in terms of data management as well as computation. These issues can be addressed by dividing both data and computation to build ensembles of classifiers in a distributed fashion, but trade-offs in cost, performance, and accuracy must be considered when designing or selecting an appropriate architecture. In this paper, we present an abstraction for scalable data mining that allows us to explore these trade-offs. Data and computation are distributed to a computing cloud with minimal effort from the user, and multiple models for data management are available depending on the workload and system configuration. We demonstrate the performance and scalability characteristics of our ensembles using a wide variety of datasets and algorithms on a Condor-based pool with Chirp to handle the storage.
[Cloud computing, Machine learning algorithms, data management, Scalability, data mining, Data engineering, Partitioning algorithms, datasets, Data mining, computing infrastructures, Distributed computing, Computer science, Ensemble Learning, Cloud Computing, USA Councils, cloud computers, scalable data mining, Large-scale systems, learning (artificial intelligence), machine learning algorithms, Distributed Data Mining]
What Sperner Family Concept Class is Easy to Be Enumerated?
2008 Eighth IEEE International Conference on Data Mining
None
2008
We study the problem of enumerating concepts in a Sperner family concept class using subconcept queries, which is a general problem including maximal frequent itemset mining as its instance. Though even the theoretically best known algorithm needs quasi-polynomial time to solve this problem in the worst case, there exist practically fast algorithms for this problem. This is because many instances of this problem in real world have low complexity in some measures. In this paper, we characterize the complexity of Sperner family concept class by the VC dimension of its intersection closure and its characteristic dimension, and analyze the worst case time complexity on the enumeration problem of its concepts in terms of the VC dimension. We also showed that the VC dimension of real data used in data mining is actually small by calculating the VC dimension of some real datasets using a new algorithm closely related to the introduced two measures, which does not only solve the problem but also let us know the VC dimension of the intersection closure of the target concept class.
[Virtual colonoscopy, Area measurement, data mining, Relational databases, time complexity, Size measurement, simple hypergraph, Data mining, frequent itemset mining, Sperner family, maxumal frequent itemset, Information science, Boolean functions, Itemsets, Tree graphs, Sperner family concept class, enumeration problem, Frequency, quasi-polynomial time, subconcept queries, enumeration, computational complexity]
Learning by Propagability
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this paper, we present a novel feature extraction framework, called learning by propagability. The whole learning process is driven by the philosophy that the data labels and optimal feature representation can constitute a harmonic system, namely, the data labels are invariant with respect to the propagation on the similarity-graph constructed by the optimal feature representation. Based on this philosophy, a unified formulation for learning by propagability is proposed for both supervised and semi-supervised configurations. Specifically, this formulation offers the semi-supervised learning two characteristics: 1) unlike conventional semi-supervised learning algorithms which mostly include at least two parameters, this formulation is parameter-free; and 2) the formulation unifies the label propagation and optimal representation pursuing, and thus the label propagation is enhanced by benefiting from the graph constructed with the derived optimal representation instead of the original representation. Extensive experiments on UCI toy data, handwritten digit recognition, and face recognition all validate the effectiveness of our proposed learning framework compared with the state-of-the-art methods for feature extraction and semi-supervised learning.
[Algorithm design and analysis, Face recognition, Scattering, harmonic system, semi-supervised configurations, Data mining, state-of-the-art methods, Design optimization, conventional semi-supervised learning algorithms, learning by propagability, dimensionality reduction, propagability, feature extraction, Training data, Semisupervised learning, face recognition, optimal feature representation, Feature extraction, Linear discriminant analysis, Principal component analysis]
One-Class Collaborative Filtering
2008 Eighth IEEE International Conference on Data Mining
None
2008
Many applications of collaborative filtering (CF), such as news item recommendation and bookmark recommendation, are most naturally thought of as one-class collaborative filtering (OCCF) problems. In these problems, the training data usually consist simply of binary data reflecting a user's action or inaction, such as page visitation in the case of news item recommendation or webpage bookmarking in the bookmarking scenario. Usually this kind of data are extremely sparse (a small fraction are positive examples), therefore ambiguity arises in the interpretation of the non-positive examples. Negative examples and unlabeled positive examples are mixed together and we are typically unable to distinguish them. For example, we cannot really attribute a user not bookmarking a page to a lack of interest or lack of awareness of the page. Previous research addressing this one-class problem only considered it as a classification task. In this paper, we consider the one-class problem under the CF setting. We propose two frameworks to tackle OCCF. One is based on weighted low rank approximation; the other is based on negative example sampling. The experimental results show that our approaches significantly outperform the baselines.
[Collaborative Filtering, Alternating Least Squares, Filtering, news item recommendation, International collaboration, information filtering, Data mining, Fuels, History, Low-Rank Approximations, Milling machines, Web page bookmarking, One-Class, Rockets, binary data, DVD, Training data, one-class collaborative filtering, groupware, training data, Sampling methods, Internet, bookmark recommendation]
DisCo: Distributed Co-clustering with Map-Reduce: A Case Study towards Petabyte-Scale End-to-End Mining
2008 Eighth IEEE International Conference on Data Mining
None
2008
Huge datasets are becoming prevalent; even as researchers, we now routinely have to work with datasets that are up to a few terabytes in size. Interesting real-world applications produce huge volumes of messy data. The mining process involves several steps, starting from pre-processing the raw data to estimating the final models. As data become more abundant, scalable and easy-to-use tools for distributed processing are also emerging. Among those, Map-Reduce has been widely embraced by both academia and industry. In database terms, Map-Reduce is a simple yet powerful execution engine, which can be complemented with other data storage and management components, as necessary. In this paper we describe our experiences and findings in applying Map-Reduce, from raw data to final models, on an important mining task. In particular, we focus on co-clustering, which has been studied in many applications such as text mining, collaborative filtering, bio-informatics, graph mining. We propose the distributed co-clustering (DisCo) framework, which introduces practical approaches for distributed data pre-processing, and co-clustering. We develop DisCo using Hadoop, an open source Map-Reduce implementation. We show that DisCo can scale well and efficiently process and analyze extremely large datasets (up to several hundreds of gigabytes) on commodity hardware.
[collaborative filtering, Memory, data mining, open source Map-Reduce implementation, mining process, distributed processing, Data mining, Engines, Distributed processing, storage management, co-clustering, data storage, graphs, Databases, DisCo, execution engine, text mining, distributed co-clustering, Text mining, distributed data pre-processing, Data analysis, Filtering, real-world applications, Hadoop, messy data, graph mining, pattern clustering, mapreduce, Collaboration, bioinformatics, petabyte-scale end-to-end mining, Energy management]
Learning Bayesian Networks: A MAP Criterion for Joint Selection of Model Structure and Parameter
2008 Eighth IEEE International Conference on Data Mining
None
2008
For learning Bayesian Network (BN) structures, it has become common practice to use the Bayesian Dirichlet (BD) scoring criterion. In contrast to most other scoring metrics that functionally can be interpreted as regularized maximum likelihood criteria, the BD metric cannot be considered as such. The functional dissimilarity of the BD metric compared to other metrics is an obstacle from an analytical point of view; this is for instance becomes clear in the context of the structural EM algorithm for learning BNs from incomplete data. Also, it is not easy to pin-point why exactly and to what extend regularization is taken care of by applying the BD metric. We introduce a Bayesian scoring criterion that is closely related to the BD metric, but solves the obvious disadvantages of the BD metric. We arrive at this result by using the same basic assumptions as for the BD metric, but in contrast to the BD metric, where focus is on learning the model structure only, we aim at learning the most probable BN pair jointly, i.e., model structure and the parameter are selected as a pair. This approach yields a scoring metric that has the functional form of a regularized maximum likelihood metric. We perform experiments, and show that this MAP BN metric also yields better results than the BIC and BD metrics on independent test data.
[Algorithm design and analysis, Performance evaluation, Geology, maximum likelihood metric, Bayesian Statistics, joint selection, Bayesian network learning, Electronic mail, Scoring Criterion, Data mining, BD metric, Learning, Bayesian Networks, MAP criterion, Graphical models, Bayesian methods, structural expectation-maximization algorithm, Machine learning, expectation-maximisation algorithm, Bayesian Dirichlet scoring criterion, Random variables, belief networks, learning (artificial intelligence), Context modeling]
Bayesian Co-clustering
2008 Eighth IEEE International Conference on Data Mining
None
2008
In recent years, co-clustering has emerged as a powerful data mining tool that can analyze dyadic data connecting two entities. However, almost all existing co-clustering techniques are partitional, and allow individual rows and columns of a data matrix to belong to only one cluster. Several current applications, such as recommendation systems and market basket analysis, can substantially benefit from a mixed membership of rows and columns. In this paper, we present Bayesian co-clustering (BCC) models, that allow a mixed membership in row and column clusters. BCC maintains separate Dirichlet priors for rows and columns over the mixed membership and assumes each observation to be generated by an exponential family distribution corresponding to its row and column clusters. We propose a fast variational algorithm for inference and parameter estimation. The model is designed to naturally handle sparse matrices as the inference is done only based on the non-missing entries. In addition to finding a co-cluster structure in observations, the model outputs a low dimensional co-embedding, and accurately predicts missing values in the original matrix. We demonstrate the efficacy of the model through experiments on both simulated and real data.
[market basket analysis, dyadic data, Data engineering, Data mining, Sparse matrices, data mining tool, variational algorithm, Clustering algorithms, Cities and towns, Motion pictures, parameter estimation, data matrix, co-clustering techniques, exponential family distribution, Bayesian co-clustering, Computer science, recommendation systems, Dirichlet priors, Bayesian methods, pattern clustering, Inference algorithms, co-cluster structure, Bayes methods, sparse matrices, Power engineering and energy]
Temporal-Relational Classifiers for Prediction in Evolving Domains
2008 Eighth IEEE International Conference on Data Mining
None
2008
Many relational domains contain temporal information and dynamics that are important to model (e.g., social networks, protein networks). However, past work in relational learning has focused primarily on modeling static "snapshots" of the data and has largely ignored the temporal dimension of these data. In this work, we extend relational techniques to temporally-evolving domains and outline a representational framework that is capable of modeling both temporal and relational dependencies in the data. We develop efficient learning and inference techniques within the framework by considering a restricted set of temporal-relational dependencies and using parameter-tying methods to generalize across relationships and entities. More specifically, we model dynamic relational data with a two-phase process, first summarizing the temporal-relational information with kernel smoothing, and then moderating attribute dependencies with the summarized relational information. We develop a number of novel temporal-relational models using the framework and then show that the current approaches to modeling static relational data are special cases within the framework. We compare the new models to the competing static relational methods on three real-world datasets and show that the temporal-relational models consistently outperform the relational models that ignore temporal information - achieving significant reductions in error ranging from 15% to 70%.
[dynamic relational data, pattern classification, Social network services, Predictive models, inference technique, Data mining, inference mechanisms, Statistics, Information analysis, static relational data, Computer science, Proteins, temporal-relational classifier, temporally-evolving domain, representational framework, kernel smoothing, Autocorrelation, Kernel, Context modeling]
xCrawl: A High-Recall Crawling Method for Web Mining
2008 Eighth IEEE International Conference on Data Mining
None
2008
Web mining systems exploit the redundancy of data published on the Web to automatically extract information from existing Web documents. The first step in the information extraction process is thus to locate within a limited period of time as many Web pages as possible that contain relevant information, a task which is commonly accomplished by applying focused crawling techniques. The performance of such a crawler can be measured by its "recall\
[document handling, Navigation, Crawlers, xCrawl, data mining, information retrieval, Information retrieval, Digital cameras, Time measurement, focused crawling, Data mining, Uniform resource locators, high-recall crawling method, automatic information extraction, Web pages, Web mining, authorative sources, Web documents, Internet, Web sites]
Comparison of Cluster Representations from Partial Second- to Full Fourth-Order Cross Moments for Data Stream Clustering
2008 Eighth IEEE International Conference on Data Mining
None
2008
Under seven external clustering evaluation measures, a comparison is made for cluster representations from the partial second order to the fourth order in data stream clustering. Two external clustering evaluation measures, purity and cross entropy, adopted for data stream clustering performance evaluation in the past, penalize the performance of an algorithm when each hypothesized cluster contains points in different target classes or true clusters, while ignoring the issue of points in a target class falling into different hypothesized clusters. The seven measures will address both sides of the clustering performance. The represented geometry by the partial second-order statistics of a cluster is non-oblique ellipsoidal and cannot describe the orientation, asymmetry, or peakedness of a cluster. The higher-order cluster representation presented in this paper introduces the third and fourth cross moments, enabling the cluster geometry to be beyond an ellipsoid. The higher-order statistics allow two clusters with different representations to merge into a multivariate normal cluster, using normality tests based on multivariate skewness and kurtosis. The clustering performance under the seven external clustering evaluation measures with a synthetic and two real data streams demonstrates the effectiveness of the higher-order cluster representations.
[cluster representations, Cross moment, Entropy, Partitioning algorithms, Data mining, Ellipsoids, Gaussian mixture model, Geometry, Computer science, data stream clustering, pattern clustering, higher-order cluster representation, Clustering algorithms, Gaussian processes, Streaming media, data structures, Cluster representation, Data stream clustering, multivariate skewness, Higher order statistics, Testing, multivariate normal cluster]
Web Mining for Understanding Stories through Graph Visualisation
2008 Eighth IEEE International Conference on Data Mining
None
2008
Rich information spaces (like the Web or scientific publications) are full of "stories": sets of statements that evolve over time, manifested as, for example, collections of newspaper articles reporting events relating to an evolving crime investigation, sets of news articles and blog posts accompanying the development of a political election campaign, or sequences of scientific papers on a topic. In this paper, we propose a method and a visualisation tool for mapping and interacting with such stories. In contrast to existing approaches, our method concentrates on relational information and on local patterns rather than on the occurrence of individual concepts and global models. In addition, we present an evaluation framework. A real-life case study is used to illustrate and evaluate the method and tool.
[Text mining, text analysis, graph theory, web mining, Nominations and elections, data mining, temporal text mining, Data mining, Computer science, news articles, Information services, Data visualization, data visualisation, Web mining, relational information, Broadcasting, text summarization and visualization, graph visualisation, Internet, Web sites]
Balancing Spectral Clustering for Segmenting Spatio-temporal Observations of Multi-agent Systems
2008 Eighth IEEE International Conference on Data Mining
None
2008
We examine the application of spectral clustering for breaking up the behavior of a multi-agent system in space and time into smaller, independent elements. We cluster observations of individual entities in order to identify significant changes in the parameter space (like spatial position)and detect temporal alterations of behavior within the same framework. Data is also influenced by knowledge about important events. Clusters are pre-processed at each step of the iterative subdivision to make the algorithm invariant against spatial scaling, rotation, replay speed and varying sampling frequency. A method is presented to balance spatial and temporal segmentation based on the expected group size. We demonstrate our results by analyzing the outcomes of a computer game.
[iterative methods, Multiagent systems, multi-agent systems, computer game, spatio-temporal observations, Data engineering, Educational institutions, iterative subdivision, data mining in multi-agent systems, Data mining, Intelligent networks, temporal segmentation, pattern clustering, Clustering algorithms, Sampling methods, Frequency, Iterative algorithms, Intelligent systems, spectral clustering, spatial segmentation]
Finding Good Itemsets by Packing Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
The problem of selecting small groups of itemsets that represent the data well has recently gained a lot of attention. We approach the problem by searching for the itemsets that compress the data efficiently. As a compression technique we use decision trees combined with a refined version of MDL. More formally, assuming that the items are ordered, we create a decision tree for each item that may only depend on the previous items. Our approach allows us to find complex interactions between the attributes, not just co-occurrences of 1s. Further, we present a link between the itemsets and the decision trees and use this link to export the itemsets from the decision trees. In this paper we present two algorithms. The first one is a simple greedy approach that builds a family of itemsets directly from data. The second one, given a collection of candidate itemsets, selects a small subset of these itemsets. Our experiments show that these approaches result in compact and high quality descriptions of the data.
[data mining, Length measurement, Encoding, Explosions, Data mining, Association rules, Proposals, Computer science, Itemsets, compression technique, complex interactions, decision trees, Frequency, itemsets, Decision trees, packing data]
Measuring Proximity on Graphs with Side Information
2008 Eighth IEEE International Conference on Data Mining
None
2008
This paper studies how to incorporate side information (such as users' feedback) in measuring node proximity on large graphs. Our method (ProSIN) is motivated by the well-studied random walk with restart (RWR). The basic idea behind ProSIN is to leverage side information to refine the graph structure so that the random walk is biased towards/away from some specific zones on the graph. Our case studies demonstrate that ProSIN is well-suited in a variety of applications, including neighborhood search, center-piece subgraphs, and image caption. Given the potential computational complexity of ProSIN, we also propose a fast algorithm (Fast-ProSIN) that exploits the smoothness of the graph structures with/without side information. Our experimental evaluation shows that fast-ProSIN achieves significant speedups (up to 49x) over straightforward implementations.
[neighborhood search, Graph Mining, random walk with restart, Proximity, Scalability, graph theory, side information, Steady-state, image caption, Data mining, Computational complexity, node proximity measurement, center-piece subgraphs, Side Information, Feedback, Information services, graph structures, Bipartite graph, Internet, Velocity measurement, Web sites, Pattern matching, computational complexity]
Fast Counting of Triangles in Large Real Networks without Counting: Algorithms and Laws
2008 Eighth IEEE International Conference on Data Mining
None
2008
How can we quickly find the number of triangles in a large graph, without actually counting them? Triangles are important for real world social networks, lying at the heart of the clustering coefficient and of the transitivity ratio. However, straight-forward and even approximate counting algorithms can be slow, trying to execute or approximate the equivalent of a 3-way database join. In this paper, we provide two algorithms, the eigentriangle for counting the total number of triangles in a graph, and the eigentrianglelocal algorithm that gives the count of triangles that contain a desired node. Additional contributions include the following: (a) We show that both algorithms achieve excellent accuracy, with up to sime 1000x faster execution time, on several, real graphs and (b) we discover two new power laws (degree-triangle and triangleparticipation laws) with surprising properties.
[Graph Generators, Machine learning algorithms, graph theory, 3-way database, Wikipedia, Data mining, eigentriangle, large real networks, transitivity ratio, Clustering algorithms, Intrusion detection, Eigenvalues and eigenfunctions, real world social networks, eigentrianglelocal algorithm, Graph Mining, Social network services, clustering coefficient, Triangles, Application software, Power laws, security of data, Machine learning, social networking (online), Web sites, approximate counting algorithms]
Improving Collaborative Filtering Recommendations Using External Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
This paper describes an approach for incorporating externally specified aggregate ratings information into certain types of collaborative filtering (CF) methods. For a statistical model-based CF approach, we formally showed that this additional aggregated information provides more accurate recommendations of individual items to individual users. Furthermore, theoretical insights gained from the analysis of this model-based method suggested a way to incorporate aggregate information into the heuristic item-based CF method. Both the model-based and the heuristic item-based CF methods were empirically tested on several datasets, and the experiments uniformly confirmed that the aggregate rating information indeed improves CF recommendations. These results also show the power of theory by demonstrating how the insights gained from theoretical developments can shed light on proper selection of good heuristic methods. We also showed the way to introduce scalability and parallelization into the estimation procedure and reported the running time for steps of the estimation procedure for large datasets.
[collaborative filtering, model-based collaborative filtering, Taxonomy, collaborative filtering recommendations, OLAP ratings, estimation procedure, International collaboration, information filtering, Information filtering, datasets, Information analysis, heuristic item-based collaborative filtering, external ratings, Aggregates, predictive models, Information filters, Motion pictures, Constraint theory, Collaborative work, aggregate rating information, Recommender systems]
A Generative Probabilistic Model for Multi-label Classification
2008 Eighth IEEE International Conference on Data Mining
None
2008
Traditional discriminative classification method makes little attempt to reveal the probabilistic structure and the correlation within both input and output spaces. In the scenario of multi-label classification, most of the classifiers simply assume the predefined classes are independently distributed, which would definitely hinder the classification performance when there are intrinsic correlations between the classes. In this article, we propose a generative probabilistic model, the Correlated Labeling Model (CoL Model), to formulate the correlation between different classes. The CoL model is presented to capture the correlation between classes and the underlying structures via the latent random variables in a supervised manner. We develop a variational procedure to approximate the posterior distribution and employ the EM algorithm for the empirical Bayes parameter estimation. In our evaluations, the proposed model achieved promising results on various data sets.
[empirical Bayes parameter estimation, Laboratories, latent random variables, multi-label classification, text classification, Data mining, intrinsic correlations, Information science, Space technology, parameter estimation, discriminative classification method, Labeling, Intelligent systems, pattern classification, generative probabilistic model, Intelligent structures, Computer science, Text categorization, multilabel classification, correlated labeling model, posterior distribution, expectation-maximisation algorithm, ontologies (artificial intelligence), Random variables, generative model, variational inference, EM algorithm]
SpecVAT: Enhanced Visual Cluster Analysis
2008 Eighth IEEE International Conference on Data Mining
None
2008
Given a pairwise dissimilarity matrix D of a set of objects, visual methods such as the VAT algorithm (for visual analysis of cluster tendency) represent (D macr )as an image (D macr ) where the objects are reordered to highlight cluster structure as dark blocks along the diagonal of the image. A major limitation of such visual methods is their inability to highlight cluster structure in 1(D macr ) when D contains clusters with highly complex structure. In this paper, we address this limitation by proposing a Spectral VAT (SpecVAT) algorithm, where D is mapped to D' in an embedding space by spectral decomposition of the Laplacian matrix, and then reordered to D' using the VAT algorithm. We also propose a strategy to automatically determine the number of clusters in (D macr '), as well as a method for cluster formation from (D macr ') based on the difference between diagonal blocks and off-diagonal blocks. We demonstrate the effectiveness of our algorithms on several synthetic and real-world data sets that are not amenable to analysis via traditional VAT.
[Algorithm design and analysis, spectral decomposition, image, Data analysis, data mining, diagonal blocks, Data engineering, Partitioning algorithms, Data mining, matrix algebra, SpecVAT, Computer science, Image analysis, enhanced visual cluster analysis, pattern clustering, Clustering algorithms, pairwise dissimilarity matrix, Laplacian matrix, Australia, Pixel, off-diagonal blocks]
Dirichlet Process Based Evolutionary Clustering
2008 Eighth IEEE International Conference on Data Mining
None
2008
Evolutionary Clustering has emerged as an important research topic in recent literature of data mining, and solutions to this problem have found a wide spectrum of applications, particularly in social network analysis. In this paper, based on the recent literature on Dirichlet processes, we have developed two different and specific models as solutions to this problem: DPChain and HDP-EVO. Both models substantially advance the literature on evolutionary clustering in the sense that not only they both perform better than the existing literature, but more importantly they are capable of automatically learning the cluster numbers and structures during the evolution. Extensive evaluations have demonstrated the effectiveness and promise of these models against the state-of-the-art literature.
[Evolutionary Clustering, HDP-EVO, Social network services, Statistical learning, data mining, evolutionary clustering, Dirichlet process, Data mining, Application software, Computer science, Dirichlet Process, USA Councils, Information services, social network analysis, DPChain, Internet, Web sites]
Evolutionary Clustering by Hierarchical Dirichlet Process with Hidden Markov State
2008 Eighth IEEE International Conference on Data Mining
None
2008
This paper studies evolutionary clustering, which is a recently hot topic with many important applications, noticeably in social network analysis. In this paper, based on the recent literature on Hierarchical Dirichlet Process (HDP) and Hidden Markov Model (HMM), we have developed a statistical model HDP-HTM that combines HDP with a Hierarchical Transition Matrix (HTM) based on the proposed Infinite Hierarchical Hidden Markov State model (iH2MS) as an effective solution to this problem. The HDP-HTM model substantially advances the literature on evolutionary clustering in the sense that not only it performs better than the existing literature, but more importantly it is capable of automatically learning the cluster numbers and structures and at the same time explicitly addresses the correspondence issue during the evolution. Extensive evaluations have demonstrated the effectiveness and promise of this solution against the state-of-the-art literature.
[Evolutionary Clustering, Social network services, data mining, evolutionary clustering, Infinite Hidden Markov Model, infinite hierarchical hidden Markov state model, Data mining, Application software, matrix algebra, HDP-HTM, Computer science, hidden Markov models, USA Councils, hierarchical transition matrix, Hidden Markov models, Information services, social network analysis, Time sharing computer systems, Hierarchical Dirichlet Process, hierarchical Dirichlet process, Internet, Web sites]
TOFA: Trace Oriented Feature Analysis in Text Categorization
2008 Eighth IEEE International Conference on Data Mining
None
2008
Dimension reduction for large-scale text data is attracting much attention lately due to the rapid growth of World Wide Web. We can consider dimension reduction algorithms in two categories: feature extraction and feature selection. An important problem remains: it has been difficult to integrate these two algorithm categories into a single framework, making it difficult to reap the benefit of both. In this paper, we formulate the two algorithm categories through a unified optimization framework. Under this framework, we develop a novel feature selection algorithm called Trace Oriented Feature Analysis (TOFA). The novel objective function of TOFA is a unified framework that integrates many prominent feature extraction algorithms such as unsupervised Principal Component Analysis and supervised Maximum Margin Criterion are special cases of it. Thus TOFA can process not only supervised problem but also unsupervised and semi-supervised problems. Experimental results on real text datasets demonstrate the effectiveness and efficiency of TOFA.
[Algorithm design and analysis, text analysis, large-scale text data, text categorization, World Wide Web, Data mining, Text Categorization, dimension reduction, Text processing, Computer science, Feature Analysis, TOFA, trace oriented feature analysis, USA Councils, Text categorization, Asia, feature extraction, Feature extraction, Large-scale systems, Internet, supervised maximum margin criterion, unsupervised principal component analysis, principal component analysis, Principal component analysis]
Clustering Distributed Time Series in Sensor Networks
2008 Eighth IEEE International Conference on Data Mining
None
2008
Event detection is a critical task in sensor networks, especially for environmental monitoring applications. Traditional solutions to event detection are based on analyzing one-shot data points, which might incur a high false alarm rate because sensor data is inherently unreliable and noisy. To address this issue, we propose a novel Distributed Single-pass Incremental Clustering (DSIC) technique to cluster the time series obtained at sensor nodes based on their underlying trends. In order to achieve scalability and energy-efficiency, our DSIC technique uses a hierarchical structure of sensor networks as the underlying infrastructure. The algorithm first compresses the time series produced at individual sensor nodes into a compact representation using Haar wavelet transform, and then, based on dynamic time warping distances, hierarchically groups the approximate time series into a global clustering model in an incremental manner. Experimental results on both real data and synthetic data demonstrate that our DSIC algorithm is accurate, energy-efficient and robust with respect to network topology changes.
[Event detection, wireless sensor networks, Scalability, wavelet transforms, sensor networks, Network topology, dynamic time warping distances, Clustering algorithms, Robustness, Monitoring, one-shot data points, Wavelet transforms, environmental monitoring applications, Data analysis, Sensor Networks, telecommunication network topology, time series, network topology, global clustering model, Haar wavelet transform, Working environment noise, distributed single-pass incremental clustering technique, event detection, Haar transforms, Time Series, Energy efficiency, distributed time series clustering, Distributed Clustering]
M3MIML: A Maximum Margin Method for Multi-instance Multi-label Learning
2008 Eighth IEEE International Conference on Data Mining
None
2008
Multi-instance multi-label learning (MIML) deals with the problem where each training example is associated with not only multiple instances but also multiple class labels. Previous MIML algorithms work by identifying its equivalence in degenerated versions of multi-instance multi-label learning. However, useful information encoded in training examples may get lost during the identification process. In this paper, a maximum margin method is proposed for MIML which directly exploits the connections between instances and labels. The learning task is formulated as a quadratic programming (QP) problem and implemented in its dual form. Applications to scene classification and text categorization show that the proposed approach achieves superior performance over existing MIML methods.
[Laboratories, supervised learning, Predictive models, identification process, Educational institutions, Data engineering, text categorization, Quadratic programming, Data mining, quadratic programming, Bridges, maximum margin method, Text categorization, Supervised learning, Layout, M3MIML, learning (artificial intelligence), training example, multiinstance multilabel learning]
RTM: Laws and a Recursive Generator for Weighted Time-Evolving Graphs
2008 Eighth IEEE International Conference on Data Mining
None
2008
How do real, weighted graphs change over time? What patterns, if any, do they obey? Earlier studies focus on unweighted graphs, and, with few exceptions, they focus on static snapshots. Here, we report patterns we discover on several real, weighted, time-evolving graphs. The reported patterns can help in detecting anomalies in natural graphs, in making link prediction and in providing more criteria for evaluation of synthetic graph generators. We further propose an intuitive and easy way to construct weighted, time-evolving graphs. In fact, we prove that our generator will produce graphs which obey many patterns and laws observed to date. We also provide empirical evidence to support our claims.
[power laws, Social network services, Blogs, graph theory, kronecker product, recursive generator, Telecommunication traffic, Gaussian distribution, RTM, tensors, Data mining, graph generators, Computer science, Tensile stress, synthetic graph generators, weighted time-evolving graphs, Character generation, link prediction, recursive estimation, Eigenvalues and eigenfunctions, unweighted graphs]
A Shrinkage Approach for Modeling Non-stationary Relational Autocorrelation
2008 Eighth IEEE International Conference on Data Mining
None
2008
Recent research has shown that collective classification in relational data often exhibit significant performance gains over conventional approaches that classify instances individually. This is primarily due to the presence of autocorrelation in relational datasets, meaning that the class labels of related entities are correlated and inferences about one instance can be used to improve inferences about linked instances. Statistical relational learning techniques exploit relational autocorrelation by modeling global autocorrelation dependencies under the assumption that the level of autocorrelation is stationary throughout the dataset. To date, there has been no work examining the appropriateness of this stationarity assumption. In this paper, we examine two real-world datasets and show that there is significant variance in the autocorrelation dependencies throughout the relational data graphs. We develop a shrinkage technique for modeling this non-stationary autocorrelation and show that it achieves significant accuracy gains over competing techniques that model either local or global autocorrelation dependencies in isolation.
[pattern classification, nonstationary relational autocorrelation modeling, collective classification, shrinkage technique, inference, data mining, statistical relational learning techniques, Predictive models, Performance gain, Relational learning, Topology, Classification algorithms, Data mining, Statistics, global autocorrelation dependencies, Computer science, relational data, class labels, Autocorrelation, shrinkage, Testing]
Latent Dirichlet Allocation and Singular Value Decomposition Based Multi-document Summarization
2008 Eighth IEEE International Conference on Data Mining
None
2008
Multi-Document Summarization deals with computing a summary for a set of related articles such that they give the user a general view about the events. One of the objectives is that the sentences should cover the different events in the documents with the information covered in as few sentences as possible. Latent Dirichlet Allocation can breakdown these documents into different topics or events. However to reduce the common information content the sentences of the summary need to be orthogonal to each other since orthogonal vectors have the lowest possible similarity and correlation between them. Singular Value Decompositions used to get the orthogonal representations of vectors and representing sentences as vectors, we can get the sentences that are orthogonal to each other in the LDA mixture model weighted term domain. Thus using LDA we find the different topics in the documents and using SVD we find the sentences that best represent these topics. Finally we present the evaluation of the algorithms on the DUC2002 Corpus multi-document summarization tasks using the ROUGE evaluator to evaluate the summaries. Compared to DUC 2002 winners, our algorithms gave significantly better ROUGE-1 recall measures.
[document handling, latent Dirichlet allocation, orthogonal vectors, abstracting, ROUGE evaluator, Natural Language Processing, orthogonal representations, Data engineering, Probability distribution, Multi-Document Summarization, LDA mixture model weighted term domain, Data mining, Computer science, vectors, DUC2002 Corpus multidocument summarization tasks, Bayesian methods, Frequency, Linear discriminant analysis, Joining processes, singular value decomposition, Singular value decomposition, Context modeling]
INSCY: Indexing Subspace Clusters with In-Process-Removal of Redundancy
2008 Eighth IEEE International Conference on Data Mining
None
2008
Subspace clustering aims at detecting clusters in any subspace projection of a high dimensional space. As the number of projections is exponential in the number of dimensions, efficiency is crucial. Moreover, the resulting subspace clusters are often highly redundant, i.e. many clusters are detected multiply in several projections. We propose a novel index for efficient subspace clustering in a novel depth-first processing with in-process-removal of redundant clusters for better pruning. Thorough experiments on real and synthetic data show that INSCY yields substantial efficiency and quality improvements.
[high dimensional space, in-process redundancy removal, Noise reduction, Project management, Lattices, data mining, Conference management, Data mining, tree searching, redundancy removal, subspace projection, Runtime, Databases, database indexing, pattern clustering, subspace clustering, high dimensional data, Clustering algorithms, INSCY mining, subspace cluster indexing, Kernel, depth-first processing, Indexing]
A Conservative Feature Subset Selection Algorithm with Missing Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
This paper introduces a novel conservative feature subset selection method with incomplete data sets. The method is conservative in the sense that it selects the minimal subset of features that renders the rest of the features independent of the target (the class variable) without making any assumption about the missing data mechanism. This is achieved in the context of determining the Markov blanket of the target that reflects the worst-case assumption about the missing data mechanism, including the case when data is not missing at random. An application of the method on synthetic incomplete data is carried out to illustrate its practical relevance. The method is compared against state-of-the-art approaches such as the expectation maximization (EM) algorithm and the available case technique.
[Performance evaluation, Feature selection, data mining, Spatial databases, Probability distribution, Data mining, missing data, incomplete data sets, Monte Carlo methods, Bayesian methods, Markov processes, Sampling methods, Markov blanket, Robustness, Testing, conservative feature subset selection method, bayesian networks]
Nonparametric Monotone Classification with MOCA
2008 Eighth IEEE International Conference on Data Mining
None
2008
We describe a monotone classification algorithm called MOCA that attempts to minimize the mean absolute prediction error for classification problems with ordered class labels.We first find a monotone classifier with minimum L<sub>1</sub> loss on the training sample, and then use a simple interpolation scheme to predict the class labels for attribute vectors not present in the training data.We compare MOCA to the ordinal stochastic dominance learner (OSDL), on artificial as well as real data sets. We show that MOCA often outperforms OSDL with respect to mean absolute prediction error.
[interpolation scheme, Costs, MOCA, Stochastic processes, data mining, Probability distribution, Classification algorithms, Data mining, Interpolation, interpolation, Monotonicity Constraint, mean absolute prediction error, attribute vectors, Training data, Classification, minimum L<sub>1</sub> loss, ordinal stochastic dominance learner, stochastic processes, nonparametric monotone classification]
Mining Large Networks with Subgraph Counting
2008 Eighth IEEE International Conference on Data Mining
None
2008
The problem of mining frequent patterns in networks has many applications, including analysis of complex networks, clustering of graphs, finding communities in social networks, and indexing of graphical and biological databases. Despite this wealth of applications, the current state of the art lacks algorithmic tools for counting the number of subgraphs contained in a large network. In this paper we develop data-stream algorithms that approximate the number of all subgraphs of three and four vertices in directed and undirected networks. We use the frequency of occurrence of all subgraphs to prove their significance in order to characterize different kinds of networks: we achieve very good precision in clustering networks with similar structure. The significance of our method is supported by the fact that such high precision cannot be achieved when performing clustering based on simpler topological properties, such as degree, assortativity, and eigenvector distributions. We have also tested our techniques using swap randomization.
[data representation, data mining, undirected network, network theory (graphs), Data mining, data-stream algorithm, biological database indexing, Information systems, social network, Streaming algorithms, Databases, Clustering algorithms, Complex networks, data structures, network characterization, Large-scale systems, Pattern analysis, graph algorithms, Biological system modeling, subgraph counting, swap randomization, complex network, directed network, graph clustering, pattern clustering, directed graphs, graphical database indexing, Frequency, Indexing, frequent pattern mining]
Comparative Evaluation of Anomaly Detection Techniques for Sequence Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
We present a comparative evaluation of a large number of anomaly detection techniques on a variety of publicly available as well as artificially generated data sets. Many of these are existing techniques while some are slight variants and/or adaptations of traditional anomaly detection techniques to sequence data.
[Performance evaluation, publicly available data set, sequence data, Sequences, anomaly detection technique, artificially generated data set, Data mining, Postal services, Nearest neighbor searches, Anomaly Detection, security of data, Intrusion detection, Hidden Markov models, Automata, Object detection, Kernel, Testing]
On Locally Linear Classification by Pairwise Coupling
2008 Eighth IEEE International Conference on Data Mining
None
2008
Locally linear classification by pairwise coupling addresses a nonlinear classification problem by three basic phases: decompose the classes of complex concepts into linearly separable subclasses, learn a linear classifier for each pair, and combine pairwise classifiers into a single classifier. A number of methods have been proposed in this framework. However, these methods have two major deficiencies: 1) lack of systematic evaluation of this framework; 2) naive application of clustering algorithms to generate subclasses. This paper proves the equivalence between three popular combination schemas under general settings, defines several global criterion functions for measuring the goodness of subclasses, and presents a supervised greedy clustering algorithm to optimize the proposed criterion functions. Extensive experiments were conducted to validate the effectiveness of the proposed techniques.
[pattern classification, Locally Linear Classification, greedy algorithms, supervised greedy clustering algorithm, Minimax techniques, locally linear classification, Data mining, global criterion functions, Support Vector Machines, Pair-wise Coupling, Couplings, Support vector machines, pairwise coupling, Voting, pattern clustering, Neural networks, nonlinear classification problem, Clustering algorithms, Prototypes, Training data, Support vector machine classification]
A Probability Model for Projective Clustering on High Dimensional Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
Clustering high dimensional data is a big challenge in data mining due to the curse of dimensionality. To solve this problem, projective clustering has been defined as an extension of traditional clustering that seeks to find projected clusters in subsets of dimensions of a data space. In this paper, the problem of modeling projected clusters is first discussed, and an extended Gaussian model is proposed. Second, a general objective criterion used with k-means type projective clustering is presented based on the model. Finally, the expressions to learn model parameters are derived and then used in a new algorithm named FPC to perform fuzzy clustering on high dimensional data. The experimental results on document clustering show the effectiveness of the proposed clustering model.
[learning algorithm, Flexible printed circuits, Extraterrestrial phenomena, Clustering methods, probability model, data mining, fuzzy set theory, probability, Partitioning algorithms, Data mining, Computer science, Monte Carlo methods, projective clustering, pattern clustering, high dimensional data, Clustering algorithms, Gaussian processes, Gaussian model, fuzzy clustering, Los Angeles Council, Random variables, learning (artificial intelligence)]
Estimating Aggregates over Multiple Sets
2008 Eighth IEEE International Conference on Data Mining
None
2008
Many datasets, including market basket data, text or hypertext documents, and measurement data collected in different nodes or time periods, are modeled as a collection of sets over a ground set of (weighted) items. We consider the problem of estimating basic aggregates such as the weight or selectivity of a subpopulation of the items. We extend classic summarization techniques based on sampling to this scenario when we have multiple sets and selection predicates based on membership in particular sets.
[document handling, Costs, Content based retrieval, approximate query processing, multiple sets, hypermedia, sampling, Time measurement, Data mining, market basket data, Computer science, hypertext documents, summarization techniques, sketching, Aggregates, USA Councils, Web pages, similarity, Sampling methods, Frequency, measurement data collected]
A Joint Matrix Factorization Approach to Unsupervised Action Categorization
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this paper, a novel unsupervised approach to mining categories from action video sequences is presented. This approach consists of two modules: action representation and learning model. Videos are regarded as spatially distributed dynamic pixel time series, which are quantized into pixel prototypes. After replacing the pixel time series with their corresponding prototype labels, the video sequences are compressed into 2D action matrices. We put these matrices together to form an multi-action tensor, and propose the joint matrix factorization method to simultaneously cluster the pixel prototypes into pixel signatures, and matrices into action classes. The approach is tested on public and popular Weizmann data set, and promising results are achieved.
[action representation, matrix factorization, Symmetric matrices, Weizmann data set, Video sequences, unsupervised action categorization, Joint matrix factorization, Electroencephalography, matrix decomposition, Matrix decomposition, Computer science, Tensile stress, Surveillance, Prototypes, Action categorization, Feature extraction, action video sequences, learning model, Matrix converters, video signal processing, image sequences, 2D action matrices]
Finding Alternative Clusterings Using Constraints
2008 Eighth IEEE International Conference on Data Mining
None
2008
The aim of data mining is to find novel and actionable insights. However, most algorithms typically just find a single explanation of the data even though alternatives could exist. In this work, we explore a general purpose approach to find an alternative clustering of the data with the aid of must-link and cannot-link constraints. This problem has received little attention in the literature and since our approach can be incorporated into the many clustering algorithms that use a distance function, compares favorably with existing work.
[Monte Carlo methods, distance function, data clustering algorithm, pattern clustering, Clustering algorithms, data mining, Euclidean distance, Sampling methods, clustering, Data mining, constraints]
Efficient Feature Selection in the Presence of Multiple Feature Classes
2008 Eighth IEEE International Conference on Data Mining
None
2008
We present an information theoretic approach to feature selection when the data possesses feature classes. Feature classes are pervasive in real data. For example, in gene expression data, the genes which serve as features may be divided into classes based on their membership in gene families or pathways. When doing word sense disambiguation or named entity extraction, features fall into classes including adjacent words, their parts of speech, and the topic and venue of the document the word is in. When predictive features occur predominantly in a small number of feature classes, our information theoretic approach significantly improves feature selection. Experiments on real and synthetic data demonstrate substantial improvement in predictive accuracy over the standard L<sub>0</sub> penalty-based stepwise and stream wise feature selection methods as well as over Lasso and Elastic Nets, all of which are oblivious to the existence of feature classes.
[gene expression data, pattern classification, Computational Intelligence Society, Data mining, Gene expression, Statistics, Biological information theory, Proteins, Feature Selection, Accuracy, multiple feature classes, feature extraction, information theoretic approach, Speech, Feature extraction, features extraction, word sense disambiguation, Minimum Description Length Coding, feature selection, Principal component analysis]
Why Stacked Models Perform Effective Collective Classification
2008 Eighth IEEE International Conference on Data Mining
None
2008
Collective classification techniques jointly infer all class labels of a relational data set, using the inferences about one class label to influence inferences about related class labels. Kou and Cohen recently introduced an efficient relational model based on stacking that, despite its simplicity, has equivalent accuracy to more sophisticated joint inference approaches. Using experiments on both real and synthetic data, we show that the primary cause for the performance of the stacked model is the reduction in bias from learning the stacked model on inferred labels rather than true labels. The reduction in variance due to conditional inference also contributes to the effect but it is not as strong. In addition, we show that the performance of the joint inference and stacked learners can be attributed to an implicit weighting of local and relational features at learning time.
[pattern classification, collective classification, relational data set, Stacking, Switches, bias/variance, Drives, joint class label inference, Data mining, inference mechanisms, relational dependency network, Convergence, Computer science, stacked model learning, Inference algorithms, learning (artificial intelligence), collective classification techniques, Testing, stacking]
Multiplicative Mixture Models for Overlapping Clustering
2008 Eighth IEEE International Conference on Data Mining
None
2008
The problem of overlapping clustering, where a point is allowed to belong to multiple clusters, is becoming increasingly important in a variety of applications. In this paper, we present an overlapping clustering algorithm based on multiplicative mixture models. We analyze a general setting where each component of the multiplicative mixture is from an exponential family, and present an efficient alternating maximization algorithm to learn the model and infer overlapping clusters. We also show that when each component is assumed to be a Gaussian, we can apply the kernel trick leading to non-linear cluster separators and obtain better clustering quality. The efficacy of the proposed algorithms is demonstrated using experiments on both UCI benchmark datasets and a microarray gene expression dataset.
[Algorithm design and analysis, overlapping clustering algorithm, microarray gene expression dataset, Particle separators, multiple clusters, Data engineering, nonlinear cluster separators, multiplicative mixture model, Computer science, Proteins, maximization algorithm, kernel trick, optimisation, pattern clustering, Clustering algorithms, Gaussian processes, Cities and towns, Inference algorithms, overlapping clustering quality, Kernel, exponential family, Context modeling]
Anomaly Detection Support Vector Machine and Its Application to Fault Diagnosis
2008 Eighth IEEE International Conference on Data Mining
None
2008
We address the issue of classification problems in the following situation: test data include data belonging to unlearned classes. To address this issue, most previous works have taken two-stage strategies where unclear data are detected using an anomaly detection algorithm in the first stage while the rest of data are classified into learned classes using a classification algorithm in the second stage. In this study, we propose anomaly detection support vector machine (ADSVM) which unifies classification and anomaly detection. ADSVM is unique in comparison with the previous work in that it addresses the two problems simultaneously. We also propose a multiclass extension of ADSVM that uses a pairwise voting strategy. We empirically present that ADSVM outperforms two-stage algorithms in application to an real automobile fault dataset, as well as to UCI benchmark datasets.
[pattern classification, UCI benchmark datasets, fault diagnosis, support vector machines, Fault Diagnosis, classification algorithm, Classification algorithms, Automobiles, software fault tolerance, Support vector machines, Fault diagnosis, Anomaly Detection, security of data, ADSVM, Fault detection, Voting, Support vector machine classification, Training data, Classification, Benchmark testing, anomaly detection support vector machine, Detection algorithms]
A Recommendation System for Preconditioned Iterative Solvers
2008 Eighth IEEE International Conference on Data Mining
None
2008
Preconditioned iterative methods are often used to solve very large sparse systems of linear systems that arise in many scientific and engineering applications. The performance and robustness of these solvers is extremely sensitive to the choice of multiple preconditioner and solver parameters. Users of iterative methods often encounter an overwhelming number of combinations of choices for solvers, matrix preprocessing steps, preconditioners, and their parameters. The lack of a unified theoretical analysis of preconditioners coupled with limited knowledge of their interaction with linear systems makes it highly challenging for practitioners to choose good solver configurations. In this paper, we propose a novel, multi-stage learning based methodology for determining the best solver configurations to optimize the desired performance behavior for any given linear system. Empirical results over real performance data for the hyper iterative solver package demonstrate the efficacy and flexibility of the proposed approach.
[Linear systems, iterative methods, preconditioned iterative solvers, multistage learning based methodology, mathematics computing, Optimization methods, Data engineering, information filtering, preconditioners, Sparse matrices, Data mining, recommendation system, hyper iterative solver package, multiple solver parameters, software packages, Robustness, Iterative methods, top-k ranking, matrix preprocessing, Vectors, linear systems, iterative method, classification, Equations, multiple preconditioner parameters, regression, Packaging, clustering, solvers]
Cost-Sensitive Parsimonious Linear Regression
2008 Eighth IEEE International Conference on Data Mining
None
2008
We examine linear regression problems where some features may only be observable at a cost (e.g., in medical domains where features may correspond to diagnostic tests that take time and costs money). This can be important in the context of data mining, in order to obtain the best predictions from the data on a limited cost budget. We define a parsimonious linear regression objective criterion that jointly minimizes prediction error and feature cost. We modify least angle regression algorithms commonly used for sparse linear regression to produce the ParLiR algorithm, which not only provides an efficient and parsimonious solution as we demonstrate empirically, but it also provides formal guarantees that we prove theoretically.
[Costs, Machine learning algorithms, Linear regression, Cost sensitivity, data mining, regression analysis, linear regression, sparsity, Data mining, sparse linear regression, Measurement units, cost-sensitive parsimonious linear regression, parsimonious linear regression objective criterion, regression, Machine learning, Medical tests, Sampling methods, prediction error, Australia, ParLiR algorithm, Medical diagnostic imaging]
Text Mining in Radiology Reports
2008 Eighth IEEE International Conference on Data Mining
None
2008
Medical text mining has gained increasing interest in recent years. Radiology reports contain rich information describing radiologistpsilas observations on the patientpsilas medical conditions in the associated medical images. However, as most reports are in free text format, the valuable information contained in those reports cannot be easily accessed and used, unless proper text mining has been applied. In this paper, we propose a text mining system to extract and use the information in radiology reports. The system consists of three main modules: a medical finding extractor, a report and image retriever, and a text-assisted image feature extractor. In evaluation, the overall precision and recall for medical finding extraction are 95.5% and 87.9% respectively, and for all modifiers of the medical findings 88.2% and 82.8% respectively. The overall result of report and image retrieval module and text-assisted image feature extraction module is satisfactory to radiologists.
[text analysis, radiology reports, text-assisted image feature extraction module, data mining, Data mining, free text format, image retrieval module, Medical conditions, feature extraction, medical finding extraction, Biomedical imaging, text mining system, Text mining, medical finding extractor, Image retrieval, image retriever, document image processing, Information retrieval, medical information systems, associated medical images, Text Mining, Hospitals, Image databases, medical text mining, text-assisted image feature extractor, Medical Informatics, Radiology, Feature extraction, patient medical conditions]
A Hierarchical Algorithm for Clustering Uncertain Data via an Information-Theoretic Approach
2008 Eighth IEEE International Conference on Data Mining
None
2008
In recent years there has been a growing interest in clustering uncertain data. In contrast to traditional, "sharp" data representation models, uncertain data objects can be represented in terms of an uncertainty region over which a probability density function (pdf) is defined. In this context, the focus has been mainly on partitional and density-based approaches, whereas hierarchical clustering schemes have drawn less attention. We propose a centroid-linkage-based agglomerative hierarchical algorithm for clustering uncertain objects, named U-AHC. The cluster merging criterion is based on an information-theoretic measure to compute the distance between cluster prototypes. These prototypes are represented as mixture densities that summarize the pdfs of all the uncertain objects in the clusters. Experiments have shown that our method outperforms state-of-the-art clustering algorithms from an accuracy viewpoint while achieving reasonably good efficiency.
[Uncertainty, Density measurement, Merging, data mining, uncertainty handling, density-based approach, knowledge discovery, Data mining, data representation model, centroid-linkage-based agglomerative hierarchical algorithm, cluster prototype, Clustering algorithms, Prototypes, Probability density function, information theory, information-theoretic distance measures, probability density function, Data analysis, probability, Biomedical measurements, Partitioning algorithms, uncertain data clustering, pattern clustering, uncertain data management, hierarchical clustering]
Discovering Significant Patterns in Multi-stream Sequences
2008 Eighth IEEE International Conference on Data Mining
None
2008
Discovering significant patterns in synchronized multi-stream sequences also known as multi-attribute event sequences (multi-sequences), is an important problem in many domains, including monitoring systems and information retrieval. In this paper we propose a new approach for assessing significance of multi-stream patterns in multi-attribute event sequences. In experiments on physiological multi-stream data we show applicability of our method.
[Telecommunication traffic, information retrieval, Sensor phenomena and characterization, multi-stream data mining, Information retrieval, Sensor systems and applications, Data mining, multi-attribute event sequences, multi-stream sequences, Patient monitoring, monitoring systems, Time factors, Joining processes, Informatics, Biomedical monitoring]
Graph-Based Rare Category Detection
2008 Eighth IEEE International Conference on Data Mining
None
2008
Rare category detection is the task of identifying examples from rare classes in an unlabeled data set. It is an open challenge in machine learning and plays key roles in real applications such as financial fraud detection, network intrusion detection, astronomy, spam image detection, etc. In this paper, we develop a new graph-based method for rare category detection named GRADE. It makes use of the global similarity matrix motivated by the manifold ranking algorithm, which results in more compact clusters for the minority classes; by selecting examples from the regions where probability density changes the most, it relaxes the assumption that the majority classes and the minority classes are separable. Furthermore, when detailed information about the data set is not available, we develop a modified version of GRADE named GRADE-LI, which only needs an upper bound on the proportion of each minority class as input. Besides working with data with structured features, both GRADE and GRADE-LI can also work with graph data, which can not be handled by existing rare category detection methods. Experimental results on both synthetic and real data sets demonstrate the effectiveness of the GRADE and GRADE-LI algorithms.
[GRADE graph-based rare category detection algorithm, majority class, global similarity matrix, graph theory, data mining, Image sampling, unlabeled data set clustering, Data mining, Manifolds, Intrusion detection, Clustering algorithms, learning (artificial intelligence), pattern classification, probability density, probability, manifold ranking algorithm, Helium, machine learning, rare category detection, graph, GRADE-LI algorithm, Upper bound, pattern clustering, Object detection, Machine learning, minority class, Astronomy]
Clustering Documents with Active Learning Using Wikipedia
2008 Eighth IEEE International Conference on Data Mining
None
2008
Wikipedia has been applied as a background knowledge base to various text mining problems, but very few attempts have been made to utilize it for document clustering. In this paper we propose to exploit the semantic knowledge in Wikipedia for clustering, enabling the automatic grouping of documents with similar themes. Although clustering is intrinsically unsupervised, recent research has shown that incorporating supervision improves clustering performance, even when limited supervision is provided. The approach presented in this paper applies supervision using active learning. We first utilize Wikipedia to create a concept-based representation of a text document, with each concept associated to a Wikipedia article. We then exploit the semantic relatedness between Wikipedia concepts to find pair-wise instance-level constraints for supervised clustering, guiding clustering towards the direction indicated by the constraints. We test our approach on three standard text document datasets. Empirical results show that our basic document representation strategy yields comparable performance to previous attempts; and adding constraints improves clustering performance further by up to 20%.
[text analysis, text clustering, document representation, data mining, document clustering, Wikipedia, unsupervised learning, active learning, pattern clustering, knowledge representation, semantic knowledge based representation, text mining, Web sites, text document dataset]
Direct Zero-Norm Optimization for Feature Selection
2008 Eighth IEEE International Conference on Data Mining
None
2008
Zero-norm, defined as the number of non-zero elements in a vector, is an ideal quantity for feature selection. However, minimization of zero-norm is generally regarded as a combinatorially difficult optimization problem. In contrast to previous methods that usually optimize a surrogate of zero-norm, we propose a direct optimization method to achieve zero-norm for feature selection in this paper. Based on Expectation Maximization (EM), this method boils down to solving a sequence of Quadratic Programming problems and hence can be practically optimized in polynomial time. We show that the proposed optimization technique has a nice Bayesian interpretation and converges to the true zero norm asymptotically, provided that a good starting point is given. Following the scheme of our proposed zero-norm, we even show that an arbitrary-norm based Support Vector Machine can be achieved in polynomial time. A series of experiments demonstrate that our proposed EM based zero-norm outperforms other state-of-the-art methods for feature selection on biological microarray data and UCI data, in terms of both the accuracy and the learning efficiency.
[direct zero-norm optimization, support vector machines, Optimization methods, UCI data, Data engineering, Mathematics, Data mining, Quadratic programming, quadratic programming problems, Computer science, Support vector machines, optimisation, support vector machine, Bayesian methods, biological microarray data, Machine learning, expectation-maximisation algorithm, Polynomials, expectation maximization, polynomial time, feature selection]
Discovering Flow Anomalies: A SWEET Approach
2008 Eighth IEEE International Conference on Data Mining
None
2008
Given a percentage-threshold and readings from a pair of consecutive upstream and downstream sensors, flow anomaly discovery identifies dominant time intervals where the fraction of time instants of significantly mis-matched sensor readings exceed the given percentage-threshold. Discovering flow anomalies (FA) is an important problem in environmental flow monitoring networks and early warning detection systems for water quality problems. However, mining FAs is computationally expensive because of the large (potentially infinite) number of time instants of measurement and potentially long delays due to stagnant (e.g. lakes) or slow moving (e.g. wetland) water bodies between consecutive sensors. Traditional outlier detection methods (e.g. t-test) are suited for detecting transient FAs (i.e., time instants of significant mis-matches across consecutive sensors) and cannot detect persistent FAs (i.e., long variable time-windows with a high fraction of time instant transient FAs) due to a lack of a pre-defined window size. In contrast, we propose a Smart Window Enumeration and Evaluation of persistence-Thresholds (SWEET) method to efficiently explore the search space of all possible window lengths. Computation overhead is brought down significantly by restricting the start and end points of a window to coincide with transient FAs, using a smart counter and efficient pruning techniques. Experimental evaluation using a real dataset shows our proposed approach outperforms Nainodotve alternatives.
[real dataset, early warning detection systems, data mining, Birth disorders, Sensor phenomena and characterization, Rivers, flow anomaly discovery, Contamination, Petroleum, pruning techniques, Condition monitoring, water quality, environmental science computing, Lakes, water quality problems, Water pollution, condition monitoring, Pollution measurement, search space, smart window enumeration and evaluation of persistence-thresholds method, Water resources, environmental flow monitoring networks, water bodies, smart counter]
Boosting Relational Sequence Alignments
2008 Eighth IEEE International Conference on Data Mining
None
2008
The task of aligning sequences arises in many applications. Classical dynamic programming approaches require the explicit state enumeration in the reward model. This is often impractical: the number of states grows very quickly with the number of domain objects and relations among these objects. Relational sequence alignment aims at exploiting symbolic structure to avoid the full enumeration. This comes at the expense of a more complex reward model selection problem: virtually infinitely many abstraction levels have to be explored. In this paper, we apply gradient-based boosting to leverage this problem. Specifically, we show how to reduce the learning problem to a series of relational regressions problems. The main benefit of this is that interactions between states variables are introduced only as needed, so that the potentially infinite search space is not explicitly considered. As our experimental results show, this boosting approach can significantly improve upon established results in challenging applications.
[Communities, regression analysis, abstraction level, relational regressions problem, Data mining, explicit state enumeration, Relational Sequences, Dynamic programming, learning (artificial intelligence), gradient methods, search problems, Computational biology, domain object, Sequences, Thumb, dynamic programming, classical dynamic programming approach, Boosting, infinite search space, reward model selection problem, Application software, Computer science, symbolic structure, learning problem, Machine learning, gradient-based boosting, boosting relational sequence alignment]
Support Vector Regression for Censored Data (SVRc): A Novel Tool for Survival Analysis
2008 Eighth IEEE International Conference on Data Mining
None
2008
A crucial challenge in predictive modeling for survival analysis is managing censored observations in the data. The Cox proportional hazards model is the standard tool for the analysis of continuous censored survival data. We propose a novel machine learning algorithm, support vector regression for censored data (SVRc) for improved analysis of medical survival data. SVRc leverages the high-dimensional capabilities of traditional SVR while adapting it for use with censored data through a modified asymmetric loss/penalty function which allows censored (left and right censored) data to be processed. We applied the new algorithm to predict the recurrence and disease progression of prostate cancer, breast cancer and lung cancer. Compared with the traditional Cox model, SVRc achieves significant improvement in overall accuracy as well as in the ability to identify high-risk and low-risk patient populations.
[Algorithm design and analysis, Laboratories, regression analysis, Predictive models, asymmetric loss and penalty, Cox proportional hazards model, censored, medical survival data, survival analysis, concordance index, Performance analysis, medical administrative data processing, learning (artificial intelligence), Support vector, data analysis, support vector machines, censored data, Medical treatment, Hazards, Breast cancer, cancer prognosis, Diseases, Support vector machines, support vector regression, Cox model, predictive modeling, Prostate cancer, machine learning algorithm]
Nearest Neighbour Classifiers for Streaming Data with Delayed Labelling
2008 Eighth IEEE International Conference on Data Mining
None
2008
We study streaming data where the true labels come with a delay. The question is whether the online nearest neighbour classifier (IB2 and IB3 here) should employ the unlabelled data. Three strategies are examined: do-nothing, replace and forget. Experiments with 28 data sets show that IB2 benefits from unlabelled data, while IB3 does not.
[Decision support systems, pattern classification, data streaming, Shape, Demography, streaming data, Pattern recognition, nearest neighbour classifier, Remuneration, Data mining, online nearest neighbour classifier, Delay, instance-based learning, Computer science, incremental learning, delayed labelling, Semisupervised learning, Error correction, data handling, IB2 algorithm, Labeling, learning (artificial intelligence), IB3 algorithm, online learning]
WiFIsViz: Effective Visualization of Frequent Itemsets
2008 Eighth IEEE International Conference on Data Mining
None
2008
Frequent itemset mining plays an essential role in the mining of many different patterns. Most existing frequent itemset mining algorithms return the mined results--namely, frequent itemsets--in the form of textual lists. However, the use of visual representation can enhance the user understanding of the inherent relations in a collection of frequent itemsets. In this paper, we propose an effective visualizer, called WiFIsViz, to display the mined frequent itemsets. WiFIsViz provides users with an overview and details about the itemsets. Moreover, this visualizer is also equipped with several interactive features for effective visualization of the frequent itemsets mined from various real-life applications.
[Frequent patterns, visual representation, Social network services, data mining, information retrieval, Displays, WiFIsViz, Data mining, Association rules, frequent itemset visualization, frequent itemset mining, Itemsets, Data visualization, data visualisation, Frequent itemset mining, Frequency, Decision trees, Joining processes, Visualization of mining results]
Fast and Memory Efficient Mining of High Utility Itemsets in Data Streams
2008 Eighth IEEE International Conference on Data Mining
None
2008
Efficient mining of high utility itemsets has become one of the most interesting data mining tasks with broad applications. In this paper, we proposed two efficient one-pass algorithms, MHUI-BIT and MHUI-TID, for mining high utility itemsets from data streams within a transaction-sensitive sliding window. Two effective representations of item information and an extended lexicographical tree-based summary data structure are developed to improve the efficiency of mining high utility itemsets. Experimental results show that the proposed algorithms outperform than the existing algorithms for mining high utility itemsets from data streams.
[Tree data structures, data mining tasks, Costs, Filtering, data mining, trees (mathematics), data streams, utility itemset mining, data structure, transaction-sensitive sliding window, one-pass algorithms, Transaction databases, Electronic mail, Data mining, Application software, Association rules, Computer science, memory efficient mining, Itemsets, lexicographical tree-based summary, data structures]
HIREL: An Incremental Clustering Algorithm for Relational Datasets
2008 Eighth IEEE International Conference on Data Mining
None
2008
Traditional clustering approaches usually analyze static datasets in which objects are kept unchanged after being processed, but many practical datasets are dynamically modified which means some previously learned patterns have to be updated accordingly. Re-clustering the whole dataset from scratch is not a good choice due to the frequent data modifications and the limited out-of-service time, so the development of incremental clustering approaches is highly desirable. Besides that, propositional clustering algorithms are not suitable for relational datasets because of their quadratic computational complexity. In this paper, we propose an incremental clustering algorithm that requires only one pass of the relational dataset. The utilization of the Representative Objects and the balanced Search Tree greatly accelerate the learning procedure. Experimental results prove the effectiveness of our algorithm.
[Algorithm design and analysis, HIREL, Incremental, Data analysis, balanced search tree, learning procedure, Relational databases, Data warehouses, Relational, Data mining, relational databases, tree searching, Clustering, Computational complexity, Computer science, relational dataset, Publishing, pattern clustering, Clustering algorithms, representative object utilization, learning (artificial intelligence), Pattern analysis, incremental clustering algorithm]
Time Sensitive Ranking with Application to Publication Search
2008 Eighth IEEE International Conference on Data Mining
None
2008
Link-based ranking has contributed significantly to the success of Web search. PageRank and HITS are the best known link-based ranking algorithms. These algorithms do not consider an important dimension, the temporal dimension. They favor older pages because these pages have many in-links accumulated over time. Bringing new and quality pages to the users is important because most users want the latest information. Existing remedies to PageRank are mostly heuristic approaches. This paper investigates the temporal aspect of ranking with application to publication search, and proposes a principled method based on the stationary probability distribution of the Markov chain. The proposed techniques are evaluated empirically using a large collection of high energy particle physics publication. The results show that the proposed methods are highly effective.
[Performance evaluation, search engines, link-based ranking, time sensitive, Probability distribution, stationary probability distribution, Data mining, Markov chain, search, time sensitive ranking, publication, Search engines, publishing, Testing, HITS, publication search, Social network services, probability, information retrieval, PageRank, Computer science, Software libraries, Web pages, Markov processes, ranking, Internet, Web search]
Releasing the SVM Classifier with Privacy-Preservation
2008 Eighth IEEE International Conference on Data Mining
None
2008
Support vector machine (SVM) is a widely used tool in classification problem. SVM solves a quadratic optimization problem to decide which instances of training dataset are support vectors, i.e., the necessarily informative instances to form the classifier. The support vectors are intact tuples taken from the training dataset. Releasing the SVM classifier to public use or shipping the SVM classifier to clients will disclose the private content of support vectors, violating the privacy-preservation requirement in some legal or commercial reasons. To the best of our knowledge, there has not been work extending the notion of privacy-preservation to releasing the SVM classifier. In this paper, we propose an approximation approach which post-processes the SVM classifier to protect the private content of support vectors. This approach is designed for the commonly used Gaussian radial basis function kernel. By applying this post-processor on the SVM classifier, the resulted privacy-preserving SVM classifier can be publicly released without exposing the private content of support vectors and is able to provide comparable classification accuracy to the original SVM classifier.
[Data privacy, pattern classification, Law, support vector machines, data mining, approximation approach, quadratic optimization problem, post-processor, Support vector machine, Data mining, Computational complexity, Gaussian radial basis function kernel, Support vector machines, private content, security of data, support vector machine, Support vector machine classification, Training data, Privacy-Preserving, privacy-preservation, Protection, Kernel, SVM classifier, Legal factors]
Text Cube: Computing IR Measures for Multidimensional Text Database Analysis
2008 Eighth IEEE International Conference on Data Mining
None
2008
Since Jim Gray introduced the concept of rdquodata cuberdquo in 1997, data cube, associated with online analytical processing (OLAP), has become a driving engine in data warehouse industry. Because the boom of Internet has given rise to an ever increasing amount of text data associated with other multidimensional information, it is natural to propose a data cube model that integrates the power of traditional OLAP and IR techniques for text. In this paper, we propose a text-cube model on multidimensional text database and study effective OLAP over such data. Two kinds of hierarchies are distinguishable inside: dimensional hierarchy and term hierarchy. By incorporating these hierarchies, we conduct systematic studies on efficient text-cube implementation, OLAP execution and query processing. Our performance study shows the high promise of our methods.
[text analysis, data mining, Optical computing, Material storage, data warehouse industry, query processing, Databases, IR measure, Cost function, multidimensional text database analysis, Multidimensional systems, Data analysis, Navigation, text cube model, dimensional hierarchy, OLAP, Text, term hierarchy, Power system modeling, online analytical processing, data models, Query processing, Cube, Internet, data cube model, data warehouses]
Multi-Space-Mapped SVMs for Multi-class Classification
2008 Eighth IEEE International Conference on Data Mining
None
2008
In SVMs-based multiple classification, it is not always possible to find an appropriate kernel function to map all the classes from different distribution functions into a feature space where they are linearly separable from each other. This is even worse if the number of classes is very large. As a result, the classification accuracy is not as good as expected. In order to improve the performance of SVMs-based multi-classifiers, this paper proposes a method, named multi-space-mapped SVMs, to map the classes into different feature spaces and then classify them. The proposed method reduces the requirements for the kernel function. Substantial experiments have been conducted on one-against-all, one-against-one, FSVM, DDAG algorithms and our algorithm using six UCI data sets. The statistical results show that the proposed method has a higher probability of finding appropriate kernel functions than traditional methods and outperforms others.
[Virtual colonoscopy, support vector machines, SVM-based multi-classifiers, SVM-based multiple classification, FSVM, multi-class classification, Appropriate technology, kernel function, Information technology, multiple classification, one-against-one algorithm, support vector machine, Space technology, Clustering algorithms, Binary trees, one-against-all algorithm, DDAG, Australia, multi-space-mapped SVM, Kernel, Classification tree analysis, Distribution functions]
Spotting Significant Changing Subgraphs in Evolving Graphs
2008 Eighth IEEE International Conference on Data Mining
None
2008
Graphs are popularly used to model structural relationships between objects. In many application domains such as social networks, sensor networks and telecommunication, graphs evolve over time. In this paper, we study a new problem of discovering the subgraphs that exhibit significant changes in evolving graphs. This problem is challenging since it is hard to define changing regions that are closely related to the actual changes (i.e., additions/deletions of edges/nodes) in graphs. We formalize the problem, and design an efficient algorithm that is able to identify the changing subgraphs incrementally. Our experimental results on real datasets show that our solution is very efficient and the resultant subgraphs are of high quality.
[Algorithm design and analysis, Social network services, graph theory, data mining, social networks, Telecommunication traffic, Routing, changing subgraphs, sensor networks, Data mining, Wireless sensor networks, graphs, Query processing, Collaborative work, Bioinformatics, Pattern analysis, evolving graphs]
Classifying High-Dimensional Text and Web Data Using Very Short Patterns
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this paper, we propose the "democratic classifier\
[Web data classification, text analysis, Machine learning algorithms, Humans, pattern-based classification, Classification algorithms, text classification, Data mining, very short pattern-based classification, minimum support threshold, power law based weighing scheme, Voting, Classification, learning (artificial intelligence), feature selection, high-dimensional text classification, Nominations and elections, classification, democratic classifier, machine learning, Support vector machines, Support vector machine classification, Frequency, Internet, Qualifications, interestingness measures]
A Practical Approach to Classify Evolving Data Streams: Training with Limited Amount of Labeled Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
Recent approaches in classifying evolving data streams are based on supervised learning algorithms, which can be trained with labeled data only. Manual labeling of data is both costly and time consuming. Therefore, in a real streaming environment, where huge volumes of data appear at a high speed, labeled data may be very scarce. Thus, only a limited amount of training data may be available for building the classification models, leading to poorly trained classifiers. We apply a novel technique to overcome this problem by building a classification model from a training set having both unlabeled and a small amount of labeled instances. This model is built as micro-clusters using semi-supervised clustering technique and classification is performed with kappa-nearest neighbor algorithm. An ensemble of these models is used to classify the unlabeled data. Empirical evaluation on both synthetic data and real botnet traffic reveals that our approach, using only a small amount of labeled data for training, outperforms state-of-the-art stream classification algorithms that use twenty times more labeled data than our approach.
[pattern classification, microclusters, Buffer storage, data streams, real streaming environment, Probability distribution, Classification algorithms, Data mining, Computer science, state-of-the-art stream classification algorithms, Data stream, supervised learning algorithms, pattern clustering, ensemble classification, Supervised learning, Training data, Clustering algorithms, Labeling, learning (artificial intelligence), data labeling, semi-supervised clustering, Testing]
Spatiotemporal Relational Probability Trees: An Introduction
2008 Eighth IEEE International Conference on Data Mining
None
2008
We introduce spatiotemporal relational probability trees (SRPTs), probability estimation trees for relational data that can vary in both space and time. The SRPT algorithm addresses the exponential increase in search complexity through sampling. We validate the SRPT using a simulated data set and we empirically demonstrate the SRPT algorithm on two real-world data sets.
[Logic programming, probability, trees (mathematics), spatiotemporal relational probability trees, statistical relational data mining, Discrete event simulation, probability estimation trees, Data mining, Floods, relational databases, Storms, Space technology, spatiotemporal, Sampling methods, Spatiotemporal phenomena, Tornadoes, Decision trees, real-world data sets]
Stream Sequential Pattern Mining with Precise Error Bounds
2008 Eighth IEEE International Conference on Data Mining
None
2008
Sequential pattern mining is an interesting data mining problem with many real-world applications. This problem has been studied extensively in static databases. However, in recent years, emerging applications have introduced a new form of data called data stream. In a data stream, new elements are generated continuously. This poses additional constraints on the methods used for mining such data: memory usage is restricted, the infinitely flowing original dataset cannot be scanned multiple times, and current results should be available on demand.This paper introduces two effective methods for mining sequential patterns from data streams: the SS-BE method and the SS-MB method. The proposed methods break the stream into batches and only process each batch once. The two methods use different pruning strategies that restrict the memory usage but can still guarantee that all true sequential patterns are output at the end of any batch. Both algorithms scale linearly in execution time as the number of sequences grows, making them effective methods for sequential pattern mining in data streams. The experimental results also show that our methods are very accurate in that only a small fraction of the patterns that are output are false positives. Even for these false positives, SS-BE guarantees that their true support is above a pre-defined threshold.
[Tree data structures, Sequences, Data analysis, data mining, Telecommunication traffic, memory usage, Data mining, stream sequential pattern mining, precise error bounds, data stream mining, Databases, Memory management, DNA, static databases, pruning strategies, sequential pattern mining, Pattern analysis, data mining problem]
Organic Pie Charts
2008 Eighth IEEE International Conference on Data Mining
None
2008
We present a new visualization of the distance and cluster structure of high dimensional data. It is particularly well suited for analysis tasks of users unfamiliar with complex data analysis techniques as it builds on the well known concept of pie charts. The non-linear projection capabilities of Emergent Self-Organizing Maps (ESOM) are used to generate a topology-preserving ordering of the data points on a circle. The distance structure within the high dimensional space is visualized on the circle analogously to the U-Matrix method for two-dimensional SOM. The resulting display resembles pie charts but has an organic structure that naturally emerges from the data. Pie segments correspond to groups of similar data points. Boundaries between segments represent low density regions with larger distances among neighboring points in the high dimensional space. The representation of distances in the form of a periodic sequence of values makes time series segmentation applicable to automated clustering of the data that is in sync with the visualization. We discuss the usefulness of the method on a variety of data sets to demonstrate the applicability in applications such as document analysis or customer segmentation.
[visualization, organic pie chart, emergent self-organizing map, complex data analysis, Data mining, nonlinear projection capability, Training, self-organising feature maps, data visualisation, periodic sequence, U-Matrix method, Trajectory, distance structure, data analysis, topology-preserving data point ordering, time series segmentation, time series, pie chart, self-organizing maps, Equations, high dimensional data cluster structure, high dimensional data visualization, Continuous wavelet transforms, pattern clustering, Data visualization, Distance measurement, clustering]
Frequent Subgraph Retrieval in Geometric Graph Databases
2008 Eighth IEEE International Conference on Data Mining
None
2008
Discovery of knowledge from geometric graph databases is of particular importance in chemistry and biology, because chemical compounds and proteins are represented as graphs with 3D geometric coordinates. In such applications, scientists are not interested in the statistics of the whole database. Instead they need information about a novel drug candidate or protein at hand, represented as a query graph. We propose a polynomial-delay algorithm for geometric frequent subgraph retrieval. It enumerates all subgraphs of a single given query graph which are frequent geometric epsi-subgraphs under the entire class of rigid geometric transformations in a database. By using geometric epsi-subgraphs, we achieve tolerance against variations in geometry. We compare the proposed algorithm to gSpan on chemical compound data, and we show that for a given minimum support the total number of frequent patterns is substantially limited by requiring geometric matching. Although the computation time per pattern is larger than for non-geometric graph mining, the total time is within a reasonable level even for small minimum support.
[Drugs, geometric graph databases, query graph, geometric frequent subgraph retrieval, geometric patterns, graph theory, data mining, Information retrieval, Spatial databases, knowledge discovery, database management systems, Chemical compounds, Statistics, Proteins, Geometry, query processing, geometric graph mining, Chemistry, chemical compound data, Polynomials, geometry, Pattern matching, 3D geometric coordinates, frequent graph mining]
Alert Detection in System Logs
2008 Eighth IEEE International Conference on Data Mining
None
2008
We present Nodeinfo, an unsupervised algorithm for anomaly detection in system logs. We demonstrate Nodeinfo's effectiveness on data from four of the world's most powerful supercomputers: using logs representing over 746 million processor-hours, in which anomalous events called alerts were manually tagged for scoring, we aim to automatically identify the regions of the log containing those alerts. We formalize the alert detection task in these terms, describe how Nodeinfo uses the information entropy of message terms to identify alerts, and present an online version of this algorithm, which is now in production use. This is the first work to investigate alert detection on (several) publicly-available supercomputer system logs, thereby providing a reproducible performance baseline.
[Production systems, information entropy, Costs, system logs, Nodeinfo, Laboratories, fault detection, Supercomputers, anomaly detection, Data mining, Personnel, Information entropy, message terms, entropy, security of data, USA Councils, Fault detection, alert detection, hpc, unsupervised algorithm, information theory, Detection algorithms, log analysis]
Variance Minimization Least Squares Support Vector Machines for Time Series Analysis
2008 Eighth IEEE International Conference on Data Mining
None
2008
Here we propose a novel machine learning method for time series forecasting which is based on the widely-used Least Squares Support Vector Machine (LS-SVM) approach. The objective function of our method contains a weighted variance minimization part as well. This modification makes the method more efficient in time series forecasting, as this paper will show. The proposed method is a generalization of the well-known LS-SVM algorithm. It has similar advantages like the applicability of the kernel-trick, it has a linear and unique solution, and a short computational time, but can perform better in certain scenarios. The main purpose of this paper is to introduce the novel Variance Minimization Least Squares Support Vector Machine (VMLS-SVM) method and to show its superiority through experimental results using standard benchmark time series prediction datasets.
[time series analysis, Predictive models, SVM, machine learning method, weighted variance minimization, learning (artificial intelligence), time series forecasting, Minimization methods, least squares approximations, variance minimization least squares support vector machines, support vector machines, Time series analysis, Artificial neural networks, time series, Least Squares SVM, Quadratic programming, Least squares methods, Support vector machines, Neural networks, forecasting theory, Time Series, standard benchmark time series prediction datasets, Risk management, minimisation, Analysis of variance]
Quantitative Association Analysis Using Tree Hierarchies
2008 Eighth IEEE International Conference on Data Mining
None
2008
Association analysis arises in many important applications such as bioinformatics and business intelligence. Given a large collection of measurements over a set of samples, association analysis aims to find dependencies of target variables to subsets of measurements. Most previous algorithms adopt a two-stage approach; they first group samples based on the similarity in the subset of measurements, and then they examine the association between these groups and the specified target variables without considering the inter-group similarities or alternative groupings. This can lead to cases where the strength of association depends significantly on arbitrary clustering choices. In this paper, we propose a tree-based method for quantitative association analysis. Tree hierarchies derived from sample similarities represent many possible sample groupings. They also provide a natural way to incorporate domain knowledge such as ontologies and to identify and remove outliers. Given a tree hierarchy, our association analysis evaluates all possible groupings and selects the one with strongest association to the target variable. We introduce an efficient algorithm, TreeQA, to systematically explore the search-space of all possible groupings in a set of input trees, with integrated permutation tests. Experimental results show that TreeQA is able to handlelarge-scale association analysis very efficiently and is more effective and robust in association analysis than previous methods.
[TreeQA, Ontologies, Tree Hierarchies, sensor fusion, set theory, Data mining, business intelligence, tree-based method, intergroup similarities, Filters, Association Analysis, Genetics, Bioinformatics, Testing, search-space, trees (mathematics), quantitative association analysis, Discrete wavelet transforms, Application software, Computer science, pattern clustering, association analysis, bioinformatics, ontologies (artificial intelligence), tree hierarchies, Analysis of variance]
Sparse Maximum Margin Logistic Regression for Credit Scoring
2008 Eighth IEEE International Conference on Data Mining
None
2008
The objective of credit scoring model is to categorize the applicants as either accepted or rejected debtors prior to granting credit. A modified logistic loss function is proposed which can approximate hinge loss and therefore the resulting model, maximum margin logistic regression (MMLR), has the classification capability of support vector machine (SVM) with low computational cost. Finally, to classify credit applicants, an efficient algorithm is also described for MMLR based on epsilon-boosting which can provide sparse estimation of coefficients for better stability and interpretability.
[sparse maximum margin logistic regression, Demography, Stability, support vector machines, regression analysis, Fasteners, epsilon boosting, logistics, Data mining, Support vector machines, Industrial training, support vector machine, Support vector machine classification, credit scoring model, Computational efficiency, Risk management, logistic loss function, finance, Logistics]
Similarity Learning for Nearest Neighbor Classification
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this paper, we propose an algorithm for learning a general class of similarity measures for kNN classification. This class encompasses, among others, the standard cosine measure, as well as the Dice and Jaccard coefficients. The algorithm we propose is an extension of the voted perceptron algorithm and allows one to learn different types of similarity functions (either based on diagonal, symmetric or asymmetric similarity matrices). The results we obtained show that learning similarity measures yields significant improvements on several collections, for two prediction rules: the standard kNN rule, which was our primary goal, and a symmetric version of it.
[Data Mining, diagonal matrix, Data mining, Machine Learning, Nearest Neighbor Classification, symmetric matrix, Databases, nearest neighbor classification, asymmetric matrix, Measurement standards, voted perceptron algorithm, learning (artificial intelligence), kNN classification, pattern classification, Symmetric matrices, Similarity Learning, standard cosine measure, Jaccard coefficient, Dice coefficient, Pattern recognition, similarity learning, Equations, Nearest neighbor searches, matrix algebra, Euclidean distance, Gaussian processes, Machine learning]
RBNBC: Repeat Based Naive Bayes Classifier for Biological Sequences
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this paper, we present RBNBC, a repeat based Naive Bayes classifier of bio-sequences that uses maximal frequent subsequences as features. RBNBC's design is based on generic ideas that can apply to other domains where the data is organized as collections of sequences. Specifically, RBNBC uses a novel formulation of Naive Bayes that incorporates repeated occurrences of subsequences within each sequence. Our extensive experiments on two collections of protein families show that it performs as well as existing state-of-the-art probabilistic classifiers for bio-sequences. This is surprising as it is a pure data mining based generic classifier that does not require domain-specific background knowledge. We note that domain-specific ideas could further increase its performance.
[pattern classification, Optimization methods, data mining, Data engineering, Data Mining, Frequency estimation, Entropy, Spatial databases, Data mining, repeat based Naive Bayes classifier, Proteins, Support vector machines, biological sequences, generic classifier, Naive Bayes, Bayesian methods, biology computing, Classification, Feature extraction, state-of-the-art probabilistic classifiers, Bayes methods, domain-specific background knowledge, Biological Sequence]
Multi-label Classification Using Ensembles of Pruned Sets
2008 Eighth IEEE International Conference on Data Mining
None
2008
This paper presents a pruned sets method (PS) for multi-label classification. It is centred on the concept of treating sets of labels as single labels. This allows the classification process to inherently take into account correlations between labels. By pruning these sets, PS focuses only on the most important correlations, which reduces complexity and improves accuracy. By combining pruned sets in an ensemble scheme (EPS), new label sets can be formed to adapt to irregular or complex data. The results from experimental evaluation on a variety of multi-label datasets show that [E]PS can achieve better performance and train much faster than other multi-label methods.
[pattern classification, Genomics, data mining, problem transformation, multi-label classification, Data mining, Nearest neighbor searches, Computer science, Support vector machines, pruned sets method, Text categorization, Layout, Support vector machine classification, multilabel classification, ensemble scheme, Tin, Bioinformatics]
Active Learning of Equivalence Relations by Minimizing the Expected Loss Using Constraint Inference
2008 Eighth IEEE International Conference on Data Mining
None
2008
Selecting promising queries is the key to effective active learning. In this paper, we investigate selection techniques for the task of learning an equivalence relation where the queries are about pairs of objects. As the target relation satisfies the axioms of transitivity, from one queried pair additional constraints can be inferred. We derive both the upper and lower bound on the number of queries needed to converge to the optimal solution. Besides restricting the set of possible solutions, constraints can be used as training data for learning a similarity measure. For selecting queries that result in a large number of meaningful constraints, we present an approximative optimal selection technique that greedily minimizes the expected loss in each round of active learning. This technique makes use of inference of expected constraints. Besides the theoretical results, an extensive evaluation for the application of record linkage shows empirically that the proposed selection method leads to both interesting and a high number of constraints.
[Uncertainty, Record Linkage, Social network services, Merging, Data mining, queries, Active Learning, Couplings, Computer science, equivalence relations, active learning, Training data, Machine learning, Constraint theory, Sampling methods, Equivalence Relation, learning (artificial intelligence), constraint inference]
Iterative Subgraph Mining for Principal Component Analysis
2008 Eighth IEEE International Conference on Data Mining
None
2008
Graph mining methods enumerate frequent subgraphs efficiently, but they are not necessarily good features for machine learning due to high correlation among features. Thus it makes sense to perform principal component analysis to reduce the dimensionality and create decorrelated features. We present a novel iterative mining algorithm that captures informative patterns corresponding to major entries of top principal components. It repeatedly calls weighted substructure mining where example weights are updated in each iteration. The Lanczos algorithm, a standard algorithm of eigen decomposition, is employed to update the weights. In experiments, our patterns are shown to approximate the principal components obtained by frequent mining.
[iterative methods, summarization, graph theory, mathematics computing, data mining, weighted substructure mining, Data mining, lanczos algorithm, Cybernetics, eigenvalues and eigenfunctions, data reduction, dimensionality reduction, iterative subgraph mining, Iterative methods, Informatics, eigen decomposition, Covariance matrix, machine learning, Least squares approximation, PCA, graph mining, Lanczos algorithm, Approximation error, Iterative algorithms, principal component analysis, Principal component analysis, Indexing]
Clustering Geospatial Objects via Hidden Markov Random Fields
2008 Eighth IEEE International Conference on Data Mining
None
2008
This paper addresses the problem of clustering objects located and correlated geographically and containing multiple attributes. For the clustering problem, it is necessary to consider both the similarities of the attributes and the spatial dependencies of the objects. A new clustering framework using hidden Markov random fields (HMRFs) and Gaussian distributions and new potential models of HMRFs for irregularly located geospatial objects are proposed in this paper. Experimental results for systematic data and two real-world data showed the availability of the proposed algorithms.
[hidden Markov models, geospatial object clustering, Gaussian distributions, Hidden Markov models, hidden Markov random fields, Gaussian distribution, geophysics computing, Data mining]
Collective Latent Dirichlet Allocation
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this paper, we propose a new variant of latent Dirichlet allocation (LDA): Collective LDA (C-LDA), for multiple corpora modeling. C-LDA combines multiple corpora during learning such that it can transfer knowledge from one corpus to another; meanwhile it keeps a discriminative node which represents the corpus ID to constrain the learned topics in each corpus. Compared with LDA locally applied to the target corpus, C-LDA results in refined topic-word distribution, while compared with applying LDA globally and straightforwardly to the combined corpus, C-LDA keeps each topic only for one corpus. We demonstrate that C-LDA has improved performance with these advantages by experiments on several benchmark document data sets.
[Text mining, document handling, Content based retrieval, Laboratories, Information retrieval, multiple corpora modeling, document classification, Data mining, classification, machine learning, Computer science, collective latent Dirichlet allocation, knowledge transfer, collective LDA, Web pages, Machine learning, Natural language processing, Linear discriminant analysis, learning (artificial intelligence), topic-word distribution]
Document-Word Co-regularization for Semi-supervised Sentiment Analysis
2008 Eighth IEEE International Conference on Data Mining
None
2008
The goal of sentiment prediction is to automatically identify whether a given piece of text expresses positive or negative opinion towards a topic of interest. One can pose sentiment prediction as a standard text categorization problem, but gathering labeled data turns out to be a bottleneck. Fortunately, background knowledge is often available in the form of prior information about the sentiment polarity of words in a lexicon. Moreover, in many applications abundant unlabeled data is also available. In this paper, we propose a novel semi-supervised sentiment prediction algorithm that utilizes lexical prior knowledge in conjunction with unlabeled examples. Our method is based on joint sentiment analysis of documents and words based on a bipartite graph representation of the data. We present an empirical study on a diverse collection of sentiment prediction problems which confirms that our semi-supervised lexical models significantly outperform purely supervised and competing semi-supervised techniques.
[text analysis, Text analysis, graph theory, standard regularized least square, Sentiment Analysis, Data mining, word processing, Semi-supervised Learning, semi supervised sentiment prediction algorithm, Motion pictures, Prediction algorithms, text categorization problem, Discussion forums, least squares approximations, Blogs, Vectors, bipartite graph representation, Graph Transduction, Linear models, document joint sentiment analysis, Text categorization, prediction algorithm, document-word co-regularization, Machine learning, Frequency]
A Non-parametric Approach to Pair-Wise Dynamic Topic Correlation Detection
2008 Eighth IEEE International Conference on Data Mining
None
2008
We introduce dynamic correlated topic models (DCTM) for analyzing discrete data over time. This model is inspired by the hierarchical Gaussian process latent variable models (GP-LVM). DCTM is essentially a non-linear dimension reduction technique which is capable of (1) detecting topic evolution within a document corpus, (2) discovering topic correlations between document corpora, and (3) monitoring topic and correlation trends dynamically. Unlike generative aspect models such like LDA, DCTM demonstrates a much faster converging rate with better model fitting to the data. We empirically assess our approach using 268,231 scientific documents, from the year 1988 to 2005. Posterior inferences suggest that DCTM is useful for capturing topic and correlation dynamics, as well as predicting their trends.
[document handling, pairwise dynamic topic correlation detection, Parameter estimation, hierarchical Gaussian process latent variable models, Predictive models, Gaussian distribution, document corpora, Data engineering, Data mining, Statistics, Computer science, correlation analysis, topic correlations, Gaussian processes, nonparametric approach, nonlinear dimension reduction technique, topic models, Linear discriminant analysis, topic evolution, Logistics]
Block-Iterative Algorithms for Non-negative Matrix Approximation
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this paper we present new algorithms for non-negative matrix approximation (NMA), commonly known as the NMF problem. Our methods improve upon the well-known methods of Lee &amp; Seung [12] for both the Frobenius norm as well the Kullback-Leibler divergence versions of the problem. For the latter problem, our results are especially interesting because it seems to have witnessed much lesser algorithmic progress as compared to the Frobenius norm NMA problem. Our algorithms are based on a particular block-iterative acceleration technique for EM, which preserves the multiplicative nature of the updates and also ensures monotonicity. Furthermore, our algorithms also naturally apply to the Bregman-divergence NMA algorithms of [6]. Experimentally,we show that our algorithms outperform the traditional Lee/Seung approach most of the time.
[Data analysis, Minimization methods, nonnegative matrix approximation, Bregman-divergence NMA algorithms, Nonnegative matrix factorization, Frobenius norm, block-iterative algorithms, approximations, Vectors, Matrix decomposition, Data mining, low-rank approximation, Bregman divergence, Linear algebra, algorithm theory, Approximation algorithms, Acceleration, Kullback-Leibler divergence versions, Biomedical imaging]
A Novel Method of Combined Feature Extraction for Recognition
2008 Eighth IEEE International Conference on Data Mining
None
2008
Multimodal recognition is an emerging technique to overcome the non-robustness of the unimodal recognition in real applications. Canonical correlation analysis (CCA) has been employed as a powerful tool for feature fusion in the realization of such multimodal system. However, CCA is the unsupervised feature extraction and it does not utilize the class information of the samples, resulting in the constraint of the recognition performance. In this paper, the class information is incorporated into the framework of CCA for combined feature extraction, and a novel method of combined feature extraction for multimodal recognition, called discriminative canonical correlation analysis (DCCA), is proposed. The experiments show that DCCA outperforms some related methods of both unimodal recognition and multimodal recognition.
[discriminative canonical correlation analysis, Data engineering, Electronic mail, Pattern recognition, Data mining, Application software, Computer science, feature fusion, Space technology, feature extraction, Speech recognition, Feature extraction, unimodal recognition, Signal mapping, image recognition, multimodal recognition]
Prediction of Skin Penetration Using Machine Learning Methods
2008 Eighth IEEE International Conference on Data Mining
None
2008
Improving predictions of the skin permeability coefficient is a difficult problem. It is also an important issue with the increasing use of skin patches as a means of drug delivery. In this work, we apply K-nearest-neighbour regression, single layer networks, mixture of experts and Gaussian processes to predict the permeability coefficient. We obtain a considerable improvement over the quantitative structure-activity relationship (QSARs) predictors. We show that using five features, which are molecular weight, solubility parameter, lipophilicity, the number of hydrogen bonding acceptor and donor groups, can produce better predictions than the one using only lipophilicity and the molecular weight. The Gaussian process regression with five compound features gives the best performance in this work.
[K-nearest-neighbour regression, Hydrogen, Medical treatment, machine learning methods, Lipidomics, Drug delivery, Permeability, skin penetration, Learning systems, skin permeability coefficient, Absorption, quantitative structure-activity relationship, regression, Gaussian processes, permeability coefficient, Skin, learning (artificial intelligence), Bonding, medical computing]
A Topic Modeling Approach and Its Integration into the Random Walk Framework for Academic Search
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this paper, we propose a unified topic modeling approach and its integration into the random walk framework for academic search. Specifically, we present a topic model for simultaneously modeling papers, authors, and publication venues. We combine the proposed topic model into the random walk framework. Experimental results show that our proposed approach for academic search significantly outperforms the baseline methods of using BM25 and language model, and those of using the existing topic models (including pLSI, LDA, and the AT model).
[Smoothing methods, search engines, Neodymium, data mining, information retrieval, Information retrieval, language model, Data mining, Computer science, random walk framework, BM25, Search engines, academic search, Frequency, unified topic modeling, Linear discriminant analysis]
Sequence Mining Automata: A New Technique for Mining Frequent Sequences under Regular Expressions
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this paper we study the problem of mining frequent sequences satisfying a given regular expression. Previous approaches to solve this problem were focusing on its search space, pushing (in some way) the given regular expression to prune unpromising candidate patterns. On the contrary, we focus completely on the given input data and regular expression. We introduce sequence mining automata (SMA), a specialized kind of Petri Net that while reading input sequences, it produces for each sequence all and only the patterns contained in the sequence and that satisfy the given regular expression. Based on this automaton, we develop a family of algorithms. Our thorough experimentation on different datasets and application domains confirms that in many cases our methods outperform the current state of the art of frequent sequence mining algorithms using regular expressions (in some cases of orders of magnitude).
[Tree data structures, pattern classification, Adaptive algorithm, Laboratories, Taxonomy, Petri nets, data mining, sequence mining automata, Data structures, petri net, regular expression, Data mining, Indexes, mining frequent sequences, unpromising candidate patterns, Databases, Automata, search space, Time factors, search problems]
Filling in the Blanks - Krimp Minimisation for Missing Data
2008 Eighth IEEE International Conference on Data Mining
None
2008
Many data sets are incomplete. For correct analysis of such data, one can either use algorithms that are designed to handle missing data or use imputation. Imputation has the benefit that it allows for any type of data analysis. Obviously, this can only lead to proper conclusions if the provided data completion is both highly accurate and maintains all statistics of the original data. In this paper, we present three data completion methods that are built on the MDL-based KRIMP algorithm. Here, we also follow the MDL principle, i.e. the completed database that can be compressed best, is the best completion because it adheres best to the patterns in the data. By using local patterns, as opposed to a global model, KRIMP captures the structure of the data in detail. Experiments show that both in terms of accuracy and expected differences of any marginal, better data reconstructions are provided than the state of the art, Structural EM.
[Algorithm design and analysis, missing data estimation, Data analysis, Statistical analysis, data analysis, data mining, KRIMP minimisation, Data mining, Statistics, missing data, Computer science, local patterns, database, Databases, Krimp, imputation, MDL, DNA, data sets, Filling, Iterative methods]
Computational Discovery of Motifs Using Hierarchical Clustering Techniques
2008 Eighth IEEE International Conference on Data Mining
None
2008
Discovery of motifs plays a key role in understanding gene regulation in organisms. Existing tools for motif discovery demonstrate some weaknesses in dealing with reliability and scalability. Therefore, development of advanced algorithms for resolving this problem will be useful. This paper aims to develop data mining techniques for discovering motifs. A mismatch based hierarchical clustering algorithm is proposed in this paper, where three heuristic rules for classifying clusters and a post-processing for ranking and refining the clusters are employed in the algorithm. Our algorithm is evaluated using two sets of DNA sequences with comparisons. Results demonstrate that the proposed techniques in this paper outperform MEME, AlignACE and SOMBRERO for most of the testing datasets.
[pattern classification, Sequences, data mining, Data engineering, Reliability engineering, Organisms, Data mining, Gene expression, Computer science, genetics, pattern clustering, gene regulation, Clustering algorithms, DNA, bioinformatics, Frequency, motif discovery, mismatch based hierarchical clustering algorithm, heuristic rule]
Inference Analysis in Privacy-Preserving Data Re-publishing
2008 Eighth IEEE International Conference on Data Mining
None
2008
Privacy-Preserving Data Re-publishing (PPDR) deals with publishing microdata in dynamic scenarios. Due to privacy concerns, data must be disguised before being published. Research in privacy-preserving data publishing (PPDP) has proposed many such methods on static data. In PPDR, multiple appeared records can be used to infer private information of other records. Therefore, inference channels exist among different releases. To understand the privacy property of data re-publishing, we need to analyze the impact of these inference channels. Previous studies show such analysis when data are updated or disguised in special ways, however, no general method has been proposed. Using the Maximum Entropy Modeling method, we have developed a general solution. Our method can conduct inference analysis when data are arbitrarily updated or arbitrarily disguised using either generalization or bucketization, two most common data disguise methods in PPDR. Through analysis and experiments, we demonstrate the advantage and the effectiveness of our method.
[Data privacy, maximum entropy modeling method, Data analysis, Uncertainty, Entropy, Data mining, inference mechanisms, privacy-preserving data re-publishing, microdata publishing, Diseases, Publishing, USA Councils, Lungs, generalization, bucketization, inference analysis, data disguise methods, maximum entropy methods, data privacy, Diabetes, inference channels]
Using Wikipedia for Co-clustering Based Cross-Domain Text Classification
2008 Eighth IEEE International Conference on Data Mining
None
2008
Traditional approaches to document classification requires labeled data in order to construct reliable and accurate classifiers. Unfortunately, labeled data are seldom available, and often too expensive to obtain. Given a learning task for which training data are not available, abundant labeled data may exist for a different but related domain. One would like to use the related labeled data as auxiliary information to accomplish the classification task in the target domain. Recently, the paradigm of transfer learning has been introduced to enable effective learning strategies when auxiliary data obey a different probability distribution. A co-clustering based classification algorithm has been previously proposed to tackle cross-domain text classification. In this work, we extend the idea underlying this approach by making the latent semantic relationship between the two domains explicit. This goal is achieved with the use of Wikipedia. As a result, the pathway that allows to propagate labels between the two domains not only captures common words, but also semantic concepts based on the content of documents. We empirically demonstrate the efficacy of our semantic-based approach to cross-domain classification using a variety of real data.
[learning strategy, text analysis, Dictionaries, Transfer learning, Wikipedia, Probability distribution, Classification algorithms, Data mining, statistical distributions, co-clustering-based cross-domain text classification, latent semantic relationship, Training data, Cross-domain text classification, probability distribution, wikipedia, learning (artificial intelligence), Co-clustering, document classification, classification, Bridges, Computer science, pattern clustering, Text categorization, Asia, Web sites]
Iterative Set Expansion of Named Entities Using the Web
2008 Eighth IEEE International Conference on Data Mining
None
2008
Set expansion refers to expanding a partial set of "seed" objects into a more complete set. One system that does set expansion is SEAL (set expander for any language), which expands entities automatically by utilizing resources from the Web in a language independent fashion. In a previous study, SEAL showed good set expansion performance using three seed entities; however, when given a larger set of seeds (e.g., ten), SEAL's expansion method performs poorly. In this paper, we present iterative SEAL (iSEAL), which allows a user to provide many seeds. Briefly, iSEAL makes several calls to SEAL, each call using a small number of seeds. We also show that iSEAL can be used in a "bootstrapping" manner, where each call to SEAL uses a mixture of user-provided and self-generated seeds. We show that the bootstrapping version of iSEAL obtains better results than SEAL even when using fewer user-provided seeds. In addition, we compare the performance of various ranking algorithms used in iSEAL, and show that the choice of ranking method has a small effect on performance when all seeds are user-provided, but a large effect when iSEAL is bootstrapped. In particular, we show that random walk with restart is nearly as good as Bayesian sets with user-provided seeds, and performs best with bootstrapped seeds.
[iterative methods, TV, bootstrapped seeds, iterative set expansion, Watches, iterative SEAL, iSEAL, seal, HTML, named entities, Data mining, set expander, seed entities, random walk, set expansion, USA Councils, self-generated seeds, Motion pictures, Natural languages, Bayesian sets, bootstrapping, Bayesian methods, Markup languages, Web, Seals, Bayes methods, Internet, bootstrapping version]
Experimental Evaluation of the Value of Structure: How to Efficiently Exploit Interdependencies in Sequence Labeling
2008 Eighth IEEE International Conference on Data Mining
None
2008
Many problems in natural language processing, information extraction or bioinformatics consist in predicting a label for each element of a sequence of observations. The sequence of labels generally presents multiple dependencies that restrict the possible labels the elements can take. Therefore, relations between labels intuitively provide information valuable for the prediction. Several approaches have been proposed to take advantage of this additional information. However, experimental results show that taking relations into account does not always improve prediction performances, while it significantly increases the computational cost of both learning and prediction. In this work, we aim at both explaining these surprising results and proposing a simple but computationally efficient approach for labeling sequences.
[Machine learning algorithms, natural language processing, sequence labeling, Buildings, Data mining, Learning systems, multiple dependencies, information extraction, Natural language processing, Inference algorithms, Computational efficiency, Dynamic programming, Labeling, Bioinformatics]
Pseudolikelihood EM for Within-network Relational Learning
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this work, we study the problem of within-network relational learning and inference, where models are learned on a partially labeled relational dataset and then are applied to predict the classes of unlabeled instances in the same graph. We categorize recent work in statistical relational learning into three alternative approaches for this setting: disjoint learning with disjoint inference, disjoint learning with collective inference, and collective learning with collective inference. Models from each of these categories has been employed previously in different settings, but to our knowledge there has been no systematic comparison of models from all three categories. In this paper, we develop a novel pseudolikelihood EM method that facilitates more general collective learning and collective inference on partially labeled relational networks. We then compare this method to competing methods from the other categories on both synthetic and real-world data. We show that collective learning and inference with the pseudolikelihood EM approach achieves significantly higher accuracy than the other types of models when there are a moderate number of labeled examples in the data graph.
[graph theory, within-network relational learning, disjoint inference, data graph, Predictive models, statistical relational learning, Data mining, inference mechanisms, Statistics, disjoint learning, partially labeled relational networks, Computer science, collective inference, Training data, pseudolikelihood EM, Semisupervised learning, expectation-maximisation algorithm, Inference algorithms, partially labeled relational dataset, Autocorrelation, learning (artificial intelligence), Testing]
Publishing Sensitive Transactions for Itemset Utility
2008 Eighth IEEE International Conference on Data Mining
None
2008
We consider the problem of publishing sensitive transaction data with privacy preservation. High dimensionality of transaction data poses unique challenges on data privacy and data utility. On one hand, re-identification attacks tend to use a subset of items that infrequently occur in transactions, called moles. On the other hand, data mining applications typically depend on subsets of items that frequently occur in transactions, called nuggets. Thus the problem is how to eliminate all moles while retaining nuggets as much as possible. A challenge is that moles and nuggets are multi-dimensional with exponential growth and are tangled together by shared items. We present a novel and scalable solution to this problem. The novelty lies in a compact border data structure that eliminates the need of generating all moles and nuggets.
[sensitive transactions, Data privacy, Data analysis, data mining, Relational databases, Data structures, Transaction databases, Data mining, itemset utility, privacy-perservation, Power measurement, Publishing, Itemsets, Motion pictures, data privacy, data utility, data publishing, data mining applications, transaction]
Learning the Latent Semantic Space for Ranking in Text Retrieval
2008 Eighth IEEE International Conference on Data Mining
None
2008
Subspace learning techniques for text analysis, such as latent semantic indexing (LSI), have been widely studied in the past decade. However, to our best knowledge, no previous study has leveraged the rank information for subspace learning in ranking tasks. In this paper, we propose a novel algorithm, called learning latent semantics for ranking (LLSR), to seek the optimal latent semantic space tailored to the ranking tasks. We first present a dual explanation for the classical latent semantic indexing (LSI) algorithm, namely learning the so-called latent semantic space (LSS) to encode the data information. Then, to handle the increasing amount of training data for the practical ranking tasks, we propose a novel objective function to derive the optimal LSS for ranking. Experimental results on two SMART sub-collections and a TREC dataset show that LLSR effectively improves the ranking performance compared with the classical LSI algorithm and ranking without subspace learning.
[Text mining, text analysis, Text analysis, latent semantic indexing, subspace learning technique, information retrieval, Information retrieval, Data engineering, Routing, text retrieval, Large scale integration, Data mining, Ranking, Asia, Training data, latent semantic space, learning (artificial intelligence), trained data handling, Latent Semantic Space, ranking task, Indexing]
Robust Time-Referenced Segmentation of Moving Object Trajectories
2008 Eighth IEEE International Conference on Data Mining
None
2008
Trajectory segmentation is the process of partitioning a given trajectory into a small number of homogeneous segments w.r.t. some criteria. Conventional segmentation techniques only focus on the spatial features of the movement and could lead to spatially homogeneous segments but with presumably dissimilar temporal structures. Furthermore, trajectories could be over-segmented in the presence of outliers. In this paper, we propose a family of three trajectory segmentation methods that takes into account both geospatial and temporal structures of movement for the segmentation and is also robust with respect to time-referenced spatial outliers. The effectiveness of our methods is empirically demonstrated over three real-world datasets.
[Data analysis, Tracking, Segmentation, Humans, temporal structure, outlier, object detection, geospatial structure, Data mining, trajectory, robust time-referenced segmentation, Vehicles, image motion analysis, time-referenced spatial outlier, Animals, image segmentation, moving object trajectory, Sampling methods, Robustness, Trajectory, Joining processes, spatio-temporal]
Maximum Margin Embedding
2008 Eighth IEEE International Conference on Data Mining
None
2008
We propose a new dimensionality reduction method called maximum margin embedding (MME), which targets to projecting data samples into the most discriminative subspace, where clusters are most well-separated. Specifically, MME projects input patterns onto the normal of the maximum margin separating hyperplanes. As a result, MME only depends on the geometry of the optimal decision boundary and not on the distribution of those data points lying further away from this boundary. Technically, MME is formulated as an integer programming problem and we propose a cutting plane algorithm to solve it. Moreover, we prove theoretically that the computational time of MME scales linearly with the dataset size. Experimental results on both toy and real world datasets demonstrate the effectiveness of MME.
[discriminative subspace, cutting plane algorithm, Error analysis, integer programming, Laboratories, data mining, Linear programming, Probability distribution, Data mining, Geometry, Information science, maximum margin embedding, optimal decision boundary, Cost function, integer programming problem, learning (artificial intelligence), Intelligent systems, dimensionality reduction method, Principal component analysis]
Graph-Based Iterative Hybrid Feature Selection
2008 Eighth IEEE International Conference on Data Mining
None
2008
When the number of labeled examples is limited, traditional supervised feature selection techniques often fail due to sample selection bias or unrepresentative sample problem. To solve this, semi-supervised feature selection techniques exploit the statistical information of both labeled and unlabeled examples in the same time. However, the results of semi-supervised feature selection can be at times unsatisfactory, and the culprit is on how to effectively use the unlabeled data. Quite different from both supervised and semi-supervised feature selection, we propose a ldquohybridrdquoframework based on graph models. We first apply supervised methods to select a small set of most critical features from the labeled data. Importantly, these initial features might otherwise be missed when selection is performed on the labeled and unlabeled examples simultaneously. Next,this initial feature set is expanded and corrected with the use of unlabeled data. We formally analyze why the expected performance of the hybrid framework is better than both supervised and semi-supervised feature selection. Experimental results demonstrate that the proposed method outperforms both traditional supervised and state-of-the-art semi-supervised feature selection algorithms by at least 10% inaccuracy on a number of text and biomedical problems with thousands of features to choose from. Software and dataset is available from the authors.
[iterative methods, Data analysis, semi-supervised, graph theory, high dimension, graph, supervised feature selection techniques, hybrid, semisupervised feature selection, Performance analysis, data handling, unlabeled data, graph-based iterative hybrid feature selection, feature selection]
Cleansing Noisy Data Streams
2008 Eighth IEEE International Conference on Data Mining
None
2008
In this paper, we identify a new research problem on cleansing noisy data streams which contain incorrectly labeled training examples. The objective is to accurately identify and remove mislabeled data, such that the prediction models built from the cleansed streams can be more accurate than the ones trained from the raw noisy streams. For this purpose, we first use bias-variance decomposition to derive a maximum variance margin (MVM) principle for stream data cleansing. Following this principle, we further propose a local and global filtering (LgF) framework to combine the strength of local noise filtering (within one single data chunk) and global noise filtering (across a number of adjacent data chunks) to identify erroneous data. Experimental results on six data streams (including two real-world data streams) demonstrate that LgF significantly outperforms simple methods in identifying noisy examples.
[Filtering, noisy data streams, global noise filtering, bias-variance decomposition, data mining, filtering theory, data streams, Predictive models, Data engineering, global filtering, Data mining, classification, Information technology, mislabeled data, local noise filtering, Computer science, incorrectly labeled training, data cleansing, Voting, USA Councils, Working environment noise, Supervised learning, maximum variance margin principle, noise]
Message from the General Co-Chairs
2009 Ninth IEEE International Conference on Data Mining
None
2009
Presents the welcome message from the conference proceedings.
[]
Message from the Program Committee Co-Chairs
2009 Ninth IEEE International Conference on Data Mining
None
2009
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
2009 Ninth IEEE International Conference on Data Mining
None
2009
Provides a listing of current committee members.
[]
Program Committee
2009 Ninth IEEE International Conference on Data Mining
None
2009
Provides a listing of current committee members.
[]
Steering Committee
2009 Ninth IEEE International Conference on Data Mining
None
2009
Provides a listing of current committee members.
[]
ICDM 2009 Program
2009 Ninth IEEE International Conference on Data Mining
None
2009
Provides a schedule of conference events and a listing of which papers were presented in each session.
[]
Explore/Exploit Schemes for Web Content Optimization
2009 Ninth IEEE International Conference on Data Mining
None
2009
We propose novel multi-armed bandit (explore/exploit) schemes to maximize total clicks on a content module published regularly on Yahoo! Intuitively, one can "explore'' each candidate item by displaying it to a small fraction of user visits to estimate the item's click-through rate (CTR), and then "exploit'' high CTR items in order to maximize clicks. While bandit methods that seek to find the optimal trade-off between explore and exploit have been studied for decades, existing solutions are not satisfactory for Web content publishing applications where dynamic set of items with short lifetimes, delayed feedback and non-stationary reward (CTR) distributions are typical. In this paper, we develop a Bayesian solution and extend several existing schemes to our setting. Through extensive evaluation with nine bandit schemes, we show that our Bayesian solution is uniformly better in several scenarios. We also study the empirical characteristics of our schemes and provide useful insights on the strengths and weaknesses of each. Finally, we validate our results with a "side-by-side'' comparison of schemes through live experiments conducted on a random sample of real user visits to Yahoo!
[Yahoo site, click-through rate, content scheduling, explore scheme, exploit scheme, web application, Meetings, Conference management, bandit schemes, Web content, multi-armed bandits, Distributed computing, Computer science, Publishing, Engineering management, Universal Serial Bus, Bayes optimal, Internet, Bayesian solution, Books, belief networks, Portals, Software engineering]
Connecting Sparsely Distributed Similar Bloggers
2009 Ninth IEEE International Conference on Data Mining
None
2009
The nature of the Blogosphere determines that the majority of bloggers are only connected with a small number of fellow bloggers, and similar bloggers can be largely disconnected from each other. Aggregating them allows for cost-effective personalized services, targeted marketing, and exploration of new business opportunities. As most bloggers have only a small number of adjacent bloggers, the problem of aggregating similar bloggers presents challenges that demand novel algorithms of connecting the non-adjacent due to the fragmented distributions of bloggers. In this work, we define the problem, delineate its challenges, and present an approach that uses innovative ways to employ contextual information and collective wisdom to aggregate similar bloggers. A real-world blog directory is used for experiments. We demonstrate the efficacy of our approach, report findings, and discuss related issues and future work.
[Meetings, Conference management, business opportunities exploration, latent semantic analysis, Distributed computing, sparse distribution, power law, Computer science, similar bloggers, Publishing, targeted marketing, Engineering management, cost-effective personalized services, Blogosphere, Long Tail, clustering, Books, Web sites, Joining processes, collective wisdom, mean average precision (MAP), Portals, Software engineering]
A Local Scalable Distributed Expectation Maximization Algorithm for Large Peer-to-Peer Networks
2009 Ninth IEEE International Conference on Data Mining
None
2009
This paper describes a local and distributed expectation maximization algorithm for learning parameters of Gaussian mixture models (GMM) in large peer-to-peer (P2P) environments. The algorithm can be used for a variety of well-known data mining tasks in distributed environments such as clustering, anomaly detection, target tracking, and density estimation to name a few, necessary for many emerging P2P applications in bioinformatics, webmining and sensor networks. Centralizing all or some of the data to build global models is impractical in such P2P environments because of the large number of data sources, the asynchronous nature of the P2P networks, and dynamic nature of the data/network. The proposed algorithm takes a two-step approach. In the monitoring phase, the algorithm checks if the model `quality' is acceptable by using an efficient local algorithm. This is then used as a feedback loop to sample data from the network and rebuild the GMM when it is outdated. We present thorough experimental results to verify our theoretical claims.
[peer-to-peer networks, Meetings, data mining, Conference management, anomaly detection, sensor networks, Distributed computing, Publishing, Engineering management, Books, local scalable distributed expectation maximization algorithm, density estimation, webmining, data mining tasks, data sources, peer-to-peer computing, Peer to peer computing, P2P applications, peer-to-peer, Computer science, Gaussian processes, target tracking, bioinformatics, expectation-maximisation algorithm, Gaussian mixture models, clustering, expectation maximization, local algorithms, Portals, Software engineering]
Cross-Guided Clustering: Transfer of Relevant Supervision across Domains for Improved Clustering
2009 Ninth IEEE International Conference on Data Mining
None
2009
Lack of supervision in clustering algorithms often leads to clusters that are not useful or interesting to human reviewers. We investigate if supervision can be automatically transferred to a clustering task in a target domain, by providing a relevant supervised partitioning of a dataset from a different source domain. The target clustering is made more meaningful for the human user by trading off intrinsic clustering goodness on the target dataset for alignment with relevant supervised partitions in the source dataset, wherever possible. We propose a cross-guided clustering algorithm that builds on traditional k-means by aligning the target clusters with source partitions. The alignment process makes use of a cross-domain similarity measure that discovers hidden relationships across domains with potentially different vocabularies. Using multiple real-world datasets, we show that our approach improves clustering accuracy significantly over traditional k-means.
[Vocabulary, cross-guided clustering, Costs, Transfer Learning, Clustering methods, supervised partitioning, Humans, Partitioning algorithms, cross-domain similarity measure, Automobiles, Personnel, Data mining, Unsupervised learning, intrinsic clustering, pattern clustering, Clustering algorithms, Training data, Relationship Discovery, improved clustering]
Audio Classification of Bird Species: A Statistical Manifold Approach
2009 Ninth IEEE International Conference on Data Mining
None
2009
Our goal is to automatically identify which species of bird is present in an audio recording using supervised learning. Devising effective algorithms for bird species classification is a preliminary step toward extracting useful ecological data from recordings collected in the field. We propose a probabilistic model for audio features within a short interval of time, then derive its Bayes risk-minimizing classifier, and show that it is closely approximated by a nearest-neighbor classifier using Kullback-Leibler divergence to compare histograms of features. We note that feature histograms can be viewed as points on a statistical manifold, and KL divergence approximates geodesic distances defined by the Fisher information metric on such manifolds. Motivated by this fact, we propose the use of another approximation to the Fisher information metric, namely the Hellinger metric. The proposed classifiers achieve over 90% accuracy on a data set containing six species of bird, and outperform support vector machines.
[Vocabulary, Costs, maximum a-posteriori, Humans, supervised learning, Kullback-Leibler divergence, Personnel, Data mining, telecommunication computing, probabilistic model, Hellinger metric, nearest neighbor, Clustering algorithms, Training data, manifold, bayes, audio, learning (artificial intelligence), codebook, map, support vector machines, Bayes risk-minimizing classifier, Fisher information metric, Birds, Partitioning algorithms, Automobiles, classification, signal classification, geodesic, audio signal processing, mfccs, bird species, statistical manifold approach, clustering, audio recording, statistical analysis, nearest-neighbor classifier, audio classification]
Finding Associations and Computing Similarity via Biased Pair Sampling
2009 Ninth IEEE International Conference on Data Mining
None
2009
Sampling-based methods have previously been proposed for the problem of finding interesting associations in data, even for low-support items. While these methods do not guarantee precise results, they can be vastly more efficient than approaches that rely on exact counting. However, for many similarity measures no such methods have been known. In this paper we show how a wide variety of measures can be supported by a simple biased sampling method. The method also extends to find high-confidence association rules. We demonstrate theoretically that our method is superior to exact methods when the threshold for "interesting similarity/confidence" is above the average pairwise similarity/confidence, and the average support is not too low. Our method is particularly good when transactions contain many items. We confirm in experiments on standard association mining benchmarks that this gives a significant speedup on real data sets (sometimes much larger than the theoretical guarantees). Reductions in computation time of over an order of magnitude, and significant savings in space, are observed.
[similarity measure, Vocabulary, association rule, algorithms, Costs, sampling methods, pairwise similarity, Humans, data mining, sampling, association rules, Partitioning algorithms, data association, Automobiles, Personnel, Data mining, association mining, pairwise confidence, Clustering algorithms, Training data, Sampling methods, biased pair sampling]
Beyond Banditron: A Conservative and Efficient Reduction for Online Multiclass Prediction with Bandit Setting Model
2009 Ninth IEEE International Conference on Data Mining
None
2009
In this paper, we consider a recently proposed supervised learning problem, called online multiclass prediction with bandit setting model. Aiming at learning from partial feedback of online classification results, i.e. ¿true¿ when the predicting label is right or ¿false¿ when the predicting label is wrong, this new kind of problems arouses much of researchers' interest due to its close relations to real world internet applications and human cognitive procedure. While some algorithms have been brought forward, we propose a novel algorithm to deal with such problems. First, we reduce the multiclass prediction problem to binary based on Conservative one-versus-all others Reduction scheme; Then Online Passive-Aggressive Algorithm is embedded as binary learning algorithm to solve the reduced problem. Also we derive a pleasing cumulative mistake bound for our algorithm and a time complexity bound linear to the sample size. Further experimental evaluation on several real world multiclass datasets including RCV1, MNIST, 20 Newsgroups and USPS shows that our method outperforms the existing algorithms with a great improvement.
[online multiclass prediction, Vocabulary, Costs, one versus all reduction, Humans, Predictive models, Partitioning algorithms, passive-aggressive algorithm, Automobiles, Personnel, Data mining, conservative one-versus-all others reduction scheme, Clustering algorithms, Training data, supervised learning problem, learning (artificial intelligence), bandit setting model, binary learning algorithm]
Probabilistic Similarity Query on Dimension Incomplete Data
2009 Ninth IEEE International Conference on Data Mining
None
2009
Retrieving similar data has drawn many research efforts in the literature due to its importance in data mining, database and information retrieval. This problem is challenging when the data is incomplete. In previous research, data incompleteness refers to the fact that data values for some dimensions are unknown. However, in many practical applications (e.g., data collection by sensor network under bad environment), not only data values but even data dimension information may also be missing, which will make most similarity query algorithms infeasible. In this work, we propose the novel similarity query problem on dimension incomplete data and adopt a probabilistic framework to model this problem. For this problem, users can give a distance threshold and a probability threshold to specify their retrieval requirements. The distance threshold is used to specify the allowed distance between query and data objects and the probability threshold is used to require that the retrieval results satisfy the distance condition at least with the given probability. Instead of enumerating all possible cases to recover the missed dimensions, we propose an efficient approach to speed up the retrieval process by leveraging the inherent relations between query and dimension incomplete data objects. During the query process, we estimate the lower/upper bounds of the probability that the query is satisfied by a given data object, and utilize these bounds to filter irrelevant data objects efficiently. Furthermore, a probability triangle inequality is proposed to further speed up query processing. According to our experiments on real data sets, the proposed similarity query method is verified to be effective and efficient on dimension incomplete data.
[data retrieval, Multidimensional systems, Costs, data mining, information retrieval, Information retrieval, Data mining, dimension incomplete data, Sun, data collection, query processing, Upper bound, Filters, probabilistic similarity query, Databases, Query processing, Asia]
flowNet: Flow-Based Approach for Efficient Analysis of Complex Biological Networks
2009 Ninth IEEE International Conference on Data Mining
None
2009
Biological networks having complex connectivity have been widely studied recently. By characterizing their inherent and structural behaviors in a topological perspective, these studies have attempted to discover hidden knowledge in the systems. However, even though various algorithms with graph-theoretical modeling have provided fundamentals in the network analysis, the availability of practical approaches to efficiently handle the complexity has been limited. In this paper, we present a novel flow-based approach, called flowNet, to efficiently analyze large-sized, complex networks. Our approach is based on the functional influence model that quantifies the influence of a biological component on another. We introduce a dynamic flow simulation algorithm to generate a flow pattern which is a unique characteristic for each component. The set of patterns can be used in identifying functional modules (i.e., clustering). The proposed flow simulation algorithm runs very efficiently in sparse networks. Since our approach uses a weighted network as an input, we also discuss supervised and unsupervised weighting schemes for unweighted biological networks. As experimental results in real applications to the yeast protein interaction network, we demonstrate that our approach outperforms previous graph clustering methods with respect to accuracy.
[Algorithm design and analysis, Availability, dynamic flow simulation algorithm, Biological system modeling, Heuristic algorithms, Clustering methods, graph theory, graph-theoretical modeling, yeast protein interaction network, unsupervised learning, biological networks, graph clustering, Fungi, Proteins, unsupervised weighting scheme, biology computing, flowNet, Clustering algorithms, Character generation, Complex networks, flow-based approach, functional influence model, complex biological network]
&#x003BD;-Anomica: A Fast Support Vector Based Novelty Detection Technique
2009 Ninth IEEE International Conference on Data Mining
None
2009
In this paper we propose &#x003BD;-Anomica, a novel anomaly detection technique that can be trained on huge data sets with much reduced running time compared to the benchmark one-class support vector machines algorithm. In &#x003BD;-Anomica, the idea is to train the machine such that it can provide a close approximation to the exact decision plane using fewer training points and without losing much of the generalization performance of the classical approach. We have tested the proposed algorithm on a variety of continuous data sets under different conditions. We show that under all test conditions the developed procedure closely preserves the accuracy of standard one-class support vector machines while reducing both the training time and the test time by 5 - 20 times.
[Support vector machines, &#x003BD;-anomica, Anomaly Detection, security of data, support vector machines, anomaly detection technique, Benchmark testing, one-class support vector machines, Standards development, Support Vector Machines, Kernel, Optimization]
Temporal Neighborhood Discovery Using Markov Models
2009 Ninth IEEE International Conference on Data Mining
None
2009
Temporal data, which is a sequence of data tuples measured at successive time instances, is typically very large. Hence instead of mining the entire data, we are interested in dividing the huge data into several smaller intervals of interest which we call temporal neighborhoods. In this paper we propose an approach to generate temporal neighborhoods through unequal depth discretization. We describe two novel algorithms (a) similarity based merging (SMerg) and, (b) stationary distribution based merging (StMerg). These algorithms are based on the robust framework of Markov models and the Markov stationary distribution respectively. We identify temporal neighborhoods with distinct demarcations based on unequal depth discretization of the data. We discuss detailed experimental results in both synthetic and real world data. Specifically we show (i) the efficacy of our approach through precision and recall of labeled bins, (ii) the ground truth validation in real world datasets and, (iii) knowledge discovery in the temporal neighborhoods such as global anomalies. Our results indicate that we are able to identify valuable knowledge based on our ground truth validation from real world traffic data.
[merging, temporal neighborhood discovery, Merging, data mining, temporal neighborhoods, stationary distribution based merging algorithm, Time measurement, temporal data, knowledge discovery, ground truth validation, Stationary Distribution, Data mining, statistical distributions, Temporal neighborhoods, Markov Model, unequal depth discretization, Discretization, Markov processes, similarity based merging algorithm, Robustness, Markov models, Markov stationary distribution, data tuples, global anomalies]
Active Learning with Generalized Queries
2009 Ninth IEEE International Conference on Data Mining
None
2009
Active learning can actively select or construct examples to label to reduce the number of labeled examples needed for building accurate classifiers. However, previous works of active learning can only ask specific queries. For example, to predict osteoarthritis from a patient dataset with 30 attributes, specific queries always contain values of all these 30 attributes, many of which may be irrelevant. A more natural way is to ask "generalized queries" with don't-care attributes, such as "are people over 50 with knee pain likely to have osteoarthritis?" (with only two attributes: age and type of pain). We assume that the oracle (and human experts) can readily answer those generalized queries by returning probabilistic labels. The power of such generalized queries is that one generalized query may be equivalent to many specific ones. However, overly general queries may receive highly uncertain labels from the oracle, and this makes learning difficult. In this paper, we propose a novel active learning algorithm that asks generalized queries. We demonstrate experimentally that our new method asks significantly fewer queries compared with the previous works of active learning. Our method can be readily deployed in real-world tasks where obtaining labeled examples is costly.
[Knee, probabilistic labels, learning algorithm, Buildings, Humans, supervised learning, don't-care attributes, active learning, Pain, generalized queries, labeled examples, learning (artificial intelligence), Osteoarthritis, query formulation]
Conditional Models for Non-smooth Ranking Loss Functions
2009 Ninth IEEE International Conference on Data Mining
None
2009
Learning to rank is an important area at the interface of machine learning, information retrieval and Web search. The central challenge in optimizing various measures of ranking loss is that the objectives tend to be non-convex and discontinuous. To make such functions amenable to gradient based optimization procedures one needs to design clever bounds. In recent years, boosting, neural networks, support vector machines, and many other techniques have been applied. However, there is little work on directly modeling a conditional probability Pr(y|x<sub>q</sub>) where y is a permutation of the documents to be ranked and x<sub>q</sub> represents their feature vectors with respect to a query q. A major reason is that the space of y is huge: n! if n documents must be ranked. We first propose an intuitive and appealing expected loss minimization objective, and give an efficient shortcut to evaluate it despite the huge space of ys. Unfortunately, the optimization is non-convex, so we propose a convex approximation. We give a new, efficient Monte Carlo sampling method to compute the objective and gradient of this approximation, which can then be used in a quasi-Newton optimizer like LBFGS. Extensive experiments with the widely-used LETOR dataset show large ranking accuracy improvements beyond recent and competitive algorithms.
[Optimization methods, neural networks, Monte Carlo sampling method, nonsmooth ranking loss functions, Loss measurement, Monte Carlo Sampling, Conditional Models, Design optimization, Monte Carlo methods, convex approximation, learning (artificial intelligence), support vector machines, boosting, LETOR dataset, information retrieval, Information retrieval, Boosting, convex programming, Learning to Rank, machine learning, Support vector machines, quasi-Newton optimizer, non-convex optimization, Neural networks, Machine learning, conditional probability, Web search, neural nets]
Unsupervised Class Separation of Multivariate Data through Cumulative Variance-Based Ranking
2009 Ninth IEEE International Conference on Data Mining
None
2009
This paper introduces a new extension of outlier detection approaches and a new concept, class separation through variance. We show that accumulating information about the outlierness of points in multiple subspaces leads to a ranking in which classes with differing variance naturally tend to separate. Exploiting this leads to a highly effective and efficient unsupervised class separation approach, especially useful in the difficult case of heavily overlapping distributions. Unlike typical outlier detection algorithms, this method can be applied beyond the `rare classes' case with great success. Two novel algorithms that implement this approach are provided. Additionally, experiments show that the novel methods typically outperform other state-of-the-art outlier detection methods on high dimensional data such as Feature Bagging, SOE1, LOF, ORCA and Robust Mahalanobis Distance and competes even with the leading supervised classification methods.
[robust Mahalanobis distance, LOF, ORCA, outlier detection approaches, feature bagging, SOE1, unsupervised learning, security of data, unsupervised class separation, Classification, multivariate data, Robustness, cumulative variance-based ranking, Outlier Detection, Subspaces, Detection algorithms, Bagging]
Execution Anomaly Detection in Distributed Systems through Unstructured Log Analysis
2009 Ninth IEEE International Conference on Data Mining
None
2009
Detection of execution anomalies is very important for the maintenance, development, and performance refinement of large scale distributed systems. Execution anomalies include both work flow errors and low performance problems. People often use system logs produced by distributed systems for troubleshooting and problem diagnosis. However, manually inspecting system logs to detect anomalies is unfeasible due to the increasing scale and complexity of distributed systems. Therefore, there is a great demand for automatic anomalies detection techniques based on log analysis. In this paper, we propose an unstructured log analysis technique for anomalies detection. In the technique, we propose a novel algorithm to convert free form text messages in log files to log keys without heavily relying on application specific knowledge. The log keys correspond to the log-print statements in the source code which can provide cues of system execution behavior. After converting log messages to log keys, we learn a Finite State Automaton (FSA) from training log sequences to present the normal work flow for each system component. At the same time, a performance measurement model is learned to characterize the normal execution performance based on the log messages' timing information. With these learned models, we can automatically detect anomalies in newly input log files. Experiments on Hadoop and SILK show that the technique can effectively detect running anomalies.
[Measurement, execution anomaly detection, Learning automata, problem diagnosis, large scale distributed systems, distributed system, unstructured log analysis technique, security of data, Large-scale systems, Timing, finite state automaton, log analysis, distributed programming]
Learning the Shared Subspace for Multi-task Clustering and Transductive Transfer Classification
2009 Ninth IEEE International Conference on Data Mining
None
2009
There are many clustering tasks which are closely related in the real world, e.g. clustering the Web pages of different universities. However, existing clustering approaches neglect the underlying relation and treat these clustering tasks either individually or simply together. In this paper, we will study a novel clustering paradigm, namely multi-task clustering, which performs multiple related clustering tasks together and utilizes the relation of these tasks to enhance the clustering performance. We aim to learn a subspace shared by all the tasks, through which the knowledge of the tasks can be transferred to each other. The objective of our approach consists of two parts: (1) Within-task clustering: clustering the data of each task in its input space individually; and (2) Cross-task clustering: simultaneous learning the shared subspace and clustering the data of all the tasks together. We will show that it can be solved by alternating minimization, and its convergence is theoretically guaranteed. Furthermore, we will show that given the labels of one task, our multi-task clustering method can be extended to transductive transfer classification (a.k.a. cross-domain classification, domain adaption). Experiments on several cross-domain text data sets demonstrate that the proposed multi-task clustering outperforms traditional single-task clustering methods greatly. And the transductive transfer classification method is comparable to or even better than several existing transductive transfer classification approaches.
[Clustering methods, Laboratories, data clustering, domain adaption, learning, Data mining, History, Convergence, Information science, multi-task clustering, multitask clustering, cross-task clustering, learning (artificial intelligence), Intelligent systems, transductive transfer classification, transfer learning, Automation, within-task clustering, cross-domain classification, multi-task learning, cross domain classification, pattern clustering, minimization, Web pages, clustering performance, Machine learning, minimisation]
Accurate Estimation of the Degree Distribution of Private Networks
2009 Ninth IEEE International Conference on Data Mining
None
2009
We describe an efficient algorithm for releasing a provably private estimate of the degree distribution of a network. The algorithm satisfies a rigorous property of differential privacy, and is also extremely efficient, running on networks of 100 million nodes in a few seconds. Theoretical analysis shows that the error scales linearly with the number of unique degrees, whereas the error of conventional techniques scales linearly with the number of nodes. We complement the theoretical analysis with a thorough empirical analysis on real and synthetic graphs, showing that the algorithm's variance and bias is low, that the error diminishes as the size of the input graph increases, and that common analyses like fitting a power-law can be carried out very accurately.
[Algorithm design and analysis, Data privacy, empirical analysis, estimation theory, conventional techniques, graph theory, privacy, privacy-preserving data mining, Data mining, synthetic graphs, Distortion measurement, Communication networks, Social network services, social networks, private networks, accurate estimation, theoretical analysis, private estimate, Diseases, real graph, Computer science, Chaotic communication, social networking (online), data privacy, degree distribution, Analysis of variance, differential privacy]
A Linear-Time Graph Kernel
2009 Ninth IEEE International Conference on Data Mining
None
2009
The design of a good kernel is fundamental for knowledge discovery from graph-structured data. Existing graph kernels exploit only limited information about the graph structures but are still computationally expensive. We propose a novel graph kernel based on the structural characteristics of graphs. The key is to represent node labels as binary arrays and characterize each node using logical operations on the label set of the connected nodes. Our kernel has a linear time complexity with respect to the number of nodes times the average number of neighboring nodes in the given graphs. The experimental result shows that the proposed kernel performs comparable and much faster than a state-of-the-art graph kernel for benchmark data sets and shows high scalability for new applications with large graphs.
[benchmark data sets, Scalability, graph theory, data mining, linear-time graph kernel, Time measurement, knowledge discovery, Data mining, Chemical compounds, logical operations, graph-structured data, Machine learning, binary arrays, Polynomials, Computational efficiency, Kernel, Informatics, linear time complexity, Logic arrays, computational complexity]
GSML: A Unified Framework for Sparse Metric Learning
2009 Ninth IEEE International Conference on Data Mining
None
2009
There has been significant recent interest in sparse metric learning (SML) in which we simultaneously learn both a good distance metric and a low-dimensional representation. Unfortunately, the performance of existing sparse metric learning approaches is usually limited because the authors assumed certain problem relaxations or they target the SML objective indirectly. In this paper, we propose a generalized sparse metric learning method (GSML). This novel framework offers a unified view for understanding many of the popular sparse metric learning algorithms including the sparse metric learning framework proposed, the large margin nearest neighbor (LMNN), and the D-ranking vector machine (D-ranking VM). Moreover, GSML also establishes a close relationship with the pairwise support vector machine. Furthermore, the proposed framework is capable of extending many current non-sparse metric learning models such as relevant vector machine (RCA) and a state-of-the-art method proposed into their sparse versions. We present the detailed framework, provide theoretical justifications, build various connections with other models, and propose a practical iterative optimization method, making the framework both theoretically important and practically scalable for medium or large datasets. A series of experiments show that the proposed approach can outperform previous methods in terms of both test accuracy and dimension reduction, on six real-world benchmark datasets.
[iterative methods, Automation, support vector machines, D-ranking vector machine, Metric Learning, Laboratories, pairwise support vector machine, Data engineering, GSML, large margin nearest neighbor, Pattern recognition, Sparse matrices, Data mining, Nearest neighbor searches, Support vector machines, optimisation, iterative optimization, Machine learning, sparse metric learning, relevant vector machine, Unified Framework, Sparse, Virtual manufacturing, learning (artificial intelligence)]
GRAPE: A Graph-Based Framework for Disambiguating People Appearances in Web Search
2009 Ninth IEEE International Conference on Data Mining
None
2009
Finding information about people using search engines is one of the most common activities on the Web. However, search engines usually return a long list of Web pages, which may be relevant to many namesakes, especially given the explosive growth of Web data. To address the challenge caused by name ambiguity in Web people search, this paper proposes a novel graph-based framework, GRAPE (abbr. a graph-based framework for disambiguating people appearances in Web search). In GRAPE, people tag information (e.g., people name, organization, and email address) surrounding the queried people name is extracted from the search results, a graph-based unsupervised algorithm is then developed to cluster the extracted tags, where a new method, cohesion, is introduced to measure the importance of a tag for clustering, and each final cluster of tags represents a unique people entity. Experimental results show that our proposed framework outperforms the state-of-the-art Web people name disambiguation approaches.
[search engines, graph-based unsupervised algorithm, Pipelines, graph theory, people tag information extraction, Named Entity, cohesion method, Data mining, tags clustering, Clustering, Open source software, Computer science, Information science, People Name Disambiguation, pattern clustering, graph-based framework for disambiguating people appearances, Web people name disambiguation approaches, Tag Extraction, Web pages, Clustering algorithms, Search engines, Computer science education, Internet, Web search]
A Tree-Based Framework for Difference Summarization
2009 Ninth IEEE International Conference on Data Mining
None
2009
Understanding the differences between two datasets is a fundamental data mining question and is also ubiquitously important across many real world scientific applications. In this paper, we propose a tree-based framework to provide a parsimonious explanation of the difference between two distributions based on rigorous two-sample statistical test. We develop two efficient approaches. The first one is a dynamic programming approach that finds a minimal number of data subsets that describe the difference between two data sets. The second one is a greedy approach that approximates the dynamic programming approach. We employ the well-known Friedman's MST (minimal spanning tree) statistics for two-sample statistical tests in our summarization tree construction, and develop novel techniques to speedup its computational procedure. We performed a detailed experimental evaluation on both real and synthetic datasets and demonstrated the effectiveness of our tree-summarization approach.
[Drugs, Kolmogorov-Smirnov test, data mining, datasets, Data mining, data subsets, USA Councils, tree-based framework, difference summarization, Marketing and sales, Dynamic programming, Testing, Multidimensional systems, Statistical analysis, Friedman minimal spanning tree statistics, two-sample statistical test, trees (mathematics), dynamic programming approach, minimal spanning tree, dynamic programming, Application software, Computer science, Friedman-Rafsky test, Chi-square test, statistical analysis, two-sample test]
TrBagg: A Simple Transfer Learning Method and its Application to Personalization in Collaborative Tagging
2009 Ninth IEEE International Conference on Data Mining
None
2009
The aim of transfer learning is to improve prediction accuracy on a target task by exploiting the training examples for tasks that are related to the target one. Transfer learning has received more attention in recent years, because this technique is considered to be helpful in reducing the cost of labeling. In this paper, we propose a very simple approach to transfer learning: TrBagg, which is the extension of bagging. TrBagg is composed of two stages: Many weak classifiers are first generated as in standard bagging, and these classifiers are then filtered based on their usefulness for the target task. This simplicity makes it easy to work reasonably well without severe tuning of learning parameters. Further, our algorithm equips an algorithmic scheme to avoid negative transfer. We applied TrBagg to personalized tag prediction tasks for social bookmarks. Our approach has several convenient characteristics for this task such as adaptation to multiple tasks with low computational cost.
[TrBagg, transfer learning, Costs, Machine learning algorithms, Filtering, bagging, information filtering, collaborative tagging, simple transfer learning method, Learning systems, Industrial training, ensemble learning, personalization, recommender system, multilabel classification problems, Collaboration, personalized tag prediction tasks, Tagging, social bookmarks, Computational efficiency, Labeling, learning (artificial intelligence), Bagging]
PEGASUS: A Peta-Scale Graph Mining System Implementation and Observations
2009 Ninth IEEE International Conference on Data Mining
None
2009
In this paper, we describe PEGASUS, an open source peta graph mining library which performs typical graph mining tasks such as computing the diameter of the graph, computing the radius of each node and finding the connected components. as the size of graphs reaches several giga-, tera- or peta-bytes, the necessity for such a library grows too. To the best of our knowledge, PEGASUS is the first such library, implemented on the top of the HADOOP platform, the open source version of MAPREDUCE. Many graph mining operations (PageRank, spectral clustering, diameter estimation, connected components etc.) are essentially a repeated matrix-vector multiplication. In this paper we describe a very important primitive for PEGASUS, called GIM-V (generalized iterated matrix-vector multiplication). GIM-V is highly optimized, achieving (a) good scale-up on the number of available machines (b) linear running time on the number of edges, and (c) more than 5 times faster performance over the non-optimized version of GIM-V. Our experiments ran on M45, one of the top 50 supercomputers in the world. We report our findings on several real graphs, including one of the largest publicly available Web graphs, thanks to Yahoo!, with ¿ 6,7 billion edges.
[peta graph mining library, Social network services, public domain software, Optimization methods, data mining, Supercomputers, peta-scale graph mining system, Data mining, PEGASUS library, software libraries, Radio access networks, generalized iterated matrix-vector multiplication, graph mining, matrix multiplication, Parallel programming, open source system, Web graph, Libraries, PEGASUS, Performance analysis, Large-scale systems, Timing, hadoop]
Efficient Discovery of Frequent Correlated Subgraph Pairs
2009 Ninth IEEE International Conference on Data Mining
None
2009
The recent proliferation of graph data in a wide spectrum of applications has led to an increasing demand for advanced data analysis techniques. In view of this, many graph mining techniques, such as frequent subgraph mining and correlated subgraph mining, have been proposed. In many applications, both frequency and correlation play an important role. Thus, this paper studies a new problem of mining the set of frequent correlated subgraph pairs. A simple algorithm that combines existing algorithms for mining frequent subgraphs and correlated subgraphs results in a multiplication of the mining operations, the majority of which are redundant. We discover that most of the graphs correlated to a common graph are also highly correlated. We establish theoretical foundations for this finding and derive a tight lower bound on the correlation of any two graphs that are correlated to a common graph. This theoretical result leads to the design of a very effective skipping mechanism, by which we skip the processing of a majority of graphs in the mining process. Our algorithm, FCP-Miner, is a fast approximate algorithm, but we show that the missing pairs are only a small set of marginally correlated pairs. Extensive experiments verify both the efficiency and effectiveness of FCP-Miner.
[Drugs, Data analysis, Biochemical analysis, data analysis, Social network services, graph theory, correlated subgraph mining, data mining, graph data, skipping mechanism, frequent subgraph mining, advanced data analysis techniques, Pattern recognition, Data mining, graph mining, Chemistry, Databases, FCP-Miner, frequent correlated subgraph pairs, Frequency, frequent correlated subgraph pair discovery, Bioinformatics, Pearson's correlation coefficient]
Self-Adaptive Anytime Stream Clustering
2009 Ninth IEEE International Conference on Data Mining
None
2009
Clustering streaming data requires algorithms which are capable of updating clustering results for the incoming data. As data is constantly arriving, time for processing is limited. Clustering has to be performed in a single pass over the incoming data and within the possibly varying inter-arrival times of the stream. Likewise, memory is limited, making it impossible to store all data. For clustering, we are faced with the challenge of maintaining a current result that can be presented to the user at any given time. In this work, we propose a parameter free algorithm that automatically adapts to the speed of the data stream. It makes best use of the time available under the current constraints to provide a clustering of the objects seen up to that point. Our approach incorporates the age of the objects to reflect the greater importance of more recent data. Moreover, we are capable of detecting concept drift, novelty and outliers in the stream. For efficient and effective handling, we introduce the ClusTree, a compact and self-adaptive index structure for maintaining stream summaries. Our experiments show that our approach is capable of handling a multitude of different stream characteristics for accurate and scalable anytime stream clustering.
[Algorithm design and analysis, self-adaptive anytime stream clustering, Data analysis, Adaptive algorithm, self-adaptive algorithms, stream clustering, Sensor phenomena and characterization, self-adaptive index structure, Partitioning algorithms, Data mining, Consumer behavior, pattern clustering, Memory management, Clustering algorithms, parameter free algorithm, ClusTree, tree data structures, Time factors, anytime algorithms]
Improving SVM Classification on Imbalanced Data Sets in Distance Spaces
2009 Ninth IEEE International Conference on Data Mining
None
2009
Imbalanced data sets present a particular challenge to the data mining community. Often, it is the rare event that is of interest and the cost of misclassifying the rare event is higher than misclassifying the usual event. When the data is highly skewed toward the usual, it can be very difficult for a learning system to accurately detect the rare event. There have been many approaches in recent years for handling imbalanced data sets, from under-sampling the majority class to adding synthetic points to the minority class in feature space. Distances between time series are known to be non-Euclidean and nonmetric, since comparing time series requires warping in time. This fact makes it impossible to apply standard methods like SMOTE to insert synthetic data points in feature spaces. We present an innovative approach that augments the minority class by adding synthetic points in distance spaces. We then use Support Vector Machines for classification. Our experimental results on standard time series show that our synthetic points significantly improve the classification rate of the rare events, and in many cases also improves the overall accuracy of SVM.
[pattern classification, Costs, Event detection, support vector machines, data mining, SVM classification, time series, learning system, Data mining, Petroleum, learning systems, Support vector machines, Learning systems, Computer science, distance spaces, imbalanced data sets, SMOTE, USA Councils, Support vector machine classification, data mining community, synthetic data points, Sampling methods]
CoCoST: A Computational Cost Efficient Classifier
2009 Ninth IEEE International Conference on Data Mining
None
2009
Computational cost of classification is as important as accuracy in on-line classification systems. The computational cost is usually dominated by the cost of computing implicit features of the raw input data. Very few efforts have been made to design classifiers which perform effectively with limited computational power; instead, feature selection is usually employed as a pre-processing step to reduce the cost of running traditional classifiers. We present CoCoST, a novel and effective approach for building classifiers which achieve state-of-the-art classification accuracy, while keeping the expected computational cost of classification low, even without feature selection. CoCost employs a wide range of novel cost-aware decision trees, each of which is tuned to specialize in classifying instances from a subset of the input space, and judiciously consults them depending on the input instance in accordance with a cost-aware meta-classifier. Experimental results on a network flow detection application show that, our approach can achieve better accuracy than classifiers such as SVM and random forests, while achieving 75%-90% reduction in the computational costs.
[online classification systems, Costs, support vector machines, cost-aware decision trees, Meta-Classifier, Suppressed Cost, computational cost efficient classifier, Inverse-Boosting, CoCoST, Data engineering, SVM, Cost Efficient Decision Tree, Data mining, Computer science, network flow detection, Machine learning, decision trees, Feature extraction, Computational efficiency, Decision trees, learning (artificial intelligence), feature selection, Testing, Classification tree analysis]
Semi-naive Exploitation of One-Dependence Estimators
2009 Ninth IEEE International Conference on Data Mining
None
2009
It is well known that the key of Bayesian classifier learning is to balance the two important issues, that is, the exploration of attribute dependencies in high orders for ensuring a sufficient flexibility in approximating the ground-truth dependencies, and the exploration of low orders for ensuring a stable probability estimate from limited training samples. By allowing one-order attribute dependencies, one-dependence estimators (ODEs) have been shown to be able to approximate the ground-truth attribute dependencies whilst keeping the effectiveness of probability estimation, and therefore leading to excellent performance. In previous studies, however, ODEs were exploited in simple ways, such as by averaging, for classification. In this paper, we propose a semi-naive exploitation of ODEs that fits a function of ODEs to pursue higher-order attribute dependencies. Extensive experiments show that the proposed SNODE approach can achieve better performance than many state-of-the-art Bayesian classifiers.
[Bayesian classifier learning, probability estimation, Maximum likelihood estimation, pattern classification, estimation theory, Laboratories, Performance gain, Data mining, Proposals, Bayesian classifier, Constraint optimization, Bayesian methods, one-dependence estimator, semi-naive exploitation, Bayes methods, belief networks, semi-naive Bayes, one-dependence estimators]
A Framework for Computing the Privacy Scores of Users in Online Social Networks
2009 Ninth IEEE International Conference on Data Mining
None
2009
A large body of work has been devoted to address corporate-scale privacy concerns related to social networks. The main focus was on how to share social networks owned by organizations without revealing the identities or sensitive relationships of the users involved. Not much attention has been given to the privacy risk of users posed by their information sharing activities. In this paper, we approach the privacy concerns arising in online social networks from the individual users' viewpoint: we propose a framework to compute a privacy score of a user, which indicates the potential privacy risk caused by his participation in the network. Our definition of privacy score satisfies the following intuitive properties: the more sensitive the information revealed by a user, the higher his privacy risk. Also, the more visible the disclosed information becomes in the network, the higher the privacy risk. We develop mathematical models to estimate both sensitivity and visibility of the information. We apply our methods to synthetic and real-world data and demonstrate their efficacy and practical utility.
[Social network services, Estimation, online social networks, Data mining, information sharing activities, social network, Privacy, Sensitivity, corporate-scale privacy concerns, privacy score, potential privacy risk, Probability density function, social networking (online), data privacy, item response theory, Mathematical model]
Least Square Incremental Linear Discriminant Analysis
2009 Ninth IEEE International Conference on Data Mining
None
2009
Linear discriminant analysis (LDA) is a well-known dimension reduction approach, which projects high-dimensional data into a low-dimensional space with the best separation of different classes. In many tasks, the data accumulates over time, and thus incremental LDA is more desirable than batch LDA. Several incremental LDA algorithms have been developed and achieved success; however, the eigen-problem involved requires a large computation cost, which hampers the efficiency of these algorithms. In this paper, we propose a new incremental LDA algorithm, LS-ILDA, based on the least square solution of LDA. When new samples are received, LS-ILDA incrementally updates the least square solution of LDA. Our analysis discloses that this algorithm produces the exact least square solution of batch LDA, while its computational cost is O(min(n, d) × d) for one update on dataset containing n instances in d-dimensional space. Experimental results show that comparing with state-of-the-art incremental LDA algorithms, our proposed LS-ILDA achieves high accuracy with low time cost.
[Algorithm design and analysis, least square incremental linear discriminant analysis, Costs, least square solution, Laboratories, high-dimensional data, Data mining, least square, eigenvalues and eigenfunctions, data reduction, incremental LDA algorithms, Space technology, incremental learning, Linear discriminant analysis, Computational efficiency, low-dimensional space, least squares approximations, computational cost, Scattering, eigen-problem, Matrix decomposition, Least squares methods, Dimension reduction, dimension reduction approach, linear discriminant analysis (LDA), LS-ILDA]
Unified Solution to Nonnegative Data Factorization Problems
2009 Ninth IEEE International Conference on Data Mining
None
2009
In this paper, we restudy the non-convex data factorization problems (regularized or not, unsupervised or supervised), where the optimization is confined in the nonnegative orthant, and provide a unified convergency provable solution based on multiplicative nonnegative update rules. This solution is general for optimization problems with block-wisely quadratic objective functions, and thus direct update rules can be derived by skipping over the tedious specific procedure deduction process and algorithmic convergence proof. By taking this unified solution as a general template, we i) re-explain several existing nonnegative data factorization algorithms, ii) develop a variant of nonnegative matrix factorization formulation for handling out-of-sample data, and Hi) propose a new nonnegative data factorization algorithm, called correlated co-decomposition (CCD), to simultaneously factorize two feature spaces by exploring the inter-correlated information. Experiments on both face recognition and multi-label image annotation tasks demonstrate the wide applicability of the unified solution as well as the effectiveness of two proposed new algorithms.
[Additives, convergence, Optimization methods, nonnegative matrix factorization formulation, correlated co-decomposition, Data engineering, matrix decomposition, Data mining, procedure deduction process, Image reconstruction, multilabel image annotation tasks, optimisation, block-wisely quadratic objective functions, direct update rules, algorithmic convergence proof, face recognition, nonconvex data factorization problems, Space exploration, nonnegative orthant, Charge coupled devices, Face recognition, multiplicative nonnegative update rules, Independent component analysis, out-of-sample data handling, Least squares approximation, nonnegative data factorization problems, data handling, optimization problems]
Extended Boolean Matrix Decomposition
2009 Ninth IEEE International Conference on Data Mining
None
2009
With the vast increase in collection and storage of data, the problem of data summarization is most critical for effective data management. Since much of this data is categorical in nature, it can be viewed in terms of a Boolean matrix. Boolean matrix decomposition (BMD) has been used to provide concise and interpretable representations of Boolean data sets. A Boolean matrix can be expressed as a product of two Boolean matrices, where the first matrix represents a set of meaningful concepts, and the second describes how the observed data can be expressed as combinations of those concepts. Typically, the combination is only in terms of the set union. In other words, a successful Boolean matrix decomposition gives a set of concepts and shows how every column of the input data can be expressed as a union of some subset of those concepts. However, this way of modeling only incompletely represents real data semantics. Essentially, it ignores a critical component -- the set difference operation: a column can be expressed as the combination of union of certain concepts as well as the exclusion of other concepts. This has two significant benefits. First, the total number of concepts required to describe the data may itself be reduced. Second, a more succinct summarization may be found for every column. In this paper, we propose the extended Boolean matrix decomposition (EBMD) problem, which aims to factor Boolean matrices using both the set union and set difference operations. We study several variants of the problem, show that they are NP-hard, and propose efficient heuristics to solve them. Extensive experimental results demonstrate the power of EBMD.
[Text mining, data management, data summarization, extended Boolean matrix decomposition, Boolean algebra, matrix decomposition, set theory, set difference operation, Matrix decomposition, Data mining, Itemsets, USA Councils, Feedback, set union, Motion pictures]
Active Learning with Adaptive Heterogeneous Ensembles
2009 Ninth IEEE International Conference on Data Mining
None
2009
One common approach to active learning is to iteratively train a single classifier by choosing data points based on its uncertainty, but it is nontrivial to design uncertainty measures unbiased by the choice of classifier. Query by committee suggests that given an ensemble of diverse but accurate classifiers, the most informative data points are those that cause maximal disagreement among the predictions of the ensemble members. However the method for finding ensembles appropriate to a given data set remains an open question. In this paper, the random subspace method is combined with active learning to create multiple instances of different classifier types, and an algorithm is introduced that adapts the ratio of different classifier types in the ensemble towards better overall accuracy. Here we show that the proposed algorithm outperforms C4.5 with uncertainty sampling, Naive Bayes with uncertainty sampling, bagging, boosting and the random subspace method with random sampling. To the best of our knowledge, our work is the first to adapt the ratio of classifiers in a heterogeneous ensemble for active learning.
[Costs, uncertainty measures, Stochastic processes, maximal disagreement, uncertainty handling, random sampling, Data mining, Learning systems, Ensemble Learning, active learning, random subspace method, Labeling, learning (artificial intelligence), Adaptive Heterogeneous Ensembles, uncertainty sampling, random processes, data points, Boosting, Active Learning, Computer science, Measurement uncertainty, adaptive heterogeneous ensembles, naive Bayes, Sampling methods, Bayes methods, Bagging]
Non-negative Laplacian Embedding
2009 Ninth IEEE International Conference on Data Mining
None
2009
Laplacian embedding provides a low dimensional representation for a matrix of pairwise similarity data using the eigenvectors of the Laplacian matrix. The true power of Laplacian embedding is that it provides an approximation of the ratio cut clustering. However, ratio cut clustering requires the solution to be nonnegative. In this paper, we propose a new approach, nonnegative Laplacian embedding, which approximates ratio cut clustering in a more direct way than traditional approaches. From the solution of our approach, clustering structures can be read off directly. We also propose an efficient algorithm to optimize the objective function utilized in our approach. Empirical studies on many real world datasets show that our approach leads to more accurate ratio cut solution and improves clustering accuracy at the same time.
[approximation theory, low dimensional representation, Laplace equations, Laplacian Embedding, Non-negative Matrix Factorization, Data engineering, Information retrieval, Vectors, matrix decomposition, Data mining, Matrix decomposition, Clustering, pairwise similarity data, nonnegative Laplacian embedding, eigenvalues and eigenfunctions, eigenvector, Computer science, Dimension reduction, Clustering algorithms, Machine learning, ratio cut clustering, Laplacian matrix, Power engineering and energy]
Scalable Algorithms for Distribution Search
2009 Ninth IEEE International Conference on Data Mining
None
2009
Distribution data naturally arise in countless domains, such as meteorology, biology, geology, industry and economics. However, relatively little attention has been paid to data mining for large distribution sets. Given n distributions of multiple categories and a query distribution Q, we want to find similar clouds (i.e., distributions), to discover patterns, rules and outlier clouds. For example, consider the numerical case of sales of items, where, for each item sold, we record the unit price and quantity; then, each customer is represented as a distribution of 2d points (one for each item he/she bought). We want to find similar users, e.g., for market segmentation, anomaly/fraud detection. We propose to address this problem and present D-search, which includes fast and effective algorithms for similarity search in large distribution datasets. Our main contributions are (1) approximate KL divergence, which can speed up cloud-similarity computations, (2) multi-step sequential scan, which efficiently prunes a significant number of search candidates and leads to a direct reduction in the search cost. We also introduce an extended version of D-search: (3) time-series distribution mining, which finds similar subsequences in time-series distribution datasets. Extensive experiments on real multi-dimensional datasets show that our solution achieves up to 2,300 faster wall-clock time over the naive implementation while it does not sacrifice accuracy.
[Industrial economics, similarity search, Clouds, distribution search, data mining, Multimedia databases, Electroencephalography, Data mining, query processing, D-search, multidimensional dataset, very large databases, cloud similarity, query distribution, Kinetic energy, Biomedical imaging, Meteorology, Singular value decomposition, Geology, Power generation economics, pattern discovery, KL divergence, time series, multistep sequential scan, large distribution dataset, time-series distribution mining, Distribution sets]
A Deep Non-linear Feature Mapping for Large-Margin kNN Classification
2009 Ninth IEEE International Conference on Data Mining
None
2009
KNN is one of the most popular data mining methods for classification, but it often fails to work well with inappropriate choice of distance metric or due to the presence of numerous class-irrelevant features. Linear feature transformation methods have been widely applied to extract class-relevant information to improve kNN classification, which is very limited in many applications. Kernels have also been used to learn powerful non-linear feature transformations, but these methods fail to scale to large datasets. In this paper, we present a scalable non-linear feature mapping method based on a deep neural network pretrained with Restricted Boltzmann Machines for improving kNN classification in a large-margin framework, which we call DNet-kNN. DNet-kNN can be used for both classification and for supervised dimensionality reduction. The experimental results on two benchmark handwritten digit datasets and one newsgroup text dataset show that DNet-kNN has much better performance than large-margin kNN using a linear mapping and kNN based on a deep autoencoder pretrained with Restricted Boltzmann Machines.
[Large Margin, handwritten digit datasets, data mining, Data mining, Non-linear Feature Mapping, Deep Neural Networks, Boltzmann machines, deep non-linear feature mapping, Graphical models, Non-linear Dimensionality Reduction, Genetics, large-margin kNN classification, RBM, Kernel, Power generation, deep neural network, DNet-kNN, pattern classification, linear feature transformation methods, restricted Boltzmann machines, Deep Learning, Nearest neighbor searches, Computer science, kNN Classification, class-relevant information extraction, High performance computing, Neural networks, newsgroup text dataset, neural nets, Principal component analysis]
Finding Time Series Motifs in Disk-Resident Data
2009 Ninth IEEE International Conference on Data Mining
None
2009
Time series motifs are sets of very similar subsequences of a long time series. They are of interest in their own right, and are also used as inputs in several higher-level data mining algorithms including classification, clustering, rule-discovery and summarization. In spite of extensive research in recent years, finding exact time series motifs in massive databases is an open problem. Previous efforts either found approximate motifs or considered relatively small datasets residing in main memory. In this work, we describe for the first time a disk-aware algorithm to find exact time series motifs in multi-gigabyte databases which contain on the order of tens of millions of time series. We have evaluated our algorithm on datasets from diverse areas including medicine, anthropology, computer networking and image processing and show that we can find interesting and meaningful motifs in datasets that are many orders of magnitude larger than anything considered before.
[disk-aware algorithm, Multidimensional systems, time series motif, disk-resident data, data mining, Data engineering, time series, Classification algorithms, Data mining, time series motifs, Computer science, closest pair, exact algorithm, Image databases, USA Councils, Clustering algorithms, DNA, Biomedical imaging]
Relevant Subspace Clustering: Mining the Most Interesting Non-redundant Concepts in High Dimensional Data
2009 Ninth IEEE International Conference on Data Mining
None
2009
Subspace clustering aims at detecting clusters in any subspace projection of a high dimensional space. As the number of possible subspace projections is exponential in the number of dimensions, the result is often tremendously large. Recent approaches fail to reduce results to relevant subspace clusters. Their results are typically highly redundant, i.e. many clusters are detected multiple times in several projections. In this work, we propose a novel model for relevant subspace clustering (RESCU). We present a global optimization which detects the most interesting non-redundant subspace clusters. We prove that computation of this model is NP-hard. For RESCU, we propose an approximative solution that shows high accuracy with respect to our relevance model. Thorough experiments on synthetic and real world data show that RESCU successfully reduces the result to manageable sizes. It reliably achieves top clustering quality while competing approaches show greatly varying performance.
[Data analysis, NP-hard, Computational modeling, Redundancy, Genomics, data mining, Sensor phenomena and characterization, Data mining, Gene expression, global optimization, redundancy removal, clustering quality, optimisation, relevance model, pattern clustering, high dimensional data, subspace clustering, nonredundant concepts, Object detection, subspace projections, relevant subspace clustering, non-redundant subspace clusters, Bioinformatics, Principal component analysis]
Stacked Gaussian Process Learning
2009 Ninth IEEE International Conference on Data Mining
None
2009
Triggered by a market relevant application that involves making joint predictions of pedestrian and public transit flows in urban areas, we address the question of how to utilize hidden common cause relations among variables of interest in order to improve performance in the two related regression tasks. Specifically, we propose stacked Gaussian process learning, a meta-learning scheme in which a base Gaussian process is enhanced by adding the posterior covariance functions of other related tasks to its covariance function in a stage-wise optimization. The idea is that the stacked posterior covariances encode the hidden common causes among variables of interest that are shared across the related regression tasks. Stacked Gaussian process learning is efficient, capable of capturing shared common causes, and can be implemented with any kind of standard Gaussian process regression model such as sparse approximations and relational variants. Our experimental results on real-world data from the market relevant application show that stacked Gaussian processes learning can significantly improve prediction performance of a standard Gaussian process.
[meta-learning scheme, Companies, Data mining, optimisation, Pricing, stacked Gaussian process learning, Gaussian Processes, learning (artificial intelligence), Advertising, Mining industry, Statistical Relational Learning, stage-wise optimization, Business, public transit flows, Stacked Learning, Urban areas, traffic engineering computing, urban areas, transportation, Bayesian methods, pedestrian flows, Gaussian processes, Information processing, market relevant application, Bayesian Regression]
Evaluating Statistical Tests for Within-Network Classifiers of Relational Data
2009 Ninth IEEE International Conference on Data Mining
None
2009
Recently a number of modeling techniques have been developed for data mining and machine learning in relational and network domains where the instances are not independent and identically distributed (i.i.d.). These methods specifically exploit the statistical dependencies among instances in order to improve classification accuracy. However, there has been little focus on how these same dependencies affect our ability to draw accurate conclusions about the performance of the models. More specifically, the complex link structure and attribute dependencies in network data violate the assumptions of many conventional statistical tests and make it difficult to use these tests to assess the models in an unbiased manner. In this work, we examine the task of within-network classification and the question of whether two algorithms will learn models which will result in significantly different levels of performance. We show that the commonly-used form of evaluation (paired t-test on overlapping network samples) can result in an unacceptable level of Type I error. Furthermore we show that Type I error increases as (1) the correlation among instances increases and (2) the size of the evaluation set increases (i.e., the proportion of labeled nodes in the network decreases). We propose a method for network cross-validation that combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power (i.e., Type II error).
[Algorithm design and analysis, pattern classification, Machine learning algorithms, complex link structure, network cross-validation, Taxonomy, Laboratories, data mining, Probability, within-network classifier, Data mining, relational databases, machine learning, relational data, statistical test, Machine learning, Performance analysis, Standards development, learning (artificial intelligence), statistical testing, Testing]
Discovering Excitatory Networks from Discrete Event Streams with Applications to Neuronal Spike Train Analysis
2009 Ninth IEEE International Conference on Data Mining
None
2009
Mining temporal network models from discrete event streams is an important problem with applications in computational neuroscience, physical plant diagnostics, and human-computer interaction modeling. We focus in this paper on temporal models representable as excitatory networks where all connections are stimulative, rather than inhibitory. Through this emphasis on excitatory networks, we show how they can be learned by creating bridges to frequent episode mining. Specifically, we show that frequent episodes help identify nodes with high mutual information relationships and which can be summarized into a dynamic Bayesian network (DBN). To demonstrate the practical feasibility of our approach, we show how excitatory networks can be inferred from both mathematical models of spiking neurons as well as real neuroscience datasets.
[temporal network model, frequent episode mining, data mining, human-computer interaction modeling, computational neuroscience, physical plant diagnostics, Data mining, Neuroscience, discrete event systems, USA Councils, Physics computing, discrete event stream, Computer networks, excitatory network, Temporal Data Mining, Neurons, Computational Neuroscience, Application software, Dynamic Bayesian Network, Computer science, neuronal spike train analysis, Bayesian methods, dynamic Bayesian network, Frequent Episodes, Spike train analysis, Bayes methods, Mutual information, neural nets, temporal data mining]
Clustering Trajectories of Moving Objects in an Uncertain World
2009 Ninth IEEE International Conference on Data Mining
None
2009
Mining trajectory databases (TD) has gained great interest due to the popularity of tracking devices. On the other hand, the inherent presence of uncertainty in TD (e.g., due to GPS errors) has not been taken yet into account during the mining process. In this paper, we study the effect of uncertainty in TD clustering and introduce a three-step approach to deal with it. First, we propose an intuitionistic point vector representation of trajectories that encompasses the underlying uncertainty and introduce an effective distance metric to cope with uncertainty. Second, we devise CenTra, a novel algorithm which tackles the problem of discovering the centroid trajectory of a group of movements. Third, we propose a variant of the fuzzy C-means (FCM) clustering algorithm, which embodies CenTra at its update procedure. The experimental evaluation over real world TD demonstrates the efficiency and effectiveness of our approach.
[CenTra, trajectory database mining, Uncertainty, distance metric, data mining, fuzzy set theory, visual databases, fuzzy C-means clustering algorithm, Data mining, Global Positioning System, Wireless communication, Fuzzy sets, Databases, pattern clustering, trajectory database clustering, intuitionistic point vector representation, centroid trajectory discovering, Clustering algorithms, Sampling methods, Trajectory, moving objects, Informatics]
Semi-Supervised Sequence Labeling with Self-Learned Features
2009 Ninth IEEE International Conference on Data Mining
None
2009
Typical information extraction (IE) systems can be seen as tasks assigning labels to words in a natural language sequence. The performance is restricted by the availability of labeled words. To tackle this issue, we propose a semi-supervised approach to improve the sequence labeling procedure in IE through a class of algorithms with self-learned features (SLF). A supervised classifier can be trained with annotated text sequences and used to classify each word in a large set of unannotated sentences. By averaging predicted labels over all cases in the unlabeled corpus, SLF training builds class label distribution patterns for each word (or word attribute) in the dictionary and re-trains the current model iteratively adding these distributions as extra word features. Basic SLF models how likely a word could be assigned to target class types. Several extensions are proposed, such as learning words' class boundary distributions. SLF exhibits robust and scalable behaviour and is easy to tune. We applied this approach on four classical IE tasks: named entity recognition (German and English), part-of-speech tagging (English) and one gene name recognition corpus. Experimental results show effective improvements over the supervised baselines on all tasks. In addition, when compared with the closely related self-training idea, this approach shows favorable advantages.
[Predictive models, named entity recognition, information extraction systems, Data mining, natural language sequence, USA Councils, National electric code, part-of-speech tagging, tasks assigning labels, annotated text sequences, Natural language processing, Labeling, learning (artificial intelligence), self-learned features, natural language processing, sequence labeling, gene name recognition corpus, Computer science, semisupervised sequence labeling, supervised classifier, information extraction, Neural networks, structural output learning, Machine learning, Tagging, semi-supervised feature learning, semi-supervised learning]
Semi-Markov kMeans Clustering and Activity Recognition from Body-Worn Sensors
2009 Ninth IEEE International Conference on Data Mining
None
2009
Subsequence clustering aims to find patterns that appear repeatedly in time series data. We introduce a novel subsequence clustering technique that we call semi-Markov kmeans clustering. The clustering results in ideal examples of the repeating patterns and in labeled segmentations that can be used as training data for sophisticated discriminative methods like max-margin semi-Markov models. We are applying the new clustering technique to activity recognition from body-worn sensors by showing how it can enable a system to learn from data that is only annotated by an ordered list of activity types that have been undertaken. This kind of annotation, unlike a detailed segmentation of the sensor data, is easily provided by a non-expert user. We show that we can achieve equally good results using only an ordered list of activity types for training as when using a full detailed labeled segmentation.
[time-series, Humans, Sensor systems, novel subsequence clustering technique, Data mining, body worn sensors, Text recognition, image segmentation, Training data, time series data, Wearable sensors, Accelerometers, sophisticated discriminative methods, detailed segmentation sensor data, time series, Pattern recognition, subsequence, sensors, pattern clustering, Markov processes, full detailed labeled segmentation, non expert user, clustering, Australia, Acceleration, semi Markov kmeans clustering, activity recognition]
A Sparsification Approach for Temporal Graphical Model Decomposition
2009 Ninth IEEE International Conference on Data Mining
None
2009
Temporal causal modeling can be used to recover the causal structure among a group of relevant time series variables. Several methods have been developed to explicitly construct temporal causal graphical models. However, how to best understand and conceptualize these complicated causal relationships is still an open problem. In this paper, we propose a decomposition approach to simplify the temporal graphical model. Our method clusters time series variables into groups such that strong interactions appear among the variables within each group and weak (or no) interactions exist for cross-group variable pairs. Specifically, we formulate the clustering problem for temporal graphical models as a regression-coefficient sparsification problem and define an interesting objective function which balances the model prediction power and its cluster structure. We introduce an iterative optimization approach utilizing the Quasi-Newton method and generalized ridge regression to minimize the objective function and to produce a clustered temporal graphical model. We also present a novel optimization procedure utilizing a graph theoretical tool based on the maximum weight independent set problem to speed up the Quasi-Newton method for a large number of variables. Finally, our detailed experimental study on both synthetic and real datasets demonstrates the effectiveness of our methods.
[graph theory, Optimization methods, regression analysis, quasi-Newton method, Data mining, temporal causal modeling, cross-group variable pairs, maximum weight independent set, Proteins, Graphical models, optimisation, generalized ridge regression, Network topology, Economic forecasting, graph theoretical tool, causal relationships, Newton method, optimization procedure, causal structure, Biomedical informatics, Power generation economics, time series, Time measurement, model prediction power, iterative optimization approach, causality, Computer science, temporal graphical model decomposition, temporal causal graphical models, sparsification approach, Quasi-Newton method, clustered temporal graphical model, time series variables, maximum weight independent set problem, regression-coefficient sparsification problem, sparse matrices]
Resolving Identity Uncertainty with Learned Random Walks
2009 Ninth IEEE International Conference on Data Mining
None
2009
A pervasive problem in large relational databases is identity uncertainty which occurs when multiple entries in a database refer to the same underlying entity in the world. Relational databases exhibit rich graphical structure and are naturally modeled as graphs whose nodes represent entities and whose typed-edges represent relations between them. We propose using random walk models for resolving identity uncertainty since they have proven effective for finding points which are proximately located in a network. Because not all types of relations are equally helpful in alleviating identity uncertainty, we develop a supervised approach to learning the usefulness of different database relations from a training set of database entries whose true identities are known. When tested on the task of resolving uncertainty of ambiguously named authors in bibliographical data, the learned random walk models yield performance superior to support vector machines, and to a related spectral clustering method.
[Pervasive computing, random walks, Uncertainty, support vector machines, Clustering methods, Relational databases, identity uncertainty resolution, Spatial databases, Data mining, relational databases, supervised learning approach, spectral clustering method, Support vector machines, Information science, random walk models, identity uncertainty, semi-supervised learning, learning (artificial intelligence), Joining processes, Testing]
Discriminative Mixed-Membership Models
2009 Ninth IEEE International Conference on Data Mining
None
2009
Although mixed-membership models have achieved great success in unsupervised learning, they have not been widely applied to classification problems. In this paper, we propose a family of discriminative mixed-membership models for classification by combining unsupervised mixed-membership models with multi-class logistic regression. In particular, we propose two variants respectively applicable to text classification based on latent Dirichlet allocation and usual feature vector classification based on mixed-membership naive Bayes models. The proposed models allow the number of components in the mixed membership to be different from the number of classes. We propose two variational inference based algorithms for learning the models, including a fast variational inference which is substantially more efficient than mean-field variational approximation. Through extensive experiments on UCI and text classification benchmark datasets, we show that the models are competitive with the state of the art, and can discover components not explicitly captured by the class labels.
[latent Dirichlet allocation, pattern classification, text analysis, feature vector classification, discriminative mixed-membership models, regression analysis, Classification algorithms, text classification, Data mining, unsupervised learning, Support vector machines, Computer science, naive Bayes models, Text categorization, Support vector machine classification, Delta modulation, Inference algorithms, classification problems, Linear discriminant analysis, Bayes methods, logistic regression, Logistics]
Argumentation Based Constraint Acquisition
2009 Ninth IEEE International Conference on Data Mining
None
2009
Efficient acquisition of constraint networks is a key factor for the applicability of constraint problem solving methods. Current techniques learn constraint networks from sets of training examples, where each example is classified as either a solution or non-solution of a target network. However, in addition to this classification, an expert can usually provide arguments as to why examples should be rejected or accepted. Generally speaking domain specialists have partial knowledge about the theory to be acquired which can be exploited for knowledge acquisition. Based on this observation, we discuss the various types of arguments an expert can formulate and develop a knowledge acquisition algorithm for processing these types of arguments which gives the expert the possibility to input arguments in addition to the learning examples. The result of this approach is a significant reduction in the number of examples which must be provided to the learner in order to learn the target constraint network.
[Vocabulary, Knowledge acquisition, argumentation, Knowledge based systems, knowledge acquisition, learning example, problem solving, Data mining, argumentation based constraint acquisition, Intelligent networks, constraint problem solving, constraint network, Constraint theory, constrains, Problem-solving, constraint handling, Intelligent systems, Informatics, Recommender systems, learning by example]
Extending Semi-supervised Learning Methods for Inductive Transfer Learning
2009 Ninth IEEE International Conference on Data Mining
None
2009
Inductive transfer learning and semi-supervised learning are two different branches of machine learning. The former tries to reuse knowledge in labeled out-of-domain instances while the later attempts to exploit the usefulness of unlabeled in-domain instances. In this paper, we bridge the two branches by pointing out that many semi-supervised learning methods can be extended for inductive transfer learning, if the step of labeling an unlabeled instance is replaced by re-weighting a diff-distribution instance. Based on this recognition, we develop a new transfer learning method, namely COITL, by extending the co-training method in semi-supervised learning. Experimental results reveal that COITL can achieve significantly higher generalization and robustness, compared with two state-of-the-art methods in inductive transfer learning.
[re-weighting instance, diff-distribution instance, semisupervised learning methods, Data mining, machine learning, Sun, cotraining method, Learning systems, Computer science, Inductive transfer learning, Web pages, Training data, Machine learning, Semisupervised learning, Robustness, semi-supervised learning, co-training, Labeling, learning (artificial intelligence), inductive transfer learning]
iTopicModel: Information Network-Integrated Topic Modeling
2009 Ninth IEEE International Conference on Data Mining
None
2009
Document networks, i.e., networks associated with text information, are becoming increasingly popular due to the ubiquity of Web documents, blogs, and various kinds of online data. In this paper, we propose a novel topic modeling framework for document networks, which builds a unified generative topic model that is able to consider both text and structure information for documents. A graphical model is proposed to describe the generative model. On the top layer of this graphical model, we define a novel multivariate Markov random field for topic distribution random variables for each document, to model the dependency relationships among documents over the network structure. On the bottom layer, we follow the traditional topic model to model the generation of text for each document. A joint distribution function for both the text and structure of the documents is thus provided. A solution to estimate this topic model is given, by maximizing the log-likelihood of the joint probability. Some important practical issues in real applications are also discussed, including how to decide the topic number and how to choose a good network structure. We apply the model on two real datasets, DBLP and Cora, and the experiments show that this model is more effective in comparison with the state-of-the-art topic modeling algorithms.
[document handling, Markov Random Field, Social network services, topic model, Blogs, iTopicModel, Data mining, multivariate Markov random field, Sun, information network-integrated topic modeling, Markov random fields, Computer science, Graphical models, Databases, text information, graphical model, Markov processes, Motion pictures, Web documents, Random variables, joint distribution function, document networks, joint probability log-likelihood]
Uncoverning Groups via Heterogeneous Interaction Analysis
2009 Ninth IEEE International Conference on Data Mining
None
2009
With the pervasive availability of Web 2.0 and social networking sites, people can interact with each other easily through various social media. For instance, popular sites like Del.icio.us, Flickr, and YouTube allow users to comment shared content (bookmark, photos, videos), and users can tag their own favorite content. Users can also connect to each other, and subscribe to or become a fan or a follower of others. These diverse individual activities result in a multi-dimensional network among actors, forming cross-dimension group structures with group members sharing certain similarities. It is challenging to effectively integrate the network information of multiple dimensions in order to discover cross-dimension group structures. In this work, we propose a two-phase strategy to identify the hidden structures shared across dimensions in multi-dimensional networks. We extract structural features from each dimension of the network via modularity analysis, and then integrate them all to find out a robust community structure among actors. Experiments on synthetic and real-world data validate the superiority of our strategy, enabling the analysis of collective behavior underneath diverse individual activities in a large scale.
[social networking site, Web 2.0, cross-dimension group structures, Social network services, Humans, Predictive models, heterogeneous interaction analysis, Multi-Dimensional Networks, Data engineering, Twitter, Data mining, YouTube, Videos, Computer science, Heterogeneous Network, Community Detection, social networking (online), multidimensional network, structural feature extraction, Large-scale systems, Heterogeneous Interaction, Cross-Dimension Network Validation]
Significance of Episodes Based on Minimal Windows
2009 Ninth IEEE International Conference on Data Mining
None
2009
Discovering episodes, frequent sets of events from a sequence has been an active field in pattern mining. Traditionally, a level-wise approach is used to discover all frequent episodes. While this technique is computationally feasible it may result in a vast number of patterns, especially when low thresholds are used. In this paper we propose a new quality measure for episodes. We say that an episode is significant if the average length of its minimal windows deviates greatly when compared to the expected length according to the independence model. We can apply this measure as a post-pruning step to test whether the discovered frequent episodes are truly interesting and consequently to reduce the number of output. As a main contribution we introduce a technique that allows us to compute the distribution of lengths of minimal windows using the independence model. Such a computation task is surpisingly complex and in order to solve it we compute the distribution iteratively starting from simple episodes and progressively moving towards the more complex ones. In our experiments we discover candidate episodes that have a sufficient amount of minimal windows and test each candidate for significance. The experimental results demonstrate that our approach finds significant episodes while ignoring uninteresting ones.
[pattern mining, data mining, probability, Length measurement, Data mining, episode mining, Distributed computing, episode discovery, Databases, statistical test, level-wise approach, independence model, minimal window, statistical testing, Testing, Random sequences]
Convex Non-negative Matrix Factorization in the Wild
2009 Ninth IEEE International Conference on Data Mining
None
2009
Non-negative matrix factorization (NMF) has recently received a lot of attention in data mining, information retrieval, and computer vision. It factorizes a non-negative input matrix V into two non-negative matrix factors V = WH such that W describes "clusters" of the datasets. Analyzing genotypes, social networks, or images, it can be beneficial to ensure V to contain meaningful "cluster centroids\
[Computer vision, Embedded computing, Data analysis, Social network services, archetypal analysis, data mining, information retrieval, Information retrieval, matrix decomposition, cluster centroids, Data mining, Image reconstruction, data convex hull, Image analysis, Voting, computer vision, random Gaussian points, social network analysis, convex hull non-negative matrix factorization, non negative matrix factorization, Large-scale systems, data handling]
On K-Means Cluster Preservation Using Quantization Schemes
2009 Ninth IEEE International Conference on Data Mining
None
2009
This work examines under what conditions compression methodologies can retain the outcome of clustering operations. We focus on the popular k-means clustering algorithm and we demonstrate how a properly constructed compression scheme based on post-clustering quantization is capable of maintaining the global cluster structure. Our analytical derivations indicate that a 1-bit moment preserving quantizer per cluster is sufficient to retain the original data clusters. Merits of the proposed compression technique include: a) reduced storage requirements with clustering guarantees, b) data privacy on the original values, and c) shape preservation for data visualization purposes. We evaluate quantization scheme on various high-dimensional datasets, including 1-dimensional and 2-dimensional time-series (shape datasets) and demonstrate the cluster preservation property. We also compare with previously proposed simplification techniques in the time-series area and show significant improvements both on the clustering and shape preservation of the compressed datasets.
[data compression, Data analysis, Costs, moment preserving quantization, Shape, time-series, Laboratories, privacy preservation, data mining, Quantization, time series, compression methodology, Partitioning algorithms, Data mining, k-means cluster preservation, global cluster structure, USA Councils, pattern clustering, Clustering algorithms, clustering preservation, post-clustering quantization, Artificial intelligence]
Scalable Classification in Large Scale Spatiotemporal Domains Applied to Voltage-Sensitive Dye Imaging
2009 Ninth IEEE International Conference on Data Mining
None
2009
We present an approach for learning models that obtain accurate classification of large scale data objects, collected in spatiotemporal domains. The model generation is structured in three phases: pixel selection (spatial dimension reduction), spatiotemporal features extraction and feature selection. Novel techniques for the first two phases are presented, with two alternatives for the middle phase. Model generation based on the combinations of techniques from each phase is explored. The introduced methodology is applied on datasets from the Voltage-Sensitive Dye Imaging (VSDI) domain, where the generated classification models successfully decode neuronal population responses in the visual cortex of behaving animals. VSDI currently is the best technique enabling simultaneous high spatial (10,000 points) and temporal (10 ms or less) resolution imaging from neuronal population in the cortex. We demonstrate that not only our approach is scalable enough to handle computationally challenging data, but it also contributes to the neuroimaging field of study with its decoding abilities.
[visual cortex, High-resolution imaging, spatiotemporal domains, neural decoding, feature extraction, pixel selection phase, Voltage, Large-scale systems, learning (artificial intelligence), medical image processing, scalable classification, feature selection phase, brain imaging, Image resolution, spatiotemporal features extraction phase, large scale data objects, Decoding, classification, Animals, voltage-sensitive dye imaging domain, application, spatiotemporal, Feature extraction, Brain modeling, Spatiotemporal phenomena, model generation, Spatial resolution]
Extracting Output Metadata from Scientific Deep Web Data Sources
2009 Ninth IEEE International Conference on Data Mining
None
2009
Increasingly, many data sources appear as online databases, hidden behind query forms, thus forming the deep Web. The popularity of this new medium for data dissemination is leading to new problems in data integration. Particularly, to enable data integration from multiple deep Web data sources, one needs to obtain the metadata for each of the data sources. Obtaining the metadata, particularly, the output schema, can be very challenging. This is because, given an input query, many deep web data sources only return a subset of the output schema attributes, i.e, the ones that have a non-NULL value for the corresponding input. In this paper, we propose two approaches, which are the sampling model approach and the mixture model approach, respectively, to efficiently obtain an approximately complete set of output schema attributes from a deep Web data source. Our experiments show while each of the above two approaches has limitations, a hybrid strategy, where we combine the two approaches, achieves high recall with good precision for most data sources.
[meta data, Humans, Documentation, online databases, sampling model, Data engineering, data dissemination, HTML, Data mining, Computer science, schema extraction, Databases, USA Councils, Web pages, Sampling methods, scientific deep Web data sources, Internet, output metadata extraction, data integration, deep web]
Semi-supervised Multi-task Learning with Task Regularizations
2009 Ninth IEEE International Conference on Data Mining
None
2009
Multi-task learning refers to the learning problem of performing inference by jointly considering multiple related tasks. There have already been many research efforts on supervised multi-task learning. However, collecting sufficient labeled data for each task is usually time consuming and expensive. In this paper, we consider the semi-supervised multitask learning (SSMTL) problem, where we are given a small portion of labeled points together with a large pool of unlabeled data within each task. We assume that the different tasks can form some task clusters and the task in the same cluster share similar classifier parameters. The final learning problem is relaxed to a convex one and an efficient gradient descent strategy is proposed. Finally the experimental results on both synthetic and real world data sets are presented to show the effectiveness of our method.
[Heart, Computer vision, task clusters, Vectors, gradient descent strategy, Data mining, Application software, Helium, task regularizations, Bayesian methods, Clustering algorithms, Semisupervised learning, learning (artificial intelligence), Bioinformatics, gradient methods, semisupervised multitask learning]
Fast Online Training of Ramp Loss Support Vector Machines
2009 Ninth IEEE International Conference on Data Mining
None
2009
A fast online algorithm OnlineSVMR for training Ramp-Loss Support Vector Machines (SVMRs) is proposed. It finds the optimal SVMR for t + 1 training examples using SVMR built on t previous examples. The algorithm retains the Karush-Kuhn-Tucker conditions on all previously observed examples. This is achieved by an SMO-style incremental learning and decremental unlearning under the Concave-Convex Procedure framework. Further speedup of training time could be achieved by dropping the requirement of optimality. A variant, called OnlineASVMR, is a greedy approach that approximately optimizes the SVMR objective function and is suitable for online active learning. The proposed algorithms were comprehensively evaluated on 9 large benchmark data sets. The results demonstrate that OnlineSVMR (1) has the similar computational cost as its offline counterpart; (2) outperforms IDSVM, its competing online algorithm that uses hinge-loss, in terms of accuracy, model sparsity and training time. The experiments on online active learning show that for a fixed number of label queries OnlineASVMR (1) achieves consistently better accuracy than QueryAll and competitive accuracy to Greedy approach; (2) outperforms the active learning version of IDSVM.
[Machine learning algorithms, onlineSVMR, Fasteners, SVM, Data mining, concave-convex procedure framework, CCCP, active learning, USA Councils, incremental learning, Training data, ramp loss, Cost function, Large-scale systems, Computational efficiency, learning (artificial intelligence), online active learning, concave programming, support vector machines, decremental unlearning, Karush-Kuhn-Tucker conditions, online training, convex programming, Support vector machines, SMO, Machine learning, computer aided instruction, ramp loss support vector machines, online learning]
Mining Peculiarity Groups in Day-by-Day Behavioral Datasets
2009 Ninth IEEE International Conference on Data Mining
None
2009
Behavior mining is one of the most important issues in data mining. The growing interest in the study of behavior mining has been credited to the availability of a large amount of individual behavioral data. Some objects containing common behavioral patterns in the dataset are dramatically different from other individual objects and show their peculiarities. It is very important for behavior analysis to mine these peculiar objects' groups as this has great potential in practice. However, to the best of our knowledge, it has not been explored before. In this paper, we identify this interesting and practical problem of behavior mining: mining peculiarity groups and defining a measurement of the degree of peculiarity. As the first attempt to tackle the problem, we present a set-value-oriented day-by-day behavioral data expression mode considering that daily behaviors with respect to an object should be recorded as a set of behaviors, and devise a peculiarity group mining algorithm in view of the set-value-oriented data expression which cannot be very well handled by existing methods. Furthermore, we show that our method is practical and efficient using real datasets.
[peculiarity groups mining, Computerized monitoring, Humans, data mining, behavioral pattern, peculiarity group mining, Transaction databases, Data mining, expression mode, Computer science, set-value-oriented data, Insurance, behavior mining, behavior analysis, set-value-oriented day-by-day behavioral dataset, Stock markets, Biomedical monitoring]
Online System Problem Detection by Mining Patterns of Console Logs
2009 Ninth IEEE International Conference on Data Mining
None
2009
We describe a novel application of using data mining and statistical learning methods to automatically monitor and detect abnormal execution traces from console logs in an online setting. Different from existing solutions, we use a two stage detection system. The first stage uses frequent pattern mining and distribution estimation techniques to capture the dominant patterns (both frequent sequences and time duration). The second stage use principal component analysis based anomaly detection technique to identify actual problems. Using real system data from a 203-node Hadoop cluster, we show that we can not only achieve highly accurate and fast problem detection, but also help operators better understand execution patterns in their system.
[stage detection system, Software maintenance, Statistical learning, data mining, online setting, anomaly detection, Data mining, Condition monitoring, 203-node Hadoop cluster, execution patterns, USA Councils, Web and internet services, data mining patterns, learning (artificial intelligence), Web server, logs, distribution estimation, console logs, Computerized monitoring, pattern mining, system management, problem detection, statistical learning method, monitoring, real system data, Machine learning, online system problem detection, statistical analysis, principal component analysis, Principal component analysis, frequent pattern mining]
Synthesizing Novel Dimension Reduction Algorithms in Matrix Trace Oriented Optimization Framework
2009 Ninth IEEE International Conference on Data Mining
None
2009
Dimension reduction (DR) algorithms are generally categorized into feature extraction and feature selection algorithms. In the past, few works have been done to contrast and unify the two algorithm categories. In this work, we introduce a matrix trace oriented optimization framework to provide a unifying view for both feature extraction and selection algorithms. We show that the unified view of DR algorithms allows us to discover some essential relationships among many state-of- the-art DR algorithms. Inspired by these essential insights, we propose to synthesize unlimited number of novel DR algorithms by combining, mapping and integrating the state-of-the-art algorithms. We present examples of newly synthesized DR algorithms with experimental results to show the effectiveness of our automatically synthesized algorithms.
[Machine learning algorithms, feature extraction algorithms, Iron, Data mining, machine learning, dimension reduction, matrix trace oriented optimization framework, Computer science, data reduction, dimension reduction algorithms, Asia, feature extraction, feature selection algorithms, Machine learning, Filtering algorithms, Feature extraction, Linear discriminant analysis, learning (artificial intelligence), feature selection, Principal component analysis]
Peculiarity Analysis for Classifications
2009 Ninth IEEE International Conference on Data Mining
None
2009
Peculiarity-oriented mining (POM) is a new data mining method consisting of peculiar data identification and peculiar data analysis. Peculiarity factor (PF) and local peculiarity factor (LPF) are important concepts employed to describe the peculiarity of points in the identification step. One can study the notions at both attribute and record levels. In this paper, a new record LPF called distance based record LPF (D-record LPF) is proposed, which is defined as the sum of distances between a point and its nearest neighbors. It is proved mathematically that D-record LPF can characterize accurately the probability density function of a continuous m-dimensional distribution. This provides a theoretical basis for some existing distance based anomaly detection techniques. More important, it also provides an effective method for describing the class conditional probabilities in the Bayesian classifier. The result enables us to apply peculiarity analysis for classification problems. A novel algorithm called LPF-Bayes classifier and its kernelized implementation are presented, which have some connection to the Bayesian classifier. Experimental results on several benchmark data sets demonstrate that the proposed classifiers are effective.
[distance based record local peculiarity factor, Laboratories, data mining, Data mining, Bayesian classifier, peculiar data identification, Probability density function, Peculiarity factor, data mining method, Intelligent systems, Informatics, probability density function, Data analysis, Automation, data analysis, distance based anomaly detection techniques, peculiar data analysis, Nearest neighbor searches, LPF-Bayes classifier, Computer science, security of data, Bayesian methods, peculiarity-oriented mining, local peculiarity factor, Bayes methods, continuous m-dimensional distribution]
Filtering and Refinement: A Two-Stage Approach for Efficient and Effective Anomaly Detection
2009 Ninth IEEE International Conference on Data Mining
None
2009
Anomaly detection is an important data mining task. Most existing methods treat anomalies as inconsistencies and spend the majority amount of time on modeling normal instances. A recently proposed, sampling-based approach may substantially boost the efficiency in anomaly detection but may also lead to weaker accuracy and robustness. In this study, we propose a two-stage approach to find anomalies in complex datasets with high accuracy as well as low time complexity and space cost. Instead of analyzing normal instances, our algorithm first employs an efficient deterministic space partition algorithm to eliminate obvious normal instances and generates a small set of anomaly candidates with a single scan of the dataset. It then checks each candidate with density-based multiple criteria to determine the final results. This two-stage framework also detects anomalies of different notions. Our experiments show that this new approach finds anomalies successfully in different conditions and ensures a good balance of efficiency, accuracy, and robustness.
[Algorithm design and analysis, Costs, Filtering, Density measurement, data mining, anomaly detection, Partitioning algorithms, Data mining, Computer science, deterministic space partition algorithm, security of data, complex dataset, USA Councils, Intrusion detection, normal instances, Robustness, refinement, density-based multiple criteria, filtering]
Mining Data Streams with Labeled and Unlabeled Training Examples
2009 Ninth IEEE International Conference on Data Mining
None
2009
In this paper, we propose a framework to build prediction models from data streams which contain both labeled and unlabeled examples. We argue that due to the increasing data collection ability but limited resources for labeling, stream data collected at hand may only have a small number of labeled examples, whereas a large portion of data remain unlabeled but can be beneficial for learning. Unleashing the full potential of the unlabeled instances for stream data mining is, however, a significant challenge, consider that even fully labeled data streams may suffer from the concept drifting, and inappropriate uses of the unlabeled samples may only make the problem even worse. To build prediction models, we first categorize the stream data into four different categories, each of which corresponds to the situation where concept drifting may or may not exist in the labeled and unlabeled data. After that, we propose a relational k-means based transfer semi-supervised SVM learning framework (RK-TS3VM), which intends to leverage labeled and unlabeled samples to build prediction models. Experimental results and comparisons on both synthetic and real-world data streams demonstrate that the proposed framework is able to help build prediction models more accurate than other simple approaches can offer.
[Computers, Availability, prediction model, transfer semisupervised learning, support vector machines, unlabeled samples, data mining, relational k-means algorithm, Predictive models, Data mining, Association rules, data stream, unlabeled training samples, Support vector machines, data stream mining, SVM learning framework, RK-TS3VM, Warehousing, Labeling, Australia, Virtual manufacturing, learning (artificial intelligence)]
Maximum Margin Clustering with Multivariate Loss Function
2009 Ninth IEEE International Conference on Data Mining
None
2009
This paper presents a simple but powerful extension of the maximum margin clustering (MMC) algorithm that optimizes multivariate performance measure specifically defined for clustering, including normalized mutual information, rand index and F-measure. Different from previous MMC algorithms that always employ the error rate as the loss function, our formulation involves a multivariate loss function that is a non-linear combination of the individual clustering results. Computationally, we propose a cutting plane algorithm to approximately solve the resulting optimization problem with a guaranteed accuracy. Experimental evaluations show clear improvements in clustering performance of our method over previous maximum margin clustering algorithms.
[multivariate performance measure, Machine learning algorithms, Error analysis, Laboratories, data mining, multivariate loss function, Loss measurement, Data mining, F-measure, Support vector machines, normalized mutual information, Clustering algorithms, Performance loss, Labeling, learning (artificial intelligence), maximum margin clustering, Mutual information, rand index]
Efficient Discovery of Confounders in Large Data Sets
2009 Ninth IEEE International Conference on Data Mining
None
2009
Given a large transaction database, association analysis is concerned with efficiently finding strongly related objects. Unlike traditional associate analysis, where relationships among variables are searched at a global level, we examine confounding factors at a local level. Indeed, many real-world phenomena are localized to specific regions and times. These relationships may not be visible when the entire data set is analyzed. Specially, confounding effects that change the direction of correlation is the most significant. Along this line, we propose to efficiently find confounding effects attributable to local associations. Specifically, we derive an upper bound by a necessary condition of confounders, which can help us prune the search space and efficiently identify confounders. Experimental results show that the proposed CONFOUND algorithm can effectively identify confounders and the computational performance is an order of magnitude faster than benchmark methods.
[transaction processing, Data analysis, Costs, Correlation, Confounder, Partial Correlation, Transaction databases, Data mining, database management systems, Diseases, transaction database, Phi Correlation coefficient, Upper bound, Local Association, USA Councils, CONFOUND algorithm, association analysis, large data sets, Economies of scale, search space, Public healthcare, Bioinformatics]
Vague One-Class Learning for Data Streams
2009 Ninth IEEE International Conference on Data Mining
None
2009
In this paper, we formulate a new research problem of learning from vaguely labeled one-class data streams, where the main objective is to allow users to label instance groups, instead of single instances, as positive samples for learning. The batch-labeling, however, raises serious issues because labeled groups may contain non-positive samples, and users may change their labeling interests at any time. To solve this problem, we propose a Vague One-Class Learning (VOCL) framework which employs a double weighting approach, at both instance and classifier levels, to build an ensembling framework for learning. At instance level, both local and global filterings are considered for instance weight adjustment. Two solutions are proposed to take instance weight values into the classifier training process. At classifier level, a weight value is assigned to each classifier of the ensemble to ensure that learning can quickly adapt to users' interests. Experimental results on synthetic and real-world data streams demonstrate that the proposed VOCL framework significantly outperforms other methods for vaguely labeled one-class data streams.
[data stream learning, local filtering, Predictive models, stream data, Educational institutions, Data engineering, global filtering, Data mining, batch labeling, instance weight adjustment, Computer science, vague one-class learning, Voting, USA Councils, Gain measurement, one-class learning, Labeling, Decision trees, learning (artificial intelligence), vague labeling]
Inverse Time Dependency in Convex Regularized Learning
2009 Ninth IEEE International Conference on Data Mining
None
2009
In the conventional regularized learning, training time increases as the training set expands. Recent work on L<sub>2</sub> linear SVM challenges this common sense by proposing the inverse time dependency on the training set size. In this paper, we first put forward a Primal Gradient Solver (PGS) to effectively solve the convex regularized learning problem. This solver is based on the stochastic gradient descent method and the Fenchel conjugate adjustment, employing the well-known online strongly convex optimization algorithm with logarithmic regret. We then theoretically prove the inverse dependency property of our PGS, embracing the previous work of the L<sub>2</sub> linear SVM as a special case and enable the ¿<sub>p</sub>-norm optimization to run within a bounded sphere, which qualifies more convex loss functions in PGS. We further illustrate this solver in three examples: SVM, logistic regression and regularized least square. Experimental results substantiate the property of the inverse dependency on training data size.
[¿<sub>p</sub>-norm optimization, convex regularized learning, Stochastic processes, Optimization methods, regression analysis, SVM, Data mining, Primal Gradient Solver, regularized learning, Accuracy, inverse time dependency, Training data, learning (artificial intelligence), logistic regression, support vector machines, convex programming, regularized least square, Physics, convex optimization algorithm, Support vector machines, Computer science, Fenchel conjugate adjustment, stochastic gradient descent method, Asia, Fenchel conjugate, online convex optimization, training set size, Logistics]
P-packSVM: Parallel Primal grAdient desCent Kernel SVM
2009 Ninth IEEE International Conference on Data Mining
None
2009
It is an extreme challenge to produce a nonlinear SVM classifier on very large scale data. In this paper we describe a novel P-packSVM algorithm that can solve the support vector machine (SVM) optimization problem with an arbitrary kernel. This algorithm embraces the best known stochastic gradient descent method to optimize the primal objective, and has 1/¿ dependency in complexity to obtain a solution of optimization error ¿. The algorithm can be highly parallelized with a special packing strategy, and experiences sub-linear speed-up with hundreds of processors. We demonstrate that P-packSVM achieves accuracy sufficiently close to that of SVM-light, and overwhelms the state-of-the-art parallel SVM trainer PSVM in both accuracy and efficiency. As an illustration, our algorithm trains CCAT dataset with 800 k samples in 13 minutes and 95% accuracy, while PSVM needs 5 hours but only has 92% accuracy. We at last demonstrate the capability of P-packSVM on 8 million training samples.
[Costs, packing strategy, Stochastic processes, Optimization methods, Data mining, parallel processing, SVM-light, state-of-the-art parallel SVM trainer, parallel, stochastic processes, nonlinear SVM classifier, Kernel, gradient methods, very large scale data, special packing strategy, support vector machine optimization, PSVM, pattern classification, support vector machines, kernel, parallel primal gradient descent kernel SVM, stochastic gradient descent, Physics, Support vector machines, Computer science, stochastic gradient descent method, sublinear speed-up, support vector machine, Asia, CCAT dataset, Support vector machine classification, arbitrary kernel, P-packSVM algorithm]
An L-infinity Norm Visual Classifier
2009 Ninth IEEE International Conference on Data Mining
None
2009
We introduce a mathematical framework, based on the L&#x0221E; norm distance metric, to describe human interactions in a visual data mining environment. We use the framework to build a classifier that involves an algebra on hyper-rectangles. Our classifier, called VisClassifier, generates set-wise rules from simple gestures in an exploratory visual GUI. Logging these rules allows us to apply our analysis to a new sample or batch of data so that we can assess the predictive power of our visual-processing motivated classifier. The accuracy of this classifier on widely-used benchmark datasets rivals the accuracy of competitive classifiers.
[pattern classification, graphical user interfaces, Humans, data mining, Visual data mining, mathematical framework, VisClassifier, exploratory visual GUI, Classification algorithms, Partitioning algorithms, Pattern recognition, Data mining, L&#x0221E; norm distance metric, human interaction, visual processing motivated classifier, Algebra, supervised classification, set wise rules generation, Clustering algorithms, Machine learning, L&#x0221E; norm visual classifier, Computer errors, visual data mining environment, Graphical user interfaces]
Outlier Detection Using Inductive Logic Programming
2009 Ninth IEEE International Conference on Data Mining
None
2009
We present a novel definition of outlier in the context of inductive logic programming. Given a set of positive and negative examples, the definition aims at singling out the examples showing anomalous behavior. We note that the task here pursued is different from noise removal, and, in fact, the anomalous observations we discover are different in nature from noisy ones. We discuss pecularities of the novel approach, present an algorithm for detecting outliers, discuss some examples of knowledge mined, and compare it with alternative approaches.
[Logic programming, Knowledge representation, Outlier detection, Encoding, Inductive Logic Programming, Data mining, outlier detection, Learning systems, security of data, Supervised learning, anomalous observations, Machine learning, noise removal, inductive logic programming]
Joint Emotion-Topic Modeling for Social Affective Text Mining
2009 Ninth IEEE International Conference on Data Mining
None
2009
This paper is concerned with the problem of social affective text mining, which aims to discover the connections between social emotions and affective terms based on user-generated emotion labels. We propose a joint emotion-topic model by augmenting latent Dirichlet allocation with an additional layer for emotion modeling. It first generates a set of latent topics from emotions, followed by generating affective terms from each topic. Experimental results on an online news collection show that the proposed model can effectively identify meaningful latent topics for each emotion. Evaluation on emotion prediction further verifies the effectiveness of the proposed model.
[Text mining, text analysis, Dirichlet allocation, Dictionaries, user-generated emotion label, emotion prediction, behavioural sciences, data mining, Social Affective Text Mining, Predictive models, online news collection, Data mining, Background noise, Statistics, Voting, social affective text mining, Emotion Prediction, Sampling methods, social emotion, Emotion-Topic Model, Recommender systems, emotion-topic modeling, affective term]
Algorithms for Large, Sparse Network Alignment Problems
2009 Ninth IEEE International Conference on Data Mining
None
2009
We propose a new distributed algorithm for sparse variants of the network alignment problem, which occurs in a variety of data mining areas including systems biology, database matching, and computer vision. Our algorithm uses a belief propagation heuristic and provides near optimal solutions for this NP-hard combinatorial optimization problem. We show that our algorithm is faster and outperforms or ties existing algorithms on synthetic problems, a problem in bioinformatics, and a problem in ontology matching. We also provide a unified framework for studying and comparing all network alignment solvers.
[systems biology, database matching, message-passing, belief propagation, data mining, graph matching, Ontologies, network theory (graphs), Data engineering, Data mining, optimisation, Energy resources, Bipartite graph, Bioinformatics, Distributed algorithms, ontology matching, Computer vision, NP-hard combinatorial optimization problem, network alignment, distributed algorithm, distributed algorithms, sparse network alignment problems, computer vision, bioinformatics, ontologies (artificial intelligence), belief propagation heuristic algorithm, Belief propagation, Power engineering and energy, computational complexity]
Dirichlet Mixture Allocation for Multiclass Document Collections Modeling
2009 Ninth IEEE International Conference on Data Mining
None
2009
Topic model, latent Dirichlet allocation (LDA), is an effective tool for statistical analysis of large collections of documents. In LDA, each document is modeled as a mixture of topics and the topic proportions are generated from the unimodal Dirichlet distribution prior. When a collection of documents are drawn from multiple classes, this unimodal prior is insufficient for data fitting. To solve this problem, we exploit the multimodal Dirichlet mixture prior, and propose the Dirichlet mixture allocation (DMA). We report experiments on the popular TDT2 Corpus demonstrating that DMA models a collection of documents more precisely than LDA when the documents are obtained from multiple classes.
[latent Dirichlet allocation, Vocabulary, text analysis, multiclass document collections modeling, data fitting, multimodal Dirichlet mixture prior, Statistical analysis, topic model, Image retrieval, Information retrieval, Data engineering, Dirichlet mixture allocation, Data mining, multiclass, TDT2 Corpus, Dirichlet mixture, Bayesian methods, text modeling, unimodal Dirichlet distribution prior, Inference algorithms, Linear discriminant analysis, statistical analysis, Indexing]
SLIDER: Mining Correlated Motifs in Protein-Protein Interaction Networks
2009 Ninth IEEE International Conference on Data Mining
None
2009
Correlated motif mining (CMM) is the problem to find overrepresented pairs of patterns, called motif pairs, in interacting protein sequences. Algorithmic solutions for CMM thereby provide a computational method for predicting binding sites for protein interaction. In this paper, we adopt a motif-driven approach where the support of candidate motif pairs is evaluated in the network. We experimentally establish the superiority of the Chi-square-based support measure over other support measures. Furthermore, we obtain that CMM is an NP-hard problem for a large class of support measures (including Chi-square) and reformulate the search for correlated motifs as a combinatorial optimization problem. We then present the method SLIDER which uses local search with a neighborhood function based on sliding motifs and employs the Chi-square-based support measure. We show that SLIDER outperforms existing motif-driven CMM methods and scales to large protein-protein interaction networks.
[protein interaction networks, Scalability, graph theory, Humans, combinatorial optimization problem, algorithmic solutions, interacting protein sequences, algorithmic languages, Data mining, provide computational method, local search, local search neighborhood, Proteins, Fungi, Correlated motifs, optimisation, SLIDER, genetics, Large-scale systems, large class support measures, Bioinformatics, motif driven approach, predicting binding sites, motif pairs, Biological information theory, Coordinate measuring machines, NP-hard problem, NP hard problem, PPI networks, Chisquare based support measure, correlated motif mining, mining correlated motifs, correlation methods]
Effective Anomaly Detection in Sensor Networks Data Streams
2009 Ninth IEEE International Conference on Data Mining
None
2009
This paper addresses a major challenge in data mining applications where the full information about the underlying processes, such as sensor networks or large online database, cannot be practically obtained due to physical limitations such as low bandwidth or memory, storage, or computing power. Motivated by the recent theory on direct information sampling called compressed sensing (CS), we propose a framework for detecting anomalies from these large-scale data mining applications where the full information is not practically possible to obtain. Exploiting the fact that the intrinsic dimension of the data in these applications are typically small relative to the raw dimension and the fact that compressed sensing is capable of capturing most information with few measurements, our work show that spectral methods that used for volume anomaly detection can be directly applied to the CS data with guarantee on performance. Our theoretical contributions are supported by extensive experimental results on large datasets which show satisfactory performance.
[data compression, spectral methods, compressed sensing, sensor network data stream, data mining, anomaly detection, Paper technology, Data mining, direct information sampling, residual analysis, Databases, security of data, Physics computing, Bandwidth, Streaming media, Sampling methods, stream data processing, Computer networks, Large-scale systems, Compressed sensing]
Hierarchical Bayesian Models for Collaborative Tagging Systems
2009 Ninth IEEE International Conference on Data Mining
None
2009
Collaborative tagging systems with user generated content have become a fundamental element of websites such as Delicious, Flickr or CiteULike. By sharing common knowledge, massively linked semantic data sets are generated that provide new challenges for data mining. In this paper, we reduce the data complexity in these systems by finding meaningful topics that serve to group similar users and serve to recommend tags or resources to users. We propose a well-founded probabilistic approach that can model every aspect of a collaborative tagging system. By integrating both user information and tag information into the well-known Latent Dirichlet Allocation framework, the developed models can be used to solve a number of important information extraction and retrieval tasks.
[hierarchical Bayesian models, collaborative tagging systems, data mining, identification technology, LDA, user modeling, Information retrieval, Educational institutions, International collaboration, Data systems, collaborative tagging, latent Dirichlet allocation framework, Data mining, Computer science, Bayesian methods, USA Councils, groupware, Tagging, Linear discriminant analysis, Bayes methods, Web sites]
Efficient Algorithm for Computing Link-Based Similarity in Real World Networks
2009 Ninth IEEE International Conference on Data Mining
None
2009
Similarity calculation has many applications, such as information retrieval, and collaborative filtering, among many others. It has been shown that link-based similarity measure, such as SimRank, is very effective in characterizing the object similarities in networks, such as the Web, by exploiting the object-to-object relationship. Unfortunately, it is prohibitively expensive to compute the link-based similarity in a relatively large graph. In this paper, based on the observation that link-based similarity scores of real world graphs follow the power-law distribution, we propose a new approximate algorithm, namely Power-SimRank, with guaranteed error bound to efficiently compute link-based similarity measure. We also prove the convergence of the proposed algorithm. Extensive experiments conducted on real world datasets and synthetic datasets show that the proposed algorithm outperforms SimRank by four-five times in terms of efficiency while the error generated by the approximation is small.
[Knowledge engineering, Graph Mining, collaborative filtering, Filtering, link-based similarity scores, information retrieval, Data engineering, information filtering, Data mining, Helium, Computer science, Power-SimRank, Web, Collaboration, Similarity Calculation, Computer networks, Iterative algorithms, Computer science education, data handling, Internet, SimRank, real world networks, object-to-object relationship, synthetic datasets]
Fine-Grain Perturbation for Privacy Preserving Data Publishing
2009 Ninth IEEE International Conference on Data Mining
None
2009
Recent work shows that conventional privacy preserving publishing techniques based on anonymity-groups are susceptible to corruption attacks. In a corruption attack, if the sensitive information of any anonymity-group member is uncovered, then the remaining group members are at risk. In this study, we abandon anonymity-groups and hide sensitive information through perturbation on the sensitive attribute. With each record being perturbed independently, corruption attacks cannot be effectively carried out. Previous anti-corruption work did not minimize information loss. This paper proposes to address this issue by allowing fine-grain privacy specification. We demonstrate the power of our approach through experiments on real medical and synthetic datasets.
[Data privacy, perturbation techniques, electronic publishing, Educational institutions, Data mining, anonymity group member, Diseases, privacy specification, privacy preserving data publishing, Publishing, Hospitals, fine grain perturbation, Sampling methods, data privacy, corruption attacks, Human immunodeficiency virus, Joining processes, Cancer, synthetic datasets]
Accelerated Gradient Method for Multi-task Sparse Learning Problem
2009 Ninth IEEE International Conference on Data Mining
None
2009
Many real world learning problems can be recast as multi-task learning problems which utilize correlations among different tasks to obtain better generalization performance than learning each task individually. The feature selection problem in multi-task setting has many applications in fields of computer vision, text classification and bio-informatics. Generally, it can be realized by solving a L-1-infinity regularized optimization problem. And the solution automatically yields the joint sparsity among different tasks. However, due to the nonsmooth nature of the L-1-infinity norm, there lacks an efficient training algorithm for solving such problem with general convex loss functions. In this paper, we propose an accelerated gradient method based on an ``optimal'' first order black-box method named after Nesterov and provide the convergence rate for smooth convex loss functions. For nonsmooth convex loss functions, such as hinge loss, our method still has fast convergence rate empirically. Moreover, by exploiting the structure of the L-1-infinity ball, we solve the black-box oracle in Nesterov's method by a simple sorting scheme. Our method is suitable for large-scale multi-task learning problem since it only utilizes the first order information and is very easy to implement. Experimental results show that our method significantly outperforms the most state-of-the-art methods in both convergence speed and learning accuracy.
[Gradient methods, Optimization methods, accelerated gradient method, optimal first order black-box method, Data engineering, text classification, Data mining, gradient descend, Convergence, Large-scale systems, learning (artificial intelligence), gradient methods, Nesterov method, feature selection problem, multitask sparse learning problem, Application software, Computer science, multi-task learning, L-1-infinity regularized optimization problem, Machine learning, computer vision, bioinformatics, optimal method, Acceleration, first order black-box method, L-1-infinity regularization]
CoFKM: A Centralized Method for Multiple-View Clustering
2009 Ninth IEEE International Conference on Data Mining
None
2009
This paper deals with clustering for multi-view data, i.e. objects described by several sets of variables or proximity matrices. Many important domains or applications such as information retrieval, biology, chemistry and marketing are concerned by this problematic. The aim of this data mining research field is to search for clustering patterns that perform a consensus between the patterns from different views. This requires to merge information from each view by performing a fusion process that identifies the agreement between the views and solves the conflicts. Various fusion strategies can be applied, occurring either before, after or during the clustering process. We draw our inspiration from the existing algorithms based on a centralized strategy. We propose a fuzzy clustering approach that generalizes the three fusion strategies and outperforms the main existing multi-view clustering algorithm both on synthetic and real datasets.
[proximity matrices, data mining, fuzzy set theory, multipleview data clustering process, Data mining, Information analysis, fuzzy clustering approach, Clustering algorithms, fuzzy clustering, Computational linguistics, fusion process, collaborative clustering, chemistry, biology, Social network services, information retrieval, Information retrieval, centralized method, marketing, Chemistry, multi-view data, pattern clustering, Collaboration, Web pages, Morphology, centralized clustering, CoFKM]
Active Selection of Sensor Sites in Remote Sensing Applications
2009 Ninth IEEE International Conference on Data Mining
None
2009
In a data-mining approach, a model for estimation of aerosol optical depth (AOD) from satellite observations is learned using collocated satellite and ground-based observations. For accurate learning of such a spatio-temporal model, it is important to collect ground-based data from a large number of sites. The objective of this project is to determine appropriate locations for the next set of ground-based data collection sites to maximize accuracy of AOD estimation. Ideally, a new site should capture the most significant unseen aerosol patterns and should be the least correlated with the previously observed patterns. We propose achieving this aim by selecting the locations on which the existing prediction model is the most uncertain. Several criteria were considered for site selection, including uncertainty, spatial diversity, similarity in temporal pattern, and their combination. Extensive experiments on globally distributed data over 90 AERONET sites from the years 2005 and 2006 provide strong evidence that sites selected using the proposed algorithms improve the overall AOD prediction accuracy at a faster rate than those selected randomly or based on spatial diversity among sites.
[Uncertainty, spatio-temporal model, aerosols, data mining, Sensor phenomena and characterization, collocated satellite, aerosol pattern, Data mining, site selection, temporal pattern similarity, atmospheric optics, Remote sensing, sensor site selection, aerosol optical depth, ground-based data collection site, active learning, AOD estimation, ground-based observation, learning (artificial intelligence), prediction model, Instruments, aerosol estimation, remote sensing application, uncertainty sampling, geophysics computing, remote sensing, satellite observation, active selection, Satellites, Neural networks, atmospheric techniques, Aerosols, sensor sites, spatial diversity, Spatiotemporal phenomena, Optical sensors]
Large Scale Relation Acquisition Using Class Dependent Patterns
2009 Ninth IEEE International Conference on Data Mining
None
2009
This paper proposes a minimally supervised method for acquiring high-level semantic relations such as causality and prevention from the Web. Our method learns linguistic patterns that express causality such as ¿x gave rise to y¿, and uses them to extract causal noun pairs like (global warming, malaria epidemic) from sentences like ¿global warming gave rise to a new malaria epidemic¿. The novelty of our method lies in the use of semantic word classes acquired by large scale clustering for learning class dependent patterns. We demonstrate the effectiveness of this class based approach on three large-scale relation mining tasks from 50 million Japanese Web pages. In two of these tasks we obtained more than 30,000 relation instances with over 80% precision, outperforming a state-of-the-art system by a large margin.
[data mining, World Wide Web, Data mining, Japanese Web pages, Global warming, Communications technology, data acquisition, high-level semantic relations, Large-scale systems, linguistic patterns, learning (artificial intelligence), supervised method, Text mining, natural language processing, Sea measurements, Information retrieval, Diseases, causality, pattern clustering, large scale relation acquisition, Web pages, class dependent patterns, Frequency, semantic word classes, large-scale relation mining tasks, Internet, large scale clustering, Web sites]
Finding Maximal Fully-Correlated Itemsets in Large Databases
2009 Ninth IEEE International Conference on Data Mining
None
2009
Finding the most interesting correlations among items is essential for problems in many commercial, medical, and scientific domains. Much previous research focuses on finding correlated pairs instead of correlated itemsets in which all items are correlated with each other. When designing gift sets, store shelf arrangements, or Website product categories, we are more interested in correlated itemsets than correlated pairs. We solve this problem by finding maximal fully-correlated itemsets (MFCIs), in which all subsets are closely related to all other subsets. Putting the items in an MFCI together can promote sales within this itemset. Though some exsiting methods find high-correlation itemsets, they suffer from both efficiency and effectiveness problems in large datasets. In this paper, we explore high-dimensional correlation in two ways. First, we expand the set of desirable properties for correlation measures and study the advantages and disadvantages of various measures. Second, we propose an MFCI framework to decouple the correlation measure from the need for efficient search. By wrapping the best measure in our MFCI framework, we take advantage of likelihood ratio's superiority in evaluating itemsets, make use of the properties of MFCI to eliminate itemsets with irrelevant items, and still achieve good computational performance.
[maximal fully-correlated itemsets, Web page design, correlation measure, data mining, Conference management, Data mining, database management systems, Wrapping, Itemsets, Databases, USA Councils, correlated pairs, Cities and towns, Marketing and sales, large databases, Contracts, correlation methods]
Bayesian Overlapping Subspace Clustering
2009 Ninth IEEE International Conference on Data Mining
None
2009
Given a data matrix, the problem of finding dense/uniform sub-blocks in the matrix is becoming important in several applications. The problem is inherently combinatorial since the uniform sub-blocks may involve arbitrary subsets of rows and columns and may even be overlapping. While there are a few existing methods based on co-clustering or subspace clustering, they typically rely on local search heuristics and in general do not have a systematic model for such data. We present a Bayesian Overlapping Subspace Clustering (BOSC) model which is a hierarchical generative model for matrices with potentially overlapping uniform sub-block structures. The BOSC model can also handle matrices with missing entries. We propose an EM-style algorithm based on approximate inference using Gibbs sampling and parameter estimation using coordinate descent for the BOSC model. Through experiments on both simulated and real datasets, we demonstrate that the proposed algorithm outperforms the state-of-the-art.
[Parameter estimation, data matrix, data mining, Bayesian overlapping subspace clustering, local search heuristics, Data engineering, Background noise, Sparse matrices, Gene expression, EM-style algorithm, Gibbs sampling, matrix algebra, Computer science, co-clustering, Bayesian methods, pattern clustering, Clustering algorithms, Cities and towns, parameter estimation, Inference algorithms]
Unsupervised Relation Extraction by Massive Clustering
2009 Ninth IEEE International Conference on Data Mining
None
2009
The goal of Information Extraction is to automatically generate structured pieces of information from the relevant information contained in text documents. Machine Learning techniques have been applied to reduce the cost of Information Extraction system adaptation. However, elements of human supervision strongly bias the learning process. Unsupervised learning approaches can avoid these biases. In this paper, we propose an unsupervised approach to learning for Relation Detection, based on the use of massive clustering ensembles. The results obtained on the ACE Relation Mention Detection task outperform in terms of F1 score by 5 points the state of the art of unsupervised techniques for this evaluation framework, in addition to being simpler and more flexible.
[text analysis, Costs, Adaptive systems, Humans, data mining, text documents, Unsupervised Methods, massive clustering, Data mining, Proposals, machine learning techniques, Learning systems, information extraction system adaptation, Relation Detection, Text mining, unsupervised learning approach, information retrieval, relation detection, Unsupervised learning, unsupervised learning, automatic content extraction, ACE relation mention detection, Automatic testing, pattern clustering, Machine learning, unsupervised relation extraction, Ensemble Clustering]
Regression Learning Vector Quantization
2009 Ninth IEEE International Conference on Data Mining
None
2009
Learning vector quantization (LVQ) is a popular class of nearest prototype classifiers for multiclass classification. Learning algorithms from this family are widely used because of their intuitively clear learning process and ease of implementation. In this paper we propose an extension of the LVQ algorithm to regression. Just like the LVQ algorithm, the proposed modification uses a supervised learning procedure to learn the best prototype positions, but unlike LVQ algorithm for classification, it also learns the best prototype target values. This results in the effective partition of the feature space, similar to the one the K-means algorithm would make. Experimental results on benchmark datasets showed that the proposed regression LVQ algorithm performs better than the nearest prototype competitors that choose prototypes randomly or through K-means clustering, classification LVQ on quantized target values, and similarly to the memory-based Parzen window and nearest neighbor algorithms.
[regression LVQ algorithm, Vector quantization, nearest neighbor algorithm, supervised learning, regression analysis, Partitioning algorithms, Classification algorithms, K-means clustering, Nearest neighbor searches, Support vector machines, memory-based Parzen window, vector quantisation, USA Councils, pattern clustering, multiclass classification, Prototypes, Clustering algorithms, Support vector machine classification, Training data, regression, learning vector quantization, learning (artificial intelligence)]
Projective Clustering Ensembles
2009 Ninth IEEE International Conference on Data Mining
None
2009
Recent advances in data clustering concern clustering ensembles and projective clustering methods, each addressing different issues in clustering problems. In this paper, we consider for the first time the projective clustering ensemble (PCE) problem, whose main goal is to derive a proper projective consensus partition from an ensemble of projective clustering solutions. We formalize PCE as an optimization problem which does not rely on any particular clustering ensemble algorithm, and which has the ability to handle hard as well as soft data clustering, and different feature weightings. We provide two formulations for PCE, namely a two-objective and a single-objective problem, in which the object-based and feature-based representations of the ensemble solutions are taken into account differently. Experiments have demonstrated that the proposed methods for PCE show clear improvements in terms of accuracy of the output consensus partition.
[optimization problem, Clustering methods, data mining, Partitioning algorithms, clustering ensembles, Data mining, feature-based representations, Computer science, optimisation, projective clustering, USA Councils, pattern clustering, subspace clustering, Clustering algorithms, soft data clustering, Feature extraction, projective clustering ensemble method, clustering, Computational efficiency, single-objective problem, object-based representation]
Knowledge Discovery from Citation Networks
2009 Ninth IEEE International Conference on Data Mining
None
2009
Knowledge discovery from scientific articles has received increasing attentions recently since huge repositories are made available by the development of the Internet and digital databases. In a corpus of scientific articles such as a digital library, documents are connected by citations and one document plays two different roles in the corpus: document itself and a citation of other documents. In the existing topic models, little effort is made to differentiate these two roles. We believe that the topic distributions of these two roles are different and related in a certain way. In this paper we propose a Bernoulli Process Topic (BPT) model which models the corpus at two levels: document level and citation level. In the BPT model, each document has two different representations in the latent topic space associated with its roles. Moreover, the multilevel hierarchical structure of the citation network is captured by a generative process involving a Bernoulli process. The distribution parameters of the BPT model are estimated by a variational approximation approach. In addition to conducting the experimental evaluations on the document modeling task, we also apply the BPT model to a well known scientific corpus to discover the latent topics. The comparisons against state-of-the-art methods demonstrate a very promising performance.
[Text mining, approximation theory, Laboratories, data mining, knowledge discovery, Data mining, variational approximation, Bernoulli process topic, latent models, Unsupervised learning, digital databases, Computer science, Graphical models, Software libraries, Databases, National electric code, citation networks, text mining, Linear discriminant analysis, Internet, IP networks]
An Effective Approach to Inverse Frequent Set Mining
2009 Ninth IEEE International Conference on Data Mining
None
2009
The inverse frequent set mining problem is the problem of computing a database on which a given collection of itemsets must emerge to be frequent. Earlier studies focused on investigating computational and approximability properties of this problem. In this paper, we face it under the pragmatic perspective of defining heuristic solution approaches that are effective and scalable in real scenarios. In particular, a general formulation of the problem is considered where minimum and maximum support constraints can be defined on each itemset, and where no bound is given beforehand on the size of the resulting output database. Within this setting, an algorithm is proposed that always satisfies the maximum support constraints, but which treats minimum support constraints as soft ones that are enforced as long as possible. A thorough experimentation evidences that minimum support constraints are hardly violated in practice, and that such negligible degradation in accuracy (which is unavoidable due to the theoretical intractability of the problem) is compensated by very good scaling performances.
[Data privacy, minimum support constraints, Inverse Frequent Mining, inverse frequent set mining problem, data mining, Frequency conversion, Transaction databases, Frequency measurement, Data mining, database management systems, Computational complexity, Data Reconstruction, maximum support constraints, Complexity, Degradation, database computing, Itemsets, Constraint theory, Particle measurements]
Parallel PathFinder Algorithms for Mining Structures from Graphs
2009 Ninth IEEE International Conference on Data Mining
None
2009
PathFinder networks are increasingly used in data mining for different purposes, like network visualization or knowledge extraction. This novel way of representing graphical data has been proven to give better results than other link reduction algorithms, like minimum spanning networks However, this increase in quality comes with a high computation cost, typically of the order of n^3 or higher, where n is the number of nodes in the graph. While this problem has previously been tackled by using mathematical properties to speed up the algorithm, in this paper, we propose two new algorithms to speed up PathFinder computation based on parallelization techniques to take advantage of the increasingly available multi-core hardware platform. Experiments show that both new algorithms are more efficient than the state of the art algorithms; one of them can achieve speed-ups of up to x127 with an average of x23 on recent hardware (2007).
[graph theory, data mining, minimum spanning networks, Data mining, Parallel algorithms, Concurrent computing, USA Councils, PathFinder networks, Parallel processing, Hardware, Computational efficiency, parallel computing, Graph Mining, parallel algorithms, Parallel Computing, parallelization techniques, Partitioning algorithms, Computer science, graph mining, Data visualization, knowledge extraction, link reduction algorithms, network visualization, mining structures, parallel PathFinder algorithms, graphical data]
A Bootstrap Approach to Eigenvalue Correction
2009 Ninth IEEE International Conference on Data Mining
None
2009
Eigenvalue analysis is an important aspect in many data modeling methods. Unfortunately, the eigenvalues of the sample covariance matrix (sample eigenvalues) are biased estimates of the eigenvalues of the covariance matrix of the data generating process (population eigenvalues). We present a new method based on bootstrapping to reduce the bias in the sample eigenvalues: the eigenvalue estimates are updated in several iterations, where in each iteration synthetic data is generated to determine how to update the population eigenvalue estimates. Comparison of the bootstrap eigenvalue correction with a state of the art correction method by Karoui shows that depending on the type of population eigenvalue distribution, sometimes the Karoui method performs better and sometimes our bootstrap method.
[Karoui, covariance matrices, Multidimensional systems, Statistical analysis, eigenvalue correction, data modeling methods, Covariance matrix, Matrix decomposition, Data mining, Statistics, sample covariance matrix, eigenvalues and eigenfunctions, covariance matrix, data models, bootstrapping, general statistical analysis, Statistical distributions, isotonic tree method, bootstrap approach, Eigenvalues and eigenfunctions, data generating process, Linear discriminant analysis, Random variables]
Modeling Syntactic Structures of Topics with a Nested HMM-LDA
2009 Ninth IEEE International Conference on Data Mining
None
2009
Latent Dirichlet allocation (LDA) is a commonly used topic modeling method for text analysis and mining. Standard LDA treats documents as bags of words, ignoring the syntactic structures of sentences. In this paper, we propose a hybrid model that embeds hidden Markov models (HMMs) within LDA topics to jointly model both the topics and the syntactic structures within each topic. Our model is general and subsumes standard LDA and HMM as special cases. Compared with standard LDA and HMM, our model can simultaneously discover both topic-specific content words and background functional words shared among topics. Our model can also automatically separate content words that play different roles within a topic. Using perplexity as evaluation metric, our model returns lower perplexity for unseen test documents compared with standard LDA, which shows its better generalization power than LDA.
[latent Dirichlet allocation, text analysis, Text analysis, Content management, syntactic structure modeling, data mining, Conference management, Information retrieval, Information management, Data mining, hidden Markov models, Text categorization, Hidden Markov models, Management information systems, background functional words, text mining, Linear discriminant analysis, topic modeling method, topic-specific content words]
Redistricting Using Heuristic-Based Polygonal Clustering
2009 Ninth IEEE International Conference on Data Mining
None
2009
Redistricting is the process of dividing a geographic area into districts or zones. This process has been considered in the past as a problem that is computationally too complex for an automated system to be developed that can produce unbiased plans. In this paper we present a novel method for redistricting a geographic area using a heuristic-based approach for polygonal spatial clustering. While clustering geospatial polygons several complex issues need to be addressed - such as: removing order dependency, clustering all polygons assuming no outliers, and strategically utilizing domain knowledge to guide the clustering process. In order to address these special needs, we have developed the constrained polygonal spatial clustering (CPSC) algorithm that holistically integrates do-main knowledge in the form of cluster-level and instance-level constraints and uses heuristic functions to grow clusters. In order to illustrate the usefulness of our algorithm we have applied it to the problem of formation of unbiased congressional districts. Furthermore, we compare and contrast our algorithm with two other approaches proposed in the literature for redistricting, namely-graph partitioning and simulated annealing.
[geospatial polygon clustering, Shape measurement, data mining, district formation, Data engineering, geographic information systems, heuristic-based approach, Electronic mail, Data mining, USA Councils, Clustering algorithms, cluster-level constraints, Simulated annealing, automated system, instance-level constraints, graph partitioning, constrained polygonal spatial clustering algorithm, polygonal spatial clustering, polygonal clustering, simulated annealing, Partitioning algorithms, Computer science, Satellites, heuristic-based polygonal clustering, pattern clustering, polygon, redistricitng, spatial data mining]
A Walk from 2-Norm SVM to 1-Norm SVM
2009 Ninth IEEE International Conference on Data Mining
None
2009
This paper studies how useful the standard 2-norm regularized SVM is in approximating the 1-norm SVM problem. To this end, we examine a general method that is based on iteratively re-weighting the features and solving a 2-norm optimization problem. The convergence rate of this method is unknown. Previous work indicates that it might require an excessive number of iterations. We study how well we can do with just a small number of iterations. In theory the convergence rate is fast, except for coordinates of the current solution that are close to zero. Our empirical experiments confirm this. In many problems with irrelevant features, already one iteration is often enough to produce accuracy as good as or better than that of the 1-norm SVM. Hence, it seems that in these problems we do not need to converge to the 1-norm SVM solution near zero values. The benefit of this approach is that we can build something similar to the 1-norm regularized solver based on any 2-norm regularized solver. This is quick to implement and the solution inherits the good qualities of the solver such as scalability and stability.
[iterative methods, 1-norm SVM, Stability, support vector machines, Scalability, convergence, Optimization methods, reductions, 1-norm minimization, Paper technology, SVM, Data mining, 2-norm SVM, Support vector machines, optimisation, Support vector machine classification, large-scale data, Software systems, iteration, Iterative algorithms, 2-norm optimization problem, Kernel, stability, solver]
Semi-supervised Density-Based Clustering
2009 Ninth IEEE International Conference on Data Mining
None
2009
Most of the effort in the semi-supervised clustering literature was devoted to variations of the K-means algorithm. In this paper we show how background knowledge can be used to bias a partitional density-based clustering algorithm. Our work describes how labeled objects can be used to help the algorithm detecting suitable density parameters for the algorithm to extract density-based clusters in specific parts of the feature space. Considering the set of constraints estabilished by the labeled dataset we show that our algorithm, called SSDBSCAN, automatically finds density parameters for each natural cluster in a dataset. Four of the most interesting characteristics of SSDBSCAN are that (1) it only requires a single, robust input parameter, (2) it does not need any user intervention, (3) it automatically finds the noise objects according to the density of the natural clusters and (4) it is able to find the natural cluster structure even when the density among clusters vary widely. The algorithm presented in this paper is evaluated with artificial and real-world datasets, demonstrating better results when compared to other unsupervised and semi-supervised density-based approaches.
[Noise figure, Machine learning algorithms, density-based clustering, Partitioning algorithms, Data mining, Indium phosphide, semisupervised density-based clustering, natural cluster structure, K-means algorithm, partitional density-based clustering algorithm, Clustering algorithms, Object detection, Machine learning, Semi-supervised, Semisupervised learning, Noise robustness, data handling, learning (artificial intelligence), labeled dataset]
VIF Regression: A Fast Regression Algorithm for Large Data
2009 Ninth IEEE International Conference on Data Mining
None
2009
We propose a fast regression algorithm that can substantially reduce the computational complexity of searching, yet retain good accuracy. It also guarantees to discover correlated features that are collectively predictive, and avoid model over-fitting. Its capability of controlling mFDR (marginal False Discovery Rate) statistically enables the one-pass search of the fast algorithm and guarantees the accuracy of the sparse model chosen by the algorithm without cross validation. Numerical results show that our algorithm is much faster than any other algorithm and is competitively as accurate as the best but slower algorithms.
[stepwise regression, Input variables, Computational modeling, variance inflation factor, regression analysis, Predictive models, Data mining, Statistics, Computational complexity, Global Positioning System, VIF regression, mFDR, marginal false discovery rate, cross validation, regression algorithm, Large-scale systems, variable selection, sparse model, false discovery rate, Testing, computational complexity]
Sparse Norm-Regularized Reconstructive Coefficients Learning
2009 Ninth IEEE International Conference on Data Mining
None
2009
Inspired by the fact that the final decision rule is mainly affected by a small subset of the training samples, i.e., Support Vector Machine (SVM) shows that the decision function relies on the few samples that are on or over the margin. We propose a new framework that explicitly strengthen this intuitive fact by adding an l<sub>1</sub>-norm regularizer. We give different formulations for our framework in different scenarios, and the experiments show that our framework can not only lead to high sparse solutions but also better performance than traditional methods.
[l<sub>1</sub>-norm regularizer, support vector machines, final decision rule, Laboratories, high sparse solution, Data structures, $l_1$ norm, Data mining, Image reconstruction, sparse norm regularized reconstructive coefficients learning, Learning systems, Information science, support vector machine, Supervised learning, sparse, Machine learning, decision function, learning (artificial intelligence), Kernel, Intelligent systems]
A Contrast Pattern Based Clustering Quality Index for Categorical Data
2009 Ninth IEEE International Conference on Data Mining
None
2009
Since clustering is unsupervised and highly explorative, clustering validation (i.e. assessing the quality of clustering solutions) has been an important and long standing research problem. Existing validity measures have significant shortcomings. This paper proposes a novel contrast pattern based clustering quality index (CPCQ) for categorical data, by utilizing the quality and diversity of the contrast patterns (CPs) which contrast the clusters in clusterings. High quality CPs can characterize clusters and discriminate them against each other. Experiments show that the CPCQ index (1) can recognize that expert-determined classes are the best clusters for many datasets from the UCI repository; (2) does not give inappropriate preference to larger number of clusters; (3) does not require a user to provide a distance function.
[Data analysis, Hamming distance, Data engineering, Pattern recognition, contrast pattern, Data mining, Noise measurement, Computer science, Databases, USA Councils, pattern clustering, CPCQ index, contrast pattern based clustering quality index, Frequency, categorical data, data handling, Clustering validation, clustering quality index]
Multi-document Summarization by Information Distance
2009 Ninth IEEE International Conference on Data Mining
None
2009
Fast changing knowledge on the Internet can be acquired more efficiently with the help of automatic document summarization and updating techniques. This paper described a novel approach for multi-document update summarization. The best summary is defined to be the one which has the minimum information distance to the entire document set. The best update summary has the minimum conditional information distance to a document cluster given that a prior document cluster has already been read. Experiments on the DUC 2007 dataset and the TAC 2008 dataset have proved that our method closely correlates with the human summaries and outperforms other programs such as LexRank in many categories under the ROUGE evaluation criterion.
[Text mining, text analysis, Government, conditional information distance, Kolmogorov Complexity, Humans, data mining, multidocument update summarization, Data Mining, ROUGE evaluation criterion, minimum information distance, Text Mining, Data mining, Computer science, Information science, Information Distance, Internet, Australia, Intelligent systems, Information theory]
On the (In)Security and (Im)Practicality of Outsourcing Precise Association Rule Mining
2009 Ninth IEEE International Conference on Data Mining
None
2009
The recent interest in outsourcing IT services onto the cloud raises two main concerns: security and cost. One task that could be outsourced is data mining. In VLDB 2007, Wong et al. propose an approach for outsourcing association rule mining. Their approach maps a set of real items into a set of pseudo items, then maps each transaction non-deterministically. This paper, analyzes both the security and costs associated with outsourcing association rule mining. We show how to break the encoding scheme from Wong et al. without using context specific information and reduce the security to a one-to-one mapping. We present a stricter notion of security than used by Wong et al., and then consider the practicality of outsourcing association rule mining. Our results indicate that outsourcing association rule mining may not be practical, if the data owner is concerned with data confidentiality.
[Costs, Data security, information technology, data mining, one-to-one mapping, Encoding, Transaction databases, data confidentiality, Association rules, Data mining, association rule mining, security, security of data, outsourcing, Information security, Frequency, Outsourcing, Computer security, outsourcing IT services]
Promoting Total Efficiency in Text Clustering via Iterative and Interactive Metric Learning
2009 Ninth IEEE International Conference on Data Mining
None
2009
In this paper, we propose a framework to make the text clustering process, as a whole, efficient. In a real text clustering task, an analyst usually has some expectation on the results in mind. However, a single run of a clustering algorithm on the preprocessed data would not satisfy the expectation. Then the analyst faces labor-intensive trials for improving the results that involve repetitive feature refinement and parameter tuning. We develop the Iterative and Interactive Metric Learning System (IIMLS) for addressing the challenge. Specifically, IIMLS allows analysts to input feedback on a current clustering result. Given the feedback, IIMLS optimizes metric in the feature space so that the clustering algorithm applied with the refined metric would reflect the feedback. As a byproduct, learned metric may be used for a similar dataset. Illustrative examples on a real-world dataset show IIMLS can dramatically improve efficiency of a text clustering task. The learned ¿knowledge¿, or the metric, is visualized for gaining insights of the optimized feature metric.
[Visualization, iterative methods, text analysis, interactive metric learning system, Laboratories, via iterative, real text clustering task, labor intensive trials, text clustering process, Data mining, Engines, Learning systems, optimisation, optimized feature metric, Feedback, Clustering algorithms, National electric code, algorithm theory, single run clustering algorithm, Man machine systems, pattern classification, data preprocessing, parameter tuning, IIMLS optimizes metric, promoting total efficiency, interactive metric learning, current clustering result, metric learning, interactive system, Iterative algorithms, repetitive feature refinement, real world dataset show]
A New Clustering Algorithm Based on Regions of Influence with Self-Detection of the Best Number of Clusters
2009 Ninth IEEE International Conference on Data Mining
None
2009
Clustering methods usually require to know the best number of clusters, or another parameter, e.g. a threshold, which is not ever easy to provide. This paper proposes a new graph-based clustering method called GBC which detects automatically the best number of clusters, without requiring any other parameter. In this method based on regions of influence, a graph is constructed and the edges of the graph having the higher values are cut according to a hierarchical divisive procedure. An index is calculated from the size average of the cut edges which self-detects the more appropriate number of clusters. The results of GBC for 3 quality indices (Dunn, Silhouette and Davies-Bouldin) are compared with those of K-Means, Ward's hierarchical clustering method and DBSCAN on 8 benchmarks. The experiments show the good performance of GBC in the case of well separated clusters, even if the data are unbalanced, non-convex or with presence of outliers, whatever the shape of the clusters.
[K-means, Shape, Davies-Bouldin indices, neighborhood graph, Clustering methods, Image edge detection, graph-based clustering method, DBSCAN, Partitioning algorithms, Data mining, unsupervised learning, hierarchical divisive procedure, unsupervised machine learning task, Tree graphs, Silhouette indices, Clustering algorithms, Machine learning, Iterative algorithms, clustering, Bioinformatics, Ward hierarchical clustering method, Dunn indices]
Automatically Extracting Dialog Models from Conversation Transcripts
2009 Ninth IEEE International Conference on Data Mining
None
2009
There is a growing need for task-oriented natural language dialog systems that can interact with a user to accomplish a given objective. Recent work on building task-oriented dialog systems have emphasized the need for acquiring task-specific knowledge from un-annotated conversational data. In our work we acquire task-specific knowledge by defining sub-task as the key unit of a task-oriented conversation. We propose an unsupervised, apriori like algorithm that extracts the sub-tasks and their valid orderings from un-annotated human-human conversations. Modeling dialogues as a combination of sub-tasks and their valid orderings easily captures the variability in conversations. It also provides us the ability to map our dialogue model to AIML constructs and therefore use off-the-shelf AIML interpreters to build task-oriented chat-bots. We conduct experiments on real world data sets to establish the effectiveness of the sub-task extraction process. We codify the extracted sub-tasks in an AIML knowledge base and build a chatbot using this knowledge base. We also show the usefulness of the chatbot in automatically handling customer requests by performing a user evaluation study.
[Performance evaluation, dialog models, modeling dialogues, Chat-bot, Dialog Models, task-oriented dialog systems, task-specific knowledge acquisition, Data mining, AIML knowledge base, task-oriented natural language dialog systems, AIML constructs, knowledge based systems, interactive systems, sub-task extraction process, AIML, human-human conversations, apriori like algorithm, natural language processing, task-oriented conversation, Natural languages, knowledge acquisition, task-oriented chat-bots, off-the-shelf AIML interpreters, un-annotated conversational data, customer requests, conversation transcripts, user evaluation study]
To Trust or Not to Trust? Predicting Online Trusts Using Trust Antecedent Framework
2009 Ninth IEEE International Conference on Data Mining
None
2009
This paper analyzes the trustor and trustee factors that lead to inter-personal trust using a well studied Trust Antecedent framework in management science. To apply these factors to trust ranking problem in online rating systems, we derive features that correspond to each factor and develop different trust ranking models. The advantage of this approach is that features relevant to trust can be systematically derived so as to achieve good prediction accuracy. Through a series of experiments on real data from Epinions, we show that even a simple model using the derived features yields good accuracy and outperforms MoleTrust, a trust propagation based model. SVM classifiers using these features also show improvements.
[online rating systems, Trust prediction, pattern classification, support vector machines, management science, Predictive models, Information management, Paper technology, Data mining, Sun, Information analysis, SVM classifiers, Technology management, Accuracy, Engineering management, trust antecedent framework, interactive programming, Management information systems, MoleTrust, trust ranking, online trusts, inter-personal trust, trust propagation based model]
Analysis of Subsequence Time-Series Clustering Based on Moving Average
2009 Ninth IEEE International Conference on Data Mining
None
2009
Subsequence time-series clustering (STSC), which consists of subsequence cutout with a sliding window and k-means clustering, had been commonly used in time-series data mining. However, a problem was pointed out that STSC always generates moderate sinusoidal patterns independently of the input. To address this problem, we theoretically explain and empirically confirm the similarity between STSC and moving average. The present analysis is consistent with, and simpler than, one of the most important analyses of STSC. We also question the pattern extraction in the time domain and discuss another solution.
[Power Spectrum, Time series analysis, subsequence cutout, data mining, time series, Time-series, Data mining, moving average, sliding window, Clustering, Moving Average, subsequence time series clustering, k-means clustering, Subsequence, time series data mining, moving average processes, sinusoidal patterns]
Permutation Tests for Studying Classifier Performance
2009 Ninth IEEE International Conference on Data Mining
None
2009
We explore the framework of permutation-based p-values for assessing the behavior of the classification error. In this paper we study two simple permutation tests. The first test estimates the null distribution by permuting the labels in the data; this has been used extensively in classification problems in computational biology. The second test produces permutations of the features within classes, inspired by restricted randomization techniques traditionally used in statistics. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classification error via permutation tests is effective; in particular, the restricted permutation test clearly reveals whether the classifier exploits the interdependency between the features in the data.
[computational biology, System testing, pattern classification, Data analysis, Statistical analysis, data mining, probability, permutation tests, null distribution estimation, Data mining, classification, Information technology, Computer science, significance testing, Statistical distributions, labeled data, Machine learning, restricted randomization, Computer errors, learning (artificial intelligence), Computational biology]
Interaction-Based Clustering of Multivariate Time Series
2009 Ninth IEEE International Conference on Data Mining
None
2009
In this paper, we present a novel approach to clustering multivariate time series. In contrast to previous approaches, we base our cluster notion on the interactions between the univariate time series within a data object. Our objective is to assign objects with a similar intrinsic interaction pattern to a common cluster. To formalize this idea, we define a cluster by a set of mathematical models describing the cluster-specific interaction pattern. In addition, we propose interaction K-means (IKM), an efficient algorithm for partitioning clustering of multivariate time series. The cluster-specific interaction patterns detected by IKM provide valuable information for interpretation of the cluster content. An extensive experimental evaluation on synthetic and real world data demonstrates the effectiveness and efficiency of our approach.
[interaction K-means, Clustering methods, Discrete Fourier transforms, multivariate time series, univariate time series, mathematical models, Time series, time series, Time measurement, Partitioning algorithms, Discrete wavelet transforms, Data mining, cluster-specific interaction pattern, cluster notion, interaction-based clustering, Algorithms, Magnetic resonance imaging, pattern clustering, Clustering algorithms, Mathematical model, Biomedical imaging]
PUB: A Class Description Technique Based on Partial Coverage of Subspace
2009 Ninth IEEE International Conference on Data Mining
None
2009
A good description of a class should be accurate and interpretable. Previous works describe classes either by analyzing the correlation of each attribute with the class, or by producing rules as in building a classifier. These solutions suffer from issues in accuracy and interpretability. A description naturally consists of sentences, where each sentence consists of a set of terms. Normally, a sentence is defined as a disjunction or conjunction of several terms, each of which specifies a constraint (range/set of values) on an attribute. From the data analysis point of view, a sentence specifies a subspace in the database. In this paper, we create a richer yet interpretable form of a sentence, i.e., a sentence describes an object if any $k$ attributes of that object satisfy the specified constraints. To that end, we design \\textsc{Pub}, an algorithm that produces descriptions with our form of sentences. While constructing a sentence (within the description), \\textsc{Pub} finds the optimal range/set of values for each attribute in linear time. We also empirically show that \\textsc{Pub} is efficient, and able to produce more accurate, concise and interpretable descriptions than current approaches on various real datasets.
[Algorithm design and analysis, pattern classification, Data analysis, Dairy products, data analysis, data mining, Data mining, classification, PUB algorithm, sentences interpretable form, subspace partial coverage, Iris, class description, Databases, Animals, class description technique, fault-tolerant pattern, Performance analysis, sentence construction, Decision trees, Classification tree analysis, frequent pattern mining]
Online and Batch Learning of Generalized Cosine Similarities
2009 Ninth IEEE International Conference on Data Mining
None
2009
In this paper, we define an online algorithm to learn the generalized cosine similarity measures for k-NN classification and hence a similarity matrix A corresponding to a bilinear form. In contrary to the standard cosine measure, the normalization is itself dependent on the similarity matrix which makes it impossible to use directly the algorithms developed for learning Mahanalobis distances, based on positive, semi-definite (PSD) matrices. We follow the approach where we first find an appropriate matrix and then project it onto the cone of PSD matrices, which we have adapted to the particular form of generalized cosine similarities, and more particularly to the fact that such measures are normalized. The resulting online algorithm as well as its batch version is fast and has got better accuracy as compared with state-of-the-art methods on standard data sets.
[pattern classification, k-NN classification, similarity matrix, Data mining, Equations, generalized cosine similarities, matrix algebra, Geometry, batch learning, Computer aided instruction, Measurement standards, semi-definite matrices, Euclidean distance, Gaussian processes, Mahanalobis distances, Particle measurements, Standards development, Ionosphere, Generalized cosine, online learning, Similarity learning]
Discovering Organizational Structure in Dynamic Social Network
2009 Ninth IEEE International Conference on Data Mining
None
2009
Applying the concept of organizational structure to social network analysis may well represent the power of members and the scope of their power in a social network. In this paper, we propose a data structure, called Community Tree, to represent the organizational structure in the social network. We combine the PageRank algorithm and random walks on graph to derive the community tree from the social network. In the real world, a social network is constantly changing. Hence, the organizational structure in the social network is also constantly changing. In order to present the organizational structure in a dynamic social network, we propose a tree learning algorithm to derive an evolving community tree. The evolving community tree enables a smooth transition between the two community trees and well represents the evolution of organizational structure in the dynamic social network. Experiments conducted on real data show our methods are effective at discovering the organizational structure and representing the evolution of organizational structure in a dynamic social network.
[Social network services, community tree data structure, PageRank algorithm, Finance, Power generation economics, Educational institutions, Partitioning algorithms, Dynamical social network, Data mining, organizational structure discovery, social network, Tree graphs, Clustering algorithms, social networking (online), Organizational structure, Computer networks, learning (artificial intelligence), Organizational aspects, tree learning algorithm]
Kernel Conditional Quantile Estimation via Reduction Revisited
2009 Ninth IEEE International Conference on Data Mining
None
2009
Quantile regression refers to the process of estimating the quantiles of a conditional distribution and has many important applications within econometrics and data mining, among other domains. In this paper, we show how to estimate these conditional quantile functions within a Bayes risk minimization framework using a Gaussian process prior. The resulting non-parametric probabilistic model is easy to implement and allows non-crossing quantile functions to be enforced. Moreover, it can directly be used in combination with tools and extensions of standard Gaussian processes such as principled hyperparameter estimation, sparsification, and quantile regression with input-dependent noise rates. No existing approach enjoys all of these desirable properties. Experiments on benchmark datasets show that our method is competitive with state-of-the-art approaches.
[Quantile Regression, estimation theory, kernel conditional quantile estimation, data mining, regression analysis, Predictive models, Regression, Probability distribution, Data mining, econometrics, Gaussian process, Gaussian noise, Gaussian processes, Machine learning, quantile regression, Gaussian Processes, Bayes methods, Risk management, Australia, Econometrics, Kernel, Bayes risk minimization]
Naive Bayes Classification of Uncertain Data
2009 Ninth IEEE International Conference on Data Mining
None
2009
Traditional machine learning algorithms assume that data are exact or precise. However, this assumption may not hold in some situations because of data uncertainty arising from measurement errors, data staleness, and repeated measurements, etc. With uncertainty, the value of each data item is represented by a probability distribution function (pdf). In this paper, we propose a novel naive Bayes classification algorithm for uncertain data with a pdf. Our key solution is to extend the class conditional probability estimation in the Bayes model to handle pdf's. Extensive experiments on UCI datasets show that the accuracy of naive Bayes model can be improved by taking into account the uncertainty information.
[uncertain data classification, Measurement errors, naive Bayes model, Machine learning algorithms, Uncertainty, probability, Probability distribution, Classification algorithms, Data mining, Sun, Uncertain data mining, Computer science, probability distribution function, naive Bayes classification, belief networks, learning (artificial intelligence), machine learning algorithms, Kernel, Testing]
Constraint-Based Pattern Mining in Dynamic Graphs
2009 Ninth IEEE International Conference on Data Mining
None
2009
Dynamic graphs are used to represent relationships between entities that evolve over time. Meaningful patterns in such structured data must capture strong interactions and their evolution over time. In social networks, such patterns can be seen as dynamic community structures, i.e., sets of individuals who strongly and repeatedly interact. In this paper, we propose a constraint-based mining approach to uncover evolving patterns. We propose to mine dense and isolated subgraphs defined by two user-parameterized constraints. The temporal evolution of such patterns is captured by associating a temporal event type to each identified subgraph. We consider five basic temporal events: The formation, dissolution, growth, diminution and stability of subgraphs from one time stamp to the next. We propose an algorithm that finds such subgraphs in a time series of graphs processed incrementally. The extraction is feasible due to efficient patterns and data pruning strategies. We demonstrate the applicability of our method on several real-world dynamic graphs and extract meaningful evolving communities.
[time stamp, graph theory, data mining, data pruning strategy, constraint-based pattern mining, Data mining, evolving pattern, temporal event type, Information analysis, Noise level, real-world dynamic graphs, structured data, constraint handling, Pattern analysis, Probes, Technological innovation, Stability, Social network services, social networks, isolated subgraphs, Size measurement, time series, dynamic graph, local pattern, Data models, temporal evolution, dynamic community structures, user-parameterized constraints]
Global Slope Change Synopses for Measurement Maps
2009 Ninth IEEE International Conference on Data Mining
None
2009
Quality control using scalar quality measures is standard practice in manufacturing. However, there are also quality measures that are determined at a large number of positions on a product, since the spatial distribution is important. We denote such a mapping of local coordinates on the product to values of a measure as a measurement map. In this paper, we examine how measurement maps can be clustered according to a novel notion of similarity - mapscape similarity - that considers the overall course of the measure on the map. We present a class of synopses called global slope change that uses the profile of the measure along several lines from a reference point to different points on the borders to represent a measurement map. We conduct an evaluation of global slope change using a real-world data set from manufacturing and demonstrate its superiority over other synopses.
[Temperature distribution, Shape measurement, quality control, product quality control, synopsis, Data mining, measurement, spatial distribution, measurement map, Temperature measurement, Coordinate measuring machines, Measurement standards, scalar quality measures, Quality control, Euclidean distance, Position measurement, measurement maps, manufacturing, Manufacturing, mapscape similarity, global slope change synopses]
Aspect Guided Text Categorization with Unobserved Labels
2009 Ninth IEEE International Conference on Data Mining
None
2009
This paper proposes a novel multiclass classification method and exhibits its advantage in the domain of text categorization with a large label space and, most importantly, when some of the labels were not observed in the training data. The key insight is the introduction of intermediate aspect variables that encode properties of the labels. Aspect variables serve as a joint representation for observed and unobserved labels. This way the classification problem can be viewed as a structure learning problem with natural constraints on assignments to the aspect variables. We solve the problem as a constrained optimization problem over multiple learners and show significant improvement in classifying short sentences into a large label space of categories, including previously unobserved categories.
[text analysis, Meetings, Conference management, text categorization, classification, Distributed computing, structure learning, Computer science, multiclass classsification, Publishing, Engineering management, multiclass classification, short sentence, Text categorization, aspect guided text categorization, unobserved label, Books, learning (artificial intelligence), aspect variable, Portals, constrained optimization, Software engineering]
A Fully Automated Method for Discovering Community Structures in High Dimensional Data
2009 Ninth IEEE International Conference on Data Mining
None
2009
Identifying modules, or natural communities, in large complex networks is fundamental in many fields, including social sciences, biological sciences and engineering. Recently several methods have been developed to automatically identify communities from complex networks by optimizing the modularity function. The advantage of this type of approaches is that the algorithm does not require any parameter to be tuned. However, the modularity-based methods for community discovery assume that the network structure is given explicitly and is correct. In addition, these methods work best if the network is unweighted and/or sparse. In reality, networks are often not directly defined, or may be given as an affinity matrix. In the first case, each node of the network is defined as a point in a high dimensional space and different networks can be obtained with different network construction methods, resulting in different community structures. In the second case, an affinity matrix may define a dense weighted graph, for which modularity-based methods do not perform well. In this work, we propose a very simple algorithm to automatically identify community structures from these two types of data. Our approach utilizes a k-nearest-neighbor network construction method to capture the topology embedded in high dimensional data, and applies a modularity-based algorithm to identify the optimal community structure. A key to our approach is that the network construction is incorporated with the community identification process and is totally parameter-free. Furthermore, our method can suggest appropriate pre-processing/ normalization of the data to improve the results of community identification. We tested our methods on several synthetic and real data sets, and evaluated its performance by internal or external accuracy indices. Compared with several existing approaches, our method is not only fully automatic, but also has the best accuracy overall.
[Communities, graph theory, affinity matrix, k-nearest-neighbor network construction, Optimization methods, data mining, Data engineering, Biology, dense weighted graph, Partitioning algorithms, optimal community structure, Data mining, Sparse matrices, image clustering, matrix algebra, Computer science, modularity, high dimensional data, modularity-based method, community structure, Clustering algorithms, Complex networks, large complex network]
Hierarchical Probabilistic Segmentation of Discrete Events
2009 Ninth IEEE International Conference on Data Mining
None
2009
Segmentation, the task of splitting a long sequence of discrete symbols into chunks, can provide important information about the nature of the sequence that is understandable to humans. Algorithms for segmenting mostly belong to the supervised learning family, where a labeled corpus is available to the algorithm in the learning phase. We are interested, however, in the unsupervised scenario, where the algorithm never sees examples of successful segmentation, but still needs to discover meaningful segments. In this paper we present an unsupervised learning algorithm for segmenting sequences of symbols or categorical events. Our algorithm, Hierarchical Multigram, hierarchically builds a lexicon of segments and computes a maximum likelihood segmentation given the current lexicon. Thus, our algorithm is most appropriate to hierarchical sequences, where smaller segments are grouped into larger segments. Our probabilistic approach also allows us to suggest conditional entropy as a measurement of the quality of a segmentation in the absence of labeled data. We compare our algorithm to two previous approaches from the unsupervised segmentation literature, showing it to provide superior segmentation over a number of benchmarks. We also compare our algorithm to previous approaches over a segmentation of the unlabeled interactions of a web service and its client.
[Software maintenance, supervised learning family, Segmentation, Humans, conditional entropy, Entropy, Data mining, hierarchical multigram, web service, Information systems, hierarchical probabilistic segmentation, Software analysis, discrete event systems, entropy, Voting, discrete events, benchmarks, lexicon, Instruments, maximum likelihood segmentation, discrete symbols, unsupervised segmentation literature, hierarchical systems, Application software, unsupervised learning, Web services, unsupervised learning algorithm, Machine learning, Multigram]
Topic Modeling for Sequences of Temporal Activities
2009 Ninth IEEE International Conference on Data Mining
None
2009
Temporally-ordered activity sequences are popular in many real-world domains. This paper presents an LDA-style topic model for sequences of temporal activities that captures three features of such sequences: 1) the counts of unique activities, 2) the Markov transition dependence and 3) the absolute or relative timestamp on each activity. In modeling the first two features we propose the concept of global transition probability and distinguish it with local transition probability used in previous work. In modeling the third feature, we employ a continuous time distribution to depict the time range of latent topics. The combination of the global transition probability and the temporal information helps to refine the mixture distribution over topics for temporal sequence analysis. We present results on the data of system call traces, showing better next activity prediction and sequence clustering.
[Laboratories, Humans, Predictive models, Markov transition dependence, LDA, topic modeling, Data mining, Information analysis, Virtual reality, Telephony, global transition probability, Linear discriminant analysis, information theory, continuous time distribution, LDA style topic model, local transition probability, sequence, Content addressable storage, temporal sequence analysis, pattern clustering, temporally ordered activity sequence, Web pages, temporal activities sequence, Markov processes, mixture distribution, relative timestamp, temporal activities]
Combining Super-Structuring and Abstraction on Sequence Classification
2009 Ninth IEEE International Conference on Data Mining
None
2009
We present an approach to adapting the data representation used by a learner on sequence classification tasks. Our approach that exploits the complementary strengths of super-structuring (constructing complex features by combining existing features) and abstraction (grouping of similar features to generate more abstract features), yields smaller and, at the same time, accurate models. Super-structuring provides a way to increase the predictive accuracy of the learned models by enriching the data representation (and hence, increases the complexity of the learned models) whereas abstraction helps reduce the number of model parameters by simplifying the data representation. The results of our experiments on two data sets drawn from macromolecular sequence classification applications show that adapting data representation by combining super-structuring and abstraction, makes it possible to construct predictive models that use significantly smaller number of features (by one to three orders of magnitude) than those that are obtained using super-structuring alone, without sacrificing predictive accuracy. Our experiments also show that simplifying data representation using abstraction yields better performing models than those obtained using feature selection.
[adapting data representation, data representation, macromolecular sequence classification, sequence classification, Predictive models, Amino acids, Data mining, super-structuring, combining super structuring, Proteins, learned models complexity, Accuracy, feature extraction, smaller number features, data sets drawn, data structures, feature selection, Computational biology, Sequences, Biological system modeling, grouping similar features, combining existing features, abstraction, constructing complex features, Computer science, Diversity reception, learner sequence classification tasks]
A Global-Model Naive Bayes Approach to the Hierarchical Prediction of Protein Functions
2009 Ninth IEEE International Conference on Data Mining
None
2009
In this paper we propose a new global-model approach for hierarchical classification, where a single global classification model is built by considering all the classes in the hierarchy - rather than building a number of local classification models as it is more usual in hierarchical classification. The method is an extension of the flat classification algorithm naive Bayes. We present the extension made to the original algorithm as well as its evaluation on eight protein function hierarchical classification datasets. The achieved results are positive and show that the proposed global model is better than using a local model approach.
[Machine learning algorithms, Biomedical informatics, protein functions prediction, hierarchical classification, global classification model, Predictive models, Biomedical computing, Classification algorithms, Data mining, Proteins, naive Bayes approach, local classification models, protein function prediction, bioinformatics, bayesian classification, belief networks, Bioinformatics, Testing, Classification tree analysis]
Spatio-temporal Energy Based Gait Recognition
2009 Ninth IEEE International Conference on Data Mining
None
2009
Recently there has been lot of interest in using the gait energy image (GEI) of human walk sequence for individual recognition. Researchers have reported very good recognition rates using both unsupervised and supervised methods for normal walk sequences. However, the performance degrades when there is a variant like change in clothing or carrying a bag. This paper shows that the performance for the variant situations can be improved by constructing the GEI with sway alignment instead of upper body alignment, and dynamically selecting just the required number of rows from the bottom of the silhouette as inputs for an unsupervised feature selection approach. The improvement in recognition rates are established with performance testing on a large gait dataset.
[Biometrics, Image recognition, gait recognition, Shape, Humans, Data mining, Gait Energy Image(GEI), gait analysis, Degradation, Gait Recognition, spatio-temporal energy, feature extraction, normal walk sequence, image sequences, Testing, Legged locomotion, sway alignment, Biological system modeling, human walk sequence, unsupervised method, unsupervised feature selection, image motion analysis, unsupervised learning, gait energy image, Feature extraction, Human Identification, image recognition]
Feature Selection in the Tensor Product Feature Space
2009 Ninth IEEE International Conference on Data Mining
None
2009
Classifying objects that are sampled jointly from two or more domains has many applications. The tensor product feature space is useful for modeling interactions between feature sets in different domains but feature selection in the tensor product feature space is challenging. Conventional feature selection methods ignore the structure of the feature space and may not provide the optimal results. In this paper we propose methods for selecting features in the original feature spaces of different domains. We obtained sparsity through two approaches, one using integer quadratic programming and another using L1-norm regularization. Experimental studies on biological data sets validate our approach.
[integer programming, L1-norm regularization, Laboratories, Stacking, Predictive models, tensors, integer quadratic programming, Application software, Data mining, quadratic programming, tensor product feature space, Proteins, Tensile stress, vectors, Industry applications, Computer graphics, Kernel, feature selection]
Topic Distributions over Links on Web
2009 Ninth IEEE International Conference on Data Mining
None
2009
It is well known that Web users create links with different intentions. However, a key question, which is not well studied, is how to categorize the links and how to quantify the strength of the influence of a Web page on another if there is a link between the two linked Web pages. In this paper, we focus on the problem of link semantics analysis, and propose a novel supervised learning approach to build a model, based on a training link-labeled and link-weighted graph where a link-label represents the category of a link and a link-weight represents the influence of one web page on the other in a link. Based on the model built, we categorize links and quantify the influence of Web pages on the others in a large graph in the same application domain. We discuss our proposed approach, namely pairwise restricted Boltzmann machines (PRBMs), and conduct extensive experimental studies to demonstrate the effectiveness of our approach using large real datasets.
[Social network services, link semantics analysis, graph theory, Information retrieval, Data engineering, pairwise restricted Boltzmann machines, Data mining, supervised learning approach, Link semantic analysis, Computer science, topic distributions, Supervised learning, Link analysis, Web pages, Web page, Systems engineering and theory, link-weighted graph, Internet, learning (artificial intelligence), Research and development management, link-labeled graph, Pairwise restricted boltzmann machines, Indexing]
Clustering with Multiple Graphs
2009 Ninth IEEE International Conference on Data Mining
None
2009
In graph-based learning models, entities are often represented as vertices in an undirected graph with weighted edges describing the relationships between entities. In many real-world applications, however, entities are often associated with relations of different types and/or from different sources, which can be well captured by multiple undirected graphs over the same set of vertices. How to exploit such multiple sources of information to make better inferences on entities remains an interesting open problem. In this paper, we focus on the problem of clustering the vertices based on multiple graphs in both unsupervised and semi-supervised settings. As one of our contributions, we propose Linked Matrix Factorization (LMF) as a novel way of fusing information from multiple graph sources. In LMF, each graph is approximated by matrix factorization with a graph-specific factor and a factor common to all graphs, where the common factor provides features for all vertices. Experiments on SIAM journal data show that (1) we can improve the clustering accuracy through fusing multiple sources of information with several models, and (2) LMF yields superior or competitive results compared to other graph-based clustering methods.
[Information resources, Industrial relations, Clustering methods, graph theory, Data engineering, Mathematics, Data mining, graph, unsupervised learning, graph clustering, multiple sources, pattern clustering, Supervised learning, Clustering algorithms, Machine learning, Inference algorithms, graph based learning models, clustering, semi-supervised learning, undirected graph, linked matrix factorization]
Two Heads Better Than One: Metric+Active Learning and its Applications for IT Service Classification
2009 Ninth IEEE International Conference on Data Mining
None
2009
Large IT service providers track service requests and their execution through problem/change tickets. It is important to classify the tickets based on the problem/change description in order to understand service quality and to optimize service processes. However, two challenges exist in solving this classification problem: 1) ticket descriptions from different classes are of highly diverse characteristics, which invalidates most standard distance metrics; 2) it is very expensive to obtain high-quality labeled data. To address these challenges, we develop two seemingly independent methods 1) discriminative neighborhood metric learning (DNML) and 2) active learning with median selection (ALMS), both of which are, however, based on the same core technique: iterated representative selection. A case study on real IT service classification application is presented to demonstrate the effectiveness and efficiency of our proposed methods.
[pattern classification, service quality, discriminative neighborhood metric learning, IT service classification, distance metrics, Data mining, Application software, Environmental management, active learning with median selection, ticket descriptions, Software quality, Hardware, Outsourcing, learning (artificial intelligence)]
Maximum Margin Clustering on Data Manifolds
2009 Ninth IEEE International Conference on Data Mining
None
2009
Clustering is one of the most fundamental and important problems in computer vision and pattern recognition communities. Maximum margin clustering (MMC) is a recently proposed clustering technique which has shown promising experimental results. The main theme behind MMC is to extend the standard maximum margin principle in support vector machine (SVM) to the unsupervised scenario. This paper will consider the problem of maximum margin clustering on data manifolds. Specifically, we propose an approach called manifold regularized maximum margin clustering (MRMMC) which combines both the maximum margin data discrimination and data manifold information in a unified clustering objective and propose an efficient algorithm to solve it. Finally the experimental results on several real world data sets are presented to show the effectiveness of our method.
[Computer vision, support vector machines, Face recognition, Clustering methods, Communities, Pattern recognition, Data mining, Support vector machines, Manifolds, data manifolds, support vector machine, pattern clustering, maximum margin data discrimination, Clustering algorithms, computer vision, manifold regularized maximum margin clustering, Stereo vision, pattern recognition]
Discovering Contexts and Contextual Outliers Using Random Walks in Graphs
2009 Ninth IEEE International Conference on Data Mining
None
2009
The identifying of contextual outliers allows the discovery of anomalous behavior that other forms of outlier detection cannot find. What may appear to be normal behavior with respect to the entire data set can be shown to be anomalous by subsetting the data according to specific spatial or temporal context. However, in many real-world applications, we may not have sufficient a priori contextual information to discover these contextual outliers. This paper addresses the problem by proposing a probabilistic approach based on random walks, which can simultaneously explore meaningful contexts and score contextual outliers therein. Our approach has several advantages including producing outlier scores which can be interpreted as stationary expectations and their calculation in closed form in polynomial time. In addition, we show that point outlier detection using the stationary distribution is a special case of our approach. It allows us to find both global and contextual outliers simultaneously and to create a meaningful ranked list consisting of both types of outliers. This is a major departure from existing work where an algorithm typically identifies one type of outlier. The effectiveness of our method is justified by empirical results on real data sets, with comparison to related work.
[polynomial time approximation, random walks, Demography, Social network services, data mining, probability, Data mining, contextual outliers detection, probabilistic approach, Computer science, stationary distribution, Aging, Polynomials, Matrix converters, Context modeling, computational complexity]
Effective Criterion Functions for Efficient Agglomerative Clustering on Very Large Networks
2009 Ninth IEEE International Conference on Data Mining
None
2009
As the agglomerative clustering algorithm is widely used in data mining, image processing, bioinformatics and pattern recognition. it has attracted great interests from both academical and industrial communities. However, existing studies neglect the decisive factor of the efficiency of the agglomerative clustering algorithm for large complex networks and usually use criterion functions which lead to inefficiency. In this paper, we propose three effective criterion functions for improving performance of agglomerative clustering algorithm. We note that clustering efficiency is determined by two factors: a) the number of neighbors of two merged clusters in each merge step; b) the number of neighbors shared by the two clusters. Based on these observations, we propose a framework for designing criterion functions in order to efficiently find clusters in very large networks. We devise three criterion functions that can effectively control the number of neighbors of clusters, and they can efficiently produce high-quality clusters. We have implemented our method and compared with existing studies on real networks, and our method outperforms state-of-the-art approaches significantly on large networks.
[image processing, agglomerative clustering, Image processing, data mining, network theory (graphs), Pattern recognition, complex networks, Data mining, criterion function, graph, Content addressable storage, Clustering algorithms, very large networks, Computer architecture, effective criterion functions, bioinformatics, Computer industry, Computer networks, Bioinformatics, Mining industry, efficient agglomerative clustering, large complex networks, pattern recognition]
Binomial Matrix Factorization for Discrete Collaborative Filtering
2009 Ninth IEEE International Conference on Data Mining
None
2009
Matrix factorization (MF) models have proved efficient and well scalable for collaborative filtering (CF) problems. Many researchers also present the probabilistic interpretation of MF. They usually assume that the factor vectors of users and items are from normal distributions, and so are the ratings when the user and item factors are given. Then they can derive the exact MF algorithm by finding a MAP estimate of the model parameters. In this paper we suggest a new probabilistic perspective on MF for discrete CF problems. We assume that all ratings are from binomial distributions with different preference parameters instead of the original normal distributions. The new interpretation is more reasonable for discrete CF problems since they only allow several legal discrete rating values. We also present two effective algorithms to learn the new model and make predictions. They are applied to the Netflix Prize data set and acquire considerably better accuracy than those of MF.
[Netflix Prize, Netflix Prize data set, collaborative filtering, Filtering, Law, (probabilistic) matrix factorization, Stochastic processes, binomial matrix factorization, Gaussian distribution, information filtering, discrete collaborative filtering, binomial, variational Bayes, Sparse matrices, binomial distributions, factor vectors, matrix algebra, Collaboration, Motion pictures, Collaborative work, Random variables, Legal factors]
Bi-relational Network Analysis Using a Fast Random Walk with Restart
2009 Ninth IEEE International Conference on Data Mining
None
2009
Identification of nodes relevant to a given node in a relational network is a basic problem in network analysis with great practical importance. Most existing network analysis algorithms utilize one single relation to define relevancy among nodes. However, in real world applications multiple relationships exist between nodes in a network. Therefore, network analysis algorithms that can make use of more than one relation to identify the relevance set for a node are needed. In this paper, we show how the Random Walk with Restart (RWR) approach can be used to study relevancy in a bi-relational network from the bibliographic domain, and show that making use of two relations results in better results as compared to approaches that use a single relation. As relational networks can be very large, we also propose a fast implementation for RWR by adapting an existing Iterative Aggregation and Disaggregation (IAD) approach. The IAD-based RWR exploits the block-wise structure of real world networks. Experimental results show significant increase in running time for the IAD-based RWR compared to the traditional power method based RWR.
[Algorithm design and analysis, random walk with restart approach, data mining, bibliographic domain, Data mining, IAD based RWR, Information analysis, fast random walk, iterative aggregation and disaggregation approach, random walk, USA Councils, iterative aggregation and disaggregation, Computer networks, birelational network analysis, Large-scale systems, Iterative methods, relational network, RWR approach, random processes, relational databases, node relevancy, IAD approach, Iterative algorithms, Relational data mining, block wise structure]
A New MCA-Based Divisive Hierarchical Algorithm for Clustering Categorical Data
2009 Ninth IEEE International Conference on Data Mining
None
2009
Clustering categorical data faces two challenges, one is lacking of inherent similarity measure, and the other is that the clusters are prone to being embedded in different subspace. In this paper, we propose the first divisive hierarchical clustering algorithm for categorical data. The algorithm, which is based on multiple correspondence analysis (MCA), is systematic, efficient and effective. In our algorithm, MCA plays an important role in analyzing the data globally. The proposed algorithm has five merits. First, our algorithm yields a dendrogram representing nested groupings of patterns and similarity levels at different granularities. Second, it is parameter-free, fully automatic and, most importantly, requires no assumption regarding the number of clusters. Third, it is independent of the order in which the data are processed. Forth, it is scalable to large data sets; and finally, using the novel data representation and Chi-square distance measures makes our algorithm capable of seamlessly discovering the clusters embedded in the subspaces. Experiments on both synthetic and real data demonstrate the superior performance of our algorithm.
[Algorithm design and analysis, Data analysis, data representation, Categorical Data, divisive hierarchical algorithm, Chi-square distance measures, Mathematics, MCA, Data mining, Clustering, Computational complexity, Computer science, group theory, multiple correspondence analysis, pattern clustering, Clustering algorithms, categorical data clustering, data structures, nested groupings, Divisive Hierarchical, dendrogram]
Non-sparse Multiple Kernel Learning for Fisher Discriminant Analysis
2009 Ninth IEEE International Conference on Data Mining
None
2009
We consider the problem of learning a linear combination of pre-specified kernel matrices in the Fisher discriminant analysis setting. Existing methods for such a task impose an ¿<sub>1</sub> norm regularisation on the kernel weights, which produces sparse solution but may lead to loss of information. In this paper, we propose to use ¿<sub>2</sub> norm regularisation instead. The resulting learning problem is formulated as a semi-infinite program and can be solved efficiently. Through experiments on both synthetic data and a very challenging object recognition benchmark, the relative advantages of the proposed method and its ¿<sub>1</sub> counterpart are demonstrated, and insights are gained as to how the choice of regularisation norm should be made.
[Semi-Infinite Programming, Speech analysis, Fisher Discriminant Analysis, Linear programming, semi-infinite programming, Fisher discriminant analysis, Object recognition, Data mining, Multiple Kernel Learning, Support vector machines, Object Recognition, multiple kernel learning, Support vector machine classification, Signal processing, learning (artificial intelligence), Kernel, Signal analysis, Speech processing, statistics, norm regularization]
Multirelational Topic Models
2009 Ninth IEEE International Conference on Data Mining
None
2009
In this paper we propose the multirelational topic model (MRTM) for multiple types of link modeling such as citation and coauthor links in document networks. In the citation network, the MRTM models the citation link between each pair of documents as a binary variable conditioned on their topic distributions. In the coauthor network, the MRTM models the coauthor link between each pair of authors as a binary variable conditioned on their expertise distributions. The topic discovery is collectively regularized by multiple relations in both citation and coauthor networks. This model can summarize topics from the document network, predict citation links between documents and coauthor links between authors. Efficient inference and learning algorithms are derived based on Gibbs sampling. Experiments demonstrate that the MRTM significantly outperforms other state-of-the-art single-relational link modeling methods for large scientific document networks.
[single-relational link modeling, scientific document network, Predictive models, citation link, Data mining, multirelational link modeling, topic discovery, multirelational topic model, Topic models, citation analysis, Robustness, document handling, learning algorithm, sampling methods, citation network, topic distribution, Gibbs sampling, Markov random fields, Computer science, coauthor network, inference algorithm, Collaboration, Sampling methods, Collaborative work, Inference algorithms, Random variables, coauthor link, document networks]
Learning Local Components to Understand Large Bayesian Networks
2009 Ninth IEEE International Conference on Data Mining
None
2009
Bayesian networks are known for providing an intuitive and compact representation of probabilistic information and allowing the creation of models over a large and complex domain. Bayesian learning and reasoning are nontrivial for a large Bayesian network. In parallel, it is a tough job for users (domain experts) to extract accurate information from a large Bayesian network due to dimensional difficulty. We define a formulation of local components and propose a clustering algorithm to learn such local components given complete data. The algorithm groups together most inter-relevant attributes in a domain. We evaluate its performance on three benchmark Bayesian networks and provide results in support. We further show that the learned components may represent local knowledge more precisely in comparison to the full Bayesian networks when working with a small amount of data.
[clustering algorithm, Buildings, large Bayesian networks, Bayesian learning, learning local components, Data mining, Proposals, Computer science, Learning systems, Bayesian methods, pattern clustering, Clustering algorithms, Complex networks, local components, Iterative algorithms, Computer networks, Bayesian reasoning, belief networks, learning (artificial intelligence)]
RING: An Integrated Method for Frequent Representative Subgraph Mining
2009 Ninth IEEE International Conference on Data Mining
None
2009
We propose a novel representative based subgraph mining model. A series of standards and methods are proposed to select invariants. Patterns are mapped into invariant vectors in a multidimensional space. To find qualified patterns, only a subset of frequent patterns is generated as representatives, such that every frequent pattern is close to one of the representative patterns while representative patterns are distant from each other. We devise the RING algorithm, integrating the representative selection into the pattern mining process. Meanwhile, we use R-trees to assist this mining process. Last but not least, a large number of real and synthetic datasets are employed for the empirical study, which show the benefits of the representative model and the efficiency of the RING algorithm.
[Multidimensional systems, Social network services, subgraph mining representative, graph theory, data mining, trees (mathematics), pattern mining process, Spatial databases, RING algorithm, Data mining, invariant vectors, Distributed computing, Monte Carlo methods, USA Councils, frequent representative subgraph mining, R-trees, Pattern analysis, Testing, Indexing]
A Cost-Effective LSH Filter for Fast Pairwise Mining
2009 Ninth IEEE International Conference on Data Mining
None
2009
The pairwise mining problem is to discover pairwise objects having measures greater than the user-specified minimum threshold from a collection of objects. It is essential in a large variety of database and data-mining applications. Of late, there has been increasing interest in applying a Locality-Sensitive Hashing (LSH) scheme for pairwise mining. LSH-type methods have shown themselves to be simply implementable and capable of achieving significant performance gain in running time over most exact methods. However, the present LSH-type methods still suffer from some bottlenecks, such as ¿the curse of threshold¿. In this paper, we proposed a novel LSH-based method, namely Cost-effective LSH filter (Ce-LSH for short), for pairwise mining. Compared with previous LSH-type methods, it uses a lower fixed number of LSH functions and is thus more cost-effective. Substantial experiments evidence that our method gives significant improvement in running time over existing LSH-type methods and some recently reported method based on upper-bound. Experimental results also indicate that it scales well even for a relatively low minimum threshold and for a fairly small miss ratio.
[locality hashing function, data mining, Performance gain, Data engineering, Information filtering, Data mining, database management systems, Information technology, Computer science, locality-sensitive hashing, cost-effective LSH filter, database, Databases, pairwise mining, Information filters, curse of threshold, Data models, Australia]
A New Kernel-Based Classification Algorithm
2009 Ninth IEEE International Conference on Data Mining
None
2009
A new kernel-based learning algorithm called kernel affine subspace nearest point (KASNP) approach is proposed in this paper. Inspired by the geometrical explanation of support vector machines (SVMs) and its nearest point problem in convex hulls, we extend the convex hull of each class to its corresponding affine subspace in high dimensional space induced by kernel. In two class affine subspaces, KASNP finds the nearest points and then constructs a separating hyperplane, which bisects the line segment joining them. The nearest point problem of KASNP is only an unconstrained optimal problem whose solution can be directly computed. Compared with SVM, KASNP avoids solving convex quadratic programming. Experiments on two-spiral dataset, two UCI credit datasets, and face recognition datasets show that our proposed KASNP is effective for data classification.
[two-spiral dataset, face recognition datasets, Data engineering, convex hulls, Classification algorithms, SVM, Data mining, kernel affine subspace nearest pointa pproach, kernel-based learning algorithm, data classification, face recognition, learning (artificial intelligence), Kernel, subspace, data analysis, support vector machines, Face recognition, Data security, kernel, convex programming, convex quadratic programming, UCI credit datasets, Quadratic programming, classification, Support vector machines, Support vector machine classification, Information security, nearest points, unconstrained optimal problem]
Welcome message from the Conference Chairs
2010 IEEE International Conference on Data Mining
None
2010
Presents the welcome message from the conference proceedings.
[]
Message from the Program Committee Co-Chairs
2010 IEEE International Conference on Data Mining
None
2010
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
2010 IEEE International Conference on Data Mining
None
2010
Provides a listing of current committee members.
[]
Program Committee
2010 IEEE International Conference on Data Mining
None
2010
Provides a listing of current committee members.
[]
Mining Billion-node Graphs: Patterns, Generators and Tools
2010 IEEE International Conference on Data Mining
None
2010
What do graphs look like? How do they evolve over time? How to handle a graph with a billion nodes? We present a comprehensive list of static and temporal laws, and some recent observations on real graphs (e.g., "eigenSpokes"). For generators, we describe some recent ones, which naturally match all of the known properties of real graphs. Finally, for tools, we present "oddball" for discovering anomalies and patterns, as well as an overview of the PEGASUS system which is designed for handling Billion-node graphs, running on top of the "hadoop" system.
[Graphics, hadoop system, graph theory, data mining, billion-node graphs, Streaming media, oddball, Generators, Data mining, PEGASUS system]
Assessing the Significance of Groups in High-Dimensional Data
2010 IEEE International Conference on Data Mining
None
2010
Summary form only only given. We consider the problem of assessing the significance of groups in high-dimensional data. In the case of supervised classification where there are data of known origin with respect to the groups under consideration, a guide to the degree of separation among the groups can be given in terms of the estimated error rate of a classifier formed to allocate a new observation to one of the groups. Even in this case with labelled training data, care has to be taken with the estimation of the error rate at least for high-dimensional data to avoid an overly optimistic assessment due to selection biases. In the case of unlabelled data, the problem of assessing whether groups identified from some data mining or cluster analytic procedure are genuine can be quite challenging, in particular for a large number of variables. We shall focus on the use of a resampling approach to this problem applied in conjunction with factor analytic models for the generation of the bootstrap samples under the null hypothesis for the number of groups. The proposed methods are to be demonstrated in their application to some high-dimensional data sets from the bioinformatics literature.
[pattern classification, null hypothesis, sampling methods, error rate estimation, Error analysis, Biological system modeling, Conferences, labelled training data, data mining, bioinformatics literature, optimistic assessment, Mathematics, Data mining, Analytical models, resampling approach, high dimensional data, supervised classification, bioinformatics, bootstrap sample, cluster analytic procedure, Bioinformatics, factor analytic model]
10 Years of Data Mining Research: Retrospect and Prospect
2010 IEEE International Conference on Data Mining
None
2010
false
[Computer science, IEEE Computer Society, Conferences, Service awards, Committees, Data mining, Artificial intelligence]
Detecting Novel Discrepancies in Communication Networks
2010 IEEE International Conference on Data Mining
None
2010
We address the problem of detecting characteristic patterns in communication networks. We introduce a scalable approach based on set-system discrepancy. By implicitly labeling each network edge with the sequence of times in which its two endpoints communicate, we view an entire communication network as a set-system. This view allows us to use combinatorial discrepancy as a mechanism to "observe" system behavior at different time scales. We illustrate our approach, called Discrepancy-based Novelty Detector (DND), on networks obtained from emails, blue tooth connections, IP traffic, and tweets. DND has almost linear runtime complexity and linear storage complexity in the number of communications. Examples of novel discrepancies that it detects are (i) asynchronous communications and (ii) disagreements in the firing rates of nodes and edges relative to the communication network as a whole.
[Measurement, Image edge detection, discrepancy based novelty detector, communication networks, network theory (graphs), set system discrepancy, linear runtime complexity, Electronic mail, Complexity theory, set theory, linear storage complexity, set-system discrepancy, telecommunication networks, network edge, Acceleration, Communication networks, IP networks, Novelty detection, computational complexity]
Spatiotemporal Event Detection in Mobility Network
2010 IEEE International Conference on Data Mining
None
2010
Learning and identifying events in network traffic is crucial for service providers to improve their mobility network performance. In fact, large special events attract cell phone users to relative small areas, which causes sudden surge in network traffic. To handle such increased load, it is necessary to measure the increased network traffic and quantify the impact of the events, so that relevant resources can be optimized to enhance the network capability. However, this problem is challenging due to several issues: (1) Multiple periodic temporal traffic patterns (i.e., nonhomogeneous process) even for normal traffic, (2) Irregularly distributed spatial neighbor information, (3) Different temporal patterns driven by different events even for spatial neighborhoods, (4) Large scale data set. This paper proposes a systematic event detection method that deals with the above problems. With the additivity property of Poisson process, we propose an algorithm to integrate spatial information by aggregating the behavior of temporal data under various areas. Markov Modulated Nonhomogeneous Poisson Process (MMNHPP) is employed to estimate the probability with which event happens, when and where the events take place, and assess the spatial and temporal impacts of the events. Localized events are then ranked globally for prioritizing more significant events. Synthetic data are generated to illustrate our procedure and validate the performance. An industrial example from a telecommunication company is also presented to show the effectiveness of the proposed method.
[irregularly distributed spatial neighbor information, Event detection, Time series analysis, mobility management (mobile radio), Spatiotemporal, multiple periodic temporal traffic patterns, Network Traffic, Wireless communication, network traffic, Event Detection, Markov Modulated Nonhomogeneous Poisson Process, optimisation, telecommunication company, Hidden Markov models, cell phone users, Markov processes, Data models, probability estimate, Spatiotemporal phenomena, spatiotemporal event detection, mobility network performance, telecommunication traffic, Markov modulated nonhomogeneous Poisson process]
An Unsupervised Approach to Modeling Personalized Contexts of Mobile Users
2010 IEEE International Conference on Data Mining
None
2010
Mobile context modeling is a process of recognizing and reasoning about contexts and situations in a mobile environment, which is critical for the success of context-aware mobile services. While there are prior work on mobile context modeling, the use of unsupervised learning techniques for mobile context modeling is still under-explored. Indeed, unsupervised techniques have the ability to learn personalized contexts which are difficult to be predefined. To that end, in this paper, we propose an unsupervised approach to modeling personalized contexts of mobile users. Along this line, we first segment the raw context data sequences of mobile users into context sessions where a context session contains a group of adjacent context records which are mutually similar and usually reflect the similar contexts. Then, we exploit topic models to learn personalized contexts in the form of probabilistic distributions of raw context data from the context sessions. Finally, experimental results on real-world data show that the proposed approach is efficient and effective for mining personalized contexts of mobile users.
[Context, personalized context mining, Adaptation model, data mining, mobile environment, Mobile communication, Entropy, personalized context modeling, context record, mobile context modeling, personal information systems, unsupervised learning, Image segmentation, Analytical models, mobile computing, mobile user, raw context data sequence, unsupervised approach, unsupervised learning technique, probabilistic distribution, Context modeling]
Fast and Flexible Multivariate Time Series Subsequence Search
2010 IEEE International Conference on Data Mining
None
2010
Multivariate Time-Series (MTS) are ubiquitous, and are generated in areas as disparate as sensor recordings in aerospace systems, music and video streams, medical monitoring, and financial systems. Domain experts are often interested in searching for interesting multivariate patterns from these MTS databases which can contain up to several gigabytes of data. Surprisingly, research on MTS search is very limited. Most existing work only supports queries with the same length of data, or queries on a fixed set of variables. In this paper, we propose an efficient and flexible subsequence search framework for massive MTS databases, that, for the first time, enables querying on any subset of variables with arbitrary time delays between them. We propose two provably correct algorithms to solve this problem - (1) an R*-tree Based Search (RBS) which uses Minimum Bounding Rectangles (MBR) to organize the subsequences, and (2) a List Based Search (LBS) algorithm which uses sorted lists for indexing. We demonstrate the performance of these algorithms using two large MTS databases from the aviation domain, each containing several millions of observations. Both these tests show that our algorithms have very high prune rates (&gt;;95%) thus needing actual disk access for only less than 5% of the observations. To the best of our knowledge, this is the first flexible MTS search algorithm capable of subsequence search on any subset of variables. Moreover, MTS subsequence search has never been attempted on datasets of the size we have used in this paper.
[provably correct algorithm, similarity search, Delay effects, Time series analysis, Discrete Fourier transforms, flexible multivariate time series subsequence search, data mining, Search problems, time series, R-tree based search, flexible subsequence search framework, tree searching, ubiquitous computing, multivariate analysis, query processing, multivariate patterns, massive MTS databases, minimum bounding rectangle, list based search, Indexing]
iSAX 2.0: Indexing and Mining One Billion Time Series
2010 IEEE International Conference on Data Mining
None
2010
There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to index and mine very large collections of time series. Examples of such applications come from astronomy, biology, the web, and other domains. It is not unusual for these applications to involve numbers of time series in the order of hundreds of millions to billions. However, all relevant techniques that have been proposed in the literature so far have not considered any data collections much larger than one-million time series. In this paper, we describe iSAX 2.0, a data structure designed for indexing and mining truly massive collections of time series. We show that the main bottleneck in mining such massive datasets is the time taken to build the index, and we thus introduce a novel bulk loading mechanism, the first of this kind specifically tailored to a time series index. We show how our method allows mining on datasets that would otherwise be completely untenable, including the first published experiments to index one billion time series, and experiments in mining massive data from domains as diverse as entomology, DNA and web-scale image collections.
[Algorithm design and analysis, indexing, indexable symbolic aggregate approximation, Time series analysis, data mining, iSAX 2.0, data structure, time series, Data mining, representations, data collection, Loading, Memory management, Indexing]
Abstraction Augmented Markov Models
2010 IEEE International Conference on Data Mining
None
2010
High accuracy sequence classification often requires the use of higher order Markov models (MMs). However, the number of MM parameters increases exponentially with the range of direct dependencies between sequence elements, thereby increasing the risk of over fitting when the data set is limited in size. We present abstraction augmented Markov models (AAMMs) that effectively reduce the number of numeric parameters of kth order MMs by successively grouping strings of length k (i.e., k-grams) into abstraction hierarchies. We evaluate AAMMs on three protein sub cellular localization prediction tasks. The results of our experiments show that abstraction makes it possible to construct predictive models that use significantly smaller number of features (by one to three orders of magnitude) as compared to MMs. AAMMs are competitive with and, in some cases, significantly outperform MMs. Moreover, the results show that AAMMs often perform significantly better than variable order Markov models, such as decomposed context tree weighting, prediction by partial match, and probabilistic suffix trees.
[Context, Gold, pattern classification, abstraction hierarchy, cellular biophysics, predictive model, sequence classification, abstraction augmented Markov model, abstraction, prediction theory, MM parameter, decomposed context tree weighting, Training, Proteins, probabilistic suffix tree, protein subcellular localization prediction task, feature extraction, Clustering algorithms, proteins, bioinformatics, Markov processes, Data models, Markov models, numeric parameter]
A Graph-Based Approach for Multi-folder Email Classification
2010 IEEE International Conference on Data Mining
None
2010
This paper presents a novel framework for multi-folder email classification using graph mining as the underlying technique. Although several techniques exist (e.g., SVM, TF-IDF, n-gram) for addressing this problem in a delimited context, they heavily rely on extracting high-frequency keywords, thus ignoring the inherent structural aspects of an email (or document in general) which can play a critical role in classification. Some of the models (e.g., n-gram) consider only the words without taking into consideration where in the structure these words appear together. This paper presents a supervised learning model that leverages graph mining techniques for multi-folder email classification. A ranking formula is presented for ordering the representative - common and recurring - substructures generated from pre-classified emails. These ranked representative substructures are then used for categorizing incoming emails. This approach is based on a global ranking model that incorporates several relevant parameters for email classification and overcomes numerous problems faced by extant approaches used for multi-folder classification. A number of parameters which influence the generation of representative substructures are analyzed, reexamined, and adapted to multiple folders. The effect of graph representations has been analyzed. The effectiveness of the proposed approach has been validated experimentally.
[document handling, pattern classification, data mining, electronic mail, keyword extraction, supervised learning, ranking formula, Electronic mail, Classification algorithms, Data mining, Equations, Training, graph mining, Accuracy, graphs, Feature extraction, data structures, graph representation, learning (artificial intelligence), graph based approach, multifolder e-mail classification, representative data substructure]
Scalable Influence Maximization in Social Networks under the Linear Threshold Model
2010 IEEE International Conference on Data Mining
None
2010
Influence maximization is the problem of finding a small set of most influential nodes in a social network so that their aggregated influence in the network is maximized. In this paper, we study influence maximization in the linear threshold model, one of the important models formalizing the behavior of influence propagation in social networks. We first show that computing exact influence in general networks in the linear threshold model is #P-hard, which closes an open problem left in the seminal work on influence maximization by Kempe, Kleinberg, and Tardos, 2003. As a contrast, we show that computing influence in directed a cyclic graphs (DAGs) can be done in time linear to the size of the graphs. Based on the fast computation in DAGs, we propose the first scalable influence maximization algorithm tailored for the linear threshold model. We conduct extensive simulations to show that our algorithm is scalable to networks with millions of nodes and edges, is orders of magnitude faster than the greedy approximation algorithm proposed by Kempe et al. and its optimized versions, and performs consistently among the best algorithms while other heuristic algorithms not design specifically for the linear threshold model have unstable performances on different real-world networks.
[Greedy algorithms, Algorithm design and analysis, approximation theory, #P-hard, Computational modeling, Social network services, Heuristic algorithms, greedy algorithms, social networks, scalable influence maximization algorithm, greedy approximation algorithm, optimisation, directed graphs, social networking (online), influence maximization, linear threshold model, directed acyclic graphs, Integrated circuit modeling]
CLUSMASTER: A Clustering Approach for Sampling Data Streams in Sensor Networks
2010 IEEE International Conference on Data Mining
None
2010
The growing usage of embedded devices and sensors in our daily lives has been profoundly reshaping the way we interact with our environment and our peers. As more and more sensors will pervade our future cities, increasingly efficient infrastructures to collect, process, and store massive amounts of data streams from a wide variety of sources will be required. Despite the different application-specific features and hardware platforms, sensor network applications share a common goal: periodically sample and store data collected from different sensors in a common persistent memory. In this article we present a clustering approach for rapidly and efficiently computing the best sampling rate which minimizes the SSE (Sum of Square Errors) for each particular sensor in a network. In order to evaluate the efficiency of the proposed approach, we carried out experiments on real electric power consumption data streams produced by a 1-thousand sensor network provided by the French energy group-EDF (Electricite de France).
[sampling methods, data streams, sampling, sensor fusion, sampling rate, Telecommunications, Servers, data stream, Optimization, Aggregates, pattern clustering, sensor network, Distributed databases, Clustering algorithms, Bandwidth, data acquisition, clustering, CLUSMASTER]
Bayesian Maximum Margin Clustering
2010 IEEE International Conference on Data Mining
None
2010
Most well-known discriminative clustering models, such as spectral clustering (SC) and maximum margin clustering (MMC), are non-Bayesian. Moreover, they merely considered to embed domain-dependent prior knowledge into data-specific kernels, while other forms of prior knowledge were seldom considered in these models. In this paper, we propose a Bayesian maximum margin clustering model (BMMC) based on the low-density separation assumption, which unifies the merits of both Bayesian and discriminative approaches. In addition to stating prior distribution on functions explicitly as traditional Gaussian processes, special prior knowledge can be embedded into BMMC implicitly via the Universum set easily. Furthermore, it is much easier to solve a BMMC than an MMC since the integer variables in the optimization are eliminated. Experimental results show that the BMMC achieves comparable or even better performance than state-of-the-art clustering methods and solving BMMC is more efficiently.
[prior distribution, Bayesian maximum margin clustering model, Noise, data mining, Optimization, domain dependent prior knowledge, Universum, optimisation, optimization, Clustering algorithms, integer variable, Kernel, spectral clustering, nonBayesian clustering, data specific kernel, low density separation assumption, Bayesian, Maximum Margin Principle, Bayesian maximum margin clustering, Clustering, discriminative approach, Support vector machines, Gaussian process, Bayesian methods, pattern clustering, Gaussian processes, Data models, Bayes methods, discriminative clustering model, Universum set, BMMC]
Viral Marketing for Multiple Products
2010 IEEE International Conference on Data Mining
None
2010
Viral Marketing, the idea of exploiting social interactions of users to propagate awareness for products, has gained considerable focus in recent years. One of the key issues in this area is to select the best seeds that maximize the influence propagated in the social network. In this paper, we define the seed selection problem (called t-Influence Maximization, or t-IM) for multiple products. Specifically, given the social network and t products along with their seed requirements, we want to select seeds for each product that maximize the overall influence. As the seeds are typically sent promotional messages, to avoid spamming users, we put a hard constraint on the number of products for which any single user can be selected as a seed. In this paper, we design two efficient techniques for the t-IM problem, called Greedy and FairGreedy. The Greedy algorithm uses simple greedy hill climbing, but still results in a 1/3-approximation to the optimum. Our second technique, FairGreedy, allocates seeds with not only high overall influence (close to Greedy in practice), but also ensures fairness across the influence of different products. We also design efficient heuristics for estimating the influence of the selected seeds, that are crucial for running the seed selection on large social network graphs. Finally, using extensive simulations on real-life social graphs, we show the effectiveness and scalability of our techniques compared to existing and naive strategies.
[Greedy algorithms, Algorithm design and analysis, Social network services, greedy algorithms, social networks, greedy hill climbing, Approximation methods, Optimization, social network, marketing, optimisation, multiple product, viral marketing, t influence maximization, influence propagation, social networking (online), Silicon, seed selection problem, Mathematical model, greedy algorithm]
Finding Local Anomalies in Very High Dimensional Space
2010 IEEE International Conference on Data Mining
None
2010
Time, cost and energy efficiency are critical factors for many data analysis techniques when the size and dimensionality of data is very large. We investigate the use of Local Outlier Factor (LOF) for data of this type, providing a motivating example from real world data. We propose Projection-Indexed Nearest-Neighbours (PINN), a novel technique that exploits extended nearest neighbour sets in the a reduced dimensional space to create an accurate approximation for k-nearest-neighbour distances, which is used as the core density measurement within LOF. The reduced dimensionality allows for efficient sub-quadratic indexing in the number of items in the data set, where previously only quadratic performance was possible. A detailed theoretical analysis of Random Projection(RP) and PINN shows that we are able to preserve the density of the intrinsic manifold of the data set after projection. Experimental results show that PINN outperforms the standard projection methods RP and PCA when measuring LOF for many high-dimensional real-world data sets of up to 300000 elements and 102600 dimensions.
[local outlier factor, approximation theory, approximation, Density measurement, data analysis, Scalability, data mining, random processes, anomaly detection, set theory, k-nearest neighbour, Object recognition, projection indexed nearest-neighbour, dimensionality reduction, Lighting, Euclidean distance, random projection, Principal component analysis, Indexing, subquadratic indexing]
PGLCM: Efficient Parallel Mining of Closed Frequent Gradual Itemsets
2010 IEEE International Conference on Data Mining
None
2010
Numerical data (e.g., DNA micro-array data, sensor data) pose a challenging problem to existing frequent pattern mining methods which hardly handle them. In this framework, gradual patterns have been recently proposed to extract covariations of attributes, such as: "When X increases, Y decreases". There exist some algorithms for mining frequent gradual patterns, but they cannot scale to real-world databases. We present in this paper GLCM, the first algorithm for mining closed frequent gradual patterns, which proposes strong complexity guarantees: the mining time is linear with the number of closed frequent gradual item sets. Our experimental study shows that GLCM is two orders of magnitude faster than the state of the art, with a constant low memory usage. We also present PGLCM, a parallelization of GLCM capable of exploiting multicore processors, with good scale-up properties on complex datasets. These algorithms are the first algorithms capable of mining large real world datasets to discover gradual patterns.
[multiprocessing systems, Multicore processing, pattern mining, data mining, parallelism, Encoding, Complexity theory, set theory, Data mining, gradual itemsets, numerical data, Itemsets, closed frequent gradual itemset, linear mining time, multicore processor, frequent pattern mining]
Sequential Latent Dirichlet Allocation: Discover Underlying Topic Structures within a Document
2010 IEEE International Conference on Data Mining
None
2010
Understanding how topics within a document evolve over its structure is an interesting and important problem. In this paper, we address this problem by presenting a novel variant of Latent Dirichlet Allocation (LDA): Sequential LDA (SeqLDA). This variant directly considers the underlying sequential structure, i.e., a document consists of multiple segments (e.g., chapters, paragraphs), each of which is correlated to its previous and subsequent segments. In our model, a document and its segments are modelled as random mixtures of the same set of latent topics, each of which is a distribution over words; and the topic distribution of each segment depends on that of its previous segment, the one for first segment will depend on the document topic distribution. The progressive dependency is captured by using the nested two-parameter Poisson Dirichlet process (PDP). We develop an efficient collapsed Gibbs sampling algorithm to sample from the posterior of the PDP. Our experimental results on patent documents show that by taking into account the sequential structure within a document, our SeqLDA model has a higher fidelity over LDA in terms of perplexity (a standard measure of dictionary-based compressibility). The SeqLDA model also yields a nicer sequential topic structure than LDA, as we show in experiments on books such as Melville's "The Whale".
[topic structure, Correlation, sampling methods, Computational modeling, data mining, Poisson-Dirichlet process, topic distribution, Approximation methods, Gibbs sampling, sequential Latent Dirichlet allocation, word processing, Latent Dirichlet Allocation, Layout, Poisson Dirichlet process, Markov processes, Inference algorithms, document structure, Resource management, stochastic processes, collapsed Gibbs sampler]
Subgroup Discovery Meets Bayesian Networks -- An Exceptional Model Mining Approach
2010 IEEE International Conference on Data Mining
None
2010
Whenever a dataset has multiple discrete target variables, we want our algorithms to consider not only the variables themselves, but also the interdependencies between them. We propose to use these interdependencies to quantify the quality of subgroups, by integrating Bayesian networks with the Exceptional Model Mining framework. Within this framework, candidate subgroups are generated. For each candidate, we fit a Bayesian network on the target variables. Then we compare the network's structure to the structure of the Bayesian network fitted on the whole dataset. To perform this comparison, we define an edit distance-based distance metric that is appropriate for Bayesian networks. We show interesting subgroups that we experimentally found with our method on datasets from music theory, semantic scene classification, biology and zoogeography.
[Measurement, pattern classification, Correlation, data mining, Exceptional Model Mining, Subgroup Discovery, Entropy, Data mining, music theory, Bayesian methods, semantic scene classification, exceptional model mining approach, Skeleton, Data models, Bayesian networks, belief networks, Bayesian network, distance-based distance metric]
Feature Selection for Unsupervised Learning Using Random Cluster Ensembles
2010 IEEE International Conference on Data Mining
None
2010
In this paper, we propose another extension of the Random Forests paradigm to unlabeled data, leading to localized unsupervised feature selection (FS). We show that the way internal estimates are used to measure variable importance in Random Forests are also applicable to FS in unsupervised learning. We first illustrate the clustering performance of the proposed method on various data sets based on widely used external criteria of clustering quality. We then assess the accuracy and the scalability of the FS procedure on UCI and real labeled data sets and compare its effectiveness against other FS methods.
[random forest paradigm, variable importance, Indexes, Unsupervised learning, unsupervised learning, Random Forest, Radio frequency, Training, clustering quality, FS method, Accuracy, random cluster ensemble, real labeled data set, pattern clustering, feature extraction, Clustering algorithms, clustering performance, UCI, unlabeled data, Bagging, feature selection, FS procedure]
Learning Attribute-to-Feature Mappings for Cold-Start Recommendations
2010 IEEE International Conference on Data Mining
None
2010
Cold-start scenarios in recommender systems are situations in which no prior events, like ratings or clicks, are known for certain users or items. To compute predictions in such cases, additional information about users (user attributes, e.g. gender, age, geographical location, occupation) and items (item attributes, e.g. genres, product categories, keywords) must be used. We describe a method that maps such entity (e.g. user or item) attributes to the latent features of a matrix (or higher-dimensional) factorization model. With such mappings, the factors of a MF model trained by standard techniques can be applied to the new-user and the new-item problem, while retaining its advantages, in particular speed and predictive accuracy. We use the mapping concept to construct an attribute-aware matrix factorization model for item recommendation from implicit, positive-only feedback. Experiments on the new-item problem show that this approach provides good predictive accuracy, while the prediction time only grows by a constant factor.
[matrix factorization, collaborative filtering, Computational modeling, new-user problem, Predictive models, matrix decomposition, attribute-to-feature mapping, cold-start, factorization models, Training, feedback, cold-start recommendation, recommender systems, recommender system, new-item problem, Collaboration, Motion pictures, Data models, Mathematical model, learning (artificial intelligence), long tail]
An Extensive Empirical Study on Semi-supervised Learning
2010 IEEE International Conference on Data Mining
None
2010
Semi-supervised classification methods utilize unlabeled data to help learn better classifiers, when only a small amount of labeled data is available. Many semi-supervised learning methods have been proposed in the past decade. However, some questions have not been well answered, e.g., whether semi-supervised learning methods outperform base classifiers learned only from the labeled data, when different base classifiers are used, whether selecting unlabeled data with efforts is superior to random selection, and how the quality of the learned classifier changes at each iteration of learning process. This paper conducts an extensive empirical study on the performance of several commonly used semi-supervised learning methods when different Bayesian classifiers (NB, NBTree, TAN, HGC, HNB, and DNB) are used as the base classifier, respectively. Results on Transductive SVM and a graph-based semi-supervised learning method LLGC are also studied for comparison. The experimental results on 26 UCI datasets and 6 widely used benchmark datasets show that these semi-supervised learning methods generally do not obtain better performance than classifiers learned only from the labeled data. Moreover, for standard self-training and co-training, when selecting the most confident unlabeled instances during learning process, the performance is not necessarily better than that of random selection of unlabeled instances. We especially discovered interesting outcomes when drawing learning curves for using NB in self-training on some UCI datasets. The accuracy of the learned classifier on the testing set may fluctuate or decrease as more unlabeled instances are used. Also on the mushroom dataset, even when all the selected unlabeled instances are correctly labeled in each iteration, the accuracy on the testing set still goes down.
[learning process, UCI dataset, Niobium, random selection, Bayesian classifier, Training, Accuracy, graphs, Semi-supervised learning, Benchmark testing, transductive SVM, base classifier, graph based semisupervised learning method, mushroom dataset, learning (artificial intelligence), learning curve, pattern classification, unlabeled instance, Support vector machines, Bayesian methods, Supervised learning, benchmark dataset, Bayes methods, unlabeled data, Bayesian classifiers, LLGC]
Efficient Discovery of the Top-K Optimal Dependency Rules with Fisher's Exact Test of Significance
2010 IEEE International Conference on Data Mining
None
2010
Statistical dependency analysis is the basis of all empirical science. A commonly occurring problem is to find the most significant dependency rules, which describe either positive or negative dependencies between categorical attributes. For example, in medical science one is interested in genetic factors, which can either predispose or prevent diseases. The requirement of statistical significance is essential, because the discoveries should hold also in the future data. Typically, the significance is estimated either by Fisher's exact test or the &#x03C7;2-measure. The problem is computationally very difficult, because the number of all possible dependency rules increases exponentially with the number of attributes. As a solution, different kinds of restrictions and heuristics have been applied, but a general, scalable search method has been missing. In this paper, we introduce an efficient algorithm for searching for the top-K globally optimal dependency rules using Fisher's exact test as a measure function. The rules can express either positive or negative dependencies between a set of positive attributes and a single consequent attribute. The algorithm is based on an application of the branch- and-bound search strategy, supplemented by several pruning properties. Especially, we prove a new lower-bound for the Fisher's p, and introduce a new effective pruning principle. The general search algorithm is applicable to other goodness measures, like the &#x03C7;2-measure, as well. According to our experiments on classical benchmark data, the algorithm is well scalable and can efficiently handle even dense and high dimensional data sets. In addition, the quality of rules is significantly better than with the &#x03C7;2-measure using the same search algorithm.
[Correlation, statistical significance, search algorithm, optimal dependency rules, data mining, Fisher exact test, Length measurement, Search problems, rule discovery, Frequency measurement, set theory, statistical dependency analysis, negative rule, positive attributes, branch-and-bound search, pruning principle, dependency rule, data analysis, Redundancy, tree searching, Diseases, Measurement uncertainty, consequent attribute, statistical analysis, Fisher's exact test]
A Variance Reduction Framework for Stable Feature Selection
2010 IEEE International Conference on Data Mining
None
2010
Besides high accuracy, stability of feature selection has recently attracted strong interest in knowledge discovery from high-dimensional data. In this study, we present a theoretical framework about the relationship between the stability and accuracy of feature selection based on a formal bias-variance decomposition of feature selection error. The framework also suggests a variance reduction approach for improving the stability of feature selection algorithms. Furthermore, we propose an empirical variance reduction framework, margin based instance weighting, which weights training instances according to their influence to the estimation of feature relevance. We also develop an efficient algorithm under this framework. Experiments based on synthetic data and real-world micro array data verify both the theoretical framework and the effectiveness of the proposed algorithm on variance reduction. The proposed algorithm is also shown to be effective at improving subset stability, while maintaining comparable classification accuracy based on selected features.
[formal bias variance decomposition, subset stability, bias-variance decomposition, data mining, lab-on-a-chip, feature selection error, high-dimensional data, variance reduction framework, knowledge discovery, classification accuracy, Training, Accuracy, Monte Carlo methods, high dimensional data, variance reduction, feature extraction, Training data, synthetic data, Prediction algorithms, stability, feature selection, stable feature selection, margin based instance weighting, real world microarray data, Stability criteria]
Exponential Family Tensor Factorization for Missing-Values Prediction and Anomaly Detection
2010 IEEE International Conference on Data Mining
None
2010
In this paper, we study probabilistic modeling of heterogeneously attributed multi-dimensional arrays. The model can manage the heterogeneity by employing an individual exponential-family distribution for each attribute of the tensor array. These entries are connected by latent variables and are shared information across the different attributes. Because a Bayesian inference for our model is intractable, we cast the EM algorithm approximated by using the Lap lace method and Gaussian process. This approximation enables us to derive a predictive distribution for missing values in a consistent manner. Simulation experiments show that our method outperforms other methods such as PARAFAC and Tucker decomposition in missing-values prediction for cross-national statistics and is also applicable to discover anomalies in heterogeneous office-logging data.
[data fusion, office logging data, Bayesian probabilistic model, sensor fusion, anomaly detection, matrix decomposition, probabilistic modeling, Approximation methods, exponential family tensor factorization, belief networks, Bayesian inference, Laplace equations, Probabilistic logic, cross national statistic, inference mechanisms, prediction theory, Laplace method, Tensile stress, Gaussian process, Bayesian methods, Gaussian processes, multidimensional array, expectation-maximisation algorithm, missing values prediction, Data models, Inference algorithms, Bayes methods, Arrays, tensor factorization]
Rare Category Characterization
2010 IEEE International Conference on Data Mining
None
2010
Rare categories abound and their characterization has heretofore received little attention. Fraudulent banking transactions, network intrusions, and rare diseases are examples of rare classes whose detection and characterization are of high value. However, accurate characterization is challenging due to high-skewness and non-separability from majority classes, e.g., fraudulent transactions masquerade as legitimate ones. This paper proposes the RACH algorithm by exploring the compactness property of the rare categories. It is based on an optimization framework which encloses the rare examples by a minimum-radius hyper ball. The framework is then converted into a convex optimization problem, which is in turn effectively solved in its dual form by the projected sub gradient method. RACH can be naturally kernelized. Experimental results validate the effectiveness of RACH.
[Conferences, fraudulent banking transactions, subgradient, compactness, hyperball, subgradient method, Classification algorithms, Approximation methods, rare category characterization, Optimization, Diseases, Support vector machines, Upper bound, optimisation, optimization, rare category, minimum-radius hyperball, network intrusions, data handling, RACH algorithm, minority class, characterization, convex optimization problem]
Algorithm for Discovering Low-Variance 3-Clusters from Real-Valued Datasets
2010 IEEE International Conference on Data Mining
None
2010
The concept of Triclusters has been investigated recently in the context of two relational datasets that share labels along one of the dimensions. By simultaneously processing two datasets to unveil triclusters, new useful knowledge and insights can be obtained. However, some recently reported methods are either closely linked to specific problems or constrain datasets to have some specific distributions. Algorithms for generating triclusters whose cell-values demonstrate simple well known statistical properties, such as upper bounds on standard deviations, are needed for many applications. In this paper we present a 3-Clustering algorithm that searches for meaningful combinations of biclusters in two related datasets. The algorithm can handle situations involving: (i) datasets in which a few data objects may be present in only one dataset and not in both datasets, (ii) the two datasets may have different numbers of objects and/or attributes, and (iii) the cell-value distributions in two datasets may be different. In our formulation the cell-values of each selected tricluster, formed by two independent biclusters, are such that the standard deviations in each bicluster obeys an upper bound and the sets of objects in the two biclusters overlap to the maximum possible extent. We present validation of our algorithm by presenting the properties of the 3-Clusters discovered from a synthetic dataset and from a real world cross-species genomic dataset. The results of our algorithm unveil interesting insights for the cross-species genomic domain.
[Context, statistical property, Lattices, Genomics, data mining, triclusters, Data mining, Upper bound, relational datasets, Triclusters, pattern clustering, real valued dataset, cell-value distributions, low variance cluster, statistical analysis, Bioinformatics, search problems, Co-clustering, Bars, standard deviation]
Improved Consistent Sampling, Weighted Minhash and L1 Sketching
2010 IEEE International Conference on Data Mining
None
2010
We propose a new Consistent Weighted Sampling method, where the probability of drawing identical samples for a pair of inputs is equal to their Jaccard similarity. Our method takes deterministic constant time per non-zero weight, improving on the best previous approach which takes expected constant time. The samples can be used as Weighted Minhash for efficient retrieval and compression (sketching) under Jaccard or L1 metric. A method is presented for using simple data statistics to reduce the running time of hash computation by two orders of magnitude. We compare our method with the random projection method and show that it has better characteristics for retrieval under L1. We present a novel method of mapping hashes to short bit-strings, apply it to Weighted Minhash, and achieve more accurate distance estimates from sketches than existing methods, as long as the inputs are sufficiently distinct. We show how to choose the optimal number of bits per hash for sketching, and demonstrate experimental results which agree with the theoretical analysis.
[Measurement, Retrieval, pattern matching, Compression, L1 sketching, Diamond-like carbon, Approximation methods, Histograms, sample retrieval, weighted Minhash, bit string, Jaccard similarity, deterministic constant time, Sampling, Kernel, running time reduction, data compression, sample compression, sampling methods, information retrieval, cryptography, hash computation, Minhash, Indexes, consistent weighted sampling method, Sketching, random projection method, Hashing, file organisation, data statistics, Random variables]
An Approach Based on Tree Kernels for Opinion Mining of Online Product Reviews
2010 IEEE International Conference on Data Mining
None
2010
Opinion mining is a challenging task to identify the opinions or sentiments underlying user generated contents, such as online product reviews, blogs, discussion forums, etc. Previous studies that adopt machine learning algorithms mainly focus on designing effective features for this complex task. This paper presents our approach based on tree kernels for opinion mining of online product reviews. Tree kernels alleviate the complexity of feature selection and generate effective features to satisfy the special requirements in opinion mining. In this paper, we define several tree kernels for sentiment expression extraction and sentiment classification, which are subtasks of opinion mining. Our proposed tree kernels encode not only syntactic structure information, but also sentiment related information, such as sentiment boundary and sentiment polarity, which are important features to opinion mining. Experimental results on a benchmark data set indicate that tree kernels can significantly improve the performance of both sentiment expression extraction and sentiment classification. Besides, a linear combination of our proposed tree kernels and traditional feature vector kernel achieves the best performances using the benchmark data set.
[opinion mining, text analysis, data mining, blogs, Classification algorithms, syntactic structure information, feature vector kernel, feature extraction, tree kernels, text mining, Kernel, feature selection, Classification tree analysis, Special issues and sections, natural language processing, sentiment classification, sentiment analysis, trees (mathematics), classification, machine learning, online product reviews, Feature extraction, Cameras, behavioural sciences computing, Lenses, sentiment expression extraction]
A Pairwise-Systematic Microaggregation for Statistical Disclosure Control
2010 IEEE International Conference on Data Mining
None
2010
Microdata protection in statistical databases has recently become a major societal concern and has been intensively studied in recent years. Statistical Disclosure Control (SDC) is often applied to statistical databases before they are released for public use. Micro aggregation for SDC is a family of methods to protect micro data from individual identification. SDC seeks to protect micro data in such a way that can be published and mined without providing any private information that can be linked to specific individuals. Micro aggregation works by partitioning the micro data into groups of at least k records and then replacing the records in each group with the centroid of the group. An optimal micro aggregation method must minimize the information loss resulting from this replacement process. The challenge is how to minimize the information loss during the micro aggregation process. This paper presents a pair wise systematic (P-S) micro aggregation method to minimize the information loss. The proposed technique simultaneously forms two distant groups at a time with the corresponding similar records together in a systematic way and then anonymized with the centroid of each group individually. The structure of P-S problem is defined and investigated and an algorithm of the proposed problem is developed. The performance of the P-S algorithm is compared against the most recent micro aggregation methods. Experimental results show that P-S algorithm incurs less than half information loss than the latest micro aggregation methods for all of the test situations.
[microdata protection, ?-anonymity, data mining, pairwise systematic microaggregation, information loss minimization, Partitioning algorithms, set theory, statistical disclosure control, Equations, Sorting, Privacy, Systematics, security of data, Microaggregation, Clustering algorithms, Euclidean distance, data privacy, statistical databases, Disclosure control, Microdata protection, k-anonymity]
Multi-label Feature Selection for Graph Classification
2010 IEEE International Conference on Data Mining
None
2010
Nowadays, the classification of graph data has become an important and active research topic in the last decade, which has a wide variety of real world applications, e.g. drug activity predictions and kinase inhibitor discovery. Current research on graph classification focuses on single-label settings. However, in many applications, each graph data can be assigned with a set of multiple labels simultaneously. Extracting good features using multiple labels of the graphs becomes an important step before graph classification. In this paper, we study the problem of multi-label feature selection for graph classification and propose a novel solution, called gMLC, to efficiently search for optimal sub graph features for graph objects with multiple labels. Different from existing feature selection methods in vector spaces which assume the feature set is given, we perform multi-label feature selection for graph data in a progressive way together with the sub graph feature mining process. We derive an evaluation criterion, named gHSIC, to estimate the dependence between sub graph features and multiple labels of graphs. Then a branch-and-bound algorithm is proposed to efficiently search for optimal sub graph features by judiciously pruning the sub graph search space using multiple labels. Empirical studies on real-world tasks demonstrate that our feature selection approach can effectively boost multi-label graph classification performances and is more efficient by pruning the sub graph search space using multiple labels.
[Context, Drugs, pattern classification, multilabel feature selection, graph theory, data mining, graph object, Data mining, tree searching, Optimization, Upper bound, feature extraction, graph classification, branch-and-bound algorithm, Feature extraction, vector space, search space, Kernel, feature mining, feature selection, multi-label learning]
A Binary Decision Diagram-Based One-Class Classifier
2010 IEEE International Conference on Data Mining
None
2010
We propose a novel approach for one-class classification problems where a logical formula is used to estimate the region that covers all examples. A formula is viewed as a model that represents a region and is approximated with respect to its hierarchical local densities. The approximation is done quite efficiently via direct manipulations of a binary decision diagram that is a compressed representation of a Boolean formula. The proposed method has only one parameter to be tuned, and the parameter can be selected properly with the help of the minimum description length principle, which requires no labeled training data. In other words, a one-class classifier is generated from an unlabeled training data thoroughly and automatically. Experimental results show that the proposed method works quite well with synthetic data and some realistic data.
[approximation theory, pattern classification, one class classifier, unlabeled training data, Data structures, parameter tuned, Approximation methods, binary decision diagram, unsupervised learning, Training, binary decision diagrams, Boolean functions, approximation method, Boolean formula, minimum description length principle, one-class classification, Approximation algorithms, Hypercubes, Mathematical model]
Detecting Blackhole and Volcano Patterns in Directed Networks
2010 IEEE International Conference on Data Mining
None
2010
In this paper, we formulate a novel problem for finding black hole and volcano patterns in a large directed graph. Specifically, a black hole pattern is a group which is made of a set of nodes in a way such that there are only in links to this group from the rest nodes in the graph. In contrast, a volcano pattern is a group which only has out links to the rest nodes in the graph. Both patterns can be observed in real world. For instance, in a trading network, a black hole pattern may represent a group of traders who are manipulating the market. In the paper, we first prove that the black hole mining problem is a dual problem of finding volcanoes. Therefore, we focus on finding the black hole patterns. Along this line, we design two pruning schemes to guide the black hole finding process. In the first pruning scheme, we strategically prune the search space based on a set of pattern-size-independent pruning rules and develop an iBlack hole algorithm. The second pruning scheme follows a divide-and-conquer strategy to further exploit the pruning results from the first pruning scheme. Indeed, a target directed graphs can be divided into several disconnected sub graphs by the first pruning scheme, and thus the black hole finding can be conducted in each disconnected sub graph rather than in a large graph. Based on these two pruning schemes, we also develop an iBlackhole-DC algorithm. Finally, experimental results on real-world data show that the iBlackhole-DC algorithm can be several orders of magnitude faster than the iBlackhole algorithm, which has a huge computational advantage over a brute-force method.
[Algorithm design and analysis, divide and conquer methods, Conferences, blackhole mining, data mining, volcano pattern, trading network, network model, directed graph, financial data processing, search space, stock markets, Business, Economics, pruning scheme, Image edge detection, blackhole pattern, directed network, iBlackhole algorithm, divide-and-conquer strategy, graph mining, directed graphs, Collaboration, fraud, fraud detection, Volcanoes]
Exploiting Local Data Uncertainty to Boost Global Outlier Detection
2010 IEEE International Conference on Data Mining
None
2010
This paper presents a novel hybrid approach to outlier detection by incorporating local data uncertainty into the construction of a global classifier. To deal with local data uncertainty, we introduce a confidence value to each data example in the training data, which measures the strength of the corresponding class label. Our proposed method works in two steps. Firstly, we generate a pseudo training dataset by computing a confidence value of each input example on its class label. We present two different mechanisms: kernel k-means clustering algorithm and kernel LOF-based algorithm, to compute the confidence values based on the local data behavior. Secondly, we construct a global classifier for outlier detection by generalizing the SVDD-based learning framework to incorporate both positive and negative examples as well as their associated confidence values. By integrating local and global outlier detection, our proposed method explicitly handles the uncertainty of the input data and enhances the ability of SVDD in reducing the sensitivity to noise. Extensive experiments on real life datasets demonstrate that our proposed method can achieve a better tradeoff between detection rate and false alarm rate as compared to four state-of-the-art outlier detection algorithms.
[Uncertainty, support vector machines, kernel LOF based algorithm, SVDD, probability, Data uncertainty, Predictive models, uncertainty handling, Outlier detection, data description, local data uncertainty, pseudo training dataset, kernel k- means clustering, Training, Support vector machines, support vector data description, pattern clustering, Training data, boost global outlier detection, SVDD based learning, Data models, learning (artificial intelligence), Kernel]
Training Conditional Random Fields Using Transfer Learning for Gesture Recognition
2010 IEEE International Conference on Data Mining
None
2010
Recently, combining Conditional Random Fields (CRF) with Neural Network has shown the success of learning high-level features in sequence labeling tasks. However, such models are difficult to train because of the increase of the parameters to tune which needs enormous of labeled data to avoid over fitting. In this paper, we propose a transfer learning framework for the sequence labeling task of gesture recognition. Taking advantage of the frame correlation, we design an unsupervised sequence model as a pseudo auxiliary task to capture the underlying information from both the labeled and unlabeled data. The knowledge learnt by the auxiliary task can be transferred to the main task of CRF with a deep architecture by sharing the hidden layers, which is very helpful for learning meaningful representation and reducing the need of labeled data. We evaluate our model under 3 gesture recognition datasets. The experimental results of both supervised learning and semi-supervised learning show that the proposed model improves the performance of the CRF with Neural Network and other baseline models.
[transfer learning, Transfer Learning, semisupervised learning, Conditional Random Fields, Artificial neural networks, Gesture recognition, neural network, unsupervised learning, Training, Learning systems, conditional random field training, Gesture Recognition, Semi-supervised Learning, gesture recognition, Feature extraction, Data models, unsupervised sequence model, Joints, frame correlation, sequence labeling tasks]
Stratified Sampling for Data Mining on the Deep Web
2010 IEEE International Conference on Data Mining
None
2010
In recent years, one mode of data dissemination has become extremely popular, which is the deep web. Like any other data source, data mining on the deep web can produce important insights or summary of results. However, data mining on the deep web is challenging because the databases cannot be accessed directly, and therefore, data mining must be performed based on sampling of the datasets. The samples, in turn, can only be obtained by querying the deep web databases with specific inputs. In this paper, we target two related data mining problems, which are association mining and differential rule mining. We develop stratified sampling methods to perform these mining tasks on a deep web source. Our contributions include a novel greedy stratification approach, which processes the query space of a deep web data source recursively, and considers both the estimation error and the sampling costs. We have also developed an optimized sample allocation method that integrates estimation error and sampling costs. Our experiment results show that our algorithms effectively and consistently reduce sampling costs, compared with a stratified sampling method that only considers estimation error. In addition, compared with simple random sampling, our algorithm has higher sampling accuracy and lower sampling costs.
[differential rule mining, sampling methods, Estimation, data mining, sample allocation method, sampling costs, data dissemination, Data Mining, Web database, Stratified Sampling, Association rules, association mining, query processing, query space, Accuracy, optimisation, Itemsets, distributed databases, Deep Web, Internet, Resource management, stratified sampling methods, estimation error, deep Web]
Learning Markov Network Structure with Decision Trees
2010 IEEE International Conference on Data Mining
None
2010
Traditional Markov network structure learning algorithms perform a search for globally useful features. However, these algorithms are often slow and prone to finding local optima due to the large space of possible structures. Ravikumar et al. recently proposed the alternative idea of applying L1 logistic regression to learn a set of pair wise features for each variable, which are then combined into a global model. This paper presents the DTSL algorithm, which uses probabilistic decision trees as the local model. Our approach has two significant advantages: it is more efficient, and it is able to discover features that capture more complex interactions among the variables. Our approach can also be seen as a method for converting a dependency network into a consistent probabilistic model. In an extensive empirical evaluation on 13 datasets, our algorithm obtains comparable accuracy to three standard structure learning algorithms while running 1-4 orders of magnitude faster.
[probability, structure learning algorithm, probabilistic methods, Probabilistic logic, Markov networks, probabilistic decision tree, structure learning, Markov random fields, Training data, decision trees, Markov processes, Markov network structure, Decision trees, Books, learning (artificial intelligence), logistic regression, DTSL algorithm, decision tree structure learner, Logistics]
Towards Structural Sparsity: An Explicit l2/l0 Approach
2010 IEEE International Conference on Data Mining
None
2010
In many cases of machine learning or data mining applications, we are not only aimed to establish accurate black box predictors, we are also interested in discovering predictive patterns in data which enhance our interpretation and understanding of underlying physical, biological and other natural processes. Sparse representation is one of the focuses in this direction. More recently, structural sparsity has attracted increasing attentions. The structural sparsity is often achieved by imposing &#x2113;<sub>2</sub>/&#x2113;<sub>1</sub> norms. In this paper, we present the explicit &#x2113;<sub>2</sub>/&#x2113;<sub>0</sub> norm to directly achieve structural sparsity. To tackle the problem of intractable &#x2113;<sub>2</sub>/&#x2113;<sub>0</sub> optimization, we develop a general Lipschitz auxiliary function which leads to simple iterative algorithms. In each iteration, optimal solution is achieved for the induced sub-problem and a guarantee of convergence is provided. Further more, the local convergent rate is also theoretically bounded. We test our optimization techniques in the multi-task feature learning problem. Experimental results suggest that our approaches outperform other approaches in both synthetic and real world data sets.
[Algorithm design and analysis, iterative methods, convergent rate, optimization technique, Adaptation model, data mining, Data mining, Optimization, Non-smooth optimization, optimisation, structural sparsity, predictive data pattern, multitask feature learning, learning (artificial intelligence), Kernel, sparse representation, natural process, Matching pursuit algorithms, convergence of numerical methods, machine learning, iterative algorithm, physical process, Lipschitz auxiliary function, black box predictor, Data models, biological process, ?2/?0-norm, sparse matrices, explicit l<sub>2</sub>/l<sub>0</sub> approach, Structural sparsity]
Multi-document Summarization Using Minimum Distortion
2010 IEEE International Conference on Data Mining
None
2010
Document summarization plays an important role in the area of natural language processing and text mining. This paper proposes several novel information-theoretic models for multi-document summarization. They consider document summarization as a transmission system and assume that the best summary should have the minimum distortion. By defining a proper distortion measure and a new representation method, the combination of the last two models (the linear representation model and the facility location model) gains good experimental results on the DUC2002 and DUC2004 datasets. Moreover, we also indicate that the model has high interpretability and extensibility.
[text analysis, Smoothing methods, information-theoretic model, multi-document summarization, natural language processing, linear representation, Redundancy, document summarization, data mining, information-theoretic summarization, Data mining, minimum distortion, Optimization, J-S Divergence, knowledge representation, Hidden Markov models, Feature extraction, text mining, Distortion measurement]
A Log-Linear Model with Latent Features for Dyadic Prediction
2010 IEEE International Conference on Data Mining
None
2010
In dyadic prediction, labels must be predicted for pairs (dyads) whose members possess unique identifiers and, sometimes, additional features called side-information. Special cases of this problem include collaborative filtering and link prediction. We present a new log-linear model for dyadic prediction that is the first to satisfy several important desiderata: (i) labels may be ordinal or nominal, (ii) side-information can be easily exploited if present, (iii) with or without side-information, latent features are inferred for dyad members, (iv) the model is resistant to sample-selection bias, (v) it can learn well-calibrated probabilities, and (vi) it can scale to large datasets. To our knowledge, no existing method satisfies all the above criteria. In particular, many methods assume that the labels are binary or numerical, and cannot use side-information. Experimental results show that the new method is competitive with previous specialized methods for collaborative filtering and link prediction. Other experimental results demonstrate that the new method succeeds for dyadic prediction tasks where previous methods cannot be used. In particular, the new method predicts nominal labels accurately, and by using side-information it solves the cold-start problem in collaborative filtering.
[collaborative filtering, probability, Predictive models, Dyadic prediction, information filtering, Approximation methods, Training, log-linear model, log linear model, Collaboration, groupware, link prediction, Data models, Numerical models, Mathematical model]
Edge Weight Regularization over Multiple Graphs for Similarity Learning
2010 IEEE International Conference on Data Mining
None
2010
The growth of the web has directly influenced the increase in the availability of relational data. One of the key problems in mining such data is computing the similarity between objects with heterogeneous feature types. For example, publications have many heterogeneous features like text, citations, authorship information, venue information, etc. In most approaches, similarity is estimated using each feature type in isolation and then combined in a linear fashion. However, this approach does not take advantage of the dependencies between the different feature spaces. In this paper, we propose a novel approach to combine the different sources of similarity using a regularization framework over edges in multiple graphs. We show that the objective function induced by the framework is convex. We also propose an efficient algorithm using coordinate descent to solve the optimization problem. We extrinsically evaluate the performance of the proposed unified similarity measure on two different tasks, clustering and classification. The proposed similarity measure outperforms three baselines and a state-of-the-art classification algorithm on a variety of standard, large data sets.
[graph theory, data mining, data clustering, convex function, Data mining, Machine Learning, Heterogeneous Features, Semantics, optimization, Clustering algorithms, data classification, Classification, Convex functions, Context, pattern classification, Laplace equations, Similarity Learning, convex programming, edge weight regularization, similarity learning, Clustering, Equations, relational data, pattern clustering, regularization framework, multiple graph]
A New SVM Approach to Multi-instance Multi-label Learning
2010 IEEE International Conference on Data Mining
None
2010
In this paper, we address the problem of multi-instance multi-label learning (MIML) where each example is associated with not only multiple instances but also multiple class labels. In our novel approach, given an MIML example, each instance in the example is only associated with a single label and the label set of the example is the aggregation of all instance labels. Many real-world tasks such as scene classification, text categorization and gene sequence encoding can be properly formalized under our proposed approach. We formulate our MIML problem as a combination of two optimizations: (1) a quadratic programming (QP) that minimizes the empirical risk with L2-norm regularization, and (2) an integer programming (IP) assigning each instance to a single label. We also present an efficient method combining the stochastic gradient decent and alternating optimization approaches to solve our QP and IP optimizations. In our experiments with both an artificially generated data set and real-world applications, i.e. scene classification and text categorization, our proposed method achieves superior performance over existing state-of-the-art MIML methods such as MIMLBOOST, MIMLSVM, M3MIML and MIMLRBF.
[Airplanes, text analysis, image classification, integer programming, SVM approach, text categorization, MIMLRBF, SVM, integer programing, Optimization, Training, MIML method, alternating optimization approach, Multi-Instance Multi-Label, M3MIML, Classification, real world application, Prediction algorithms, learning (artificial intelligence), scene classification, gradient methods, MIMLSVM, multiinstance multilabel learning, support vector machines, stochastic gradient decent, Linear programming, MIMLBOOST, quadratic programming, Image segmentation, gene sequence encoding, Text categorization, L2-norm regularization]
Bayesian Aggregation of Binary Classifiers
2010 IEEE International Conference on Data Mining
None
2010
Multiclass classification problems are often decomposed into multiple binary problems that are solved by individual binary classifiers whose results are integrated into a final answer. Various methods have been developed to aggregate binary classifiers, including voting heuristics, loss-based decoding, and probabilistic decoding methods, but a little work on the optimal aggregation has been done. In this paper we present a Bayesian method for optimally aggregating binary classifiers where class membership probabilities are determined by predictive probabilities. We model the class membership probability as a softmax function whose input argument is a linear combination of discrepancies between code words and probability estimates obtained by the binary classifiers. We consider a lower bound on the softmax function, which is represented as a product of logistic sigmoids, and we formulate the problem of learning aggregation weights as a variational logistic regression. Predictive probabilities computed by variational logistic regression yield the class membership probabilities. We stress two notable advantages over existing methods in the viewpoint of complexity and over fitting. Numerical experiments on several datasets confirm its useful behavior.
[pattern classification, softmax function, Biological system modeling, Bayesian method, probability, logistic sigmoid, regression analysis, Classifier aggregation, Probabilistic logic, Encoding, Decoding, Approximation methods, prediction theory, predictive probability, aggregating binary classifier, variational logistic regression, probabilistic decoding, Bayesian methods, multiclass classification, variational techniques, Bayes methods, Logistics]
Permutations as Angular Data: Efficient Inference in Factorial Spaces
2010 IEEE International Conference on Data Mining
None
2010
Distributions over permutations arise in applications ranging from multi-object tracking to ranking of instances. The difficulty of dealing with these distributions is caused by the size of their domain, which is factorial in the number of considered entities (n!). It makes the direct definition of a multinomial distribution over permutation space impractical for all but a very small n. In this work we propose an embedding of all n! permutations for a given n in a surface of a hyper sphere defined in &#x211D;(n-1). As a result of the embedding, we acquire ability to define continuous distributions over a hyper sphere with all the benefits of directional statistics. We provide polynomial time projections between the continuous hyper sphere representation and the n!-element permutation space. The framework provides a way to use continuous directional probability densities and the methods developed thereof for establishing densities over permutations. As a demonstration of the benefits of the framework we derive an inference procedure for a state-space model over permutations. We demonstrate the approach with simulations on a large number of objects hardly manageable by the state of the art inference methods, and an application to a real flight traffic control dataset.
[hypersphere representation, approximation theory, Computational modeling, data mining, directional probability densities, multinomial distribution, state-space model, Probability distribution, approximate representation, Noise measurement, Data mining, inference mechanisms, statistical distributions, directional statistics, factorial space, inference procedure, Bayesian methods, object tracking, Polynomials, statistical inference, permutation space, polynomial time projections, computational complexity, state-space methods]
Separation of Interleaved Web Sessions with Heuristic Search
2010 IEEE International Conference on Data Mining
None
2010
We describe a heuristic search-based method for interleaved HTTP (Web) session reconstruction building upon first order Markov models. An interleaved session is generated by a user who is concurrently browsing the same web site in two or more web sessions (browser tabs or windows). In order to assure data quality for subsequent phases in analyzing user's browsing behavior, such sessions need to be separated in advance. We propose a separating process based on best-first search and trained first order Markov chains. We develop a testing method based on various measures of reconstructed sessions similarity to original ones. We evaluate the developed method on two real world click stream data sources: a web shop and a university student records information system. Preliminary results show that the proposed method performs well.
[pattern matching, best first search, hypermedia, Search problems, session separation process, Complexity theory, HTTP session, Strontium, Markov chain, interleaved HTTP session, sessionization, search problems, user browsing behavior, university student records information system, clickstream, Browsers, Markov model, interleaved session, Web shop, separating process, heuristic search based method, data quality, records management, Hidden Markov models, Markov processes, sessions similarity, Internet, Web sites, Web site, user behavior, interleaved Web session, clickstream data source]
Consequences of Variability in Classifier Performance Estimates
2010 IEEE International Conference on Data Mining
None
2010
The prevailing approach to evaluating classifiers in the machine learning community involves comparing the performance of several algorithms over a series of usually unrelated data sets. However, beyond this there are many dimensions along which methodologies vary wildly. We show that, depending on the stability and similarity of the algorithms being compared, these sometimes-arbitrary methodological choices can have a significant impact on the conclusions of any study, including the results of statistical tests. In particular, we show that performance metrics and data sets used, the type of cross-validation employed, and the number of iterations of cross-validation run have a significant, and often predictable, effect. Based on these results, we offer a series of recommendations for achieving consistent, reproducible results in classifier performance comparisons.
[pattern classification, reproducibility, Steady-state, Data mining, variability, machine learning, classification, Niobium, Training, evaluation, Machine learning, Decision trees, learning (artificial intelligence), classifier performance estimation]
Mining Sensor Streams for Discovering Human Activity Patterns over Time
2010 IEEE International Conference on Data Mining
None
2010
In recent years, new emerging application domains have introduced new constraints and methods in data mining field. One of such application domains is activity discovery from sensor data. Activity discovery and recognition plays an important role in a wide range of applications from assisted living to security and surveillance. Most of the current approaches for activity discovery assume a static model of the activities and ignore the problem of mining and discovering activities from a data stream over time. Inspired by the unique requirements of activity discovery application domain, in this paper we propose a new stream mining method for finding sequential patterns over time from streaming non-transaction data using multiple time granularities. Our algorithm is able to find sequential patterns, even if the patterns exhibit discontinuities (interruptions) or variations in the sequence order. Our algorithm also addresses the problem of dealing with rare events across space and over time. We validate the results of our algorithms using data collected from two different smart apartments.
[static model, sensor data, Humans, data mining, Time frequency analysis, sensor fusion, Data mining, History, activity discovery, data stream, sequential pattern, Equations, Itemsets, Activity Data Mining, Stream Sequence Mining, Smart Environments, Sensor Data, Monitoring, activity recognition]
Decision Trees for Uplift Modeling
2010 IEEE International Conference on Data Mining
None
2010
Most classification approaches aim at achieving high prediction accuracy on a given dataset. However, in most practical cases, some action, such as mailing an offer or treating a patient, is to be taken on the classified objects and we should model not the class probabilities themselves, but instead, the change in class probabilities caused by the action. The action should then be performed on those objects for which it will be most profitable. This problem is known as uplift modeling, differential response analysis or true lift modeling, but has received very little attention in Machine Learning literature. In the paper we present a tree based classifier tailored specifically to this task. To this end, we design new splitting criteria and pruning methods. The experiments confirm the usefulness of the proposed approach and show significant improvement over previous uplift modeling techniques.
[pattern classification, Buildings, probability, Predictive models, Entropy, Probability distribution, prediction theory, machine learning, Analytical models, uplift modeling, decision tree, Euclidean distance, decision trees, information theory, Decision trees, learning (artificial intelligence)]
Co-clustering of Lagged Data
2010 IEEE International Conference on Data Mining
None
2010
The paper focuses on mining clusters that are characterized by a lagged relationship between the data objects. We call such clusters lagged co-clusters. A lagged co-cluster of a matrix is a sub matrix determined by a subset of rows and their corresponding lag over a subset of columns. Extracting such subsets (not necessarily successive) may reveal an underlying governing regulatory mechanism. Such a regulatory mechanism is quite common in real life settings. It appears in a variety of fields: meteorology, seismic activity, stock market behavior, neuronal brain activity, river flow and navigation, are but a limited list of examples. Mining such lagged co-clusters not only helps in understanding the relationship between objects in the domain, but assists in forecasting their future behavior. For most interesting variants of this problem, finding an optimal lagged co-cluster is an NP-complete problem. We present a polynomial-time Monte-Carlo algorithm for finding a set of lagged co-clusters whose error does not exceed a pre-specified value, which handles noise, anti-correlations, missing values, and overlapping patterns. Moreover, we prove that the list includes, with fixed probability, a lagged co-cluster which is optimal in its dimensions. The algorithm was extensively evaluated using various environments. First, artificial data, enabling the evaluation of specific, isolated properties of the algorithm. Secondly, real-world data, using river flow and topographic data, enabling the evaluation of the algorithm to efficiently mine relevant and coherent lagged co-clusters in environments that are temporal, i.e., time reading data, and non-temporal, respectively.
[Algorithm design and analysis, timelagged, Correlation, Monte Carlo algorithm, Noise, lagged clustering, data mining, set theory, Data mining, NP-complete problem, lagged cocluster, matrix algebra, Computer science, co-clustering, Monte Carlo methods, regulatory mechanism, pattern clustering, Clustering algorithms, submatrix, Polynomials, clustering, subset, computational complexity]
Polishing the Right Apple: Anytime Classification Also Benefits Data Streams with Constant Arrival Times
2010 IEEE International Conference on Data Mining
None
2010
Classification of items taken from data streams requires algorithms that operate in time sensitive and computationally constrained environments. Often, the available time for classification is not known a priori and may change as a consequence of external circumstances. Many traditional algorithms are unable to provide satisfactory performance while supporting the highly variable response times that exemplify such applications. In such contexts, anytime algorithms, which are amenable to trading time for accuracy, have been found to be exceptionally useful and constitute an area of increasing research activity. Previous techniques for improving anytime classification have generally been concerned with optimizing the probability of correctly classifying individual objects. However, as we shall see, serially optimizing the probability of correctly classifying individual objects K times, generally gives inferior results to batch optimizing the probability of correctly classifying K objects. In this work, we show that this simple observation can be exploited to improve overall classification performance by using an anytime framework to allocate resources among a set of objects buffered from a fast arriving stream. Our ideas are independent of object arrival behavior, and, perhaps unintuitively, even in data streams with constant arrival rates our technique exhibits a marked improvement in performance. The utility of our approach is demonstrated with extensive experimental evaluations conducted on a wide range of diverse datasets.
[Schedules, data stream classification, Heuristic algorithms, time sensitive, individual object, batch optimization, overall classification performance, Classification algorithms, satisfactory performance, computationally constrained environment, variable response time, Training, Accuracy, optimisation, resource allocation, nearest neighbor, anytime algorithms, pattern classification, streaming data, probability, classification, Nearest neighbor searches, anytime algorithm, Processor scheduling, constant arrival time, probability optimization]
Discovering Correlated Subspace Clusters in 3D Continuous-Valued Data
2010 IEEE International Conference on Data Mining
None
2010
Subspace clusters represent useful information in high-dimensional data. However, mining significant subspace clusters in continuous-valued 3D data such as stock-financial ratio-year data, or gene-sample-time data, is difficult. Firstly, typical metrics either find subspaces with very few objects, or they find too many insignificant subspaces - those which exist by chance. Besides, typical 3D subspace clustering approaches abound with parameters, which are usually set under biased assumptions, making the mining process a `guessing game'. We address these concerns by proposing an information theoretic measure, which allows us to identify 3D subspace clusters that stand out from the data. We also develop a highly effective, efficient and parameter-robust algorithm, which is a hybrid of information theoretical and statistical techniques, to mine these clusters. From extensive experimentations, we show that our approach can discover significant 3D subspace clusters embedded in 110 synthetic datasets of varying conditions. We also perform a case study on real-world stock datasets, which shows that our clusters can generate higher profits compared to those mined by other approaches.
[Measurement, Correlation, parameter robust algorithm, statistical technique, data analysis, correlated subspace cluster, 3D subspace clustering, data mining, information hybrid, mining process, Probability, Data mining, pattern clustering, Clustering algorithms, continuous valued 3D data, 3D subspace clustering approache, financial data mining, Three dimensional displays, information theory, Kernel, stock dataset]
gSkeletonClu: Density-Based Network Clustering via Structure-Connected Tree Division or Agglomeration
2010 IEEE International Conference on Data Mining
None
2010
Community detection is an important task for mining the structure and function of complex networks. Many pervious approaches are difficult to detect communities with arbitrary size and shape, and are unable to identify hubs and outliers. A recently proposed network clustering algorithm, SCAN, is effective and can overcome this difficulty. However, it depends on a sensitive parameter: minimum similarity threshold &#x03B5;, but provides no automated way to find it. In this paper, we propose a novel density-based network clustering algorithm, called gSkeletonClu (graph-skeleton based clustering). By projecting a network to its Core-Connected Maximal Spanning Tree (CCMST), the network clustering problem is converted to finding core-connected components in the CCMST. We discover that all possible values of the parameter &#x03B5; lie in the edge weights of the corresponding CCMST. By means of tree divisive or agglomerative clustering, our algorithm can find the optimal parameter &#x03B5; and detect communities, hubs and outliers in large-scale undirected networks automatically without any user interaction. Extensive experiments on both real-world and synthetic networks demonstrate the superior performance of gSkeletonClu over the baseline methods.
[Knee, synthetic networks, Shape, Clustering methods, Communities, Density-based Network Clustering, large-scale undirected networks, complex networks, outliers, real-world networks, Parameter Selection, Clustering algorithms, density based network clustering, optimal parameter, core connected maximal spanning tree, community detection, graph skeleton based clustering, gSkeletonClu, agglomerative clustering, structure connected tree division, Image edge detection, trees (mathematics), Hubs and Outliers, edge weights, Community Discovery, hubs, pattern clustering, social networking (online), Periodic structures]
LogTree: A Framework for Generating System Events from Raw Textual Logs
2010 IEEE International Conference on Data Mining
None
2010
Modern computing systems are instrumented to generate huge amounts of system logs and these data can be utilized for understanding and complex system behaviors. One main fundamental challenge in automated log analysis is the generation of system events from raw textual logs. Recent works apply clustering techniques to translate the raw log messages into system events using only the word/term information. In this paper, we first illustrate the drawbacks of existing techniques for event generation from system logs. We then propose Log Tree, a novel and algorithm-independent framework for events generation from raw system log messages. Log Tree utilizes the format and structural information of the raw logs in the clustering process to generate system events with better accuracy. In addition, an indexing data structure, Message Segment Table, is proposed in Log Tree to significantly improve the efficiency of events creation. Extensive experiments on real system logs demonstrate the effectiveness and efficiency of Log Tree.
[log messages, clustering techniques, Data structures, fault trees, Servers, Accuracy, event creation, event generation, LogTree, raw textual logs, Clustering algorithms, message segment table, message clustering, system monitoring, statistical analysis, complex system behaviors, Kernel, log analysis, automated log analysis, Indexing]
Mining Closed Strict Episodes
2010 IEEE International Conference on Data Mining
None
2010
Discovering patterns in a sequence is an important aspect of data mining. One popular choice of such patterns are episodes, patterns in sequential data describing events that often occur in the vicinity of each other. Episodes also enforce in which order events are allowed to occur. In this work we introduce a technique for discovering closed episodes. Adopting existing approaches for discovering traditional patterns, such as closed item sets, to episodes is not straightforward. First of all, we cannot define a unique closure based on frequency because an episode may have several closed super episodes. Moreover, to define a closedness concept for episodes we need a subset relationship between episodes, which is not trivial to define. We approach these problems by introducing strict episodes. We argue that this class is general enough, and at the same time we are able to define a natural subset relationship within it and use it efficiently. In order to mine closed episodes we define an auxiliary closure operator. We show that this closure satisfies the needed Galois connection so that we can use the existing framework for mining closed patterns. Discovering the true closed episodes can be done as a post-processing step. We combine these observations into an efficient mining algorithm and demonstrate empirically its performance in practice.
[pattern classification, closed episode discovery, auxiliary closure operator, Conferences, graph theory, data mining, closed superepisode, Explosions, Graph theory, postprocessing step, Data mining, Indexes, Galois fields, Level-wise Algorithm, Itemsets, sequential data, closed pattern mining, Galois connection, closedness concept, Skeleton, Frequent Episode Mining, Closed Episodes]
Multi-dimensional Mass Estimation and Mass-based Clustering
2010 IEEE International Conference on Data Mining
None
2010
Mass estimation, an alternative to density estimation, has been shown recently to be an effective base modelling mechanism for three data mining tasks of regression, information retrieval and anomaly detection. This paper advances this work in two directions. First, we generalise the previously proposed one-dimensional mass estimation to multidimensional mass estimation, and significantly reduce the time complexity to O(&#x03C8;h) from O(&#x03C8;h)-making it feasible for a full range of generic problems. Second, we introduce the first clustering method based on mass-it is unique because it does not employ any distance or density measure. The structure of the new mass model enables different parts of a cluster to be identified and merged without expensive evaluations. The characteristics of the new clustering method are: (i) it can identify arbitrary-shape clusters; (ii) it is significantly faster than existing density-based or distance-based methods; and (iii) it is noise-tolerant.
[base modelling mechanism, Clustering methods, Estimation, data mining, arbitrary-shape cluster, information retrieval, density-based method, time complexity, anomaly detection, Complexity theory, Data mining, Equations, one dimensional mass estimation, Runtime, mass-based clustering, pattern clustering, Mass estimation, Data models, data mining task, density estimation, distance-based method, computational complexity, multidimensional mass estimation]
minCEntropy: A Novel Information Theoretic Approach for the Generation of Alternative Clusterings
2010 IEEE International Conference on Data Mining
None
2010
Traditional clustering has focused on creating a single good clustering solution, while modern, high dimensional data can often be interpreted, and hence clustered, in different ways. Alternative clustering aims at creating multiple clustering solutions that are both of high quality and distinctive from each other. Methods for alternative clustering can be divided into objective-function-oriented and data-transformation-oriented approaches. This paper presents a novel information theoretic-based, objective-function-oriented approach to generate alternative clusterings, in either an unsupervised or semi-supervised manner. We employ the conditional entropy measure for quantifying both clustering quality and distinctiveness, resulting in an analytically consistent combined criterion. Our approach employs a computationally efficient nonparametric entropy estimator, which does not impose any assumption on the probability distributions. We propose a partitional clustering algorithm, named minCEntropy, to concurrently optimize both clustering quality and distinctiveness. minCEntropy requires setting only some rather intuitive parameters, and performs competitively with existing methods for alternative clustering.
[nonparametric entropy estimator, Estimation, Transforms, alternative clustering generation, Entropy, partitional clustering algorithm, statistical distributions, transformation, Optimization, objective-function-oriented approach, information theoretic clustering, minCEntropy, entropy, pattern clustering, probability distributions, Clustering algorithms, information theoretic approach, alternative clustering, clustering, multi-objective optimization, nonparametric statistics, Kernel, Mutual information, data-transformation-oriented approach]
A Conscience On-line Learning Approach for Kernel-Based Clustering
2010 IEEE International Conference on Data Mining
None
2010
Kernel-based clustering is one of the most popular methods for partitioning nonlinearly separable dataset. However, exhaustive search for the global optimum is NP-hard. Iterative procedure such as k-means can be used to seek one of the local minima. Unfortunately, it is easily trapped into degenerate local minima when the prototypes of clusters are ill-initialized. In this paper, we restate the optimization problem of kernel-based clustering in an on-line learning framework, whereby a conscience mechanism is easily integrated to tackle the ill-initialization problem and faster convergence rate is achieved. Thus, we propose a novel approach termed conscience on-line learning (COLL). For each randomly taken data point, our method selects the winning prototype based on the conscience mechanism to bias the ill-initialized prototype to avoid degenerate local minima, and efficiently updates the winner by the on-line learning rule. Therefore, it can more efficiently obtain smaller distortion error than k-means with the same initialization. Experimental results on synthetic and large-scale real-world datasets, as well as that in the application of video clustering, have demonstrated the significant improvement over existing kernel clustering methods.
[iterative methods, Optimization, Convergence, optimisation, conscience mechanism, video clustering, optimization, Prototypes, Clustering algorithms, k-means, Matrices, kernel-based clustering, on-line learning, learning (artificial intelligence), Kernel, video signal processing, ill-initialization problem, conscience online learning, distortion error, iterative method, nonlinearly separable dataset partitioning, NP-hard problem, pattern clustering, Arrays, distortion]
Weighted Feature Subset Non-negative Matrix Factorization and Its Applications to Document Understanding
2010 IEEE International Conference on Data Mining
None
2010
Keyword (Feature) selection enhances and improves many Information Retrieval (IR) tasks such as document categorization, automatic topic discovery, etc. The problem of keyword selection is usually solved using supervised algorithms. In this paper, we propose an unsupervised approach that combines keyword selection and document clustering (topic discovery) together. The proposed approach extends non-negative matrix factorization (NMF) by incorporating a weight matrix to indicate the importance of the keywords. The proposed approach is further extended to a weighted version in which each document is also assigned a weight to assess its importance in the cluster. This work considers both theoretical and empirical weighted feature subset selection for NMF and draws the connection between unsupervised feature selection and data clustering. We apply our proposed approaches to various document understanding tasks including document clustering, summarization, and visualization. Experimental results demonstrate the effectiveness of our approach for these tasks.
[document handling, Non-negative matrix factorization, document clustering, information retrieval, data clustering, Large scale integration, matrix decomposition, nonnegative matrix factorization, unsupervised feature selection, Optimization, unsupervised learning, weighted feature subset non-negative matrix factorization, Computer science, pattern clustering, weighted feature subset selection, Semantics, Clustering algorithms, Hidden Markov models, Data visualization, keyword selection, feature selection]
Learning a Bi-Stochastic Data Similarity Matrix
2010 IEEE International Conference on Data Mining
None
2010
An idealized clustering algorithm seeks to learn a cluster-adjacency matrix such that, if two data points belong to the same cluster, the corresponding entry would be 1; otherwise the entry would be 0. This integer (1/0) constraint makes it difficult to find the optimal solution. We propose a relaxation on the cluster-adjacency matrix, by deriving a bi-stochastic matrix from a data similarity (e.g., kernel) matrix according to the Bregman divergence. Our general method is named the Bregmanian Bi-Stochastication (BBS) algorithm. We focus on two popular choices of the Bregman divergence: the Euclidian distance and the KL divergence. Interestingly, the BBS algorithm using the KL divergence is equivalent to the Sinkhorn-Knopp (SK) algorithm for deriving bi-stochastic matrices. We show that the BBS algorithm using the Euclidian distance is closely related to the relaxed K-means clustering and can often produce noticeably superior clustering results than the SK algorithm (and other algorithms such as Normalized Cut), through extensive experiments on public data sets.
[Symmetric matrices, clustering algorithm, integer programming, bistochastic data similarity matrix, Optimization, Tuning, Radio access networks, matrix algebra, Training, Bregmanian bistochastication algorithm, k-means clustering, pattern clustering, Euclidian distance, Clustering algorithms, Bregman divergence, cluster adjacency matrix, data handling, Sinkhorn-Knopp algorithm, learning (artificial intelligence), integer constraint, Kernel]
Active Spectral Clustering
2010 IEEE International Conference on Data Mining
None
2010
The technique of spectral clustering is widely used to segment a range of data from graphs to images. Our work marks a natural progression of spectral clustering from the original passive unsupervised formulation to our active semi-supervised formulation. We follow the widely used area of constrained clustering and allow supervision in the form of pair wise relations between two nodes: Must-Link and Cannot-Link. Unlike most previous constrained clustering work, our constraints are specified incrementally by querying an oracle (domain expert). Since in practice, each query comes with a cost, our goal is to maximally improve the result with as few queries as possible. The advantages of our approach include: 1) it is principled by querying the constraints which maximally reduce the expected error, 2) it can incorporate both hard and soft constraints which are prevalent in practice. We empirically show that our method significantly outperforms the baseline approach, namely constrained spectral clustering with randomly selected constraints, on UCI benchmark data sets.
[graph theory, pairwise relations, Must-Link, active semisupervised formulation, randomly selected constraints, active spectral clustering, natural progression, query processing, Iris, graphs, active learning, Clustering algorithms, Benchmark testing, Eigenvalues and eigenfunctions, original passive unsupervised formulation, spectral clustering, constrained clustering, Symmetric matrices, Laplace equations, oracle, Estimation, querying, unsupervised learning, pattern clustering, UCI benchmark data /fevwwds-spectral, Cannot-Link]
Discovering Overlapping Groups in Social Media
2010 IEEE International Conference on Data Mining
None
2010
The increasing popularity of social media is shortening the distance between people. Social activities, e.g., tagging in Flickr, book marking in Delicious, twittering in Twitter, etc. are reshaping people's social life and redefining their social roles. People with shared interests tend to form their groups in social media, and users within the same community likely exhibit similar social behavior (e.g., going for the same movies, having similar political viewpoints), which in turn reinforces the community structure. The multiple interactions in social activities entail that the community structures are often overlapping, i.e., one person is involved in several communities. We propose a novel co-clustering framework, which takes advantage of networking information between users and tags in social media, to discover these overlapping communities. In our method, users are connected via tags and tags are connected to users. This explicit representation of users and tags is useful for understanding group evolution by looking at who is interested in what. The efficacy of our method is supported by empirical evaluation in both synthetic and online social networking data.
[Co-Clustering, Overlapping, Social network services, Image edge detection, Communities, Social Media, social behavior, Media, users representation, tags, co-clustering, USA Councils, online social networking, Community Detection, Motion pictures, social networking (online), Bipartite graph, Internet, social media]
Adaptive Distances on Sets of Vectors
2010 IEEE International Conference on Data Mining
None
2010
Recently, there has been a growing interest in learning distances directly from training data. While the previous works focused mainly on adapting distance measures over vectorial data, it is a well-known fact that many real-world data could not be easily represented as fixed length tuples of constants. In this paper we address this limitation and propose a novel class of distance learning techniques for learning problems in which instances are set of vectors, examples of such problems include, among others, automatic image annotation and graph classification. We investigate the behavior of the adaptive set distances on a number of artificial and real-world problems and demonstrate that they improve over the standard set distances.
[Measurement, k-nearest neighbor algorithm, sets, learning distances, set theory, complex objects, distance learning, Training, Computer aided instruction, vectors, graphs, training data, adaptive distances, Cost function, vector, DSL, learning (artificial intelligence), tuples, Kernel, vectorial data]
SMILE: A Similarity-Based Approach for Multiple Instance Learning
2010 IEEE International Conference on Data Mining
None
2010
Multiple instance learning (MIL) is a generalization of supervised learning which attempts to learn useful information from bags of instances. In MIL, the true labels of the instances in positive bags are not always available for training. This leads to a critical challenge, namely, handling the ambiguity of instance labels in positive bags. To address this issue, this paper proposes a novel MIL method named SMILE (Similarity-based Multiple Instance LEarning). It introduces a similarity weight to each instance in positive bag, which represents the instance similarity towards the positive and negative classes. The instances in positive bags, together with their similarity weights, are thereafter incorporated into the learning phase to build an extended SVM-based predictive classifier. Experiments on three real-world datasets consisting of 12 subsets show that SMILE achieves markedly better classification accuracy than state-of-the-art MIL methods.
[supervised learning generalization, pattern classification, support vector machines, generalisation (artificial intelligence), Equations, SVM-based predictive classifier, Support vector machines, Training, Learning systems, MIL method, Accuracy, similarity-based multiple instance learning, Data models, Multiple Instance Learning, Mathematical model, learning (artificial intelligence), SMILE]
Modeling Information Diffusion in Implicit Networks
2010 IEEE International Conference on Data Mining
None
2010
Social media forms a central domain for the production and dissemination of real-time information. Even though such flows of information have traditionally been thought of as diffusion processes over social networks, the underlying phenomena are the result of a complex web of interactions among numerous participants. Here we develop the Linear Influence Model where rather than requiring the knowledge of the social network and then modeling the diffusion by predicting which node will influence which other nodes in the network, we focus on modeling the global influence of a node on the rate of diffusion through the (implicit) network. We model the number of newly infected nodes as a function of which other nodes got infected in the past. For each node we estimate an influence function that quantifies how many subsequent infections can be attributed to the influence of that node over time. A nonparametric formulation of the model leads to a simple least squares problem that can be solved on large datasets. We validate our model on a set of 500 million tweets and a set of 170 million news articles and blog posts. We show that the Linear Influence Model accurately models influences of nodes and reliably predicts the temporal dynamics of information diffusion. We find that patterns of influence of individual participants differ significantly depending on the type of the node and the topic of the information.
[Solid modeling, least mean squares methods, linear influence model, modeling information diffusion, social networks, Predictive models, Media, least squares problem, Twitter, multimedia computing, blog posts, Online media, Information services, social networking (online), Internet, Information diffusion, social media, Node influence]
Term Filtering with Bounded Error
2010 IEEE International Conference on Data Mining
None
2010
In this paper, we consider a novel problem referred to as term filtering with bounded error to reduce the term (feature) space by eliminating terms without (or with bounded) information loss. Different from existing works, the obtained term space provides a complete view of the original term space. More interestingly, several important questions can be answered such as: 1) how different terms interact with each other and 2) how the filtered terms can be represented by the other terms. We perform a theoretical investigation of the term filtering problem and link it to the Geometric Covering By Discs problem, and prove its NP-hardness. We present two novel approaches for both lossless and lossy term filtering with bounds on the introduced error. Experimental results on multiple text mining tasks validate the effectiveness of the proposed approaches.
[Text mining, Measurement, feature space reduction, Vocabulary, bounded error, Correlation, Filtering, discs problem, data mining, information filtering, Complexity theory, Partitioning algorithms, lossless term filtering, geometric covering, lossy term filtering, NP-hardness, term space reduction, multiple text mining tasks, computational complexity]
Exploiting Unlabeled Data to Enhance Ensemble Diversity
2010 IEEE International Conference on Data Mining
None
2010
Ensemble learning aims to improve generalization ability by using multiple base learners. It is well-known that to construct a good ensemble, the base learners should be accurate as well as diverse. In this paper, unlabeled data is exploited to facilitate ensemble learning by helping augment the diversity among the base learners. Specifically, a semi-supervised ensemble method named UDEED is proposed. Unlike existing semi-supervised ensemble methods where error-prone pseudo-labels are estimated for unlabeled data to enlarge the labeled data to improve accuracy, UDEED works by maximizing accuracies of base learners on labeled data while maximizing diversity among them on unlabeled data. Experiments show that UDEED can effectively utilize unlabeled data for ensemble learning and is highly competitive to well-established semi-supervised ensemble methods.
[data analysis, Communities, Loss measurement, accuracy maximization, Training, ensemble learning, Accuracy, optimisation, diversity, UDEED, Data models, unlabeled data, learning (artificial intelligence), semisupervised ensemble method, Bagging, Logistics]
Constraint Based Dimension Correlation and Distance Divergence for Clustering High-Dimensional Data
2010 IEEE International Conference on Data Mining
None
2010
Clusters are hidden in subspaces of high dimensional data, i.e., only a subset of features is relevant for each cluster. Subspace clustering is challenging since the search for the relevant features of each cluster and the detection of the final clusters are circular dependent and should be solved simultaneously. In this paper, we point out that feature correlation and distance divergence are important to subspace clustering, but both have not been considered in previous works. Feature correlation groups correlated features independently thus helps to reduce the search space for the relevant features search problem. Distance divergence distinguishes distances on different dimensions and helps to find the final clusters accurately. We tackle the two problems with the aid of a small amount domain knowledge in the form of must-links and cannot-links. We then devise a semi-supervised subspace clustering algorithm CDCDD. CDCDD integrates our solutions of the feature correlation and distance divergence problems, and uses an adaptive dimension voting scheme, which is derived from a previous unsupervised subspace clustering algorithm FINDIT. Experimental results on both synthetic data sets and real data sets show that the proposed CDCDD algorithm outperforms FINDIT in terms of accuracy, and outperforms the other constraint based algorithm SCMINER in terms of both accuracy and efficiency.
[Correlation, high-dimensional data, pair-wise constraint, semisupervised subspace clustering algorithm, Search problems, set theory, Accuracy, feature extraction, Clustering algorithms, feature correlation, high-dimensional data clustering, constraint handling, search problems, distance divergence, Nearest neighbor searches, feature search, pattern clustering, adaptive dimension voting scheme, subspace clustering, Feature extraction, US Department of Defense, semi-supervised learning, constraint based dimension correlation, subset, CDCDD algorithm, correlation methods]
Active Learning from Multiple Noisy Labelers with Varied Costs
2010 IEEE International Conference on Data Mining
None
2010
In active learning, where a learning algorithm has to purchase the labels of its training examples, it is often assumed that there is only one labeler available to label examples, and that this labeler is noise-free. In reality, it is possible that there are multiple labelers available (such as human labelers in the online annotation tool Amazon Mechanical Turk) and that each such labeler has a different cost and accuracy. We address the active learning problem with multiple labelers where each labeler has a different (known) cost and a different (unknown) accuracy. Our approach uses the idea of adjusted cost, which allows labelers with different costs and accuracies to be directly compared. This allows our algorithm to find low-cost combinations of labelers that result in high-accuracy labelings of instances. Our algorithm further reduces costs by pruning under-performing labelers from the set under consideration, and by halting the process of estimating the accuracy of the labelers as early as it can. We found that our algorithm often outperforms, and is always competitive with, other algorithms in the literature.
[algorithms, adjusted cost, high accuracy labeling, Noise, under performing labeler pruning, Humans, data mining, label purchase, active learning algorithm, low cost combination, Noise measurement, Training, Accuracy, active learning, noisy labelers, Speech recognition, multiple noisy labelers, Labeling, learning (artificial intelligence), noise-free labeler, multiple labelers]
A Novel Contrast Co-learning Framework for Generating High Quality Training Data
2010 IEEE International Conference on Data Mining
None
2010
The good performances of most classical learning algorithms are generally founded on high quality training data, which are clean and unbiased. The availability of such data is however becoming much harder than ever in many real world problems due to the difficulties in collecting large scale unbiased data and precisely labeling them for training. In this paper, we propose a general Contrast Co-learning (CCL) framework to refine the biased and noisy training data when an unbiased yet unlabeled data pool is available. CCL starts with multiple sets of probably biased and noisy training data and trains a set of classifiers individually. Then under the assumption that the confidently classified data samples may have higher probabilities to be correctly classified, CCL iteratively and automatically filtering out possible data noises as well as adding those confidently classified samples from the unlabeled data pool to correct the bias. Through this process, we can generate a cleaner and unbiased training dataset with theoretical guarantees. Extensive experiments on two public text datasets clearly show that CCL consistently improves the algorithmic classification performance on biased and noisy training data compared with several state-of-the-art classical algorithms.
[pattern classification, noisy training data, Filtering, Noise, Training data bias, high quality training, Classification algorithms, set theory, Noise measurement, data collection, Noisy training data, unsupervised learning, Training, Co-learning, Training data, data classification, Contrast Classifier, noise, Data models, contrast colearning]
Network Simplification with Minimal Loss of Connectivity
2010 IEEE International Conference on Data Mining
None
2010
We propose a novel problem to simplify weighted graphs by pruning least important edges from them. Simplified graphs can be used to improve visualization of a network, to extract its main structure, or as a pre-processing step for other data mining algorithms. We define a graph connectivity function based on the best paths between all pairs of nodes. Given the number of edges to be pruned, the problem is then to select a subset of edges that best maintains the overall graph connectivity. Our model is applicable to a wide range of settings, including probabilistic graphs, flow graphs and distance graphs, since the path quality function that is used to find best paths can be defined by the user. We analyze the problem, and give lower bounds for the effect of individual edge removal in the case where the path quality function has a natural recursive property. We then propose a range of algorithms and report on experimental results on real networks derived from public biological databases. The results show that a large fraction of edges can be removed quite fast and with minimal effect on the overall graph connectivity. A rough semantic analysis of the removed edges indicates that few important edges were removed, and that the proposed approach could be a valuable tool in aiding users to view or explore weighted graphs.
[pruning least, Force, graph theory, data mining, probabilistic graphs, network theory (graphs), path quality function, Complexity theory, Data mining, connectivity, rough semantic analysis, data visualisation, Erbium, probability, flow graphs, public biological databases, Equations, Sorting, graph mining, graph connectivity function, Data visualization, decision trees, simplify weighted graphs, network visualization, distance graphs, network simplification]
Improving Kernel Methods through Complex Data Mapping
2010 IEEE International Conference on Data Mining
None
2010
This paper introduces a simple yet powerful data transformation strategy for kernel machines. Instead of adapting the parameters of the kernel function w.r.t. the given data (as in conventional methods), we adjust both the kernel hyper-parameters and the given data itself. Using this approach, the input data is transformed to be more representative of the assumptions encoded in the kernel function. A novel complex mapping is proposed to nonlinearly adjust the data. Optimization of the data transformation parameters is performed in two different manners. Firstly, the complex data mapping parameters and kernel hyper-parameters are selected separately, with the former guided by frequency metrics and the latter under the Bayesian framework. Next, the complex data mapping parameters and kernel hyper-parameters are optimized simultaneously in a Bayesian formulation by creating a new category of "integrated kernel" with the complex data mapping embedded. Experiments using Gaussian Process learning have shown that both methods improve the learning accuracy in either classification or regression tasks, with the complex mapping embedded kernel approach outperforming the separate complex mapping one.
[support vector machines, kernel method, data mapping, frequency metrics, Data mining, Noise measurement, frequency domain, Optimization, Gaussian process learning, kernel hyper parameter, optimisation, Bayesian methods, Gaussian processes, regression task, kernel methods, Gaussian Process, Data models, Bayes methods, data handling, data transformation, Bayesian formulation, learning (artificial intelligence), Kernel, complex mapping]
NESVM: A Fast Gradient Method for Support Vector Machines
2010 IEEE International Conference on Data Mining
None
2010
Support vector machines (SVMs) are invaluable tools for many practical applications in artificial intelligence, e.g., classification and event recognition. However, popular SVM solvers are not sufficiently efficient for applications with a great deal of samples as well as a large number of features. In this paper, thus, we present NESVM, a fast gradient SVM solver that can optimize various SVM models, e.g., classical SVM, linear programming SVM and least square SVM. Compared against SVM-Perf (whose convergence rate in solving the dual SVM is upper bounded by O(1/&#x221A;k) where k is the number of iterations) and Pegasos (online SVM that converges at rate O(1/k) for the primal SVM), NESVM achieves the optimal convergence rate at O(1/k2) and a linear time complexity. In particular, NESVM smoothes the nondifferentiable hinge loss and &#x2113;<sub>1</sub>-norm in the primal SVM. Then the optimal gradient method without any line search is adopted to solve the optimization. In each iteration round, the current gradient and historical gradients are combined to determine the descent direction, while the Lipschitz constant determines the step size. Only two matrix-vector multiplications are required in each iteration round. Therefore, NESVM is more efficient than existing SVM solvers. In addition, NESVM is available for both linear and nonlinear kernels. We also propose "homotopy NESVM" to accelerate NESVM by dynamically decreasing the smooth parameter and using the continuation method. Our experiments on census income categorization, indoor/outdoor scene classification event recognition and scene recognition suggest the efficiency and the effectiveness of NESVM. The MATLAB code of NESVM will be available on our website for further assessment.
[iterative methods, hinge loss, support vector machines, Fasteners, gradient method, matrix vector multiplication, Complexity theory, Optimization, Convergence, Support vector machines, optimisation, $\\ell_1$ norm, support vector machine, NESVM, continuation method, Approximation error, Nesterov's method, iteration round, convergence rate, gradient methods, linear time complexity, Nesterov method, computational complexity, smooth]
Clustering Large Attributed Graphs: An Efficient Incremental Approach
2010 IEEE International Conference on Data Mining
None
2010
In recent years, many networks have become available for analysis, including social networks, sensor networks, biological networks, etc. Graph clustering has shown its effectiveness in analyzing and visualizing large networks. The goal of graph clustering is to partition vertices in a large graph into clusters based on various criteria such as vertex connectivity or neighborhood similarity. Many existing graph clustering methods mainly focus on the topological structures, but largely ignore the vertex properties which are often heterogeneous. Recently, a new graph clustering algorithm, SA-Cluster, has been proposed which combines structural and attribute similarities through a unified distance measure. SA-Cluster performs matrix multiplication to calculate the random walk distances between graph vertices. As the edge weights are iteratively adjusted to balance the importance between structural and attribute similarities, matrix multiplication is repeated in each iteration of the clustering process to recalculate the random walk distances which are affected by the edge weight update. In order to improve the efficiency and scalability of SA-Cluster, in this paper, we propose an efficient algorithm Inc-Cluster to incrementally update the random walk distances given the edge weight increments. Complexity analysis is provided to estimate how much runtime cost Inc-Cluster can save. Experimental results demonstrate that Inc-Cluster achieves significant speedup over SA-Cluster on large graphs, while achieving exactly the same clustering quality in terms of intra-cluster structural cohesiveness and attribute value homogeneity.
[iterative methods, Gallium, SA-cluster, Communities, graph theory, complexity analysis, sensor networks, Complexity theory, large attributed graph clustering, Inc-Cluster, biological networks, topological structure, Clustering algorithms, Social network services, social networks, incremental computation, Partitioning algorithms, Matrix decomposition, iterative method, incremental approach, large network analysis, graph clustering, matrix multiplication, large network visualization, statistical analysis]
Mother Fugger: Mining Historical Manuscripts with Local Color Patches
2010 IEEE International Conference on Data Mining
None
2010
Initiatives such as the Google Print Library Project and the Million Book Project have already archived more than ten million books in digital format, and within the next decade the majority of world's books will be online. Although most of the data will naturally be text, there will also be tens of millions of pages of images, many in color. While there is an active research community pursuing data mining of text from historical manuscripts, there has been very little work that exploits the rich color information which is often present. In this work we introduce a simple color measure which both addresses and exploits typical features of historical manuscripts. To enable the efficient mining of massive archives, we propose a tight lower bound to the measure. Beyond the fast similarity search, we show how this lower bound allows us to build several higher-level data mining tools, including motif discovery and link analyses. We demonstrate our ideas in several data mining tasks on manuscripts dating back to the fifteenth century.
[indexing, Color Indexing, data mining, Artificial neural networks, Color, Google print library project, archive, history, Historical Manuscripts, Data mining, Histograms, Million Book project, records management, Image color analysis, historical manuscript, color image, motif discovery, image colour analysis, Books, Pixel, search problems, link analyses]
D-LDA: A Topic Modeling Approach without Constraint Generation for Semi-defined Classification
2010 IEEE International Conference on Data Mining
None
2010
We study what we call semi-defined classification, which deals with the categorization tasks where the taxonomy of the data is not well defined in advance. It is motivated by the real-world applications, where the unlabeled data may also come from some other unknown classes besides the known classes for the labeled data. Given the unlabeled data, our goal is to not only identify the instances belonging to the known classes, but also cluster the remaining data into other meaningful groups. It differs from traditional semi-supervised clustering in the sense that in semi-supervised clustering the supervision knowledge is too far from being representative of a target classification, while in semi-defined classification the labeled data may be enough to supervise the learning on the known classes. In this paper we propose the model of Double-latent-layered LDA (D-LDA for short) for this problem. Compared with LDA with only one latent variable y for word topics, D-LDA contains another latent variable z for (known and unknown) document classes. With this double latent layers consisting of y and z and the dependency between them, D-LDA directly injects the class labels into z to supervise the exploiting of word topics in y. Thus, the semi-supervised learning in D-LDA does not need the generation of pair wise constraints, which is required in most of the previous semi-supervised clustering approaches. We present the experimental results on ten different data sets for semi-defined classification. Our results are either comparable to (on one data sets), or significantly better (on the other nine data set) than the six compared methods, including the state-of-the-art semi-supervised clustering methods.
[Topic modeling, pattern classification, semidefined classification, Gibbs Sampling, semisupervised learning, data cluster, Companies, Semi-supervised clustering, topic modeling approach, Equations, constraint generation, Analytical models, D-LDA, pattern clustering, Clustering algorithms, Web pages, labeled data, double-latent-layered LDA, Semi-defined classification, Data models, unlabeled data, Mathematical model, learning (artificial intelligence), data taxonomy]
SONNET: Efficient Approximate Nearest Neighbor Using Multi-core
2010 IEEE International Conference on Data Mining
None
2010
Approximate Nearest Neighbor search over high dimensional data is an important problem with a wide range of practical applications. In this paper, we propose SONNET, a simple multi-core friendly approximate nearest neighbor algorithm that is based on rank aggregation. SONNET is particularly suitable for very high dimensional data, its performance gets better as the dimension increases, whereas the majority of the existing algorithms show a reverse trend. Furthermore, most of the existing algorithms are hard to parallelize either due to the sequential nature of the algorithm or due to the inherent complexity of the algorithm. On the other hand, SONNET has inherent parallelism embedded in the core concept of the algorithm, which earns it almost a linear speed-up as the number of cores increases. Finally, SONNET is very easy to implement and it has an approximation parameter which is intuitively simple.
[multi-core, approximation theory, parallel algorithms, Instruction sets, data mining, Artificial neural networks, nearest neighbors, approximate nearest neighbor search, rank aggregation, Approximation methods, tree searching, Nearest neighbor searches, early termination, approximate nearest neighbors, Approximation algorithms, Sonnet, Indexing]
Two of a Kind or the Ratings Game? Adaptive Pairwise Preferences and Latent Factor Models
2010 IEEE International Conference on Data Mining
None
2010
While latent factor models are built using ratings data, which is typically assumed static, the ability to incorporate different kinds of subsequent user feedback is an important asset. For instance, the user might want to provide additional information to the system in order to improve his personal recommendations. To this end, we examine a novel scheme for efficiently learning (or refining) user parameters from such feedback. We propose a scheme where users are presented with a sequence of pair wise preference questions: "Do you prefer item A over B?". User parameters are updated based on their response, and subsequent questions are chosen adaptively after incorporating the feedback. We operate in a Bayesian framework and the choice of questions is based on an information gain criterion. We validate the scheme on the Netflix movie ratings data set. A user study and automated experiments validate our findings.
[Recommender Systems, Adaptation model, game theory, Bayesian framework, latent factor model, information filtering, Latent factor models, Netflix movie ratings data set, Proposals, Approximation methods, Active Learning, recommender systems, recommender system, ratings game, Bayesian methods, Motion pictures, Prediction algorithms, adaptive pairwise preference, Data models, Pairwise preferences, Bayes methods, learning (artificial intelligence)]
Document Similarity Self-Join with MapReduce
2010 IEEE International Conference on Data Mining
None
2010
Given a collection of objects, the Similarity Self-Join problem requires to discover all those pairs of objects whose similarity is above a user defined threshold. In this paper we focus on document collections, which are characterized by a sparseness that allows effective pruning strategies. Our contribution is a new parallel algorithm within the MapReduce framework. This work borrows from the state of the art in serial algorithms for similarity join and MapReduce-based techniques for set-similarity join. The proposed algorithm shows that it is possible to leverage a distributed file system to support communication patterns that do not naturally fit the MapReduce framework. Scalability is achieved by introducing a partitioning strategy able to overcome memory bottlenecks. Experimental evidence on real world data shows that our algorithm outperforms the state of the art by a factor 4.5.
[Algorithm design and analysis, document handling, parallel algorithms, user defined threshold, serial algorithm, memory bottlenecks, Web Information Retrieval, Scalability, distributed file system, Similarity Self-Join, document similarity, parallel algorithm, partitioning strategy, Partitioning algorithms, Sorting, MapReduce, object collection, network operating systems, Clustering algorithms, distributed databases, pruning strategy, MapReduce-based technique, Indexing]
Quantification via Probability Estimators
2010 IEEE International Conference on Data Mining
None
2010
Quantification is the name given to a novel machine learning task which deals with correctly estimating the number of elements of one class in a set of examples. The output of a quantifier is a real value, since training instances are the same as a classification problem, a natural approach is to train a classifier and to derive a quantifier from it. Some previous works have shown that just classifying the instances and counting the examples belonging to the class of interest classify count typically yields bad quantifiers, especially when the class distribution may vary between training and test. Hence, adjusted versions of classify count have been developed by using modified thresholds. However, previous works have explicitly discarded (without a deep analysis) any possible approach based on the probability estimations of the classifier. In this paper, we present a method based on averaging the probability estimations of a classifier with a very simple scaling that does perform reasonably well, showing that probability estimators for quantification capture a richer view of the problem than methods based on a threshold.
[classifier training, pattern classification, probability estimations, quantification, Estimation, probability, Probability, probability estimators, Data mining, machine learning, quantifier, classification, Training, Accuracy, class imbalance, Machine learning, Robustness, learning (artificial intelligence)]
Learning Collaborative Filtering and Its Application to People to People Recommendation in Social Networks
2010 IEEE International Conference on Data Mining
None
2010
Predicting people who other people may like has recently become an important task in many online social networks. Traditional collaborative filtering (CF) approaches are popular in recommender systems to effectively predict user preferences for items. One major problem in CF is computing similarity between users or items. Traditional CF methods often use heuristic methods to combine the ratings given to an item by similar users, which may not reflect the characteristics of the active user and can give unsatisfactory performance. In contrast to heuristic approaches we have developed CollabNet, a novel algorithm that uses gradient descent to learn the relative contributions of similar users or items to the ranking of recommendations produced by a recommender system, using weights to represent the contributions of similar users for each active user. We have applied CollabNet to the challenging problem of people to people recommendation in social networks, where people have a dual role as both "users" and "items\
[collaborative filtering, Recommender Systems, Collaborative Filtering, Filtering, Social network services, Computational modeling, data mining, Data Mining, information filtering, learning, datasets, CollabNet, Machine Learning, Training, Strontium, social network, recommender systems, recommender system, Collaboration, Machine learning, groupware, social networking (online), learning (artificial intelligence)]
Approximation of Frequentness Probability of Itemsets in Uncertain Data
2010 IEEE International Conference on Data Mining
None
2010
Mining frequent item sets from transactional datasets is a well known problem with good algorithmic solutions. Most of these algorithms assume that the input data is free from errors. Real data, however, is often affected by noise. Such noise can be represented by uncertain datasets in which each item has an existence probability. Recently, Bernecker et al. (2009) proposed the frequentness probability, i.e., the probability that a given item set is frequent, to select item sets in an uncertain database. A dynamic programming approach to evaluate this measure was given as well. We argue, however, that for the setting of Bernecker et al. (2009), that assumes independence between the items, already well-known statistical tools exist. We show how the frequentness probability can be approximated extremely accurately using a form of the central limit theorem. We experimentally evaluated our approximation and compared it to the dynamic programming approach. The evaluation shows that our approximation method is extremely accurate even for very small databases while at the same time it has much lower memory overhead and computation time.
[transaction processing, approximation theory, frequentness probability approximation, data mining, Probability, dynamic programming, Probabilistic logic, Approximation methods, Data mining, statistical distributions, frequent itemset mining, transactional dataset, central limit theorem, Itemsets, uncertain data, Dynamic programming, statistical tool]
On Finding Frequent Patterns in Event Sequences
2010 IEEE International Conference on Data Mining
None
2010
Given a directed a cyclic graph with labeled vertices, we consider the problem of finding the most common label sequences ("traces") among all paths in the graph (of some maximum length m). Since the number of paths can be huge, we propose novel algorithms whose time complexity depends only on the size of the graph, and on the frequency \\varepsilon of the most frequent traces. In addition, we apply techniques from streaming algorithms to achieve space usage that depends only on \\varepsilon, and not on the number of distinct traces. The abstract problem considered models a variety of tasks concerning finding frequent patterns in event sequences. Our motivation comes from working with a data set of 2 million RFID readings from baggage trolleys at Copenhagen Airport. The question of finding frequent passenger movement patterns is mapped to the above problem. We report on experimental findings for this data set.
[algorithms, abstract problem, Error probability, radiofrequency identification, frequent patterns, data mining, label sequences, directed acyclic graph, frequent traces, Complexity theory, Data mining, sequences, event sequences, graphs, passenger movement patterns, Copenhagen Airport, sampling, patterns discovery, time complexity, baggage trolleys, Time frequency analysis, Airports, streaming algorithms, pattern mapping, pattern clustering, directed graphs, Antennas, RFID readings, Radiofrequency identification]
Active Improvement of Hierarchical Object Features under Budget Constraints
2010 IEEE International Conference on Data Mining
None
2010
When we think of an object in a supervised learning setting, we usually perceive it as a collection of fixed attribute values. Although this setting may be suited well for many classification tasks, we propose a new object representation and therewith a new challenge in data mining: an object is no longer described by one set of attributes but is represented in a hierarchy of attribute sets in different levels of quality. Obtaining a more detailed representation of an object comes with a cost. This raises the interesting question of which objects we want to enhance under a given budget and cost model. This new setting is very useful whenever resources like computing power, memory or time are limited. We propose a new Active Adaptive Algorithm (AAA) to improve objects in an iterative fashion. We demonstrate how to create a hierarchical object representation and prove the effectiveness of our new selection algorithm on these datasets.
[pattern classification, active adaptive algorithm, Humans, data mining, supervised learning, budget constraint, object detection, classification task, Support vector machines, Training, hierarchical object feature, Accuracy, feature extraction, object representation, Pattern classification, Machine learning, Active vision, Labeling, learning (artificial intelligence), Testing]
Pseudo Conditional Random Fields: Joint Training Approach to Segmenting and Labeling Sequence Data
2010 IEEE International Conference on Data Mining
None
2010
Cascaded approach has been used for a long time to conduct sub-tasks in order to accomplish a major task. We put cascaded approach in a probabilistic framework and analyze possible reasons for cascaded errors. To reduce the occurrence of cascaded errors, we need to add a constraint when performing joint training. We suggest a pseudo Conditional Random Field (pseudo-CRF) approach that models two sub-tasks as two Conditional Random Fields (CRFs). We then present the formulation in the context of a linear chain CRF for solving problems on sequence data. In conducting joint training for a pseudo-CRF, we reuse all existing well-developed efficient inference algorithms for a linear chain CRF, which would otherwise require the use of approximate inference algorithms or simulations that involve long computational time. Our experimental results show an interesting fact that a jointly trained CRF model in a pseudo-CRF may perform worse than a separately trained CRF on a sub-task. However the overall system performance of a pseudo-CRF would outperform that of a cascaded approach. We implement the implicit constraint in the form of a soft constraint such that users can define the penalty cost for violating the constraint. In order to work on large-scale datasets, we further suggest a parallel implementation of the pseudo-CRF approach, which can be implemented on a multi-core CPU or GPU on a graphics card that supports multi-threading. Our experimental results show that it can achieve a 12 times increase in speedup.
[inference algorithms, parallel algorithms, sequence data segmention, data mining, Artificial neural networks, random processes, Sequence Labeling Problem, Noun-phrase Chunking, Cascaded Approach, inference mechanisms, GPU, Training, Accuracy, sequence data labeling, pseudo conditional random field, CRF, Markov processes, Approximation algorithms, Inference algorithms, multithreading, Joints, multicore CPU, Joint Training]
Location and Scatter Matching for Dataset Shift in Text Mining
2010 IEEE International Conference on Data Mining
None
2010
Dataset shift from the training data in a source domain to the data in a target domain poses a great challenge for many statistical learning methods. Most algorithms can be viewed as exploiting only the first-order statistics, namely, the empirical mean discrepancy to evaluate the distribution gap. Intuitively, considering only the empirical mean may not be statistically efficient. In this paper, we propose a non-parametric distance metric with a good property which jointly considers the empirical mean (Location) and sample covariance (Scatter) difference. More specifically, we propose an improved symmetric Stein's loss function which combines the mean and covariance discrepancy into a unified Bregman matrix divergence of which Jensen-Shannon divergence between normal distributions is a particular case. Our target is to find a good feature representation which can reduce the distribution gap between different domains, at the same time, ensure that the new derived representation can encode most discriminative components with respect to the label information. We have conducted extensive experiments on several document classification datasets to demonstrate the effectiveness of our proposed method.
[Measurement, covariance discrepancy, feature representation, Adaptation model, data mining, Domain Adaptation, location matching, Feature Extraction, Bregman matrix divergence, sample covariance difference, Jensen Shannon divergence, Training, feature extraction, Training data, text mining, empirical mean discrepancy, Kernel, Testing, dataset shift, statistical learning, scatter matching, Covariance matrix, symmetric Stein loss function, covariance analysis, nonparametric distance metric, nonparametric statistics, distribution gap]
Learning Preferences with Millions of Parameters by Enforcing Sparsity
2010 IEEE International Conference on Data Mining
None
2010
We study the retrieval task that ranks a set of objects for a given query in the pair wise preference learning framework. Recently researchers found out that raw features (e.g. words for text retrieval) and their pair wise features which describe relationships between two raw features (e.g. word synonymy or polysemy) could greatly improve the retrieval precision. However, most existing methods can not scale up to problems with many raw features (e.g. English vocabulary), due to the prohibitive computational cost on learning and the memory requirement to store a quadratic number of parameters. In this paper, we propose to learn a sparse representation of the pair wise features under the preference learning framework using the L1 regularization. Based on stochastic gradient descent, an online algorithm is devised to enforce the sparsity using a mini-batch shrinkage strategy. On multiple benchmark datasets, we show that our method achieves better performance with fast convergence, and takes much less memory on models with millions of parameters.
[text analysis, Dictionaries, Error analysis, mini-batch shrinkage strategy, Stochastic processes, data mining, retrieval task, Sparse matrices, preference learning, Training, query processing, stochastic gradient descent algorithm, Semantics, feature extraction, multiple benchmark dataset, text mining, learning (artificial intelligence), stochastic processes, gradient methods, sparse model, sparse representation, online algorithm, Memory management, pairwise preference learning, L1 regularization, raw feature, object set, learning to rank, pairwise feature, online learning]
QMAS: Querying, Mining and Summarization of Multi-modal Databases
2010 IEEE International Conference on Data Mining
None
2010
Given a large collection of images, very few of which have labels, how can we guess the labels of the remaining majority, and how can we spot those images that need brand new labels, different from the existing ones? Current automatic labeling techniques usually scale super linearly with the data size, and/or they fail when only a tiny amount of labeled data is provided. In this paper, we propose QMAS (Querying, Mining And Summarization of Multi-modal Databases), a fast solution to the following problems: (i) low-labor labeling (L3) - given a collection of images, very few of which are labeled with keywords, find the most suitable labels for the remaining ones, and (ii) mining and attention routing - in the same setting, find clusters, the top-NO outlier images, and the top-NR representative images. We report experiments on real satellite images, two large sets (1.5 GB and 2.25 GB) of proprietary images and a smaller set (17 MB) of public images. We show that QMAS scales linearly with the data size, being up to 40 times faster than top competitors (GCap), obtaining better or equal accuracy. In contrast to other methods, QMAS does low-labor labeling (L3), that is, it works even with tiny initial label sets. It also solves both presented problems and spots tiles that potentially require new labels.
[image collection, labeling technique, low labor labeling, data mining, visual databases, multimodal databases querying, QMAS, Data mining, multimodal databases mining, Clustering, Summarization and Multi-modal databases, query processing, public image, Satellites, Tiles, Clustering algorithms, Automatic labeling, Feature extraction, multimodal databases summarization, Labeling, satellite image, Pixel, image recognition]
Block-GP: Scalable Gaussian Process Regression for Multimodal Data
2010 IEEE International Conference on Data Mining
None
2010
Regression problems on massive data sets are ubiquitous in many application domains including the Internet, earth and space sciences, and finances. In many cases, regression algorithms such as linear regression or neural networks attempt to fit the target variable as a function of the input variables without regard to the underlying joint distribution of the variables. As a result, these global models are not sensitive to variations in the local structure of the input space. Several algorithms, including the mixture of experts model, classification and regression trees (CART), and others have been developed, motivated by the fact that a variability in the local distribution of inputs may be reflective of a significant change in the target variable. While these methods can handle the non-stationarity in the relationships to varying degrees, they are often not scalable and, therefore, not used in large scale data mining applications. In this paper we develop Block-GP, a Gaussian Process regression framework for multimodal data, that can be an order of magnitude more scalable than existing state-of-the-art nonlinear regression algorithms. The framework builds local Gaussian Processes on semantically meaningful partitions of the data and provides higher prediction accuracy than a single global model with very high confidence. The method relies on approximating the covariance matrix of the entire input space by smaller covariance matrices that can be modeled independently, and can therefore be parallelized for faster execution. Theoretical analysis and empirical studies on various synthetic and real data sets show high accuracy and scalability of Block-GP compared to existing nonlinear regression techniques.
[parallel computation, earth sciences, regression analysis, multimodal data, Approximation methods, covariance matrix, Training, Accuracy, scalable Gaussian process regression, very large databases, semantically meaningful partitions, mixture of experts model, Gaussian Process, Covariance matrix, Partitioning algorithms, state-of-the-art nonlinear regression algorithms, pattern clustering, Block-GP, space sciences, regression, Gaussian processes, Data models, Internet, finances, classification and regression trees]
Active Learning with Human-Like Noisy Oracle
2010 IEEE International Conference on Data Mining
None
2010
When active learning is applied to real-world applications, human experts usually act as oracles to provide labels. However, human make mistakes, thus noise might be introduced during the learning process. Most previous studies simplify the problem by assuming uniformly-distributed noise over the sample space. Such assumption, however, might fail to precisely reflect the human experts' behaviour in real-world situations. In this paper, we therefore study active learning with such human-like oracles, by making a more realistic assumption that the noise is example-dependent (i.e., non-uniformly distributed over the sample space). More specifically, when the human-like oracle is highly confident in labelling examples, it is naturally less likely to provide incorrect answers, whereas when such confidence is low, the noise would be more likely to be introduced. Based on the analysis of such human-like oracle, we propose a generic yet simple active learning algorithm to simultaneously explore the unlabelled data and exploit the labelled data. Empirical study on both synthetic and real-world data sets verifies the superiority of the proposed algorithm, compared with the traditional uncertainty sampling.
[unlabelled data, Uncertainty, sampling methods, oracle, Noise, Humans, data mining, uncertainty sampling, Noise measurement, Training, active learning, oracles, labelled data, human experts, noise, Labeling, Mathematical model, learning (artificial intelligence)]
Monotone Relabeling in Ordinal Classification
2010 IEEE International Conference on Data Mining
None
2010
In many applications of data mining we know beforehand that the response variable should be increasing (or decreasing) in the attributes. Such relations between response and attributes are called monotone. In this paper we present a new algorithm to compute an optimal monotone classification of a data set for convex loss functions. Moreover, we show how the algorithm can be extended to compute all optimal monotone classifications with little additional effort. Monotone relabeling is useful for at least two reasons. Firstly, models trained on relabeled data sets often have better predictive performance than models trained on the original data. Secondly, relabeling is an important building block for the construction of monotone classifiers. We apply the new algorithm to investigate the effect on the prediction error of relabeling the training sample for k nearest neighbour classification and classification trees. In contrast to previous work in this area, we consider all optimal monotone relabelings. The results show that, for small training samples, relabeling the training data results in significantly better predictive performance.
[k nearest neighbour classification, pattern classification, Computational modeling, data mining, monotone classification, convex loss functions, Predictive models, monotone relabeling, Data mining, Training, Upper bound, ordinal classification, Prediction algorithms, Data models, learning (artificial intelligence), classification trees]
The Effect of History on Modeling Systems' Performance: The Problem of the Demanding Lord
2010 IEEE International Conference on Data Mining
None
2010
In several concept attainment systems, ranging from recommendation systems to information filtering, a sliding window of learning instances has been used in the learning process to allow the learner to follow concepts that change over time. However, no analytic study has been performed on the relation between the size of the sliding window and the performance of a learning system. In this work, we present such an analytic model that describes the effect of the sliding window size on the prediction performance of a learning system based on iterative feedback. Using a signal-to-noise approach to model the learning ability of the underlying machine learning algorithms, we can provide good estimates of the average performance of a modeling system independently of the supervised machine learning algorithm employed. We experimentally validate the effectiveness of the proposed methodology with detailed experiments using synthetic and real datasets, and a variety of learning algorithms, including Support Vector Machines, Naive Bayes, Nearest Neighbor and Decision Trees. The results validate the analysis and indicate very good estimation performance in different settings.
[iterative methods, Correlation, Noise, iterative feedback, nearest neighbor algorithm, user modeling, information filtering, signal-to-noise approach, adaptive learning, recommendation system, Learning systems, Training, Decision trees, learning (artificial intelligence), support vector machines, learning ability, Estimation, learning system, sliding window, demanding lord problem, recommender systems, support vector machine, concept drift, Machine learning, decision trees, naive Bayes, Bayes methods, machine learning algorithm]
Resilient K-d Trees: K-Means in Space Revisited
2010 IEEE International Conference on Data Mining
None
2010
We develop a k-d tree variant that is resilient to a pre-described number of memory corruptions while still using only linear space. We show how to use this data structure in the context of clustering in high-radiation environments and demonstrate that our approach leads to a significantly higher resiliency rate compared to previous results.
[linear space, revisited space, data analysis, resilient k-d tree, Buildings, Random access memory, data structure, k-d tree, resilient algorithm, Indexes, Runtime, pattern clustering, Clustering algorithms, Filtering algorithms, k-means, memory corruption, high radiation environment, clustering, tree data structures, Arrays, resiliency rate]
Advertising Campaigns Management: Should We Be Greedy?
2010 IEEE International Conference on Data Mining
None
2010
We consider the problem of displaying advertisements on web pages in the "cost per click" model, which necessitates to learn the appeal of visitors for the different advertisements in order to maximize the revenue. In a realistic context, the advertisements have constraints such as a certain number of clicks to draw, as well as a lifetime. This problem is thus inherently dynamic, and intimately combines combinatorial and statistical issues. To set the stage, it is also noteworthy that we deal with very rare events of interest, since the base probability of one click is in the order of 10-4. We introduce an adaptive policy learning algorithm based on linear programming, and investigate its performance through simulations on a realistic model designed with an important commercial web actor.
[Uncertainty, statistical issue, Biological system modeling, cost per click model, CTR estimation, linear programming, advertising, combinatorial issue, Exploration/exploitation trade-off, Optimization, advertising campaigns management, adaptive policy learning algorithm, Non-stationary setting, commercial Web actor, Web page, Planning, Random variables, Advertisement selection, Resource management, learning (artificial intelligence), Web sites, Advertising, Linear Programming]
Accelerating Radius-Margin Parameter Selection for SVMs Using Geometric Bounds
2010 IEEE International Conference on Data Mining
None
2010
By considering the geometric properties of the Support Vector Machine (SVM) and Minimal Enclosing Ball (MEB) optimization problems, we show that upper and lower bounds on the radius-margin ratio of an SVM can be efficiently computed at any point during training. We use these bounds to accelerate radius-margin parameter selection by terminating training routines as early as possible, while still obtaining a guarantee that the parameters minimize the radius-margin ratio. Once an SVM has been partially trained on any set of parameters, we also show that these bounds can be used to evaluate and possibly reject neighboring parameter values with little or no additional training required. Empirical results show that, when selecting two parameter values, this process can reduce the number of training iterations required by a factor of 10 or more, while suffering no loss of precision in minimizing the radius-margin ratio.
[Heart, iterative methods, support vector machines, computational geometry, SVM, training iteration, parameter selection, Support vector machines, Training, geometric bound, Accuracy, optimisation, support vector machine, training routine, minimal enclosing ball optimization problem, radius-margin parameter selection, parameter estimation, Polynomials, Diabetes, Kernel, MEB optimization problem]
Enhancing Single-Objective Projective Clustering Ensembles
2010 IEEE International Conference on Data Mining
None
2010
Projective Clustering Ensembles (PCE) has recently been formulated to solve the problem of deriving a robust projective consensus clustering from an ensemble of projective clustering solutions. PCE is formalized as an optimization problem with either a two-objective or a single-objective function, depending on whether the object-based and the feature-based representations of the clusters in the ensemble are treated separately. A major result in is that single-objective PCE outperforms two-objective PCE in terms of efficiency, at the cost of lower accuracy in consensus clustering. In this paper, we enhance the single-objective PCE formulation, with the ultimate goal of providing more effective formulations capable of reducing the accuracy gap with the two-objective counterpart, while maintaining the efficiency advantages. We provide theoretical insights into the single-objective function, and introduce two heuristics that overcome the major limitations of the previous single-objective PCE formulation. Experimental evidence has demonstrated the significance of our proposed heuristics. In fact, results have not only confirmed a far better efficiency w.r.t. two-objective PCE, but have also shown the claimed improvements in accuracy of the consensus clustering obtained by the new single-objective PCE.
[projective clustering ensembles, Heuristic algorithms, Cognition, Data mining, Optimization, Equations, Accuracy, optimisation, PCE, pattern clustering, optimization, Clustering algorithms, robust projective consensus clustering, objective function]
Minimizing the Variance of Cluster Mixture Models for Clustering Uncertain Objects
2010 IEEE International Conference on Data Mining
None
2010
The increasing demand for dealing with uncertainty in data has led to the development of effective and efficient approaches in the data management and mining contexts. Clustering uncertain data objects has particularly attracted great attention in the data mining community. Most existing clustering methods however have urgently to come up with a number of issues, some of which are related to a poor efficiency mainly due to an expensive computation of the distance between uncertain objects. In this work, we propose a novel formulation to the problem of clustering uncertain objects, which allows for reaching accurate solutions by minimizing the variance of the mixture models that represent the clusters to be identified. We define a heuristic, MMVar, which exploits some analytical properties about the computation of variance for mixture models to compute local minima of the objective function at the basis of the proposed formulation. This characteristic allows MMVar to discard any distance measure between uncertain objects and, therefore, to achieve high efficiency. Experiments have shown that MMVar outperforms state-of-the-art algorithms from an efficiency viewpoint, while achieving better average performance in terms of accuracy.
[Uncertainty, data uncertainty, data management, Computational modeling, data mining, MMVar, cluster mixture model, uncertainty handling, Accuracy, pattern clustering, Measurement uncertainty, Clustering algorithms, Prototypes, Data models, uncertain data object clustering]
Subspace Clustering Meets Dense Subgraph Mining: A Synthesis of Two Paradigms
2010 IEEE International Conference on Data Mining
None
2010
Today's applications deal with multiple types of information: graph data to represent the relations between objects and attribute data to characterize single objects. Analyzing both data sources simultaneously can increase the quality of mining methods. Recently, combined clustering approaches were introduced, which detect densely connected node sets within one large graph that also show high similarity according to all of their attribute values. However, for attribute data it is known that this full-space clustering often leads to poor clustering results. Thus, subspace clustering was introduced to identify locally relevant subsets of attributes for each cluster. In this work, we propose a method for finding homogeneous groups by joining the paradigms of subspace clustering and dense sub graph mining, i.e. we determine sets of nodes that show high similarity in subsets of their dimensions and that are as well densely connected within the given graph. Our twofold clusters are optimized according to their density, size, and number of relevant dimensions. Our developed redundancy model confines the clustering to a manageable size of only the most interesting clusters. We introduce the algorithm Gamer for the efficient calculation of our clustering. In thorough experiments on synthetic and real world data we show that Gamer achieves low runtimes and high clustering qualities.
[dense subgraph mining, data attribute, Redundancy, Noise, graph theory, data mining, graph data, Sensor phenomena and characterization, Data mining, redundancy removal, optimisation, pattern clustering, subspace clustering, combined clustering approach, optimization, Clustering algorithms, attribute data, Games, redundancy]
Multi-stream Join Answering for Mining Significant Cross-Stream Correlations
2010 IEEE International Conference on Data Mining
None
2010
Sliding-window multi-stream join (SWMJ) is a fundamental operation for correlating information from different streams. We provide a solution to the problem of assessing significance of the SWMJ result by focusing on the relative frequency of windows satisfying a given equijoin predicate as the most important parameter of the SWMJ result. In particular, we derive an analytic formula for computing the average relative frequency of windows satisfying a given equijoin predicate that can be evaluated in quadratic time in the window size given a probabilistic model of the multi-stream. In experiments we demonstrated remarkable accuracy of our method, which confirmed our theoretical analysis.
[Correlation, data Mining, Computational modeling, cross stream correlation, data mining, probability, Probabilistic logic, multistream join answering, Data mining, probabilistic model, query processing, Analytical models, Markov processes, quadratic time, sliding window multistream join, Monitoring]
Category Mining by Heterogeneous Data Fusion Using PdLSI Model in a Retail Service
2010 IEEE International Conference on Data Mining
None
2010
This paper describes an appropriate category discovery method that simultaneously involves a customer's lifestyle category and item category for the sustainable management of retail services, designated as ``category mining''. Category mining is realized using a large-scale ID-POS data and customer's questionnaire responses with respect to their lifestyle. For the heterogeneous data fusion, we propose a probabilistic double-latent semantic indexing (PdLSI) model that is an extension of PLSI model. In the PdLSI model, customers and items are classified probabilistically into some latent lifestyle categories and latent item category. Then, understanding of relation between the latent categories and various purchased situations is realized using Bayesian network modeling. This method provides useful knowledge based on a large-scale data for efficient customer relationship management and category management, and can be applicable for other service industries.
[Correlation, data mining, item classification, sensor fusion, Data mining, customer relationship management, latent lifestyle category, heterogeneous data fusion, retail service industries, Marketing and sales, category management, Mathematical model, belief networks, Bayesian network, retail data processing, large scale ID POS data, large-scale ID-POS data, latent item category, Biological system modeling, topic model, probability, Probabilistic logic, category discovery, category mining, service engineering, Bayesian methods, service industries, probabilistic double latent semantic indexing method]
Content-Based Methods for Predicting Web-Site Demographic Attributes
2010 IEEE International Conference on Data Mining
None
2010
Demographic information plays an important role in gaining valuable insights about a web-site's user-base and is used extensively to target online advertisements and promotions. This paper investigates machine-learning approaches for predicting the demographic attributes of web-sites using information derived from their content and their hyper linked structure and not relying on any information directly or indirectly obtained from the web-site's users. Such methods are important because users are becoming increasingly more concerned about sharing their personal and behavioral information on the Internet. Regression-based approaches are developed and studied for predicting demographic attributes that utilize different content-derived features, different ways of building the prediction models, and different ways of aggregating web-page level predictions that take into account the web's hyper linked structure. In addition, a matrix-approximation based approach is developed for coupling the predictions of individual regression models into a model designed to predict the probability mass function of the attribute. Extensive experiments show that these methods are able to achieve an RMSE of 8-10% and provide insights on how to best train and apply such models.
[regression analysis, Web hyperlinked structure, Predictive models, Probability Mass Function, Probability distribution, HTML, History, content based method, matrix-approximation based approach, Training, Web site demographic attributes prediction, Web-page level predictions, Demographic Attribute Prediction, demography, probability mass function, learning (artificial intelligence), online advertisements, Testing, approximation theory, advertising data processing, Content Based Models, Regression, content-based retrieval, matrix algebra, regression based approach, machine-learning approaches, hyperlinked structure, Feature extraction, Internet, Inlink Count, Web sites]
Discrimination Aware Decision Tree Learning
2010 IEEE International Conference on Data Mining
None
2010
Recently, the following discrimination aware classification problem was introduced: given a labeled dataset and an attribute B, find a classifier with high predictive accuracy that at the same time does not discriminate on the basis of the given attribute B. This problem is motivated by the fact that often available historic data is biased due to discrimination, e.g., when B denotes ethnicity. Using the standard learners on this data may lead to wrongfully biased classifiers, even if the attribute B is removed from training data. Existing solutions for this problem consist in &#x201C;cleaning away&#x201D; the discrimination from the dataset before a classifier is learned. In this paper we study an alternative approach in which the non-discrimination constraint is pushed deeply into a decision tree learner by changing its splitting criterion and pruning strategy. Experimental evaluation shows that the proposed approach advances the state-of-the-art in the sense that the learned decision trees have a lower discrimination than models provided by previous methods, with little loss in accuracy.
[Economics, pattern classification, Biological system modeling, splitting criterion, data mining, Data Mining, Cleaning, Data mining, decision tree learning, Accuracy, Training data, decision making, decision trees, pruning strategy, Classification, Discrimination Aware Data Mining, discrimination aware classification, Decision trees, learning (artificial intelligence)]
Patterns on the Connected Components of Terabyte-Scale Graphs
2010 IEEE International Conference on Data Mining
None
2010
How do connected components evolve? What are the regularities that govern the dynamic growth process and the static snapshot of the connected components? In this work, we study patterns in connected components of large, real-world graphs. First, we study one of the largest static Web graphs with billions of nodes and edges and analyze the regularities among the connected components using GFD(Graph Fractal Dimension) as our main tool. Second, we study several time evolving graphs and find dynamic patterns and rules that govern the dynamics of connected components. We analyze the growth rates of top connected components and study their relation over time. We also study the probability that a newcomer absorbs to disconnected components as a function of the current portion of the disconnected components and the degree of the newcomer. Finally, we propose a generative model that explains both the dynamic growth process and the static regularities of connected components.
[Graph Mining, Patents, connected components, dynamic growth process, graph theory, data mining, probability, Artificial neural networks, Fractals, static regularities, Evolution of Connected Components, CommunityConnection Model, fractals, real-world graphs, graph fractal dimension, terabyte scale graph, Web pages, Feature extraction, Internet, static Web graphs, generative model, Mathematical model, IEEE Potentials]
Attribution of Conversion Events to Multi-channel Media
2010 IEEE International Conference on Data Mining
None
2010
This paper presents a practical method for measuring the impact of multiple marketing events on sales, including marketing events that are not traditionally trackable. The technique infers which of several competing media events are likely to have caused a given conversion. We test the method using hold-out sets, and also a live media experiment in which we test whether the method can accurately predict television-generated web conversions.
[Geography, TV, advertising data processing, credit assignment, Atmospheric modeling, marketing event, Media, multimedia systems, multichannel media, advertising, media events, attribution, conversion event, Advertising, Manganese]
Mining Public Transport Usage for Personalised Intelligent Transport Systems
2010 IEEE International Conference on Data Mining
None
2010
Traveller information, route planning, and service updates have become essential components of public transport systems: they help people navigate built environments by providing access to information regarding delays and service disruptions. However, one aspect that these systems lack is a way of tailoring the information they offer in order to provide personalised trip time estimates and relevant notifications to each traveller. Mining each user's travel history, collected by automated ticketing systems, has the potential to address this gap. In this work, we analyse one such dataset of travel history on the London underground. We then propose and evaluate methods to (a) predict personalised trip times for the system users and (b) rank stations based on future mobility patterns, in order to identify the subset of stations that are of greatest interest to the user and thus provide useful travel updates.
[Personalization, data mining, public transport systems, Electron tubes, History, Data mining, personalised trip times, route planning, Frequency control, traffic information systems, London underground, service disruptions, travel history, Intelligent Transport Systems, Context, personalised intelligent transport systems, Estimation, public information systems, mobility patterns, service updates, public administration, rapid transit systems, automated ticketing systems, traveller information, Context modeling]
Micro-blogging Sentiment Detection by Collaborative Online Learning
2010 IEEE International Conference on Data Mining
None
2010
We study the online micro-blog sentiment detection problem, which aims to determine whether a micro-blog post expresses emotions. This problem is challenging because a micro-blog post is very short and individuals have distinct ways of expressing emotions. A single classification model trained on the entire corpus may fail to capture characteristics unique to each user. On the other hand, a personalized model for each user may be inaccurate due to the scarcity of training data, especially at the very beginning where users have just posted a few entries. To overcome these challenges, we propose learning a global model over all micro-bloggers, which is then leveraged to continuously refine the individual models through a collaborative online learning way. We evaluate our algorithm on a real-life micro-blog dataset collected from the popular micro-blog site - Twitter. Results show that our algorithm is effective and efficient for timely sentiment detection in real micro-blogging applications.
[Algorithm design and analysis, pattern classification, mining methods and algorithms, Error analysis, Heuristic algorithms, Adaptation model, data mining, online microblog sentiment detection problem, Twitter, real life microblog dataset, classification, distance learning, Training, microblog site, microblog post, Collaboration, groupware, training data, computer aided instruction, Web sites, collaborative online learning]
Enforcing Vocabulary k-Anonymity by Semantic Similarity Based Clustering
2010 IEEE International Conference on Data Mining
None
2010
Web query logs provide a rich wealth of information, but also present serious privacy risks. We consider publishing vocabularies, bags of query-terms extracted from web query logs, which has a variety of applications. We aim at preventing identity disclosure of such bag-valued data. The key feature of such data is the extreme sparsity, which renders conventional anonymization techniques not working well in retaining enough utility. We propose a semantic similarity based clustering approach to address the issue. We measure the semantic similarity between two vocabularies by a weighted bipartite matching and present a greedy algorithm to cluster vocabularies by the semantic similarities. Extensive experiments on the AOL query log show that our approach retains more data utility than existing approaches.
[Measurement, cluster vocabulary k-anonymity, Vocabulary, Taxonomy, privacy, Data mining, Web query log, query processing, Privacy, anonymization technique, vocabulary, Semantics, bag valued data, greedy algorithm, publishing, greedy algorithms, Footwear, Anonymity, semantic similarity based clustering, weighted bipartite matching, web query logs, pattern clustering, publishing vocabulary, query-term extraction, AOL query log, data utility, Internet, bag-valued data]
Efficient Probabilistic Latent Semantic Analysis with Sparsity Control
2010 IEEE International Conference on Data Mining
None
2010
Probabilistic latent semantic analysis is a topic modeling technique to discover the hidden structure in binary and count data. As a mixture model, it performs a probabilistic mixture decomposition on the co-occurrence matrix, which produces two matrices assigned with probabilistic explanations. However, the factorized matrices may be rather smooth, which means we may obtain global feature and topic representations rather than expected local ones. To resolve this problem, one of the solutions is to revise the decomposition process with considerations of sparsity. In this paper, we present an approach that provides direct control over sparsity during the expectation maximization process. Furthermore, by using the log penalty function as sparsity measurement instead of the widely used L2 norm, we can approximate the re-estimation of parameters in linear time, as same as original PLSA does, while many other approaches require much more time. Experiments on face databases are reported to show visual representations on obtaining local features, and detailed improvements in clustering tasks compared with the original process.
[factorized matrices, probabilistic latent semantic analysis, information retrieval, visual databases, cooccurrence matrix, Probabilistic logic, topic modeling, Encoding, sparsity, matrix decomposition, Matrix decomposition, probabilistic mixture decomposition, Equations, unsupervised learning, Accuracy, Databases, data-adaptive representations, sparsity control, Mathematical model, learning (artificial intelligence), expectation maximization process, sparse matrices, plsa, opic model]
Understanding of Internal Clustering Validation Measures
2010 IEEE International Conference on Data Mining
None
2010
Clustering validation has long been recognized as one of the vital issues essential to the success of clustering applications. In general, clustering validation can be categorized into two classes, external clustering validation and internal clustering validation. In this paper, we focus on internal clustering validation and present a detailed study of 11 widely used internal clustering validation measures for crisp clustering. From five conventional aspects of clustering, we investigate their validation properties. Experiment results show that S_Dbw is the only internal validation measure which performs well in all five aspects, while other measures have certain limitations in different application scenarios.
[Economics, crisp clustering, Noise, data mining, internal clustering validation, Indexes, Noise measurement, unsupervised learning, S_Dbw, Current measurement, pattern clustering, Clustering algorithms, clustering application, Elbow]
Transfer Learning via Cluster Correspondence Inference
2010 IEEE International Conference on Data Mining
None
2010
Transfer learning targets to leverage knowledge from one domain for tasks in a new domain. It finds abundant applications, such as text/sentiment classification. Many previous works are based on cluster analysis, which assume some common clusters shared by both domains. They mainly focus on the one-to-one cluster correspondence to bridge different domains. However, such a correspondence scheme might be too strong for real applications where each cluster in one domain corresponds to many clusters in the other domain. In this paper, we propose a Cluster Correspondence Inference (CCI) method to iteratively infer many-to-many correspondence among clusters from different domains. Specifically, word clusters and document clusters are exploited for each domain using nonnegative matrix factorization, then the word clusters from different domains are corresponded in a many-to-many scheme, with the help of shared word space as a bridge. These two steps are run iteratively and label information is transferred from source domain to target domain through the inferred cluster correspondence. Experiments on various real data sets demonstrate that our method outperforms several state-of-the-art approaches for cross-domain text classification.
[pattern classification, text analysis, transfer learning, Transfer Learning, cluster correspondence inference, sentiment classification, matrix decomposition, text classification, inference mechanisms, many-to-many scheme, nonnegative matrix factorization, Cluster Correspondence Inference, Optimization, Convergence, Bridges, Support vector machines, one-to-one scheme, Accuracy, pattern clustering, Text Classification, Clustering algorithms, learning (artificial intelligence), cluster analysis, Joints]
Supervised Link Prediction Using Multiple Sources
2010 IEEE International Conference on Data Mining
None
2010
Link prediction is a fundamental problem in social network analysis and modern-day commercial applications such as Face book and My space. Most existing research approaches this problem by exploring the topological structure of a social network using only one source of information. However, in many application domains, in addition to the social network of interest, there are a number of auxiliary social networks and/or derived proximity networks available. The contribution of the paper is twofold: (1) a supervised learning framework that can effectively and efficiently learn the dynamics of social networks in the presence of auxiliary networks, (2) a feature design scheme for constructing a rich variety of path-based features using multiple sources, and an effective feature selection strategy based on structured sparsity. Extensive experiments on three real-world collaboration networks show that our model can effectively learn to predict new links using multiple sources, yielding higher prediction accuracy than unsupervised and single-source supervised models.
[Social network services, supervised learning, Color, Predictive models, Probability, collaboration networks, supervised link prediction, social network, multiple sources, Supervised learning, Collaboration, social network analysis, link prediction, information source, social networking (online), learning (artificial intelligence), Logistics]
Addressing Concept-Evolution in Concept-Drifting Data Streams
2010 IEEE International Conference on Data Mining
None
2010
The problem of data stream classification is challenging because of many practical aspects associated with efficient processing and temporal behavior of the stream. Two such well studied aspects are infinite length and concept-drift. Since a data stream may be considered a continuous process, which is theoretically infinite in length, it is impractical to store and use all the historical data for training. Data streams also frequently experience concept-drift as a result of changes in the underlying concepts. However, another important characteristic of data streams, namely, concept-evolution is rarely addressed in the literature. Concept-evolution occurs as a result of new classes evolving in the stream. This paper addresses concept-evolution in addition to the existing challenges of infinite-length and concept-drift. In this paper, the concept-evolution phenomenon is studied, and the insights are used to construct superior novel class detection techniques. First, we propose an adaptive threshold for outlier detection, which is a vital part of novel class detection. Second, we propose a probabilistic approach for novel class detection using discrete Gini Coefficient, and prove its effectiveness both theoretically and empirically. Finally, we address the issue of simultaneous multiple novel class occurrence, and provide an elegant solution to detect more than one novel classes at the same time. We also consider feature-evolution in text data streams, which occurs because new features (i.e., words) evolve in the stream. Comparison with state-of-the-art data stream classification techniques establishes the effectiveness of the proposed approach.
[Measurement, text analysis, data stream classification, Noise, Adaptation model, feature evolution, Twitter, outlier detection, probabilistic approach, adaptive threshold, Training, concept-evolution, infinite length, data analysis, probability, discrete Gini Coefficient, outlier, Probabilistic logic, text data streams, concept drifting, concept-evolution phenomenon, novel class detection, data stream, novel class, Data models]
Sparse Boolean Matrix Factorizations
2010 IEEE International Conference on Data Mining
None
2010
Matrix factorizations are commonly used methods in data mining. When the input data is Boolean, replacing the standard matrix multiplication with Boolean matrix multiplication can yield more intuitive results. Unfortunately, finding a good Boolean decomposition is known to be computationally hard, with even many sub-problems being hard to approximate. Many real-world data sets are sparse, and it is often required that also the factor matrices are sparse. This requirement has motivated many new matrix decomposition methods and many modifications of the existing methods. This paper studies how Boolean matrix factorizations behave with sparse data: can we assume some sparsity on the factor matrices, and does the sparsity help with the computationally hard problems. The answer to these problems is shown to be positive.
[approximation theory, Noise, data mining, Matrix decompositions, Boolean rank, Linear matrix inequalities, Boolean algebra, matrix decomposition, approximation algorithms, Sparse matrices, Approximation methods, Matrix decomposition, Data mining, matrix multiplication, Approximation algorithms, sparse Boolean matrix factorizations, Boolean matrix multiplication, sparse matrices]
On the Computation of Stochastic Search Variable Selection in Linear Regression with UDFs
2010 IEEE International Conference on Data Mining
None
2010
Computing Bayesian statistics with traditional techniques is extremely slow, specially when large data has to be exported from a relational DBMS. We propose algorithms for large scale processing of stochastic search variable selection (SSVS) for linear regression that can work entirely inside a DBMS. The traditional SSVS algorithm requires multiple scans of the input data in order to compute a regression model. Due to our optimizations, SSVS can be done in either one scan over the input table for large number of records with sufficient statistics, or one scan per iteration for high-dimensional data. We consider storage layouts which efficiently exploit DBMS parallel processing of aggregate functions. Experimental results demonstrate correctness, convergence and performance of our algorithms. Finally, the algorithms show good scalability for data with a very large number of records, or a very high number of dimensions.
[iterative methods, Computational modeling, Input variables, data mining, regression analysis, linear regression, relational databases, parallel processing, Optimization, Convergence, user defined function, optimisation, Layout, Bayesian statistics, UDF, stochastic search variable selection, relational DBMS, SSVS algorithm, Data models, Bayes methods, variable selection, Mathematical model, stochastic processes, data scanning]
Data Editing Techniques to Allow the Application of Distance-Based Outlier Detection to Streams
2010 IEEE International Conference on Data Mining
None
2010
The problem of finding outliers in data has broad applications in areas as diverse as data cleaning, fraud detection, network monitoring, invasive species monitoring, etc. While there are dozens of techniques that have been proposed to solve this problem for static data collections, very simple distance-based outlier detection methods are known to be competitive or superior to more complex methods. However, distance-based methods have time and space complexities that make them impractical for streaming data and/or resource limited sensors. In this work, we show that simple data-editing techniques can make distance-based outlier detection practical for very fast streams and resource limited sensors. Our technique generalizes to produce two algorithms, which, relative to the original algorithm, can guarantee to produce no false positives, or guarantee to produce no false negatives. Our methods are independent of both data type and distance measure, and are thus broadly applicable.
[Greedy algorithms, data streaming, text editing, Data editing, distance-based outlier detection, data editing techniques, Anomaly detection, Nearest neighbor searches, Training, Sensitivity, Data stream, Databases, Approximation algorithms, data acquisition, Sensors, static data collections, resource limited sensors]
Anomaly Detection Using an Ensemble of Feature Models
2010 IEEE International Conference on Data Mining
None
2010
We present a new approach to semi-supervised anomaly detection. Given a set of training examples believed to come from the same distribution or class, the task is to learn a model that will be able to distinguish examples in the future that do not belong to the same class. Traditional approaches typically compare the position of a new data point to the set of ``normal'' training data points in a chosen representation of the feature space. For some data sets, the normal data may not have discernible positions in feature space, but do have consistent relationships among some features that fail to appear in the anomalous examples. Our approach learns to predict the values of training set features from the values of other features. After we have formed an ensemble of predictors, we apply this ensemble to new data points. To combine the contribution of each predictor in our ensemble, we have developed a novel, information-theoretic anomaly measure that our experimental results show selects against noisy and irrelevant features. Our results on 47 data sets show that for most data sets, this approach significantly improves performance over current state-of-the-art feature space distance and density-based approaches.
[feature space, semisupervised learning, data mining, Predictive models, Entropy, anomaly detection, machine learning, Training, Support vector machines, feature model, Feature extraction, Prediction algorithms, learning (artificial intelligence), Kernel, feature selection]
Assessing Data Mining Results on Matrices with Randomization
2010 IEEE International Conference on Data Mining
None
2010
Randomization is a general technique for evaluating the significance of data analysis results. In randomization-based significance testing, a result is considered to be interesting if it is unlikely to obtain as good result on random data sharing some basic properties with the original data. Recently, the randomization approach has been applied to assess data mining results on binary matrices and limited types of real-valued matrices. In these works, the row and column value distributions are approximately preserved in randomization. However, the previous approaches suffer from various technical and practical shortcomings. In this paper, we give solutions to these problems and introduce a new practical algorithm for randomizing various types of matrices while preserving the row and column value distributions more accurately. We propose a new approach for randomizing matrices containing features measured in different scales. Compared to previous work, our approach can be applied to assess data mining results on different types of real-life matrices containing dissimilar features, nominal values, non-Gaussian value distributions, missing values and sparse structure. We provide an easily usable implementation that does not need problematic manual tuning as theoretically justified parameter values are given. We perform extensive experiments on various real-life datasets showing that our approach produces reasonable results on practically all types of matrices while being easy and fast to use.
[data analysis, randomization approach, data mining, Sparse matrices, Data mining, binary matrices, Convergence, matrix algebra, randomised algorithms, Measurement uncertainty, Markov processes, Testing, Principal component analysis]
A Generalized Linear Threshold Model for Multiple Cascades
2010 IEEE International Conference on Data Mining
None
2010
This paper presents a generalized version of the linear threshold model for simulating multiple cascades on a network while allowing nodes to switch between them. The proposed model is shown to be a rapidly mixing Markov chain and the corresponding steady state distribution is used to estimate highly likely states of the cascades' spread in the network. Results on a variety of real world networks demonstrate the high quality of the estimated solution.
[multiple cascade simulation, cascading processes, Computational modeling, Social network services, graph theory, social networks, Color, Switches, steady state distribution, Steady-state, Markov chain, real world network, network diffusion, rapidly mixing markov chains, Markov processes, social networking (online), Data models, linear threshold model]
Recommending Social Events from Mobile Phone Location Data
2010 IEEE International Conference on Data Mining
None
2010
A city offers thousands of social events a day, and it is difficult for dwellers to make choices. The combination of mobile phones and recommender systems can change the way one deals with such abundance. Mobile phones with positioning technology are now widely available, making it easy for people to broadcast their whereabouts, recommender systems can now identify patterns in people's movements in order to, for example, recommend events. To do so, the system relies on having mobile users who share their attendance at a large number of social events: cold-start users, who have no location history, cannot receive recommendations. We set out to address the mobile cold-start problem by answering the following research question: how can social events be recommended to a cold-start user based only on his home location? To answer this question, we carry out a study of the relationship between preferences for social events and geography, the first of its kind in a large metropolitan area. We sample location estimations of one million mobile phone users in Greater Boston, combine the sample with social events in the same area, and infer the social events attended by 2,519 residents. Upon this data, we test a variety of algorithms for recommending social events. We find that the most effective algorithm recommends events that are popular among residents of an area. The least effective, instead, recommends events that are geographically close to the area. This last result has interesting implications for location-based services that emphasize recommending nearby events.
[Measurement, mobile phone location, positioning technology, mobile phone users, social events, mobile, Mobile communication, Mobile handsets, mobility management (mobile radio), Sparse matrices, History, location-based service, cold start user, mobile computing, recommender systems, recommender system, mobile cold start problem, location estimations, metropolitan area, Prediction algorithms, social sciences computing, web 2.0, Recommender systems, mobile handsets]
On Normalizing Fuzzy Coincidence Matrices to Compare Fuzzy and/or Possibilistic Partitions with the Rand Index
2010 IEEE International Conference on Data Mining
None
2010
Most already existing indices used to compare two strict partitions with different number of clusters are based on coincidence matrices. To extend such indices to fuzzy partitions, one can define fuzzy coincidence matrices by means of triangular norms. It has been shown this can require some kind of normalization to reinforce the corresponding indices. We propose in this paper a generic solution to perform this normalization considering the generators of the used triangular norms. Although the solution is not index-dependant, we focus on the Rand index and some of its fuzzy counterparts.
[pattern classification, Additives, Conferences, fuzzy set theory, possibilistic partitions, triangular norms, Gaussian distribution, coincidence matrix, Generators, Partitioning algorithms, Indexes, Rand index, matrix algebra, unsupervised learning, Fuzzy sets, fuzzy coincidence matrices normalization, fuzzy partition]
Financial Forecasting with Gompertz Multiple Kernel Learning
2010 IEEE International Conference on Data Mining
None
2010
Financial forecasting is the basis for budgeting activities and estimating future financing needs. Applying machine learning and data mining models to financial forecasting is both effective and efficient. Among different kinds of machine learning models, kernel methods are well accepted since they are more robust and accurate than traditional models, such as neural networks. However, learning from multiple data sources is still one of the main challenges in the financial forecasting area. In this paper, we focus on applying the multiple kernel learning models to the multiple major international stock indexes. Our experiment results indicate that applying multiple kernel learning to the financial forecasting problem suffers from both the short training period problem and non-stationary problem. Therefore we propose a novel multiple kernel learning model to address the challenge by introducing the Gompertz model and considering a non-linear combination of different kernel matrices. The experiment results show that our Gompertz multiple kernel learning model addresses the challenges and achieves better performance than the original multiple kernel learning model and single SVM models.
[support vector machines, Biological system modeling, data mining, gompertz multiple kernel learning, Predictive models, non-linear kernel combination, financial forecasting, Indexes, machine learning, Forecasting, training period problem, budgeting, Support vector machines, SVM model, nonstationary problem, multiple kernel learning, kernel matrix, Data models, economic forecasting, learning (artificial intelligence), Kernel, stock index]
Leveraging D-Separation for Relational Data Sets
2010 IEEE International Conference on Data Mining
None
2010
Testing for marginal and conditional independence is a common task in machine learning and knowledge discovery applications. Prior work has demonstrated that conventional independence tests suffer from dramatically increased rates of Type I errors when naively applied to relational data. We use graphical models to specify the conditions under which these errors occur, and use those models to devise novel and accurate conditional independence tests.
[conditional independence, Correlation, Machine learning algorithms, Error analysis, Computational modeling, data mining, knowledge discovery, relational databases, machine learning, testing marginal, Graphical models, relational data sets, graphical models, D-separation, Data models, Inference algorithms, learning (artificial intelligence)]
Factorization Machines
2010 IEEE International Conference on Data Mining
None
2010
In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models.
[Frequency modulation, support vector machines, sparse data, Computational modeling, data mining, Predictive models, matrix decomposition, factorization machine, SVM, Equations, feature vector, Support vector machines, support vector machine, model parameters, parameter estimation, Data models, Mathematical model, tensor factorization]
Accelerating Dynamic Time Warping Subsequence Search with GPUs and FPGAs
2010 IEEE International Conference on Data Mining
None
2010
Many time series data mining problems require subsequence similarity search as a subroutine. Dozens of similarity/distance measures have been proposed in the last decade and there is increasing evidence that Dynamic Time Warping (DTW) is the best measure across a wide range of domains. Given DTW's usefulness and ubiquity, there has been a large community-wide effort to mitigate its relative lethargy. Proposed speedup techniques include early abandoning strategies, lower-bound based pruning, indexing and embedding. In this work we argue that we are now close to exhausting all possible speedup from software, and that we must turn to hardware-based solutions. With this motivation, we investigate both GPU (Graphics Processing Unit) and FPGA (Field Programmable Gate Array) based acceleration of subsequence similarity search under the DTW measure. As we shall show, our novel algorithms allow GPUs to achieve two orders of magnitude speedup and FPGAs to produce four orders of magnitude speedup. We conduct detailed case studies on the classification of astronomical observations and demonstrate that our ideas allow us to tackle problems that would be untenable otherwise.
[dynamic time warping, computer graphic equipment, similarity search, Instruction sets, field programmable gate arrays, FPGA, data mining, coprocessors, subroutine, GPU, Graphics processing unit, lower-bound based pruning, DTW subsequence search, field programmable gate array, Hardware, search problems, Time series analysis, astronomical observation classification, time series, dynamic time warping subsequence search, subsequence similarity search, graphics processing unit, time series data mining, Field programmable gate arrays, early abandoning strategy, Clocks]
An Approach for Automatic Sleep Stage Scoring and Apnea-Hypopnea Detection
2010 IEEE International Conference on Data Mining
None
2010
This paper presents an application of data mining to the medical domain sleep research, i.e. an approach for automatic sleep stage scoring and apnea-hypopnea detection. By several combined techniques (Fourier and wavelet transform, DDTW and waveform recognition), our approach extracts meaningful features (frequencies and special patterns) from EEG, ECG, EOG and EMG data, on which a decision trees classifier is built for classifying epochs into their sleep stages (according to the rules by Rechtschaffen and Kales) and annotating occurrences of apnea-hypopnea (total or partial cessation of respiration). After that, case-based reasoning is applied to improve quality. We evaluated our approach on 3 large public databases from PhysioBank, which showed an overall accuracy of 95.2% for sleep stage scoring and 94.5% for classifying apneic/non-apneic minutes.
[electro-oculography, data mining, Electroencephalography, Cognition, automatic sleep stage scoring, electrocardiography, sleep, Accuracy, feature extraction, Electrocardiography, classifier, public databases, Decision trees, EOG, EMG, electroencephalography, pattern classification, medical domain sleep research, EEG, Time series, ECG, Data processing, Sleep, electromyography, Pattern classification, decision trees, PhysioBank, Feature extraction, case-based reasoning, Apnea-Hypopnea detection, medical computing, Biomedical signal processing]
Bonsai: Growing Interesting Small Trees
2010 IEEE International Conference on Data Mining
None
2010
Graphs are increasingly used to model a variety of loosely structured data such as biological or social networks and entity-relationships. Given this profusion of large-scale graph data, efficiently discovering interesting substructures buried within is essential. These substructures are typically used in determining subsequent actions, such as conducting visual analytics by humans or designing expensive biomedical experiments. In such settings, it is often desirable to constrain the size of the discovered results in order to directly control the associated costs. In this paper, we address the problem of finding cardinality-constrained connected sub trees in large node-weighted graphs that maximize the sum of weights of selected nodes. We provide an efficient constant-factor approximation algorithm for this strongly NP-hard problem. Our techniques can be applied in a wide variety of application settings, for example in differential analysis of graphs, a problem that frequently arises in bioinformatics but also has applications on the web.
[Algorithm design and analysis, Steiner trees, Graph Mining, Subgraph Discovery, approximation theory, Visual Analytics, Heuristic algorithms, cardinality-constrained connected subtrees, Optimized production technology, data structure, Cardinalty Constrained Subgraphs, Approximation methods, optimisation, NP-hard problem, Clustering algorithms, bioinformatics, approximation algorithm, Approximation algorithms, grpahs, differential analysis, tree data structures]
Mixed-Membership Stochastic Block-Models for Transactional Networks
2010 IEEE International Conference on Data Mining
None
2010
Transactional network data can be thought of as a list of one-to-many communications (e.g., email) between nodes in a social network. Most social network models convert this type of data into binary relations between pairs of nodes. We develop a latent mixed membership model capable of modeling richer forms of transactional network data, including relations between more than two nodes. The model can cluster nodes and predict transactions. The block-model nature of the model implies that groups can be characterized in very general ways. This flexible notion of group structure enables discovery of rich structure in transactional networks. Estimation and inference are accomplished via a variational EM algorithm. Simulations indicate that the learning algorithm can recover the correct generative model. Interesting structure is discovered in the Enron email dataset and another dataset extracted from the Reddit website. Analysis of the Reddit data is facilitated by a novel performance measure for comparing two soft clusterings. The new model is superior at discovering mixed membership in groups and in predicting transactions.
[Measurement, transaction processing, Social Network Analysis, variational EM algorithm, data mining, Predictive models, soft clustering, Electronic mail, social network, mixed membership stochastic block model, binary relation, Enron email dataset, Mixedmembership, learning (artificial intelligence), stochastic processes, transactional network data, learning algorithm, Social network services, Reddit Website, Clustering, transaction prediction, Email Data, Tin, Variational EM, social networking (online), Data models, Inference algorithms, generative model, one-to-many communication]
Generalized Probabilistic Matrix Factorizations for Collaborative Filtering
2010 IEEE International Conference on Data Mining
None
2010
Probabilistic matrix factorization (PMF) methods have shown great promise in collaborative filtering. In this paper, we consider several variants and generalizations of PMF framework inspired by three broad questions: Are the prior distributions used in existing PMF models suitable, or can one get better predictive performance with different priors? Are there suitable extensions to leverage side information? Are there benefits to taking into account row and column biases? We develop new families of PMF models to address these questions along with efficient approximate inference algorithms for learning and prediction. Through extensive experiments on movie recommendation datasets, we illustrate that simpler models directly capturing correlations among latent factors can outperform existing PMF models, side information can benefit prediction accuracy, and accounting for row/column biases leads to improvements in predictive performance.
[collaborative filtering, approximate inference algorithm, Predictive models, information filtering, learning, matrix decomposition, probabilistic matrix factorization, inference mechanisms, Accuracy, Bayesian methods, pattern clustering, Collaboration, groupware, Motion pictures, Approximation algorithms, Prediction algorithms, topic models, learning (artificial intelligence), variational inference]
Topic Modeling Ensembles
2010 IEEE International Conference on Data Mining
None
2010
In this paper we propose a framework of topic modeling ensembles, a novel solution to combine the models learned by topic modeling over each partition of the whole corpus. It has the potentials for applications such as distributed topic modeling for large corpora, and incremental topic modeling for rapidly growing corpora. Since only the base models, not the original documents, are required in the ensemble, all these applications can be performed in a privacy preserving manner. We explore the theoretical foundation of the proposed framework, give its geometric interpretation, and implement it for both PLSA and LDA. The evaluation of the implementations over the synthetic and real-life data sets shows that the proposed framework is much more efficient than modeling the original corpus directly while achieves comparable effectiveness in terms of perplexity and classification accuracy.
[document handling, privacy preserving manner, latent Dirichlet allocation, Solid modeling, probabilistic latent semantic analysis, LDA, incremental topic modeling, Complexity theory, topic modeling ensemble, distributed topic modeling, PLSA, Analytical models, Distributed databases, Ensemble, Data models, data privacy, Mathematical model, learning (artificial intelligence), Pixel, Topic model]
Interval-valued Matrix Factorization with Applications
2010 IEEE International Conference on Data Mining
None
2010
In this paper, we propose the Interval-valued Matrix Factorization (IMF) framework. Matrix Factorization (MF) is a fundamental building block of data mining. MF techniques, such as Nonnegative Matrix Factorization (NMF) and Probabilistic Matrix Factorization (PMF), are widely used in applications of data mining. For example, NMF has shown its advantage in Face Analysis (FA) while PMF has been successfully applied to Collaborative Filtering (CF). In this paper, we analyze the data approximation in FA as well as CF applications and construct interval-valued matrices to capture these approximation phenomenons. We adapt basic NMF and PMF models to the interval-valued matrices and propose Interval-valued NMF (I-NMF) as well as Interval-valued PMF (I-PMF). We conduct extensive experiments to show that proposed I-NMF and I-PMF significantly outperform their single-valued counterparts in FA and CF applications.
[collaborative filtering, Face recognition, data mining, probability, NMF, PMF, interval-valued matrices, Matrix factorization, matrix decomposition, probabilistic matrix factorization, Matrix decomposition, Sparse matrices, uncertainty, MF techniques, nonnegative matrix factorization, face analysis, Linear approximation, face recognition, Face, Pixel]
Efficient Semi-supervised Spectral Co-clustering with Constraints
2010 IEEE International Conference on Data Mining
None
2010
Co-clustering was proposed to simultaneously cluster objects and features to explore inter-correlated patterns. For example, by analyzing the blog click-through data, one finds the group of users who are interested in a specific group of blogs in order to perform applications such as recommendations. However, it is usually very difficult to achieve good co-clustering quality by just analyzing the object-feature correlation data due to the sparsity of the data and the noise. Meanwhile, one may have some prior knowledge that indicates the internal structure of the co-clusters. For instance, one may find user cluster information from the social network system, and the blog-blog similarity from the social tags or contents. This prior information provides some supervision toward the co-cluster structures, and may help reduce the effect of sparsity and noise. However, most co-clustering algorithms do not use this information and may produce unmeaningful results. In this paper we study the problem of finding the optimal co-clusters when some objects and features are believed to be in the same cluster a priori. A matrix decomposition based approach is proposed to formulate as a trace minimization problem, and solve it efficiently with the selected eigenvectors. The asymptotic complexity of the proposed approach is the same as co-clustering without constraints. Experiments include graph-pattern co-clustering and document-word co-clustering. For instance, in graph-pattern data set, the proposed model can improve the normalized mutual information by as much as 5.5 times and 10 times faster than two naive solutions that expand the edges and vertices in the graphs.
[Noise, graph theory, Complexity theory, matrix decomposition, Optimization, eigenvalues and eigenfunctions, trace minimization problem, normalized mutual information, Semi-supervised Learning, asymptotic complexity, Clustering algorithms, Bipartite graph, constraint handling, graph-pattern co-clustering, semisupervised spectral coclustering algorithm, Co-clustering, matrix decomposition based approach, Minimization, pattern clustering, object feature analysis, social networking (online), document-word co-clustering, Spectral, Internet, data sparsity, social network system, correlation methods]
Transfer Learning on Heterogenous Feature Spaces via Spectral Transformation
2010 IEEE International Conference on Data Mining
None
2010
Labeled examples are often expensive and time-consuming to obtain. One practically important problem is: can the labeled data from other related sources help predict the target task, even if they have (a) different feature spaces (e.g., image vs. text data), (b) different data distributions, and (c) different output spaces? This paper proposes a solution and discusses the conditions where this is possible and highly likely to produce better results. It works by first using spectral embedding to unify the different feature spaces of the target and source data sets, even when they have completely different feature spaces. The principle is to cast into an optimization objective that preserves the original structure of the data, while at the same time, maximizes the similarity between the two. Second, a judicious sample selection strategy is applied to select only those related source examples. At last, a Bayesian-based approach is applied to model the relationship between different output spaces. The three steps can bridge related heterogeneous sources in order to learn the target task. Among the 12 experiment data sets, for example, the images with wavelet-transformed-based features are used to predict another set of images whose features are constructed from color-histogram space. By using these extracted examples from heterogeneous sources, the models can reduce the error rate by as much as ~50\\%, compared with the methods using only the examples from the target task.
[Drugs, spectral, wavelet transforms, data distribution, data mining, wavelet transform, spectral analysis, heterogeneous feature space, heterogeneous, Optimization, image data, Microorganisms, optimisation, feature extraction, text data, Training data, spectral transformation, image colour analysis, learning (artificial intelligence), transfer learning, Symmetric matrices, feature space, Vectors, color histogram space, labeled data, Data models, Bayesian-based approach, Bayes methods]
One-Class Matrix Completion with Low-Density Factorizations
2010 IEEE International Conference on Data Mining
None
2010
Consider a typical recommendation problem. A company has historical records of products sold to a large customer base. These records may be compactly represented as a sparse customer-times-product ``who-bought-what" binary matrix. Given this matrix, the goal is to build a model that provides recommendations for which products should be sold next to the existing customer base. Such problems may naturally be formulated as collaborative filtering tasks. However, this is a {\\it one-class} setting, that is, the only known entries in the matrix are one-valued. If a customer has not bought a product yet, it does not imply that the customer has a low propensity to {\\it potentially} be interested in that product. In the absence of entries explicitly labeled as negative examples, one may resort to considering unobserved customer-product pairs as either missing data or as surrogate negative instances. In this paper, we propose an approach to explicitly deal with this kind of ambiguity by instead treating the unobserved entries as optimization variables. These variables are optimized in conjunction with learning a weighted, low-rank non-negative matrix factorization (NMF) of the customer-product matrix, similar to how Transductive SVMs implement the low-density separation principle for semi-supervised learning. Experimental results show that our approach gives significantly better recommendations in comparison to various competing alternatives on one-class collaborative filtering tasks.
[collaborative filtering, Collaborative Filtering, matrix completion, low density factorization, information filtering, Complexity theory, matrix decomposition, Sparse matrices, Optimization, missing data, Training, optimisation, Non-negative Matrix Factorizations, groupware, Motion pictures, recommendation problem, optimization variables, matrix factorization, Recommender Systems, Matrix Completion, Sensitivity, recommender systems, Collaboration, customer-product pairs, Implicit Feedback]
A System for Mining Temporal Physiological Data Streams for Advanced Prognostic Decision Support
2010 IEEE International Conference on Data Mining
None
2010
We present a mining system that can predict the future health status of the patient using the temporal trajectories of health status of a set of similar patients. The main novelties of this system are its use of stream processing technology for handling the incoming physiological time series data and incorporating domain knowledge in learning the similarity metric between patients represented by their temporal data. The proposed approach and system were tested using the MIMIC II database, which consists of physiological waveforms, and accompanying clinical data obtained for ICU patients. The study was carried out on 1500 patients from this database. In the experiments we report the efficiency and throughput of the stream processing unit for feature extraction, the effectiveness of the supervised similarity measure both in the context of classification and retrieval tasks compared to unsupervised approaches, and the accuracy of the temporal projections of the patient data.
[ICU patient, Error analysis, data mining, retrieval task, Data mining, patient monitoring, clinical data, Accuracy, Databases, feature extraction, medical administrative data processing, learning (artificial intelligence), Patient similarity, supervised similarity measure, information retrieval, temporal physiological data stream mining, physiological time series data, time series, MIMIC II database, classification task, Physiological streams, Feature extraction, Biomedical monitoring, advanced prognostic decision support, patient health status]
Averaged Stochastic Gradient Descent with Feedback: An Accurate, Robust, and Fast Training Method
2010 IEEE International Conference on Data Mining
None
2010
On large datasets, the popular training approach has been stochastic gradient descent (SGD). This paper proposes a modification of SGD, called averaged SGD with feedback (ASF), that significantly improves the performance (robustness, accuracy, and training speed) over the traditional SGD. The proposal is based on three simple ideas: averaging the weight vectors across SGD iterations, feeding the averaged weights back into the SGD update process, and deciding when to perform the feedback (linearly slowing down feedback). Theoretically, we demonstrate the reasonable convergence properties of the ASF. Empirically, the ASF outperforms several strong baselines in terms of accuracy, robustness over the noise, and the training speed. To our knowledge, this is the first study of ``feedback'' in stochastic gradient learning. Although we choose latent conditional models for verifying the ASF in this paper, the ASF is a general purpose technique just like SGD, and can be directly applied to other models.
[averaged stochastic gradient descent, weight vectors, Stochastic processes, data mining, Proposals, classification, large datasets, Convergence, Training, feedback, Accuracy, very large databases, Robustness, Acceleration, learning (artificial intelligence), stochastic processes, gradient methods]
Visualizing Graphs Using Minimum Spanning Dendrograms
2010 IEEE International Conference on Data Mining
None
2010
We present a novel visualization methodology for graphs and high-dimensional data which combines the neighborhood preservation characteristics of the minimum spanning trees, with the grouping properties of dendrograms. We call the method `minimum spanning dendrogram'. We highlight the ability of the mapping to accurately capture both neighborhood and cluster structures. The technique accommodates the interactive cluster formation at progressively more granular levels, allowing the user to explore data relationships at different resolutions. We also compare our work with other visualization methodologies, such as ISOMAP, and highlight the distinct merits of our approach.
[Measurement, data analysis, minimum spanning tree, graph visualization, minimum spanning dendrogram, single linkage hierarchical clustering, granular level, Object recognition, neighborhood preservation characteristics, graphs, pattern clustering, high dimensional data, Layout, Data visualization, Clustering algorithms, data visualisation, cluster structure, Simulated annealing, tree data structures, dendrogram, interactive cluster formation, ISOMAP, distance preservation]
Tru-Alarm: Trustworthiness Analysis of Sensor Networks in Cyber-Physical Systems
2010 IEEE International Conference on Data Mining
None
2010
A Cyber-Physical System (CPS) integrates physical devices (e.g., sensors, cameras) with cyber (or informational)components to form a situation-integrated analytical system that responds intelligently to dynamic changes of the real-world scenarios. One key issue in CPS research is trustworthiness analysis of the observed data: Due to technology limitations and environmental influences, the CPS data are inherently noisy that may trigger many false alarms. It is highly desirable to sift meaningful information from a large volume of noisy data. In this paper, we propose a method called Tru-Alarm which finds out trustworthy alarms and increases the feasibility of CPS. Tru-Alarm estimates the locations of objects causing alarms, constructs an object-alarm graph and carries out trustworthiness inferences based on linked information in the graph. Extensive experiments show that Tru-Alarm filters out noises and false information efficiently and guarantees not missing any meaningful alarms.
[wireless sensor networks, Noise, object-alarm graph, Alarm Detection, network theory (graphs), Noise measurement, computer network security, Cyber Physical System, Cyber physical system, Coherence, Inference algorithms, Mice, alarm systems, sensor networks trustworthiness analysis, Joining processes, Monitoring, situation-integrated analytical system, Tru-Alarm]
Node Similarities from Spreading Activation
2010 IEEE International Conference on Data Mining
None
2010
In this paper we propose two methods to derive two different kinds of node similarities in a network based on their neighborhood. The first similarity measure focuses on the overlap of direct and indirect neighbors. The second similarity compares nodes based on the structure of their - possibly also very distant - neighborhoods. Instead of using standard node measures, both similarities are derived from spreading activation patterns over time. Whereas in the first method the activation patterns are directly compared, in the second method the relative change of activation over time is compared. We apply both methods to a real-world graph dataset and discuss the results.
[Electronic publishing, data analysis, node similarity, Communities, graph theory, graph analysis, Educational institutions, Equations, Convergence, graph dataset, spreading activation, node similarities, Internet, node signatures, Joining processes]
On the Vulnerability of Large Graphs
2010 IEEE International Conference on Data Mining
None
2010
Given a large graph, like a computer network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? We need (a) a measure of the 'Vulnerability' of a given network, (b) a measure of the 'Shield-value' of a specific set of k nodes and (c) a fast algorithm to choose the best such k nodes. We answer all these three questions: we give the justification behind our choices, we show that they agree with intuition as well as recent results in immunology. Moreover, we propose NetShield a fast and scalable algorithm. Finally, we give experiments on large real graphs, where NetShield achieves tremendous speed savings exceeding 7 orders of magnitude, against straightforward competitors.
[Correlation, Scalability, graph theory, scalable algorithm, Vulnerability, set theory, Approximation methods, computer network security, scalability, large graph vulnerability, shield value, graph mining, NetShield, immunization, Eigenvalues and eigenfunctions, Robustness, Computer networks, fast algorithm, Immune system]
Testing the Significance of Patterns in Data with Cluster Structure
2010 IEEE International Conference on Data Mining
None
2010
Clustering is one of the basic operations in data analysis, and the cluster structure of a dataset often has a marked effect on observed patterns in data. Testing whether a data mining result is implied by the cluster structure can give substantial information on the formation of the dataset. We propose a new method for empirically testing the statistical significance of patterns in real-valued data in relation to the cluster structure. The method relies on principal component analysis and is based on the general idea of decomposing the data for the purpose of isolating the null model. We evaluate the performance of the method and the information it provides on various real datasets. Our results show that the proposed method is robust and provides nontrivial information about the origin of patterns in data, such as the source of classification accuracy and the observed correlations between attributes.
[Correlation, statistical significance, data analysis, data mining, classification accuracy, Optical character recognition software, Matrix decomposition, observed correlation, Support vector machines, Accuracy, significance testing, pattern clustering, randomization, cluster structure, nontrivial information, dataset formation, clustering, principal component analysis, Testing, Principal component analysis]
Compressed Nonnegative Sparse Coding
2010 IEEE International Conference on Data Mining
None
2010
Sparse Coding (SC), which models the data vectors as sparse linear combinations over basis vectors, has been widely applied in machine learning, signal processing and neuroscience. In this paper, we propose a dual random projection method to provide an efficient solution to Nonnegative Sparse Coding (NSC) using small memory. Experiments on real world data demonstrate the effectiveness of the proposed method.
[Dictionaries, source coding, data vector, sparse linear combination, Encoding, matrix decomposition, dual random projection method, Nonnegative Sparse Coding, Sparse matrices, Approximation methods, Optimization, data reduction, vectors, Face, Bars]
Anonymizing Temporal Data
2010 IEEE International Conference on Data Mining
None
2010
Temporal data are time-critical in that the snapshot at each timestamp must be made available to researchers in a timely fashion. However, due to the limited data, each snapshot likely has a skewed distribution on sensitive values, which renders classical anonymization methods not possible. In this work, we propose the &#x201C;reposition model&#x201D; to allow a record to be published within a close proximity of original timestamp. We show that reposition over a small proximity of timestamp is sufficient for reducing the skewness of a snapshot, therefore, minimizing the impact on window queries. We formalize the optimal reposition problem and present a linear-time solution. The contribution of this work is that it enables classical methods on temporal data.
[linear-time solution, skewed distribution, temporal data, window queries, Relays, Diseases, Privacy, Anonymization, Publishing, temporal databases, reposition model, classical anonymization, Cost function, Silicon, data privacy, Temporal Data, Mathematical model, publishing, snapshot, timestamp]
Homotopy Regularization for Boosting
2010 IEEE International Conference on Data Mining
None
2010
In this paper, we present a homotopy regularization algorithm for boosting. We introduce a regularization term with adaptive weight into the boosting framework and compose a homotopy objective function. Optimization of this objective approximately composes a solution path for the regularized boosting. Following this path, we can find suitable solution efficiently using early stopping. Experiments show that this adaptive regularization method gives a more efficient parameter selection strategy than regularized boosting and semi supervised boosting algorithms, and significantly improves the performances of traditional AdaBoost and related methods.
[Algorithm design and analysis, Laplace equations, semisupervised boosting, boosting, Boosting, Tuning, Training, Manifolds, homotopy regularization algorithm, optimisation, AdaBoost, optimization, Benchmark testing, homotopy regularization, parameter selection strategy, regularized boosting, learning (artificial intelligence)]
What Do People Want in Microblogs? Measuring Interestingness of Hashtags in Twitter
2010 IEEE International Conference on Data Mining
None
2010
When micro logging becomes a very popular social media, finding interesting posts from high volume stream of user posts is a challenging research problem. To organize large number of posts, users can assign tags to posts so that these posts can be navigated and searched by tag. In this paper, we focus on modeling the interestingness of hash tags in Twitter, the largest and most active micro logging site. We propose to first construct communities based on both follow links and tagged interactions. We then measure the dispersion and divergence of users and tweets using hash tags among the constructed communities. The interestingness of hash tags are then derived from these community-based dispersion and divergence features. We further introduce a supervised approach to rank hash tags by interestingness. Our experiments on a Twitter dataset show that the proposed approach achieves a fairly good performance.
[Correlation, Event detection, high volume stream, hashtags, Communities, Media, supervised approach, Twitter, microblogging site, Dispersion, Support vector machines, divergence features, interestingness, community-based dispersion, social networking (online), ranking, social media, hashtag]
Probabilistic Inference Protection on Anonymized Data
2010 IEEE International Conference on Data Mining
None
2010
Background knowledge is an important factor in privacy preserving data publishing. Probabilistic distribution-based background knowledge is a powerful kind of background knowledge which is easily accessible to adversaries. However, to the best of our knowledge, there is no existing work that can provide a privacy guarantee under adversary attack with such background knowledge. The difficulty of the problem lies in the high complexity of the probability computation and the non-monotone nature of the privacy condition. The only solution known to us relies on approximate algorithms with no known error bound. In this paper, we propose a new bounding condition that overcomes the difficulties of the problem and gives a privacy guarantee. This condition is based on probability deviations in the anonymized data groups, which is much easier to compute and which is a monotone function on the grouping sizes.
[Data privacy, data analysis, anonymized data, l-diversity, Probabilistic logic, nonmonotone nature, inference mechanisms, Couplings, Privacy, privacy preserving data publishing, computation complexity, Lungs, background knowledge, probabilistic inference protection, data privacy, probabilistic distribution, Joining processes, probability deviation, Cancer, computational complexity, k-anonymity]
Collaborative Learning between Visual Content and Hidden Semantic for Image Retrieval
2010 IEEE International Conference on Data Mining
None
2010
Similarity measure is a critical component in image retrieval systems, and learning similarity measure from the relevance feedback has become a promising way to enhance retrieval performance. Existing approaches mainly focus on learning the visual similarity measure from online feedbacks or constructing the semantic similarity measure depended on historical feedbacks log. However, there is still a big room to elevate the retrieval performance, because few works take the relationship between the visual similarity and the semantic similarity into account. This paper proposes the collaborative learning similarity measure, CoSim, which focuses on the collaborative learning between the visual content of images and the hidden semantic in log. Concretely, the semantic similarity is first learned from log data and serves as prior knowledge. Then, the visual similarity is learned from a mixture of labeled and unlabeled images. In particular, unlabeled images are exploited for the relevant and irrelevant classes in different ways. Finally, the collaborative learning similarity is produced by integrating the visual similarity and the semantic similarity in a nonlinear way. An empirical study shows that the proposed CoSim is significantly more effective than some existing approaches.
[Visualization, image retrieval system, semantic similarity measure, short-term learning, CoSim, Classification algorithms, Training, Support vector machines, hidden semantic, collaborative learning similarity measure, Databases, Semantics, relevance feedback, groupware, collaborative learning, image retrieval, Collaborative work, long-term learning, learning (artificial intelligence), learning similarity measure, image visual content]
Max-Clique: A Top-Down Graph-Based Approach to Frequent Pattern Mining
2010 IEEE International Conference on Data Mining
None
2010
Frequent pattern mining is a fundamental problem in data mining research. We note that almost all state-of-the art algorithms may not be able to mine very long patterns in a large database with a huge set of frequent patterns. In this paper, we point our research to solve this difficult problem from a different perspective: we focus on mining top-k long maximal frequent patterns because long patterns are in general more interesting ones. Different from traditional level-wise mining or tree-growth strategies, our method works in a top-down manner. We pull large maximal cliques from a pattern graph constructed after some fast initial processing, and directly use such large-sized maximal cliques as promising candidates for long frequent patterns. A separate refinement stage is needed to further transform these candidates into true maximal patterns.
[data mining, Transforms, max clique, pattern graph, tree growth strategies, level wise mining, Data mining, large sized maximal clique, Itemsets, tree data structures, top down graph based approach, Acceleration, top-down, Accidents, frequent pattern mining, top-k long maximal frequent patterns]
Personalizing Web Page Recommendation via Collaborative Filtering and Topic-Aware Markov Model
2010 IEEE International Conference on Data Mining
None
2010
Web-page recommendation is to predict the next request of pages that Web users are potentially interested in when surfing the Web. This technique can guide Web users to find more useful pages without asking for them explicitly and has attracted much attention in the community of Web mining. However, few studies on Web page recommendation consider personalization, which is an indispensable feature to meet various preferences of users. In this paper, we propose a personalized Web page recommendation model called PIGEON (abbr. for PersonalIzed web paGe rEcommendatiON) via collaborative filtering and a topic-aware Markov model. We propose a graph-based iteration algorithm to discover users' interested topics, based on which user similarities are measured. To recommend topically coherent pages, we propose a topic-aware Markov model to learn users' navigation patterns which capture both temporal and topical relevance of pages. A thorough experimental evaluation conducted on a large real dataset demonstrates PIGEON's effectiveness and efficiency.
[iterative methods, collaborative filtering, Collaborative Filtering, Navigation, graph theory, information filtering, Markov model, Personalized Recommendation, recommender systems, graph based iteration algorithm, topic aware Markov model, Web pages, Collaboration, Clustering algorithms, Web Page Clustering, groupware, Markov processes, Feature extraction, Internet, Integrated circuit modeling, personalized Web page recommendation model, requested Web pages prediction, PIGEON]
Passive Sampling for Regression
2010 IEEE International Conference on Data Mining
None
2010
Active sampling (also called active learning or selective sampling) has been extensively researched for classification and rank learning methods, which is to select the most informative samples from unlabeled data such that, once the samples are labeled, the accuracy of the function learned from the samples is maximized. While active sampling methods require learning a function at each iteration to find the most informative samples, this paper proposes passive sampling techniques for regression, which find the informative samples not based on the learned function but based on the samples' geometric characteristics in the feature space. Passive sampling is more efficient than active sampling, as it does not require, at each iteration, learning and validating the regression functions and evaluating the unlabeled data using the function. For regression, passive sampling is also more effective, Active sampling for regression suffers from serious performance fluctuations in practice, because it selects the samples of highest regression errors and such samples are likely noisy. Passive sampling, on the other hand, shows more stable performance. We observe from our extensive experiments that our passive sampling methods perform even better than the ``omniscient'' active sampling that knows the labels of unlabeled data.
[rank learning method, passive sampling, Uncertainty, sampling methods, regression analysis, Noise measurement, Training, selective sampling, Accuracy, active learning, regression, active sampling, Sampling methods, Approximation algorithms, learning (artificial intelligence), Kernel]
Modeling Experts and Novices in Citizen Science Data for Species Distribution Modeling
2010 IEEE International Conference on Data Mining
None
2010
Citizen scientists, who are volunteers from the community that participate as field assistants in scientific studies, enable research to be performed at much larger spatial and temporal scales than trained scientists can cover. Species distribution modeling, which involves understanding species-habitat relationships, is a research area that can benefit greatly from citizen science. The eBird project is one of the largest citizen science programs in existence. By allowing birders to upload observations of bird species to an online database, eBird can provide useful data for species distribution modeling. However, since birders vary in their levels of expertise, the quality of data submitted to eBird is often questioned. In this paper, we develop a probabilistic model called the Occupancy-Detection-Expertise (ODE) model that incorporates the expertise of birders submitting data to eBird. We show that modeling the expertise of birders can improve the accuracy of predicting observations of a bird species at a site. In addition, we can use the ODE model for two other tasks: predicting birder expertise given their history of eBird checklists and identifying bird species that are difficult for novices to detect.
[modeling novices, eBird project, Predictive models, occupancy detection expertise model, Species Distribution Modeling, Training, Contrast Mining, biology computing, birder expertise prediction, eBird checklists, Citizen Science, Graphical Models, citizen scientist, Biological system modeling, Computational modeling, Applications, probability, Birds, citizen science data, species distribution modeling, modeling expert, Feature extraction, Data models, field assistant, zoology]
Causal Discovery from Streaming Features
2010 IEEE International Conference on Data Mining
None
2010
In this paper, we study a new research problem of causal discovery from streaming features. A unique characteristic of streaming features is that not all features can be available before learning begins. Feature generation and selection often have to be interleaved. Managing streaming features has been extensively studied in classification, but little attention has been paid to the problem of causal discovery from streaming features. To this end, we propose a novel algorithm to solve this challenging problem, denoted as CDFSF (Causal Discovery From Streaming Features) which consists of two phases: growing and shrinking. In the growing phase, CDFSF finds candidate parents or children for each feature seen so far, while in the shrinking phase the algorithm dynamically removes false positives from the current sets of candidate parents and children. In order to improve the efficiency of CDFSF, we present S-CDFSF, a faster version of CDFSF, using two symmetry theorems. Experimental results validate our algorithms in comparison with other state-of-art algorithms of causal discovery.
[Algorithm design and analysis, Context, Measurement, S-CDFSF, streaming feature, Classification algorithms, causality, causal discovery, feature generation, Bayesian methods, feature extraction, Machine learning, Markov processes, streaming features, Bayesian networks, belief networks, feature selection]
ABS: The Anti Bouncing Model for Usage Data Streams
2010 IEEE International Conference on Data Mining
None
2010
Usage data mining is an important research area with applications in various fields. However, usage data is usually considered streaming, due to its high volumes and rates. Because of these characteristics, we only have access, at any point in time, to a small fraction of the stream. When the data is observed through such a limited window, it is challenging to give a reliable description of the recent usage data. We study the important consequences of these constraints, through the &#x201C;bounce rate&#x201D; problem and the clustering of usage data streams. Then, we propose the ABS (Anti-Bouncing Stream) model which combines the advantages of previous models but discards their drawbacks. First, under the same resource constraints as existing models in the literature, ABS can better model the recent data. Second, owing to its simple but effective management approach, the data in ABS is available at any time for analysis. We demonstrate its superiority through a theoretical study and experiments on two real-world data sets.
[anti bouncing model, data streams clustering, Navigation, data streams, usage, Mobile communication, Entropy, Data mining, data stream, Analytical models, pattern clustering, bounce rate problem, Clustering algorithms, Data models, clustering, data handling, bounce rate]
Classifier and Cluster Ensembles for Mining Concept Drifting Data Streams
2010 IEEE International Conference on Data Mining
None
2010
Ensemble learning is a commonly used tool for building prediction models from data streams, due to its intrinsic merits of handling large volumes stream data. Despite of its extraordinary successes in stream data mining, existing ensemble models, in stream data environments, mainly fall into the ensemble classifiers category, without realizing that building classifiers requires labor intensive labeling process, and it is often the case that we may have a small number of labeled samples to train a few classifiers, but a large number of unlabeled samples are available to build clusters from data streams. Accordingly, in this paper, we propose a new ensemble model which combines both classifiers and clusters together for mining data streams. We argue that the main challenges of this new ensemble model include (1) clusters formulated from data streams only carry cluster IDs, with no genuine class label information, and (2) concept drifting underlying data streams makes it even harder to combine clusters and classifiers into one ensemble framework. To handle challenge (1), we present a label propagation method to infer each cluster's class label by making full use of both class label information from classifiers, and internal structure information from clusters. To handle challenge (2), we present a new weighting schema to weight all base models according to their consistencies with the up-to-date base model. As a result, all classifiers and clusters can be combined together, through a weighted average mechanism, for prediction. Experiments on real-world data streams demonstrate that our method outperforms simple classifier ensemble and cluster ensemble for stream data mining.
[pattern classification, ensemble clustering, weighted average mechanism, Computational modeling, data mining, Predictive models, stream data mining, Data Stream Mining, Concept Drifting, ensemble classifier category, Data mining, ensemble learning, label propagation method, Ensemble Learning, Accuracy, Clustering algorithms, Classification, Prediction algorithms, internal structure information, Data models, data handling, learning (artificial intelligence), concept drifting data stream, class label information]
Graph-Based Semi-supervised Learning with Adaptive Similarity Estimation
2010 IEEE International Conference on Data Mining
None
2010
Graph-based semi-supervised learning algorithms have attracted a lot of attention. Constructing a good graph is playing an essential role for all these algorithms. Many existing graph construction methods(e.g. Gaussian Kernel etc.) require user input parameter, which is hard to configure manually. In this paper, we propose a parameter-free similarity measure Adaptive Similarity Estimation (ASE), which constructs the graph by adaptively optimizing linear combination of its neighbors. Experimental results show the effectiveness of our proposed method.
[Measurement, pattern classification, pattern matching, adaptively optimizing linear combination, Estimation, Sonar, graph based semisupervised learning, graph construction method, Complexity theory, classification, adaptive similarity estimation, Manifolds, Iris, graphs, optimisation, parameter free similarity, semi-supervised learning, learning (artificial intelligence), Kernel, adaptive estimation]
K-AP: Generating Specified K Clusters by Efficient Affinity Propagation
2010 IEEE International Conference on Data Mining
None
2010
The Affinity Propagation (AP) clustering algorithm proposed by Frey and Dueck (2007) provides an understandable, nearly optimal summary of a data set. However, it suffers two major shortcomings: i) the number of clusters is vague with the user-defined parameter called self-confidence, and ii) the quadratic computational complexity. When aiming at a given number of clusters due to prior knowledge, AP has to be launched many times until an appropriate setting of self-confidence is found. The re-launched AP increases the computational cost by one order of magnitude. In this paper, we propose an algorithm, called K-AP, to exploit the immediate results of K clusters by introducing a constraint in the process of message passing. Through theoretical analysis and experimental validation, K-AP was shown to be able to directly generate K clusters as user defined, with a negligible increase of computational cost compared to AP. In the meanwhile, K-AP preserves the clustering quality as AP in terms of the distortion. K-AP is more effective than k-medoids w.r.t. the distortion minimization and higher clustering purity.
[Availability, message passing, specified K clusters generation, distortion minimization, quadratic computational complexity, Electronic mail, Computational complexity, theoretical analysis, affinity propagation clustering algorithm, K-AP, Message passing, pattern clustering, Clustering algorithms, k-medoids, clustering, affinity propagation, Face, Distortion measurement, computational complexity]
MoodCast: Emotion Prediction via Dynamic Continuous Factor Graph Model
2010 IEEE International Conference on Data Mining
None
2010
Human emotion is one important underlying force affecting and affected by the dynamics of social networks. An interesting question is &#x201C;can we predict a person's mood based on his historic emotion log and his social network?&#x201D;. In this paper, we propose a Mood Cast method based on a dynamic continuous factor graph model for modeling and predicting users' emotions in a social network. Mood Cast incorporates users' dynamic status information (e.g., locations, activities, and attributes) and social influence from users' friends into a unified model. Based on the historical information (e.g., network structure and users' status from time 0 to t-1), Mood Cast learns a discriminative model for predicting users' emotion status at time t. To the best of our knowledge, this work takes the first step in designing a principled model for emotion prediction in social networks. Our experimental results on both real social network and virtual web-based network show that we can accurately predict emotion status of more than 62% of users and 8+% improvement than the baseline methods.
[Social influence, Correlation, emotion prediction, Social network services, Biological system modeling, graph theory, social networks, Predictive model, Predictive models, Mobile communication, Social network, prediction theory, emotion recognition, MoodCast, dynamic continuous factor graph model, historical information, Mood, Emotion dynamics, social networking (online), human computer interaction, virtual Web based network, Mobile computing]
Hierarchical Ensemble Clustering
2010 IEEE International Conference on Data Mining
None
2010
Ensemble clustering has emerged as an important elaboration of the classical clustering problems. Ensemble clustering refers to the situation in which a number of different (input) clusterings have been obtained for a particular dataset and it is desired to find a single (consensus) clustering which is a better fit in some sense than the existing clusterings. Many approaches have been developed to solve ensemble clustering problems over the last few years. However, most of these ensemble techniques are designed for partitional clustering methods. Few research efforts have been reported for ensemble hierarchical clustering methods. In this paper, we propose a hierarchical ensemble clustering framework which can naturally combine both partitional clustering and hierarchical clustering results. We notice the importance of ultra-metric distance for hierarchical clustering and propose a novel method for learning the ultra-metric distance from the aggregated distance matrices and generating final hierarchical clustering with enhanced cluster separation. Experimental results demonstrate the effectiveness of our proposed approaches.
[Measurement, ensemble clustering, Clustering methods, data mining, aggregated distance matrix, hierarchical clustering method, Linear matrix inequalities, Data mining, ultrametric distance, matrix algebra, Aggregates, USA Councils, pattern clustering, Clustering algorithms, partitional clustering method, Ultra-metric, Hierarchical ensemble clustering]
Frequent Instruction Sequential Pattern Mining in Hardware Sample Data
2010 IEEE International Conference on Data Mining
None
2010
When parallelism and heterogeneity has become the trend for computer system design, both the size and the complexity of the hardware sample data generated by Performance Monitoring Unit (PMU) increase fast, thus automatic analysis methods, i.e. data mining methods, are urgently needed to accelerate hardware sample data analysis. We are the first to study instruction sequential pattern mining for hardware sample data. It is a challenging task due to the implicit sequential relationship contained in the data and due to the importance of low frequency patterns. As a solution, we i) provide a novel algorithm ProfSpan, ii) adapt two existing algorithms, which are based on candidate generation and projected database generation, to hardware sample data. Our evaluation results show ProfSpan can reduce up to 75% and 80% of execution time compared with other two algorithms. Particularly, up to 50% of frequent patterns mined by ProfSpan in hardware sample data are crossing basic block boundaries and can not be found by existing methods for source code or disassembly code. We also analyze three example patterns identified by ProfSpan: consecutive loads, JIT entry sequence, and conditional code dependency sequence, to illustrate how ProfSpan can benefit performance analysis. Finally, we apply patterns to module classification and obtain promising results.
[JIT entry sequence, data mining, database generation, hardware sample data analysis, Data mining, database management systems, parallel processing, Optimization, computer system design, conditional code dependency sequence, candidate generation, Hardware, Performance analysis, performance monitoring unit, sequential patter mining, Java, pattern classification, data analysis, source coding, source code, instruction sequential pattern mining, automatic analysis methods, ProfSpan, Support vector machine classification, hardware sample data, performance analysis, disassembly code]
Efficient Episode Mining with Minimal and Non-overlapping Occurrences
2010 IEEE International Conference on Data Mining
None
2010
Frequent serial episodes within an event sequence describe the behavior of users or systems about the application. Existing mining algorithms calculate the frequency of an episode based on overlapping or non-minimal occurrences, which is prone to over-counting the support of long episodes or poorly characterizing the followed-by-closely relationship over event types. In addition, due to utilizing the Apriori-style level wise approach, these algorithms are computationally expensive. In this paper, we propose an efficient algorithm MANEPI (Minimal And Non-overlapping EPIsode) for mining more interesting frequent episodes within the given event sequence. The proposed frequency measure takes both minimal and non-overlapping occurrences of an episode into consideration and ensures better mining quality. The introduced depth first search strategy with the Apriori Property for performing episode growth greatly improves the efficiency of mining long episodes because of scanning the given sequence only once and not generating candidate episodes. Moreover, an optimization technique is presented to narrow down search space and speed up the mining process. Experimental evaluation on both synthetic and real-world datasets demonstrates that our algorithms are more efficient and effective.
[Algorithm design and analysis, Apriori style level wise approach, MANEPI algorithm, event sequence, data mining, Frequent episode, Data structures, Frequency measurement, Complexity theory, Event sequence, Data mining, Optimization, frequent serial episode, Minimal and non-overlapping occurrences, Databases, Minimal And Non-overlapping EPIsode, efficient episode mining, Prefix tree, nonoverlapping occurrence]
Spatial and Spatio-temporal Data Mining
2010 IEEE International Conference on Data Mining
None
2010
Summary form only given. The recent advances and price reduction of technologies for collecting spatial and spatio-temporal data like Satellite Images, Cellular Phones, Sensor Networks, and GPS devices has facilitated the collection of data referenced in space and time. These huge collections of data often hide interesting information which conventional systems and classical data mining techniques are unable to discover. Spatial and spatio-temporal data are embedded in continuous space, whereas classical datasets (e.g. transactions) are often discrete. Spatial and spatio-temporal data require complex data preprocessing, transformation, data mining, and post-processing techniques to extract novel, useful, and understandable patterns. The importance of spatial and spatio-temporal data mining is growing with the increasing incidence and importance of large geo-spatial datasets such as maps, repositories of remote-sensing images, trajectories of moving objects generated by mobile devices, etc. Applications include Mobile-commerce industry (location-based services), climatologically effects of El Nino, land-use classification and global change using satellite imagery, finding crime hot spots, local instability in traffic, migration of birds, fishing control, pedestrian behavior analysis, and so on. Thus, new methods are needed to analyze spatial and spatio-temporal data to extract interesting, useful, and non-trivial patterns. The main goal of this tutorial is to disseminate this research field, giving an overview of the current state of the art and the main methodologies and algorithms for spatial and spatio-temporal data mining. This tutorial is directed to researches and practitioners, experts in data mining, analysts of spatial and spatio-temporal data, as well as knowledge engineers and domain experts from different application areas.
[Geographic Information Systems, Conferences, information hide, data mining, geographic information systems, spatiotemporal data mining, semantic trajectory data mining, Data mining, climatologically effect, semantic trajectory pattern mining, mobile computing, USA Councils, moving object trajectory, satellite imagery, cellular phone, GPS device, Trajectory, trajectory data mining, electronic commerce, land use classification, data preprocessing, remote sensing, Spatial databases, fishing control, spatiotemporal phenomena, image motion analysis, Computer science, mobile device, pattern extraction, conventional system, sensor network, geospatial dataset, price reduction, knowledge engineer, remote sensing image, mobile commerce industry, spatial data mining, data encapsulation, satellite image, mobile handsets, pedestrian behavior analysis]
Knowledge Discovery in Academic Drug Discovery Programs: Opportunities and Challenges
2010 IEEE International Conference on Data Mining
None
2010
In United State several universities and research institutes including the national health institute (NIH) recently started programs aiming for drug discovery. With the initiatives, huge volumes of data have been collected and shared with public free of charge. Those initiatives provide an unprecedented opportunity for data miner and machine learner to study knowledge discovery problems associated with drug design. In this tutorial, the presenter will review the knowledge discovery and management needs in the drug discovery process. Latest methodology development, primarily those from data mining, machine learning, and statistical learning will be discussed.
[Drugs, Electrical engineering, statistical learning, drug design, data mining, Tutorials, Academic Drug Discovery Programs, knowledge discovery, Data mining, pharmaceuticals, machine learning, Chemicals, Computer science, drug discovery process, Lead, national health institute, learning (artificial intelligence)]
How to Do Good Data Mining Research and Get it Published in Top Venues
2010 IEEE International Conference on Data Mining
None
2010
While ICDM has traditionally enjoyed an unusually high quality of reviewing, there is no doubt that publishing in ICDM is very challenging. In this tutorial Dr. Keogh will demonstrate some simple ideas to enhance the probability of success in getting your paper published in a top data mining conference, and after the work is published, getting it highly cited.
[ICDM, data mining conference, Conferences, Communities, data mining, Tutorials, data mining research, Research, Data mining, Publishing, Databases, Awards activities, Experimentation]
Discovering Multiple Clustering Solutions: Grouping Objects in Different Views of the Data
2010 IEEE International Conference on Data Mining
None
2010
Traditional clustering algorithms identify just a single clustering of the data. Today's complex data, however, allow multiple interpretations leading to several valid groupings hidden in different views of the database. Each of these multiple clustering solutions is valuable and interesting as different perspectives on the same data and several meaningful groupings for each object are given. Especially for high dimensional data where each object is described by multiple attributes, alternative clusters in different attribute subsets are of major interest. In this tutorial, we describe several real world application scenarios for multiple clustering solutions. We abstract from these scenarios and provide the general challenges in this emerging research area. We describe state-of-the-art paradigms, we highlight specific techniques, and we give an overview of this topic by providing a taxonomy of the existing methods. By focusing on open challenges, we try to attract young researchers for participating in this emerging research field.
[multiple perspectives, Conferences, Communities, data mining, Tutorials, Data mining, orthogonal clustering, Databases, Atmospheric measurements, pattern clustering, high dimensional data, subspace clustering, Focusing, multiple interpretation, alternative clustering, multiple clustering solution, grouping object]
[Publisher information]
2010 IEEE International Conference on Data Mining
None
2010
Provides a listing of current committee members and society officers.
[]
Message from the Conference General Chairs
2011 IEEE 11th International Conference on Data Mining
None
2011
Presents the welcome message from the conference proceedings.
[]
Message from the Program Co-Chairs
2011 IEEE 11th International Conference on Data Mining
None
2011
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
2011 IEEE 11th International Conference on Data Mining
None
2011
Provides a listing of current committee members.
[]
Steering Committee
2011 IEEE 11th International Conference on Data Mining
None
2011
Provides a listing of current committee members.
[]
Program Committee
2011 IEEE 11th International Conference on Data Mining
None
2011
Provides a listing of current committee members and society officers.
[]
Infrastructure Pattern Discovery in Configuration Management Databases via Large Sparse Graph Mining
2011 IEEE 11th International Conference on Data Mining
None
2011
A configuration management database (CMDB) can be considered to be a large graph representing the IT infrastructure entities and their inter-relationships. Mining such graphs is challenging because they are large, complex, and multi-attributed, and have many repeated labels. These characteristics pose challenges for graph mining algorithms, due to the increased cost of sub graph isomorphism (for support counting), and graph isomorphism (for eliminating duplicate patterns). The notion of pattern frequency or support is also more challenging in a single graph, since it has to be defined in terms of the number of its (potentially, exponentially many) embeddings. We present CMDB-Miner, a novel two-step method for mining infrastructure patterns from CMDB graphs. It first samples the set of maximal frequent patterns, and then clusters them to extract the representative infrastructure patterns. We demonstrate the effectiveness of CMDB-Miner on real-world CMDB graphs.
[Image edge detection, graph theory, data mining, single graph mining, Entropy, maximal frequent patterns, CMDB-Miner, Data mining, Servers, database management systems, frequent subgraphs, representative infrastructure patterns, Upper bound, Databases, Organizations, IT infrastructure, configuration management databases, infrastructure pattern discovery, business data processing, organisational aspects, large sparse graph mining]
Role-Behavior Analysis from Trajectory Data by Cross-Domain Learning
2011 IEEE 11th International Conference on Data Mining
None
2011
Behavior analysis using trajectory data presents a practical and interesting challenge for KDD. Conventional analyses address discriminative tasks of behaviors, e.g., classification and clustering typically using the subsequences extracted from the trajectory of an object as a numerical feature representation. In this paper, we explore further to identify the difference in the high-level semantics of behaviors such as roles and address the task in a cross-domain learning approach. The trajectory, from which the features are sampled, is intuitively viewed as a domain, and we assume that its intrinsic structure is characterized by the underlying role associated with the tracked object. We propose a novel hybrid method of spectral clustering and density approximation for comparing clustering structures of two independently sampled trajectory data and identifying patterns of behaviors unique to a role. We present empirical evaluations of the proposed method in two practical settings using real-world robotic trajectories.
[transfer learning, Conferences, density-based outlier detection, behavior discriminative tasks, KDD, behavior high-level semantics, behavior pattern identification, Data mining, time-series subsequence clustering, role-behavior analysis, robotic trajectories, pattern clustering, robots, trajectory data mining, learning (artificial intelligence), cross-domain learning approach, spectral clustering, trajectory data, density approximation]
Semi-supervised Feature Importance Evaluation with Ensemble Learning
2011 IEEE 11th International Conference on Data Mining
None
2011
We consider the problem of using a large amount of unlabeled data to improve the efficiency of feature selection in high dimensional datasets, when only a small set of labeled examples is available. We propose a new semi-supervised feature importance evaluation method (SSFI for short), that combines ideas from co-training and random forests with a new permutation-based out-of-bag feature importance measure. We provide empirical results on several benchmark datasets indicating that SSFI can lead to significant improvement over state-of-the-art semi-supervised and supervised algorithms.
[pattern classification, cotraining, Semi-Supervised Learning, random forests, Vectors, high dimensional datasets, permutation-based out-of-bag feature importance measure, Training, Radio frequency, Manifolds, ensemble learning, Feature Selection, Co-training, semisupervised feature importance evaluation, Prediction algorithms, unlabeled data, Labeling, learning (artificial intelligence), Bagging, feature selection, Ensemble Method, Random Subspaces Method]
COMET: A Recipe for Learning and Using Large Ensembles on Massive Data
2011 IEEE 11th International Conference on Data Mining
None
2011
COMET is a single-pass MapReduce algorithm for learning on large-scale data. It builds multiple random forest ensembles on distributed blocks of data and merges them into a mega-ensemble. This approach is appropriate when learning from massive-scale data that is too large to fit on a single machine. To get the best accuracy, IVoting should be used instead of bagging to generate the training subset for each decision tree in the random forest. Experiments with two large datasets (5GB and 50GB compressed) show that COMET compares favorably (in both accuracy and training time) to learning on a sub sample of data using a serial algorithm. Finally, we propose a new Gaussian approach for lazy ensemble evaluation which dynamically decides how many ensemble members to evaluate per data point, this can reduce evaluation cost by 100X or more.
[serial algorithm, importance-sampled voting, COMET, evaluation cost reduction, distributed processing, MapReduce, Training, Accuracy, decision tree, massive data learning, multiple random forest, Distributed databases, single-pass MapReduce algorithm, data distributed blocks, Lazy Ensemble Evaluation, training subset generation, Decision trees, learning (artificial intelligence), cost reduction, Gaussian approach, Computational modeling, IVoting, lazy ensemble evaluation, Massive Data, Decision Tree Ensembles, Vegetation, decision trees, Gaussian processes, data handling, Bagging]
Overlapping Correlation Clustering
2011 IEEE 11th International Conference on Data Mining
None
2011
We introduce a new approach to the problem of overlapping clustering. The main idea is to formulate overlapping clustering as an optimization problem in which each data point is mapped to a small set of labels, representing membership to different clusters. The objective is to find a mapping so that the distances between data points agree as much as possible with distances taken over their label sets. To define distances between label sets, we consider two measures: a set-intersection indicator function and the Jaccard coefficient. To solve the main optimization problem we propose a local-search algorithm. The iterative step of our algorithm requires solving non-trivial optimization sub problems, which, for the measures of set-intersection and Jaccard, we solve using a greedy method and non-negative least squares, respectively. Since our frameworks uses pair wise similarities of objects as the input, it lends itself naturally to the task of clustering structured objects for which feature vectors can be difficult to obtain. As a proof of concept we show how easily our framework can be applied in two different complex application domains. Firstly, we develop overlapping clustering of animal trajectories, obtaining zoologically meaningful results. Secondly, we apply our framework for overlapping clustering of proteins based on pair wise similarities of amino acid sequences, outperforming the of state-of-the-art method in matching a ground truth taxonomy.
[Algorithm design and analysis, optimization problem, Correlation, least squares approximations, greedy algorithms, local-search algorithm, nontrivial optimization sub problems, data points, Jaccard coefficient, set theory, label sets, Equations, Optimization, nonnegative least squares, Proteins, optimisation, pattern clustering, Clustering algorithms, membership representation, Labeling, set-intersection indicator function, search problems, overlapping correlation clustering, greedy method]
Learning with Minimum Supervision: A General Framework for Transductive Transfer Learning
2011 IEEE 11th International Conference on Data Mining
None
2011
Transductive transfer learning is one special type of transfer learning problem, in which abundant labeled examples are available in the source domain and only unlabeled examples are available in the target domain. It easily finds applications in spam filtering, microblogging mining and so on. In this paper, we propose a general framework to solve the problem by mapping the input features in both the source domain and target domain into a shared latent space and simultaneously minimizing the feature reconstruction loss and prediction loss. We develop one specific example of the framework, namely latent large-margin transductive transfer learning (LATTL) algorithm, and analyze its theoretic bound of classification loss via Rademacher complexity. We also provide a unified view of several popular transfer learning algorithms under our framework. Experiment results on one synthetic dataset and three application datasets demonstrate the advantages of the proposed algorithm over the other state-of-the-art ones.
[Conferences, target domain, Rademacher complexity, microblogging mining, spam filtering, LATTL, Data mining, learning (artificial intelligence), source domain, information networks, large-margin transductive transfer learning algorithm, computational complexity]
Confidence in Predictions from Random Tree Ensembles
2011 IEEE 11th International Conference on Data Mining
None
2011
Obtaining an indication of confidence of predictions is desirable for many data mining applications. Such confidence levels, together with the predicted value, can inform on the certainty or extent of reliability that may be associated with the prediction. This can be useful, for example, where model outputs are used in making potentially costly decisions, and one may then focus on the higher confidence predictions, and in general across risk sensitive applications. The conformal prediction framework presents a novel approach for complementing predictions from machine learning algorithms with valid confidence measures. Confidence levels are obtained from the underlying algorithm, using a non-conformity measure which indicates how 'atypical' a given example set is. The non-conformity measure is key to determining the usefulness and efficiency of the approach. This paper considers inductive conformal prediction in the context of random tree ensembles like random forests, which have been noted to perform favorably across problems. Focusing on classification tasks, and considering realistic data contexts including class imbalance, we develop non-conformity measures for assessing the confidence of predicted class labels from random forests. We examine the performance of these measures on multiple datasets. Results demonstrate the usefulness and validity of the measures, their relative differences, and highlight the effectiveness of conformal prediction random forests for obtaining predictions with associated confidence.
[pattern classification, conformal prediction, data mining, Predictive models, prediction confidence, classification tasks, Extraterrestrial measurements, random forests, Calibration, Data mining, classification, Training, conformal prediction framework, Confidence, Training data, Vegetation, random tree ensembles, data mining applications]
Mining Heavy Subgraphs in Time-Evolving Networks
2011 IEEE 11th International Conference on Data Mining
None
2011
Networks from different genres are not static entities, but exhibit dynamic behavior. The congestion level of links in transportation networks varies in time depending on the traffic. Similarly, social and communication links are employed at varying rates as information cascades unfold. In recent years there has been an increase of interest in modeling and mining dynamic networks. However, limited attention has been placed in high-scoring sub graph discovery in time-evolving networks. We define the problem of finding the highest-scoring temporal sub graph in a dynamic network, termed Heaviest Dynamic Sub graph (HDS). We show that HDS is NP-hard even with edge weights in {-1,1} and devise an efficient approach for large graph instances that evolve over long time periods. While a naive approach would enumerate all O(t2) sub-intervals, our solution performs an effective pruning of the sub-interval space by considering O(t&#x00B7;log(t)) groups of sub-intervals and computing an aggregate of each group in logarithmic time. We also define a fast heuristic and a tight upper bound for approximating the static version of HDS, and use them for further pruning the sub-interval space and quickly verifying candidate sub-intervals. We perform an extensive experimental evaluation of our algorithm on transportation, communication and social media networks for discovering sub graphs that correspond to traffic congestions, communication overflow and localized social discussions. Our method is two orders of magnitude faster than a naive approach and scales well with network size and time length.
[Steiner trees, heavy subgraph, pattern mining, graph theory, dynamic behavior, Transportation, transportation networks, Biology, time evolving networks, Complexity theory, Data mining, mining heavy subgraphs, transportation, Upper bound, optimisation, traffic congestions, NP-hard problem, HDS, dynamic networks, heaviest dynamic sub graph, graph discovery, Approximation algorithms, social media networks]
Multi-Class L2,1-Norm Support Vector Machine
2011 IEEE 11th International Conference on Data Mining
None
2011
Feature selection is an essential component of data mining. In many data analysis tasks where the number of data point is much less than the number of features, efficient feature selection approaches are desired to extract meaningful features and to eliminate redundant ones. In the previous study, many data mining techniques have been applied to tackle the above challenging problem. In this paper, we propose a new &#x2113;<sub>2,1</sub>-norm SVM, that is, multi-class hinge loss with a structured regularization term for all the classes to naturally select features for multi-class without bothering further heuristic strategy. Rather than directly solving the multi-class hinge loss with &#x2113;<sub>2,1</sub>-norm regularization minimization, which has not been solved before due to its optimization difficulty, we are the first to give an efficient algorithm bridging the new problem with a previous solvable optimization problem to do multi-class feature selection. A global convergence proof for our method is also presented. Via the proposed efficient algorithm, we select features across multiple classes with jointly sparsity, i.e., each feature has either small or large score over all classes. Comprehensive experiments have been performed on six bioinformatics data sets to show that our method can obtain better or competitive performance compared with exiting state-of-art multi-class feature selection approaches.
[multiclass hinge loss, data analysis, support vector machines, 1-norm support vector machine, data mining, multiclass feature selection, Fasteners, data analysis tasks, bioinformatics data sets, solvable optimization problem, Support Vector Machine, Data mining, Optimization, Convergence, Support vector machines, optimisation, Lungs, multiclass L2, multi-class feature selection, optimization difficulty, bioinformatics, Feature extraction, structured regularization term, feature selection]
SolarMap: Multifaceted Visual Analytics for Topic Exploration
2011 IEEE 11th International Conference on Data Mining
None
2011
Documents in rich text corpora often contain multiple facets of information. For example, an article from a medical document collection might consist of multifaceted information about symptoms, treatments, causes, diagnoses, prognoses, and preventions. Thus, documents in the collection may have different relations across each of these various facets. Topic analysis and exploration for such multi-relational corpora is a challenging visual analytic task. This paper presents Solar Map, a multifaceted visual analytic technique for visually exploring topics in multi-relational data. Solar Map simultaneously visualizes the topic distribution of the underlying entities from one facet together with keyword distributions that convey the semantic definition of each cluster along a secondary facet. Solar Map combines several visual techniques including 1) topic contour clusters and interactive multifaceted keyword topic rings, 2) a global layout optimization algorithm that aligns each topic cluster with its corresponding keywords, and 3) 2) an optimal temporal network segmentation and layout method that renders temporal evolution of clusters. Finally, the paper concludes with two case studies and quantitative user evaluation which show the power of the Solar Map technique.
[Visualization, text analysis, Visual Analytics, Multifaceted Information Visualization, rich text corpora, Diseases, topic analysis, keyword distributions, Temporal topic visualization, Layout, Data visualization, data visualisation, SolarMap, Tag clouds, Data models, medical document collection, topic exploration, Kernel, multifaceted visual analytic technique, optimal temporal network segmentation]
Efficiently Mining Unordered Trees
2011 IEEE 11th International Conference on Data Mining
None
2011
Frequent tree patterns have many applications in different domains such as XML document mining, user web log analysis, network routing and bioinformatics. In this paper, we first introduce three new tree encodings and accordingly present an efficient algorithm for finding frequent patterns from rooted unordered trees with the assumption that children of every node in database trees are identically labeled. Then, we generalize the method and propose the UITree algorithm to find frequent patterns from rooted unordered trees without any restriction. Compared to other algorithms in the literature, UItree manages occurrences of a candidate tree in database trees more efficiently. Our extensive experiments on both real and synthetic datasets show that UITree significantly outperforms the most efficient existing works on mining unordered trees.
[network routing, data mining, XML document mining, Routing, Encoding, frequent tree patterns, user web log analysis, Data mining, tree encoding, UITree algorithm, rooted unordered trees, frequency counting, candidate generation, Frequent tree patterns, Databases, Clustering algorithms, XML, bioinformatics, database trees, tree data structures, efficiently mining unordered trees, Bioinformatics]
CEMiner -- An Efficient Algorithm for Mining Closed Patterns from Time Interval-Based Data
2011 IEEE 11th International Conference on Data Mining
None
2011
The mining of closed sequential patterns has attracted researchers for its capability of using compact results to preserve the same expressive power as conventional mining. However, existing studies only focus on time point-based data. Few research efforts have elaborated on discovering closed sequential patterns from time interval-based data, where each data persists for a period of time. Mining closed time interval-based patterns, also called closed temporal patterns, is an arduous problem since the pair wise relationships between two interval-based events are intrinsically complex. In this paper, an efficient algorithm, CEMiner is developed to discover closed temporal patterns from interval-based data. Algorithm CEMiner employs some optimization techniques to effectively reduce the search space. The experimental results on both synthetic and real datasets indicate that CEMiner not only significantly outperforms the prior interval-based mining algorithms in terms of execution time but also possesses graceful scalability. The experiment conducted on real dataset shows the practicability of time interval-based closed pattern mining.
[Algorithm design and analysis, Scalability, data mining, closed sequential patterns mining, time point based data, interval based events, Data mining, closed temporal pattern, time interval-based data, Diseases, optimisation, Itemsets, CEMiner, optimization techniques, endpoint representation, sequential pattern mining, temporal patterns, time interval based data, Finishing]
LinkBoost: A Novel Cost-Sensitive Boosting Framework for Community-Level Network Link Prediction
2011 IEEE 11th International Conference on Data Mining
None
2011
Link prediction is a challenging task due to the inherent skew ness of network data. Typical link prediction methods can be categorized as either local or global. Local methods consider the link structure in the immediate neighborhood of a node pair to determine the presence or absence of a link, whereas global methods utilize information from the whole network. This paper presents a community (cluster) level link prediction method without the need to explicitly identify the communities in a network. Specifically, a variable-cost loss function is defined to address the data skew ness problem. We provide theoretical proof that shows the equivalence between maximizing the well-known modularity measure used in community detection and minimizing a special case of the proposed loss function. As a result, any link prediction method designed to optimize the loss function would result in more links being predicted within a community than between communities. We design a boosting algorithm to minimize the loss function and present an approach to scale-up the algorithm by decomposing the network into smaller partitions and aggregating the weak learners constructed from each partition. Experimental results show that our proposed Link Boost algorithm consistently performs as good as or better than many existing methods when evaluated on 4 real-world network datasets.
[Algorithm design and analysis, Communities, Predictive models, network theory (graphs), Boosting, Loss measurement, boosting algorithm, Partitioning algorithms, Modularity, LinkBoost, Link Prediction, Social Networks, Community Detection, community level network link prediction, Prediction algorithms, data handling, novel cost sensitive boosting framework]
Learning Dirichlet Processes from Partially Observed Groups
2011 IEEE 11th International Conference on Data Mining
None
2011
Motivated by the task of vernacular news analysis using known news topics from national news-papers, we study the task of topic analysis, where given source datasets with observed topics, data items from a target dataset need to be assigned either to observed source topics or to new ones. Using Hierarchical Dirichlet Processes for addressing this task imposes unnecessary and often inappropriate generative assumptions on the observed source topics. In this paper, we explore Dirichlet Processes with partially observed groups (POG-DP). POG-DP avoids modeling the given source topics. Instead, it directly models the conditional distribution of the target data as a mixture of a Dirichlet Process and the posterior distribution of a Hierarchical Dirichlet Process with known groups and topics. This introduces coupling between selection probabilities of all topics within a source, leading to effective identification of source topics. We further improve on this with a Combinatorial Dirichlet Process with partially observed groups (POG-CDP) that captures finer grained coupling between related topics by choosing intersections between sources. We evaluate our models in three different real-world applications. Using extensive experimentation, we compare against several baselines to show that our model performs significantly better in all three applications.
[information resources, vernacular news analysis, partial observations, probability, combinatorial dirichlet process, Companies, Equations, news topics, Couplings, national news-papers, Dirichlet Process, grouped data, topic analysis, Hidden Markov models, Data models, Inference algorithms, combinatorial dirichlet process with partially observed groups, selection probabilities, Mathematical model]
Exploiting False Discoveries -- Statistical Validation of Patterns and Quality Measures in Subgroup Discovery
2011 IEEE 11th International Conference on Data Mining
None
2011
Subgroup discovery suffers from the multiple comparisons problem: we search through a large space, hence whenever we report a set of discoveries, this set will generally contain false discoveries. We propose a method to compare subgroups found through subgroup discovery with a statistical model we build for these false discoveries. We determine how much the subgroups we find deviate from the model, and hence statistically validate the found subgroups. Furthermore we propose to use this subgroup validation to objectively compare quality measures used in subgroup discovery, by determining how much the top subgroups we find with each measure deviate from the statistical model generated with that measure. We thus aim to determine how good individual measures are in selecting significant findings. We invoke our method to experimentally compare popular quality measures in several subgroup discovery settings.
[data mining, Search problems, Size measurement, statistical model, Complexity theory, Association rules, Histograms, quality measures, false discoveries, statistical pattern validation, Statistical validation, Silicon, statistical analysis, subgroup discovery]
An Efficient Greedy Method for Unsupervised Feature Selection
2011 IEEE 11th International Conference on Data Mining
None
2011
In data mining applications, data instances are typically described by a huge number of features. Most of these features are irrelevant or redundant, which negatively affects the efficiency and effectiveness of different learning algorithms. The selection of relevant features is a crucial task which can be used to allow a better understanding of data or improve the performance of other learning tasks. Although the selection of relevant features has been extensively studied in supervised learning, feature selection with the absence of class labels is still a challenging task. This paper proposes a novel method for unsupervised feature selection, which efficiently selects features in a greedy manner. The paper first defines an effective criterion for unsupervised feature selection which measures the reconstruction error of the data matrix based on the selected subset of features. The paper then presents a novel algorithm for greedily minimizing the reconstruction error based on the features selected so far. The greedy algorithm is based on an efficient recursive formula for calculating the reconstruction error. Experiments on real data sets demonstrate the effectiveness of the proposed algorithm in comparison to the state-of-the-art methods for unsupervised feature selection.
[Greedy algorithms, Laplace equations, greedy algorithms, learning algorithms, data mining, supervised learning, Vectors, Unsupervised Learning, unsupervised feature selection, performance improvement, matrix algebra, unsupervised learning, Feature Selection, data instances, data matrix reconstruction error, Greedy Algorithms, learning tasks, feature extraction, Clustering algorithms, Feature extraction, Matrices, Principal component analysis, greedy method]
Structured Feature Selection and Task Relationship Inference for Multi-task Learning
2011 IEEE 11th International Conference on Data Mining
None
2011
Multi-task Learning (MTL) aims to enhance the generalization performance of supervised regression or classification by learning multiple related tasks simultaneously. In this paper, we aim to extend the current MTL techniques to high dimensional data sets with structured input and structured output (SISO), where the SI means the input features are structured and the SO means the tasks are structured. We investigate a completely ignored problem in MTL with SISO data: the interaction of structured feature selection and task relationship modeling. We hypothesize that combining the structure information of features and task relationship inference enables us to build more accurate MTL models. Based on the hypothesis, we have designed an efficient learning algorithm, in which we utilize a task covariance matrix related to the model parameters to capture the task relationship. In addition, we design a regularization formulation for incorporating the structure of features in MTL. We have developed an efficient iterative optimization algorithm to solve the corresponding optimization problem. Our algorithm is based on the accelerated first order gradient method in conjunction with the projected gradient scheme. Using two real-world data sets, the experimental results demonstrate the utility of the proposed learning methods.
[MTL techniques, task relationship modeling, regression analysis, Multi-task Learning, Optimization, Convergence, covariance matrix, task relationship inference, optimisation, SISO data, supervised classification, feature extraction, supervised regression, data sets, Task Relationship Inference, Silicon, iterative optimization algorithm, learning (artificial intelligence), gradient methods, projected gradient scheme, regularization formulation, first order gradient method, covariance matrices, Structural Sparsity, Structured Input and Structured Output, Covariance matrix, inference mechanisms, multitask learning, structured feature selection, Data models, data handling, Acceleration, Cancer]
A Taxi Driving Fraud Detection System
2011 IEEE 11th International Conference on Data Mining
None
2011
Advances in GPS tracking technology have enabled us to install GPS tracking devices in city taxis to collect a large amount of GPS traces under operational time constraints. These GPS traces provide unparallel opportunities for us to uncover taxi driving fraud activities. In this paper, we develop a taxi driving fraud detection system, which is able to systematically investigate taxi driving fraud. In this system, we first provide functions to find two aspects of evidences: travel route evidence and driving distance evidence. Furthermore, a third function is designed to combine the two aspects of evidences based on Dempster-Shafer theory. To implement the system, we first identify interesting sites from a large amount of taxi GPS logs. Then, we propose a parameter-free method to mine the travel route evidences. Also, we introduce route mark to represent a typical driving path from an interesting site to another one. Based on route mark, we exploit a generative statistical model to characterize the distribution of driving distance and identify the driving distance evidences. Finally, we evaluate the taxi driving fraud detection system with large scale real-world taxi GPS logs. In the experiments, we uncover some regularity of driving fraud activities and investigate the motivation of drivers to commit a driving fraud by analyzing the produced taxi fraud data.
[Estimation, Independent component analysis, driving distance evidence, Dempster-Shafer theory, Encoding, Vectors, taxi driving fraud detection system, Taxi Driving Fraud, parameter-free method, Dempster-Shafer Theory, Global Positioning System, Vehicles, Location Traces, driver information systems, fraud, Trajectory, statistical analysis, travel route evidence, generative statistical model]
Isograph: Neighbourhood Graph Construction Based on Geodesic Distance for Semi-supervised Learning
2011 IEEE 11th International Conference on Data Mining
None
2011
Semi-supervised learning based on manifolds has been the focus of extensive research in recent years. Convenient neighbourhood graph construction is a key component of a successful semi-supervised classification method. Previous graph construction methods fail when there are pairs of data points that have small Euclidean distance, but are far apart over the manifold. To overcome this problem, we start with an arbitrary neighbourhood graph and iteratively update the edge weights by using the estimates of the geodesic distances between points. Moreover, we provide theoretical bounds on the values of estimated geodesic distances. Experimental results on real-world data show significant improvement compared to the previous graph construction methods.
[Image edge detection, Isograph, semisupervised learning, graph theory, Estimation, geodesic distance, data points, Data mining, Graph Construction, Manifolds, Semi-supervised Learning, Manifold, Euclidean distance, neighbourhood graph construction, Geodesic distance, Labeling, learning (artificial intelligence), Joining processes]
D-cores: Measuring Collaboration of Directed Graphs Based on Degeneracy
2011 IEEE 11th International Conference on Data Mining
None
2011
Community detection and evaluation is an important task in graph mining. In many cases, a community is defined as a sub graph characterized by dense connections or interactions among its nodes. A large variety of measures have been proposed to evaluate the quality of such communities - in most cases ignoring the directed nature of edges. In this paper, we introduce novel metrics for evaluating the collaborative nature of directed graphs - a property not captured by the single node metrics or by other established community evaluation metrics. In order to accomplish this objective, we capitalize on the concept of graph degeneracy and define a novel D-core framework, extending the classic graph-theoretic notion of k-cores for undirected graphs to directed ones. Based on the D-core, which essentially can be seen as a measure of the robustness of a community under degeneracy, we devise a wealth of novel metrics used to evaluate graph collaboration features of directed graphs. We applied the D-core approach on large real-world graphs such as Wikipedia and DBLP and report interesting results at the graph as well at node level.
[Measurement, Graph Mining, collaboration measurement, Communities, graph theory, Encyclopedias, Indexes, graph mining, Cores, D-cores, Degeneracy, directed graphs, Collaboration, graph degeneracy, community evaluation, Internet, community detection, Community evaluation metrics, k-cores]
SIMPATH: An Efficient Algorithm for Influence Maximization under the Linear Threshold Model
2011 IEEE 11th International Conference on Data Mining
None
2011
There is significant current interest in the problem of influence maximization: given a directed social network with influence weights on edges and a number k, find k seed nodes such that activating them leads to the maximum expected number of activated nodes, according to a propagation model. Kempe et al. showed, among other things, that under the Linear Threshold Model, the problem is NP-hard, and that a simple greedy algorithm guarantees the best possible approximation factor in PTIME. However, this algorithm suffers from various major performance drawbacks. In this paper, we propose SIMPATH, an efficient and effective algorithm for influence maximization under the linear threshold model that addresses these drawbacks by incorporating several clever optimizations. Through a comprehensive performance study on four real data sets, we show that SIMPATH consistently outperforms the state of the art w.r.t. running time, memory consumption and the quality of the seed set chosen, measured in terms of expected influence spread achieved.
[Greedy algorithms, Linear Threshold Model, memory consumption, SlMPATH, Optimization, approximation factor, optimisation, PTIME, directed social network, Viral Marketing, Mathematical model, greedy algorithm, approximation theory, Computational modeling, greedy algorithms, Estimation, Simple Path Enumeration, Influence Spread, Algorithms, NP-hard problem, Social Networks, social networking (online), influence maximization, propagation model, linear threshold model, seed set quality, Integrated circuit modeling, computational complexity, optimizations]
Cross Domain Random Walk for Query Intent Pattern Mining from Search Engine Log
2011 IEEE 11th International Conference on Data Mining
None
2011
Understanding search intents of users through their condensed short queries has attracted much attention both in academia and industry. The search intents of users are generally assumed to be associated with various query patterns, such as "MobileName price\
[Algorithm design and analysis, transfer learning, search engines, transition probability, MobileName price, semisupervised learning, data mining, probability, Manuals, random processes, search engine click-through log data, query intent pattern, Mobile handsets, Data mining, Bridges, query processing, random walk, cross domain random walk algorithm, Training data, Search engines, query intent pattern mining, semi-supervised learning, learning (artificial intelligence), mobile phone model]
Flexible Fault Tolerant Subspace Clustering for Data with Missing Values
2011 IEEE 11th International Conference on Data Mining
None
2011
In today's applications, data analysis tasks are hindered by many attributes per object as well as by faulty data with missing values. Subspace clustering tackles the challenge of many attributes by cluster detection in any subspace projection of the data. However, it poses novel challenges for handling missing values of objects, which are part of multiple subspace clusters in different projections of the data. In this work, we propose a general fault tolerance definition enhancing subspace clustering models to handle missing values. We introduce a flexible notion of fault tolerance that adapts to the individual characteristics of subspace clusters and ensures a robust parameterization. Allowing missing values in our model increases the computational complexity of subspace clustering. Thus, we prove novel monotonicity properties for an efficient computation of fault tolerant subspace clusters. Experiments on real and synthetic data show that our fault tolerance model yields high quality results even in the presence of many missing values. For repeatability, we provide all datasets and executables on our website.
[robust parameterization, Adaptation models, data analysis, fault tolerance, Computational modeling, data analysis tasks, Website, Approximation methods, Data mining, Fault tolerance, Databases, pattern clustering, subspace clustering, Fault tolerant systems, cluster detection, fault tolerant computing, Web sites, monotonicity properties, fault tolerant subspace data clustering, incomplete data, computational complexity, missing values]
Heuristic Updatable Weighted Random Subspaces for Non-stationary Environments
2011 IEEE 11th International Conference on Data Mining
None
2011
Learning in non-stationary environments is an increasingly important problem in a wide variety of real-world applications. In non-stationary environments data arrives incrementally, however the underlying generating function may change over time. While there is a variety of research into such environments, the research mainly consists of detecting concept drift (and then relearning the model), or developing classifiers which adapt to drift incrementally. We introduce Heuristic Up datable Weighted Random Subspaces (HUWRS), a new technique based on the Random Subspace Method that detects drift in individual features via the use of Hellinger distance, a distributional divergence metric. Through the use of subspaces, HUWRS allows for a more fine-grained approach to dealing with concept drift which is robust to feature drift even without class labels. We then compare our approach to two state of the art algorithms, concluding that for a wide range of datasets and window sizes HUWRS outperforms the other methods.
[Context, data mining, random processes, distributional divergence matrix, heuristic updatable weighted random subspaces, Classification algorithms, Data mining, Random Subspaces, Hellinger Distance, Training, Concept Drift, Hellinger distance, Accuracy, fine-grained approach, nonstationary learning environments, Feature extraction, learning (artificial intelligence), Non-stationary learning, Testing]
Learning Tags from Unsegmented Videos of Multiple Human Actions
2011 IEEE 11th International Conference on Data Mining
None
2011
Providing methods to support semantic interaction with growing volumes of video data is an increasingly important challenge for data mining. To this end, there has been some success in recognition of simple objects and actions in video, however most of this work requires strongly supervised training data. The supervision cost of these approaches therefore renders them economically non-scalable for real world applications. In this paper we address the problem of learning to annotate and retrieve semantic tags of human actions in realistic video data with sparsely provided tags of semantically salient activities. This is challenging because of (1) the multi-label nature of the learning problem and (2) realistic videos are often dominated by (semantically uninteresting) background activity un-supported by any tags of interest, leading to a strong irrelevant data problem. To address these challenges, we introduce a new topic model based approach to video tag annotation. Our model simultaneously learns a low dimensional representation of the video data, which dimensions are semantically relevant (supported by tags), and how to annotate videos with tags. Experimental evaluation on three different video action/activity datasets demonstrate the challenge of this problem, and value of our contribution.
[object recognition, Visualization, background activity, Humans, data mining, Predictive models, video data representation, Videos, Training, image segmentation, human action, video retrieval, video tag annotation, learning (artificial intelligence), video signal processing, annotation, semantic tag retrieval, realistic video data mining, topic model, Vectors, unsegmented video, activity dataset, tag learning, learning problem, action recognition, Data models, video data mining]
Generating Breakpoint-based Timeline Overview for News Topic Retrospection
2011 IEEE 11th International Conference on Data Mining
None
2011
Though news readers can easily access a large number of news articles from the Internet, they can be overwhelmed by the quantity of information available, making it hard to get a concise, global picture of a news topic. In this paper we propose a novel method to address this problem. Given a set of articles for a given news topic, the proposed method models theme variation through time and identifies the breakpoints, which are time points when decisive changes occur. For each breakpoint, a brief summary is automatically constructed based on articles associated with the particular time point. Summaries are then ordered chronologically to form a timeline overview of the news topic. In this fashion, readers can easily track various news topics efficiently. We have conducted experiments on 15 popular topics in 2010. Empirical experiments show the effectiveness of our approach and its advantages over other approaches.
[information resources, Google, Breakpoint, breakpoint based timeline overview, Data Mining, Electronic mail, Text Mining, Dispersion, Analytical models, News Topic Retrospection, news topic retrospection, Hidden Markov models, Markov processes, Search engines, Internet, news readers]
A Robust Clustering Algorithm Based on Aggregated Heat Kernel Mapping
2011 IEEE 11th International Conference on Data Mining
None
2011
Current spectral clustering algorithms suffer from both sensitivity to scaling parameter selection in similarity matrix construction, and data perturbation. This paper aims to improve robustness in clustering algorithms and combat these two limitations based on heat kernel theory. Heat kernel can statistically depict traces of random walk, so it has an intrinsic connection with diffusion distance, with which we can ensure robustness during any clustering process. By integrating heat distributed along time scale, we propose a novel method called Aggregated Heat Kernel (AHK) to measure the distance between each point pair in their eigen space. Using AHK and Laplace-Beltrami Normalization (LBN) we are able to apply an advanced noise-resisting robust spectral mapping to original dataset. Moreover it offers stability on scaling parameter tuning. Experimental results show that, compared to other popular spectral clustering methods, our algorithm can achieve robust clustering results on both synthetic and UCI real datasets.
[Noise, data mining, intrinsic connection, Laplace-Beltrami normalization, data perturbation, knowledge discovery, similarity matrix construction, unsupervised knowledge exploration, distance measurement, Clustering algorithms, aggregated heat kernel mapping, Robustness, robust clustering algorithm, Kernel, Laplace equations, scaling parameter selection, Diffusion processes, sensitivity parameter selection, Spectral analysis, matrix algebra, unsupervised learning, diffusion distance, Sensitivity, pattern clustering, Heating, spectral clustering algorithms, noise-resisting robust spectral mapping, Green's function methods]
Patent Maintenance Recommendation with Patent Information Network Model
2011 IEEE 11th International Conference on Data Mining
None
2011
Patents are of crucial importance for businesses, because they provide legal protection for the invented techniques, processes or products. A patent can be held for up to 20 years. However, large maintenance fees need to be paid to keep it enforceable. If the patent is deemed not valuable, the owner may decide to abandon it by stopping paying the maintenance fees to reduce the cost. For large companies or organizations, making such decisions is difficult because too many patents need to be investigated. In this paper, we introduce the new patent mining problem of automatic patent maintenance prediction, and propose a systematic solution to analyze patents for recommending patent maintenance decision. We model the patents as a heterogeneous time-evolving information network and propose new patent features to build model for a ranked prediction on whether to maintain or abandon a patent. In addition, a network-based refinement approach is proposed to further improve the performance. We have conducted experiments on the large scale United States Patent and Trademark Office (USPTO) database which contains over four million granted patents. The results show that our technique can achieve high performance.
[United States patent and trademark office database, patent information network, patent maintenance decision, data mining, Companies, Predictive models, patent information network model, patent maintenance recommendation, maintenance fees, invented techniques, patents, patent mining problem, businesses, automatic patent maintenance prediction, Patents, Maintenance engineering, decision support systems, recommender systems, patent maintenance, legal protection, Writing, organizations, patent mining, prediction, Feature extraction, ranking, organisational aspects]
S-preconditioner for Multi-fold Data Reduction with Guaranteed User-Controlled Accuracy
2011 IEEE 11th International Conference on Data Mining
None
2011
The growing gap between the massive amounts of data generated by petascale scientific simulation codes and the capability of system hardware and software to effectively analyze this data necessitates data reduction. Yet, the increasing data complexity challenges most, if not all, of the existing data compression methods. In fact, lossless compression techniques offer no more than 10% reduction on scientific data that we have experience with, which is widely regarded as effectively incompressible. To bridge this gap, in this paper, we advocate a transformative strategy that enables fast, accurate, and multi-fold reduction of double-precision floating-point scientific data. The intuition behind our method is inspired by an effective use of preconditioners for linear algebra solvers optimized for a particular class of computational "dwarfs" (e.g., dense or sparse matrices). Focusing on a commonly used multi-resolution wavelet compression technique as the underlying "solver" for data reduction we propose the S-preconditioner, which transforms scientific data into a form with high global regularity to ensure a significant decrease in the number of wavelet coefficients stored for a segment of data. Combined with the subsequent EQ-calibrator, our resultant method (called S-Preconditioned EQ-Calibrated Wavelets (SPEQC-Wavelets)), robustly achieved a 4- to 5-fold data reduction-while guaranteeing user-defined accuracy of reconstructed data to be within 1% point-by-point relative error, lower than 0.01 Normalized RMSE, and higher than 0.99 Pearson Correlation. In this paper, we show the results we obtained by testing our method on six petascale simulation codes including fusion, combustion, climate, astrophysics, and subsurface groundwater in addition to 13 publicly available scientific datasets. We also demonstrate that application-driven data mining tasks performed on decompressed variables or their derived quantities produce results of comparable quality with the ones for the original data.
[Correlation, wavelet transforms, user-controlled accuracy, data mining, data reconstruction, data mining over decompressed data, lossless data compression techniques, Sparse matrices, in situ data analytics, data reduction, Accuracy, wavelet coefficients, multiresolution wavelet compression technique, application driven data mining, mean square error methods, Wavelet transforms, data analysis, petascale scientific simulation codes, transformative strategy, system software capability, double-precision floating-point scientific data, S-preconditioner, Vectors, linear algebra solvers, system hardware capability, Indexes, extreme-scale data analytics, multifold data reduction, preconditioners for data mining, data complexity, Data models, Pearson correlation]
Beyond 'Caveman Communities': Hubs and Spokes for Graph Compression and Mining
2011 IEEE 11th International Conference on Data Mining
None
2011
Given a real world graph, how should we lay-out its edges? How can we compress it? These questions are closely related, and the typical approach so far is to find clique-like communities, like the `cavemen graph', and compress them. We show that the block-diagonal mental image of the `cavemen graph' is the wrong paradigm, in full agreement with earlier results that real world graphs have no good cuts. Instead, we propose to envision graphs as a collection of hubs connecting spokes, with super-hubs connecting the hubs, and so on, recursively. Based on the idea, we propose the Slash Burn method (burn the hubs, and slash the remaining graph into smaller connected components). Our view point has several advantages: (a) it avoids the `no good cuts' problem, (b) it gives better compression, and (c) it leads to faster execution times for matrix-vector operations, which are the back-bone of most graph processing tools. Experimental results show that our Slash Burn method consistently outperforms other methods on all datasets, giving good compression and faster running time.
[Algorithm design and analysis, Graph Mining, data compression, Graph Compression, Communities, graph theory, data mining, Vectors, Hubs and Spokes, Complexity theory, Matrix decomposition, block-diagonal mental image, Equations, graph mining, cavemen graph, graph compression, slashburn method, Cost function, caveman community]
Improving Product Classification Using Images
2011 IEEE 11th International Conference on Data Mining
None
2011
Product classification in Commerce search (e.g., Google Product Search, Bing Shopping) involves associating categories to offers of products from a large number of merchants. The categorized offers are used in many tasks including product taxonomy browsing and matching merchant offers to products in the catalog. Hence, learning a product classifier with high precision and recall is of fundamental importance in order to provide high quality shopping experience. A product offer typically consists of a short textual description and an image depicting the product. Traditional approaches to this classification task is to learn a classifier using only the textual descriptions of the products. In this paper, we show that the use of images, a weaker signal in our setting, in conjunction with the textual descriptions, a more discriminative signal, can considerably improve the precision of the classification task, irrespective of the type of classifier being used. We present a novel classification approach, Confusion Driven Probabilistic Fusion++ (CDPF++), that is cognizant of the disparity in the discriminative power of different types of signals and hence makes use of the confusion matrix of dominant signal (text in our setting) to prudently leverage the weaker signal (image), for an improved performance. Our evaluation performed on data from a major Commerce search engine's catalog shows a 12% (absolute) improvement in precision at 100% coverage, and a 16% (absolute) improvement in recall at 90% precision compared to classifiers that only use textual description of products. In addition, CDPF++ also provides a more accurate classifier based only on the dominant signal (text) that can be used in situations in which only the dominant signal is available during application time.
[Computers, confusion matrix, Vocabulary, text analysis, commerce search engine catalog, search engines, image classification, discriminative signal, Taxonomy, product classification task, CDPF++, Training, textual description, product quality, e-commerce, text, learning (artificial intelligence), shopping experience, Business, electronic commerce, retail data processing, cataloguing, disparity cognizant, image, cognition, dominant signal, product taxonomy browsing, probability, Probabilistic logic, product classifier, product classification, confusion driven probabilistic fusion++, Internet, Catalogs]
Learning Markov Logic Networks via Functional Gradient Boosting
2011 IEEE 11th International Conference on Data Mining
None
2011
Recent years have seen a surge of interest in Statistical Relational Learning (SRL) models that combine logic with probabilities. One prominent example is Markov Logic Networks (MLNs). While MLNs are indeed highly expressive, this expressiveness comes at a cost. Learning MLNs is a hard problem and therefore has attracted much interest in the SRL community. Current methods for learning MLNs follow a two-step approach: first, perform a search through the space of possible clauses and then learn appropriate weights for these clauses. We propose to take a different approach, namely to learn both the weights and the structure of the MLN simultaneously. Our approach is based on functional gradient boosting where the problem of learning MLNs is turned into a series of relational functional approximation problems. We use two kinds of representations for the gradients: clause-based and tree-based. Our experimental evaluation on several benchmark data sets demonstrates that our new approach can learn MLNs as good or better than those found with state-of-the-art methods, but often in a fraction of the time.
[benchmark data sets, approximation theory, functional gradient boosting, Grounding, Boosting, relational functional approximation problems, Probabilistic models, Relational Learning, Markov random fields, Training, statistical relational learning models, Markov logic networks, Markov processes, clause based gradients, data handling, learning (artificial intelligence), statistical analysis, space search, tree based gradients, Ensemble learning, Joints, gradient methods, Statistical Relational Learning, Regression tree analysis]
Signature Pattern Covering via Local Greedy Algorithm and Pattern Shrink
2011 IEEE 11th International Conference on Data Mining
None
2011
Pattern mining is a fundamental problem that has a wide range of applications. In this paper, we study the problem of finding a minimum set of signature patterns that explain all data. In the problem, we are given objects where each object has an item set and a label. A pattern is called a signature pattern if all objects with the pattern have the same label. This problem has many interesting applications such as assertion mining in hardware design and identifying failure causes from various log data. We show that the previous pattern mining methods are not suitable for mining signature patterns and identify the problems. Then we propose a novel pattern enumeration method which we call Pattern Shrink. Our method is strongly coupled with another novel method that is very similar to finding a local optimum with a negligible loss in performance. Our proposed methods show a speedup of more than ten times over the previous methods. Our methods are flexible enough to be extended to mining high confidence patterns, instead of signature patterns.
[Greedy algorithms, Algorithm design and analysis, signature pattern covering, Set Covering, greedy algorithms, data mining, Search problems, set theory, Data mining, Approximation methods, pattern shrink, set covering, failure cause identification, Signature Mining, local greedy algorithm, hardware design, Approximation algorithms, Hardware, assertion mining, pattern mining methods, Pattern Covering, Local Greedy Algorithm]
TWITOBI: A Recommendation System for Twitter Using Probabilistic Modeling
2011 IEEE 11th International Conference on Data Mining
None
2011
Twitter provides search services to help people find new users to follow by recommending popular users or their friends' friends. However, these services do not offer the most relevant users to follow for a user. Furthermore, Twitter does not provide yet the search services to find the most interesting tweet messages for a user either. In this paper, we propose TWITOBI, a recommendation system for Twitter using probabilistic modeling for collaborative filtering which can recommend top-K users to follow and top-K tweets to read for a user. Our novel probabilistic model utilizes not only tweet messages but also the relationships between users. We develop an estimation algorithm for learning our model parameters and present its parallelized algorithm using MapReduce to handle large data. Our performance study with real-life data sets confirms the effectiveness and scalability of our algorithms.
[collaborative filtering, top-k users, Computational modeling, TWITOBI, real life data sets, probability, Twitter, Probabilistic logic, Probability distribution, search services, estimation algorithm, learning, probabilistic modeling, Equations, recommendation system, probabilistic model, MapReduce, recommender systems, social networking (online), Data models, Mathematical model, learning (artificial intelligence)]
Maximum Entropy Modelling for Assessing Results on Real-Valued Data
2011 IEEE 11th International Conference on Data Mining
None
2011
Statistical assessment of the results of data mining is increasingly recognised as a core task in the knowledge discovery process. It is of key importance in practice, as results that might seem interesting at first glance can often be explained by well-known basic properties of the data. In pattern mining, for instance, such trivial results can be so overwhelming in number that filtering them out is a necessity in order to identify the truly interesting patterns. In this paper, we propose an approach for assessing results on real-valued rectangular databases. More specifically, using our analytical model we are able to statistically assess whether or not a discovered structure may be the trivial result of the row and column marginal distributions in the database. Our main approach is to use the Maximum Entropy principle to fit a background model to the data while respecting its marginal distributions. To find these distributions, we employ an MDL based histogram estimator, and we fit these in our model using efficient convex optimization techniques. Subsequently, our model can be used to calculate probabilities directly, as well as to efficiently sample data with the purpose of assessing results by means of empirical hypothesis testing. Notably, our approach is efficient, parameter-free, and naturally deals with missing values. As such, it represents a well-founded alternative to swap randomisation.
[hypothesis testing, data mining, probability, Probabilistic logic, Entropy, Data mining, swap randomizations, statistical assessment, MDL based histogram estimator, Maximum Entropy modelling, marginal distribution, Histograms, optimisation, real-valued rectangular database, convex optimization technique, Databases, maximum entropy modelling, background knowledge, maximum entropy methods, Data models, statistical analysis, knowledge discovery process, Testing]
Local Models for Expectation-Driven Subgroup Discovery
2011 IEEE 11th International Conference on Data Mining
None
2011
Subgroup discovery (also known as Pattern Mining or Supervised Descriptive Rule Discovery) searches for descriptions of subsets in a dataset that differ from the total population with respect to a given target concept. In this paper we argue that in the traditional approach potentially interesting complex patterns with an unexpected relative increase of the target share remain undiscovered while on the other hand less surprising patterns are returned. Therefore, we present a generalized approach on subgroup discovery, in which the target share in the subgroup is not compared to the target share in the total population, but to the expectations a user has given the knowledge of more general (simpler) patterns. We claim that the resulting complex patterns are more interesting for the user and are less biased towards simpler patterns with a positive influence on the target concept. In order to estimate these expectations we utilize local models, i.e., fragments of Bayesian Networks. The proposed approach is evaluated using data from the UCI repository as well as on two totally different real world applications that investigate university student drop-out rates and identify spammers in a social book marking system.
[Knowledge engineering, Additives, expectation driven subgroup discovery, pattern mining, Estimation, data mining, complex patterns, Data mining, supervised descriptive rule discovery, Bayesian Networks, Itemsets, Bayesian methods, simpler patterns, Mathematical model, belief networks, learning (artificial intelligence), social book marking system, subgroup discovery, interestingness measures]
Context-Aware Multi-instance Learning Based on Hierarchical Sparse Representation
2011 IEEE 11th International Conference on Data Mining
None
2011
Multi-instance learning (MIL), a variant of supervised learning framework, has been applied in many applications. More recently, researchers focus on two important issues for MIL: Instances' contextual structures representation in the same bag and online MIL schemes. In this paper, we present an effective context-aware multi-instance learning technique using a hierarchical sparse representation (HSR-MIL) that addresses the two challenges simultaneously. We firstly construct the inner contextual structure among instances in the same bag based on a novel sparse &#x03B5;-graph. We then propose a graph kernel based sparse bag classifier through a modified kernel sparse coding in higher-dimension feature space. At last, the HSR-MIL approach is extended to achieve online learning manner with an incremental kernel matrix update scheme. The experiments on several data sets demonstrate that our method has better performances and online learning ability.
[Frequency selective surfaces, graph theory, sparse &#x03B5;-graph, Multi-Instance Learning, Encoding, Vectors, contextual structures representation, Sparse matrices, ubiquitous computing, hierarchical sparse representation, Training, Support vector machines, MIL, supervised learning framework, Context-aware, kernel matrix, learning (artificial intelligence), Kernel, context aware multiinstance learning, graph kernel, sparse bag classifier, online learning, Hierarchical Sparse Representation]
The Joint Inference of Topic Diffusion and Evolution in Social Communities
2011 IEEE 11th International Conference on Data Mining
None
2011
The prevalence of Web 2.0 techniques has led to the boom of various online communities, where topics spread ubiquitously among user-generated documents. Working together with this diffusion process is the evolution of topic content, where novel contents are introduced by documents which adopt the topic. Unlike explicit user behavior (e.g., buying a DVD), both the diffusion paths and the evolutionary process of a topic are implicit, making their discovery challenging. In this paper, we track the evolution of an arbitrary topic and reveal the latent diffusion paths of that topic in a social community. A novel and principled probabilistic model is proposed which casts our task as an joint inference problem, which considers textual documents, social influences, and topic evolution in a unified way. Specifically, a mixture model is introduced to model the generation of text according to the diffusion and the evolution of the topic, while the whole diffusion process is regularized with user-level social influences through a Gaussian Markov Random Field. Experiments on both synthetic data and real world data show that the discovery of topic diffusion and evolution benefits from this joint inference, and the probabilistic model we propose performs significantly better than existing methods.
[text analysis, Communities, Gaussian Markov random field, Web 2.0 techniques, online communities, user level social influence, real world data, mixture model, Tides, latent diffusion paths, joint inference problem, probabilistic model, text generation, synthetic data, social community, Joints, social influence, Social network services, Computational modeling, Diffusion processes, probability, random processes, user generated document, topic diffusion process, Vectors, inference mechanisms, topic content, textual document, Gaussian processes, Markov processes, social networking (online), Internet, topic evolution]
Towards Optimal Discriminating Order for Multiclass Classification
2011 IEEE 11th International Conference on Data Mining
None
2011
In this paper, we investigate how to design an optimized discriminating order for boosting multiclass classification. The main idea is to optimize a binary tree architecture, referred to as Sequential Discriminating Tree (SDT), that performs the multiclass classification through a hierarchical sequence of coarse-to-fine binary classifiers. To infer such a tree architecture, we employ the constrained large margin clustering procedure which enforces samples belonging to the same class to locate at the same side of the hyper plane while maximizing the margin between these two partitioned class subsets. The proposed SDT algorithm has a theoretic error bound which is shown experimentally to effectively guarantee the generalization performance. Experiment results indicate that SDT clearly beats the state-of-the-art multiclass classification algorithms.
[Algorithm design and analysis, pattern classification, SDT, sequential discriminating tree, trees (mathematics), Partitioning algorithms, binary tree architecture, Sequential Discriminating Tree, optimal discriminating order, Discriminating Order, Optimization, Training, Support vector machines, multiclass classification, Clustering algorithms, Multiclass, Testing]
A Hypergraph-based Method for Discovering Semantically Associated Itemsets
2011 IEEE 11th International Conference on Data Mining
None
2011
In this paper, we address an interesting data mining problem of finding semantically associated itemsets, i.e., items connected via indirect links. We propose a novel method for discovering semantically associated itemsets based on a hypergraph representation of the database. We describe two similarity measures to compute the strength of associations between items. Specifically, we introduce the average commute time similarity, s<sub>CT</sub>, based on the random walk model on hypergraph, and the inner-product similarity, s<sub>L+</sub>, based on the Moore-Penrose pseudoinverse of the hypergraph Laplacian matrix. Given semantically associated 2-itemsets generated by these measures, we design a hypergraph expansion method with two search strategies, namely, the clique and connected component search, to generate k-itemsets (k &gt;; 2). We show the proposed method is indeed capable of capturing semantically associated itemsets through experiments performed on three datasets ranging from low to high dimensionality. The semantically associated itemsets discovered in our experiment is promising to provide valuable insights on interrelationship between medical concepts and other domain specific concepts.
[average commute time similarity, graph theory, data mining, connected component search, medical concepts, database management systems, Blood, domain specific concepts, hypergraph database representation, random walk, hypergraph based method, hypergraph, Itemsets, Moore-Penrose pseudoinverse, Semantics, hypergraph Laplacian matrix, Semantically associated itemset, Mathematical model, data mining problem, search problems, inner product similarity, Laplace equations, hypergraph expansion method, semantically associated itemset discovery, search strategies, Equations, clique search, Joining processes]
Personalized Travel Package Recommendation
2011 IEEE 11th International Conference on Data Mining
None
2011
As the worlds of commerce, entertainment, travel, and Internet technology become more inextricably linked, new types of business data become available for creative use and formal analysis. Indeed, this paper provides a study of exploiting online travel information for personalized travel package recommendation. A critical challenge along this line is to address the unique characteristics of travel data, which distinguish travel packages from traditional items for recommendation. To this end, we first analyze the characteristics of the travel packages and develop a Tourist-Area-Season Topic (TAST) model, which can extract the topics conditioned on both the tourists and the intrinsic features (i.e. locations, travel seasons) of the landscapes. Based on this TAST model, we propose a cocktail approach on personalized travel package recommendation. Finally, we evaluate the TAST model and the cocktail approach on real-world travel package data. The experimental results show that the TAST model can effectively capture the unique characteristics of the travel data and the cocktail approach is thus much more effective than traditional recommendation methods for travel package recommendation.
[TAST model, business data, entertainment, real-world travel package data, Companies, commerce, cocktail approach, travel data, Internet technology, Motion pictures, formal analysis, Mathematical model, Recommender systems, personalized travel package recommendation, electronic commerce, landscapes, data analysis, Vectors, online travel information, recommender systems, travel industry, Collaboration, Internet, tourists, tourist-area-season topic model]
Tag Clustering and Refinement on Semantic Unity Graph
2011 IEEE 11th International Conference on Data Mining
None
2011
Recently, there has been extensive research towards the user-provided tags on photo sharing websites which can greatly facilitate image retrieval and management. However, due to the arbitrariness of the tagging activities, these tags are often imprecise and incomplete. As a result, quite a few technologies has been proposed to improve the user experience on these photo sharing systems, including tag clustering and refinement, etc. In this work, we propose a novel framework to model the relationships among tags and images which can be applied to many tag based applications. Different from previous approaches which model images and tags as heterogeneous objects, images and their tags are uniformly viewed as compositions of Semantic Unities in our framework. Then Semantic Unity Graph (SUG) is introduced to represent the complex and high-order relationships among these Semantic Unities. Based on the representation of Semantic Unity Graph, the relevance of images and tags can be naturally measured in terms of the similarity of their Semantic Unities. Then Tag clustering and refinement can then be performed on SUG and the polysemy of images and tags is explicitly considered in this framework. The experiment results conducted on NUS-WIDE and MIR-Flickr datasets demonstrate the effectiveness and efficiency of the proposed approach.
[Hypergraph, Visualization, Laplace equations, photo sharing systems, Image edge detection, SUG, graph theory, NUS-WIDE, Media, tag clustering, Clustering, MIR-Flickr datasets, semantic unities, pattern clustering, Semantics, tag refinement, Tagging, image retrieval, semantic unity graph, Bipartite graph, Web sites, Tag refinement]
Minimizing Seed Set for Viral Marketing
2011 IEEE 11th International Conference on Data Mining
None
2011
Viral marketing has attracted considerable concerns in recent years due to its novel idea of leveraging the social network to propagate the awareness of products. Specifically, viral marketing is to first target a limited number of users (seeds) in the social network by providing incentives, and these targeted users would then initiate the process of awareness spread by propagating the information to their friends via their social relationships. Extensive studies have been conducted for maximizing the awareness spread given the number of seeds. However, all of them fail to consider the common scenario of viral marketing where companies hope to use as few seeds as possible yet influencing at least a certain number of users. In this paper, we propose a new problem, called J-MIN-Seed, whose objective is to minimize the number of seeds while at least J users are influenced. J-MIN-Seed, unfortunately, is proved to be NP-hard in this work. In such case, we develop a greedy algorithm that can provide error guarantees for J-MIN-Seed. Furthermore, for the problem setting where J is equal to the number of all users in the social network, denoted by Full-Coverage, we design other efficient algorithms. Extensive experiments were conducted on real datasets to verify our algorithm.
[Algorithm design and analysis, Greedy algorithms, least J users, Seeds, Social network services, Probabilistic logic, marketing data processing, social relationships, social network, J-MIN-Seed, minimizing seed set, NP-hard problem, viral marketing, Approximation algorithms, social networking (online), Viral Marketing, Internet, Integrated circuit modeling]
Privacy Risk in Graph Stream Publishing for Social Network Data
2011 IEEE 11th International Conference on Data Mining
None
2011
To understand how social networks evolve over time, graphs representing the networks need to be published periodically or on-demand. The identity of the participants (nodes) must be anonymized to protect the privacy of the individuals and their relationships (edges) to the other members in the social network. We identify a new form of privacy attack, which we name the degree-trail attack. This attack re-identifies the nodes belonging to a target participant from a sequence of published graphs by comparing the degree of the nodes in the published graphs with the degree evolution of a target. The power of this attack is that the adversary can actively influence the degree of the target individual by interacting with the social network. We show that the adversary can succeed with a high probability even if published graphs are anonymized by strongest known privacy preserving techniques in the literature. Moreover, this success does not depend on the distinctiveness of the target nodes nor require the adversary to behave differently from a normal participant. One of our contributions is a formal method to assess the privacy risk of this type of attacks and empirically study the severity on real social network data.
[Data privacy, risk management, graph stream publishing, Social network services, graph theory, privacy attack, degree-trail attack, Educational institutions, privacy, privacy preserving techniques, privacy risk, social network, Privacy, Handheld computers, social network data, privacy protection, social networking (online), data privacy, anonymity, Pins, graph representation, data publishing, publishing]
Boolean Tensor Factorizations
2011 IEEE 11th International Conference on Data Mining
None
2011
Tensors are multi-way generalizations of matrices, and similarly to matrices, they can also be factorized, that is, represented (approximately) as a product of factors. These factors are typically either all matrices or a mixture of matrices and tensors. With the widespread adoption of matrix factorization techniques in data mining, also tensor factorizations have started to gain attention. In this paper we study the Boolean tensor factorizations. We assume that the data is binary multi-way data, and we want to factorize it to binary factors using Boolean arithmetic (i.e. defining that 1+1=1). Boolean tensor factorizations are, therefore, natural generalization of the Boolean matrix factorizations. We will study the theory of Boolean tensor factorizations and show that at least some of the benefits Boolean matrix factorizations have over normal matrix factorizations carry over to the tensor data. We will also present algorithms for Boolean variations of CP and Tucker decompositions, the two most-common types of tensor factorizations. With experimentation done with synthetic and real-world data, we show that Boolean tensor factorizations are a viable alternative when the data is naturally binary.
[data mining, tensor data, Vectors, tensors, Boolean algebra, matrix decomposition, Matrix decomposition, Data mining, Approximation methods, Tensile stress, Boolean Tucker decomposition, CP factorization, Boolean tensor factorization, Boolean matrix factorization, Boolean CP decomposition, Tensor factorization, Approximation algorithms, Argon, matrices, Tucker factorization, binary multiway data]
Sparse Domain Adaptation in Projection Spaces Based on Good Similarity Functions
2011 IEEE 11th International Conference on Data Mining
None
2011
We address the problem of domain adaptation for binary classification which arises when the distributions generating the source learning data and target test data are somewhat different. We consider the challenging case where no target labeled data is available. From a theoretical standpoint, a classifier has better generalization guarantees when the two domain marginal distributions are close. We study a new direction based on a recent framework of Balcan et al. allowing to learn linear classifiers in an explicit projection space based on similarity functions that may be not symmetric and not positive semi-definite. We propose a general method for learning a good classifier on target data with generalization guarantees and we improve its efficiency thanks to an iterative procedure by reweighting the similarity function - compatible with Balcan et al. framework - to move closer the two distributions in a new projection space. Hyper parameters and reweighting quality are controlled by a reverse validation procedure. Our approach is based on a linear programming formulation and shows good adaptation performances with very sparse models. We evaluate it on a synthetic problem and on real image annotation task.
[marginal distributions, Adaptation models, iterative methods, Transfer Learning, Binary Classification, sparse domain adaptation, Domain Adaptation, Aerospace electronics, linear programming, good similarity functions, Machine Learning, Training, source learning data, Robustness, learning (artificial intelligence), Kernel, Joints, pattern classification, hyper parameters, target test data, projection spaces, Similarity Functions, Machine learning, iterative procedure, data handling]
Incremental Elliptical Boundary Estimation for Anomaly Detection in Wireless Sensor Networks
2011 IEEE 11th International Conference on Data Mining
None
2011
Wireless Sensor Networks (WSNs) provide a low cost option for gathering spatially dense data from different environments. However, WSNs have limited energy resources that hinder the dissemination of the raw data over the network to a central location. This has stimulated research into efficient data mining approaches, which can exploit the restricted computational capabilities of the sensors to model their normal behavior. Having a normal model of the network, sensors can then forward anomalous measurements to the base station. Most of the current data modeling approaches proposed for WSNs require a fixed offline training period and use batch training in contrast to the real streaming nature of data in these networks. In addition they usually work in stationary environments. In this paper we present an efficient online model construction algorithm that captures the normal behavior of the system. Our model is capable of tracking changes in the data distribution in the monitored environment. We illustrate the proposed algorithm with numerical results on both real-life and simulated data sets, which demonstrate the efficiency and accuracy of our approach compared to existing methods.
[incremental elliptical boundary estimation, Adaptation models, data modeling approaches, Streaming Data Analysis, wireless sensor networks, data mining approaches, Computational modeling, data mining, anomaly detection, fixed offline training period, Incremental Elliptical Boundary Estimation, Covariance matrix, IDCAD, Training, batch training, Anomaly Detection, Wireless sensor networks, security of data, spatially dense data, Data models, Sensors]
Learning Classification with Auxiliary Probabilistic Information
2011 IEEE 11th International Conference on Data Mining
None
2011
Finding ways of incorporating auxiliary information or auxiliary data into the learning process has been the topic of active data mining and machine learning research in recent years. In this work we study and develop a new framework for classification learning problem in which, in addition to class labels, the learner is provided with an auxiliary (probabilistic) information that reflects how strong the expert feels about the class label. This approach can be extremely useful for many practical classification tasks that rely on subjective label assessment and where the cost of acquiring additional auxiliary information is negligible when compared to the cost of the example analysis and labelling. We develop classification algorithms capable of using the auxiliary information to make the learning process more efficient in terms of the sample complexity. We demonstrate the benefit of the approach on a number of synthetic and real world data sets by comparing it to the learning with class labels only.
[classification learning problem, pattern classification, learning process, Noise, machine learning research, classification learning, Humans, data mining, probability, classification algorithm, Probabilistic logic, auxiliary probabilistic information, sample complexity, auxiliary data, Machine learning, Data models, Concrete, learning with auxiliary label information, learning (artificial intelligence), active data mining, Logistics]
Word Cloud Model for Text Categorization
2011 IEEE 11th International Conference on Data Mining
None
2011
In centroid-based classification, each class is represented by a prototype or centroid document vector that is formed by averaging all member vectors during the training phase. In the prediction phase, the label of a test document vector is assigned to that of its nearest class prototype. Recently there has been revived interest in reformulating the prototype/centroid to improve classification performance. In this paper, we study the theoretical properties of the recently proposed Class Feature Centroid (CFC) classifier by considering the rate of change of each prototype vector with respect to individual dimensions (terms). The implication of our theoretical finding is that CFC is inherently biased towards large (dominant majority) classes, which means it is destined to perform poorly for highly class-imbalanced data. Another practical concern about CFC lies in its overly-aggressive design in weeding out terms that appear in all classes. To overcome these CFC limitations while retaining its intrinsic and worthy design goals, we propose an improved and robust centroid-based classifier that uses precise term-class distribution properties instead of simple presence or absence of terms in classes. Specifically, terms are weighted based on the Kullback-Leibler divergence measure between pairs of class-conditional term probabilities, we call this the CFC-KL centroid classifier. We then generalized CFC-KL to handle multi-class data by summing pair wise class-conditioned word probability ratios. Our proposed approach has been evaluated on 5 datasets, on which it consistently outperforms CFC and the baseline Support Vector Machine classifier. We also devise a word cloud visualization approach to highlight the important class-specific words picked out by our CFC-KL, and visually compare it with other popular term weigthing approaches. Our encouraging results show that the centroid based generalized CFC-KL classifier is both robust and efficient to deal with real-world text classification.
[text analysis, term-class distribution, data mining, text categorization, text classification, Text Categorization, CFC-KL centroid classifier, Radio frequency, Prototypes, data visualisation, Motion pictures, pair wise class-conditioned word probability ratio, probability, Vectors, test document vector, classification, CFC classifier, Kullback-Leibler divergence measure, class feature centroid, word cloud visualization approach, Support vector machines, word cloud model, Text categorization, centroid-based classification, class-conditional term probability, Tag clouds, Centroid-based Classification]
SLIM: Sparse Linear Methods for Top-N Recommender Systems
2011 IEEE 11th International Conference on Data Mining
None
2011
This paper focuses on developing effective and efficient algorithms for top-N recommender systems. A novel Sparse Linear Method (SLIM) is proposed, which generates top-N recommendations by aggregating from user purchase/rating profiles. A sparse aggregation coefficient matrix W is learned from SLIM by solving an &#x2113;<sub>1</sub>-norm and &#x2113;<sub>2</sub>-norm regularized optimization problem. W is demonstrated to produce high quality recommendations and its sparsity allows SLIM to generate recommendations very fast. A comprehensive set of experiments is conducted by comparing the SLIM method and other state-of-the-art top-N recommendation methods. The experiments show that SLIM achieves significant improvements both in run time performance and recommendation quality over the best existing methods.
[Measurement, sparse linear methods, sparse aggregation coefficient matrix, &#x2113;<sub>1</sub>-norm regularized optimization problem, &#x2113;<sub>2</sub>-norm regularized optimization problem, Vectors, Sparse matrices, Equations, Optimization, matrix algebra, purchase profiles, Sparse Linear Methods, rating profiles, optimisation, recommender systems, recommendation quality, run time performance, top-N recommender systems, l1-norm Regularization, SLIM, Mathematical model, Recommender systems, Top-N Recommender Systems]
Novel Recommendation Based on Personal Popularity Tendency
2011 IEEE 11th International Conference on Data Mining
None
2011
Recently, novel recommender systems have attracted considerable attention in the research community. Recommending popular items may not always satisfy users. For example, although most users likely prefer popular items, such items are often not very surprising or novel because users may already know about the items. Also, such recommender systems hardly satisfy a group of users who prefer relatively obscure items. Existing novel recommender systems, however, still recommend mainly popular items or degrade the quality of recommendation. They do so because they do not consider the balance between novelty and preference-based recommendation. This paper proposes an efficient novel-recommendation method called Personal Popularity Tendency Matching (PPTM) which recommends novel items by considering an individual's Personal Popularity Tendency (or PPT). Considering PPT helps to diversify recommendations by reasonably penalizing popular items while improving the recommendation accuracy. We experimentally show that the proposed method, PPTM, is better than other methods in terms of both novelty and accuracy.
[personal popularity tendency matching, Novel recommendation, Communities, Degradation, popular items, Personal Popularity Tendency, recommender systems, Atmospheric measurements, recommendation quality, Collaboration, recommendation accuracy, Motion pictures, Particle measurements, EMD, Recommender systems]
An Analysis of Performance Measures for Binary Classifiers
2011 IEEE 11th International Conference on Data Mining
None
2011
If one is given two binary classifiers and a set of test data, it should be straightforward to determine which of the two classifiers is the superior. Recent work, however, has called into question many of the methods heretofore accepted as standard for this task. In this paper, we analyze seven ways of determining if one classifier is better than another, given the same test data. Five of these are long established and two are relative newcomers. We review and extend work showing that one of these methods is clearly inappropriate, and then conduct an empirical analysis with a large number of datasets to evaluate the real-world implications of our theoretical analysis. Both our empirical and theoretical results converge strongly towards one of the newer methods.
[Weight measurement, pattern classification, empirical analysis, Performance Metrics, Communities, performance measurement analysis, Loss measurement, Vectors, set theory, test data set, Equations, Supervised Learning, Accuracy, Classifier Evaluation, binary classifiers, real-world implications, data handling]
Detection of Cross-Channel Anomalies from Multiple Data Channels
2011 IEEE 11th International Conference on Data Mining
None
2011
We identify and formulate a novel problem: cross channel anomaly detection from multiple data channels. Cross channel anomalies are common amongst the individual channel anomalies, and are often portent of significant events. Using spectral approaches, we propose a two-stage detection method: anomaly detection at a single-channel level, followed by the detection of cross-channel anomalies from the amalgamation of single channel anomalies. Our mathematical analysis shows that our method is likely to reduce the false alarm rate. We demonstrate our method in two applications: document understanding with multiple text corpora, and detection of repeated anomalies in video surveillance. The experimental results consistently demonstrate the superior performance of our method compared with related state-of-art methods, including the one-class SVM and principal component pursuit. In addition, our framework can be deployed in a decentralized manner, lending itself for large scale data stream analysis.
[text analysis, Dictionaries, Noise, multiple data channels, cross-channel anomaly detection, Data mining, Spectral methods, two-stage detection method, topic detection, amalgamation, video surveillance, data analysis, support vector machines, Peer to peer computing, one-class SVM, large scale data stream analysis, Vectors, single channel anomaly, document understanding, Covariance matrix, mathematical analysis, state-of-art methods, Anomaly detection, multiple text corpora, principal component pursuit, false alarm rate, security of data, spectral approaches, Nickel, principal component analysis, single-channel level]
Threshold Conditions for Arbitrary Cascade Models on Arbitrary Networks
2011 IEEE 11th International Conference on Data Mining
None
2011
Given a network of who-contacts-whom or who links-to-whom, will a contagious virus/product/meme spread and 'take-over' (cause an epidemic) or die-out quickly? What will change if nodes have partial, temporary or permanent immunity? The epidemic threshold is the minimum level of virulence to prevent a viral contagion from dying out quickly and determining it is a fundamental question in epidemiology and related areas. Most earlier work focuses either on special types of graphs or on specific epidemiological/cascade models. We are the first to show the G2-threshold (twice generalized) theorem, which nicely de-couples the effect of the topology and the virus model. Our result unifies and includes as special case older results and shows that the threshold depends on the first eigenvalue of the connectivity matrix, (a) for any graph and (b) for all propagation models in standard literature (more than 25, including H.I.V.) [20], [12]. Our discovery has broad implications for the vulnerability of real, complex networks, and numerous applications, including viral marketing, blog dynamics, influence propagation, easy answers to 'what-if' questions, and simplified design and evaluation of immunization policies. We also demonstrate our result using extensive simulations on one of the biggest available social contact graphs containing more than 31 million interactions among more than 1 million people representing the city of Portland, Oregon, USA.
[epidemic threshold, Terminology, graph theory, network theory (graphs), complex networks, eigenvalues and eigenfunctions, social contact graph, complex network vulnerability, viral marketing, arbitrary cascade model, Eigenvalues and eigenfunctions, Mathematical model, eigenvalue, Immune system, G2-threshold theorem, Computational modeling, topology, computer viruses, Vectors, Topology, virus propagation, computer network security, twice generalized theorem, virus propagation model, blog dynamics, influence propagation, viral contagion, cascade models, social networking (online), connectivity matrix, arbitrary network]
Time Series Epenthesis: Clustering Time Series Streams Requires Ignoring Some Data
2011 IEEE 11th International Conference on Data Mining
None
2011
Given the pervasiveness of time series data in all human endeavors, and the ubiquity of clustering as a data mining application, it is somewhat surprising that the problem of time series clustering from a single stream remains largely unsolved. Most work on time series clustering considers the clustering of individual time series, e.g., gene expression profiles, individual heartbeats or individual gait cycles. The few attempts at clustering time series streams have been shown to be objectively incorrect in some cases, and in other cases shown to work only on the most contrived datasets by carefully adjusting a large set of parameters. In this work, we make two fundamental contributions. First, we show that the problem definition for time series clustering from streams currently used is inherently flawed, and a new definition is necessary. Second, we show that the Minimum Description Length (MDL) framework offers an efficient, effective and essentially parameter-free method for time series clustering. We show that our method produces objectively correct results on a wide variety of datasets from medicine, zoology and industrial process analyses.
[Time series analysis, Handicapped aids, data mining, minimum description length framework, time series, Encoding, Entropy, medicine, parameter free method, Data mining, industrial process analyses, time series epenthesis, pattern clustering, MDL, Clustering algorithms, Euclidean distance, gene expression profiles, human endeavors, clustering, time series streams clustering, data mining application, zoology]
Mining Historical Documents for Near-Duplicate Figures
2011 IEEE 11th International Conference on Data Mining
None
2011
The increasing interest in archiving all of humankind's cultural artifacts has resulted in the digitization of millions of books, and soon a significant fraction of the world's books will be online. Most of the data in historical manuscripts is text, but there is also a significant fraction devoted to images. This fact has driven much of the recent increase in interest in query-by-content systems for images. While querying/indexing systems can undoubtedly be useful, we believe that the historical manuscript domain is finally ripe for true unsupervised discovery of patterns and regularities. To this end, we introduce an efficient and scalable system which can detect approximately repeated occurrences of shape patterns both within and between historical texts. We show that this ability to find repeated shapes allows automatic annotation of manuscripts, and allows users to trace the evolution of ideas. We demonstrate our ideas on datasets of scientific and cultural manuscripts dating back to the fourteenth century.
[document handling, text analysis, Shape, Scalability, shape patterns, Force, indexing systems, data mining, historical texts, querying systems, history, mining historical documents, Data mining, nearduplicate figures, duplication detection, Pathology, cultural artifacts, repeated patterns, Approximation algorithms, cultural manuscripts, scientific manuscripts, Bioinformatics, historical manuscripts]
Analysis of Textual Variation by Latent Tree Structures
2011 IEEE 11th International Conference on Data Mining
None
2011
We introduce Semstem, a new method for the reconstruction of so called stemmatic trees, i.e., trees encoding the copying relationships among a set of textual variants. Our method is based on a structural expectation-maximization (structural EM) algorithm. It is the first computer-based method able to estimate general latent tree structures, unlike earlier methods that are usually restricted to bifurcating trees where all the extant texts are placed in the leaf nodes. We present experiments on two well known benchmark data sets, showing that the new method outperforms current state-of-the-art both in terms of a numerical score as well as interpretability.
[text analysis, Biological system modeling, stemmatology, latent trees, Phylogeny, numerical score, textual criticism, Semstem, Analytical models, computer-based method, general latent tree structure estimation, stemmatic trees, structural expectation-maximization algorithm, Vegetation, graphical models, expectation-maximisation algorithm, Inference algorithms, textual variation analysis, tree data structures, Bioinformatics, EM algorithm]
Healing Sample Selection Bias by Source Classifier Selection
2011 IEEE 11th International Conference on Data Mining
None
2011
Domain Adaptation (DA) methods are usually carried out by means of simply reducing the marginal distribution differences between the source and target domains, and subsequently using the resultant trained classifier, namely source classifier, for use in the target domain. However, in many cases, the true predictive distributions of the source and target domains can be vastly different especially when their class distributions are skewed, causing the issues of sample selection bias in DA. Hence, DA methods which leverage the source labeled data may suffer from poor generalization in the target domain, resulting in negative transfer. In addition, we observed that many DA methods use either a source classifier or a linear combination of source classifiers with a fixed weighting for predicting the target unlabeled data. Essentially, the labels of the target unlabeled data are spanned by the prediction of these source classifiers. Motivated by these observations, in this paper, we propose to construct many source classifiers of diverse biases and learn the weight for each source classifier by directly minimizing the structural risk defined on the target unlabeled data so as to heal the possible sample selection bias. Since the weights are learned by maximizing the margin of separation between opposite classes on the target unlabeled data, the proposed method is established here as Maximal Margin Target Label Learning (MMTLL), which is in a form of Multiple Kernel Learning problem with many label kernels. Extensive experimental studies of MMTLL against several state-of-the-art methods on the Sentiment and Newsgroups datasets with various imbalanced class settings showed that MMTLL exhibited robust accuracies on all the settings considered and was resilient to negative transfer, in contrast to other counterpart methods which suffered significantly in prediction accuracy.
[Sample Selection Bias, newsgroups datasets, marginal distribution differences, sentiment datasets, Domain Adaptation, Complexity theory, sample selection bias, Multiple Kernel Learning, source classifier selection, trained classifier, Accuracy, domain adaptation methods, negative transfer, target unlabeled data, learning (artificial intelligence), Kernel, Joints, pattern classification, structural risk minimization, Vectors, Maximum Margin Separation, Support vector machines, maximal margin target label learning, predictive distributions, Classifier Selection, Negative Transfer, Machine learning]
An In-depth Study of Stochastic Kronecker Graphs
2011 IEEE 11th International Conference on Data Mining
None
2011
Graph analysis is playing an increasingly important role in science and industry. Due to numerous limitations in sharing real-world graphs, models for generating massive graphs are critical for developing better algorithms. In this paper, we analyze the stochastic Kronecker graph model (SKG), which is the foundation of the Graph500 supercomputer benchmark due to its many favorable properties and easy parallelization. Our goal is to provide a deeper understanding of the parameters and properties of this model so that its functionality as a benchmark is increased. We develop a rigorous mathematical analysis that shows this model cannot generate a power-law distribution or even a lognormal distribution. However, we formalize an enhanced version of the SKG model that uses random noise for smoothing. We prove both in theory and in practice that this enhancement leads to a lognormal distribution. Additionally, we provide a precise analysis of isolated vertices, showing that graphs that are produced by SKG might be quite different than intended. For example, between 50% and 75% of the vertices in the Graph500 benchmarks will be isolated. Finally, we show that this model tends to produce extremely small core numbers (compared to most social networks and other real graphs) for common parameter choices.
[Algorithm design and analysis, Graph Mining, Noise, graph theory, Lognormal Degree Distributions, social networks, graph analysis, mathematical analysis, Approximation methods, Random Graph Generation, parallel machines, Oscillators, SKG, Analytical models, Social Networks, stochastic Kronecker graphs, power law distribution, lognormal distribution, Stochastic Kronecker Graphs, Benchmark testing, Graph500 supercomputer benchmark, Mathematical model, stochastic processes, Graph500]
Learning Spectral Embedding for Semi-supervised Clustering
2011 IEEE 11th International Conference on Data Mining
None
2011
In recent years, semi-supervised clustering (SSC) has aroused considerable interests from the machine learning and data mining communities. In this paper, we propose a novel semi-supervised clustering approach with enhanced spectral embedding (ESE) which not only considers structure information contained in data sets but also makes use of prior side information such as pair wise constraints. Specially, we first construct a symmetry-favored k-NN graph which is highly robust to noisy objects and can reflect the underlying manifold structure of data. Then we learn the enhanced spectral embedding towards an ideal representation as consistent with the pair wise constraints as possible. Finally, through taking advantage of Laplacian regularization, we formulate learning spectral representation as semi definite-quadratic-linear programs (SQLPs) under the squared loss function or small semi definitive programs (SDPs) under the hinge loss function, which both can be efficiently solved. Experimental results on a variety of synthetic and real-world data sets show that our approach outperforms the state-of-the-art SSC algorithms on both vector-based and graph-based clustering.
[Algorithm design and analysis, Laplace equations, graph theory, spectral embedding, data mining, Fasteners, visual databases, pairwise constraint, linear programming, Complexity theory, machine learning, Optimization, enhanced spectral embedding, semisupervised clustering (SSC), pattern clustering, Clustering algorithms, vector-based clustering, graph-based clustering, semisupervised clustering, Laplacian regularization, semidefinite-quadratic-linear program, learning (artificial intelligence), Kernel, symmetry-favored k-NN graph]
Detection of Arbitrarily Oriented Synchronized Clusters in High-Dimensional Data
2011 IEEE 11th International Conference on Data Mining
None
2011
How to address the challenges of the "curse of dimensionality" in clustering? Clustering is a powerful data mining technique for structuring and organizing vast amounts of data. However, the high-dimensional data space is usually very sparse and meaningful clusters can only be found in lower dimensional subspaces. In many applications the subspaces hosting the clusters provide valuable information for interpreting the major patterns in the data. Detection of subspace clusters is challenging since usually many of the attributes are noisy, some attributes may exhibit correlations among each other and only few of the attributes truly contribute to the cluster structure. In this paper, we propose ORSC (Arbitrarily ORiented Synchronized Clusters), a novel effective and efficient method to subspace clustering inspired by synchronization. Synchronization is a basic phenomenon prevalent in nature, capable of controlling even highly complex processes such as opinion formation in a group. Control of complex processes is achieved by simple operations based on interactions between objects. Relying on the interaction model for synchronization, our approach ORSC (1) naturally detects correlation clusters in arbitrarily oriented subspaces, including (2) arbitrarily shaped non-linear correlation clusters. Our approach is (3) robust against noise points and outliers. In contrast to previous methods, ORSC is (4) easy to parameterize, since there is no need to specify the subspace dimensionality and all interesting subspace clusters can be detected. Finally, (5) ORSC outperforms most comparison methods in terms of runtime efficiency and is highly scalable to large and high-dimensional data sets.
[Correlation, data mining, high-dimensional data, arbitrarily oriented subspaces, Covariance matrix, outliers, subspace dimensionality, Synchronization, interaction model, Oscillators, arbitrarily oriented synchronized cluster detection, complex processes, noise points, pattern clustering, high dimensional data, subspace clustering, Clustering algorithms, ORSC, correlation clusters, synchronization, Eigenvalues and eigenfunctions, data mining technique, Principal component analysis]
A Generalized Fast Subset Sums Framework for Bayesian Event Detection
2011 IEEE 11th International Conference on Data Mining
None
2011
We present Generalized Fast Subset Sums (GFSS), a new Bayesian framework for scalable and accurate detection of irregularly shaped spatial clusters using multiple data streams. GFSS extends the previously proposed Multivariate Bayesian Scan Statistic (MBSS) and Fast Subset Sums (FSS) approaches for detection of emerging events. The detection power of MBSS is primarily limited by computational considerations, which limit it to searching over circular spatial regions. GFSS enables more accurate and timely detection by defining a hierarchical prior over all subsets of the N locations, first selecting a local neighborhood consisting of a center location and its neighbors, and introducing a sparsity parameter p to describe how likely each location in the neighborhood is to be affected. This approach allows us to consider all possible subsets of locations (including irregularly-shaped regions) but also puts higher weight on more compact regions. We demonstrate that MBSS and FSS are both special cases of this general framework (assuming p = 1 and p = 0.5 respectively), but substantially higher detection power can be achieved by choosing an appropriate value of p. Thus we show that the distribution of the sparsity parameter p can be accurately learned from a small number of labeled events. Our evaluation results (on synthetic disease outbreaks injected into real-world hospital data) show that the GFSS method with learned sparsity parameter has higher detection power and spatial accuracy than MBSS and FSS, particularly when the affected region is irregular or elongated. We also show that the learned models can be used for event characterization, accurately distinguishing between two otherwise identical event types based on the sparsity of the affected spatial region.
[Frequency selective surfaces, Event detection, data streams, fast subset sum, set theory, multivariate Bayesian scan statistic, Training, irregularly shaped spatial clusters, MBSS, Accuracy, Bayesian event detection, sparsity parameter distribution, learning (artificial intelligence), GFSS, GFSS method, synthetic disease outbreaks, scan statistics, FSS, biosurveillance, Diseases, Bayesian methods, real world hospital data, event detection, generalized fast subset sum, learned sparsity parameter, Bayes methods, data handling]
Learning to Rank for Query-Focused Multi-document Summarization
2011 IEEE 11th International Conference on Data Mining
None
2011
In this paper, we explore how to use ranking SVM to train the feature weights for query-focused multi-document summarization. To apply a supervised learning method to sentence extraction in multi-document summarization, we need to derive the sentence labels for training corpus from the existing human labeling data in form of. However, this process is not trivial, because the human summaries are abstractive, and do not necessarily well match the sentences in the documents. In this paper, we try to address the above problem from the following two aspects. First, we make use of sentence-to-sentence relationships to better estimate the probability of a sentence in the document set to be a summary sentence. Second, to make the derived training data less sensitive, we adopt a cost sensitive loss in the ranking SVM's objective function. The experimental results demonstrate the effectiveness of our proposed method.
[document handling, support vector machines, Redundancy, Humans, probability, query-based multi-document summarization, sentence-to-sentence relationships, Support vector machines, Training, query processing, rank learning, support vector machine, Supervised learning, Training data, feature weights, query-focused multidocument summarization, Feature extraction, SVM ranking, learning (artificial intelligence), sentence probability estimation, learning to rank]
Simple Multiple Noisy Label Utilization Strategies
2011 IEEE 11th International Conference on Data Mining
None
2011
With the outsourcing of small tasks becoming easier, it is possible to obtain non-expert/imperfect labels at low cost. With low-cost imperfect labeling, it is straightforward to collect multiple labels for the same data items. This paper addresses the strategies of utilizing these multiple labels for improving the performance of supervised learning, based on two basic ideas: majority voting and pair wise solutions. We show several interesting results based on our experiments. The soft majority voting strategies can reduce the bias and roughness, and improve the performance of the directed hard majority voting strategy. Pair wise strategies can completely avoid the bias by having both sides (potential correct and incorrect/noisy information) considered (for binary classification). They have very good performance whenever there are a few or many labels available. However, it could also keep the noise. The improved variation that reduces the impact of the noisy information is recommended. All five strategies investigated are labeling quality agnostic strategies, and can be applied to real world applications directly. The experimental results show some of them perform better than or at least very close to the gnostic strategies.
[Uncertainty, crowdsourcing, multiple noisy labels, Estimation, multiple noisy label utilization strategies, supervised learning, soft majority voting strategies, Noise measurement, classification, Training, Accuracy, hard majority voting strategy, outsourcing, Training data, pairwise solutions, data handling, Labeling, learning (artificial intelligence), quality agnostic strategies]
Partitionable Kernels for Mapping Kernels
2011 IEEE 11th International Conference on Data Mining
None
2011
Many of tree kernels in the literature are designed tanking advantage of the mapping kernel framework. The most important advantage of using this framework is that we have a strong theorem to examine positive definiteness of the resulting tree kernels. In the mapping kernel framework, each data object is viewed as a collection of components, and a mapping kernel for a pair of data objects is determined as a sum of kernel values of component pairs over a certain range determined according to the purpose of use of the resulting mapping kernel. For those tree kernels known to belong to the mapping kernel category, the string kernel of the product type is commonly used to compute the kernel values of component pairs. This is because it is known that use of the product-type string kernel together with the mapping kernel framework allows us to have recursive formulas to calculate the resulting tree kernels efficiently. We significantly generalizes this result. In fact, we show that we can use partition able kernels, a new class of string kernels instead of the product-type string kernel to enjoy the same advantage, that is, efficient computation based on recursive formulas. The class of partition able kernels is abundant, and contains the product-type string kernels just as an instance. Also, this result, not limited to tree kernels, can be applied to general mapping kernels after we formalize the decomposition properties of trees as the new notion of pretty decomposability.
[Mapping Kernels, support vector machines, Heuristic algorithms, trees (mathematics), partitionable kernels, Educational institutions, Vectors, SVM, positive definiteness, Support vector machines, Convolution, data object, Vegetation, tree kernels, Kernel]
Mining Dominant Patterns in the Sky
2011 IEEE 11th International Conference on Data Mining
None
2011
Pattern discovery is at the core of numerous data mining tasks. Although many methods focus on efficiency in pattern mining, they still suffer from the problem of choosing a threshold that influences the final extraction result. The goal of our study is to make the results of pattern mining useful from a user-preference point of view. To this end, we integrate into the pattern discovery process the idea of skyline queries in order to mine skyline patterns in a threshold-free manner. Because the skyline patterns satisfy a formal property of dominations, they not only have a global interest but also have semantics that are easily understood by the user. In this work, we first establish theoretical relationships between pattern condensed representations and skyline pattern mining. We also show that it is possible to compute automatically a subset of measures involved in the user query which allows the patterns to be condensed and thus facilitates the computation of the skyline patterns. This forms the basis for a novel approach to mining skyline patterns. We illustrate the efficiency of our approach over several data sets including a use case from chemo informatics and show that small sets of dominant patterns are produced under various measures.
[dominant pattern mining, Area measurement, data mining, user query, Length measurement, skyline patterns, Frequency measurement, Pattern mining, Data mining, query processing, chemo informatics, Databases, Skyline analysis, Semantics, skyline query, user-preferences, pattern classification, data mining tasks, Redundancy, formal property, chemistry computing, pattern discovery process, skyline pattern mining, user-preference, pattern condensed representations]
Ranking Web-Based Partial Orders by Significance Using a Markov Reference Model
2011 IEEE 11th International Conference on Data Mining
None
2011
Mining web traffic data has been addressed in literature mostly using sequential pattern mining techniques. Recently, a more powerful pattern called partial order was introduced, with the hope of providing a more compact result set. A further approach towards this goal, valid for both sequential patterns and partial orders, consists in building a statistical significance test for frequent patterns. Our method is based on probabilistic generative models and provides a direct way to rank the extracted patterns. It leaves open the number of patterns of interest, which depends on the application, but provides an alternative criterion to frequency of occurrence: statistical significance. In this paper, we focus on the construction of an algorithm which calculates the probability of partial orders under a first-order Markov reference model, and we show how to use those probabilities to assess the statistical significance of a set of mined partial orders.
[data mining, pattern, Web based partial orders ranking, Data mining, Databases, Absorption, web, Transient analysis, statistical testing, partial order, poset, statistical significance test, Computational modeling, test, Web traffic data mining, Markov, probability, Probability, sequential patterns, sequential pattern mining techniques, significance, statisstatistical, Markov processes, Markov reference model, ranking, Internet]
Interesting Multi-relational Patterns
2011 IEEE 11th International Conference on Data Mining
None
2011
Mining patterns from multi-relational data is a problem attracting increasing interest within the data mining community. Traditional data mining approaches are typically developed for highly simplified types of data, such as an attribute-value table or a binary database, such that those methods are not directly applicable to multi-relational data. Nevertheless, multi-relational data is a more truthful and therefore often also a more powerful representation of reality. Mining patterns of a suitably expressive syntax directly from this representation, is thus a research problem of great importance. In this paper we introduce a novel approach to mining patterns in multi-relational data. We propose a new syntax for multi-relational patterns as complete connected sub graphs in a representation of the database as a k-partite graph. We show how this pattern syntax is generally applicable to multirelational data, while it reduces to well-known tiles [7] when the data is a simple binary or attribute-value table. We propose RMiner, an efficient algorithm to mine such patterns, and we introduce a method for quantifying their interestingness when contrasted with prior information of the data miner. Finally, we illustrate the usefulness of our approach by discussing results on real-world and synthetic databases.
[pattern classification, K-partite graph, multirelational data mining patterns, graph theory, data mining, Entropy, Data mining, binary database, connected subgraphs, Itemsets, RMiner, Syntactics, Motion pictures, Bipartite graph, attribute-value table]
On Generating All Optimal Monotone Classifications
2011 IEEE 11th International Conference on Data Mining
None
2011
In many applications of data mining one knows beforehand that the response variable should be monotone (either increasing or decreasing) in the attributes. In ordinal classification, changing the class labels of a data set (relabeling) so that the data becomes monotone, is useful for at least two reasons. Firstly, models trained on relabeled data tend to have better predictive performance than models trained on the original data. Secondly, relabeling is an important building block for the construction of monotone classifiers. However, optimal monotone relabelings are rarely unique, and so far an efficient algorithm to generate them all has been lacking. The main result of this paper is an efficient algorithm to produce the structure of all optimal monotone relabelings. We also show that counting the solutions is #P-complete and give algorithms for efficiently enumerating all solutions, as well as sampling uniformly from the set of solutions. Experiments show that relabeling non-monotone data can improve the predictive performance of models trained on that data.
[pattern classification, isotonic regression, data mining, data set, monotone classification, Predictive models, Vectors, Partitioning algorithms, optimal monotone classifications, Data mining, ordinal classification, optimal monotone relabelings, Bismuth, Prediction algorithms, Data models]
Recursive Multi-step Time Series Forecasting by Perturbing Data
2011 IEEE 11th International Conference on Data Mining
None
2011
The Recursive strategy is the oldest and most intuitive strategy to forecast a time series multiple steps ahead. At the same time, it is well-known that this strategy suffers from the accumulation of errors as long as the forecasting horizon increases. We propose a variant of the Recursive strategy, called RECNOISY, which perturbs the initial dataset at each step of the forecasting process in order to i) handle more properly the estimated values at each forecasting step and ii) decrease the accumulation of errors induced by the Recursive strategy. In addition to the RECNOISY strategy, we propose another strategy, called HYBRID, which for each horizon selects the most accurate approach among the REC and the RECNOISY strategies according to the estimated accuracy. In order to assess the effectiveness of the proposed strategies, we carry out an experimental session based on the 111 times series of the NN5 forecasting competition. Accuracy results are presented together with a paired comparison over the horizons and the time series. The preliminary results show that our proposed approaches are promising in terms of forecasting performance.
[Time series analysis, Time series, Predictive models, recursive strategy, time series, data perturbation, NN5 forecasting competition, Data mining, Forecasting, RECNOISY, Machine Learning, Recursive forecasting, Multi-step forecasting, Training, Accuracy, forecasting theory, Machine learning, recursive estimation, recursive multistep time series forecasting, Forecasting strategies, data handling, HYBRID]
Finding Robust Itemsets under Subsampling
2011 IEEE 11th International Conference on Data Mining
None
2011
Mining frequent patterns is plagued by the problem of pattern explosion making pattern reduction techniques a key challenge in pattern mining. In this paper we propose a novel theoretical framework for pattern reduction. We do this by measuring the robustness of a property of an item set such as closed ness or non-derivability. The robustness of a property is the probability that this property holds on random subsets of the original data. We study four properties: closed, free, non-derivable and totally shattered item sets, demonstrating how we can compute the robustness analytically without actually sampling the data. Our concept of robustness has many advantages: Unlike statistical approaches for reducing patterns, we do not assume a null hypothesis or any noise model and the patterns reported are simply a subset of all patterns with this property as opposed to approximate patterns for which the property does not really hold. If the underlying property is monotonic, then the measure is also monotonic, allowing us to efficiently mine robust item sets. We further derive a parameter-free technique for ranking item sets that can be used for top-k approaches. Our experiments demonstrate that we can successfully use the robustness measure to reduce the number of patterns and that ranking yields interesting itemsets.
[null hypothesis, TV, pattern explosion, top-k approaches, data mining, closed itemsets, subsampling, Vectors, Data mining, non-derivable itemsets, pattern reduction, Itemsets, robust itemsets, pattern reduction techniques, free itemsets, Robustness, Polynomials, noise model, Random variables, totally shattered itemsets, parameter free technique, frequent pattern mining]
Density Estimation Based on Mass
2011 IEEE 11th International Conference on Data Mining
None
2011
Density estimation is the ubiquitous base modelling mechanism employed for many tasks such as clustering, classification, anomaly detection and information retrieval. Commonly used density estimation methods such as kernel density estimator and k-nearest neighbour density estimator have high time and space complexities which render them inapplicable in problems with large data size and even a moderate number of dimensions. This weakness sets the fundamental limit in existing algorithms for all these tasks. We propose the first density estimation method which stretches this fundamental limit to an extent that dealing with millions of data can now be done easily and quickly. We analyze the error of the new estimation (from the true density) using a bias-variance analysis. We then perform an empirical evaluation of the proposed method by replacing existing density estimators with the new one in two current density-based algorithms, namely, DBSCAN and LOF. The results show that the new density estimation method significantly improves the runtime of DBSCAN and LOF, while maintaining or improving their task-specific performances in clustering and anomaly detection, respectively. The new method empowers these algorithms, currently limited to small data size only, to process very large databases - setting a new benchmark for what density-based algorithms can achieve.
[task specific performance, LOF, Noise, error analysis, Estimation, data size, density estimation method, DBSCAN, ubiquitous base modelling mechanism, anomaly detection, density based algorithm, Complexity theory, Approximation methods, ubiquitous computing, bias variance analysis, empirical evaluation, Databases, pattern clustering, Clustering algorithms, density-based algorithms, Kernel, density estimation]
Diverse Dimension Decomposition of an Itemset Space
2011 IEEE 11th International Conference on Data Mining
None
2011
We introduce the problem of diverse dimension decomposition in transactional databases. A dimension is a set of mutually-exclusive item sets, and our problem is to find a decomposition of the item set space into dimensions, which are orthogonal to each other, and that provide high coverage of the input database. The mining framework we propose effectively represents a dimensionality-reducing transformation from the space of all items to the space of orthogonal dimensions. Our approach relies on information-theoretic concepts, and we are able to formulate the dimension-finding problem with a single objective function that simultaneously captures constraints on coverage, exclusivity and orthogonality. We describe an efficient greedy method for finding diverse dimensions from transactional databases. The experimental evaluation of the proposed approach using two real datasets, flickr and delicious, demonstrates the effectiveness of our solution. Although we are motivated by the applications in the collaborative tagging domain, we believe that the mining task we introduce in this paper is general enough to be useful in other application domains.
[transaction processing, information theoretic concepts, Art, transactional databases, Entropy, collaborative tagging, Data mining, database management systems, itemset space, dimensionality reduction, Itemsets, itemset mining, orthogonal dimensions, diverse dimension decomposition, Joints, Mutual information]
Conditional Anomaly Detection with Soft Harmonic Functions
2011 IEEE 11th International Conference on Data Mining
None
2011
In this paper, we consider the problem of conditional anomaly detection that aims to identify data instances with an unusual response or a class label. We develop a new non-parametric approach for conditional anomaly detection based on the soft harmonic solution, with which we estimate the confidence of the label to detect anomalous mislabeling. We further regularize the solution to avoid the detection of isolated examples and examples on the boundary of the distribution support. We demonstrate the efficacy of the proposed method on several synthetic and UCI ML datasets in detecting unusual labels when compared to several baseline approaches. We also evaluate the performance of our method on a real-world electronic health record dataset where we seek to identify unusual patient-management decisions.
[Solid modeling, random walks, Design automation, Laplace equations, graph methods, graph theory, soft harmonic solution, distribution support, Harmonic analysis, Educational institutions, conditional anomaly detection, outlier and anomaly detection, Electronic mail, backbone graph, Manifolds, data instances, harmonic solution, soft harmonic functions, health care informatics, UCI ML datasets, patient management decisions, data handling]
Random Forest Based Feature Induction
2011 IEEE 11th International Conference on Data Mining
None
2011
We propose a simple yet effective strategy to induce a task dependent feature representation using ensembles of random decision trees. The new feature mapping is efficient in space and time, and provides a metric transformation that is non parametric and not implicit in nature (i.e. not expressed via a kernel matrix), nor limited to the transductive setup. The main advantage of the proposed mapping lies in its flexibility to adapt to several types of learning tasks ranging from regression to multi-label classification, and to deal in a natural way with missing values. Finally, we provide an extensive empirical study of the properties of the learned feature representation over real and artificial datasets.
[pattern classification, task dependent feature representation, feature induction, feature representation, data mining, random processes, regression analysis, random forests, Vectors, Encoding, Complexity theory, matrix algebra, Training, feature extraction, metric transformation, random decision trees, Vegetation, decision trees, multilabel classification, feature mapping, Decision trees, Kernel, random forest based feature induction]
Class Imbalance, Redux
2011 IEEE 11th International Conference on Data Mining
None
2011
Class imbalance (i.e., scenarios in which classes are unequally represented in the training data) occurs in many real-world learning tasks. Yet despite its practical importance, there is no established theory of class imbalance, and existing methods for handling it are therefore not well motivated. In this work, we approach the problem of imbalance from a probabilistic perspective, and from this vantage identify dataset characteristics (such as dimensionality, sparsity, etc.) that exacerbate the problem. Motivated by this theory, we advocate the approach of bagging an ensemble of classifiers induced over balanced bootstrap training samples, arguing that this strategy will often succeed where others fail. Thus in addition to providing a theoretical understanding of class imbalance, corroborated by our experiments on both simulated and real datasets, we provide practical guidance for the data mining practitioner working with imbalanced data.
[Measurement, pattern classification, Particle separators, dataset characteristics, classifier ensemble, balanced bootstrap training sample, data mining, probability, Predictive models, Equations, Training, class imbalance, training data, Classification, class imbalance handling, Mathematical model, statistical analysis, Bagging]
Combining Feature Context and Spatial Context for Image Pattern Discovery
2011 IEEE 11th International Conference on Data Mining
None
2011
Once an image is decomposed into a number of visual primitives, e.g., local interest points or salient image regions, it is of great interests to discover meaningful visual patterns from them. Conventional clustering (e.g., k-means) of visual primitives, however, usually ignores the spatial dependency among them, thus cannot discover the high-level visual patterns of complex spatial structure. To overcome this problem, we propose to consider both spatial and feature contexts among visual primitives for pattern discovery. By discovering both spatial co-occurrence patterns among visual primitives and feature co-occurrence patterns among different types of features, our method can better handle the ambiguities of visual primitives, by leveraging these co-occurrences. We formulate the problem as a regularized k-means clustering, and propose an iterative bottom-up/top-down self-learning procedure to gradually refine the result until it converges. The experiments of image text on discovery and image region clustering convince that combining spatial and feature contexts can significantly improve the pattern discovery results.
[Context, Visualization, image region clustering, TV, Uncertainty, feature context, Shape, spatial context, iterative bottom-up self-learning procedure, local interest points, regularized k-means clustering, image pattern discovery, spatial co-occurrence patterns, pattern clustering, salient image regions, feature extraction, Clustering algorithms, visual primitives, spatial dependency, clustering, learning (artificial intelligence), Spatial resolution, feature co-occurrence patterns, iterative top-down self-learning procedure]
Nonnegative Matrix Tri-factorization Based High-Order Co-clustering and Its Fast Implementation
2011 IEEE 11th International Conference on Data Mining
None
2011
The fast growth of Internet and modern technologies has brought data involving objects of multiple types that are related to each other, called as Multi-Type Relational data. Traditional clustering methods for single-type data rarely work well on them, which calls for new clustering techniques, called as high-order co-clustering (HOCC), to deal with the multiple types of data at the same time. A major challenge in developing HOCC methods is how to effectively make use of all available information contained in a multi-type relational data set, including both inter-type and intra-type relationships. Meanwhile, because many real world data sets are often of large sizes, clustering methods with computationally efficient solution algorithms are of great practical interest. In this paper, we first present a general HOCC framework, named as Orthogonal Nonnegative Matrix Tri-factorization (O-NMTF), for simultaneous clustering of multi-type relational data. The proposed O-NMTF approach employs Nonnegative Matrix Tri-Factorization (NMTF) to simultaneously cluster different types of data using the inter-type relationships, and incorporate intra-type information through manifold regularization, where, different from existing works, we emphasize the importance of the orthogonal ties of the factor matrices of NMTF. Based on O-NMTF, we further develop a novel Fast Nonnegative Matrix Tri-Factorization (F-NMTF) approach to deal with large-scale data. Instead of constraining the factor matrices of NMTF to be nonnegative as in existing methods, F-NMTF constrains them to be cluster indicator matrices, a special type of nonnegative matrices. As a result, the optimization problem of the proposed method can be decoupled, which results in sub problems of much smaller sizes requiring much less matrix multiplications, such that our new algorithm scales well to real world data of large sizes. Extensive experimental evaluations have demonstrated the effectiveness of our new approaches.
[Cluster Indicator Matrix, single type data, Symmetric matrices, matrix multiplications, Linear matrix inequalities, matrix decomposition, fast nonnegative matrix trifactorization approach, Multi-Type Relational Data, Optimization, Convergence, Manifolds, matrix multiplication, intertype relationships, manifold regularization, orthogonal nonnegative matrix trifactorization, pattern clustering, intratype relationships, Clustering algorithms, Web pages, Nonnegative Matrix Tri-Factorization, High-Order Co-Clustering, Internet, multitype relational data, high order coclustering]
Detecting Community Kernels in Large Social Networks
2011 IEEE 11th International Conference on Data Mining
None
2011
In many social networks, there exist two types of users that exhibit different influence and different behavior. For instance, statistics have shown that less than 1% of the Twitter users (e.g. entertainers, politicians, writers) produce 50% of its content, while the others (e.g. fans, followers, readers) have much less influence and completely different social behavior. In this paper, we define and explore a novel problem called community kernel detection in order to uncover the hidden community structure in large social networks. We discover that influential users pay closer attention to those who are more similar to them, which leads to a natural partition into different community kernels. We propose Greedy and We BA, two efficient algorithms for finding community kernels in large social networks. Greedy is based on maximum cardinality search, while We BA formalizes the problem in an optimization framework. We conduct experiments on three large social networks: Twitter, Wikipedia, and Coauthor, which show that We BA achieves an average 15%-50% performance improvement over the other state-of-the-art algorithms, and We BA is on average 6-2,000 times faster in detecting community kernels.
[Electronic publishing, greedy algorithms, Communities, social networks, social behavior, Encyclopedias, Twitter, Wikipedia, hidden community structure, auxiliary communities, Greedy algorithm, Coauthor, social networking (online), Internet, community kernel detection, Twitter users, community kernels, Kernel]
ADANA: Active Name Disambiguation
2011 IEEE 11th International Conference on Data Mining
None
2011
Name ambiguity has long been viewed as a challenging problem in many applications, such as scientific literature management, people search, and social network analysis. When we search a person name in these systems, many documents (e.g., papers, web pages) containing that person's name may be returned. It is hard to determine which documents are about the person we care about. Although much research has been conducted, the problem remains largely unsolved, especially with the rapid growth of the people information available on the Web. In this paper, we try to study this problem from a new perspective and propose an ADANA method for disambiguating person names via active user interactions. In ADANA, we first introduce a pairwise factor graph (PFG) model for person name disambiguation. The model is flexible and can be easily extended by incorporating various features. Based on the PFG model, we propose an active name disambiguation algorithm, aiming to improve the disambiguation performance by maximizing the utility of the user's correction. Experimental results on three different genres of data sets show that with only a few user corrections, the error rate of name disambiguation can be reduced to 3.1%. A real system has been developed based on the proposed method and is available online.
[utility maximizing, Social Network Analysis, Correlation, ADANA method, PFG model, graph theory, name ambiguity, information retrieval, pairwise factor graph model, Educational institutions, active user interactions, user interfaces, Active Learning, Accuracy, active name disambiguation algorithm, Web pages, Clustering algorithms, Name Disambiguation, Approximation algorithms, Inference algorithms, Internet, active person name disambiguation, user correction, Digital Library]
Document Clustering via Matrix Representation
2011 IEEE 11th International Conference on Data Mining
None
2011
Vector Space Model (VSM) is widely used to represent documents and web pages. It is simple and easy to deal computationally, but it also oversimplifies a document into a vector, susceptible to noise, and cannot explicitly represent underlying topics of a document. A matrix representation of document is proposed in this paper: rows represent distinct terms and columns represent cohesive segments. The matrix model views a document as a set of segments, and each segment is a probability distribution over a limited number of latent topics which can be mapped to clustering structures. The latent topic extraction based on the matrix representation of documents is formulated as a constraint optimization problem in which each matrix (i.e., a document) A<sub>i</sub> is factorized into a common base determined by non-negative matrices L and RT, and a non-negative weight matrix M<sub>i</sub> such that the sum of reconstruction error on all documents is minimized. Empirical evaluation demonstrates that it is feasible to use the matrix model for document clustering: (1) compared with vector representation, using matrix representation improves clustering quality consistently, and the proposed approach achieves a relative accuracy improvement up to 66% on the studied datasets, and (2) the proposed method outperforms baseline methods such as k-means and NMF, and complements the state-of-the-art methods like LDA and PLSI. Furthermore, the proposed matrix model allows more refined information retrieval at a segment level instead of at a document level, which enables the return of more relevant documents in information retrieval tasks.
[document clustering, LDA, constraint optimization problem, Probability distribution, Non-Negative Matrix Approximation, Approximation methods, Data mining, optimisation, Clustering algorithms, Bismuth, probability distribution, k-means, Document Representation, matrix representation, constraint handling, Document Clustering, document handling, probability, vector space model, NMF, information retrieval, matrix model, Vectors, Matrix decomposition, matrix algebra, clustering structures, Matrix Representation, pattern clustering, information retrieval tasks, Web pages, PLSI, Web sites, cohesive segments]
Using Bayesian Network Learning Algorithm to Discover Causal Relations in Multivariate Time Series
2011 IEEE 11th International Conference on Data Mining
None
2011
Many applications naturally involve time series data, and the vector auto regression (VAR) and the structural VAR (SVAR) are dominant tools to investigate relations between variables in time series. In the first part of this work, we show that the SVAR method is incapable of identifying contemporaneous causal relations when data follow Gaussian distributions. In addition, least squares estimators become unreliable when the scales of the problems are large and observations are limited. In the remaining part, we propose an approach to apply Bayesian network learning algorithms to identify SVARs from time series data in order to capture both temporal and contemporaneous causal relations and avoid high-order statistical tests. The difficulty of applying Bayesian network learning algorithms to time series is that the sizes of the networks corresponding to time series tend to be large and high-order statistical tests are required by Bayesian network learning algorithms in this case. To overcome the difficulty, we show that the search space of conditioning sets d-separating two vertices should be subsets of Markov blankets. Based on this fact, we propose an algorithm learning Bayesian networks locally and making the largest order of statistical tests independent of the scales of the problems. Empirical results show that our algorithm outperforms existing methods in terms of both efficiency and accuracy.
[vector autoregression, Bayesian network learning algorithm, data mining, structural VAR, VAR, regression analysis, Gaussian distribution, causal relation discovery, high-order statistical tests, Reactive power, Markov blankets, least squares estimators, search space, Mathematical model, belief networks, learning (artificial intelligence), Gaussian distributions, Time series analysis, multivariate time series, time series, Covariance matrix, Equations, SVAR, Bayesian methods, Markov processes, Bayesian networks, Causality]
Efficient Mining of a Concise and Lossless Representation of High Utility Itemsets
2011 IEEE 11th International Conference on Data Mining
None
2011
Mining high utility item sets from transactional databases is an important data mining task, which refers to the discovery of item sets with high utilities (e.g. high profits). Although several studies have been carried out, current methods may present too many high utility item sets for users, which degrades the performance of the mining task in terms of execution and memory efficiency. To achieve high efficiency for the mining task and provide a concise mining result to users, we propose a novel framework in this paper for mining closed+ high utility item sets, which serves as a compact and loss less representation of high utility item sets. We present an efficient algorithm called CHUD (Closed+ High Utility item set Discovery) for mining closed+ high utility item sets. Further, a method called DAHU (Derive All High Utility item sets) is proposed to recover all high utility item sets from the set of closed+ high utility item sets without accessing the original database. Results of experiments on real and synthetic datasets show that CHUD and DAHU are very efficient with a massive reduction (up to 800 times in our experiments) in the number of high utility item sets. In addition, when all high utility item sets are recovered by DAHU, the approach combining CHUD and DAHU also outperforms the state-of-the-art algorithms in mining high utility item sets.
[Algorithm design and analysis, transaction processing, utility mining, closed+ high utility item set mining, data mining, transactional databases, derive all high utility item sets, Educational institutions, high utility itemsets concise representation mining, CHUD, Data mining, database management systems, lossless and concise representation, Itemsets, frequent itemset, Memory management, high utility itemsets lossless representation mining, DAHU, closed+ high utility item set discovery, closed+ high utility itemset, Arrays, data mining task]
Understanding Propagation Error and Its Effect on Collective Classification
2011 IEEE 11th International Conference on Data Mining
None
2011
Recent empirical evaluation has shown that the performance of collective classification models can vary based on the amount of class label information available for use during inference. In this paper, we further demonstrate that the relative performance of statistical relational models learned with different estimation methods changes as the availability of test set labels increases. We reason about the cause of this phenomenon from an information-theoretic perspective and this points to a previously unidentified consideration in the development of relational learning algorithms. In particular, we characterize the high propagation error of collective inference models that are estimated with maximum pseudolikelihood estimation (MPLE), and show how this affects performance across the spectrum of label availability when compared to MLE, which has low propagation error. Our formal study leads to a quantitative characterization that can be used to predict the confidence of local propagation for MPLE models. We use this to propose a mixture model that can learn the best trade-off between high and low propagation models. Empirical evaluation on synthetic and real-world data show that our proposed method achieves comparable, or superior, results to both MPLE and low propagation models across the full spectrum of label availability.
[inference, information theoretic perspective, mixture model, maximum likelihood estimation, probabilistic relational models, Training, empirical evaluation, label availability, information theory, learning (artificial intelligence), maximum pseudolikelihood estimation, class label information, Availability, Maximum likelihood estimation, pattern classification, collective classification, Computational modeling, statistical relational learning, Probabilistic logic, relational learning algorithms, inference mechanisms, test set labels, Microscopy, propagation error, collective classification models, Data models]
Direct Robust Matrix Factorizatoin for Anomaly Detection
2011 IEEE 11th International Conference on Data Mining
None
2011
Matrix factorization methods are extremely useful in many data mining tasks, yet their performances are often degraded by outliers. In this paper, we propose a novel robust matrix factorization algorithm that is insensitive to outliers. We directly formulate robust factorization as a matrix approximation problem with constraints on the rank of the matrix and the cardinality of the outlier set. Then, unlike existing methods that resort to convex relaxations, we solve this problem directly and efficiently. In addition, structural knowledge about the outliers can be incorporated to find outliers more effectively. We applied this method in anomaly detection tasks on various data sets. Empirical results show that this new algorithm is effective in robust modeling and anomaly detection, and our direct solution achieves superior performance over the state-of-the-art methods based on the L1-norm and the nuclear norm of matrices.
[matrix factorization, robust, Estimation, data mining, convex programming, Vectors, anomaly detection, matrix approximation, direct robust matrix factorizatoin, Approximation methods, Matrix decomposition, matrix algebra, Measurement uncertainty, Approximation algorithms, Robustness, convex relaxation, structural knowledge]
A New Markov Model for Clustering Categorical Sequences
2011 IEEE 11th International Conference on Data Mining
None
2011
Clustering categorical sequences remains an open and challenging task due to the lack of an inherently meaningful measure of pair wise similarity between sequences. Model initialization is an unsolved problem in model-based clustering algorithms for categorical sequences. In this paper, we propose a simple and effective Markov model to approximate the conditional probability distribution (CPD) model, and use it to design a novel two-tier Markov model to represent a sequence cluster. Furthermore, we design a novel divisive hierarchical algorithm for clustering categorical sequences based on the two-tier Markov model. The experimental results on the data sets from three different domains demonstrate the promising performance of our models and clustering algorithm.
[Algorithm design and analysis, pairwise similarity, divisive hierarchical algorithm, Vectors, Markov model, sequences, statistical distributions, model based clustering algorithm, categorical sequence, conditional probability distribution model, pattern clustering, Hidden Markov models, Clustering algorithms, Markov processes, categorical sequence clustering, Data models, sequence cluster, clustering, Numerical models, two-tier Markov model]
BibClus: A Clustering Algorithm of Bibliographic Networks by Message Passing on Center Linkage Structure
2011 IEEE 11th International Conference on Data Mining
None
2011
Multi-type objects with multi-type relations are ubiquitous in real-world networks, e.g. bibliographic networks. Such networks are also called heterogeneous information networks. However, the research on clustering for heterogeneous information networks is little. A new algorithm, called NetClus, has been proposed in recent two years. Although NetClus is applied on a heterogeneous information network with a star network schema, considering the relations between center objects and all attribute objects linking to them, it ignores the relations between center objects such as citation relations, which also contain rich information. Hence, we think the star network schema cannot be used to characterize all possible relations without integrating the linkage structure among center objects, which we call the Center Linkage Structure, and there has been no practical way good enough to solve it. In this paper, we present a novel algorithm, BibClus, for clustering heterogeneous objects with center linkage structure by taking a bibliographic information network as an example. In BibClus, we build a probabilistic model of pair wise hidden Markov random field (P-HMRF) to characterize the center linkage structure, and convert it to a factor graph. We further combine EM algorithm with factor graph theory, and design an efficient way based on message passing algorithm to inference marginal probabilities and estimate parameters at each iteration of EM. We also study how factor functions affect clustering performance with different function forms and constraints. For evaluating our proposed method, we have conducted thorough experiments on a real dataset that we had crawled from ACM Digital Library. The experimental results show that BibClus is effective and has a much higher quantity than the recently proposed algorithm, NetClus, in both recall and precision.
[factor graph, iterative methods, heterogeneous information network, graph theory, factor graph theory, heterogeneous information networks, bibliographic systems, bibliographic networks, center linkage structure, BibClus, Clustering algorithms, multitype relations, ACM digital library, multitype objects, Joints, message passing, clustering algorithm, NetClus, star network schema, Probability, digital libraries, message passing algorithm, inference mechanisms, Couplings, Message passing, pattern clustering, inference marginal probabilities, Hidden Markov models, Markov processes, Approximation algorithms, clustering, EM iteration, pairwise hidden Markov random field]
Multi-instance Metric Learning
2011 IEEE 11th International Conference on Data Mining
None
2011
Multi-instance learning, like other machine learning and data mining tasks, requires distance metrics. Although metric learning methods have been studied for many years, metric learners for multi-instance learning remain almost untouched. In this paper, we propose a framework called Multi-Instance MEtric Learning (MIMEL) to learn an appropriate distance under the multi-instance setting. The distance metric between two bags is defined using the Mahalanobis distance function. The problem is formulated by minimizing the KL divergence between two multivariate Gaussians under the constraints of maximizing the between-class bag distance and minimizing the within-class bag distance. To exploit the mechanism of how instances determine bag labels in multi-instance learning, we design a nonparametric density-estimation-based weighting scheme to assign higher "weights" to the instances that are more likely to be positive in positive bags. The weighting scheme itself has a small workload, which adds little extra computing costs to the proposed framework. Moreover, to further boost the classification accuracy, a kernel version of MIMEL is presented. We evaluate MIMEL, using not only several typical multi-instance tasks, but also two activity recognition datasets. The experimental results demonstrate that MIMEL achieves better classification accuracy than many state-of-the-art distance based algorithms or kernel methods for multi-instance learning.
[Measurement, data mining, MIMEL, nonparametric density-estimation-based weighting scheme, classification accuracy, distance metrics, Multi-instance learning, Data mining, machine learning task, Learning systems, Training, multivariate Gaussians, Mathematical model, learning (artificial intelligence), Weighting scheme, Kernel, Mahalanobis distance function, multiinstance metric learning, Machine learning, Gaussian processes, KL divergence minimization, data mining task, between-class bag distance maximization, within-class bag distance minimization, Metric learning]
Multi-task Learning with Task Relations
2011 IEEE 11th International Conference on Data Mining
None
2011
Multi-task and relational learning with Gaussian processes are two active but also orthogonal areas of research. So far, there has been few attempt at exploring relational information within multi-task Gaussian processes. While existing relational Gaussian process methods have focused on relations among entities and in turn could be employed within an individual task, we develop a class of Gaussian process models which incorporates relational information across multiple tasks. As we will show, inference and learning within the resulting class of models, called relational multi-task Gaussian processes, can be realized via a variational EM algorithm. Experimental results on synthetic and real-world datasets verify the usefulness of this approach: The observed relational knowledge at the level of tasks can indeed reveal additional pair wise correlations between tasks of interest and, in turn, improve prediction performance.
[knowledge relation, Multi-task Learning, Probabilistic logic, Vectors, Covariance matrix, multitask learning, Equations, Relational Learning, Link-based Analysis, Gaussian process, Nonparametric Bayesian Models, Gaussian processes, relational information, Mathematical model, learning (artificial intelligence), Kernel, task relations]
Secure Clustering in Private Networks
2011 IEEE 11th International Conference on Data Mining
None
2011
Many clustering methods have been proposed for analyzing the relations inside networks with complex structures. Some of them can detect a mixture of assortative and disassortative structures in networks. All these methods are based on the fact that the entire network is observable. However, in the real world, the entities in networks, for example a social network, may be private, and thus, cannot be observed. We focus on private peer-to-peer networks in which all vertices are independent and private, and each vertex only knows about itself and its neighbors. We propose a privacy-preserving Gibbs sampling for clustering these types of private networks and detecting their mixed structures without revealing any private information about any individual entity. Moreover, the running cost of our method is related only to the number of clusters and the maximum degree, but is nearly independent of the number of vertices in the entire network.
[Protocols, sampling methods, peer-to-peer computing, Peer to peer computing, Social network services, assortative network structure, peer-to-peer networks, vertices, private networks, privacy, Vectors, Servers, Data mining, Gibbs sampling, computer network security, privacy-preserving Gibbs sampling, peer-to-peer, Privacy, pattern clustering, clustering, secure clustering, disassortative network structure, mixed network]
LPTA: A Probabilistic Model for Latent Periodic Topic Analysis
2011 IEEE 11th International Conference on Data Mining
None
2011
This paper studies the problem of latent periodic topic analysis from time stamped documents. The examples of time stamped documents include news articles, sales records, financial reports, TV programs, and more recently, posts from social media websites such as Flickr, Twitter, and Face book. Different from detecting periodic patterns in traditional time series database, we discover the topics of coherent semantics and periodic characteristics where a topic is represented by a distribution of words. We propose a model called LPTA (Latent Periodic Topic Analysis) that exploits the periodicity of the terms as well as term co-occurrences. To show the effectiveness of our model, we collect several representative datasets including Seminar, DBLP and Flickr. The results show that our model can discover the latent periodic topics effectively and leverage the information from both text and time well.
[document handling, social media websites, Computational modeling, sales records, time series, Twitter, topic modeling, Complexity theory, database management systems, latent periodic topic analysis, Equations, probabilistic model, Seminars, Analytical models, time stamped documents, Flickr, LPTA, Databases, TV programs, Face book, periodic topics, time series database, Mathematical model, financial reports]
Causal Associative Classification
2011 IEEE 11th International Conference on Data Mining
None
2011
Associative classifiers have received considerable attention due to their easy to understand models and promising performance. However, with a high dimensional dataset, associative classifiers inevitably face two challenges: (1) how to extract a minimal set of strong predictive rules from an explosive number of generated association rules, and (2) how to deal with the highly sensitive choice of the minimal support threshold. In order to address these two challenges, we introduce causality into associative classification, and propose a new framework of causal associative classification. In this framework, we use causal Bayesian networks to bridge irrelevant and redundant features with irrelevant and redundant rules in associative classification. Without loss of prediction power, the feature space involved with the antecedent of a classification rule is reduced to the space of the direct causes, direct effects, and direct causes of the direct effects, a.k.a. the Markov blanket, of the consequent of the rule in causal Bayesian networks. The proposed framework is instantiated via baseline classifiers using emerging patterns. Experimental results show that our framework significantly reduces the model complexity while outperforming the other state-of-the-art algorithms.
[pattern classification, baseline classifiers, feature space, classification rule, causal Bayesian networks, Classification algorithms, Association rules, causal bayesian networks, causal associative classification, model complexity, minimal support threshold, Bayesian methods, Lungs, associative classification, Markov processes, Feature extraction, Markov blanket, associative classifiers, belief networks, strong predictive rules, Cancer, computational complexity, emerging patterns]
Multi-task Learning for Bayesian Matrix Factorization
2011 IEEE 11th International Conference on Data Mining
None
2011
Data sparsity is a big challenge for collaborative filtering. This problem becomes more serious if the dataset is newly created and has even fewer ratings. By sharing knowledge among different datasets, multi-task learning is a promising technique to address this issue. Most prior work methods directly share objects (users or items) across different datasets. However, object identities and correspondences may not be known in many cases. We extend the previous work of Bayesian matrix factorization with Dirichlet process mixture into a multi-task learning approach by sharing latent parameters among different tasks. Our method does not require object identities and thus is more widely applicable. The proposed model is fully non-parametric in that the dimension of latent feature vectors is automatically determined. Inference is performed using the variational Bayesian algorithm, which is much faster than Gibbs sampling used by most other related Bayesian methods.
[Bayesian matrix factorization, collaborative filtering, variational Bayesian algorithm, matrix factorization, sampling methods, knowledge sharing, Dirichlet process, Vectors, Covariance matrix, matrix decomposition, multitask learning approach, Matrix decomposition, multitask learning, Gibbs sampling, Training, latent feature vectors, co-clustering, Bayesian methods, knowledge representation, Motion pictures, data sparsity, belief networks, learning (artificial intelligence), Principal component analysis]
Enabling Fast Lazy Learning for Data Streams
2011 IEEE 11th International Conference on Data Mining
None
2011
Lazy learning, such as k-nearest neighbor learning, has been widely applied to many applications. Known for well capturing data locality, lazy learning can be advantageous for highly dynamic and complex learning environments such as data streams. Yet its high memory consumption and low prediction efficiency have made it less favorable for stream oriented applications. Specifically, traditional lazy learning stores all the training data and the inductive process is deferred until a query appears, whereas in stream applications, data records flow continuously in large volumes and the prediction of class labels needs to be made in a timely manner. In this paper, we provide a systematic solution that overcomes the memory and efficiency limitations and enables fast lazy learning for concept drifting data streams. In particular, we propose a novel Lazy-tree (Ltree for short) indexing structure that dynamically maintains compact high-level summaries of historical stream records. L-trees are M-Tree [5] like, height-balanced, and can help achieve great memory consumption reduction and sub-linear time complexity for prediction. Moreover, L-trees continuously absorb new stream records and discard outdated ones, so they can naturally adapt to the dynamically changing concepts in data streams for accurate prediction. Extensive experiments on real-world and synthetic data streams demonstrate the performance of our approach.
[data stream classification, lazy-tree, data mining, dynamic learning environments, Complexity theory, M-tree, Ltree, Learning systems, fast lazy learning, data locality, Training data, tree data structures, learning (artificial intelligence), pattern classification, k-nearest neighbor learning, complex learning environments, sublinear time complexity, lazy learning, Routing, Vectors, memory consumption reduction, concept drifting, data stream mining, Memory management, data mining community, Spatial indexing, Indexing, computational complexity]
Clusterability Analysis and Incremental Sampling for Nystr&#x0F6;m Extension Based Spectral Clustering
2011 IEEE 11th International Conference on Data Mining
None
2011
To alleviate the memory and computational burdens of spectral clustering for large scale problems, some kind of low-rank matrix approximation is usually employed. Nystro&#x0308;m method is an efficient technique to generate low rank matrix approximation and its most important aspect is sampling. The matrix approximation errors of several sampling schemes have been theoretically analyzed for a number of learning tasks. However, the impact of matrix approximation error on the clustering performance of spectral clustering has not been studied. In this paper, we firstly analyze the performance of Nystro&#x0308;m method in terms of cluster ability, thus answer the impact of matrix approximation error on the clustering performance of spectral clustering. Our analysis immediately suggests an incremental sampling scheme for the Nystro&#x0308;m method based spectral clustering. Experimental results show that the proposed incremental sampling scheme outperforms existing sampling schemes on various clustering tasks and image segmentation applications, and its efficiency is comparable with existing sampling schemes.
[approximation theory, sampling methods, learning task, Nystro&#x0308;m extension based spectral clustering performance, low rank matrix approximation error, Vectors, matrix algebra, incremental sampling, Image segmentation, pattern clustering, image segmentation, Clustering algorithms, Approximation error, Sampling methods, Matrices, Nystr&#x0F6;m extension, learning (artificial intelligence), clusterability analysis, incremental sampling scheme, spectral clustering]
Fast and Robust Graph-based Transductive Learning via Minimum Tree Cut
2011 IEEE 11th International Conference on Data Mining
None
2011
In this paper, we propose an efficient and robust algorithm for graph-based transductive classification. After approximating a graph with a spanning tree, we develop a linear-time algorithm to label the tree such that the cut size of the tree is minimized. This significantly improves typical graph-based methods, which either have a cubic time complexity (for a dense graph) or O(kn2) (for a sparse graph with k denoting the node degree). Furthermore, our method shows great robustness to the graph construction both theoretically and empirically; this overcomes another big problem of traditional graph-based methods. In addition to its good scalability and robustness, the proposed algorithm demonstrates high accuracy. In particular, on a graph with 400,000 nodes (in which 10,000 nodes are labeled) and 10,455,545 edges, our algorithm achieves the highest accuracy of 99.6% but takes less than 10 seconds to label all the unlabeled data.
[Algorithm design and analysis, Laplace equations, robust graph based transductive learning, graph theory, transductive learning, graph based transductive classification, cubic time complexity, Complexity theory, graph mining, linear time algorithm, graph-based semi-supervised learning, spanning tree, Approximation algorithms, Prediction algorithms, fast graph based transductive learning, Robustness, minimum tree cut, Labeling, learning (artificial intelligence), graph construction, computational complexity]
Positive and Unlabeled Learning for Graph Classification
2011 IEEE 11th International Conference on Data Mining
None
2011
The problem of graph classification has drawn much attention in the last decade. Conventional approaches on graph classification focus on mining discriminative sub graph features under supervised settings. The feature selection strategies strictly follow the assumption that both positive and negative graphs exist. However, in many real-world applications, the negative graph examples are not available. In this paper we study the problem of how to select useful sub graph features and perform graph classification based upon only positive and unlabeled graphs. This problem is challenging and different from previous works on PU learning, because there are no predefined features in graph data. Moreover, the sub graph enumeration problem is NP-hard. We need to identify a subset of unlabeled graphs that are most likely to be negative graphs. However, the negative graph selection problem and the sub graph feature selection problem are correlated. Before the reliable negative graphs can be resolved, we need to have a set of useful sub graph features. In order to address this problem, we first derive an evaluation criterion to estimate the dependency between sub graph features and class labels based on a set of estimated negative graphs. In order to build accurate models for the PU learning problem on graph data, we propose an integrated approach to concurrently select the discriminative features and the negative graphs in an iterative manner. Experimental results illustrate the effectiveness and efficiency of the proposed method.
[pattern classification, graph theory, PU learning, data mining, Vectors, unlabeled learning, Data mining, negative graph selection problem, Optimization, Support vector machines, NP-hard problem, graph classification, discriminative subgraph features mining, positive and unlabeled data, Data models, subgraph enumeration problem, Reliability, learning (artificial intelligence), Kernel, feature selection, computational complexity, positive learning, unlabeled graphs]
Finding Novel Diagnostic Gene Patterns Based on Interesting Non-redundant Contrast Sequence Rules
2011 IEEE 11th International Conference on Data Mining
None
2011
Diagnostic genes refer to the genes closely related to a specific disease phenotype, the powers of which to distinguish between different classes are often high. Most methods to discovering the powerful diagnostic genes are either singleton discriminability-based or combination discriminability-based. However, both ignore the abundant interactions among genes, which widely exist in the real world. In this paper, we tackle the problem from a new point of view and make the following contributions: (1) we propose an EWave model, which profitably exploits the ordered expressions among genes based on the defined equivalent dimension group sequences taking into account the "noise" universal in the real data, (2) we devise a novel sequence rule, namely interesting non-redundant contrast sequence rule, which is able to capture the difference between different phenotypes in a high accuracy using as few as possible genes, (3) we present an efficient algorithm called NRMINER to find such rules. Unlike the conventional column enumeration and the more recent row enumeration, it performs a novel template-driven enumeration by making use of the special characteristic of micro array data modeled by EWave. Extensive experiments conducted on various synthetic and real datasets show that: (1) NRMINER is significantly faster than the competing algorithm by up to about one order of magnitude, (2) it provides a higher accuracy using fewer genes. Many diagnostic genes discovered by NRMINER are proved biologically related to some disease.
[finding novel diagnostic gene patterns, interesting nonredundant contrast sequence rules, biology, Noise, data mining, disease phenotype, NRMINER, Generators, sequence rule, Gene expression, diagnostic gene, Diseases, EWave model, Accuracy, column enumeration, Data models, powerful diagnostic genes, micro array data, pattern recognition]
Semi-supervised Hierarchical Clustering
2011 IEEE 11th International Conference on Data Mining
None
2011
Semi-supervised clustering (i.e., clustering with knowledge-based constraints) has emerged as an important variant of the traditional clustering paradigms. However, most existing semi-supervised clustering algorithms are designed for partitional clustering methods and few research efforts have been reported on semi-supervised hierarchical clustering methods. In addition, current semi-supervised clustering methods have been focused on the use of background information in the form of instance level must-link and cannot-link constraints, which are not suitable for hierarchical clustering where data objects are linked over different hierarchy levels. In this paper, we propose a novel semi-supervised hierarchical clustering framework based on ultra-metric dendrogram distance. The proposed framework is able to incorporate triple-wise relative constraints. We establish the connection between hierarchical clustering and ultra-metric transformation of dissimilarity matrix and propose two techniques (the constrained optimization technique and the transitive dissimilarity based technique) for semi-supervised hierarchical clustering. Experimental results demonstrate the effectiveness and the efficiency of our proposed methods.
[Measurement, partitional clustering methods, Clustering methods, must link constraints, Educational institutions, Vectors, dissimilarity matrix, Hierarchical clustering, Optimization, matrix algebra, cannot link constraints, USA Councils, pattern clustering, triple wise relative constraints, Clustering algorithms, semisupervised hierarchical clustering, ultra metric transformation, constraint handling, semi-supervised clustering, triple-wise relative constraints]
Handling Conditional Discrimination
2011 IEEE 11th International Conference on Data Mining
None
2011
Historical data used for supervised learning may contain discrimination. We study how to train classifiers on such data, so that they are discrimination free with respect to a given sensitive attribute, e.g., gender. Existing techniques that deal with this problem aim at removing all discrimination and do not take into account that part of the discrimination may be explainable by other attributes, such as, e.g., education level. In this context, we introduce and analyze the issue of conditional non-discrimination in classifier design. We show that some of the differences in decisions across the sensitive groups can be explainable and hence tolerable. We observe that in such cases, the existing discrimination aware techniques will introduce a reverse discrimination, which is undesirable as well. Therefore, we develop local techniques for handling conditional discrimination when one of the attributes is considered to be explanatory. Experimental evaluation demonstrates that the new local techniques remove exactly the bad discrimination, allowing differences in decisions as long as they are explainable.
[handling conditional discrimination, Correlation, Decision making, supervised learning, Educational institutions, Remuneration, Data mining, classification, independence, Computer science, historical data, education level, Data models, discrimination, data handling, learning (artificial intelligence)]
On the Hardness of Graph Anonymization
2011 IEEE 11th International Conference on Data Mining
None
2011
In this paper, we examine the problem of node re-identification from anonymized graphs. Typical graphs encountered in real applications are massive and sparse. In this paper, we will show that massive and sparse graphs have certain theoretical properties which make them susceptible to re-identification attacks. We design a systematic way to exploit these theoretical properties in order to construct re- identification signatures, which are also known as characteristic vectors. These signatures have the property that they are extremely robust to perturbations, especially for massive and sparse graphs. Our results show that even low levels of anonymization require perturbation levels which are significant enough to result in a massive loss of utility. Our experimental results also show that the true anonymization level of graphs is much lower than is implied by measures such as k-anonymity. Thus, the results of this paper establish that the problem of massive graph anonymization has fundamental theoretical barriers which prevent a fully effective solution.
[data analysis, Social network services, graph theory, data mining, sparse graphs, privacy, Vectors, Approximation methods, re-identification attacks, Couplings, characteristic vectors, social network, Privacy, anonymization, graphs, graph anonymization, social networking (online), re-identification signatures, Robustness, digital signatures, Random variables, massive graphs, node re-identification]
SPO: Structure Preserving Oversampling for Imbalanced Time Series Classification
2011 IEEE 11th International Conference on Data Mining
None
2011
This paper presents a novel structure preserving over sampling (SPO) technique for classifying imbalanced time series data. SPO generates synthetic minority samples based on multivariate Gaussian distribution by estimating the covariance structure of the minority class and regularizing the unreliable eigen spectrum. By preserving the main covariance structure and intelligently creating protective variances in the trivial eigen feature dimensions, the synthetic samples expand effectively into the void area in the data space without being too closely tied with existing minority-class samples. Extensive experiments based on several public time series datasets demonstrate that our proposed SPO in conjunction with support vector machines can achieve better performances than existing over sampling methods and state-of-the-art methods in time series classification.
[imbalance, support vector machines, covariance structure, time series datasets, Time series analysis, eigen regularization, Gaussian distribution, time series, Vectors, learning, Classification algorithms, Oversampling, SVM, Training, Support vector machines, synthetic minority samples, structure preserving oversampling, eigen feature dimensions, multivariate Gaussian distribution, Eigenvalues and eigenfunctions, structure preserving, Reliability, time series classification]
Manifold Learning and Missing Data Recovery through Unsupervised Regression
2011 IEEE 11th International Conference on Data Mining
None
2011
We propose an algorithm that, given a high-dimensional dataset with missing values, achieves the distinct goals of learning a nonlinear low-dimensional representation of the data (the dimensionality reduction problem) and reconstructing the missing high-dimensional data (the matrix completion, or imputation, problem). The algorithm follows the Dimensionality Reduction by Unsupervised Regression approach, where one alternately optimizes over the latent coordinates given the reconstruction and projection mappings, and vice versa, but here we also optimize over the missing data, using an efficient, globally convergent Gauss-Newton scheme. We also show how to project or reconstruct test data with missing values. We achieve impressive reconstructions while learning good latent representations in image restoration with 50% missing pixels.
[Linear systems, missing high-dimensional data reconstruction, matrix completion, regression analysis, Vectors, nonlinear low-dimensional representation, projection mappings, Image reconstruction, Optimization, missing data, matrix algebra, unsupervised learning, manifold learning, Manifolds, Training, Jacobian matrices, data reduction, high-dimensional datasets, dimensionality reduction, image restoration, Gaussian processes, missing data recovery, convergent Gauss-Newton scheme, unsupervised regression approach]
Characterizing Inverse Time Dependency in Multi-class Learning
2011 IEEE 11th International Conference on Data Mining
None
2011
The training time of most learning algorithms increases as the size of training data increases. Yet, recent advances in linear binary SVM and LR challenge this commonsense by proposing an inverse dependency property, where the training time decreases as the size of training data increases. In this paper, we study the inverse dependency property of multi-class classification problem. We describe a general framework for multi-class classification problem with a single objective to achieve inverse dependency and extend it to three popular multi-class algorithms. We present theoretical results demonstrating its convergence and inverse dependency guarantee. We conduct experiments to empirically verify the inverse dependency of all the three algorithms on large-scale datasets as well as to ensure the accuracy.
[Algorithm design and analysis, inverse dependency, pattern classification, support vector machines, supervised learning, multiclass classification problem, linear binary SVM, Convergence, multiclass learning, LR challenge, Support vector machines, Training, Accuracy, multi-class learning, inverse time dependency, Training data, inverse dependency property, learning (artificial intelligence), large-scale classification, Logistics]
Supervised Lazy Random Walk for Topic-Focused Multi-document Summarization
2011 IEEE 11th International Conference on Data Mining
None
2011
Topic-focused multi-document summarization aims to produce a summary given a specific topic description and a set of related documents. It has become a crucial text processing task in many real applications that can help users consume the massive information. This paper presents a novel extractive approach based on supervised lazy random walk (Super Lazy). This approach naturally combines the rich features of sentences with the intrinsic sentence graph structure in a principled way, and thus enjoys the advantages of both the existing supervised and unsupervised approaches. Moreover, our approach can achieve the three major goals of topic-focused multi-document summarization (i.e. relevance, salience and diversity) simultaneously with a unified ranking process. Experiments on the benchmark dataset TAC2008 and TAC2009 are performed and the ROUGE evaluation results demonstrate that our approach can significantly outperform both the state-of-the-art supervised and unsupervised methods.
[Measurement, text analysis, topic-focused multi-document summarization, Humans, supervised lazy random walk, information retrieval, random processes, benchmark dataset TAC2008, Vectors, benchmark dataset TAC2009, ROUGE evaluation, relevance, Support vector machines, Training, salience, diversity, unified ranking process, topic focused multidocument summarization, Lead, SuperLazy, Feature extraction, text processing task, learning (artificial intelligence), intrinsic sentence graph structure]
Unsupervised Anomaly Intrusion Detection via Localized Bayesian Feature Selection
2011 IEEE 11th International Conference on Data Mining
None
2011
In recent years, an increasing number of security threats have brought a serious risk to the internet and computer networks. Intrusion Detection System (IDS) plays a vital role in detecting various kinds of attacks. Developing adaptive and flexible oriented IDSs remains a challenging and demanding task due to the incessantly appearance of new types of attacks and sabotaging approaches. In this paper, we propose a novel unsupervised statistical approach for detecting network based attacks. In our approach, patterns of normal and intrusive activities are learned through finite generalized Dirichlet mixture models, in the context of Bayesian variational inference. Under the proposed variational framework, the parameters, the complexity of the mixture model, and the features saliency can be estimated simultaneously, in a closed-form. We evaluate the proposed approach using the popular KDD CUP 1999 data set. Experimental results show that this approach is able to detect many different types of intrusions accurately with a low false positive rate.
[Entropy, anomaly detection, Training, Accuracy, security threats, generalized Dirichlet, Intrusion detection, network based attack detection, feature selection, Computational modeling, computer networks, model selection, finite generalized Dirichlet mixture models, inference mechanisms, computer network security, Bayesian variational inference, unsupervised learning, intrusion detection system, unsupervised statistical approach, Hidden Markov models, localized Bayesian feature selection, Feature extraction, unsupervised anomaly intrusion detection, Bayes methods, Internet, statistical analysis, mixture models, variational inference]
Identifying Differentially Expressed Genes via Weighted Rank Aggregation
2011 IEEE 11th International Conference on Data Mining
None
2011
Identifying differentially expressed genes is an important problem in gene expression analysis, since these genes, exhibiting sufficiently different expression levels under distinct experiment conditions, could be critical for tracing the progression of a disease. In a micro array study, genes are usually sorted in terms of their differentiation abilities with the more differentially expressed genes being ranked higher in the list. As more micro array studies are conducted, rank aggregation becomes an important means to combine such ranked gene lists in order to discover more reliable differentially expressed genes. In this paper, we study a novel weighted gene rank aggregation problem whose complexity is at least NP-hard. To tackle the problem, we develop a new Markov-chain based rank aggregation method called Weighted MC (WMC). The WMC algorithm makes use of rank-based weight information to generate the transition matrix. Extensive experiments on the real biological datasets show that our approach is more efficient in aggregating long gene lists. Importantly, the WMC method is much more robust for identifying biologically significant genes compared with the state-of-the-art methods.
[disease progression tracing, gene expression analysis, weighted gene rank aggregation problem, lab-on-a-chip, diseases, differential expression, Complexity theory, rank aggregation, Gene expression, Diseases, Markov chain, Itemsets, genetics, NP-hard problem, transition matrix, Markov-chain based rank aggregation method, Markov processes, existence disagreement, Reliability, differentially expressed gene identification, ordering disagreement, computational complexity]
Efficient Mining of Closed Sequential Patterns on Stream Sliding Window
2011 IEEE 11th International Conference on Data Mining
None
2011
As a typical data mining research topic, sequential pattern mining has been studied extensively for the past decade. Recently, mining various sequential patterns incrementally over stream data has raised great interest. Due to the challenges of mining stream data, many difficulties not so obvious in static data mining have to be reconsidered carefully. In this paper, we propose a novel algorithm which stores only frequent closed prefixes in its enumeration tree structure, used for mining and maintaining patterns in the current sliding window, to solve the frequent closed sequential pattern mining problem efficiently over stream data. Some effective search space pruning and pattern closure checking strategies have been also devised to accelerate the algorithm. Experimental results show that our algorithm outperforms other state-of-the-art algorithm significantly in both running time and memory use.
[Algorithm design and analysis, pattern mining, Unsolicited electronic mail, data mining, stream data, Data mining, stream sliding window, Runtime, Itemsets, Sliding Window, closed sequential patterns, Acceleration, search space pruning, pattern closure checking, Closed Sequential Pattern]
A Spectral Framework for Detecting Inconsistency across Multi-source Object Relationships
2011 IEEE 11th International Conference on Data Mining
None
2011
In this paper, we propose to conduct anomaly detection across multiple sources to identify objects that have inconsistent behavior across these sources. We assume that a set of objects can be described from various perspectives (multiple information sources). The underlying clustering structure of normal objects is usually shared by multiple sources. However, anomalous objects belong to different clusters when considering different aspects. For example, there exist movies that are expected to be liked by kids by genre, but are liked by grown-ups based on user viewing history. To identify such objects, we propose to compute the distance between different eigen decomposition results of the same object with respect to different sources as its anomalous score. We also give interpretations from the perspectives of constrained spectral clustering and random walks over graph. Experimental results on several UCI as well as DBLP and Movie Lens datasets demonstrate the effectiveness of the proposed approach.
[Laplace equations, eigen decomposition, spectral methods, Vectors, object detection, anomaly detection, History, Object recognition, movielens datasets, Clustering algorithms, Motion pictures, clustering structure, UCI, multi source object relationships, Joints, DBLP, multiple information sources]
Tracking and Connecting Topics via Incremental Hierarchical Dirichlet Processes
2011 IEEE 11th International Conference on Data Mining
None
2011
Much research has been devoted to topic detection from text, but one major challenge has not been addressed: revealing the rich relationships that exist among the detected topics. Finding such relationships is important since many applications are interested in how topics come into being, how they develop, grow, disintegrate, and finally disappear. In this paper, we present a novel method that reveals the connections between topics discovered from the text data. Specifically, our method focuses on how one topic splits into multiple topics, and how multiple topics merge into one topic. We adopt the hierarchical Dirichlet process (HDP) model, and propose an incremental Gibbs sampling algorithm to incrementally derive and refine the labels of clusters. We then characterize the splitting and merging patterns among clusters based on how labels change. We propose a global analysis process that focuses on cluster splitting and merging, and a finer granularity analysis process that helps users to better understand the content of the clusters and the evolution patterns. We also develop a visualization process to present the results.
[text analysis, Merging, merging pattern, Predictive models, incremental hierarchical Dirichlet process, splitting pattern, evolution pattern clustering, global analysis process, Semantics, text data, Clustering algorithms, data visualisation, Mixture models, topic detection, Business, merging, sampling methods, granularity analysis process, Hierarchical Dirichlet processes, Time measurement, Clustering, incremental Gibbs sampling algorithm, pattern clustering, Data models, Incremental Gibbs Sampling]
Learning Protein Folding Energy Functions
2011 IEEE 11th International Conference on Data Mining
None
2011
A critical open problem in \\emph{ab initio} protein folding is protein energy function design, which pertains to defining the energy of protein conformations in a way that makes folding most efficient and reliable. In this paper, we address this issue as a weight optimization problem and utilize a machine learning approach, learning-to-rank, to solve this problem. We investigate the ranking-via-classification approach, especially the Ranking SVM method and compare it with the state-of-the-art approach to the problem using the MINUIT optimization package. To maintain the physicality of the results, we impose non-negativity constraints on the weights. For this we develop two efficient non-negative support vector machine (NNSVM) methods, derived from L2-norm SVM and L1-norm SVMs, respectively. We demonstrate an energy function which maintains the correct ordering with respect to structure dissimilarity to the native state more often, is more efficient and reliable for learning on large protein sets, and is qualitatively superior to the current state-of-the-art energy function.
[optimization problem, Correlation, non-negativity constrained SVM optimization, support vector machines, \\emph{ab initio} protein folding, MINUIT optimization package, NNSVM, Vectors, learning protein folding energy functions, machine learning, Optimization, Proteins, Support vector machines, optimisation, energy function, support vector machine, biology computing, learning-to-rank, protein conformations, Reliability, nonnegative support vector machine, ranking SVM method]
How Does Research Evolve? Pattern Mining for Research Meme Cycles
2011 IEEE 11th International Conference on Data Mining
None
2011
Recent years have witnessed a great deal of attention in tracking news memes over the web, modeling shifts in the ebb and flow of their popularity. One of the most important features of news memes is that they seldom occur repeatedly, instead, they tend to shift to different but similar memes. In this work, we consider patterns in research memes, which differ significantly from news memes and have received very little attention. One significant difference between research memes and news memes lies in that research memes have cyclic development, motivating the need for models of cycles of research memes. Furthermore, these cycles may reveal important patterns of evolving research, shedding lights on how research progresses. In this paper, we formulate the modeling of the cycles of research memes, and propose solutions to the problem of identifying cycles and discovering patterns among these cycles. Experiments on two different domain applications indicate that our model does find meaningful patterns and our algorithms for pattern discovery are efficient for large scale data analysis.
[data analysis, Computational modeling, Heuristic algorithms, Social network services, pattern mining, frequent patterns, data mining, Research memes, news memes, Ontologies, pattern discovery, Complexity theory, cyclic development, Data mining, Computer science, research meme cycles, MeSH hierarchy, large scale data analysis, topic mining, Web, shortest paths, research and development, Internet, topic evolution]
Twin Gaussian Processes for Binary Classification
2011 IEEE 11th International Conference on Data Mining
None
2011
Gaussian process classifiers (GPCs) have recently attracted more and more attention from the machine learning community. However, because the posterior needs to be approximated by using a tractable Gaussian distribution, they usually suffer from high computational cost which is prohibitive for practical applications. In this paper, we present a new Gaussian process model termed as twin Gaussian processes for binary classification. The basic idea is to make predictions based on two latent functions with Gaussian process prior, each of which is close to one of the two classes and is as far as possible from the other. Being compared with the published GPCs, the proposed algorithm allows for an explicit inference based on analytical methods, thereby avoiding the high computational cost caused by approximating the posterior with Gaussian distribution. Experimental results on several benchmark data sets show that the proposed algorithm is valid and can achieve superior performance to the published algorithms.
[pattern classification, Twin Gaussian processes, Computational modeling, Gaussian process classifier, Gaussian distribution, tractable Gaussian distribution, Covariance matrix, Approximation methods, machine learning community, Training, twin Gaussian process, latent function, Bayesian methods, Gaussian processes, Approximation algorithms, Computational efficiency, learning (artificial intelligence), binary classification, Binary classification, Kernel machine]
Constraint Selection-Based Semi-supervised Feature Selection
2011 IEEE 11th International Conference on Data Mining
None
2011
In this paper, we present a novel feature selection approach based on an efficient selection of pair wise constraints. This aims at selecting the most coherent constraints extracted from labeled part of data. The relevance of features is then evaluated according to their efficient locality preserving and chosen constraint preserving ability. Finally, experimental results are provided for validating our proposal with respect to other known feature selection methods.
[Dimensionality reduction, Laplace equations, constraint selection based semisupervised feature selection, Vectors, Data mining, constraint selection, Accuracy, pairwise constraints, locality preservation, Clustering algorithms, Coherence, constraint preservation, Feature extraction, data handling, constraint handling, feature selection]
Discovering the Intrinsic Cardinality and Dimensionality of Time Series Using MDL
2011 IEEE 11th International Conference on Data Mining
None
2011
Most algorithms for mining or indexing time series data do not operate directly on the original data, but instead they consider alternative representations that include transforms, quantization, approximation, and multi-resolution abstractions. Choosing the best representation and abstraction level for a given task/dataset is arguably the most critical step in time series data mining. In this paper, we investigate techniques to discover the natural intrinsic representation model, dimensionality and alphabet cardinality of a time series. The ability to discover these intrinsic features has implications beyond selecting the best parameters for particular algorithms, as characterizing data in such a manner is useful in its own right and an important sub-routine in algorithms for classification, clustering and outlier discovery. We will frame the discovery of these intrinsic features in the Minimal Description Length (MDL) framework. Extensive empirical tests show that our method is simpler, more general and significantly more accurate than previous methods, and has the important advantage of being essentially parameter-free.
[alphabet cardinality, Time series analysis, Discrete Fourier transforms, Piecewise linear approximation, data mining, Muscles, time series, Approximation methods, Data mining, intrinsic cardinality discovering, Dimensionality Reduction, MDL, intrinsic representation model, Time Series, Data models, time series data mining, minimal description length]
Discovery of Versatile Temporal Subspace Patterns in 3-D Datasets
2011 IEEE 11th International Conference on Data Mining
None
2011
Most existing methods for clustering temporal data are based on either a strict similarity metric or a precisely defined temporal profile such as a sine, exponential wave etc. Also, these methods compute similarity metric across the entire time-span of the objects. However these types of temporal patterns are more useful in many biological analysis, where it is important to observe gene expression pattens across arbitrary subintervals. These types of temporal patterns are very useful in bioinformatics, where it is important to observe gene expression pattens across arbitrary subintervals. In this paper we present an algorithm for searching for multiple contiguous temporal subintervals within which the selected objects demonstrate existence of clear patterns. We demonstrate the power and advantages of our algorithm by using a synthetic dataset and a pharmacokinetics dataset for which other researchers have recently published their results. We compare and contrast our results with these results to show superiority of our approach.
[Context, Algorithm design and analysis, versatile temporal subspace patterns, Correlation, synthetic dataset, Tricluster, Search problems, Data Mining, biological analysis, Gene expression, pharmacokinetics dataset, temporal data clustering, pattern clustering, Coherence, bioinformatics, 3D datasets, gene expression pattens, Bioinformatics, Temporal patterns]
A Fixed Parameter Tractable Integer Program for Finding the Maximum Order Preserving Submatrix
2011 IEEE 11th International Conference on Data Mining
None
2011
Order-preserving sub matrices are an important tool for the analysis of gene expression data. As finding large order-preserving sub matrices is a computationally hard problem, previous work has investigated both exact but exponential-time as well as polynomial-time but inexact algorithms for finding large order-preserving sub matrices. In this paper, we propose a novel exact algorithm to find maximum order preserving sub matrices which is fixed parameter tractable with respect to the number of columns of the provided gene expression data. In particular, our algorithm is based on solving a sequence of mixed integer linear programs and it exhibits better guarantees as well as better runtime performance as compared to the state-of-the-art exact algorithms. Our empirical study in benchmark datasets shows large improvement in terms of computational speed.
[gene expression data, biology, integer programming, Noise, Programming, Linear programming, linear programming, fixed parameter tractable integer program, Gene expression, Optimization, Radio access networks, matrix algebra, Upper bound, exponential time, integer linear programs, polynomial time, maximum order preserving submatrix, benchmark datasets]
ASAP: A Self-Adaptive Prediction System for Instant Cloud Resource Demand Provisioning
2011 IEEE 11th International Conference on Data Mining
None
2011
The promise of cloud computing is to provide computing resources instantly whenever they are needed. The state-of-art virtual machine (VM) provisioning technology can provision a VM in tens of minutes. This latency is unacceptable for jobs that need to scale out during computation. To truly enable on-the-fly scaling, new VM needs to be ready in seconds upon request. In this paper, We present an online temporal data mining system called ASAP, to model and predict the cloud VM demands. ASAP aims to extract high level characteristics from VM provisioning request stream and notify the provisioning system to prepare VMs in advance. For quantification issue, we propose Cloud Prediction Cost to encodes the cost and constraints of the cloud and guide the training of prediction algorithms. Moreover, we utilize a two-level ensemble method to capture the characteristics of the high transient demands time series. Experimental results using historical data from an IBM cloud in operation demonstrate that ASAP significantly improves the cloud service quality and provides possibility for on-the-fly provisioning.
[service quality improvement, time series prediction, Heuristic algorithms, Time series analysis, data mining, cloud prediction cost, Artificial neural networks, instant cloud resource demand provisioning, Predictive models, online temporal data mining system, virtual machine provisioning technology, software quality, cloud service quality, Training, two-level ensemble method, cloud service, virtual machines, Prediction algorithms, self-adaptive prediction system, Data models, cloud computing, ASAP]
Learning from Negative Examples in Set-Expansion
2011 IEEE 11th International Conference on Data Mining
None
2011
This paper addresses the task of set-expansion on free text. Set-expansion has been viewed as a problem of generating an extensive list of instances of a concept of interest, given a few examples of the concept as input. Our key contribution is that we show that the concept definition can be significantly improved by specifying some negative examples in the input, along with the positive examples. The state-of-the art centroid-based approach to set-expansion doesn't readily admit the negative examples. We develop an inference-based approach to set-expansion which naturally allows for negative examples and show that it performs significantly better than a strong baseline.
[Vocabulary, text analysis, state-of-the art centroid, Vectors, learning, Set-Expansion, set theory, Equations, Negative Examples, set expansion, free text, USA Councils, negative example learning, Semantics, Information Extraction, Mathematical model, IP networks, learning (artificial intelligence)]
Entropy-Based Graph Clustering: Application to Biological and Social Networks
2011 IEEE 11th International Conference on Data Mining
None
2011
Complex systems have been widely studied to characterize their structural behaviors from a topological perspective. High modularity is one of the recurrent features of real-world complex systems. Various graph clustering algorithms have been applied to identifying communities in social networks or modules in biological networks. However, their applicability to real-world systems has been limited because of the massive scale and complex connectivity of the networks. In this study, we exploit a novel information-theoretic model for graph clustering. The entropy-based clustering approach finds locally optimal clusters by growing a random seed in a manner that minimizes graph entropy. We design and analyze modifications that further improve its performance. Assigning priority in seed-selection and seed-growth is well applicable to the scale-free networks characterized by the hub-oriented structure. Computing seed-growth in parallel streams also decomposes an extremely large network efficiently. The experimental results with real biological and social networks show that the entropy-based approach has better performance than competing methods in terms of accuracy and efficiency.
[scale free networks, graph theory, social networks, seed growth, entropy based graph clustering, network theory (graphs), hub oriented structure, Entropy, complex networks, information theoretic model, graph entropy, large-scale systems, biological networks, graph clustering, Proteins, graph mining, Accuracy, complex systems, pattern clustering, Clustering algorithms, seed selection, social networking (online), MySpace, real world complex systems]
Semi-supervised Discriminant Hashing
2011 IEEE 11th International Conference on Data Mining
None
2011
Hashing refers to methods for embedding high dimensional data into a similarity-preserving low-dimensional Hamming space such that similar objects are indexed by binary codes whose Hamming distances are small. Learning hash functions from data has recently been recognized as a promising approach to approximate nearest neighbor search for high dimensional data. Most of 'learning to hash' methods resort to either unsupervised or supervised learning to determine hash functions. Recently semi-supervised learning approach was introduced in hashing where pair wise constraints (must link and cannot-link) using labeled data are leveraged while unlabeled data are used for regularization to avoid over-fitting. In this paper we base our semi-supervised hashing on linear discriminant analysis, where hash functions are learned such that labeled data are used to maximize the separability between binary codes associated with different classes while unlabeled data are used for regularization as well as for balancing condition and pair wise decor relation of bits. The resulting method is referred to as semi-supervised discriminant hashing (SSDH). Numerical experiments on MNIST and CIFAR-10 datasets demonstrate that our method outperforms existing methods, especially in the case of short binary codes.
[similarity search, semisupervised, semi-supervised discriminant hashing, Training, high dimensional data, Training data, Binary codes, Eigenvalues and eigenfunctions, Decorrelation, learning (artificial intelligence), linear discriminant analysis, approximation theory, binary codes, CIFAR-10 dataset, Hamming distance, regularized discriminant analysis, information retrieval, cryptography, Vectors, nearest neighbor search approximation, Hamming distances, MNIST dataset, labeled data, Hashing, similarity-preserving low-dimensional Hamming space, unlabeled data, statistical analysis, hash function learning]
Using Frequent Closed Itemsets for Data Dimensionality Reduction
2011 IEEE 11th International Conference on Data Mining
None
2011
We address important issues of dimensionality reduction of transactional data sets where the input data consists of lists of transactions, each of them being a finite set of items. The reduction consists in finding a small set of new items, so-called factor-items, which is considerably smaller than the original set of items while comprising full or nearly full information about the original items. Using this type of reduction, the original data set can be represented by a smaller transactional data set using factor-items instead of the original items, thus reducing its dimensionality. The procedure utilized in this paper is based on approximate Boolean matrix decomposition. In this paper, we focus on the role of frequent closed item sets that can be used to determine factor-items. We present the factorization problem, its reduction to Boolean matrix decompositions, experiments with publicly available data sets, and an algorithm for computing decompositions.
[Boolean matrices, factorization problem, boolean matrix decomposition, matrix decomposition, frequent closed itemsets, Approximation methods, Matrix decomposition, Data mining, set covering, data dimensionality reduction, Boolean functions, dimensionality reduction, Itemsets, Approximation algorithms, Arrays, transactional data sets]
Modeling High-Level Behavior Patterns for Precise Similarity Analysis of Software
2011 IEEE 11th International Conference on Data Mining
None
2011
The analysis of software similarity has many applications such as detecting code clones, software plagiarism, code theft, and polymorphic malware. Because often source code is unavailable and code obfuscation is used to avoid detection, there has been much research on developing effective models to capture runtime behavior to aid detection. Existing models focus on low-level information such as dependency or purely occurrence of function calls, and suffer from poor precision, poor scalability, or both. To overcome limitations of existing models, this paper introduces a precise and succinct behavior representation that characterizes high-level object-accessing patterns as regular expressions. We first distill a set of high-level patterns (the alphabet S of the regular language) based on two pieces of information: function call patterns to access objects and type state information of the objects. Then we abstract a runtime trace of a program P into a regular expression e over the pattern alphabet S to produce P's behavior signature. We show that software instances derived from the same code exhibit similar behavior signatures and develop effective algorithms to cluster and match behavior signatures. To evaluate the effectiveness of our behavior model, we have applied it to the similarity analysis of polymorphic malware. Our results on a large malware collection demonstrate that our model is both precise and succinct for effective and scalable matching and detection of polymorphic malware.
[Measurement, Algorithm design and analysis, invasive software, pattern matching, program runtime tracing, precise behavior representation, software behavior model, industrial property, source code theft, function call pattern, Analytical models, Clustering algorithms, succinct behavior representation, Malware, code clone detection, poor precision, polymorphic malware, runtime behavior, sequence clustering, code obfuscation, program diagnostics, Software algorithms, software plagiarism, high-level object-accessing pattern modeling, behavior signature, malware analysis and clustering, software precise similarity analysis, Software, poor scalability]
Co-clustering for Binary and Categorical Data with Maximum Modularity
2011 IEEE 11th International Conference on Data Mining
None
2011
To tackle the co-clustering problem for binary and categorical data, we propose a generalized modularity measure and a spectral approximation of the modularity matrix. A spectral algorithm maximizing the modularity measure is then presented. Experimental results are performed on a variety of simulated and real-world data sets confirming the interest of the use of the modularity in co-clustering and assessing the number of clusters contexts.
[spectral decomposition, Clustering methods, spectral approximation, modularity matrix, Educational institutions, Partitioning algorithms, Approximation methods, matrix algebra, binary data, co-clustering, Accuracy, modularity, pattern clustering, Clustering algorithms, categorical data, Eigenvalues and eigenfunctions, coclustering, maximum modularity]
Calculating Feature Weights in Naive Bayes with Kullback-Leibler Measure
2011 IEEE 11th International Conference on Data Mining
None
2011
Naive Bayesian learning has been popular in data mining applications. However, the performance of naive Bayesian learning is sometimes poor due to the unrealistic assumption that all features are equally important and independent given the class value. Therefore, it is widely known that the performance of naive Bayesian learning can be improved by mitigating this assumption, and many enhancements to the basic naive Bayesian learning have been proposed to resolve this problem including feature selection and feature weighting. In this paper, we propose a new method for calculating the weights of features in naive Bayesian learning using Kullback-Leibler measure. Empirical results are presented comparing this new feature weighting method with some other methods for a number of datasets.
[Weight measurement, Feature Weighting, data mining, naive Bayesian learning, feature weights calculation, Kullback-Leibler measurement, feature weighting, Equations, Accuracy, Naive Bayes, Bayesian methods, Training data, Classification, Bayes methods, Decision trees, Mathematical model, learning (artificial intelligence), feature selection]
Scalable Diversified Ranking on Large Graphs
2011 IEEE 11th International Conference on Data Mining
None
2011
Enhancing diversity in ranking on graphs has been identified as an important retrieval and mining task. Nevertheless, many existing diversified ranking algorithms cannot be scalable to large graphs as they have high time or space complexity. In this paper, we propose a scalable algorithm to find the top-K diversified ranking list on graphs. The key idea of our algorithm is that we first compute the Pagerank of the nodes of the graph, and then perform a carefully designed vertex selection algorithm to find the top-K diversified ranking list. Specifically, we firstly present a new diversified ranking measure, which can capture both relevance and diversity. Secondly, we prove the submodularity of the proposed measure. And then we propose an efficient greedy algorithm with linear time and space complexity with respect to the size of the graph to achieve near-optimal diversified ranking. Finally, we evaluate the proposed method through extensive experiments on four real networks. The experimental results indicate that the proposed method outperforms existing diversified ranking algorithms both on improving diversity in ranking and the efficiency of the algorithms.
[Algorithm design and analysis, Greedy algorithms, Measurement, Social network services, graph theory, data mining, large graphs, information retrieval, retrieval task, diversity enhancement, Complexity theory, diversified ranking algorithms, top-K diversified ranking list, linear space complexity, Diversity reception, Collaboration, social network analysis, social networking (online), scalable diversified ranking, mining task, Pagerank, vertex selection algorithm, linear time complexity, computational complexity]
Web Horror Image Recognition Based on Context-Aware Multi-instance Learning
2011 IEEE 11th International Conference on Data Mining
None
2011
Along with the ever-growing Web, horror contents sharing in the Internet has interfered with our daily life and affected our, especially children's, health. Therefore horror image recognition is becoming more important for web objectionable content filtering. This paper presents a novel context-aware multi-instance learning (CMIL) model for this task. This work is distinguished by three key contributions. Firstly, the traditional multi-instance learning is extended to context-aware multi-instance learning model through integrating an undirected graph in each bag that represents contextual relationships among instances. Secondly, by introducing a novel energy function, a heuristic optimization algorithm based on Fuzzy Support Vector Machine (FSVM) is given out to find the optimal classifier on CMIL. Finally, the CMIL is applied to recognize horror images. Experimental results on an image set collected from the Internet show that the proposed method is effective on horror image recognition.
[Image recognition, Horror Image Recognition, graph theory, fuzzy set theory, Web objectionable content filtering, Web horror image recognition, ubiquitous computing, Context-aware Multi-Instance Learning, Optimization, CMIL, Image color analysis, Mathematical model, undirected graph, context aware multiinstance learning, support vector machines, FSVM, Image Emotion, Equations, Support vector machines, heuristic optimization algorithm, fuzzy support vector machine, computer aided instruction, Internet, image recognition, Context modeling]
Mixture of Softmax sLDA
2011 IEEE 11th International Conference on Data Mining
None
2011
In this paper, we propose a new variant of supervised Latent Dirichlet Allocation(sLDA): mixture of soft max sLDA, for image classification. Ensemble classification methods can combine multiple weak classifiers to construct a strong classifier. Inspired by the ensemble idea, we try to improve sLDA model using the idea. The mixture of soft max model is a probabilistic ensemble classification model, it can fit the training data and class label well. We embed the mixture of soft max model into LDA model under the framwork of sLDA, and construct an ensemble supervised topic model for image classification. Meanwhile, we derive an elegant parameters estimation algorithm based on variational EM method, and give a simple and efficient approximation method for classifying a new image. Finally, we demonstrate the effectiveness of our model by comparing with some existing approaches on two real world datasets. The results show that our model enhances classification accuracy by 7% on the 1600-image Label Me dataset and 9% on the 1791-image UIUC-Sport dataset.
[supervised latent Dirichlet allocation, Parameter estimation, Computational modeling, image classification, 1600-image LabelMe dataset, probability, parameter estimation algorithm, Predictive models, softmax sLDA, probabilistic modeling, cation, Support vector machines, Accuracy, probabilistic ensemble classification model, feature extraction, supervised topic model, the mixture of softmax model, ensemble classi&#x0026;#64257, Feature extraction, parameter estimation, Data models, softmax model, 179-image UlUC-Sport dataset]
Optimizing Performance Measures for Feature Selection
2011 IEEE 11th International Conference on Data Mining
None
2011
Feature selection with specific multivariate performance measures is the key to the success of many applications, such as information retrieval and bioinformatics. The existing feature selection methods are usually designed for classification error. In this paper, we present a unified feature selection framework for general loss functions. In particular, we study the novel feature selection paradigm by optimizing multivariate performance measures. The resultant formulation is a challenging problem for high-dimensional data. Hence, a two-layer cutting plane algorithm is proposed to solve this problem, and the convergence is presented. Extensive experiments on large-scale and high-dimensional real world datasets show that the proposed method outperforms l<sub>1</sub>-SVM and SVM-RFE when choosing a small subset of features, and achieves significantly improved performances over SVMperf in terms of F<sub>1</sub>-score.
[multivariate performance measure, information retrieval, Loss measurement, Vectors, multivariate performance measures, Accuracy, optimisation, Atmospheric measurements, multiple kernel learning, bioinformatics, Approximation algorithms, Particle measurements, multivariate performance measure optimisation, structural SVMs, feature selection, Testing, classification error]
Detecting Recurring and Novel Classes in Concept-Drifting Data Streams
2011 IEEE 11th International Conference on Data Mining
None
2011
Concept-evolution is one of the major challenges in data stream classification, which occurs when a new class evolves in the stream. This problem remains unaddressed by most state-of-the-art techniques. A recurring class is a special case of concept-evolution. This special case takes place when a class appears in the stream, then disappears for a long time, and again appears. Existing data stream classification techniques that address the concept-evolution problem, wrongly detect the recurring classes as novel class. This creates two main problems. First, much resource is wasted in detecting a recurring class as novel class, because novel class detection is much more computationally- and memory-intensive, as compared to simply recognizing an existing class. Second, when a novel class is identified, human experts are involved in collecting and labeling the instances of that class for future modeling. If a recurrent class is reported as novel class, it will be only a waste of human effort to find out whether it is really a novel class. In this paper, we address the recurring issue, and propose a more realistic novel class detection technique, which remembers a class and identifies it as "not novel" when it reappears after a long disappearance. Our approach has shown significant reduction in classification error over state-of-the-art stream classification techniques on several benchmark data streams.
[pattern classification, Error analysis, Humans, recurring class, recurring detection, data stream classification techniques, stream classification, Training, Analytical models, novel class, Training data, concept-evolution, Data models, data handling, Copper, concept-drifting data streams]
Performances and Characteristics of DIGRank, Ranking in the Incomplete Networks
2011 IEEE 11th International Conference on Data Mining
None
2011
Page Rank has been widely used in ranking retrieval results on the web, finding the top influential papers in citation networks or detecting valuable users in online social networks. However, in practice, it is usually hard to obtain a complete structure of any above networks to rank nodes. Thus, some researchers have begun to explore how to get estimated ranks efficiently without acquiring the whole network. They have proposed some approximating methods, however, it is difficult to determine which method is the best one or which is suitable to a certain application. In this case, we set experiments in small-world and scale-free generated networks to certify the feasibility and characteristics of four approximating methods. We also use eleven real networks to mention different optimal conditions for these methods. We find the DIG Rank method performs better than other local estimation methods in almost every given sub graph. Besides, Mean field approach method tends to perform well in networks that have low average shortest path length, small amount of nodes with the same low in degree, or weak community structure. Finally, we apply the most versatile method DIG Rank to Sina micro-blog website to precisely classify users in a group as elites, grassroots or mummy users.
[DIGRank characteristics, Fans, approximation theory, Correlation, incomplete networks, Error analysis, Neodymium, Social network services, ranking retrieval, information retrieval, online social networks, PageRank, Small-world, Electronic mail, Accuracy, Scale-free, approximating methods, social networking (online), Internet, Online Social Network]
Cross-Temporal Link Prediction
2011 IEEE 11th International Conference on Data Mining
None
2011
The increasing interest in dynamically changing networks has led to growing interest in a more general link prediction problem called temporal link prediction in the data mining and machine learning communities. However, only links in identical time frames are considered in temporal link prediction. We propose a new link prediction problem called cross-temporal link prediction in which the links among nodes in different time frames are inferred. A typical example of cross-temporal link prediction is cross-temporal entity resolution to determine the identity of real entities represented by data objects observed in different time periods. In dynamic environments, the features of data change over time, making it difficult to identify cross-temporal links by directly comparing observed data. Other examples of cross-temporal links are asynchronous communications in social networks such as Face book and Twitter, where a message is posted in reply to a previous message. We adopt a dimension reduction approach to cross-temporal link prediction, that is, data objects in different time frames are mapped into a common low-dimensional latent feature space, and the links are identified on the basis of the distance between the data objects. The proposed method uses different low-dimensional feature projections in different time frames, enabling it to adapt to changes in the latent features over time. Using multi-task learning, it jointly learns a set of feature projection matrices from the training data, given the assumption of temporal smoothness of the projections. The optimal solutions are obtained by solving a single generalized eigenvalue problem. Experiments using a real-world set of bibliographic data for cross-temporal entity resolution showed that introducing time-dependent feature projections improves the accuracy of link prediction.
[data mining, asynchronous communications, temporal data, dimension reduction, Optimization, Training, Accuracy, Databases, feature extraction, Training data, low-dimensional feature projections, link prediction, Eigenvalues and eigenfunctions, learning (artificial intelligence), generalized eigenvalue problem, Social network services, cross-temporal link prediction, feature projection matrices, social networks, entity resolution, machine learning, matrix algebra, time-dependent feature projections, cross-temporal entity resolution, dimension reduction approach, social network analysis, social networking (online), low-dimensional latent feature space]
Helix: Unsupervised Grammar Induction for Structured Activity Recognition
2011 IEEE 11th International Conference on Data Mining
None
2011
The omnipresence of mobile sensors has brought tremendous opportunities to ubiquitous computing systems. In many natural settings, however, their broader applications are hindered by three main challenges: rarity of labels, uncertainty of activity granularities, and the difficulty of multi-dimensional sensor fusion. In this paper, we propose building a grammar to address all these challenges using a language-based approach. The proposed algorithm, called Helix, first generates an initial vocabulary using unlabeled sensor readings, followed by iteratively combining statistically collocated sub-activities across sensor dimensions and grouping similar activities together to discover higher level activities. The experiments using a 20-minute ping-pong game demonstrate favorable results compared to a Hierarchical Hidden Markov Model (HHMM) baseline. Closer investigations to the learned grammar also shows that the learned grammar captures the natural structure of the underlying activities.
[Context, Vocabulary, HHMM, unsupervised grammar induction, mobile sensors, natural language processing, unlabeled sensor readings, sensor fusion, Grammar, ubiquitous computing, Unsupervised Grammar Induction, Ubiquitous Knowledge Discovery, hidden Markov models, mobile computing, Semantics, activity granularities, Helix, multidimensional sensor fusion, Sensors, Hidden Markov Model, structured activity recognition, Joints, Mutual information, Heterogeneous Sensor Fusion]
Distance Preserving Graph Simplification
2011 IEEE 11th International Conference on Data Mining
None
2011
Large graphs are difficult to represent, visualize, and understand. In this paper, we introduce "gate graph" a new approach to perform graph simplification. A gate graph provides a simplified topological view of the original graph. Specifically, we construct a gate graph from a large graph so that for any "non-local" vertex pair (distance greater than some threshold) in the original graph, their shortest-path distance can be recovered by consecutive "local" walks through the gate vertices in the gate graph. We perform a theoretical investigation on the gate-vertex set discovery problem. We characterize its computational complexity and reveal the upper bound of minimum gate- vertex set using VC-dimension theory. We propose an efficient mining algorithm to discover a gate-vertex set with guaranteed logarithmic bound. The detailed experimental results using both real and synthetic graphs demonstrate the effectiveness and efficiency of our approach.
[Greedy algorithms, shortest-path distance, graph theory, local walks, Set Cover Problem, Educational institutions, gate-vertex set discovery problem, Complexity theory, set theory, Gate Vertices, Graph Simplification, gate graph, nonlocal vertex pair, Road transportation, Logic gates, VC-Dimension, Sampling methods, Approximation algorithms, Gate Graph, distance preserving graph simplification, VC-dimension theory, computational complexity]
Clustering with Attribute-Level Constraints
2011 IEEE 11th International Conference on Data Mining
None
2011
In many clustering applications the incorporation of background knowledge in the form of constraints is desirable. In this paper, we introduce a new constraint type and the corresponding clustering problem: attribute constrained clustering. The goal is to induce clusters of binary instances that satisfy constraints on the attribute level. These constraints specify whether instances may or may not be grouped to a cluster, depending on specific attribute values. We show how the well-established instance-level constraints, must-link and cannot-link, can be adapted to the attribute level. A variant of the k-Medoids algorithm taking into account attribute level constraints is evaluated on synthetic and real-world data. Experimental results show that such constraints may provide better clustering results at lower specification costs if constraints can be expressed on the attribute level.
[attribute-level constraint clustering, attribute level, Electronic mail, set theory, Data mining, Equations, Runtime, constraint satisfaction problems, Animals, pattern clustering, background knowledge, constraint satisfaction, Clustering algorithms, instance-level constraints, constraint handling, constrained clustering, k-Medoids algorithm]
A Fast and Flexible Clustering Algorithm Using Binary Discretization
2011 IEEE 11th International Conference on Data Mining
None
2011
We present in this paper a new clustering algorithm for multivariate data. This algorithm, called BOOL (Binary coding Oriented clustering), can detect arbitrarily shaped clusters and is noise tolerant. BOOL handles data using a two-step procedure: data points are first discretized and represented as binary words, clusters are then iteratively constructed by agglomerating smaller clusters using this representation. This latter step is carried out with linear complexity by sorting such binary representations, which results in dramatic speedups when compared with other techniques. Experiments show that BOOL is faster than K-means, and about two to three orders of magnitude faster than two state-of-the-art algorithms that can detect non-convex clusters of arbitrary shapes. We also show that BOOL's results are robust to changes in parameters, whereas most algorithms for arbitrarily shaped clusters are known to be overly sensitive to such changes. The key to the robustness of BOOL is the hierarchical structure of clusters that is introduced automatically by increasing the accuracy of the discretization.
[binary discretization, Conferences, linear complexity, Noise, data mining, knowledge discovery, Data mining, Hierarchical clustering, Discretization, multivariate data, learning (artificial intelligence), Shape-based clustering, clustering algorithm, data points, binary representations, Indexes, machine learning, nonconvex cluster detection, Sorting, Binary encoding, pattern clustering, BOOL, binary words, binary coding oriented clustering, data handling, computational complexity]
A New Multi-task Learning Method for Personalized Activity Recognition
2011 IEEE 11th International Conference on Data Mining
None
2011
Personalized activity recognition usually faces the problem of data sparseness. We aim at improving accuracy of personalized activity recognition by incorporating the information from other persons. We propose a new online multi-task learning method for personalized activity recognition. The proposed online multi-task learning method automatically learns the ``transfer-factors" (similarities) among different tasks (i.e., among different persons in our case). Experiments demonstrate that the proposed method significantly outperforms existing methods. The novelty of this paper is twofold: (1) A new multi-task learning framework, which can naturally learn similarities among tasks, (2) To our knowledge, this is the first study of large-scale personalized activity recognition.
[data sparseness, Vectors, transfer factor, Convergence, online multitask learning method, Training, Accuracy, gesture recognition, Polynomials, Acceleration, learning (artificial intelligence), Kernel, large scale personalized activity recognition]
Identities Anonymization in Dynamic Social Networks
2011 IEEE 11th International Conference on Data Mining
None
2011
Privacy in social network data publishing is always an important concern. Nowadays most prior privacy protection techniques focus on static social networks. However, there are additional privacy disclosures in dynamic social networks due to the sequential publications. In this paper, we first show that the risks of vertex and community re-identification exist in a dynamic social network, even if the release at each time instance is protected by a static anonymity scheme. To prevent vertex and community re-identification in a dynamic social network, we propose novel dynamic kw-structural diversity anonymity, where w is the time that an adversary can monitor a victim. This scheme extends the k-structural diversity anonymity to a dynamic scenario. We also present a heuristic to anonymize the releases of networks to satisfy the proposed privacy scheme. The evaluations show that our approach can retain much of the characteristics of the networks while confirming the privacy protection.
[Social network services, Heuristic algorithms, Communities, identities anonymization, privacy, Cultural differences, privacy protection techniques, Diseases, static social networks, sequential publications, static anonymity scheme, social network, Privacy, anonymization, Publishing, social network data publishing, dynamic kw-structural diversity anonymity, social networking (online), data privacy, dynamic social networks, dynamic, community reidentification]
Discovering Emerging Topics in Social Streams via Link Anomaly Detection
2011 IEEE 11th International Conference on Data Mining
None
2011
Detection of emerging topics are now receiving renewed interest motivated by the rapid growth of social networks. Conventional term-frequency-based approaches may not be appropriate in this context, because the information exchanged are not only texts but also images, URLs, and videos. We focus on the social aspects of theses networks. That is, the links between users that are generated dynamically intentionally or unintentionally through replies, mentions, and retweets. We propose a probability model of the mentioning behaviour of a social network user, and propose to detect the emergence of a new topic from the anomaly measured through the model. We combine the proposed mention anomaly score with a recently proposed change-point detection technique based on the Sequentially Discounting Normalized Maximum Likelihood (SDNML), or with Kleinberg's burst model. Aggregating anomaly scores from hundreds of users, we show that we can detect emerging topics only based on the reply/mention relationships in social network posts. We demonstrate our technique in a number of real data sets we gathered from Twitter. The experiments show that the proposed mention-anomaly-based approaches can detect new topics at least as early as the conventional term-frequency-based approach, and sometimes much earlier when the keyword is ill-defined.
[Maximum likelihood detection, link anomaly detection, probability model, emerging topic discovery, Sequentially Discounted Maximum Likelihood Coding, data mining, probability, Topic Detection, social network user behaviour, sequentially discounting normalized maximum likelihood, Twitter, Encoding, Burst detection, Training, Anomaly Detection, security of data, Kleinberg burst model, Social Networks, Hidden Markov models, term-frequency-based approach, Density functional theory, social networking (online), social streams, change-point detection technique]
Finding Communities in Dynamic Social Networks
2011 IEEE 11th International Conference on Data Mining
None
2011
Communities are natural structures observed in social networks and are usually characterized as "relatively dense" subsets of nodes. Social networks change over time and so do the underlying community structures. Thus, to truly uncover this structure we must take the temporal aspect of networks into consideration. Previously, we have represented framework for finding dynamic communities using the social cost model and formulated the corresponding optimization problem [33], assuming that partitions of individuals into groups are given in each time step. We have also presented heuristics and approximation algorithms for the problem, with the same assumption [32]. In general, however, dynamic social networks are represented as a sequence of graphs of snapshots of the social network and the assumption that we have partitions of individuals into groups does not hold. In this paper, we extend the social cost model and formulate an optimization problem of finding community structure from the sequence of arbitrary graphs. We propose a semi definite programming formulation and a heuristic rounding scheme. We show, using synthetic data sets, that this method is quite accurate on synthetic data sets and present its results on a real social network.
[social cost model, approximation theory, optimization problem, Heuristic algorithms, Social network services, Communities, graph theory, Vectors, Partitioning algorithms, heuristic rounding scheme, Approximation methods, sequences, semidefinite programming, optimisation, semideflnite programming formulation, graph sequence, community structure, approximation algorithm, Approximation algorithms, social networking (online), dynamic social networks, data handling, synthetic data sets]
Review Graph Based Online Store Review Spammer Detection
2011 IEEE 11th International Conference on Data Mining
None
2011
Online reviews provide valuable information about products and services to consumers. However, spammers are joining the community trying to mislead readers by writing fake reviews. Previous attempts for spammer detection used reviewers' behaviors, text similarity, linguistics features and rating patterns. Those studies are able to identify certain types of spammers, e.g., those who post many similar reviews about one target entity. However, in reality, there are other kinds of spammers who can manipulate their behaviors to act just like genuine reviewers, and thus cannot be detected by the available techniques. In this paper, we propose a novel concept of a heterogeneous review graph to capture the relationships among reviewers, reviews and stores that the reviewers have reviewed. We explore how interactions between nodes in this graph can reveal the cause of spam and propose an iterative model to identify suspicious reviewers. This is the first time such intricate relationships have been identified for review spam detection. We also develop an effective computation method to quantify the trustiness of reviewers, the honesty of reviews, and the reliability of stores. Different from existing approaches, we don't use review text information. Our model is thus complementary to existing approaches and able to find more difficult and subtle spamming activities, which are agreed upon by human judges after they evaluate our results.
[Computational modeling, Humans, Companies, online store review, unsolicited e-mail, spammer detection, Data mining, iterative model, graphs, heterogeneous review graph, reviews, Spammer detection, Writing, text information review, Reliability, retail data processing, review graph]
Classifying Categorical Data by Rule-Based Neighbors
2011 IEEE 11th International Conference on Data Mining
None
2011
A new learning algorithm for categorical data, named CRN (Classification by Rule-based Neighbors) is proposed in this paper. CRN is a nonmetric and parameter-free classifier, and can be regarded as a hybrid of rule induction and instance-based learning. Based on a new measure of attributes quality and the separate-and-conquer strategy, CRN learns a collection of feature sets such that for each pair of instances belonging to different classes, there is a feature set on which the two instances disagree. For an unlabeled instance I and a labeled instance I', I' is a neighbor of I if and only if they agree on all attributes of a feature set. Then, CRN classifies an unlabeled instance I based on I's neighbors on those learned feature sets. To validate the performance of CRN, CRN is compared with six state-of-the-art classifiers on twenty-four datasets. Experimental results demonstrate that although the underlying idea of CRN is simple, the predictive accuracy of CRN is comparable to or better than that of the state-of-the-art classifiers on most datasets.
[Measurement, pattern classification, learning algorithm, feature sets, Entropy, Classification algorithms, classification, nonmetric classifier, instance-based learning, rule induction, separate-and-conquer strategy, Training, Accuracy, Impurities, knowledge based systems, parameter-free classifier, categorical data, rule-based neighbors, data handling, learning (artificial intelligence), categorical data classification, feature selection]
Tensor Fold-in Algorithms for Social Tagging Prediction
2011 IEEE 11th International Conference on Data Mining
None
2011
Social tagging predictions involve the co occurrence of users, items and tags. The tremendous growth of users require the recommender system to produce tag recommendations for millions of users and items at any minute. The triplets of users, items and tags are most naturally described by a 3D tensor, and tensor decomposition-based algorithms can produce high quality recommendations. However, each day, thousands of new users are added to the system and the decompositions must be updated daily in a online fashion. In this paper, we provide analysis of the new user problem, and present fold-in algorithms for Tucker, Para Fac, and Low-order tensor decompositions. We show that these algorithm can very efficiently compute the needed decompositions. We evaluate the fold-in algorithms experimentally on several datasets and the results demonstrate the effectiveness of these algorithms.
[Algorithm design and analysis, Graph Mining, 3D tensor decomposition based algorithm, ParaFac decomposition, data mining, social tagging prediction, Predictive models, tensors, Matrix decomposition, Social Network, Tensile stress, Accuracy, recommender systems, recommender system, low-order tensor decomposition, Tagging, Prediction algorithms, social networking (online), tag recommendation, Recommender System, tensor fold-in algorithm, Tucker decomposition, solid modelling]
Discovering Thematic Patterns in Videos via Cohesive Sub-graph Mining
2011 IEEE 11th International Conference on Data Mining
None
2011
One category of videos usually contains the same thematic pattern, e.g., the spin action in skating videos. The discovery of the thematic pattern is essential to understand and summarize the video contents. This paper addresses two critical issues in mining thematic video patterns: (1) automatic discovery of thematic patterns without any training or supervision information, and (2) accurate localization of the occurrences of all thematic patterns in videos. The major contributions are two-fold. First, we formulate the thematic video pattern discovery as a cohesive sub-graph selection problem by finding a sub-set of visual words that are spatio-temporally collocated. Then spatio-temporal branch-and-bound search can locate all instances accurately. Second, a novel method is proposed to efficiently find the cohesive sub-graph of maximum overall mutual information scores. Our experimental results on challenging commercial and action videos show that our approach can discover different types of thematic patterns despite variations in scale, view-point, color and lighting conditions, or partial occlusions. Our approach is also robust to the videos with cluttered and dynamic backgrounds.
[Visualization, Video sequences, graph theory, data mining, Vectors, Data mining, hidden feature removal, overall mutual information score, Videos, visual word subset, automatic discovery, video contents, thematic pattern, partial occlusion, cohesive subgraph mining, unsupervised, Feature extraction, spatiotemporal branch and bound search, cohesive subgraph selection problem, video signal processing, thematic video pattern discovery, Pattern matching, search problems, lighting condition]
Low Rank Metric Learning with Manifold Regularization
2011 IEEE 11th International Conference on Data Mining
None
2011
In this paper, we present a semi-supervised method to learn a low rank Mahalanobis distance function. Based on an approximation to the projection distance from a manifold, we propose a novel parametric manifold regularizer. In contrast to previous approaches that usually exploit side information only, our proposed method can further take advantages of the intrinsic manifold information from data. In addition, we focus on learning a metric of low rank directly, this is different from traditional approaches that often enforce the l<sub>1</sub> norm on the metric. The resulting configuration is convex with respect to the manifold structure and the distance function, respectively. We solve it with an alternating optimization algorithm, which proves effective to find a satisfactory solution. For efficient implementation, we even present a fast algorithm, in which the manifold structure and the distance function are learned independently without alternating minimization. Experimental results over 12 standard UCI data sets demonstrate the advantages of our method.
[fixed-point iterative algorithm, approximation theory, iterative methods, parametric manifold regularizer, low rank, optimization algorithm, intrinsic manifold information, linearly constrained nuclear norm minimization, Optimization, Manifolds, Training, manifold regularization, optimisation, low rank Mahalanobis distance function, Semi-supervised metric learning, Euclidean distance, Eigenvalues and eigenfunctions, Iterative methods, learning (artificial intelligence), low rank metric learning, semisupervised method]
A Study of Laplacian Spectra of Graph for Subgraph Queries
2011 IEEE 11th International Conference on Data Mining
None
2011
The spectrum of graph has been widely used in graph mining to extract graph topological information. It has also been employed as a characteristic of graph to check the sub graph isomorphism testing since it is an invariant of a graph. However, the spectrum cannot be directly applied to a graph and its sub graph, which is a bottleneck for sub graph isomorphism testing. In this paper, we study the Laplacian spectra between a graph and its sub graph, and propose a method by straightforward adoption of them for sub graph queries. In our proposed method, we first encode every vertex and graph by extracting their Laplacian spectra, and generate a novel two-step filtering conditions. Then, we follow the filtering-and verification framework to conduct sub graph queries. Extensive experiments show that, compared with existing counterpart method, as a graph feature, Laplacian spectra can be used to efficiently improves the efficiency of sub graph queries and thus indicate that it have considerable potential.
[Laplace equations, Filtering, two-step filtering condition, subgraph queries, graph theory, data mining, Laplacian spectra of graph, information filtering, Indexes, Data mining, flltering and veriflcation framework, query processing, graph mining, subgraph isomorphism testing, graph feature, graph topological information extraction, Feature extraction, graph spectrum, Laplacian spectra, Eigenvalues and eigenfunctions]
Text Clustering via Constrained Nonnegative Matrix Factorization
2011 IEEE 11th International Conference on Data Mining
None
2011
Semi-supervised nonnegative matrix factorization (NMF)receives more and more attention in text mining field. The semi-supervised NMF methods can be divided into two types, one is based on the explicit category labels, the other is based on the pair wise constraints including must-link and cannot-link. As it is hard to obtain the category labels in some tasks, the latter one is more widely used in real applications. To date, all the constrained NMF methods treat the must-link and cannot-link constraints in a same way. However, these two kinds of constraints play different roles in NMF clustering. Thus a novel constrained NMF method is proposed in this paper. In the new method, must-link constraints are used to control the distance of the data in the compressed form, and cannot-ink constraints are used to control the encoding factor. Experimental results on real-world text data sets have shown the good performance of the proposed method.
[Gradient methods, text analysis, Clustering methods, text clustering, must link constraints, Vectors, Encoding, Complexity theory, matrix decomposition, Matrix decomposition, nonnegative matrix factorization, cannot link constraints, pairwise constraints, constrained nonnegative matrix factorization, Data models, text mining, semi-supervised clustering]
[Publishers information]
2011 IEEE 11th International Conference on Data Mining
None
2011
Provides a listing of current committee members and society officers.
[]
Message from General Co-chairs
2012 IEEE 12th International Conference on Data Mining
None
2012
This is the twelfth annual IEEE International Conference on Data Mining. It has grown in size and stature, and is considered today as a premier international research conference on data mining. After San Jose, USA (2001), Maebashi City, Japan (2001), Melbourne, USA (2003), Brighton, UK (2004), Houston, USA (2005), Hong Kong, China (2006), Omaha, USA (2007), Pisa, Italy (2008), Miami, USA (2009), Sydney, Australia (2010) and Vancouver, Canada (2011) it is fitting that ICDM return to Europe.
[]
Message from Program Co-chairs
2012 IEEE 12th International Conference on Data Mining
None
2012
The ICDM conference is truly an international forum. During its twelve-year history, the conference has been held in nine countries around the world. This year's conference continues this global trend: our organizing and program committee members represent 31 countries and authors submitted papers from 54 different countries. This year's conference was extremely competitive. A total of 756 papers were submitted for review. Each paper was reviewed by at least three program committee members and the selection was made on the basis of discussion among the reviewers, a vice chair, and the program co-chairs. This year, 81 regular papers and 70 short papers were accepted for presentation, representing an acceptance rate of 19.97%. In keeping with the goal of advancing the state-of-the-art in data mining, paper topics span numerous active and emerging topic areas including feature analysis, classification, privacy, anomaly detection, semi-supervised learning, clustering, recommendations, time series mining, sparse representations, data summarization, and mining data found in graphs, video, images, and text.
[]
Conference Organization
2012 IEEE 12th International Conference on Data Mining
None
2012
Provides a listing of current committee members and society officers.
[]
Keynotes [3 abstracts]
2012 IEEE 12th International Conference on Data Mining
None
2012
Provides an abstract for each of the three keynote presentations and a brief professional biography of each presenter.
[]
Tutorial i: Outlier detection in high dimensional data
2012 IEEE 12th International Conference on Data Mining
None
2012
Summary form only given, as follows. High dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term "curse of dimensionality\
[]
Differentially Private Histogram Publishing through Lossy Compression
2012 IEEE 12th International Conference on Data Mining
None
2012
Differential privacy has emerged as one of the most promising privacy models for private data release. It can be used to release different types of data, and, in particular, histograms, which provide useful summaries of a dataset. Several differentially private histogram releasing schemes have been proposed recently. However, most of them directly add noise to the histogram counts, resulting in undesirable accuracy. In this paper, we propose two sanitization techniques that exploit the inherent redundancy of real-life datasets in order to boost the accuracy of histograms. They lossily compress the data and sanitize the compressed data. Our first scheme is an optimization of the Fourier Perturbation Algorithm (FPA) presented in [13]. It improves the accuracy of the initial FPA by a factor of 10. The other scheme relies on clustering and exploits the redundancy between bins. Our extensive experimental evaluation over various real-life and synthetic datasets demonstrates that our techniques preserve very accurate distributions and considerably improve the accuracy of range queries over attributed histograms.
[Data privacy, perturbation techniques, Noise, Fourier transform, Histograms, Privacy, differentially private histogram releasing schemes, Databases, publishing, synthetic datasets, Fourier perturbation algorithm, data compression, Differential privacy, lossy compression, Discrete Fourier transforms, differentially private histogram publishing, compressed data sanitization techniques, Fourier analysis, attributed histograms, histogram, Sensitivity, pattern clustering, FPA, real-life datasets, differential privacy models, data privacy, clustering, real-life dataset inherent redundancy]
Spotting Culprits in Epidemics: How Many and Which Ones?
2012 IEEE 12th International Conference on Data Mining
None
2012
Given a snapshot of a large graph, in which an infection has been spreading for some time, can we identify those nodes from which the infection started to spread? In other words, can we reliably tell who the culprits are? In this paper we answer this question affirmatively, and give an efficient method called NETSLEUTH for the well-known Susceptible-Infected virus propagation model. Essentially, we are after that set of seed nodes that best explain the given snapshot. We propose to employ the Minimum Description Length principle to identify the best set of seed nodes and virus propagation ripple, as the one by which we can most succinctly describe the infected graph. We give an highly efficient algorithm to identify likely sets of seed nodes given a snapshot. Then, given these seed nodes, we show we can optimize the virus propagation ripple in a principled way by maximizing likelihood. With all three combined, NETSLEUTH can automatically identify the correct number of seed nodes, as well as which nodes are the culprits. Experimentation on our method shows high accuracy in the detection of seed nodes, in addition to the correct automatic identification of their number. Moreover, we show NETSLEUTH scales linearly in the number of nodes of the graph.
[infection, seed nodes, diffusion, virus propagation ripple, graph theory, seeds, graph snapshot, Encoding, Equations, NETSLEUTH, epidemics, infected graph, culprits, microorganisms, minimum description length principle, susceptible-infected virus propagation model, Silicon, Data models, Nickel, Reliability, Integrated circuit modeling, culprit spotting]
Stream Classification with Recurring and Novel Class Detection Using Class-Based Ensemble
2012 IEEE 12th International Conference on Data Mining
None
2012
Concept-evolution has recently received a lot of attention in the context of mining data streams. Concept-evolution occurs when a new class evolves in the stream. Although many recent studies address this issue, most of them do not consider the scenario of recurring classes in the stream. A class is called recurring if it appears in the stream, disappears for a while, and then reappears again. Existing data stream classification techniques either misclassify the recurring class instances as another class, or falsely identify the recurring classes as novel. This increases the prediction error of the classifiers, and in some cases causes unnecessary waste in memory and computational resources. In this paper we address the recurring class issue by proposing a novel "class-based" ensemble technique, which substitutes the traditional "chunk-based" ensemble approaches and correctly distinguishes between a recurring class and a novel one. We analytically and experimentally confirm the superiority of our method over state-of-the-art techniques.
[pattern classification, classifier prediction error, chunk- based ensemble approaches, class-based ensemble technique, concept evolution, Humans, data mining, memory waste, recurring class, Educational institutions, recurring class detection, Electronic mail, Classification algorithms, novel class detection, stream classification, data stream classification techniques, Training, class-based ensemble, data stream mining, computational resources, state-of-the-art techniques, novel class, Prediction algorithms, Data models]
Feature Weighting and Selection Using Hypothesis Margin of Boosting
2012 IEEE 12th International Conference on Data Mining
None
2012
Utilizing the concept of hypothesis margins to measure the quality of a set of features has been a growing line of research in the last decade. However, most previous algorithms have been developed under the large hypothesis margin principles of the 1-NN algorithm, such as Simba. Little attention has been paid so far to exploiting the hypothesis margins of boosting to evaluate features. Boosting is well known to maximize the training examples' hypothesis margins, in particular, the average margins which are known to be the first statistics that considers the whole margin distribution. In this paper, we describe how to utilize the training examples' mean margins of boosting to select features. A weight criterion, termed Margin Fraction (MF), is assigned to each feature that contributes to the average margin distribution combined in the final output produced by boosting. Applying the idea of MF to a sequential backward selection method, a new embedded selection algorithm is proposed, called SBS-MF. Experimentation is carried out using different data sets, which compares the proposed SBS-MF with two boosting based feature selection approaches, as well as to Simba. The results show that SBS-MF is effective in most of the cases.
[Computers, embedded selection algorithm, Additives, 1-NN algorithm, Feature selection, Predictive models, SBS-MF, feature weighting, Training, weight criterion, MF, learning (artificial intelligence), boosting hypothesis margin, average margin distribution, Weight measurement, boosting, Boosting, Educational institutions, margin fraction, margin distribution, sequential backward selection method, boosting based feature selection approach, average margin, Simba, training example mean margins, statistics]
GPU-Accelerated Feature Selection for Outlier Detection Using the Local Kernel Density Ratio
2012 IEEE 12th International Conference on Data Mining
None
2012
Effective outlier detection requires the data to be described by a set of features that captures the behavior of normal data while emphasizing those characteristics of outliers which make them different than normal data. In this work, we present a novel non-parametric evaluation criterion for filter-based feature selection which caters to outlier detection problems. The proposed method seeks the subset of features that represents the inherent characteristics of the normal dataset while forcing outliers to stand out, making them more easily distinguished by outlier detection algorithms. Experimental results on real datasets show the advantage of our feature selection algorithm compared to popular and state-of-the-art methods. We also show that the proposed algorithm is able to overcome the small sample space problem and perform well on highly imbalanced datasets. Furthermore, due to the highly parallelizable nature of the feature selection, we implement the algorithm on a graphics processing unit (GPU) to gain significant speedup over the serial version. The benefits of the GPU implementation are two-fold, as its performance scales very well in terms of the number of features, as well as the number of data points.
[filter-based feature selection, imbalanced datasets, Graphics processing units, data mining, GPU-accelerated feature selection, Search problems, Educational institutions, outlier detection problems, graphics processing units, Feature Selection, normal data, GPU Acceleration, USA Councils, nonparametric evaluation criterion, graphics processing unit, Feature extraction, Outlier Detection, local kernel density ratio, Kernel, Detection algorithms, Imbalanced Data, small sample space problem]
Sequential Alternating Proximal Method for Scalable Sparse Structural SVMs
2012 IEEE 12th International Conference on Data Mining
None
2012
Structural Support Vector Machines (SSVMs) have recently gained wide prominence in classifying structured and complex objects like parse-trees, image segments and Part-of-Speech (POS) tags. Typical learning algorithms used in training SSVMs result in model parameters which are vectors residing in a large-dimensional feature space. Such a high-dimensional model parameter vector contains many non-zero components which often lead to slow prediction and storage issues. Hence there is a need for sparse parameter vectors which contain a very small number of non-zero components. L1-regularizer and elastic net regularizer have been traditionally used to get sparse model parameters. Though L1-regularized structural SVMs have been studied in the past, the use of elastic net regularizer for structural SVMs has not been explored yet. In this work, we formulate the elastic net SSVM and propose a sequential alternating proximal algorithm to solve the dual formulation. We compare the proposed method with existing methods for L1-regularized Structural SVMs. Experiments on large-scale benchmark datasets show that the proposed dual elastic net SSVM trained using the sequential alternating proximal algorithm scales well and results in highly sparse model parameters while achieving a comparable generalization performance. Hence the proposed sequential alternating proximal algorithm is a competitive method to achieve sparse model parameters and a comparable generalization performance when elastic net regularized Structural SVMs are used on very large datasets.
[complex object classification, Scalability, scalable sparse structural SVM, large-dimensional feature space, dual elastic net SSVM, Electronic mail, Structural SVMs, structured object classification, sparse model parameters, sequential alternating proximal method, L1-regularizer, learning (artificial intelligence), Context, pattern classification, support vector machines, learning algorithms, high-dimensional model parameter vector, Alternating Proximal method, Vectors, Computer science, sparse parameter vectors, elastic net regularizer, Support vector machine classification, structural support vector machines, nonzero components]
Computational Television Advertising
2012 IEEE 12th International Conference on Data Mining
None
2012
Ever wonder why that Kia Ad ran during Iron Chef? Traditional advertising methodology on television is a fascinating mix of marketing, branding, measurement, and predictive modeling. While still a robust business, it is at risk with the recent growth of online and time-shifted (recorded) television. A particular issue is that traditional methods for television advertising are far less efficient than their counterparts in the online world which employ highly sophisticated computational techniques. This paper formalizes an approach to eliminate some of these inefficiencies by recasting the process of television advertising media campaign generation in a computational framework. We describe efficient mathematical approaches to solve for the task of finding optimal campaigns for specific target audiences. In two case studies, our campaigns report gains in key operational metrics of up to 56% compared to campaigns generated by traditional methods.
[Measurement, TV, branding, Predictive models, advertising, Optimization, measurement, Media Campaign Generation, time-shifted television, television advertising media campaign generation, mathematical approaches, operational metrics, Advertising, Computational Advertising, computational television advertising, Computational modeling, television, Media, mathematical analysis, Iron Chef, Kia ad, marketing, online television, predictive modeling, Television Advertising, computational techniques]
Topic-Aware Social Influence Propagation Models
2012 IEEE 12th International Conference on Data Mining
None
2012
We study social influence from a topic modeling perspective. We introduce novel topic-aware influence-driven propagation models that experimentally result to be more accurate in describing real-world cascades than the standard propagation models studied in the literature. In particular, we first propose simple topic-aware extensions of the well-known Independent Cascade and Linear Threshold models. Next, we propose a different approach explicitly modeling authoritativeness, influence and relevance under a topic-aware perspective. We devise methods to learn the parameters of the models from a dataset of past propagations. Our experimentation confirms the high accuracy of the proposed models and learning schemes.
[Greedy algorithms, Atmospheric modeling, Computational modeling, Social network services, independent cascade model, learning scheme, modeling authoritativeness, topic modeling perspective, social sciences, influence-driven propagation model, Data models, topic-aware social influence propagation model, linear threshold model, learning (artificial intelligence), Integrated circuit modeling]
GUISE: Uniform Sampling of Graphlets for Large Graph Analysis
2012 IEEE 12th International Conference on Data Mining
None
2012
Graphlet frequency distribution (GFD) has recently become popular for characterizing large networks. However, the computation of GFD for a network requires the exact count of embedded graphlets in that network, which is a computationally expensive task. As a result, it is practically infeasible to compute the GFD for even a moderately large network. In this paper, we propose GUISE, which uses a Markov Chain Monte Carlo (MCMC) sampling method for constructing the approximate GFD of a large network. Our experiments on networks with millions of nodes show that GUISE obtains the GFD within few minutes, whereas the exhaustive counting based approach takes several days.
[Context, sampling methods, computationally expensive task, Radiation detectors, GFD, graph theory, Markov chain Monte Carlo sampling method, MCMC, Vectors, Probability distribution, Biological information theory, uniform graphlet sampling, Monte Carlo methods, large graph analysis, Markov processes, embedded graphlets, GUISE, graphlet frequency distribution]
Hierarchical Multilabel Classification with Minimum Bayes Risk
2012 IEEE 12th International Conference on Data Mining
None
2012
Hierarchical multilabel classification (HMC) allows an instance to have multiple labels residing in a hierarchy. A popular loss function used in HMC is the H-loss, which penalizes only the first classification mistake along each prediction path. However, the H-loss metric can only be used on tree-structured label hierarchies, but not on DAG hierarchies. Moreover, it may lead to misleading predictions as not all misclassifications in the hierarchy are penalized. In this paper, we overcome these deficiencies by proposing a hierarchy-aware loss function that is more appropriate for HMC. Using Bayesian decision theory, we then develop a Bayes-optimal classifier with respect to this loss function. Instead of requiring an exhaustive summation and search for the optimal multilabel, the proposed classification problem can be efficiently solved using a greedy algorithm on both tree-and DAG-structured label hierarchies. Experimental results on a large number of real-world data sets show that the proposed algorithm outperforms existing HMC methods.
[Greedy algorithms, pattern classification, Bayes-optimal classifier, hierarchical multilabel classification, decision theory, greedy algorithms, hierarchical classification, hierarchy-aware loss function, Bayesian decision theory, DAG-structured label hierarchy, Optimization, minimum Bayes risk, Support vector machines, H-loss metric, Bayesian methods, Decision theory, optimal multilabel, multilabel classification, Bismuth, Prediction algorithms, Bayes methods, tree-structured label hierarchy, greedy algorithm]
The Mixture of Multi-kernel Relevance Vector Machines Model
2012 IEEE 12th International Conference on Data Mining
None
2012
We present a new regression mixture model where each mixture component is a multi-kernel version of the Relevance Vector Machine (RVM). In the proposed model, we exploit the enhanced modeling capability of RVMs due to their embedded sparsity enforcing properties. %The main contribution of this %work is the employment of RVM models as components of a mixture %model and their application to the time series clustering problem. Moreover, robustness is achieved with respect to the kernel parameters, by employing a weighted multi-kernel scheme. The mixture model is trained using the maximum a posteriori (MAP) approach, where the Expectation Maximization (EM) algorithm is applied offering closed form update equations for the model parameters. An incremental learning methodology is also presented to tackle the parameter initialization problem of the EM algorithm. The efficiency of the proposed mixture model is empirically demonstrated on the time series clustering problem using various artificial and real benchmark datasets and by performing comparisons with other regression mixture models.
[closed form update equation, regression analysis, multi-kernel, mixture model, incremental EM learning, RVM model, Training, kernel parameter, maximum-a-posteriori approach, Mathematical model, learning (artificial intelligence), Kernel, support vector machines, regression mixture model, time series, Vectors, MAP approach, sparse prior, Covariance matrix, parameter initialization problem, time series clustering problem, model parameter, sparsity enforcing property, pattern clustering, weighted multikernel scheme, Relevance Vector Machines, Hidden Markov models, multikernel relevance vector machines, expectation-maximisation algorithm, Data models, expectation maximization algorithm, incremental learning methodology, mixture models, EM algorithm]
Diffusion of Information in Social Networks: Is It All Local?
2012 IEEE 12th International Conference on Data Mining
None
2012
Recent studies on the diffusion of information in social networks have largely focused on models based on the influence of local friends. In this paper, we challenge the generalizability of this approach and revive theories introduced by social scientists in the context of diffusion of innovations to model user behavior. To this end, we study various diffusion models in two different online social networks, Digg and Twitter. We first evaluate the applicability of two representative local influence models and show that the behavior of most social networks users are not captured by these local models. Next, driven by theories introduced in the diffusion of innovations research, we introduce a novel diffusion model called Gaussian Logit Curve Model (GLCM) that models user behavior with respect to the behavior of the general population. Our analysis shows that GLCM captures user behavior significantly better than local models, especially in the context of Digg. Aiming to capture both the local and global signals, we introduce various hybrid models and evaluate them through statistical methods. Our methodology models each user separately, automatically determining which users are driven by their local relations and which users are better defined through adopter categories, therefore capturing the complexity of human behavior.
[innovation diffusion, local friends influence, hybrid models, behavioural sciences, online social networks, Twitter, Gaussian logit curve model, local influence models, social scientists, Sociology, diffusion of innovations, Mathematical model, information diffusion, gaussian logit curve, Technological innovation, Maximum likelihood estimation, social network users behavior, ?rth logistic regression, Digg, social networks, human behavior complexity, model user behavior, GLCM, Gaussian processes, social networking (online), innovation management, diffusion models, statistical methods, Logistics]
Efficient Pattern-Based Time Series Classification on GPU
2012 IEEE 12th International Conference on Data Mining
None
2012
Time series shapelet discovery algorithm finds subsequences from a set of time series for use as primitives for time series classification. This algorithm has drawn a lot of interest because of the interpretability of its results. However, computation requirements restrict the algorithm from dealing with large data sets and may limit its application in many domains. In this paper, we address this issue by redesigning the algorithm for implementation on highly parallel Graphics Process Units (GPUs). We investigate several concepts of GPU programming and propose a dynamic programming algorithm that is suitable for implementation on GPUs. Results show that the proposed GPU implementation significantly reduces the running time of the shapelet discovery algorithm. For example, on the largest sample dataset from the original authors, the running time is reduced from half a day to two minutes.
[pattern classification, GPU programming, Instruction sets, Heuristic algorithms, time series shapelet discovery algorithm, Time series analysis, Graphics processing units, Programming, dynamic programming, time series, Registers, pattern-based time series classification, Pattern-based Classification, graphics processing units, GPU, dynamic programming algorithm, Signal processing algorithms, Classification, Time Series]
Inferring the Root Cause in Road Traffic Anomalies
2012 IEEE 12th International Conference on Data Mining
None
2012
We propose a novel two-step mining and optimization framework for inferring the root cause of anomalies that appear in road traffic data. We model road traffic as a time-dependent flow on a network formed by partitioning a city into regions bounded by major roads. In the first step we identify link anomalies based on their deviation from their historical traffic profile. However, link anomalies on their own shed very little light on what caused them to be anomalous. In the second step we take a generative approach by modeling the flow in a network in terms of the origin-destination (OD) matrix which physically relates the latent flow between origin and destination and the observable flow on the links. The key insight is that instead of using all of link traffic as the observable vector we only use the link anomaly vector. By solving an L1 inverse problem we infer the routes (the origin-destination pairs) which gave rise to the link anomalies. Experiments on a very large GPS data set consisting on nearly eight hundred million data points demonstrate that we can discover routes which can clearly explain the appearance of link anomalies. The use of optimization techniques to explain observable anomalies in a generative fashion is, to the best of our knowledge, entirely novel.
[road traffic, gps data, Roads, data mining, GPS data set, Vectors, anomaly detection, Covariance matrix, Optimization, Global Positioning System, matrix algebra, vectors, road traffic anomalies, origin-destination matrix, optimization framework, time-dependent flow, Trajectory, two-step mining, link anomaly vector, Principal component analysis]
Student-t Based Robust Spatio-temporal Prediction
2012 IEEE 12th International Conference on Data Mining
None
2012
This paper describes an efficient and effective design of Robust Spatio-Temporal Prediction based on Student's t distribution, namely, St-RSTP, to provide estimations based on observations over spatio-temporal neighbors. The proposed St-RSTP is more resilient to outliers or other small departures from model assumptions than its ancestor, the Spatio-Temporal Random Effects (STRE) model. STRE is a state-of-the-art statistical model with linear order complexity for large scale processing. However, it assumes Gaussian observations, which has the well-known limitation of non-robustness. In our StRSTP design, the measurement error follows Student's t distribution, instead of a traditional Gaussian distribution. This design reduces the influence of outliers, improves prediction quality, and keeps the problem analytically intractable. We propose a novel approximate inference approach, which approximates the model into the form that separates the high dimensional latent variables into groups, and then estimates the posterior distributions of different groups of variables separately in the framework of Expectation Propagation. As a good property, our approximate approach degeneralizes to the standard STRE based prediction, when the degree of freedom of the Student's t distribution is set to infinite. Extensive experimental evaluations based on both simulation and real-life data sets demonstrated the robustness and the efficiency of our Student-t prediction model. The proposed approach provides critical functionality for stochastic processes on spatio-temporal data.
[approximate inference approach, Spatio-Temporal Process, Predictive models, statistical model, expectation propagation, Approximation methods, statistical distributions, high dimensional latent variable, Robustness, Mathematical model, STRE model, robust spatio-temporal prediction, Gaussian observation, Estimation, random processes, stochastic process, Vectors, spatio-temporal random effect, posterior distributions, inference mechanisms, Student-t distribution, Students t Distribution, Equations, St-RSTP, Expectation Propagation, measurement error, Gaussian processes, STRE based prediction, linear order complexity, large scale processing]
Efficient Kernel Clustering Using Random Fourier Features
2012 IEEE 12th International Conference on Data Mining
None
2012
Kernel clustering algorithms have the ability to capture the non-linear structure inherent in many real world data sets and thereby, achieve better clustering performance than Euclidean distance based clustering algorithms. However, their quadratic computational complexity renders them non-scalable to large data sets. In this paper, we employ random Fourier maps, originally proposed for large scale classification, to accelerate kernel clustering. The key idea behind the use of random Fourier maps for clustering is to project the data into a low-dimensional space where the inner product of the transformed data points approximates the kernel similarity between them. An efficient linear clustering algorithm can then be applied to the points in the transformed space. We also propose an improved scheme which uses the top singular vectors of the transformed data matrix to perform clustering, and yields a better approximation of kernel clustering under appropriate conditions. Our empirical studies demonstrate that the proposed schemes can be efficiently applied to large data sets containing millions of data points, while achieving accuracy similar to that achieved by state-of-the-art kernel clustering algorithms.
[pattern classification, Fourier transforms, random Fourier maps, random Fourier features, Scalability, nonlinear structure, Vectors, quadratic computational complexity, Complexity theory, Approximation methods, transformed data matrix, large scale classification, Random Fourier features, Accuracy, pattern clustering, kernel clustering algorithms, Kernel clustering, Clustering algorithms, Kernel k-means, Approximation algorithms, Euclidean distance based clustering algorithms, real world data sets, Kernel, computational complexity]
Detecting Anomalies in Bipartite Graphs with Mutual Dependency Principles
2012 IEEE 12th International Conference on Data Mining
None
2012
Bipartite graphs can model many real life applications including users-rating-products in online marketplaces, users-clicking-webpages on the World Wide Web and users referring- users in social networks. In these graphs, the anomalousness of nodes in one partite often depends on that of their connected nodes in the other partite. Previous studies have shown that this dependency can be positive (the anomalousness of a node in one partite increases or decreases along with that of its connected nodes in the other partite) or negative (the anomalousness of a node in one partite rises or falls in opposite direction to that of its connected nodes in the other partite). In this paper, we unify both positive and negative mutual dependency relationships in an unsupervised framework for detecting anomalous nodes in bipartite graphs. This is the first work that integrates both mutual dependency principles to model the complete set of anomalous behaviors of nodes that cannot be identified by either principle alone. We formulate our principles and design an iterative algorithm to simultaneously compute the anomaly scores of nodes in both partites. Moreover, we mathematically prove that the ranking of nodes by anomaly scores in each partite converges. Our framework is examined on synthetic graphs and the results show that our model outperforms existing models with only positive or negative mutual dependency principles. We also apply our framework to two real life datasets: Goodreads as a users-rating-books setting and Buzzcity as a users-clicking advertisements setting. The results show that our method is able to detect suspected spamming users and spammed books in Goodreads and achieve higher precision in identifying fraudulent advertisement publishers than existing approaches.
[iterative methods, electronic publishing, users-clicking-webpages application, graph theory, synthetic graph, mutual dependency principle, World Wide Web, anomaly detection, advertising, Convergence, negative node dependency, unsupervised framework, online marketplace, Eigenvalues and eigenfunctions, Bipartite graph, Mathematical model, Anomaly Detection; Bipartite Graph; Mutual Dependency; Mutual Reinforcement; Node Anomalies, users-rating-products application, fraudulent advertisement publisher identification, Image edge detection, Computational modeling, Buzzcity setting, social networks, Vectors, bipartite graph, iterative algorithm, spammed books detection, security of data, positive node dependency, node ranking, Goodreads setting, suspected spamming user detection]
Link Prediction and Recommendation across Heterogeneous Social Networks
2012 IEEE 12th International Conference on Data Mining
None
2012
Link prediction and recommendation is a fundamental problem in social network analysis. The key challenge of link prediction comes from the sparsity of networks due to the strong disproportion of links that they have potential to form to links that do form. Most previous work tries to solve the problem in single network, few research focus on capturing the general principles of link formation across heterogeneous networks. In this work, we give a formal definition of link recommendation across heterogeneous networks. Then we propose a ranking factor graph model (RFG) for predicting links in social networks, which effectively improves the predictive performance. Motivated by the intuition that people make friends in different networks with similar principles, we find several social patterns that are general across heterogeneous networks. With the general social patterns, we develop a transfer-based RFG model that combines them with network structure information. This model provides us insight into fundamental principles that drive the link formation and network evolution. Finally, we verify the predictive performance of the presented transfer model on 12 pairs of transfer cases. Our experimental results demonstrate that the transfer of general social patterns indeed help the prediction of links.
[Link prediction, Correlation, Social network analysis, graph theory, network evolution, network structure information, Predictive models, network theory (graphs), Twitter, Indexes, Data mining, transfer-based RFG model, Recommendation, heterogeneous social networks, general social patterns, ranking factor graph model, Factor graph, link recommendation, link prediction, social network analysis, social networking (online), Data models, Heterogeneous networks, link formation principles]
Multi-task Semi-supervised Semantic Feature Learning for Classification
2012 IEEE 12th International Conference on Data Mining
None
2012
Multi-task learning has proven to be useful to boost the learning of multiple related but different tasks. Meanwhile, latent semantic models such as LSA and LDA are popular and effective methods to extract discriminative semantic features of high dimensional dyadic data. In this paper, we present a method to combine these two techniques together by introducing a new matrix tri-factorization based formulation for semi-supervised latent semantic learning, which can incorporate labeled information into traditional unsupervised learning of latent semantics. Our inspiration for multi-task semantic feature learning comes from two facts, i.e., 1) multiple tasks generally share a set of common latent semantics, and 2) a semantic usually has a stable indication of categories no matter which task it is from. Thus to make multiple tasks learn from each other we wish to share the associations between categories and those common semantics among tasks. Along this line, we propose a novel joint Nonnegative matrix tri-factorization framework with the aforesaid associations shared among tasks in the form of a semantic-category relation matrix. Our new formulation for multi-task learning can simultaneously learn (1) discriminative semantic features of each task, (2) predictive structure and categories of unlabeled data in each task, (3) common semantics shared among tasks and specific semantics exclusive to each task. We give alternating iterative algorithm to optimize our objective and theoretically show its convergence. Finally extensive experiments on text data along with the comparison with various baselines and three state-of-the-art multi-task learning algorithms demonstrate the effectiveness of our method.
[pattern classification, multi-task semi-supervised semantic feature learning, joint nonnegative matrix tri-factorization, matrix decomposition, text classification, Data mining, classification, discriminative semantic features, Optimization, semantic feature learning, latent semantic models, joint nonnegative matrix trifactorization framework, multi-task learning, Semantics, multiprogramming, Feature extraction, Data models, semantic-category relation matrix, semi-supervised learning, Iterative methods, learning (artificial intelligence), Joints, high dimensional dyadic data]
Robust Nonnegative Matrix Factorization via Half-Quadratic Minimization
2012 IEEE 12th International Conference on Data Mining
None
2012
Nonnegative matrix factorization (NMF) is a popular technique for learning parts-based representation and data clustering. It usually uses the squared residuals to quantify the quality of factorization, which is optimal specifically to zero-mean, Gaussian noise and sensitive to outliers in general cases. In this paper, we propose a robust NMF method based on the correntropy induced metric, which is much more insensitive to outliers. A half-quadratic optimization algorithm is developed to solve the proposed problem efficiently. The proposed method is further extended to handle outlier rows by incorporating structural knowledge about the outliers. Experimental results on data sets with and without apparent outliers demonstrate the effectiveness of the proposed algorithms.
[correntropy induced metric, zero mean, Computer integrated manufacturing, data clustering, Linear programming, Minimization, matrix decomposition, Matrix decomposition, parts based representation, Optimization, half quadratic optimization algorithm, Gaussian noise, robust non-negative matrix factorization, half quadratic minimization, half-quadratic optimization, Robustness, data handling, robust nonnegative matrix factorization, minimisation, Kernel, structural knowledge]
RankTopic: Ranking Based Topic Modeling
2012 IEEE 12th International Conference on Data Mining
None
2012
Topic modeling has become a widely used tool for document management due to its superior performance. However, there are few topic models distinguishing the importance of documents on different topics. In this paper, we investigate how to utilize the importance of documents to improve topic modeling and propose to incorporate link based ranking into topic modeling. Specifically, topical pagerank is used to compute the topic level ranking of documents, which indicates the importance of documents on different topics. By retreating the topical ranking of a document as the probability of the document involved in corresponding topic, a generalized relation is built between ranking and topic modeling. Based on the relation, a ranking based topic model Rank Topic is proposed. With Rank Topic, a mutual enhancement framework is established between ranking and topic modeling. Extensive experiments on paper citation data and Twitter data are conducted to compare the performance of Rank Topic with that of some state-of-the-art topic models. Experimental results show that Rank Topic performs much better than some baseline models and is comparable with the state-of-the-art link combined relational topic model (RTM) in generalization performance, document clustering and classification by setting a proper balancing parameter. It is also demonstrated in both quantitative and qualitative ways that topics detected by Rank Topic are more interpretable than those detected by some baseline models and still competitive with RTM.
[generalization performance, Twitter data, Noise, document clustering, document management, Ranking, document ranking, relational topic model, Classification, Mathematical model, paper citation data, balancing parameter, Document Network, document handling, pattern classification, generalized relation, mutual enhancement framework, Computational modeling, probability, Educational institutions, RankTopic tool, document classification, generalisation (artificial intelligence), topical pagerank, Clustering, Equations, Topic Modeling, pattern clustering, document importance, Web pages, Data models, ranking based topic modeling]
An Ellipsoidal K-Means for Document Clustering
2012 IEEE 12th International Conference on Data Mining
None
2012
We propose an extension of the spherical K-means algorithm to deal with settings where the number of data points is largely inferior to the number of dimensions. We assume the data to lie in local and dense regions of the original space and we propose to embed each cluster into its specific ellipsoid. A new objective function is introduced, analytical solutions are derived for both the centroids and the associated ellipsoids. Furthermore, a study on the complexity of this algorithm highlights that it is of same order as the regular K-means algorithm. Results on both synthetic and real data show the efficiency of the proposed method.
[document handling, document clustering, information retrieval, Linear programming, Vectors, ellipsoidal k-means, Partitioning algorithms, Ellipsoids, Tuning, ellipsoid, spherical k-means algorithm, pattern clustering, Clustering algorithms, Feature extraction, clustering, algorithm complexity, feature selection, computational complexity, spherical k-means]
Reconstructing Graphs from Neighborhood Data
2012 IEEE 12th International Conference on Data Mining
None
2012
Consider a social network and suppose that we are given the number of common friends between each pair of users. Can we reconstruct the underlying network? Similarly, consider a set of documents and the words that appear in them. If we know the number of common words for every pair of documents, as well as the number of common documents for every pair of words, can we infer which words appear in which documents? In this paper, we develop a general methodology for answering questions like the ones above. We formalize these questions in what we call the Reconstruct problem: Given information about the common neighbors of nodes in a network, our goal is to reconstruct the hidden binary matrix that indicates the presence or absence of relationships between individual nodes. We propose an effective and practical heuristic, which exploits properties of the singular value decomposition of the hidden binary matrix. More specifically, we show that using the available neighborhood information, we can reconstruct the hidden matrix by finding the components of its singular value decomposition and then combining them appropriately. Our extensive experimental study suggests that our methods are able to reconstruct binary matrices of different characteristics with up to 100% accuracy.
[Measurement, reconstruct problem, hidden binary matrix reconstruction, Symmetric matrices, neighborhood data, graph theory, documents, Vectors, Matrix decomposition, social network, neighborhood information, graph reconstruction, individual nodes, Motion pictures, Bipartite graph, data handling, singular value decomposition, Singular value decomposition]
Isometric Multi-manifold Learning for Feature Extraction
2012 IEEE 12th International Conference on Data Mining
None
2012
Manifold learning is an important topic in pattern recognition and computer vision. However, most manifold learning algorithms implicitly assume the data are aligned on a single manifold, which is too strict in actual applications. Isometric feature mapping (Isomap), as a promising manifold learning method, fails to work on data which distribute on clusters in a single manifold or manifolds. In this paper, we propose a new multi-manifold learning algorithm (M-Isomap). The algorithm first discovers the data manifolds and then reduces the dimensionality of the manifolds separately. Meanwhile, a skeleton representing the global structure of whole data set is built and kept in low-dimensional space. Secondly, by referring to the low-dimensional representation of the skeleton, the embeddings of the manifolds are relocated to a global coordinate system. Compared with previous methods, these algorithms can keep both of the intra and inter manifolds geodesics faithfully. The features and effectiveness of the proposed multi-manifold learning algorithms are demonstrated and compared through experiments.
[Algorithm design and analysis, isometric multimanifold learning, isomap, global data structure, Heuristic algorithms, Manifolds, skeleton, dimensionality reduction, feature extraction, intramanifolds geodesics, Clustering algorithms, differential geometry, isometric feature mapping, Skeleton, data structures, M-Isomap, multi-manifold learning, intermanifolds geodesics, pattern recognition, global coordinate system, geodesic distance, Vectors, multimanifold learning algorithm, computer vision, Approximation algorithms, manifold learning method, Feature extraction]
Graph-Oriented Learning via Automatic Group Sparsity for Data Analysis
2012 IEEE 12th International Conference on Data Mining
None
2012
The key task in graph-oriented learning is constructing an informative graph to model the geometrical and discriminant structure of a data manifold. Since traditional graph construction methods are sensitive to noise and less datum-adaptive to changes in density, a new graph construction method so-called &#x2113;1-Graph has been proposed [1] recently. A graph construction method needs to have two important properties: sparsity and locality. However, the &#x2113;1-Graph is strong in sparsity property, but weak in locality. In order to overcome such limitation, we propose a new method of constructing an informative graph using automatic group sparse regularization based on the work of &#x2113;1-Graph, which is called as group sparse graph (GroupSp-Graph). The newly developed GroupSp-Graph has the same noise-insensitive property as &#x2113;1-Graph, and also can successively preserve the group and local information in the graph. In other words, the proposed group sparse graph has both properties of sparsity and locality simultaneously. Furthermore, we integrate the proposed graph with several graph-oriented learning algorithms: spectral embedding, spectral clustering, subspace learning and manifold regularized non-negative matrix factorization. The empirical studies on benchmark data sets show that the proposed algorithms achieve considerable improvement over classic graph constructing methods and the &#x2113;1-Graph method in various learning task.
[Noise, graph theory, spectral embedding, manifold regularized nonnegative matrix factorization, &#x2113;1-graph, automatic group sparsity, matrix decomposition, Sparse matrices, data manifold, graph-oriented learning, Manifolds, geometrical structure, Clustering algorithms, graph learning, learning (artificial intelligence), sparsity property, GroupSp-graph, spectral clustering, sparse representation, non-negative matrix factoriza-tion, Laplace equations, informative graph, learning task, data analysis, group sparse graph, Educational institutions, Equations, noise-insensitive property, subspace learning, pattern clustering, discriminant structure, automatic group sparse regularization, geometry, graph construction]
Tuple MapReduce: Beyond Classic MapReduce
2012 IEEE 12th International Conference on Data Mining
None
2012
This paper proposes Tuple Map Reduce, a new foundational model extending Map Reduce with the notion of tuples. Tuple Map Reduce allows to bridge the gap between the low-level constructs provided by Map Reduce and higher-level needs required by programmers, such as compound records, sorting or joins. This paper presents as well Pangool, an open-source framework implementing Tuple Map Reduce. Pangool eases the design and implementation of applications based on Map Reduce and increases their flexibility, still maintaining Hadoop's performance.
[low-level constructs, Computational modeling, public domain software, Scalability, Companies, Hadoop, Programming, foundational model, Big Data, tuple MapReduce, Compounds, Sorting, MapReduce, open-source framework, Pangool, Distributed systems, Data models, Mathematical model, compound records, distributed programming]
Assessing the Significance of Data Mining Results on Graphs with Feature Vectors
2012 IEEE 12th International Conference on Data Mining
None
2012
Assessing the significance of data mining results is an important step in the knowledge discovery process. While results might appear interesting at a first glance, they can often be explained by already known characteristics of the data. Randomization is an established technique for significance testing, and methods to assess data mining results on vector data or network data have been proposed. In many applications, however, both sources are simultaneously given. Since these sources are rarely independent of each other but highly correlated, naively applying existing randomization methods on each source separately is questionable. In this work, we present a method to assess the significance of mining results on graphs with binary features vectors. We propose a novel null model that preserves correlation information between both sources. Our randomization exploits an adaptive Metropolis sampling and interweaves attribute randomization and graph randomization steps. In thorough experiments, we demonstrate the application of our technique. Our results indicate that while simultaneously using both sources is beneficial, often one source of information is dominant for determining the mining results.
[Correlation, graph theory, data mining, network data, Data mining, graph randomization, network, randomization, Clustering algorithms, vector data, data mining result, correlation information, binary feature vector, Testing, adaptive Metropolis sampling, sampling methods, randomization technique, attribute randomization, Vectors, graph, vectors, significance testing, Markov processes, information source, Data models, knowledge discovery process]
Sequential Network Change Detection with Its Applications to Ad Impact Relation Analysis
2012 IEEE 12th International Conference on Data Mining
None
2012
We are concerned with the issue of tracking changes of variable dependencies from multivariate time series. Conventionally, this issue has been addressed in the batch scenario where the whole data set is given at once, and the change detection must be done in a retrospective way. This paper addresses this issue in a sequential scenario where multivariate data are sequentially input and the detection must be done in a sequential fashion. We propose a new method for sequential tracking of variable dependencies. In it we employ a Bayesian network as a representation of variable dependencies. The key ideas of our method are, 1) we extend the theory of dynamic model selection (DMS), which has been developed in the batch-learning scenario, into the sequential setting, and apply it to our issue, 2) we conduct the change detection sequentially using dynamic programming per a window where we employ the Hoeffding's bound to automatically determine the window size. We empirically demonstrate that our proposed method is able to perform change detection more efficiently than a conventional batch method. Further, we give a new framework of an application of variable dependency change detection, which we call Ad Impact Relation analysis (AIR). In it, we detect the time point when a commercial message advertisement has given an impact on the market and effectively visulaize the impact through network changes. We employ real data sets to demonstrate the validity of AIR.
[Algorithm design and analysis, variable dependency change detection, Bayesian Network, Hoeffding's bound, Marketing, Graphical models, multivariate data, dynamic model selection, Advertisement, belief networks, learning (artificial intelligence), sequential scenario, commercial message advertisement, Bayesian network, conventional batch method, ad impact relation analysis, advertising data processing, data analysis, Computational modeling, Time series analysis, multivariate time series, sequential fashion, sequential setting, dynamic programming, time series, Dynamic Model Selection, Encoding, batch-learning scenario, AIR, DMS, Netword Change Detection, Bayesian methods, Data models, sequential network change detection, sequential tracking]
Hierarchical Multi-task Learning with Application to Wafer Quality Prediction
2012 IEEE 12th International Conference on Data Mining
None
2012
Many real problems of multi-task learning exhibit hierarchical task relatedness. In other words, the tasks are partitioned into multiple groups. Different tasks within the same group are related on the task-level, whereas different groups are related on the group-level. For example, in semiconductor manufacturing, the problem of wafer quality prediction can be considered as hierarchical multi-task learning, where each task corresponds to a single side of a chamber with side-level relatedness, and a group of tasks corresponds to a chamber of multiple sides with chamber-level relatedness. Motivated by this application, in this paper, we propose an optimization framework for hierarchical multi-task learning, which partitions all the input features into 2 sets based on their characteristics, and models task-level and group-level relatedness by imposing different constraints on the coefficient vectors of the 2 sets. This is different from existing work on task clustering where the goal is to uncover the grouping of tasks, the tasks do not exhibit group-level relatedness, and the input features are not discriminated in the prediction model to model task-level and group-level relatedness. To solve this framework, we propose the \\hear\\ algorithm based on block coordinate descent, and demonstrate its effectiveness on both synthetic and real data sets from domains of semiconductor manufacturing and document classification.
[production engineering computing, Predictive models, real data sets, task-level relatedness, hierarchical task relatedness, block coordinate descent-based HEAR algorithm, semiconductor technology, Optimization, Semiconductor device modeling, optimisation, optimization framework, chamber-level relatedness, Prediction algorithms, integrated circuit manufacture, task clustering, semiconductor manufacturing, Manufacturing, learning (artificial intelligence), synthetic data sets, hierarchical multi-task learning, task relatedness, coefficient vectors, prediction model, wafer quality, Linear programming, Vectors, group-level relatedness, document classification, prediction theory, side-level relatedness, pattern clustering, hierarchical multitask learning, wafer quality prediction]
Dimensional Testing for Multi-step Similarity Search
2012 IEEE 12th International Conference on Data Mining
None
2012
In data mining applications such as subspace clustering or feature selection, changes to the underlying feature set can require the reconstruction of search indices to support fundamental data mining tasks. For such situations, multi-step search approaches have been proposed that can accommodate changes in the underlying similarity measure without the need to rebuild the index. In this paper, we present a heuristic multi-step search algorithm that utilizes a measure of intrinsic dimension, the generalized expansion dimension (GED), as the basis of its search termination condition. Compared to the current state-of-the-art method, experimental results show that our heuristic approach is able to obtain significant improvements in both the number of candidates and the running time, while losing very little in the accuracy of the query results.
[Algorithm design and analysis, Measurement, similarity measure, Heuristic algorithms, dimensional testing, data mining, search indices, feature set, Data mining, query processing, query result, nearest neighbor, intrinsic dimension, multi-step, feature selection, multistep similarity search, GED, adaptive similarity, Vectors, generalized expansion dimension, kNN, Indexes, Similarity search, search termination condition, pattern clustering, subspace clustering, heuristic multistep search algorithm, Approximation algorithms, intrinsic dimensionality]
Discovery of Causal Rules Using Partial Association
2012 IEEE 12th International Conference on Data Mining
None
2012
Discovering causal relationships in large databases of observational data is challenging. The pioneering work in this area was rooted in the theory of Bayesian network (BN) learning, which however, is a NP-complete problem. Hence several constraint-based algorithms have been developed to efficiently discover causations in large databases. These methods usually use the idea of BN learning, directly or indirectly, and are focused on causal relationships with single cause variables. In this paper, we propose an approach to mine causal rules in large databases of binary variables. Our method expands the scope of causality discovery to causal relationships with multiple cause variables, and we utilise partial association tests to exclude noncausal associations, to ensure the high reliability of discovered causal rules. Furthermore an efficient algorithm is designed for the tests in large databases. We assess the method with a set of real-world diagnostic data. The results show that our method can effectively discover interesting causal rules in large databases.
[data mining, constraint-based algorithm, large database, causal rule, binary variable, Bayesian network learning, Association rules, observational data, NP-complete problem, Equations, Diseases, causality, Databases, causal rule mining, Bayesian methods, very large databases, partial association test, partial association, Bayes methods, Reliability, causal rule discovery, Testing, computational complexity]
Bounded Matrix Low Rank Approximation
2012 IEEE 12th International Conference on Data Mining
None
2012
Matrix lower rank approximations such as non-negative matrix factorization (NMF) have been successfully used to solve many data mining tasks. In this paper, we propose a new matrix lower rank approximation called Bounded Matrix Low Rank Approximation (BMA) which imposes a lower and an upper bound on every element of a lower rank matrix that best approximates a given matrix with missing elements. This new approximation models many real world problems, such as recommender systems, and performs better than other methods, such as singular value decompositions (SVD) or NMF. We present an efficient algorithm to solve BMA based on coordinate descent method. BMA is different from NMF as it imposes bounds on the approximation itself rather than on each of the low rank factors. We show that our algorithm is scalable for large matrices with missing elements on multi core systems with low memory. We present substantial experimental results illustrating that the proposed method outperforms the state of the art algorithms for recommender systems such as Stochastic Gradient Descent, Alternating Least Squares with regularization, SVD++, Bias-SVD on real world data sets such as Jester, Movie lens, Book crossing, Online dating and Netflix.
[matrix factorization, data mining, regularization, scalable algorithm, Vectors, block coordinate de- scent method, Matrix decomposition, Least squares approximation, nonnegative matrix factorization, coordinate descent method, stochastic gradient descent, bound constraints, matrix algebra, multicore systems, Upper bound, recommender systems, Approximation algorithms, block, bounded matrix low rank approximation model, data mining task, gradient methods, Recommender systems, singular value decomposition, Low rank approximation]
Active Evaluation of Classifiers on Large Datasets
2012 IEEE 12th International Conference on Data Mining
None
2012
The goal of this work is to estimate the accuracy of a classifier on a large unlabeled dataset based on a small labeled set and a human labeler. We seek to estimate accuracy and select instances for labeling in a loop via a continuously refined stratified sampling strategy. For stratifying data we develop a novel strategy of learning r bit hash functions to preserve similarity in accuracy values. We show that our algorithm provides better accuracy estimates than existing methods for learning distance preserving hash functions. Experiments on a wide spectrum of real datasets show that our estimates achieve between 15% and 62% relative reduction in error compared to existing approaches. We show how to perform stratified sampling on unlabeled data that is so large that in an interactive setting even a single sequential scan is impractical. We present an optimal algorithm for performing importance sampling on a static index over the data that achieves close to exact estimates while reading three orders of magnitude less data.
[labeling instance, learning strategy, pattern classification, sampling methods, Estimation, Humans, cryptography, Accuracy estimation, Vectors, labeling accuracy, importance sampling, unlabeled dataset classifier, Learning systems, active evaluation, Accuracy, active classifier evaluation, learning hash functions, continuously refined stratified sampling strategy, Labeling, Reliability, learning (artificial intelligence), distance preserving hash function]
ConfDTree: Improving Decision Trees Using Confidence Intervals
2012 IEEE 12th International Conference on Data Mining
None
2012
Decision trees have three main disadvantages: reduced performance when the training set is small, rigid decision criteria and the fact that a single "uncharacteristic" attribute might "derail" the classification process. In this paper we present ConfDTree - a post-processing method which enables decision trees to better classify outlier instances. This method, which can be applied on any decision trees algorithm, uses confidence intervals in order to identify these hard-to-classify instances and proposes alternative routes. The experimental study indicates that the proposed post-processing method consistently and significantly improves the predictive performance of decision trees, particularly for small, imbalanced or multi-class datasets in which an average improvement of 5%-9% in the AUC performance is reported.
[training set, classification process, pattern classification, imbalanced datasets, decision trees algorithm, AUC performance, Gaussian distribution, Classification algorithms, set theory, Standards, Training, multiclass datasets, decision criteria, ConfDTree, Vegetation, decision trees, post-processing method, Prediction algorithms, confidence intervals, Decision trees, hard-to-classify instances]
ETM: Entity Topic Models for Mining Documents Associated with Entities
2012 IEEE 12th International Conference on Data Mining
None
2012
Topic models, which factor each document into different topics and represent each topic as a distribution of terms, have been widely and successfully used to better understand collections of text documents. However, documents are also associated with further information, such as the set of real-world entities mentioned in them. For example, news articles are usually related to several people, organizations, countries or locations. Since those associated entities carry rich information, it is highly desirable to build more expressive, entity-based topic models, which can capture the term distributions for each topic, each entity, as well as each topic-entity pair. In this paper, we therefore introduce a novel Entity Topic Model (ETM) for documents that are associated with a set of entities. ETM not only models the generative process of a term given its topic and entity information, but also models the correlation of entity term distributions and topic term distributions. A Gibbs sampling-based algorithm is proposed to learn the model. Experiments on real datasets demonstrate the effectiveness of our approach over several state-of-the-art baselines.
[entity topic models, text analysis, Correlation, data mining, text documents, Data mining, model learning, Analytical models, Monte Carlo methods, Gibbs sampling-based algorithm, Mathematical model, learning (artificial intelligence), topic term distributions, topic information, entity term distribution correlation, Computational modeling, topic-entity pair, Vectors, entity information, Markov processes, topic models, Data models, document mining, entity, ETM]
Understanding Data Completeness in Network Monitoring Systems
2012 IEEE 12th International Conference on Data Mining
None
2012
In many networks including Internet Service Providers, transportation monitoring systems and the electric grid, measurements from a set of objects are continuously taken over time and used for important decisions such as provisioning and pricing. It is therefore vital to understand the completeness and reliability of such data. Given the large volume of data generated by these systems, rather than enumerating the times and objects incurring missing or spurious data, it is more effective to provide patterns (groups of tuples) concisely summarizing trends that may not otherwise be readily observable. In this paper, we define the Graph Tableau Discovery Problem where the measured tuples can be thought of as edges in a bipartite graph on an ordered attribute (time) and an unordered attribute (object identifiers). We define the problem of finding an optimal summary, show that it is NP-complete, and then provide a polynomial-time approximation algorithm with guarantees to find a good summary. Experiments on real and synthetic data demonstrate the effectiveness and efficiency of our approach.
[Greedy algorithms, graph theory, graph tableau discovery problem, Time measurement, Loss measurement, network monitoring systems, Internet service providers, NP-complete, Approximation methods, transportation monitoring systems, polynomial approximation, data completeness, electric grid, Approximation algorithms, Silicon, data handling, polynomial-time approximation algorithm, Monitoring, pricing, computational complexity]
Effective and Robust Mining of Temporal Subspace Clusters
2012 IEEE 12th International Conference on Data Mining
None
2012
Mining temporal multivariate data by clustering is an important research topic. In today's complex data, interesting patterns are often neither bound to the whole dimensional nor temporal extent of the data domain. This challenge is met by temporal subspace clustering methods. Their effectiveness, however, is impeded by aspects unavoidable in real world data: Misalignments between time series, for example caused by out-of-sync sensors, and measurement errors. Under these conditions, existing temporal subspace clustering approaches miss the patterns contained in the data. In this paper, we propose a novel clustering method that mines temporal subspace clusters reflected by sets of objects and relevant intervals. We enable flexible handling of misaligned time series by adaptively shifting time series in the time domain, and we achieve robustness to measurement errors by allowing certain fractions of deviating values in each relevant point in time. We show the effectiveness of our method in experiments on real and synthetic data.
[data domain, Clustering methods, Time series analysis, temporal multivariate data mining, measurement errors, data mining, out-of-sync sensors, time series, Vectors, Time measurement, temporal subspace clustering methods, Data mining, pattern clustering, Clustering algorithms, Robustness]
Outlier Detection in Arbitrarily Oriented Subspaces
2012 IEEE 12th International Conference on Data Mining
None
2012
In this paper, we propose a novel outlier detection model to find outliers that deviate from the generating mechanisms of normal instances by considering combinations of different subsets of attributes, as they occur when there are local correlations in the data set. Our model enables to search for outliers in arbitrarily oriented subspaces of the original feature space. We show how in addition to an outlier score, our model also derives an explanation of the outlierness that is useful in investigating the results. Our experiments suggest that our novel method can find different outliers than existing work and can be seen as a complement of those approaches.
[outlier score, Correlation, data mining, Vectors, anomaly detection, Covariance matrix, outlier detection, unsupervised learning, arbitrarily oriented subspace, correlation, Feature extraction, Eigenvalues and eigenfunctions, Data models, statistical analysis, outlier detection model, normal instance mechanism, Principal component analysis]
Dynamic Multi-relational Chinese Restaurant Process for Analyzing Influences on Users in Social Media
2012 IEEE 12th International Conference on Data Mining
None
2012
We study the problem of analyzing influence of various factors affecting individual messages posted in social media. The problem is challenging because of various types of influences propagating through the social media network that act simultaneously on any user. Additionally, the topic composition of the influencing factors and the susceptibility of users to these influences evolve over time. This problem has not been studied before, and off-the-shelf models are unsuitable for this purpose. To capture the complex interplay of these various factors, we propose a new non-parametric model called the Dynamic Multi-Relational Chinese Restaurant Process. This accounts for the user network for data generation and also allows the parameters to evolve over time. Designing inference algorithms for this model suited for large scale social-media data is another challenge. To this end, we propose a scalable and multi-threaded inference algorithm based on online Gibbs Sampling. Extensive evaluations on large-scale Twitter and Face book data show that the extracted topics when applied to authorship and commenting prediction outperform state-of-the-art baselines. More importantly, our model produces valuable insights on topic trends and user personality trends beyond the capability of existing approaches.
[Algorithm design and analysis, multithreaded inference algorithm, nonparametric model, data mining, Analytical models, large-scale Twitter, off-the-shelf model, Facebook data, Mathematical model, stochastic processes, social media, Social Media Analysis, sampling methods, large scale social-media data, Media, inference mechanisms, topic trend, Equations, data generation, user personality trend, Parallel Inference, topic composition, dynamic multirelational Chinese restaurant process, Non-parametric Modeling, social networking (online), Data models, Inference algorithms, scalable inference algorithm, online Gibbs Sampling]
Nested Subtree Hash Kernels for Large-Scale Graph Classification over Streams
2012 IEEE 12th International Conference on Data Mining
None
2012
Most studies on graph classification focus on designing fast and effective kernels. Several fast subtree kernels have achieved a linear time-complexity w.r.t. the number of edges under the condition that a common feature space (e.g., a subtree pattern list) is needed to represent all graphs. This will be infeasible when graphs are presented in a stream with rapidly emerging subtree patterns. In this case, computing a kernel matrix for graphs over the entire stream is difficult since the graphs in the expired chunks cannot be projected onto the unlimitedly expanding feature space again. This leads to a big trouble for graph classification over streams - Different portions of graphs have different feature spaces. In this paper, we aim to enable large-scale graph classification over streams using the classical ensemble learning framework, which requires the data in different chunks to be in the same feature space. To this end, we propose a Nested Subtree Hashing (NSH) algorithm to recursively project the multi-resolution subtree patterns of different chunks onto a set of common low-dimensional feature spaces. We theoretically analyze the derived NSH kernel and obtain a number of favorable properties: 1) The NSH kernel is an unbiased and highly concentrated estimator of the fast subtree kernel. 2) The bound of convergence rate tends to be tighter as the NSH algorithm steps into a higher resolution. 3) The NSH kernel is robust in tolerating concept drift between chunks over a stream. We also empirically test the NSH kernel on both a large-scale synthetic graph data set and a real-world chemical compounds data set for anticancer activity prediction. The experimental results validate that the NSH kernel is indeed efficient and robust for graph classification over streams.
[pattern classification, trees (mathematics), Vectors, linear time-complexity, Graph classification, Data mining, nested subtree hash kernels, Chemical compounds, Convergence, ensemble learning, graph hash kernels, large-scale graph classification, data stream mining, nested subtree hashing, multi-resolution subtree patterns, Feature extraction, file organisation, Robustness, learning (artificial intelligence), Kernel, computational complexity]
Efficient Behavior Targeting Using SVM Ensemble Indexing
2012 IEEE 12th International Conference on Data Mining
None
2012
Behavior targeting (BT) is a promising tool for online advertising. The state-of-the-art BT methods, which are mainly based on regression models, have two limitations. First, learning regression models for behavior targeting is difficult since user clicks are typically several orders of magnitude fewer than views. Second, the user interests are not fixed, but often transient and influenced by media and pop culture. In this paper, we propose to formulate behavior targeting as a classification problem. Specifically, we propose to use an SVM ensemble for behavior prediction. The challenge of using ensemble SVM for BT stems from the computational complexity (it takes 53 minutes in our experiments to predict behavior for 32 million users, which is inadequate for online application). To this end, we propose a fast ensemble SVM prediction framework, which builds an indexing structure for SVM ensemble to achieve sub-linear prediction time complexity. Experimental results on real-world large scale behavior targeting data demonstrate that the proposed method is efficient and outperforms existing linear regression based BT models.
[pattern classification, real-world large scale behavior targeting data, advertising data processing, support vector machines, learning regression model, Behavior Targeting, Conferences, indexing, behavioural sciences, SVM ensemble indexing, regression analysis, Data mining, classification problem, indexing structure, sublinear prediction time complexity, ensemble SVM, SVM index, BT method, learning (artificial intelligence), online advertising, computational complexity, behavior prediction]
Co-labeling: A New Multi-view Learning Approach for Ambiguous Problems
2012 IEEE 12th International Conference on Data Mining
None
2012
We propose a multi-view learning approach called co-labeling which is applicable for several machine learning problems where the labels of training samples are uncertain, including semi-supervised learning (SSL), multi-instance learning (MIL) and max-margin clustering (MMC). Particularly, we first unify those problems into a general ambiguous problem in which we simultaneously learn a robust classifier as well as find the optimal training labels from a finite label candidate set. To effectively utilize multiple views of data, we then develop our co-labeling approach for the general multi-view ambiguous problem. In our work, classifiers trained on different views can teach each other by iteratively passing the predictions of training samples from one classifier to the others. The predictions from one classifier are considered as label candidates for the other classifiers. To train a classifier with a label candidate set for each view, we adopt the Multiple Kernel Learning (MKL) technique by constructing the base kernel through associating the input kernel calculated from input features with one label candidate. Compared with the traditional co-training method which was specifically designed for SSL, the advantages of our co-labeling are two-fold: 1) it can be applied to other ambiguous problems such as MIL and MMC, 2) it is more robust by using the MKL method to integrate multiple labeling candidates obtained from different iterations and biases. Promising results on several real-world multi-view data sets clearly demonstrate the effectiveness of our proposed co-labeling for both MIL and SSL.
[semisupervised learning, multiple labeling candidates, ambiguous learning, finite label candidate set, machine learning problems, multiple kernel learning technique, set theory, max-margin clustering, real-world multiview data sets, Training, multiview learning approach, multiple kernel learning, colabeling approach, training samples, Prediction algorithms, Robustness, robust classifier, Labeling, learning (artificial intelligence), Kernel, pattern classification, MMC, multi-instance learning, multiview ambiguous problem, input kernel, multiinstance learning, SSL, TBIR, MIL, pattern clustering, Supervised learning, Semisupervised learning, ambiguous problems, semi-supervised learning, optimal training labels, MKL technique]
A General and Scalable Approach to Mixed Membership Clustering
2012 IEEE 12th International Conference on Data Mining
None
2012
Spectral clustering methods are elegant and effective graph-based node clustering methods, but they do not allow mixed membership clustering. We describe an approach that first transforms the data from a node-centric representation to an edge-centric one, and then use this representation to define a scalable and competitive mixed membership alternative to spectral clustering methods. Experimental results show the proposed approach improves substantially in mixed membership clustering tasks over node clustering methods.
[graph-based node clustering methods, Clustering methods, Social network services, Communities, graph theory, spectral clustering methods, scalable approach, Vectors, Sparse matrices, general approach, edge-centric representation, pattern clustering, competitive mixed membership clustering, Clustering algorithms, scalable mixed membership, clustering; scalable methods; unsupervised learning; large scale learning; mixed membership clustering;, Bipartite graph, node-centric representation]
Time Constrained Influence Maximization in Social Networks
2012 IEEE 12th International Conference on Data Mining
None
2012
Influence maximization is a fundamental research problem in social networks. Viral marketing, one of its applications, is to get a small number of users to adopt a product, which subsequently triggers a large cascade of further adoptions by utilizing "Word-of-Mouth" effect in social networks. Influence maximization problem has been extensively studied recently. However, none of the previous work considers the time constraint in the influence maximization problem. In this paper, we propose the time constrained influence maximization problem. We show that the problem is NP-hard, and prove the monotonicity and submodularity of the time constrained influence spread function. Based on this, we develop a greedy algorithm with performance guarantees. To improve the algorithm scalability, we propose two Influence Spreading Path based methods. Extensive experiments conducted over four public available datasets demonstrate the efficiency and effectiveness of the Influence Spreading Path based methods.
[Greedy algorithms, Social network services, greedy algorithms, social networks, monotonicity, Delay, optimisation, viral marketing, NP hard, submodularity, time constrained influence maximization problem, influence spreading path based method, Approximation algorithms, social networking (online), time constrained influence spread function, Time factors, greedy algorithm, Integrated circuit modeling]
A Stochastic Model for Context-Aware Anomaly Detection in Indoor Location Traces
2012 IEEE 12th International Conference on Data Mining
None
2012
Rapid growth in the development of real-time location system solutions has led to an increased interest in indoor location-aware services, such as hospital asset management. Although there are extensive studies in the literature on the analysis of outdoor location traces, the studies of indoor location traces are less touched and fragmented. To that end, in this paper, we provide a focused study of indoor location traces collected by the sensors attached to medical devices in a hospital environment. Along this line, we first introduce some unique properties of these indoor location traces. We show that they can capture the movement patterns of the medical devices, which are tightly coupled with the work flow in the controlled hospital environment. Based on this observation, we propose a stochastic model for context-aware anomaly detection in indoor location traces, which exploits the hospital work flow and models the movements of medical devices as transitions in finite state machines. In detail, we first develop a density-based method to identify the hotspots filled with high-level abnormal activities in the indoor environment. The discovered hotspots serve as the context for nearby trajectories. Then, we introduce an N-gram based method for measuring the degree of anomaly based on the detected hotspots, which is able to predict the missing events possibly due to the devices being stolen. Besides, to address the noisy nature of the indoor sensor networks, we also propose an iterative algorithm to estimate the transition probabilities. This algorithm allows to effectively recover the missing location records which are critical for the abnormality estimation. Finally, the experimental results on the real-world date sets validate the effectiveness of the proposed context-aware anomaly detection method for identifying abnormal events.
[hospitals, iterative methods, Stochastic processes, data mining, density-based method, finite state machines, context-aware anomaly detection, hotspots, Indoor Trajectory, mobile computing, indoor location traces, Clustering algorithms, Stochastic Modeling, Sensors, Trajectory, indoor radio, medical device, real-time location system, transition probability, Buildings, hospital asset management, iterative algorithm, N-gram based method, Anomaly Detection, hospital environment, Hospitals, indoor sensor networks, real-time systems, finite state machine, stochastic model, Context modeling]
Reliable clustering on uncertain graphs
2012 IEEE 12th International Conference on Data Mining
None
2012
Many graphs in practical applications are not deterministic, but are probabilistic in nature because the existence of the edges is inferred with the use of a variety of statistical approaches. In this paper, we will examine the problem of clustering uncertain graphs. Uncertain graphs are best clustered with the use of a possible worlds model in which the most reliable clusters are discovered in the presence of uncertainty. Reliable clusters are those which are not likely to be disconnected in the context of different instantiations of the uncertain graph. We present experimental results which illustrate the effectiveness of our model and approach.
[graph theory, uncertain systems, reliable clustering, reliable clusters, reliability, Linear programming, worlds model, Channel coding, Equations, statistical approaches, uncertain graph, pattern clustering, Clustering algorithms, uncertain graphs, clustering, Reliability, statistical analysis]
Robust Prediction and Outlier Detection for Spatial Datasets
2012 IEEE 12th International Conference on Data Mining
None
2012
Spatial kriging is a widely used predictive model for spatial datasets. In spatial kriging model, the observations are assumed to be Gaussian for computational convenience. However, its predictive accuracy could be significantly compromised if the observations are contaminated by outliers. This deficiency can be systematically addressed by increasing the robustness of spatial kriging model using heavy tailed distributions, such as the Huber, Laplace, and Student's t distributions. This paper presents a novel Robust and Reduced Rank Spatial Kriging Model (R3-SKM), which is resilient to the influences of outliers and allows for fast spatial inference. Furthermore, three effective and efficient algorithms are proposed based on R3-SKM framework that can perform robust parameter estimation, spatial prediction, and spatial outlier detection with a linear-order time complexity. Extensive experiments on both simulated and real data sets demonstrated the robustness and efficiency of our proposed techniques.
[spatial outlier detection, Approximation methods, statistical distributions, spatial prediction, student t distribution, R3-SKM, Robust Estimation, parameter estimation, Robustness, Outlier Detection, Huber distribution, Laplace Approximation, predictive model, Vectors, Spatial databases, spatial inference, inference mechanisms, prediction theory, Gaussian approximation, Laplace distribution, reduced rank spatial kriging model, spatial dataset, robust prediction, Gaussian processes, predictive accuracy, Approximation algorithms, Gaussian, linear-order time complexity, computational complexity]
Profit Maximization over Social Networks
2012 IEEE 12th International Conference on Data Mining
None
2012
Influence maximization is the problem of finding a set of influential users in a social network such that the expected spread of influence under a certain propagation model is maximized. Much of the previous work has neglected the important distinction between social influence and actual product adoption. However, as recognized in the management science literature, an individual who gets influenced by social acquaintances may not necessarily adopt a product (or technology), due, e.g., to monetary concerns. In this work, we distinguish between influence and adoption by explicitly modeling the states of being influenced and of adopting a product. We extend the classical Linear Threshold (LT) model to incorporate prices and valuations, and factor them into users' decision-making process of adopting a product. We show that the expected profit function under our proposed model maintains submodularity under certain conditions, but no longer exhibits monotonicity, unlike the expected influence spread function. To maximize the expected profit under our extended LT model, we employ an unbudgeted greedy framework to propose three profit maximization algorithms. The results of our detailed experimental study on three real-world datasets demonstrate that of the three algorithms, PAGE, which assigns prices dynamically based on the profit potential of each candidate seed, has the best performance both in the expected profit achieved and in running time.
[profitability, profit maximization, PAGE algorithm, product adoption, monetary concern, social acquaintance, Cost accounting, social network, optimisation, viral marketing, price, Pricing, social influence, Social network services, Computational modeling, greedy algorithms, social networks, Vectors, network propagation model, marketing, valuation, Profit maximization, expected influence spread function, social networking (online), influence maximization, linear threshold model, Integrated circuit modeling, pricing, expected profit function, unbudgeted greedy framework]
Parallelization with Multiplicative Algorithms for Big Data Mining
2012 IEEE 12th International Conference on Data Mining
None
2012
We propose a nontrivial strategy to parallelize a series of data mining and machine learning problems, including 1-class and 2-class support vector machines, nonnegative least square problems, and $\\ell_1$ regularized regression (LASSO) problems. Our strategy fortunately leads to extremely simple multiplicative algorithms which can be straightforwardly implemented in parallel computational environments, such as Map Reduce, or CUDA. We provide rigorous analysis of the correctness and convergence of the algorithm. We demonstrate the scalability and accuracy of our algorithms in comparison with other current leading algorithms.
[Algorithm design and analysis, Machine learning algorithms, Graphics processing units, data mining, regression analysis, nontrivial strategy, Support Vector Machine, Data mining, Optimization, Convergence, machine learning problem, MapReduce, multiplicative algorithm, Map Reduce, LASSO, 2-class support vector machine, learning (artificial intelligence), LASSO problem, nonnegative least square problem, support vector machines, 1-class support vector machine, regularized regression, Big Data, Support vector machines, CUDA, parallel computational environment]
Efficient Algorithms for Finding Richer Subgroup Descriptions in Numeric and Nominal Data
2012 IEEE 12th International Conference on Data Mining
None
2012
Subgroup discovery systems are concerned with finding interesting patterns in labeled data. How these systems deal with numeric and nominal data has a large impact on the quality of their results. In this paper, we consider two ways to extend the standard pattern language of subgroup discovery: using conditions that test for interval membership for numeric attributes, and value set membership for nominal attributes. We assume a greedy search setting, that is, iteratively refining a given subgroup, with respect to a (convex) quality measure. For numeric attributes, we propose an algorithm that finds the optimal interval in linear (rather than quadratic) time, with respect to the number of examples and split points. Similarly, for nominal attributes, we show that finding the optimal set of values can be achieved in linear (rather than exponential) time, with respect to the number of examples and the size of the domain of the attribute. These algorithms operate by only considering subgroup refinements that lie on a convex hull in ROC space, thus significantly narrowing down the search space. We further provide efficient algorithms specifically for the popular Weighted Relative Accuracy quality measure, taking advantage of some of its properties. Our algorithms are shown to perform well in practice, and furthermore provide additional expressive power leading to higher-quality results.
[interval membership, ROC space, nominal attributes, data mining, Complexity theory, Data mining, weighted relative accuracy quality measure, subgroup descriptions, Accuracy, greedy search setting, Decision trees, subgroup discovery, Context, Weight measurement, subgroup discovery systems, ROC analysis, numeric data, standard pattern language, convex hull, labeled data, value set membership, convex functions, interesting patterns, Gain measurement, numeric attributes, nominal data, linear time]
Community Preserving Lossy Compression of Social Networks
2012 IEEE 12th International Conference on Data Mining
None
2012
Compression plays an important role in social network analysis from both practical and theoretical points of view. Although there are a few pioneering studies on social network compression, they mainly focus on lossless approaches. In this paper, we tackle the novel problem of community preserving lossy compression of social networks. The trade-off between space and information preserved in a lossy compression presents an interesting angle for social network analysis, and, at the same time, makes the problem very challenging. We propose a sequence graph compression approach, discuss the design of objective functions towards community preservation, and present an interesting and practically effective greedy algorithm. Our experimental results on both real data sets and synthetic data sets demonstrate the promise of our method.
[Algorithm design and analysis, data compression, community preserving lossy compression, Social network services, greedy algorithms, Communities, Noise, graph theory, sequence graph compression approach, social networks, objective functions, Linear programming, Educational institutions, Equations, social network analysis, social networking (online), lossless approaches, greedy algorithm, compression, synthetic data sets, communities]
Dynamic Boolean Matrix Factorizations
2012 IEEE 12th International Conference on Data Mining
None
2012
Boolean matrix factorization is a method to decompose a binary matrix into two binary factor matrices. Akin to other matrix factorizations, the factor matrices can be used for various data analysis tasks. Many (if not most) real-world data sets are dynamic, though, meaning that new information is recorded over time. Incorporating this new information into the factorization can require a re-computation of the factorization -- something we cannot do if we want to keep our factorization up-to-date after each update. This paper proposes a method to dynamically update the Boolean matrix factorization when new data is added to the data base. This method is extended with a mechanism to improve the factorization with a trade-off in speed of computation. The method is tested with a number of real-world and synthetic data sets including studying its efficiency against off-line methods. The results show that with good initialization the proposed online and dynamic methods can beat the state-of-the-art offline Boolean matrix factorization algorithms.
[Algorithm design and analysis, Heuristic algorithms, data analysis tasks, matrix decomposition, Approximation methods, Data mining, database management systems, On-line algorithms, Matrices, binary matrix, synthetic data sets, off-line methods, data analysis, dynamic Boolean matrix factorizations, recomputation, state-of-the-art offline Boolean matrix factorization algorithms, dynamic methods, Vectors, Encoding, Boolean algebra, Dynamic algorithms, Boolean matrix factorization, binary factor matrices, online methods, real-world data sets]
Outlier Ranking via Subspace Analysis in Multiple Views of the Data
2012 IEEE 12th International Conference on Data Mining
None
2012
Outlier mining is an important task for finding anomalous objects. In practice, however, there is not always a clear distinction between outliers and regular objects as objects have different roles w.r.t. different attribute sets. An object may deviate in one subspace, i.e. a subset of attributes. And the same object might appear perfectly regular in other subspaces. One can think of subspaces as multiple views on one database. Traditional methods consider only one view (the full attribute space). Thus, they miss complex outliers that are hidden in multiple subspaces. In this work, we propose Outrank, a novel outlier ranking concept. Outrank exploits subspace analysis to determine the degree of outlierness. It considers different subsets of the attributes as individual outlier properties. It compares clustered regions in arbitrary subspaces and derives an outlierness score for each object. Its principled integration of multiple views into an outlierness measure uncovers outliers that are not detectable in the full attribute space. Our experimental evaluation demonstrates that Outrank successfully determines a high quality outlier ranking, and outperforms state-of-the-art outlierness measures.
[Thyristors, data analysis, Conferences, multiple subspaces, data mining, OutRank, Educational institutions, Data mining, outlier ranking, database, subspace analysis, Databases, clusterings, Clustering algorithms, Abstracts, outlierness score, outlier ranking concept, outlierness degree determination]
Clash of the Contagions: Cooperation and Competition in Information Diffusion
2012 IEEE 12th International Conference on Data Mining
None
2012
In networks, contagions such as information, purchasing behaviors, and diseases, spread and diffuse from node to node over the edges of the network. Moreover, in real-world scenarios multiple contagions spread through the network simultaneously. These contagions not only propagate at the same time but they also interact and compete with each other as they spread over the network. While traditional empirical studies and models of diffusion consider individual contagions as independent and thus spreading in isolation, we study how different contagions interact with each other as they spread through the network. We develop a statistical model that allows for competition as well as cooperation of different contagions in information diffusion. Competing contagions decrease each other's probability of spreading, while cooperating contagions help each other in being adopted throughout the network. We evaluate our model on 18,000 contagions simultaneously spreading through the Twitter network. Our model learns how different contagions interact with each other and then uses these interactions to more accurately predict the diffusion of a contagion through the network. Moreover, the model also provides a compelling hypothesis for the principles that govern content interaction in information diffusion. Most importantly, we find very strong effects of interactions between contagions. Interactions cause a relative change in the spreading probability of a contagion by 71% on the average.
[Context, network contagion, information management, competing contagions, probability, Predictive models, Media, Twitter, statistical model, Twitter network, spreading probability, pattern clustering, contagion cooperation, social networking (online), Data models, Mathematical model, gradient methods, information diffusion, social media, contagion competition]
High Performance Offline and Online Distributed Collaborative Filtering
2012 IEEE 12th International Conference on Data Mining
None
2012
Big data analytics is a hot research area both in academia and industry. It envisages processing massive amounts of data at high rates to generate new insights leading to positive impact (for both users and providers) of industries such as E-commerce, Telecom, Finance, Life Sciences and so forth. We consider collaborative filtering (CF) and Clustering algorithms that are key fundamental analytics kernels that help in achieving these aims. High throughput CF and co-clustering on highly sparse and massive datasets, along with a high prediction accuracy, is a computationally challenging problem. In this paper, we present a novel hierarchical design for soft real-time (less than 1-minute.) distributed co-clustering based collaborative filtering algorithm. We study both the online and offline variants of this algorithm. Theoretical analysis of the time complexity of our algorithm proves the efficacy of our approach. Further, we present the impact of load balancing based optimizations on multi-core cluster architectures. Using the Netflix dataset(900M training ratings with replication) as well as the Yahoo KDD Cup(2.3B training ratings with replication) datasets, we demonstrate the performance and scalability of our algorithm on a large multi-core cluster architecture. In offline mode, our distributed algorithm demonstrates around 4x better performance (on Blue Gene/P) as compared to the best prior work, along with high accuracy. In online mode, we demonstrated around 3x better performance compared to baseline MPI implementation. To the best of our knowledge, our algorithm provides the best known online and offline performance and scalability results with high accuracy on multi-core cluster architectures.
[Algorithm design and analysis, collaborative filtering, Approximation methods, Parallel Performance Optimizations, Training, Netflix dataset, resource allocation, Clustering algorithms, online distributed collaborative filtering algorithm, high performance offline distributed collaborative filtering, multiprocessing systems, Distributed Collaborative Filtering, data analytics, clustering algorithm, time complexity, Partitioning algorithms, Matrix decomposition, analytics kernels, Performance &amp;amp; Scalability Analysis, load balancing based optimization, distributed algorithm, multicore cluster architecture, distributed algorithms, Collaboration, computational complexity]
Automatically Discovering Talented Musicians with Acoustic Analysis of YouTube Videos
2012 IEEE 12th International Conference on Data Mining
None
2012
Online video presents a great opportunity for up-and-coming singers and artists to be visible to a worldwide audience. However, the sheer quantity of video makes it difficult to discover promising musicians. We present a novel algorithm to automatically identify talented musicians using machine learning and acoustic analysis on a large set of "home singing" videos. We describe how candidate musician videos are identified and ranked by singing quality. To this end, we present new audio features specifically designed to directly capture singing quality. We evaluate these vis-a-vis a large set of generic audio features and demonstrate that the proposed features have good predictive performance. We also show that this algorithm performs well when videos are normalized for production quality.
[intonation, singing, Vectors, acoustic analysis, video, YouTube videos, machine learning, Standards, YouTube, Tuning, Videos, Histograms, melody, music, online video, talent discovery, Feature extraction, social networking (online), talented musicians discovery, learning (artificial intelligence), video signal processing, acoustic signal processing, home singing videos]
Robust Matrix Completion via Joint Schatten p-Norm and lp-Norm Minimization
2012 IEEE 12th International Conference on Data Mining
None
2012
The low-rank matrix completion problem is a fundamental machine learning problem with many important applications. The standard low-rank matrix completion methods relax the rank minimization problem by the trace norm minimization. However, this relaxation may make the solution seriously deviate from the original solution. Meanwhile, most completion methods minimize the squared prediction errors on the observed entries, which is sensitive to outliers. In this paper, we propose a new robust matrix completion method to address these two problems. The joint Schatten p-norm and &#x2113;<sub>p</sub>-norm are used to better approximate the rank minimization problem and enhance the robustness to outliers. The extensive experiments are performed on both synthetic data and real world applications in collaborative filtering and social network link prediction. All empirical results show our new method outperforms the standard matrix completion methods.
[collaborative filtering, low-rank matrix recovery, rank minimization problem, Social network services, matrix completion, outlier, Minimization, squared prediction error, machine learning, &#x2113;p-norm minimization, low-rank matrix completion problem, Standards, recommendation system, trace norm minimization, optimization, Collaboration, Schatten p-norm, social network link prediction, Motion pictures, social networking (online), Robustness, learning (artificial intelligence), minimisation, Joints]
Healing Truncation Bias: Self-Weighted Truncation Framework for Dual Averaging
2012 IEEE 12th International Conference on Data Mining
None
2012
We propose a new truncation framework for online supervised learning. Learning a compact predictive model in an online setting has recently attracted a great deal of attention. The combination of online learning with sparsity-inducing regularization enables faster learning with a smaller memory space than a conventional learning framework. However, a simple combination of these triggers the truncation of weights whose corresponding features rarely appear, even if these features are crucial for prediction. Furthermore, it is difficult to emphasize these features in advance while preserving the advantages of online learning. We develop an extensional truncation framework to Dual Averaging, which retains rarely occurring but informative features. Our proposed framework integrates information on all previous sub gradients of the loss functions into a regularization term. Our enhancement of a conventional L<sub>1</sub>-regularization accomplishes the automatic adjustment of each feature's truncations. This extension enables us to identify and retain rare but informative features without preprocessing. In addition, our framework achieves the same computational complexity and regret bound as standard Dual Averaging. Experiments demonstrated that our framework outperforms other sparse online learning algorithms.
[informative features, healing truncation bias, Predictive models, compact predictive model, Sentiment Analysis, Optimization, Feature Selection, conventional L<sub>1</sub>-regularization, Prediction algorithms, learning (artificial intelligence), self-weighted truncation framework, sparsity-inducing regularization, Educational institutions, loss functions, Sparsity-inducing Regularization, Vectors, Online Learning, Indexes, dual averaging, Equations, online supervised learning, Supervised Learning, extensional truncation framework, regularization term, computational complexity]
Unsupervised Multi-class Regularized Least-Squares Classification
2012 IEEE 12th International Conference on Data Mining
None
2012
Regularized least-squares classification is one of the most promising alternatives to standard support vector machines, with the desirable property of closed-form solutions that can be obtained analytically, and efficiently. While the supervised, and mostly binary case has received tremendous attention in recent years, unsupervised multi-class settings have not yet been considered. In this work we present an efficient implementation for the unsupervised extension of the multi-class regularized least-squares classification framework, which is, to the best of the authors' knowledge, the first one in the literature addressing this task. The resulting kernel-based framework efficiently combines steepest descent strategies with powerful meta-heuristics for avoiding local minima. The computational efficiency of the overall approach is ensured through the application of matrix algebra shortcuts that render efficient updates of the intermediate candidate solutions possible. Our experimental evaluation indicates the potential of the novel method, and demonstrates its superior clustering performance over a variety of competing methods on real-world data sets.
[pattern classification, least squares approximations, matrix algebra shortcuts, support vector machines, closed-form solutions, Maximum Margin Clustering, Vectors, Unsupervised Learning, kernel-based framework, Optimization, Unsupervised learning, matrix algebra, Support vector machines, Training, pattern clustering, steepest descent strategies, Multi-Class Regularized Least-Squares Classification, Clustering algorithms, clustering performance, unsupervised multiclass regularized least-squares classification, Kernel, gradient methods, meta-heuristics]
Utilizing Real-World Transportation Data for Accurate Traffic Prediction
2012 IEEE 12th International Conference on Data Mining
None
2012
For the first time, real-time high-fidelity spatiotemporal data on transportation networks of major cities have become available. This gold mine of data can be utilized to learn about traffic behavior at different times and locations, potentially resulting in major savings in time and fuel, the two important commodities of 21st century. As a first step towards the utilization of this data, in this paper, we study the real-world data collected from Los Angeles County transportation network in order to incorporate the data's intrinsic behavior into a time-series mining technique to enhance its accuracy for traffic prediction. In particular, we utilized the spatiotemporal behaviors of rush hours and events to perform a more accurate prediction of both short-term and long-term average speed on road-segments, even in the presence of infrequent events (e.g., accidents). Our result shows that taking historical rush-hour behavior we can improve the accuracy of traditional predictors by up to 67% and 78% in short-term and long-term predictions, respectively. Moreover, we can incorporate the impact of an accident to improve the prediction accuracy by up to 91%.
[Transportation, data mining, real-world transportation data, Los Angeles County, Predictive models, Data mining, Accuracy, data utilization, road accidents, time-series mining technique, transportation data, time-series mining, prediction accuracy, traffic prediction, road traffic, event impact analysis, time series, traffic engineering computing, spatiotemporal data, short-term prediction, accident impact, traffic behavior, rush hour traffic behavior, Data models, long-term prediction, transportation network, Accidents, Autoregressive processes]
Efficient Episode Mining of Dynamic Event Streams
2012 IEEE 12th International Conference on Data Mining
None
2012
Discovering frequent episodes over event sequences is an important data mining problem. Existing methods typically require multiple passes over the data, rendering them unsuitable for streaming contexts. We present the first streaming algorithm for mining frequent episodes over a window of recent events in the stream. We derive approximation guarantees for our algorithm in terms of: (i) the separation of frequent episodes from infrequent ones, and (ii) the rate of change of stream characteristics. Our parameterization of the problem provides a new sweet spot in the tradeoff between making distributional assumptions over the stream and algorithmic efficiencies of mining. We illustrate how this yields significant benefits when mining practical streams from neuroscience and telecommunications logs.
[Algorithm design and analysis, Data Streams, Photonic band gap, dynamic event streams, data mining, neuroscience, Streaming Algorithms, Event Sequences, Electronic mail, Approximation Algorithms, Data mining, Approximation methods, event sequences, telecommunications logs, Frequency shift keying, Approximation algorithms, Frequent Episodes, efficient episode mining, Pattern Discovery]
A Novel Semantic Smoothing Method Based on Higher Order Paths for Text Classification
2012 IEEE 12th International Conference on Data Mining
None
2012
It has been shown that Latent Semantic Indexing (LSI) takes advantage of implicit higher-order (or latent) structure in the association of terms and documents. Higher order relations in LSI capture "latent semantics". Inspired by this, a novel Bayesian framework for classification named Higher Order Nai&#x0308;ve Bayes (HONB), which can explicitly make use of these higher-order relations, has been introduced previously. We present a novel semantic smoothing method named Higher Order Smoothing (HOS) for the Naive Bayes algorithm. HOS is built on a similar graph based data representation of HONB which allows semantics in higher order paths to be exploited. Additionally, we take the concept one step further in HOS and exploited the relationships between instances of different classes in order to improve the parameter estimation when dealing with insufficient labeled data. As a result, we have not only been able to move beyond instance boundaries, but also class boundaries to exploit the latent information in higher-order paths. The results of our extensive experiments demonstrate the value of HOS on several benchmark datasets.
[text analysis, Semantic Smoothing, latent semantic indexing, smoothing methods, Classification algorithms, text classification, Niobium, Training, implicit higher-order structure, Text Classification, Semantics, semantic smoothing method, HONB, pattern classification, Smoothing methods, LSI, indexing, higher order smoothing, higher order nai&#x0308;ve Bayes, Bayesian framework, HOS, Support vector machines, higher order paths, Naive Bayes, Higher Order Smoothing, Text categorization, Higher Order Naive Bayes, Bayes methods]
Online Induction of Probabilistic Real Time Automata
2012 IEEE 12th International Conference on Data Mining
None
2012
Probabilistic real time automata (PRTAs) are a representation of dynamic processes arising in the sciences and industry. Currently, the induction of automata is divided into two steps: the creation of the prefix tree acceptor (PTA) and the merge procedure based on clustering of the states. These two steps can be very time intensive when a PRTA is to be induced for massive or even unbounded data sets. The latter one can be efficiently processed, as there exist scalable online clustering algorithms. However, the creation of the PTA still can be very time consuming. To overcome this problem, we propose a genuine online PRTA induction approach that incorporates new instances by first collapsing them and then using a maximum frequent pattern based clustering. The approach is tested against a predefined synthetic automaton and real-world data sets, for which the approach is scalable and stable. Moreover, we present a broad evaluation on a real world disease group data set that shows the applicability of such a model to the analysis of medical processes.
[synthetic automaton, disease group data set, automata theory, Merging, online PRTA induction approach, merge procedure, History, state clustering, online induction, medical process analysis, Real-time systems, unbounded data sets, Probabilistic real time automata, probabilistic real time automata, maximum frequent pattern based clustering, Probabilistic logic, diseases, Diseases, pattern clustering, Automata, massive data sets, Data models, Internet, dynamic process representation, online clustering algorithms, medical computing, prefix tree acceptor creation, real-world data sets]
Dimensionality Reduction on Heterogeneous Feature Space
2012 IEEE 12th International Conference on Data Mining
None
2012
Combining correlated data sources may help improve the learning performance of a given task. For example, in recommendation problems, one can combine (1) user profile database (e.g. genders, age, etc.), (2) users' log data (e.g., clickthrough data, purchasing records, etc.), and (3) users' social network (useful in social targeting) to build a recommendation model. All these data sources provide informative but heterogeneous features. For instance, user profile database usually has nominal features reflecting users' background, log data provides term-based features about users' historical behaviors, and social network database has graph relational features. Given multiple heterogeneous data sources, one important challenge is to find a unified feature subspace that captures the knowledge from all sources. To this aim, we propose a principle of collective component analysis (CoCA), in order to handle dimensionality reduction across a mixture of vector-based features and graph relational features. The CoCA principle is to find a feature subspace with maximal variance under two constraints. First, there should be consensus among the projections from different feature spaces. Second, the similarity between connected data (in any of the network databases) should be maximized. The optimal solution is obtained by solving an eigenvalue problem. Moreover, we discuss how to use prior knowledge to distinguish informative data sources, and optimally weight them in CoCA. Since there is no previous model that can be directly applied to solve the problem, we devised a straightforward comparison method by performing dimension reduction on the concatenation of the data sources. Three sets of experiments show that CoCA substantially outperforms the comparison method.
[Noise, data mining, term-based feature, user log data, user social network, heterogeneous feature space, CoCA, graph relational features, Optimization, social network database, dimensionality reduction, Databases, styling, correlated data sources, Eigenvalues and eigenfunctions, collective component analysis, recommendation problem, Social network services, vector-based feature, vectors, maximal variance, component, user profile database, multiple heterogeneous data sources, user historical behavior, social targeting, social networking (online), Data models, style, principal component analysis, formatting, Principal component analysis]
Online Maritime Abnormality Detection Using Gaussian Processes and Extreme Value Theory
2012 IEEE 12th International Conference on Data Mining
None
2012
Novelty, or abnormality, detection aims to identify patterns within data streams that do not conform to expected behaviour. This paper introduces a novelty detection technique using a combination of Gaussian Processes and extreme value theory to identify anomalous behaviour in streaming data. The proposed combination of continuous and count stochastic processes is a principled approach towards dynamic extreme value modeling that accounts for the dynamics in the time series, the streaming nature of its observation as well as its sampling process. The approach is tested on both synthetic and real data, showing itself to be effective in our primary application of maritime vessel track analysis.
[sampling process, data streams, anomalous behaviour, media streaming, Gaussian Process, Extreme Value, Outlier Detection, Mathematical model, Kernel, Context, Maritime Traffic, sampling methods, streaming data, count stochastic processes, online maritime abnormality detection, time series, Covariance matrix, marine engineering, Novelty Detection, Equations, Gaussian processes, Data models, data handling, extreme value theory, dynamic extreme value modeling, maritime vessel track analysis]
Distributed Matrix Completion
2012 IEEE 12th International Conference on Data Mining
None
2012
We discuss parallel and distributed algorithms for large-scale matrix completion on problems with millions of rows, millions of columns, and billions of revealed entries. We focus on in-memory algorithms that run on a small cluster of commodity nodes, even very large problems can be handled effectively in such a setup. Our DALS, ASGD, and DSGD++ algorithms are novel variants of the popular alternating least squares and stochastic gradient descent algorithms, they exploit thread-level parallelism, in-memory processing, and asynchronous communication. We provide some guidance on the asymptotic performance of each algorithm and investigate the performance of both our algorithms and previously proposed Map Reduce algorithms in large-scale experiments. We found that DSGD++ outperforms competing methods in terms of overall runtime, memory consumption, and scalability. Using DSGD++, we can factor a matrix with 10B entries on 16 compute nodes in around 40 minutes.
[Algorithm design and analysis, Schedules, Instruction sets, memory consumption, parallel and distributed matrix factorization, data mining, parallel algorithm, asynchronous communication, ALS, DSGD++ algorithm, thread-level parallelism, Convergence, Training, stochastic gradient descent algorithm, in-memory algorithm, Clustering algorithms, Distributed algorithms, gradient methods, commodity node, Map Reduce algorithm, parallel algorithms, least squares approximations, DALS algorithm, alternating least squares algorithm, asymptotic performance, large-scale matrix completion, in-memory processing, stochastic gradient descent, ASGD algorithm, distributed algorithm, recommender systems, distributed matrix completion]
Sparse Group Selection on Fused Lasso Components for Identifying Group-Specific DNA Copy Number Variations
2012 IEEE 12th International Conference on Data Mining
None
2012
Detecting DNA copy number variations (CNVs) from arrayCGH or genotyping-array data to correlate with cancer outcomes is crucial for understanding the molecular mechanisms underlying cancer. Previous methods either focus on detecting CNVs in each individual patient sample or common CNVs across all the patient samples. These methods ignore the discrepancies introduced by the heterogeneity in the patient samples, which implies that common CNVs might only be shared within some groups of samples instead of all samples. In this paper, we propose a latent feature model that couples sparse sample group selection with fused lasso on CNV components to identify group-specific CNVs. Assuming a given group structure on patient samples by clinical information, sparse group selection on fused lasso (SGS-FL) identifies the optimal latent CNV components, each of which is specific to the samples in one or several groups. The group selection for each CNV component is determined dynamically by an adaptive algorithm to achieve a desired sparsity. Simulation results show that SGS-FL can more accurately identify the latent CNV components when there is a reliable underlying group structure in the samples. In the experiments on arrayCGH breast cancer and bladder cancer datasets, SGS-FL detected CNV regions that are more relevant to cancer, and provided latent feature weights that can be used for better sample classification.
[sample classification, breast cancer dataset, clinical information, Optimization, DNA copy number variations, latent feature weight, sparse group selection, fused lasso, biology computing, CNV component, patient sample, adaptive algorithm, genotyping-array data, fused lasso component, group lasso, learning (artificial intelligence), Probes, pattern classification, cancer molecular mechanism, arrayCGH data, cancer outcome, sparse group learning, Matrix decomposition, group-specific DNA copy number variation, DNA, Feature extraction, latent feature model, cancer, Arrays, molecular biophysics, Cancer, bladder cancer dataset]
Kernel-Based Weighted Multi-view Clustering
2012 IEEE 12th International Conference on Data Mining
None
2012
Exploiting multiple representations, or views, for the same set of instances within a clustering framework is a popular practice for boosting clustering accuracy. However, some of the available sources may be misleading (due to noise, errors in measurement etc.) in revealing the true structure of the data, thus, their inclusion in the clustering process may have negative influence. This aspect seems to be overlooked in the multi-view literature where all representations are equally considered. In this work, views are expressed in terms of given kernel matrices and a weighted combination of the kernels is learned in parallel to the partitioning. Weights assigned to kernels are indicative of the quality of the corresponding views' information. Additionally, the combination scheme incorporates a parameter that controls the admissible sparsity of the weights to avoid extremes and tailor them to the data. Two efficient iterative algorithms are proposed that alternate between updating the view weights and recomputing the clusters to optimize the intra-cluster variance from different perspectives. The conducted experiments reveal the effectiveness of our methodology compared to other multi-view methods.
[combination scheme, iterative methods, multi-view clustering, kernel-based weighted multiview clustering framework, learning, Partitioning algorithms, Noise measurement, Optimization, Convergence, matrix algebra, Training, kernel matrices, multiple kernel learning, pattern clustering, view weights, Clustering algorithms, intracluster variance, learning (artificial intelligence), Kernel, kernel k-means, view information, iterative algorithms]
Local and Global Algorithms for Learning Dynamic Bayesian Networks
2012 IEEE 12th International Conference on Data Mining
None
2012
Learning optimal Bayesian networks (BN) from data is NP-hard in general. Nevertheless, certain BN classes with additional topological constraints, such as the dynamic BN (DBN) models, widely applied in specific fields such as systems biology, can be efficiently learned in polynomial time. Such algorithms have been developed for the Bayesian-Dirichlet (BD), Minimum Description Length (MDL), and Mutual Information Test (MIT) scoring metrics. The BD-based algorithm admits a large polynomial bound, hence it is impractical for even modestly sized networks. The MDL-and MIT-based algorithms admit much smaller bounds, but require a very restrictive assumption that all variables have the same cardinality, thus significantly limiting their applicability. In this paper, we first propose an improvement to the MDL-and MIT-based algorithms, dropping the equicardinality constraint, thus significantly enhancing their generality. We also explore local Markov blanket based algorithms for constructing BN in the context of DBN, and show an interesting result: under the faithfulness assumption, the mutual information test based local Markov blanket algorithms yield the same network as learned by the global optimization MIT-based algorithm. Experimental validation on small and large scale genetic networks demonstrates the effectiveness of our proposed approaches.
[Measurement, Algorithm design and analysis, systems biology, NP-hard, MDL-based algorithm, genetic network, optimal Bayesian network, polynomial time algorithms, mutual information test based local Markov blanket algorithm, Polynomials, topological constraint, polynomial time, minimum description length, belief networks, learning (artificial intelligence), dynamic BN model, BN classes, polynomial bound, MIT scoring metrics, Bayesian-Dirichlet, topology, DBN model, global optimization MIT-based algorithm, local algorithm, genetic algorithms, gene regulatory network, cardinality, global optimization, learning dynamic Bayesian network, Bayesian methods, dynamic Bayesian network, Markov processes, BD-based algorithm, Time complexity, Mutual information, computational complexity]
Class Probability Estimates are Unreliable for Imbalanced Data (and How to Fix Them)
2012 IEEE 12th International Conference on Data Mining
None
2012
Obtaining good probability estimates is imperative for many applications. The increased uncertainty and typically asymmetric costs surrounding rare events increases this need. Experts (and classification systems) often rely on probabilities to inform decisions. However, we demonstrate that class probability estimates attained via supervised learning in imbalanced scenarios systematically underestimate the probabilities for minority class instances, despite ostensibly good overall calibration. To our knowledge, this problem has not previously been explored. Motivated by our exposition of this issue, we propose a simple, effective and theoretically motivated method to mitigate the bias of probability estimates for imbalanced data that bags estimators calibrated over balanced bootstrap samples. This approach drastically improves performance on the minority instances without greatly affecting overall calibration. We show that additional uncertainty can be exploited via a Bayesian approach by considering posterior distributions over bagged probability estimates.
[pattern classification, imbalanced data, estimation theory, Estimation, supervised learning, Calibration, statistical distributions, class probability estimation, classification system, probability estimates, Sensitivity, Bayesian approach, class imbalance, posterior distribution, Bayes methods, Mathematical model, learning (artificial intelligence), Bagging, Logistics]
Scalable and Memory-Efficient Clustering of Large-Scale Social Networks
2012 IEEE 12th International Conference on Data Mining
None
2012
Clustering of social networks is an important task for their analysis, however, most existing algorithms do not scale to the massive size of todaya&#x0302;s social networks. A popular class of graph clustering algorithms for large-scale networks, such as PMetis, KMetis and Graclus, is based on a multilevel framework. Generally, these multilevel algorithms work reasonably well on networks with a few million vertices. However, when the network size increases to the scale of 10 million vertices or greater, the performance of these algorithms rapidly degrades. Furthermore, an inherent property of social networks, the power law degree distribution, makes these algorithms infeasible to apply to large-scale social networks. In this paper, we propose a scalable and memory-efficient clustering algorithm for large-scale social networks. We name our algorithm GEM, by mixing two key concepts of the algorithm, Graph Extraction and weighted kernel k-Means. GEM efficiently extracts a good skeleton graph from the original graph, and propagates the clustering result of the extracted graph to the rest of the network. Experimental results show that GEM produces clusters of quality comparable to or better than existing state-of-the-art graph clustering algorithms, while it is much faster and consumes much less memory. Furthermore, the parallel implementation of GEM, called PGEM, not only produces higher quality of clusters but also achieves much better scalability than most current parallel graph clustering algorithms.
[Algorithm design and analysis, memory-efficient clustering, graph theory, Twitter, parallel graph clustering algorithms, KMetis, weighted kernel k-means, Graclus, Clustering algorithms, large-scale social networks, Skeleton, graph partitioning, Kernel, PMetis, social networks, multilevel framework, scalable computing, GEM, graph clustering, pattern clustering, Memory management, graph extraction, power law degree distribution, scalable clustering, social networking (online), clustering, kernel k-means]
Ensemble Pruning via Constrained Eigen-Optimization
2012 IEEE 12th International Conference on Data Mining
None
2012
An ensemble is composed of a set of base learners that make predictions jointly. The generalization performance of an ensemble has been justified both theoretically and in practice. However, existing ensemble learning methods sometimes produce unnecessarily large ensembles, with an expense of extra computational costs and memory consumption. The purpose of ensemble pruning is to select a subset of base learners with comparable or better prediction performance. In this paper, we formulate the ensemble pruning problem into a combinatorial optimization problem with the goal to maximize the accuracy and diversity at the same time. Solving this problem exactly is computationally hard. Fortunately, we can relax and reformulate it as a constrained eigenvector problem, which can be solved with an efficient algorithm that is guaranteed to converge globally. Convincing experimental results demonstrate that this optimization based ensemble pruning algorithm outperforms the state-of-the-art heuristics in the literature.
[combinatorial mathematics, ensemble learning method, combinatorial optimization problem, Predictive models, Vectors, Complexity theory, Optimization, eigenvalues and eigenfunctions, ensemble pruning, Training, Accuracy, optimisation, constrained eigen-optimization, optimization, constrained eigenvector problem, learning (artificial intelligence), Bagging]
Handling Ambiguity via Input-Output Kernel Learning
2012 IEEE 12th International Conference on Data Mining
None
2012
Data ambiguities exist in many data mining and machine learning applications such as text categorization and image retrieval. For instance, it is generally beneficial to utilize the ambiguous unlabeled documents to learn a more robust classifier for text categorization under the semi-supervised learning setting. To handle general data ambiguities, we present a unified kernel learning framework named Input-Output Kernel Learning (IOKL). Based on our framework, we further propose a novel soft margin group sparse Multiple Kernel Learning (MKL) formulation by introducing a group kernel slack variable to each group of base input-output kernels. Moreover, an efficient block-wise coordinate descent algorithm with an analytical solution for the kernel combination coefficients is developed to solve the proposed formulation. We conduct comprehensive experiments on benchmark datasets for both semi-supervised learning and multiple instance learning tasks, and also apply our IOKL framework to a computer vision application called text-based image retrieval on the NUS-WIDE dataset. Promising results demonstrate the effectiveness of our proposed IOKL framework.
[text analysis, Uncertainty, data mining, text categorization, Training, kernel learning framework, Group Multiple Kernel Learning, kernel combination coefficients, Semi-supervised Learning, text based image retrieval, semisupervised learning setting, input output kernel learning, group kernel slack, robust classifier, learning (artificial intelligence), multiple instance learning task, Kernel, NUS-WIDE dataset, benchmark datasets, ambiguous unlabeled documents, operating system kernels, Linear programming, Multi-Instance Learning, Vectors, machine learning application, block wise coordinate descent algorithm, Support vector machines, computer vision application, multiple kernel learning formulation, Semisupervised learning, Text-based Image Retrieval, general data ambiguity, Input-Output Kernel Learning]
Efficient Learning for Hashing Proportional Data
2012 IEEE 12th International Conference on Data Mining
None
2012
Spectral hashing (SH) seeks compact binary codes of data points so that Hamming distances between codes correlate with data similarity. Quickly learning such codes typically boils down to principle component analysis (PCA). However, this is only justified for normally distributed data. For proportional data (normalized histograms), this is not the case. Due to the sum-to-unity constraint, features that are as independent as possible will not all be uncorrelated. In this paper, we show that a linear-time transformation efficiently copes with sum-to-unity constraints: first, we select a small number K of diverse data points by maximizing the volume of the simplex spanned by these prototypes; second, we represent each data point by means of its cosine similarities to the K selected prototypes. This maximum volume hashing is sensible since each dimension in the transformed space is likely to follow a von Mises (vM) distribution, and, in very high dimensions, the vM distribution closely resembles a Gaussian distribution. This justifies to employ PCA on the transformed data. Our extensive experiments validate this: maximum volume hashing outperforms spectral hashing and other state of the art techniques.
[data learning, Gaussian distribution, Optimization, Dimensionality Reduction, Semantics, principle component analysis, Binary codes, Eigenvalues and eigenfunctions, normalized histogram, learning (artificial intelligence), Proportional Data, data similarity, maximum volume hashing, Hamming distance, linear-time transformation, Spectral Hashing, cryptography, Vectors, spectral hashing, von Mises Distribution, PCA, sum-to-unity constraint, proportional data hashing, von Mises distribution, binary code, principal component analysis, Principal component analysis, simplex volume]
Defining and Evaluating Network Communities Based on Ground-Truth
2012 IEEE 12th International Conference on Data Mining
None
2012
Nodes in real-world networks organize into densely linked communities where edges appear with high concentration among the members of the community. Identifying such communities of nodes has proven to be a challenging task mainly due to a plethora of definitions of a community, intractability of algorithms, issues with evaluation and the lack of a reliable gold-standard ground-truth. In this paper we study a set of 230 large real-world social, collaboration and information networks where nodes explicitly state their group memberships. For example, in social networks nodes explicitly join various interest based social groups. We use such groups to define a reliable and robust notion of ground-truth communities. We then propose a methodology which allows us to compare and quantitatively evaluate how different structural definitions of network communities correspond to ground-truth communities. We choose 13 commonly used structural definitions of network communities and examine their sensitivity, robustness and performance in identifying the ground-truth. We show that the 13 structural definitions are heavily correlated and naturally group into four classes. We find that two of these definitions, Conductance and Triad-participation-ratio, consistently give the best performance in identifying ground-truth communities. We also investigate a task of detecting communities given a single seed node. We extend the local spectral clustering algorithm into a heuristic parameter-free community detection method that easily scales to networks with more than hundred million nodes. The proposed method achieves 30% relative improvement over current local clustering methods.
[Measurement, Communities, collaboration networks, conductance, Modularity, real-world networks, densely linked communitty, Community scoring functions, gold-standard ground-truth community, Robustness, node community detection, triad-participation-ratio, heuristic parameter-free community detection method, Network communities, Social network services, Image edge detection, social networks, Community detection, interest based social groups, local spectral clustering algorithm, information networks, network community evaluation, pattern clustering, Collaboration, social networking (online)]
Predicting Links in Multi-relational and Heterogeneous Networks
2012 IEEE 12th International Conference on Data Mining
None
2012
Link prediction is an important task in network analysis, benefiting researchers and organizations in a variety of fields. Many networks in the real world, for example social networks, are heterogeneous, having multiple types of links and complex dependency structures. Link prediction in such networks must model the influence propagating between heterogeneous relationships to achieve better link prediction performance than in homogeneous networks. In this paper, we introduce Multi-Relational Influence Propagation (MRIP), a novel probabilistic method for heterogeneous networks. We demonstrate that MRIP is useful for predicting links in sparse networks, which present a significant challenge due to the severe disproportion of the number of potential links to the number of real formed links. We also explore some factors that can inform the task of classification yet remain unexplored, such as temporal information. In this paper we make use of the temporal-related features by carefully investigating the issues of feasibility and generality. In accordance with our work in unsupervised learning, we further design an appropriate supervised approach in heterogeneous networks. Our experiments on co-authorship prediction demonstrate the effectiveness of our approach.
[multirelational influence propagation, Correlation, coauthorship prediction, multirelational network, sparse network, social network, dependency structure, heterogeneous network, MRIP probabilistic method, link prediction, Genetics, pattern classification, Time series analysis, probability, information networks, Unsupervised learning, Equations, classification task, Diseases, unsupervised learning, Link Prediction, Heterogeneous Network, network analysis, Integrated circuit modeling, Temporal Analysis]
Scalable Coordinate Descent Approaches to Parallel Matrix Factorization for Recommender Systems
2012 IEEE 12th International Conference on Data Mining
None
2012
Matrix factorization, when the matrix has missing values, has become one of the leading techniques for recommender systems. To handle web-scale datasets with millions of users and billions of ratings, scalability becomes an important issue. Alternating Least Squares (ALS) and Stochastic Gradient Descent (SGD) are two popular approaches to compute matrix factorization. There has been a recent flurry of activity to parallelize these algorithms. However, due to the cubic time complexity in the target rank, ALS is not scalable to large-scale datasets. On the other hand, SGD conducts efficient updates but usually suffers from slow convergence that is sensitive to the parameters. Coordinate descent, a classical optimization approach, has been used for many other large-scale problems, but its application to matrix factorization for recommender systems has not been explored thoroughly. In this paper, we show that coordinate descent based methods have a more efficient update rule compared to ALS, and are faster and have more stable convergence than SGD. We study different update sequences and propose the CCD++ algorithm, which updatesrank-one factors one by one. In addition, CCD++ can be easily parallelized on both multi-core and distributed systems. We empirically show that CCD++ is much faster than ALS and SGD in both settings. As an example, on a synthetic dataset with 2 billion ratings, CCD++ is 4 times faster than both SGD and ALS using a distributed system with 20 machines.
[slow convergence, convergence, ALS, classical optimization approach, Matrix factorization, matrix decomposition, update sequences, scalable coordinate descent, Convergence, optimisation, distributed systems, stochastic processes, gradient methods, Recommender systems, Charge coupled devices, parallel algorithms, synthetic dataset, web-scale datasets, updates rank-one factors, cubic time complexity, update rule, Least squares approximation, stochastic gradient descent, multicore systems, alternating least squares, large-scale datasets, SGD, large-scale problems, recommender systems, coordinate descent based methods, CCD++ algorithm, parallel matrix factorization, Acceleration, stable convergence, Time complexity, Low rank approximation, Parallelization, computational complexity]
Scalable Training of Sparse Linear SVMs
2012 IEEE 12th International Conference on Data Mining
None
2012
Sparse linear support vector machines have been widely applied to variable selection in many applications. For large data, managing the cost of training a sparse model with good predication performance is an essential topic. In this work, we propose a scalable training algorithm for large-scale data with millions of examples and features. We develop a dual alternating direction method for solving L1-regularized linear SVMs. The learning procedure simply involves quadratic programming in the same form as the standard SVM dual, followed by a soft-thresholding operation. The proposed training algorithm possesses two favorable properties. First, it is a decomposable algorithm by which a large problem can be reduced to small ones. Second, the sparsity of intermediate solutions is maintained throughout the training process. It naturally promotes the solution sparsity by soft-thresholding. We demonstrate that, by experiments, our method outperforms state-of-the-art approaches on large-scale benchmark data sets. We also show that it is well suited for training large sparse models on a distributed system.
[sparse linear SVM, large-scale linear classification, learning procedure, distributed system, Approximation methods, Convergence, Training, soft-thresholding operation, decomposable algorithm, L1-regularized linear SVM, scalable training, pattern classification, support vector machines, dual alternating direction method, Minimization, sparse classifier, predication performance, quadratic programming, Standards, Support vector machines, L1 regularization, large-scale data, optimization techniques, Data models, variable selection, sparse linear support vector machine]
Clustering Time Series Using Unsupervised-Shapelets
2012 IEEE 12th International Conference on Data Mining
None
2012
Time series clustering has become an increasingly important research topic over the past decade. Most existing methods for time series clustering rely on distances calculated from the entire raw data using the Euclidean distance or Dynamic Time Warping distance as the distance measure. However, the presence of significant noise, dropouts, or extraneous data can greatly limit the accuracy of clustering in this domain. Moreover, for most real world problems, we cannot expect objects from the same class to be equal in length. As a consequence, most work on time series clustering only considers the clustering of individual time series "behaviors," e.g., individual heart beats or individual gait cycles, and contrives the time series in some way to make them all equal in length. However, contriving the data in such a way is often a harder problem than the clustering itself. In this work, we show that by using only some local patterns and deliberately ignoring the rest of the data, we can mitigate the above problems and cluster time series of different lengths, i.e., cluster one heartbeat with multiple heartbeats. To achieve this we exploit and extend a recently introduced concept in time series data mining called shapelets. Unlike existing work, our work demonstrates for the first time the unintuitive fact that shapelets can be learned from unlabeled time series. We show, with extensive empirical evaluation in diverse domains, that our method is more accurate than existing methods. Moreover, in addition to accurate clustering results, we show that our work also has the potential to give insights into the domains to which it is applied.
[Time series analysis, time series, Vectors, Time measurement, dynamic time warping distance measure, Data mining, shapelets, unsupervised learning, Earth, unsupervised shapelet concept, pattern clustering, Clustering algorithms, unsupervised, Euclidean distance, Euclidean distance measure, clustering, clustering accuracy, time series clustering]
Self-Training with Selection-by-Rejection
2012 IEEE 12th International Conference on Data Mining
None
2012
Practical machine learning and data mining problems often face shortage of labeled training data. Self-training algorithms are among the earliest attempts of using unlabeled data to enhance learning. Traditional self-training algorithms label unlabeled data on which classifiers trained on limited training data have the highest confidence. In this paper, a self-training algorithm that decreases the disagreement region of hypotheses is presented. The algorithm supplements the training set with self-labeled instances. Only instances that greatly reduce the disagreement region of hypotheses are labeled and added to the training set. Empirical results demonstrate that the proposed self-training algorithm can effectively improve classification performance.
[Algorithm design and analysis, self labeled instances, self training algorithm, Noise, data mining, machine learning, labeled training data, limited training data, Training, Accuracy, self-training, selection by rejection, Distributed databases, Semisupervised learning, unlabeled data, semi-supervised learning, Labeling, learning (artificial intelligence), data mining problems, disagreement region]
Multiple Kernel Learning Clustering with an Application to Malware
2012 IEEE 12th International Conference on Data Mining
None
2012
With the increasing prevalence of richer, more complex data sources, learning with multiple views is becoming more widespread. Multiple kernel learning (MKL) has been developed to address this problem, but in general, the solutions provided by traditional MKL are restricted to a classification objective function. In this work, we develop a novel multiple kernel learning algorithm that is based on a spectral clustering objective function which is able to find an optimal kernel weight vector for the clustering problem. We go on to show how this optimization problem can be cast as a semidefinite program and efficiently solved using off-the-shelf interior point methods.
[invasive software, optimization problem, multiple kernel learning algorithm, classification objective function, multiple kernel learning clustering, Multiple Kernel Learning, off-the-shelf interior point method, clustering problem, Clustering algorithms, Malware, learning (artificial intelligence), Kernel, spectral clustering, pattern classification, Laplace equations, malware, Linear programming, Vectors, Clustering, Equations, mathematical programming, MKL, complex data source, optimal kernel weight vector, pattern clustering, semidefinite program, Convex Optimization]
Risks of Friendships on Social Networks
2012 IEEE 12th International Conference on Data Mining
None
2012
In this paper, we explore the risks of friends in social networks caused by their friendship patterns, by using real life social network data and starting from a previously defined risk model. Particularly, we observe that risks of friendships can be mined by analyzing users' attitude towards friends of friends. This allows us to give new insights into friendship and risk dynamics on social networks.
[risk management, friendship risk, friendship pattern, friendship, Social network services, Computational modeling, Estimation, Online Social Networks, risk model, social network, Privacy, Friendship, Clustering algorithms, social networking (online), data privacy, Labeling, risk dynamics, Risk Model, user attitude, Logistics]
Semantic Aspect Discovery for Online Reviews
2012 IEEE 12th International Conference on Data Mining
None
2012
The number of opinions and reviews about different products and services is growing online. Users frequently look for important aspects of a product or service in the reviews. Usually, they are interested in semantic (i.e., sentiment-oriented) aspects. However, extracting semantic aspects with supervised methods is very expensive. We propose a domain independent unsupervised model to extract semantic aspects, and conduct qualitative and quantitative experiments to evaluate the extracted aspects. The experiments show that our model effectively extracts semantic aspects with correlated top words. In addition, the conducted evaluation on aspect sentiment classification shows that our model outperforms other models by 5-7% in terms of macro-average F1.
[Context, aspect sentiment classification, opinion mining, pattern classification, sentiment-oriented aspect, qualitative experiments, Computational modeling, topic model, top word correlation, sentiment analysis, Airports, semantic aspect discovery, Contamination, aspect discovery, Analytical models, domain independent unsupervised model, online reviews, quantitative experiments, Semantics, Internet, Joints, F1 macro-average]
Rough Set Subspace Error-Correcting Output Codes
2012 IEEE 12th International Conference on Data Mining
None
2012
Among the proposed methods to deal with multi-class classification problems, the Error-Correcting Output Codes (ECOC) represents a powerful framework. The key factor in designing any ECOC matrix is the independency of the binary classifiers, without which the ECOC method would be ineffective. This paper proposes an efficient new approach to the ECOC framework in order to improve independency among classifiers. The underlying rationale for our work is that we design three-dimensional codematrix, where the third dimension is the feature space of the problem domain. Using rough set-based feature selection, a new algorithm, named "Rough Set Subspace ECOC (RSS-ECOC)" is proposed. We introduce the Quick Multiple Reduct algorithm in order to generate a set of reducts for a binary problem, where each reduct is used to train a dichotomizer. In addition to creating more independent classifiers, ECOC matrices with longer codes can be built. The numerical experiments in this study compare the classification accuracy of the proposed RSS-ECOC with classical ECOC, one-versus-one, and one-versus-all methods on 24 UCI datasets. The results show that the proposed technique increases the classification accuracy in comparison with the state of the art coding methods.
[Algorithm design and analysis, rough set subspace error-correcting output code matrix, pattern classification, dichotomizer training, error correction codes, rough set-based feature selection, multiclass classification problem, Rough Set, three-dimensional codematrix, Encoding, Vectors, classification accuracy, Decoding, binary classifier, Data mining, rough set subspace ECOC, Multiclass classification, matrix algebra, Training, quick multiple reduct algorithm, Accuracy, rough set theory, Error Correcting Output Codes, Feature subspace]
Co-clustering of Multi-view Datasets: A Parallelizable Approach
2012 IEEE 12th International Conference on Data Mining
None
2012
In many applications, entities of the domain are described through different views that clustering methods often process one by one. We introduce here the architecture MVSim, that is able to deal simultaneously with all the information contained in such multi-view datasets by using several instances of a co-similarity algorithm. We show that this architecture provides better results than both single-view and multi-view approaches and that it can be easily parallelized thus reducing both time and space complexities of the computations.
[Damping, Symmetric matrices, multiview dataset coclustering, Clustering methods, clustering method, time complexity, Complexity theory, parallel processing, MVSim architecture, cosimilarity algorithm, pattern clustering, Clustering algorithms, Computer architecture, Silicon, Multi-view and Similarity Learning, parallelizable approach, space complexity, Co-clustering, computational complexity]
Multi-task Learning for Classifying Proteins Using Dual Hierarchies
2012 IEEE 12th International Conference on Data Mining
None
2012
Several biological databases organize information in taxonomies/hierarchies. These databases differ in terms of curation process, input data, coverage and annotation errors. SCOP and CATH are examples of two databases that classify proteins hierarchically into structurally related groups based on experimentally determined structures. Given the large number of protein sequences with unavailable structure, there is a need to develop prediction methods to classify protein sequences into structural classes. We have developed a novel classification approach that utilizes the underlying relationships across multiple hierarchical source databases within a multi-task learning (MTL) framework. MTL is used to simultaneously learn multiple related tasks, and has been shown to improve generalization performance. Specifically, we have developed and evaluated an MTL approach for predicting the structural class, as defined by two hierarchical databases, CATH and SCOP, using protein sequence information only. We define one task per node of the hierarchies and formulate the MTL problem as a combination of these binary classification tasks. Our experimental evaluation demonstrates that the MTL approach that integrates both the hierarchies outperforms the base-line approach that trains independent models per task, as well as a MTL approach that integrates tasks across a single hierarchical database. We also performed extensive experiments that evaluate different regularization penalties and incorporate different task relationships that achieve superior classification performance.
[Context, biological databases, proteins classification, annotation errors, Topology, database management systems, CATH, Optimization, Proteins, Training, multi-task learning, dual hierarchies, Databases, biology computing, taxonomies, proteins, multiprogramming, SCOP, Mathematical model, learning (artificial intelligence), curation process, protein sequences, multiple hierarchical source databases]
Privacy-Preserving SimRank over Distributed Information Network
2012 IEEE 12th International Conference on Data Mining
None
2012
Information network analysis has drawn a lot attention in recent years. Among all the aspects of network analysis, similarity measure of nodes has been shown useful in many applications, such as clustering, link prediction and community identification, to name a few. As linkage data in a large network is inherently sparse, it is noted that collecting more data can improve the quality of similarity measure. This gives different parties a motivation to cooperate. In this paper, we address the problem of link-based similarity measure of nodes in an information network distributed over different parties. Concerning the data privacy, we propose a privacy-preserving Sim Rank protocol based on fully-homomorphic encryption to provide cryptographic protection for the links.
[link prediction application, Ciphers, Protocols, cryptographic protocols, distributed information network, Similarity, privacy-preserving SimRank protocol, information network analysis, cryptographic protection, Vectors, information analysis, Encryption, link-based similarity measure, information networks, node similarity measure, data collection, Privacy, community identification application, fully-homomorphic encryption, clustering application, Motion pictures, data privacy, Joints]
Adapting Component Analysis
2012 IEEE 12th International Conference on Data Mining
None
2012
A main problem in machine learning is to predict the response variables of a test set given the training data and its corresponding response variables. A predictive model can perform satisfactorily only if the training data is an appropriate representative of the test data. This is usually reflected in the assumption that the training data and the test data are drawn from the same underlying probability distribution. However, the assumption may not be correct in many applications for various reasons. We propose a method based on kernel distribution embedding and Hilbert-Schmidt Independence Criterion (HSIC) to address this problem. The proposed method explores a new representation of the data in a new feature space with two properties: (i) the distributions of the training and the test data sets are as close as possible in the new feature space, and (ii) the important structural information of the data is preserved. The algorithm can reduce the dimensionality of the data while it preserves the aforementioned properties and therefore it can be seen as a dimensionality reduction method as well. Our method has a closed-form solution and the experimental results show that it works well in practice.
[Algorithm design and analysis, Error analysis, data representation, HSIC, Domain Adaptation, Predictive models, Probability distribution, data dimensionality, statistical distributions, response variable, Hilbert-Schmidt independence criterion, Training, test data, dimensionality reduction, Training data, training data, probability distribution, kernel distribution embedding, data structures, learning (artificial intelligence), component analysis, Kernel, predictive model, machine learning, Kernel Embedding, Hilbert- Schmidt independence criterion, data handling]
Geodesic Based Semi-supervised Multi-manifold Feature Extraction
2012 IEEE 12th International Conference on Data Mining
None
2012
Manifold learning is an important feature extraction approach in data mining. This paper presents a new semi-supervised manifold learning algorithm, called Multi-Manifold Discriminative Analysis (Multi-MDA). The proposed method is designed to explore the discriminative information hidden in geodesic distances. The main contributions of the proposed method are: 1) we propose a semi-supervised graph construction method which can effectively capture the multiple manifolds structure of the data, 2) each data point is replaced with an associated feature vector whose elements are the graph distances from it to the other data points. Information of the nonlinear structure is contained in the feature vectors which are helpful for classification, 3) we propose a new semi-supervised linear dimension reduction method for feature vectors which introduces the class information into the manifold learning process and establishes an explicit dimension reduction mapping. Experiments on benchmark data sets are conducted to show the effectiveness of the proposed method.
[Algorithm design and analysis, semisupervised graph construction method, semisupervised linear dimension reduction method, graph theory, geodesic based semisupervised multimanifold feature extraction, data mining, multimanifold discriminative analysis, geodesic distance, data structure, Vectors, explicit dimension reduction mapping, manifold learning, discriminative information, feature vector, graph distance, Manifolds, Training, Accuracy, vectors, feature extraction, Multi-MDA algorithm, Feature extraction, learning (artificial intelligence), Principal component analysis]
Self-Taught Active Learning from Crowds
2012 IEEE 12th International Conference on Data Mining
None
2012
The emergence of social tagging and crowdsourcing systems provides a unique platform where multiple weak labelers can form a crowd to fulfill a labeling task. Yet crowd labelers are often noisy, inaccurate, and have limited labeling knowledge, and worst of all, they act independently without seeking complementary knowledge from each other to improve labeling performance. In this paper, we propose a Self-Taught Active Learning (STAL) paradigm, where imperfect labelers are able to learn complementary knowledge from one another to expand their knowledge sets and benefit the underlying active learner. We employ a probabilistic model to characterize the knowledge of each labeler through which a weak labeler can learn complementary knowledge from a stronger peer. As a result, the self-taught active learning process eventually helps achieve high classification accuracy with minimized labeling costs and labeling errors.
[Uncertainty, crowd, probability, self-taught, Educational institutions, classification accuracy, probabilistic model, Computer science, Learning systems, active learner, self-taught active learning, Graphical models, active learning, labeling error minimisation, labeling task, labeling cost minimisation, Labeling, Reliability, learning (artificial intelligence), social tagging, crowdsourcing system]
Simultaneously Combining Multi-view Multi-label Learning with Maximum Margin Classification
2012 IEEE 12th International Conference on Data Mining
None
2012
Multiple feature views arise in various important data classification scenarios. However, finding a consensus feature view from multiple feature views for a classifier is still a challenging task. We present a new classification framework using the multi-label correlation information to address the problem of simultaneously combining multiple feature views and maximum margin classification. Under this framework, we propose a novel algorithm that iteratively computes the multiple view feature mapping matrices, the consensus feature view representation, and the coefficients of the classifier. Extensive experimental evaluations demonstrate the effectiveness and promise of this framework as well as the algorithm for discovering a consensus view from multiple feature views.
[pattern classification, consensus representation, Correlation, label dependence maximization, Linear programming, Data mining, consensus feature view, Optimization, matrix algebra, Training, multilabel correlation information, multi-view learning, maximum margin classification, Training data, data classification, multiple view feature mapping matrices, multiview multilabel learning, feature mapping, Feature extraction, learning (artificial intelligence), consensus feature view representation, classifier coefficient]
Mining Permission Request Patterns from Android and Facebook Applications
2012 IEEE 12th International Conference on Data Mining
None
2012
Android and Facebook provide third-party applications with access to users' private data and the ability to perform potentially sensitive operations (e.g., post to a user's wall or place phone calls). As a security measure, these platforms restrict applications' privileges with permission systems: users must approve the permissions requested by applications before the applications can make privacy-or security-relevant API calls. However, recent studies have shown that users often do not understand permission requests and are unsure of which permissions are typical for applications. As a first step towards simplifying permission systems, we cluster a corpus of 188,389 Android applications and 27,029 Facebook applications to find patterns in permission requests. Using a method for Boolean matrix factorization to find overlapping clusters of permissions, we find that Facebook permission requests follow a clear structure that can be fitted well with only five patterns, whereas Android applications demonstrate more complex permission requests. We also find that low-reputation applications often deviate from the permission request patterns that we identified for high-reputation applications, which suggests that permission request patterns can be indicative of user satisfaction or application quality.
[low-reputation application, application program interfaces, Humanoid robots, data mining, privacy-or security-relevant API call, application quality, matrix decomposition, private data, Training, Smartphones, Permissions, permission system, Hardware, Malware, Facebook, security measure, pattern mining, Boolean algebra, overlapping permission clusters, Unsupervised learning, Android, third-party application, Boolean matrix factorization, operating systems (computers), social networking (online), data privacy, Androids, permission request pattern mining, Smart phones, high-reputation application, user satisfaction]
Estimating Local Information Trustworthiness via Multi-source Joint Matrix Factorization
2012 IEEE 12th International Conference on Data Mining
None
2012
We investigate how to estimate information trustworthiness by considering multiple information sources jointly in a latent matrix space. We particularly focus on user review and recommendation systems, as there are multiple platforms where people can rate items and services that they have purchased, and many potential customers rely on these opinions to make decisions. Information trustworthiness is a serious problem because ratings are generated freely by end-users so that many stammers take advantage of freedom of speech to promote their business or damage reputation of competitors. We propose to simply use customer ratings to estimate each individual source's reliability by exploring correlations among multiple sources. Ratings of items are provided by users of diverse tastes and styles, and thus may appear noisy and conflicting across sources, however, they share some underlying common behavior. Therefore, we can group users based on their opinions, and a source is reliable on an item if its opinions given by latent groups are consistent across platforms. Inspired by this observation, we solve the problem by a two-step model -- a joint matrix factorization procedure followed by reliability score computation. We propose two effective approaches to decompose rating matrices as the products of group membership and group rating matrices, and then compute consistency degrees from group rating matrices as source reliability scores. We conduct experiments on both synthetic data and real user ratings collected from Orbitz, Priceline and Trip Advisor on all the hotels in Las Vegas and New York City. Results show that the proposed method is able to give accurate estimates of source reliability and thus successfully identify inconsistent, conflicting and unreliable information.
[Algorithm design and analysis, Trustworthiness, Joint Matrix Factorization, Recommendation System, matrix decomposition, user interfaces, Optimization, multisource joint matrix factorization, recommendation system, Convergence, group rating matrix, latent matrix space, New York City, Orbitz, Priceline, Cities and towns, local information trustworthiness estimation, item rating, user review system, Joints, reliability score computation, Las Vegas, Trip Advisor, group membership matrix, Matrix decomposition, recommender systems, information source, data privacy, Reliability, customer rating]
Towards Active Learning on Graphs: An Error Bound Minimization Approach
2012 IEEE 12th International Conference on Data Mining
None
2012
Active learning on graphs has received increasing interest in the past years. In this paper, we propose a \\textit{nonadaptive} active learning approach on graphs, based on generalization error bound minimization. In particular, we present a data-dependent error bound for a graph-based learning method, namely learning with local and global consistency (LLGC). We show that the empirical transductive Rademacher complexity of the function class for LLGC provides a natural criterion for active learning. The resulting active learning approach is to select a subset of nodes on a graph such that the empirical transductive Rademacher complexity of LLGC is minimized. We propose a simple yet effective sequential optimization algorithm to solve it. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art active learning methods on graphs.
[Algorithm design and analysis, error bound minimization approach, graph theory, generalization error bound minimization, LLGC learning, Complexity theory, sequential optimization algorithm, Optimization, Sequential Optimization, Learning systems, active learning, empirical transductive Rademacher complexity, learning (artificial intelligence), Minimization, Vectors, generalisation (artificial intelligence), graph-based learning method, Generalization Error Bound, learning-with-local-and-global consistency, Active Learning, Graph, data-dependent error bound, Machine learning, minimisation, computational complexity]
Transductive Representation Learning for Cross-Lingual Text Classification
2012 IEEE 12th International Conference on Data Mining
None
2012
In cross-lingual text classification problems, it is costly and time-consuming to annotate documents for each individual language. To avoid the expensive re-labeling process, domain adaptation techniques can be applied to adapt a learning system trained in one language domain to another language domain. In this paper we develop a transductive subspace representation learning method to address domain adaptation for cross-lingual text classifications. The proposed approach is formulated as a nonnegative matrix factorization problem and solved using an iterative optimization procedure. Our empirical study on cross-lingual text classification tasks shows the proposed approach consistently outperforms a number of comparison methods.
[iterative methods, text analysis, domain adaptation technique, representation learning, Linear programming, Vectors, learning system, matrix decomposition, classification, transductive subspace representation learning method, Optimization, Standards, Training, Learning systems, Accuracy, optimisation, iterative optimization, language domain, cross-lingual text classification, relabeling process, nonnegative matrix factorization problem, learning (artificial intelligence), domain adaptation, document annotation]
IceCube: Efficient Targeted Mining in Data Cubes
2012 IEEE 12th International Conference on Data Mining
None
2012
We address the problem of mining targeted association rules over multidimensional market-basket data. Here, each transaction has, in addition to the set of purchased items, ancillary dimension attributes associated with it. Based on these dimensions, transactions can be visualized as distributed over cells of an n-dimensional cube. In this framework, a targeted association rule is of the form {X &#x2192; Y}<sub>R</sub>, where R is a convex region in the cube and X &#x2192; Y is a traditional association rule within region R. We first describe the TOARM algorithm, based on classical techniques, for identifying targeted association rules. Then, we discuss the concepts of bottom-up aggregation and cubing, leading to the Cell Union technique. This approach is further extended, using notions of cube-count interleaving and credit-based pruning, to derive the Ice Cube algorithm. Our experiments demonstrate that Ice Cube consistently provides the best execution time performance, especially for large and complex data cubes.
[Algorithm design and analysis, bottom-up aggregation concept, cell union technique, data mining, Generators, marketing data processing, Association rules, cube-count interleaving notion, association rule mining, targeted association rule mining, ancillary dimension attribute, Itemsets, credit-based pruning notion, Aggregates, TOARM algorithm, Filtering algorithms, cubing concept, data cube, multidimensional market-basket data, IceCube algorithm, localized rules]
A New Anomaly Detection Algorithm Based on Quantum Mechanics
2012 IEEE 12th International Conference on Data Mining
None
2012
The primary originality of this paper lies at the fact that we have made the first attempt to apply quantum mechanics theory to anomaly (outlier) detection in high-dimensional datasets for data mining. We propose Fermi Density Descriptor (FDD) which represents the probability of measuring a fermion at a specific location for anomaly detection. We also quantify and examine different Laplacian normalization effects and choose the best one for anomaly detection. Both theoretical proof and quantitative experiments demonstrate that our proposed FDD is substantially more discriminative and robust than the commonly-used algorithms.
[Laplace equations, anomaly detection algorithm, Quantum Mechanics, data mining, Laplacian normalization effect, quantum theory, outlier detection, Equations, Manifolds, Anomaly Detection, security of data, Quantum mechanics, quantum mechanics theory, Fermi density descriptor, Eigenvalues and eigenfunctions, Robustness, fermion measurement probability, statistical analysis, Distribution functions]
Towards Automatic Image Understanding and Mining via Social Curation
2012 IEEE 12th International Conference on Data Mining
None
2012
The amount and variety of multimedia data such as images, movies and music available on over social networks are increasing rapidly. However, the ability to analyze and exploit these unorganized multimedia data remains inadequate, even with state-of-the-art media processing techniques. Our finding in this paper is that the emerging social curation service is a promising information source for the automatic understanding and mining of images distributed and exchanged via social media. One remarkable virtue of social curation service datasets is that they are weakly supervised: the content in the service is manually collected, selected and maintained by users. This is very different from other social information sources, and we can utilize this characteristics for media content mining without expensive media processing techniques. In this paper we present a machine learning system for predicting view counts of images in social curation data as the first step to automatic image content evaluation. Our experiments confirm that the simple features extracted from a social curation corpus are much superior in terms of count prediction than the gold-standard image features of computer vision research.
[Social curation, machine learning system, data mining, Data mining, multimedia computing, movies, social network, Histograms, music, feature extraction, Motion pictures, learning (artificial intelligence), social media, automatic image understanding, automatic image understanding and evaluation, Context, image processing, Computer vision, media content mining, automatic image content evaluation, multimedia data, Media, social curation service, view count prediction, regression, Feature extraction, social networking (online), automatic image mining]
Learning to Refine an Automatically Extracted Knowledge Base Using Markov Logic
2012 IEEE 12th International Conference on Data Mining
None
2012
A number of text mining and information extraction projects such as Text Runner and NELL seek to automatically build knowledge bases from the rapidly growing amount of information on the web. In order to scale to the size of the web, these projects often employ ad hoc heuristics to reason about uncertain and contradictory information rather than reasoning jointly about all candidate facts. In this paper, we present a Markov logic-based system for cleaning an extracted knowledge base. This allows a scalable system such as NELL to take advantage of joint probabilistic inference, or, conversely, allows Markov logic to be applied to a web scale problem. Our system uses only the ontological constraints and confidence values of the original system, along with human-labeled data if available. The labeled data can be used to calibrate the confidence scores from the original system or learn the effectiveness of individual extraction patterns. To achieve scalability, we introduce a neighborhood grounding method that only instantiates the part of the network most relevant to the given query. This allows us to partition the knowledge cleaning task into tractable pieces that can be solved individually. In experiments on NELL's knowledge base, we evaluate several variants of our approach and find that they improve both F1 and area under the precision-recall curve.
[knowledge base, text analysis, Web information, data mining, Ontologies, ontological constraint, confidence value, Information extraction, Data mining, neighborhood grounding method, formal logic, query processing, NELL, precision-recall curve, confidence score, Training data, knowledge based systems, probabilistic inference, text mining, Joints, ontology, ad hoc heuristics, Knowledge based systems, query, text runner, Markov logic-based system, inference mechanisms, human-labeled data, extraction pattern, knowledge base extraction, knowledge cleaning task, information extraction, Markov processes, ontologies (artificial intelligence), Markov logic, Internet, Web scale problem, Logistics]
IRIE: Scalable and Robust Influence Maximization in Social Networks
2012 IEEE 12th International Conference on Data Mining
None
2012
Influence maximization is the problem of selecting top k seed nodes in a social network to maximize their influence coverage under certain influence diffusion models. In this paper, we propose a novel algorithm IRIE that integrates the advantages of influence ranking (IR) and influence estimation (IE) methods for influence maximization in both the independent cascade (IC) model and its extension IC-N that incorporates negative opinion propagations. Through extensive experiments, we demonstrate that IRIE matches the influence coverage of other algorithms while scales much better than all other algorithms. Moreover IRIE is much more robust and stable than other algorithms both in running time and memory usage for various density of networks and cascade size. It runs up to two orders of magnitude faster than other state-of-the-art algorithms such as PMIA for large networks with tens of millions of nodes and edges, while using only a fraction of memory.
[Greedy algorithms, Algorithm design and analysis, Computational modeling, Social network services, independent cascade model, IC model, negative opinion propagation, belief maintenance, influence diffusion model, social network, marketing, optimisation, viral marketing, IRIE algorithm, influence estimation method, social network analysis, social networking (online), influence ranking method, influence maximization, social network mining, Mathematical model, Integrated circuit modeling]
Decision Theory for Discrimination-Aware Classification
2012 IEEE 12th International Conference on Data Mining
None
2012
Social discrimination (e.g., against females) arising from data mining techniques is a growing concern worldwide. In recent years, several methods have been proposed for making classifiers learned over discriminatory data discrimination-aware. However, these methods suffer from two major shortcomings: (1) They require either modifying the discriminatory data or tweaking a specific classification algorithm and (2) They are not flexible w.r.t. discrimination control and multiple sensitive attribute handling. In this paper, we present two solutions for discrimination-aware classification that neither require data modification nor classifier tweaking. Our first and second solutions exploit, respectively, the reject option of probabilistic classifier(s) and the disagreement region of general classifier ensembles to reduce discrimination. We relate both solutions with decision theory for better understanding of the process. Our experiments using real-world datasets demonstrate that our solutions outperform existing state-of-the-art methods, especially at low discrimination which is a significant advantage. The superior performance coupled with flexible control over discrimination and easy applicability to multiple sensitive attributes makes our solutions an important step forward in practical discrimination-aware classification.
[pattern classification, discrimination-aware classification, decision theory, discrimination control, Communities, data mining, probability, social discrimination, Probabilistic logic, sensitive attribute handling, probabilistic classifier, Data mining, classification, Standards, Accuracy, Decision trees, Logistics, ensembles]
Deep Learning to Hash with Multiple Representations
2012 IEEE 12th International Conference on Data Mining
None
2012
Hashing seeks an embedding of high-dimensional objects into a similarity-preserving low-dimensional Hamming space such that similar objects are indexed by binary codes with small Hamming distances. A variety of hashing methods have been developed, but most of them resort to a single view (representation) of data. However, objects are often described by multiple representations. For instance, images are described by a few different visual descriptors (such as SIFT, GIST, and HOG), so it is desirable to incorporate multiple representations into hashing, leading to multi-view hashing. In this paper we present a deep network for multi-view hashing, referred to as deep multi-view hashing, where each layer of hidden nodes is composed of view-specific and shared hidden nodes, in order to learn individual and shared hidden spaces from multiple views of data. Numerical experiments on image datasets demonstrate the useful behavior of our deep multi-view hashing (DMVH), compared to recently-proposed multi-modal deep network as well as existing shallow models of hashing.
[Visualization, hashing, hashing method, Training, deep learning, multi-view learning, data single view representation, Binary codes, learning (artificial intelligence), shared hidden node, Hamming distance, Computational modeling, view-specific hidden node, Linear programming, cryptography, restricted Boltzmann machines, multiple representation, visual descriptor, multiview hashing, image dataset, Machine learning, image representation, similarity-preserving low-dimensional Hamming space, binary code, harmonium, image coding, high-dimensional object]
Low Dimensional Localized Clustering (LDLC)
2012 IEEE 12th International Conference on Data Mining
None
2012
In the space of high-dimensional data, it is generally reasonable to assume that the data points are on (or close to) one or more submanifolds. Each of these submanifolds can be modeled by a number of linear subspaces. This is in fact the main intuition behind a majority of subspace clustering algorithms. In many cases, however, the subspaces computed by these algorithms consist of disconnected subsets of the underlying submanifolds and therefore, do not form localized and compact clusters. To address this problem, we propose "Low Dimensional Localized Clustering (LDLC)\
[local dimensionality reduction, subspace clustering algorithm, topology, high-dimensional data, Linear programming, Partitioning algorithms, semisupervised classification, Clustering, Image reconstruction, Manifolds, VQPCA, Dimensionality Reduction, Manifold, pattern clustering, Measurement uncertainty, Clustering algorithms, Data visualization, linear subspace, submanifold, k-means, data visualization, low dimensional localized clustering]
A Semi-definite Positive Linear Discriminant Analysis and Its Applications
2012 IEEE 12th International Conference on Data Mining
None
2012
Linear Discriminant Analysis (LDA) is widely used for dimension reduction in classification tasks. However, standard LDA formulation is not semi definite positive (s.d.p), and thus it is difficult to obtain the global optimal solution when standard LDA formulation is combined with other loss functions or graph embedding. In this paper, we present an alternative approach to LDA. We rewrite the LDA criterion as a convex formulation (semi-definite positive LDA, i.e., sdpLDA) using the largest eigen-value of the generalized eigen-value problem of standard LDA. We give applications by incorporating sdpLDA as a regularization term into discriminant regression analysis. Another application is to incorporate sdpLDA into standard Laplacian embedding, which utilizes the supervised information to improve the Laplacian embedding performance. Proposed sdpLDA formulation can be used for both multi-class classification tasks. Extensive experiments results on 10 multi-class datasets indicate promising results of proposed method.
[semi-definite positive, standard Laplacian embedding, graph theory, regression analysis, classification tasks, LDA, graph embedding, global optimal solution, dimension reduction, eigenvalues and eigenfunctions, multi-label, Accuracy, multi-class, discriminant regression analysis, semi-definite positive linear discriminant analysis, sdpLDA, Eigenvalues and eigenfunctions, Convex functions, Kernel, eigenvalue, pattern classification, Laplace equations, data analysis, Linear regression, convex formulation, kernel LDA, convex programming, Standards, LDA criterion]
Predicting Directed Links Using Nondiagonal Matrix Decompositions
2012 IEEE 12th International Conference on Data Mining
None
2012
We present a method for trust prediction based on no diagonal decompositions of the asymmetric adjacency matrix of a directed network. The method we propose is based on a no diagonal decomposition into directed components (DEDICOM), which we use to learn the coefficients of a matrix polynomial of the network's adjacency matrix. We show that our method can be used to compute better low-rank approximations to a polynomial of a network's adjacency matrix than using the singular value decomposition, and that a higher precision can be achieved at the task of predicting directed links than by undirected or bipartite methods.
[trust, Symmetric matrices, trust prediction, low-rank approximations, asymmetric adjacency matrix, directed network, undirected methods, decomposition into directed components, matrix decomposition, Matrix decomposition, Approximation methods, nondiagonal decompositions, matrix polynomial coefficients, Training, polynomial matrices, directed graphs, network adjacency matrix, Eigenvalues and eigenfunctions, Polynomials, bipartite methods, trusted computing, singular value decomposition, Singular value decomposition]
Rapid and Robust Denoising of Pyrosequenced Amplicons for Metagenomics
2012 IEEE 12th International Conference on Data Mining
None
2012
Metagenomic sequencing has become a crucial tool for obtaining a gene catalogue of operational taxonomic units (OTUs) in a microbial community. High-throughput pyrosequencing is a next-generation sequencing technique very popular in microbial community analysis due to its longer read length compared to alternative methods. Computational tools are inevitable to process raw data from pyrosequencers, and in particular, noise removal is a critical data-mining step to obtain robust sequence reads. However, the slow rate of existing denoisers has bottlenecked the whole pyrosequencing process, let alone hindering efforts to improve robustness. To address these, we propose a new approach that can accelerate the denoising process substantially. By using our approach, it now takes only about 2 hours to denoise 62,873 pyrosequenced amplicons from a mixture of 91 full-length 16S rRNA clones. It would otherwise take nearly 2.5 days if existing software tools were used. Furthermore, our approach can effectively reduce overestimating the number of OTUs, producing 6.7 times fewer species-level OTUs on average than a state-of-the-art alternative under the same condition. Leveraged by our approach, we hope that metagenomic sequencing will become an even more appealing tool for microbial community analysis.
[metagenomic sequencing, time 2 hour, Instruction sets, sequence read, Noise, Noise reduction, Communities, Graphics processing units, data mining, GPU, high-throughput pyrosequencing technique, biology computing, 16S rRNA clone, genomics, Robustness, cluster analysis, operational taxonomic unit, data processing, pyrosequencing, graphics processing units, gene catalogue, pyrosequenced amplicons denoising, biomedical informatics, microbial community, software tool, metagenomics, graphics processing unit, amplicons, Acceleration, molecular biophysics, noise removal]
CT-IC: Continuously Activated and Time-Restricted Independent Cascade Model for Viral Marketing
2012 IEEE 12th International Conference on Data Mining
None
2012
Influence maximization problem with applications to viral marketing has gained much attention. Underlying influence diffusion models affect influence maximizing nodes because they focus on difference aspect of influence diffusion. Nevertheless, existing diffusion models overlook two important aspects of real-world marketing - continuous trials and time restriction. This paper proposes a new realistic influence diffusion model called Continously activated and Time-restricted IC (CT-IC) model which generalizes the IC model by embedding the above two aspects. We first prove that CT-IC model satisfies two crucial properties - monotonicity and submodularity. We then provide an efficient method for calculating exact influence spread when a social network is restricted to a directed tree and a simple path. Finally, we propose a scalable algorithm for influence maximization under CT-IC model called CT-IPA. Our experiments show that CT-IC model provides seeds of higher influence spread than IC model and CT-IPA is four orders of magnitude faster than the greedy algorithm while providing similar influence spread to the greedy algorithm.
[Greedy algorithms, CT-IC, Computational modeling, Social network services, trees (mathematics), monotonicity property, continuous trials, influence diffusion model, marketing data processing, time-restricted independent cascade model, social network, viral marketing social networks, continuously activated cascade model, influence maximization problem, viral marketing, submodularity property, social networking (online), time restriction, directed tree, Data models, influence maximization, influence diffusion, Mathematical model, Integrated circuit modeling]
Fast Kernel Sparse Representation Approaches for Classification
2012 IEEE 12th International Conference on Data Mining
None
2012
Sparse representation involves two relevant procedures - sparse coding and dictionary learning. Learning a dictionary from data provides a concise knowledge representation. Learning a dictionary in a higher feature space might allow a better representation of a signal. However, it is usually computationally expensive to learn a dictionary if the numbers of training data and(or) dimensions are very large using existing algorithms. In this paper, we propose a kernel dictionary learning framework for three models. We reveal that the optimization has dimension-free and parallel properties. We devise fast active-set algorithms for this framework. We investigated their performance on classification. Experimental results show that our kernel sparse representation approaches can obtain better accuracy than their linear counterparts. Furthermore, our active-set algorithms are faster than the existing interior-point and proximal algorithms.
[Dictionaries, kernel dictionary learning framework, set theory, dictionary learning, Optimization, Training, Accuracy, sparse coding procedure, non-negative least squares, data structures, Mathematical model, learning (artificial intelligence), Kernel, interior-point algorithm, kernel sparse representation, dictionary learning procedure, fast active-set algorithm, signal representation, sparse coding, Equations, signal classification, classification performance, knowledge representation, kernel sparse representation approach, proximal algorithm, $l_1$ regularization]
Estimating the Expected Effectiveness of Text Classification Solutions under Subclass Distribution Shifts
2012 IEEE 12th International Conference on Data Mining
None
2012
Automated text classification is one of the most important learning technologies to fight information overload. However, the information society is not only confronted with an information flood but also with an increase in "information volatility\
[text analysis, unknown distributions, learning technology, statistical distributions, Concept Drift, information volatility, Clustering algorithms, Classification, Model Selection, Mathematical model, text classification solution, subclass distribution shift, repetitive resampling, pattern classification, sampling methods, information flood, expected effectiveness estimation, Estimation, Media, model selection, Vectors, topic popularity, margin distribution, probabilistic lower bound, Standards, information overload, pattern clustering, Machine learning, clustering]
Metric Learning from Relative Comparisons by Minimizing Squared Residual
2012 IEEE 12th International Conference on Data Mining
None
2012
Recent studies [1] -- [5] have suggested using constraints in the form of relative distance comparisons to represent domain knowledge: d(a, b) &lt;; d(c, d) where d(&#x00B7;) is the distance function and a, b, c, d are data objects. Such constraints are readily available in many problems where pairwise constraints are not natural to obtain. In this paper we consider the problem of learning a Mahalanobis distance metric from supervision in the form of relative distance comparisons. We propose a simple, yet effective, algorithm that minimizes a convex objective function corresponding to the sum of squared residuals of constraints. We also extend our model and algorithm to promote sparsity in the learned metric matrix. Experimental results suggest that our method consistently outperforms existing methods in terms of clustering accuracy. Furthermore, the sparsity extension leads to more stable estimation when the dimension is high and only a small amount of supervision is given.
[Measurement, Symmetric matrices, distance function, pairwise constraint, Linear programming, convex programming, squared residual minimisation, Covariance matrix, relative comparisons, Convergence, relative distance comparison, Accuracy, metric learning, pattern clustering, Clustering algorithms, learned metric matrix, clustering accuracy, learning (artificial intelligence), minimisation, Mahalanobis metric, Mahalanobis distance metric learning, convex objective function minimisation]
Direct Discovery of High Utility Itemsets without Candidate Generation
2012 IEEE 12th International Conference on Data Mining
None
2012
Utility mining emerged recently to address the limitation of frequent itemset mining by introducing interestingness measures that reflect both the statistical significance and the user's expectation. Among utility mining problems, utility mining with the itemset share framework is a hard one as no anti-monotone property holds with the interestingness measure. The state-of-the-art works on this problem all employ a two-phase, candidate generation approach, which suffers from the scalability issue due to the huge number of candidates. This paper proposes a high utility itemset growth approach that works in a single phase without generating candidates. Our basic approach is to enumerate itemsets by prefix extensions, to prune search space by utility upper bounding, and to maintain original utility information in the mining process by a novel data structure. Such a data structure enables us to compute a tight bound for powerful pruning and to directly identify high utility itemsets in an efficient and scalable way. We further enhance the efficiency significantly by introducing recursive irrelevant item filtering with sparse data, and a lookahead strategy with dense data. Extensive experiments on sparse and dense, synthetic and real data suggest that our algorithm outperforms the state-of-the-art algorithms over one order of magnitude.
[statistical significance, Scalability, data mining, prefix extension, high utility itemset discovery, Utility mining, information filtering, Electronic mail, high utility itemset growth approach, Data mining, recursive irrelevant item filtering, Itemsets, candidate generation approach, frequent itemsets, interestingness measure, dense data, utility mining, sparse data, pattern mining, user expectation, lookahead strategy, Educational institutions, frequent itemset mining, high utility itemsets, Upper bound, utility upper bounding]
Progressive Mining of Transition Dynamics for Autonomous Control
2012 IEEE 12th International Conference on Data Mining
None
2012
Autonomous agents are emerging in diverse areas and many rely on reinforcement learning (RL) to learn optimal control policies by acting in the environment. This form of learning generates large amounts of transition dynamics data, which can be mined to improve the agent's understanding of the environment. There could be many uses for this data, here we focus on mining it to identify a relevant feature subspace. This is vital since RL performs poorly in high-dimensional spaces, such as those that autonomous agents would commonly face in real-world problems. This paper demonstrates the necessity and feasibility of integrating data mining into the learning process while an agent is learning, enabling it to learn to act by both acting and understanding. Doing so requires overcoming challenges regarding data quantity and quality, and difficulty measuring feature relevance with respect to the control policy. We propose the progressive mining framework to address these challenges by relying on cyclic interaction between data mining and RL. We show that a feature selection algorithm developed under this framework, PROFESS, can improve RL scalability better than a competing approach.
[Algorithm design and analysis, multi-agent systems, Heuristic algorithms, data mining, optimal control, transition dynamics data, Data mining, progressive feature selection, transition dynamics progressive mining, reinforcement learning, feature subspace identification, autonomous agent, Aggregates, autonomous control, feature selection algorithm, Autonomous agents, Silicon, Sensors, learning (artificial intelligence), optimal control policy, control engineering computing]
Analysis of Temporal High-Dimensional Gene Expression Data for Identifying Informative Biomarker Candidates
2012 IEEE 12th International Conference on Data Mining
None
2012
Identifying informative biomarkers from a large pool of candidates is the key step for accurate prediction of an individual's health status. In clinical applications traditional static feature selection methods that flatten the temporal data cannot be directly applied since the patient's observed clinical condition is a temporal multivariate time series where different variables can capture various stages of temporal change in the patient's health status. In this study, in order to identify informative genes in temporal microarray data, a margin based feature selection filter is proposed. The proposed method is based on well-established machine learning techniques without any assumptions about the data distribution. The objective function of temporal margin-based feature selection is defined to maximize each subject's temporal margin in its own relevant subspace. In the objective function, the uncertainty in calculating nearest neighbors is taken into account by considering the change in feature weights in each iteration. A fixed-point gradient descent method is proposed to solve the formulated objective function. The experimental results on both synthetic and real data provide evidence that the proposed method can identify more informative features than the alternatives that flatten the temporal data in advance.
[Uncertainty, fixed-point gradient descent method, data distribution, temporal data, temporal margin-based feature selection, Optimization, high dimensional, genetics, nearest neighbor, iteration, gradient methods, feature selection, pattern recognition, clinical application, margin, data analysis, Time series analysis, temporal high-dimensional gene expression data analysis, multivariate time series data, Linear programming, Time measurement, Vectors, Gene expression, machine learning, informative biomarker candidate identification, margin based feature selection filter, medical computing, temporal microarray data, individual health status prediction]
Heterogeneous Constraint Propagation with Constrained Sparse Representation
2012 IEEE 12th International Conference on Data Mining
None
2012
This paper presents a graph-based method for heterogeneous constraint propagation on multi-modal data using constrained sparse representation. Since heterogeneous pair wise constraints are defined over pairs of data points from different modalities, heterogeneous constraint propagation is more challenging than the transitional homogeneous constraint propagation on single-modal data which has been studied extensively in previous work. The main difficulty of heterogeneous constraint propagation lies in how to effectively propagate heterogeneous pair wise constraints across different modalities. To address this issue, we decompose heterogeneous constraint propagation into semi-supervised learning sub problems which can then be efficiently solved by graph-based label propagation. Moreover, we develop a constrained sparse representation method for graph construction over each modality using homogeneous pair wise constraints. The experimental results in cross-modal retrieval have shown the superior performance of our heterogeneous constraint propagation.
[cross-modal retrieval, Correlation, modality, semisupervised learning subproblems, graph theory, Encyclopedias, constrained sparse representation method, multimodal data, Sparse matrices, multimodal data analysis, transitional homogeneous constraint propagation, Semantics, constraint handling, learning (artificial intelligence), sparse representation, Laplace equations, graph-based label propagation, data analysis, heterogeneous constraint propagation, graph-based method, homogeneous pairwise constraints, Equations, single-modal data, Semisupervised learning, graph construction, heterogeneous pairwise constraints]
A Classification Based Framework for Concept Summarization
2012 IEEE 12th International Conference on Data Mining
None
2012
In this paper we propose a novel classification based framework for finding a small number of images that summarize a given concept. Our method exploits metadata information available with the images to get category information using Latent Dirichlet Allocation. Using this category information for each image, we solve the underlying classification problem by building a sparse classifier model for each concept. We demonstrate that the images that specify the sparse model form a good summary. In particular, our summary satisfies important properties such as likelihood, diversity and balance in both visual and semantic sense. Furthermore, the framework allows users to specify desired distributions over categories to create personalized summaries. Experimental results on seven broad query types show that the proposed method performs better than state-of-the-art methods.
[Measurement, Visualization, metadata, image classification, concept summarization, likelihood property, Linear programming, Latent dirichlet allocation, Vectors, classification, Optimization, sparse classifier model, Semantics, balance property, diversity property, Kernel, metadata information, classification based framework]
A Similarity Model and Segmentation Algorithm for White Matter Fiber Tracts
2012 IEEE 12th International Conference on Data Mining
None
2012
Recently, fiber segmentation has become an emerging technique in neuroscience. Grouping fiber tracts into anatomical meaningful bundles allows to study the structure of the brain and to investigate onset and progression of neurodegenerative and mental diseases. In this paper, we propose a novel technique for fiber tracts based on shape similarity and connection similarity. For shape similarity, we propose some new techniques adapted from existing similarity measures for trajectory data. We also propose a new technique called Warped Longest Common Subsequence (WLCS) for which we additionally developed a lower-bounding distance function to speed up the segmentation process. Our segmentation is based on an outlier-robust density-based clustering algorithm. Extensive experiments on diffusion tensor images demonstrate the efficiency and effectiveness of our technique.
[diffusion tensor images, Shape, white matter fiber tracts, Shape measurement, Noise, fiber segmentation, tensors, neurodegenerative diseases, Neuroscience, lower-bounding distance function, image segmentation, Clustering algorithms, warped longest common subsequence, segmentation process, outlier-robust density-based clustering algorithm, Robustness, medical image processing, Gold, neuroscience, mental diseases, diseases, WLCS, Standards, Diffusion Tensor Imaging, segmentation algorithm, Fiber Segmentation, Fiber Similarity Measure]
Learning Attitudes and Attributes from Multi-aspect Reviews
2012 IEEE 12th International Conference on Data Mining
None
2012
Most online reviews consist of plain-text feedback together with a single numeric score. However, understanding the multiple `aspects' that contribute to users' ratings may help us to better understand their individual preferences. For example, a user's impression of an audio book presumably depends on aspects such as the story and the narrator, and knowing their opinions on these aspects may help us to recommend better products. In this paper, we build models for rating systems in which such dimensions are explicit, in the sense that users leave separate ratings for each aspect of a product. By introducing new corpora consisting of five million reviews, rated with between three and six aspects, we evaluate our models on three prediction tasks: First, we uncover which parts of a review discuss which of the rated aspects. Second, we summarize reviews by finding the sentences that best explain a user's rating. Finally, since aspect ratings are optional in many of the datasets we consider, we recover ratings that are missing from a user's evaluation. Our model matches state-of-the-art approaches on existing small-scale datasets, while scaling to the real-world datasets we introduce. Moreover, our model is able to `disentangle' content and sentiment words: we automatically learn content words that are indicative of a particular aspect as well as the aspect-specific sentiment words that are indicative of a particular rating.
[rating recovery, summarization, Correlation, Predictive models, user evaluation, plain-text feedback, user interfaces, Optimization, content word, Training, online review, learning attitude, segmentation, Bipartite graph, learning (artificial intelligence), rating system, user audio book impression, sentiment analysis, sentiment word, numeric score, machine learning, information services, Unsupervised learning, aspect rating, learning attribute, Data models, multiaspect review]
Learning Heterogeneous Similarity Measures for Hybrid-Recommendations in Meta-Mining
2012 IEEE 12th International Conference on Data Mining
None
2012
The notion of meta-mining has appeared recently and extends traditional meta-learning in two ways. First it provides support for the whole data-mining process. Second it pries open the so called algorithm black-box approach where algorithms and workflows also have descriptors. With the availability of descriptors both for datasets and data-mining workflows we are faced with a problem the nature of which is much more similar to those appearing in recommendation systems. In order to account for the meta-mining specificities we derive a novel metric-based-learning recommender approach. Our method learns two homogeneous metrics, one in the dataset and one in the workflow space, and a heterogeneous one in the dataset-workflow space. All learned metrics reflect similarities established from the dataset-workflow preference matrix. The latter is constructed from the performance results obtained by the application of workflows to datasets. We demonstrate our method on meta-mining over biological (microarray datasets) problems. The application of our method is not limited to the meta-mining problem, its formulation is general enough so that it can be applied on problems with similar requirements.
[Measurement, metalearning, data mining, hybrid recommendation, datasets, Data mining, dataset-workflow space, Meta-Learning, Optimization, recommendation system, Learning systems, algorithm black-box approach, homogeneous metric, learning (artificial intelligence), heterogeneous metric, Hybrid Recommendation, metric-based-learning recommender approach, Meta-Mining, Linear programming, biological problem, Vectors, microarray dataset, metamining notion, recommender systems, heterogeneous similarity measure learning, Machine learning, dataset-workflow preference matrix, Metric-Learning]
Scaling Inference for Markov Logic via Dual Decomposition
2012 IEEE 12th International Conference on Data Mining
None
2012
Markov logic is a knowledge-representation language that allows one to specify large graphical models. However, the resulting large graphical models can make inference for Markov logic a computationally challenging problem. Recently, dual decomposition (DD) has become a popular approach for scalable inference on graphical models. We study how to apply DD to scale up inference in Markov logic. A standard approach for DD first partitions a graphical model into multiple tree-structured sub problems. We apply this approach to Markov logic and show that DD can outperform prior inference approaches. Nevertheless, we observe that the standard approach for DD is suboptimal as it does not exploit the rich structure often present in the Markov logic program. Thus, we describe a novel decomposition strategy that partitions a Markov logic program into parts based on its structure. A crucial advantage of our approach is that we can use specialized algorithms for portions of the input problem -- some of which have been studied for decades, e.g., coreference resolution. Empirically, we show that our program-level decomposition approach outperforms both non-decomposition and graphical model-based decomposition approaches to Markov logic inference on several data-mining tasks.
[Scalability, data mining, Master-slave, scaling inference, program-level decomposition approach, formal logic, Graphical models, dual decomposition, coreference resolution algorithm, tree-structured subproblem, knowledge representation language, inference approach, trees (mathematics), inference mechanisms, graphical model-based decomposition approach, knowledge representation languages, Standards, nondecomposition based decomposition approach, Message passing, graphical model, Markov processes, DD first partition approach, Inference algorithms, Markov logic, data mining task, solid modelling]
Mining User Mobility Features for Next Place Prediction in Location-Based Services
2012 IEEE 12th International Conference on Data Mining
None
2012
Mobile location-based services are thriving, providing an unprecedented opportunity to collect fine grained spatio-temporal data about the places users visit. This multi-dimensional source of data offers new possibilities to tackle established research problems on human mobility, but it also opens avenues for the development of novel mobile applications and services. In this work we study the problem of predicting the next venue a mobile user will visit, by exploring the predictive power offered by different facets of user behavior. We first analyze about 35 million check-ins made by about 1 million Foursquare users in over 5 million venues across the globe, spanning a period of five months. We then propose a set of features that aim to capture the factors that may drive users' movements. Our features exploit information on transitions between types of places, mobility flows between venues, and spatio-temporal characteristics of user check-in patterns. We further extend our study combining all individual features in two supervised learning models, based on linear regression and M5 model trees, resulting in a higher overall prediction accuracy. We find that the supervised methodology based on the combination of multiple features offers the highest levels of prediction accuracy: M5 model trees are able to rank in the top fifty venues one in two user check-ins, amongst thousands of candidate items in the prediction list.
[mobile service, Humans, data mining, regression analysis, Predictive models, linear regression, Mobile communication, user interfaces, supervised learning model, Accuracy, next place prediction, fine grained spatio-temporal data, information exploitation, Cities and towns, user mobility feature mining, location-based services, learning (artificial intelligence), prediction accuracy, Filtering, data multidimensional source, trees (mathematics), mobile application, Foursquare user, data collection, M5 model trees, human mobility, Supervised learning, social networking (online), user check-in, user behavior]
Spatial Interpolation Using Multiple Regression
2012 IEEE 12th International Conference on Data Mining
None
2012
Many real world data mining applications involve analyzing geo-referenced data. Frequently, this type of data sets are incomplete in the sense that not all geographical coordinates have measured values of the variable(s) of interest. This incompleteness may be caused by poor data collection, measurement errors, costs management and many other factors. These missing values may cause several difficulties in many applications. Spatial imputation/interpolation methods try to fill in these unknown values in geo-referenced data sets. In this paper we propose a new spatial imputation method based on machine learning algorithms and a series of data pre-processing steps. The key distinguishing factor of this method is allowing the use of data from faraway regions, contrary to the state of the art on spatial data mining. Images (e.g. from a satellite or video surveillance cameras) may also suffer from this incompleteness where some pixels are missing, which again may be caused by many factors. An image can be seen as a spatial data set in a Cartesian coordinates system, where each pixel (location) registers some value (e.g. degree of gray on a black and white image). Being able to recover the original image from a partial or incomplete version of the reality is a key application in many domains (e.g. surveillance, security, etc.). In this paper we evaluate our general methodology for spatial interpolation on this type of problems. Namely, we check the ability of our method to fill in unknown pixels on several images. We compare it to state of the art methods and provide strong experimental evidence of the advantages of our proposal.
[image recovery, data mining, spatial imputation method, regression analysis, Predictive models, Proposals, spatial prediction, white image, data preprocessing step, geo-referenced data, learning (artificial intelligence), Context, image processing, black image, gray image, data pre-processing, Spatial databases, Interpolation, interpolation, spatial interpolation, Cartesian coordinates system, Data models, multiple regression, spatial data mining, data mining application, machine learning algorithm]
Clustering by Learning Constraints Priorities
2012 IEEE 12th International Conference on Data Mining
None
2012
A method for creating a constrained clustering ensemble by learning the priorities of pair wise constraints is proposed in this paper. This method integrates multiple clusters produced by using a simple constrained K-means algorithm that we modify to utilize the constraints priorities. The cluster ensemble is executed according to a boosting framework, which adaptively learns the constraints priorities and provides them for the modified constrained K-means to create diverse clusters that finally improve the clustering performance. The experimental results show that our proposed method outperforms the original constrained K-means and is comparable to several state-of-the-art constrained clustering methods.
[Measurement, K-means, Clustering methods, Glass, pairwise constraint, Boosting, constraint priority learning, constrained clustering ensemble, Clustering, pattern clustering, Clustering algorithms, clustering performance, simple constrained K-means algorithm, Computational efficiency, learning (artificial intelligence), boosting framework, Kernel, Learning Kernel Matrix, Cluster Ensemble]
Exclusive Row Biclustering for Gene Expression Using a Combinatorial Auction Approach
2012 IEEE 12th International Conference on Data Mining
None
2012
The availability of large microarray data has led to a growing interest in biclustering methods in the past decade. Several algorithms have been proposed to identify subsets of genes and conditions according to different similarity measures and under varying constraints. In this paper we focus on the exclusive row biclusteing problem (also known as projected clustering) for gene expression data sets, in which each row can only be a member of a single bicluster while columns can participate in multiple clusters. This type of biclustering may be adequate, for example, for clustering groups of cancer patients where each patient (row) is expected to be carrying only a single type of cancer, while each cancer type is associated with multiple (and possibly overlapping) genes (columns). In this paper we present a novel method to identify these exclusive row biclusters through a combination of existing biclustering algorithms and combinatorial auction techniques. We devise an approach for tuning the threshold for our algorithm based on comparison to a null model in the spirit of the Gap statistic approach. We demonstrate our approach on both synthetic and real-world gene expression data and show its power in identifying large span non-overlapping rows sub matrices, while considering their unique nature. The Gap statistic approach succeeds in identifying appropriate thresholds in all our examples.
[Algorithm design and analysis, similarity measure, cancer type, combinatorial mathematics, microarray data, Optimization, Projected Clustering, genetics, biology computing, exclusive row biclustering, null model, Clustering algorithms, combinatorial auction approach, biclustering method, Exclusive Row Biclustering, Gene Expression, Biclustering, Educational institutions, Partitioning algorithms, Gene expression, Gap statistic approach, pattern clustering, projected clustering, cancer, molecular biophysics, gene expression, Cancer, statistics]
Topic Models over Spoken Language
2012 IEEE 12th International Conference on Data Mining
None
2012
Virtually all work on topic modeling has assumed that the topics are to be learned over a text-based document corpus. However, there exist important applications where topic models must be learned over an audio corpus of spoken language. Unfortunately, speech-to-text programs can have very low accuracy. We therefore propose a novel topic model for spoken language that incorporates a statistical model of speech-to-text software behavior. Crucially, our model exploits the uncertainty numbers returned by the software. Our ideas apply to any domain in which it would be useful to build a topic model over data in which uncertainties are explicitly represented.
[text analysis, Uncertainty, Text analysis, Biological system modeling, Computational modeling, topic model, spoken language, speech-to-text program, Vectors, statistical model, audio corpus, Accuracy, Speech recognition, speech processing, speech-to-text software behavior, Software, Data models, text-based document corpus, statistical analysis, Uncertain data]
Multiplicative Algorithms for Constrained Non-negative Matrix Factorization
2012 IEEE 12th International Conference on Data Mining
None
2012
Non-negative matrix factorization (NMF) provides the advantage of parts-based data representation through additive only combinations. It has been widely adopted in areas like item recommending, text mining, data clustering, speech denoising, etc. In this paper, we provide an algorithm that allows the factorization to have linear or approximately linear constraints with respect to each factor. We prove that if the constraint function is linear, algorithms within our multiplicative framework will converge. This theory supports a large variety of equality and inequality constraints, and can facilitate application of NMF to a much larger domain. Taking the recommender system as an example, we demonstrate how a specialized weighted and constrained NMF algorithm can be developed to fit exactly for the problem, and the tests justify that our constraints improve the performance for both weighted and unweighted NMF algorithms under several different metrics. In particular, on the Movie lens data with 94% of items, the Constrained NMF improves recall rate 3% compared to SVD50 and 45% compared to SVD150, which were reported as the best two in the top-N metric.
[Measurement, data clustering, Linear Constraints, Convergence, multiplicative algorithm, Multiplicative Algorithm, Clustering algorithms, Motion pictures, inequality constraint, data structures, text mining, item recommendation, singular value decomposition, speech denoising, approximation theory, equality constraint, NMF, Non-negative Matrix Factorization, Vectors, Matrix decomposition, parts-based data representation, recall rate, constrained nonnegative matrix factorization, Approximation algorithms, linear constraint, constraint function]
Granger Causality for Time-Series Anomaly Detection
2012 IEEE 12th International Conference on Data Mining
None
2012
Recent developments in industrial systems provide us with a large amount of time series data from sensors, logs, system settings and physical measurements, etc. These data are extremely valuable for providing insights about the complex systems and could be used to detect anomalies at early stages. However, the special characteristics of these time series data, such as high dimensions and complex dependencies between variables, as well as its massive volume, pose great challenges to existing anomaly detection algorithms. In this paper, we propose Granger graphical models as an effective and scalable approach for anomaly detection whose results can be readily interpreted. Specifically, Granger graphical models are a family of graphical models that exploit the temporal dependencies between variables by applying L1-regularized learning to Granger causality. Our goal is to efficiently compute a robust "correlation anomaly" score for each variable via Granger graphical models that can provide insights on the possible reasons of anomalies. We evaluate the effectiveness of our proposed algorithms on both synthetic and application datasets. The results show the proposed algorithm achieves significantly better performance than other baseline algorithms and is scalable for large-scale applications.
[Algorithm design and analysis, time-series anomaly detection, L1-regularized learning, anomaly detection algorithm, Time series analysis, Stochastic processes, reliability, time series, Granger graphical model, Optimization, manufacturing data processing, correlation anomaly score, Anomaly Detection, Graphical models, Time Series Analysis, industrial system, manufacturing systems, Data models, learning (artificial intelligence), Granger causality, Principal component analysis, solid modelling]
Active Label Correction
2012 IEEE 12th International Conference on Data Mining
None
2012
Active Label Correction (ALC) is an interactive method that cleans an established training set of mislabeled examples in conjunction with a domain expert. ALC presumes that the expert who conducts this review is either more accurate than the original annotator or has access to additional resources that ensure a high quality label. A high-cost re-review is possible because ALC proceeds iteratively, scoring the full training set but selecting only small batches of examples that are likely mislabeled. The expert reviews each batch and corrects any mislabeled examples, after which the classifier is retrained and the process repeats until the expert terminates it. We compare several instantiations of ALC to fully-automated methods that attempt to discard or correct label noise in a single pass. Our empirical results show that ALC outperforms single-pass methods in terms of selection efficiency and classifier accuracy. We evaluate the best ALC instantiation on our motivating task of detecting mislabeled and poorly formulated sites within a land cover classification training set from the geography domain.
[pattern classification, Uncertainty, terrain mapping, classifier accuracy, land cover classification training, Noise, supervised learning, domain expert, land cover classification, Noise level, Training, active label correction, single-pass method, Accuracy, selection efficiency, data cleaning, Training data, geography, geography domain, label noise, ALC method, Labeling, learning (artificial intelligence), classifier retraining]
Inductive Model Generation for Text Categorization Using a Bipartite Heterogeneous Network
2012 IEEE 12th International Conference on Data Mining
None
2012
Usually, algorithms for categorization of numeric data have been applied for text categorization after a preprocessing phase which assigns weights for textual terms deemed as attributes. However, due to characteristics of textual data, some algorithms for data categorization are not efficient for text categorization. Characteristics of textual data such as sparsity and high dimensionality sometimes impair the quality of general purpose classifiers. Here, we propose a text classifier based on a bipartite heterogeneous network used to represent textual document collections. Such algorithm induces a classification model assigning weights to objects that represents terms of the textual document collection. The induced weights correspond to the influence of the terms in the classification of documents they appear. The least-mean-square algorithm is used in the inductive process. Empirical evaluation using a large amount of textual document collections shows that the proposed IMBHN algorithm produces significantly better results than the k-NN, C4.5, SVM and Nai&#x0308;ve Bayes algorithms.
[least mean squares methods, text analysis, network theory (graphs), SVM algorithm, text categorization, textual term, Text Categorization, numeric data categorization algorithm, Niobium, Training, Accuracy, k-NN algorithm, Mathematical model, bipartite heterogeneous network, least mean square algorithm, pattern classification, IMBHN algorithm, support vector machines, inductive model generation, Vectors, C4.5 algorithm, k-nearest neighbor, textual document collection, Equations, dimensionality characteristics, classification model, Heterogeneous Network, Data models, textual data characteristics, Naive Bayes algorithm, sparsity characteristics, general purpose classifier]
Sparse Subspace Representation for Spectral Document Clustering
2012 IEEE 12th International Conference on Data Mining
None
2012
We present a novel method for document clustering using sparse representation of documents in conjunction with spectral clustering. An &#x2113;<sub>1</sub>-norm optimization formulation is posed to learn the sparse representation of each document, allowing us to characterize the affinity between documents by considering the overall information instead of traditional pair wise similarities. This document affinity is encoded through a graph on which spectral clustering is performed. The decomposition into multiple subspaces allows documents to be part of a sub-group that shares a smaller set of similar vocabulary, thus allowing for cleaner clusters. Extensive experimental evaluations on two real-world datasets from Reuters-21578 and 20Newsgroup corpora show that our proposed method consistently outperforms state-of-the-art algorithms. Significantly, the performance improvement over other methods is prominent for this datasets.
[document handling, Symmetric matrices, Laplace equations, pairwise similarity, graph theory, Reuters-21578 dataset, Matrix decomposition, Sparse matrices, L<sub>1</sub>-norm optimization formulation, graph, document affinity, subspace decomposition, sparse subspace representation, optimisation, Document clustering, pattern clustering, Clustering algorithms, Sparse representation, Eigenvalues and eigenfunctions, data structures, spectral clustering, 20Newsgroup dataset, Indexing, spectral document clustering]
Learning Target Predictive Function without Target Labels
2012 IEEE 12th International Conference on Data Mining
None
2012
In the absence of the labeled samples in a domain referred to as target domain, Domain Adaptation (DA) techniques come in handy. Generally, DA techniques assume there are available source domains that share similar predictive function with the target domain. Two core challenges of DA typically arise, variance that exists between source and target domains, and the inherent source hypothesis bias. In this paper, we first propose a Stability Transfer criterion for selecting relevant source domains with low variance. With this criterion, we introduce a TARget learning Assisted by Source Classifier Adaptation (TARASCA) method to address the two core challenges that have impeded the performances of DA techniques. To verify the robustness of TARASCA, extensive experimental studies are carried out with comparison to several state-of-the-art DA methods on the real-world Sentiment and Newsgroups datasets, where various settings for the class ratios of the source and target domains are considered.
[pattern classification, domain adaptation technique, Transfer Learning, target domain, DA technique, domain variance, Domain Adaptation, source domain, target learning assisted by source classifier adaptation, Standards, Support vector machines, Stability criteria, Source Hypothesis bias, stability transfer criterion, Prediction algorithms, TARASCA method, Sentiment dataset, Robustness, learning (artificial intelligence), statistical analysis, predictive function learning, Kernel, Joints, Newsgroups dataset]
Low-Rank Transfer Subspace Learning
2012 IEEE 12th International Conference on Data Mining
None
2012
One of the most important challenges in machine learning is performing effective learning when there are limited training data available. However, there is an important case when there are sufficient training data coming from other domains (source). Transfer learning aims at finding ways to transfer knowledge learned from a source domain to a target domain by handling the subtle differences between the source and target. In this paper, we propose a novel framework to solve the aforementioned knowledge transfer problem via low-rank representation constraints. This is achieved by finding an optimal subspace where each datum in the target domain can be linearly represented by the corresponding subspace in the source domain. Extensive experiments on several databases, i.e., Yale B, CMU PIE, UB Kin Face databases validate the effectiveness of the proposed approach and show the superiority to the existing, well-established methods.
[transfer learning, transfer subspace learning, CMU PIE database, database management systems, machine learning, Knowledge transfer, Convergence, Learning systems, Yale B database, Databases, knowledge transfer problem, Machine learning, training data, low-rank, UB Kin Face database, Silicon, learning (artificial intelligence), low-rank representation constraint, domain adaptation, Principal component analysis]
Socialized Gaussian Process Model for Human Behavior Prediction in a Health Social Network
2012 IEEE 12th International Conference on Data Mining
None
2012
Modeling and predicting human behaviors, such as the activity level and intensity, is the key to prevent the cascades of obesity, and help spread wellness and healthy behavior in a social network. In this work, we propose a Socialized Gaussian Process (SGP) for socialized human behavior modeling. In the proposed SGP model, we naturally incorporates human's personal behavior factor and social correlation factor into a unified model, where basic Gaussian Process model is leveraged to capture individual's personal behavior pattern. Furthermore, we extend the Gaussian Process Model to socialized Gaussian Process (SGP) which aims to capture social correlation phenomena in the social network. The detailed experimental evaluation has shown the SGP model achieves the best prediction accuracy compared with other baseline methods.
[SGP model, Correlation, Social network services, Humans, behavioural sciences, Socialized Gaussian Process, Predictive models, healthy behavior, Educational institutions, social correlation factor, socialized Gaussian process model, human intensity, personal behavior factor, human activity level, Accuracy, wellness behavior, health social network, social correlation phenomenon, social sciences, Gaussian processes, Human Behavior Prediction, human behavior prediction, prediction accuracy]
An AdaBoost Algorithm for Multiclass Semi-supervised Learning
2012 IEEE 12th International Conference on Data Mining
None
2012
We present an algorithm for multiclass Semi-Supervised learning which is learning from a limited amount of labeled data and plenty of unlabeled data. Existing semi-supervised algorithms use approaches such as one-versus-all to convert the multiclass problem to several binary classification problems which is not optimal. We propose a multiclass semi-supervised boosting algorithm that solves multiclass classification problems directly. The algorithm is based on a novel multiclass loss function consisting of the margin cost on labeled data and two regularization terms on labeled and unlabeled data. Experimental results on a number of UCI datasets show that the proposed algorithm performs better than the state-of-the-art boosting algorithms for multiclass semi-supervised learning.
[pattern classification, binary classification problem, margin cost, boosting, multiclass loss function, Semi-Supervised Learning, Boosting, Linear programming, multiclass semisupervised learning, Optimization, one-versus-all approach, multiclass semisupervised boosting algorithm, Training, multiclass classification, regularization term, AdaBoost algorithm, labeled data, Semisupervised learning, Prediction algorithms, unlabeled data, learning (artificial intelligence)]
A Cluster-Oriented Genetic Algorithm for Alternative Clustering
2012 IEEE 12th International Conference on Data Mining
None
2012
Supervised alternative clusterings is the problem of finding a set of clusterings which are of high quality and different from a given negative clustering. The task is therefore a clear multi-objective optimization problem. Optimizing two conflicting objectives requires dealing with trade-offs. Most approaches in the literature optimize these objectives sequentially or indirectly, resulting in solutions which are dominated. We develop a multi-objective algorithm, called COGNAC, able to optimize the objectives directly and simultaneously and producing solutions approximating the Pareto front. COGNAC performs the recombination operator at the cluster level instead of the object level as in traditional genetic algorithms. It can accept arbitrary clustering quality and dissimilarity objectives and provide solutions dominating those of other state-of-the-art algorithms. COGNAC can also be used to generate a sequence of alternative clusterings, each of which is guaranteed to be different from all previous ones.
[cluster-oriented genetic algorithm, Complexity theory, multiobjective optimization problem, Genetic algorithms, genetic algorithm, conflicting objective, Sociology, Clustering algorithms, arbitrary clustering quality, Pareto front approximation, cluster-oriented, recombination operator, negative clustering, Face, learning (artificial intelligence), approximation theory, Pareto optimisation, COGNAC algorithm, Birds, genetic algorithms, Statistics, supervised alternative clustering, dissimilarity objective, pattern clustering, alternative clustering, multi-objective optimization]
A New Proposal for Score Normalization in Biometric Signature Recognition Based on Client Threshold Prediction
2012 IEEE 12th International Conference on Data Mining
None
2012
Score Normalization is a usual technique in pattern recognition to standardize the classifier output ranges so as to, for example, fuse these outputs. The use of score normalization in biometric recognition is a very important part of the system, specially in those based on behavioral traits, such as written signature or voice, conditioning the final system performance. Then, many works can be found that focus on the problem. A successful new approach for client threshold prediction, based on Multiple Linear Prediction, has been presented in recent works. Here, a new approach for score normalization, based on this proposal for biometric manuscript signature user verification, is shown. This proposal is compared with the state of the art methods, achieving an improvement of 19% and 16% for Equal Error Rate (EER) and 60% and 26% for Detection Cost Function (DCF) performance measures, for random and skilled forgeries, respectively.
[voice, random forgery, Biometric Signature Recognition, Predictive models, client threshold prediction, multiple linear prediction, Proposals, biometrics (access control), handwriting recognition, Forgery, equal error rate, skilled forgery, Mathematical model, pattern recognition, DCF performance measure, written signature, biometric signature recognition, biometric manuscript signature user verification, score normalization, Equations, Standards, detection cost function, Data models, Score Normalization, Client Threshold Prediction]
Signal Disaggregation via Sparse Coding with Featured Discriminative Dictionary
2012 IEEE 12th International Conference on Data Mining
None
2012
As the issue of freshwater shortage is increasing daily, it's critical to take effective measures for water conservation. Based on previous studies, device level consumption could lead to significant conservation of freshwater. However, current smart meter deployments only produce low sample rate aggregated data. In this paper, we examine the task of separating whole-home water consumption into its component appliances. A key challenge is to address the unique features of low sample rate data. To this end, we propose Sparse Coding with Featured Discriminative Dictionary (SCFDD) by incorporating inherent shape and activation features to capture the discriminative characteristics of devices. In addition, extensive experiments were performed to validate the effectiveness of SCFDD.
[Performance evaluation, Dictionaries, Shape, Discriminative dictionary, signal processing, data mining, SCFDD, component appliances, Water conservation, freshwater shortage, smart meters, Market research, Testing, smart meter, Shape and activation features, activation features, Disaggregation, Encoding, device level consumption, whole home water consumption, encoding, water conservation, sparse coding, signal disaggregation, featured discriminative dictionary, Low sample rate]
Cost-Sensitive Online Classification
2012 IEEE 12th International Conference on Data Mining
None
2012
Both cost-sensitive classification and online learning have been studied extensively in data mining and machine learning communities, respectively. It is a bit surprising that there was very limited comprehensive study for addressing an important intersecting problem, that is, cost-sensitive online classification. In this paper, we formally study this problem, and propose a new framework for cost-sensitive online classification by exploiting the idea of online gradient descent techniques. Based on the framework, we propose a family of cost-sensitive online classification algorithms, which are designed to directly optimize two well-known cost-sensitive measures: (i) maximization of weighted sum of sensitivity and specificity, and (ii) minimization of weighted misclassification cost. We analyze the theoretical bounds of the cost-sensitive measures made by the proposed algorithms, and extensively examine their empirical performance on a variety of cost-sensitive online classification tasks.
[pattern classification, Machine learning algorithms, cost-sensitive measure, cost-sensitive learning, data mining, specificity sum, Data mining, machine learning, classification, weighted misclassification cost minimization, Sensitivity, Accuracy, cost-sensitive online classification, Machine learning, Prediction algorithms, online gradient descent technique, learning (artificial intelligence), minimisation, sensitivity sum, gradient methods, online learning]
Labels vs. Pairwise Constraints: A Unified View of Label Propagation and Constrained Spectral Clustering
2012 IEEE 12th International Conference on Data Mining
None
2012
In many real-world applications we can model the data as a graph with each node being an instance and the edges indicating a degree of similarity. Side information is often available in the form of labels for a small subset of instances, which gives rise to two problem settings and two types of algorithms. In the label propagation style algorithms, the known labels are propagated to the unlabeled nodes. In the constrained clustering style algorithms, known labels are first converted to pair wise constraints (Must-Link and Cannot-Link), then a constrained cut is computed as a tradeoff between minimizing the cut cost and maximizing the constraint satisfaction. Both techniques are evaluated by their ability to recover the ground truth labeling, i.e. by 0/1 loss function either directly on the labels or on the pair wise relations derived from the labels. These two fields have developed separately, but in this paper, we show that they are indeed related. This insight allows us to propose a novel way to generate constraints from the propagated labels, which our empirical study shows outperforms and is more stable than the state-of-the-art label propagation and constrained spectral clustering algorithms.
[constrained clustering style algorithm, must-link constraint, constrained spectral clustering, label propagation style algorithm, graph theory, data mining, side information, label propagation, loss function, graph node, constraint satisfaction, Clustering algorithms, similarity degree, Eigenvalues and eigenfunctions, cut cost, Labeling, learning (artificial intelligence), cannot-link constraint, pairwise constraint, ground truth labeling, Educational institutions, Partitioning algorithms, Indexes, machine learning, Computer science, graph edge, pattern clustering, semi-supervised learning]
Collaborative Filtering with Aspect-Based Opinion Mining: A Tensor Factorization Approach
2012 IEEE 12th International Conference on Data Mining
None
2012
Collaborative filtering (CF) aims to produce user specific recommendations based on other users' ratings of items. Most existing CF methods rely only on users' overall ratings of items, ignoring the variety of opinions users may have towards different aspects of the items. Using the movie domain as a case study, we propose a framework that is able to capture users' opinions on different aspects from the textual reviews, and use that information to improve the effectiveness of CF. This framework has two components, an opinion mining component and a rating inference component. The former extracts and summarizes the opinions on multiple aspects from the reviews, generating ratings on the various aspects. The latter component, on the other hand, infers the overall ratings of items based on the aspect ratings, which forms the basis for item recommendation. Our core contribution is in the proposal of a tensor factorization approach for the rating inference. Operating on the tensor composed of the overall and aspect ratings, this approach is able to capture the intrinsic relationships between users, items, and aspects, and provide accurate predictions on unknown ratings. Experiments on a movie dataset show that our proposal significantly improves the prediction accuracy compared with two baseline methods.
[collaborative filtering, Collaborative Filtering, data mining, Recommendation System, tensors, textual reviews, Sentiment Analysis, Data mining, Proposals, movie dataset, opinion mining component, Motion pictures, Tensor Factorization, item recommendation, opinion summarization, item user rating, tensor factorization approach, CF, rating inference component, Estimation, Educational institutions, opinion extraction, aspect ratings, Tensile stress, recommender systems, Opinion Mining, Collaboration, aspect-based opinion mining, movie domain]
Towards Annotating Media Contents through Social Diffusion Analysis
2012 IEEE 12th International Conference on Data Mining
None
2012
Recently, the boom of media contents on the Internet raises challenges in managing them effectively and thus requires automatic media annotation techniques. Motivated by the observation that media contents are usually shared frequently in online communities and thus have a lot of social diffusion records, we propose a novel media annotating approach depending on these social diffusion records instead of metadata. The basic assumption is that the social diffusion records reflect the common interests (CI) between users, which can be analyzed for generating annotations. With this assumption, we present a novel CI-based social diffusion model and translate the automatic annotating task into the CI-based diffusion maximization (CIDM) problem. Moreover, we propose to solve the CIDM problem through two optimization tasks, corresponding to the training and test stages in supervised learning. Extensive experiments on real-world data sets show that our approach can effectively generate high quality annotations, and thus demonstrate the capability of social diffusion analysis in annotating media.
[social diffusion analysis, online community, supervised learning, Data mining, Optimization, content management, social diffusion records, Training, CI-based diffusion maximization problem, CIDM problem, optimisation, test stage, Motion pictures, diffusion maximization, learning (artificial intelligence), social media, automatic media content annotation techniques, optimization tasks, Computational modeling, training stage, common interests, Media, Automatic annotation, Feature extraction, social networking (online), CI-based social diffusion model, Internet, social diffusion]
An Approach to Evaluate the Local Completeness of an Event Log
2012 IEEE 12th International Conference on Data Mining
None
2012
Process mining links traditional model-driven Business Process Management and data mining by means of deriving knowledge from event logs to improve operational business processes. As an impact factor of the quality of process mining results, the degree of completeness of the given event log should be necessarily measured. In this paper an approach is proposed in the context of mining control-flow dependencies to evaluate the local completeness of an event log without knowing any information about the original process model. Experiment results show that the proposed approach works robustly and gives better estimation than approaches available.
[Algorithm design and analysis, Process control, Estimation, data mining, Probabilistic logic, Data mining, local completeness evaluation, information completeness, model-driven business process management, business process re-engineering, business process management, mining control-flow dependency, event log, Data models, process mining, Business]
Community-Affiliation Graph Model for Overlapping Network Community Detection
2012 IEEE 12th International Conference on Data Mining
None
2012
One of the main organizing principles in real-world networks is that of network communities, where sets of nodes organize into densely linked clusters. Communities in networks often overlap as nodes can belong to multiple communities at once. Identifying such overlapping communities is crucial for the understanding the structure as well as the function of real-world networks. Even though community structure in networks has been widely studied in the past, practically all research makes an implicit assumption that overlaps between communities are less densely connected than the non-overlapping parts themselves. Here we validate this assumption on 6 large scale social, collaboration and information networks where nodes explicitly state their community memberships. By examining such ground-truth communities we find that the community overlaps are more densely connected than the non-overlapping parts, which is in sharp contrast to the conventional wisdom that community overlaps are more sparsely connected than the communities themselves. Practically all existing community detection methods fail to detect communities with dense overlaps. We propose Community-Affiliation Graph Model, a model-based community detection method that builds on bipartite node-community affiliation networks. Our method successfully captures overlapping, non-overlapping as well as hierarchically nested communities, and identifies relevant communities more accurately than the state-of-the-art methods in networks ranging from biological to social and information networks.
[community-affiliation graph model, densely linked clusters, Image edge detection, Communities, graph theory, social networks, collaboration networks, Community detection, information networks, Organizing, YouTube, real-world networks, bipartite node-community affiliation networks, pattern clustering, overlapping network community detection, hierarchically nested communities, model-based community detection method, Collaboration, Overlapping communities, ground-truth communities, social networking (online), community memberships, nonoverlapping communities, Reliability]
Robust Ensemble Clustering by Matrix Completion
2012 IEEE 12th International Conference on Data Mining
None
2012
Data clustering is an important task and has found applications in numerous real-world problems. Since no single clustering algorithm is able to identify all different types of cluster shapes and structures, ensemble clustering was proposed to combine different partitions of the same data generated by multiple clustering algorithms. The key idea of most ensemble clustering algorithms is to find a partition that is consistent with most of the available partitions of the input data. One problem with these algorithms is their inability to handle uncertain data pairs, i.e. data pairs for which about half of the partitions put them into the same cluster and the other half do the opposite. When the number of uncertain data pairs is large, they can mislead the ensemble clustering algorithm in generating the final partition. To overcome this limitation, we propose an ensemble clustering approach based on the technique of matrix completion. The proposed algorithm constructs a partially observed similarity matrix based on the data pairs whose cluster memberships are agreed upon by most of the clustering algorithms in the ensemble. It then deploys the matrix completion algorithm to complete the similarity matrix. The final data partition is computed by applying an efficient spectral clustering algorithm to the completed matrix. Our empirical studies with multiple real-world datasets show that the proposed algorithm performs significantly better than the state-of-the-art algorithms for ensemble clustering.
[state-of-the-art algorithms, similarity matrix, data clustering, Sparse matrices, uncertain data pair handling, cluster memberships, robust ensemble clustering algorithm, Clustering algorithms, matrix completion algorithm, Robustness, Low Rank, data partition, spectral clustering algorithm, cluster shapes, Partitioning algorithms, Matrix decomposition, Matrix Completion, matrix algebra, pattern clustering, Measurement uncertainty, Sparse, data handling, cluster structures, real-world problems, Ensemble Clustering]
A General Framework for Publishing Privacy Protected and Utility Preserved Graph
2012 IEEE 12th International Conference on Data Mining
None
2012
The privacy protection of graph data has become more and more important in recent years. Many works have been proposed to publish a privacy preserving graph. All these works prefer publishing a graph, which guarantees the protection of certain privacy with the smallest change to the original graph. However, there is no guarantee on how the utilities are preserved in the published graph. In this paper, we propose a general fine-grained adjusting framework to publish a privacy protected and utility preserved graph. With this framework, the data publisher can get a trade-off between the privacy and utility according to his customized preferences. We used the protection of a weighted graph as an example to demonstrate the implementation of this framework.
[Data privacy, graph data privacy protection, weighted graph, Social network services, graph theory, customized preferences, privacy, Equations, general privacy protected graph publishing framework, privacy preserving graph, Privacy, Publishing, utility preserved graph publishing framework, general fine-grained adjusting framework, data privacy, Mathematical model, publishing]
Hashing with Generalized Nyström Approximation
2012 IEEE 12th International Conference on Data Mining
None
2012
Hashing, which involves learning binary codes to embed high-dimensional data into a similarity-preserving low-dimensional Hamming space, is often formulated as linear dimensionality reduction followed by binary quantization. Linear dimensionality reduction, based on maximum variance formulation, requires leading eigenvectors of data covariance or graph Laplacian matrix. Computing leading singular vectors or eigenvectors in the case of high-dimension and large sample size, is a main bottleneck in most of data-driven hashing methods. In this paper we address the use of generalized Nystrom method where a subset of rows and columns are used to approximately compute leading singular vectors of the data matrix, in order to improve the scalability of hashing methods in the case of high-dimensional data with large sample size. Especially we validate the useful behavior of generalized Nystrom approximation with uniform sampling, in the case of a recently-developed hashing method based on principal component analysis (PCA) followed by an iterative quantization, referred to as PCA+ITQ, developed by Gong and Lazebnik. We compare the performance of generalized Nystrom approximation with uniform and non-uniform sampling, to the full singular value decomposition (SVD) method, confirming that the uniform sampling improves the computational and space complexities dramatically, while the performance is not much sacrificed. In addition we present low-rank approximation error bounds for generalized Nystrom approximation with uniform sampling, which is not a trivial extension of available results on the non-uniform sampling case.
[iterative methods, uniform sampling, iterative quantization, graph theory, pseudoskeleton approximation, high-dimensional data, hashing, Quantization, data covariance, Hamming codes, SVD method, learning binary codes, eigenvalues and eigenfunctions, PCA-based hashing method, leading singular vectors, Binary codes, quantisation (signal), binary quantization, low-rank approximation error bounds, generalized Nystrom approximation, data-driven hashing methods, approximation theory, CUR decomposition, binary codes, sampling methods, large sample size, maximum variance formulation, nonuniform sampling, data matrix, Vectors, principal component analysis-based hashing method, matrix algebra, space complexities, singular value decomposition method, linear dimensionality reduction, eigenvectors, Approximation error, Approximation algorithms, file organisation, similarity-preserving low-dimensional Hamming space, graph Laplacian matrix, computational complexities, principal component analysis, Principal component analysis, computational complexity]
Detecting Spam and Promoting Campaigns in the Twitter Social Network
2012 IEEE 12th International Conference on Data Mining
None
2012
The Twitter social network has become a target platform for both promoters and stammers to disseminate their target messages. There are a large number of campaigns containing coordinated spam or promoting accounts in Twitter, which are more harmful than the traditional methods, such as email spamming. Since traditional solutions mainly check individual accounts or messages, it is an urgent task to detect spam and promoting campaigns in Twitter. In this paper, we propose a scalable framework to detect both spam and promoting campaigns. Our framework consists of three steps: firstly linking accounts who post URLs for similar purposes, secondly extracting candidate campaigns which may exist for spam or promoting purpose and finally distinguishing their intents. One salient aspect of the framework is introducing a URL-driven estimation method to measure the similarity between accounts' purposes of posting URLs, the other one is proposing multiple features to distinguish the candidate campaigns based on a machine learning method. Over a large-scale dataset from Twitter, we can extract the actual campaigns with high precision and recall and distinguish the majority of the candidate campaigns correctly.
[similarity measure, coordinated spam, URL-driven estimation method, Unsolicited electronic mail, Estimation, scalable campaign promotion framework, scalable spam detection framework, Twitter, Educational institutions, unsolicited e-mail, email spamming, machine learning method, candidate campaigns, message dissemination, candidate campaign extraction, Twitter social network, large-scale dataset, Feature extraction, social networking (online), social spam, campaign detect, learning (artificial intelligence), salient aspect]
Cross-Language Opinion Target Extraction in Review Texts
2012 IEEE 12th International Conference on Data Mining
None
2012
Opinion target extraction is a subtask of opinion mining which is very useful in many applications. In this study, we investigate the problem in a cross-language scenario which leverages the rich labeled data in a source language for opinion target extraction in a different target language. The English labeled corpus is used as training set. We generate two Chinese training datasets with different features. Two labeling models for Chinese opinion target extraction are learned based on Conditional Random Fields (CRF). After that, we use a monolingual co-training algorithm to improve the performance of both models by leveraging the enormous unlabeled Chinese review texts on the web. Experimental results show the effectiveness of our proposed approach.
[Algorithm design and analysis, opinion mining, training set, text analysis, data mining, English labeled corpus, cross-language information extraction, training, Data mining, Training, Labeling, Chinese review texts, Corporate acquisitions, natural language processing, cross-language opinion target extraction, Information retrieval, target language, opinion target extraction, CRF-based learning, rich labeled data, conditional random fields-based learning, Feature extraction, monolingual cotraining algorithm, Chinese opinion target extraction, source language]
Sparse Bayesian Adversarial Learning Using Relevance Vector Machine Ensembles
2012 IEEE 12th International Conference on Data Mining
None
2012
Data mining tasks are made more complicated when adversaries attack by modifying malicious data to evade detection. The main challenge lies in finding a robust learning model that is insensitive to unpredictable malicious data distribution. In this paper, we present a sparse relevance vector machine ensemble for adversarial learning. The novelty of our work is the use of individualized kernel parameters to model potential adversarial attacks during model training. We allow the kernel parameters to drift in the direction that minimizes the likelihood of the positive data. This step is interleaved with learning the weights and the weight priors of a relevance vector machine. Our empirical results demonstrate that an ensemble of such relevance vector machine models is more robust to adversarial attacks.
[malicious data, Error analysis, adversarial learning, data mining, kernel parameter learning, robust learning model, relevance vector machine models, sparse Bayesian adversarial learning, Training, evade detection, Training data, adversaries attack, sparse relevance vector, spare Bayesian learning, belief networks, learning (artificial intelligence), Kernel, relevance vector machine, data mining tasks, support vector machines, relevance vector machine ensembles, individualized kernel parameters, Vectors, Support vector machines, Data models, minimisation]
Mining Personal Context-Aware Preferences for Mobile Users
2012 IEEE 12th International Conference on Data Mining
None
2012
In this paper, we illustrate how to extract personal context-aware preferences from the context-rich device logs (i.e., context logs) for building novel personalized context-aware recommender systems. A critical challenge along this line is that the context log of each individual user may not contain sufficient data for mining his/her context-aware preferences. Therefore, we propose to first learn common context-aware preferences from the context logs of many users. Then, the preference of each user can be represented as a distribution of these common context-aware preferences. Specifically, we develop two approaches for mining common context-aware preferences based on two different assumptions, namely, context independent and context dependent assumptions, which can fit into different application scenarios. Finally, extensive experiments on a real-world data set show that both approaches are effective and outperform baselines with respect to mining personal context-aware preferences for mobile users.
[Context, sufficient data mining, context dependent assumptions, data mining, Context-Aware Recommendation, Mobile communication, context-aware preference mining, Data mining, Training, Mobile Users, personalized context-aware recommender systems, mobile computing, recommender systems, context log, context-rich device logs, Markov processes, mobile users, personal context-aware preference mining, Personal Context-Aware Preferences, Recommender systems, Context modeling]
Inferring the Underlying Structure of Information Cascades
2012 IEEE 12th International Conference on Data Mining
None
2012
In social networks, information and influence diffuse among users as cascades. While the importance of studying cascades has been recognized in various applications, it is difficult to observe the complete structure of cascades in practice. In this paper we study the cascade inference problem following the independent cascade model, and provide a full treatment from complexity to algorithms: (a) we propose the idea of consistent trees as the inferred structures for cascades, these trees connect source nodes and observed nodes with paths satisfying the constraints from the observed temporal information. (b) We introduce metrics to measure the likelihood of consistent trees as inferred cascades, as well as several optimization problems for finding them. (c) We show that the decision problems for consistent trees are in general NP-complete, and that the optimization problems are hard to approximate. (d) We provide approximation algorithms with performance guarantees on the quality of the inferred cascades, as well as heuristics. We experimentally verify the efficiency and effectiveness of our inference algorithms, using real and synthetic data.
[Measurement, Steiner trees, Uncertainty, decision theory, independent cascade model, approximation algorithms, Approximation methods, optimisation, underlying information cascade structure inference problem, constraint satisfaction, performance guarantees, NP-complete problems, consistent trees, inferred cascade quality, information diffusion, approximation theory, temporal information, social networks, inferred structures, source nodes, inference mechanisms, constraint satisfaction problems, Vegetation, cascade prediction, Approximation algorithms, social networking (online), Inference algorithms, decision problems, optimization problems]
Message from the Conference Chairs
2013 IEEE 13th International Conference on Data Mining
None
2013
Presents the introductory welcome message from the conference proceedings.
[]
Message from the Program Co-Chairs
2013 IEEE 13th International Conference on Data Mining
None
2013
Presents the introductory welcome message from the conference proceedings.
[]
Conference Organizers
2013 IEEE 13th International Conference on Data Mining
None
2013
Provides a listing of current committee members and society officers.
[]
Program Committee
2013 IEEE 13th International Conference on Data Mining
None
2013
Provides a listing of current committee members and society officers.
[]
Keynotes
2013 IEEE 13th International Conference on Data Mining
None
2013
Provides an abstract for each of the keynote presentations and may include a brief professional biography of each
[]
Tutorials [3 abstracts]
2013 IEEE 13th International Conference on Data Mining
None
2013
Provides an abstract of the three tutorial presentations. The complete presentation was not made available for publication as part of the conference proceedings.
[Analytical models, Privacy, Sociology, Tutorials, Media, Data mining, Informatics]
Subgraph Enumeration in Dynamic Graphs
2013 IEEE 13th International Conference on Data Mining
None
2013
A fundamental problem in many applications involving social and biological networks is to identify and count the number of embeddings of a given small sub graph in a large graph. Often, they involve dynamic graphs, in which the graph changes incrementally (e.g., by edge addition/deletion). We study the Dynamic Sub graph Enumeration (DSE) Problem, where the goal is to maintain a dynamic data structure to solve the sub graph enumeration problem efficiently when the graph changes incrementally. We develop a new data structure that combines two techniques: (i) the color-coding technique of Alon et al., 2008, for enumerating trees, and (ii) a dynamic data structure for maintaining the h-index of the graph (developed by Eppstein and Spiro, 2009). We derive worst case bounds for the update time in terms of the h-index of the graph and the maximum degree. We also study the empirical performance of our algorithm in a large set of real networks, and find significant improvement over the static methods.
[Algorithm design and analysis, h-index, Heuristic algorithms, graph theory, dynamic data structure, data mining, trees (mathematics), social networks, DSE problem, Color, Subgraph enumeration, Data structures, color coding, Approximation methods, worst case bounds, biological networks, color-coding technique, dynamic subgraph enumeration problem, subgraph enumeration, Approximation algorithms, Silicon, data structures, enumerating trees]
A Masking Index for Quantifying Hidden Glitches
2013 IEEE 13th International Conference on Data Mining
None
2013
Data glitches are errors in a data set, they are complex entities that often span multiple attributes and records. When they co-occur in data, the presence of one type of glitch can hinder the detection of another type of glitch. This phenomenon is called masking. In this paper, we define two important types of masking, and we propose a novel, statistically rigorous indicator called masking index for quantifying the hidden glitches in four cases of masking: outliers masked by missing values, outliers masked by duplicates, duplicates masked by missing values, and duplicates masked by outliers. The masking index is critical for data quality profiling and data exploration, it enables a user to measure the extent of masking and hence the confidence in the data. In this sense, it is a valuable data quality index for measuring the true cleanliness of the data. It is also an objective and quantitative basis for choosing an anomaly detection method that is best suited for the glitches that are present in any given data set. We demonstrate the utility and effectiveness of the masking index by intensive experiments on synthetic and real-world datasets.
[data quality index, data glitch detection, true cleanliness, anomaly detection method, outliers, Data mining, real-world datasets, data quality profiling, outlier detection, Robustness, data exploration, missing values, synthetic datasets, masking, data analysis, statistical rigorous indicator, Cleaning, Indexes, Equations, Anomaly detection, data cleaning, Software, masking index, Arrays, duplicate record identification]
CSI: Charged System Influence Model for Human Behavior Prediction
2013 IEEE 13th International Conference on Data Mining
None
2013
Social influence has been widely studied in areas of viral marketing, information diffusion and health care. Currently, most influence models only deal with a single influence without the interference of other influences. Also, the influence spreading in previous models must be triggered by individuals who have been activated by the influence. In this paper, we argue that it is the attraction from a specific influence makes an individual choose to spread it among multiple influences. Inspired by charged system theory in physics, a new influence model is proposed, considering individual features and social structure features. It also gives a natural description about how individuals make decisions among multiple influences. Then a novel algorithm based on this model is provided to predict human behavior. Extensive experiments on three real-world datasets demonstrate that our model and algorithm statistically outperform the state-of-the-art methods in terms of prediction accuracy.
[Correlation, Social network services, Force, Predictive models, Gaussian distribution, social structure features, charged system influence model, CSI, Prediction algorithms, social networking (online), behavioural sciences computing, human behavior prediction, prediction accuracy, Integrated circuit modeling]
Context-Aware MIML Instance Annotation
2013 IEEE 13th International Conference on Data Mining
None
2013
In multi-instance multi-label (MIML) instance annotation, the goal is to learn an instance classifier while training on a MIML dataset, which consists of bags of instances paired with label sets, instance labels are not provided in the training data. The MIML formulation can be applied in many domains. For example, in an image domain, bags are images, instances are feature vectors representing segments in the images, and the label sets are lists of objects or categories present in each image. Although many MIML algorithms have been developed for predicting the label set of a new bag, only a few have been specifically designed to predict instance labels. We propose MIML-ECC (ensemble of classifier chains), which exploits bag-level context through label correlations to improve instance-level prediction accuracy. The proposed method is scalable in all dimensions of a problem (bags, instances, classes, and feature dimension), and has no parameters that require tuning (which is a problem for prior methods). In experiments on two image datasets, a bioacoustics dataset, and two artificial datasets, MIML-ECC achieves higher or comparable accuracy in comparison to several recent methods and baselines.
[classifier chain ensemble, artificial datasets, bag-level context, label correlations, MLC, ensemble, instance annotation, ubiquitous computing, multi-label, Training, Accuracy, Training data, training data, Bismuth, MIML-ECC, learning (artificial intelligence), MIML, Context, pattern classification, context-aware MIML instance annotation, multi-instance multilabel instance annotation, ECC, bioacoustics, instance-level prediction accuracy, Probabilistic logic, Vectors, MIML dataset, instance labels, classifier chains, image datasets, bioacoustics dataset, instance classifier learning, MIML formulation, multiple instance]
Maximizing Expected Model Change for Active Learning in Regression
2013 IEEE 13th International Conference on Data Mining
None
2013
Active learning is well-motivated in many supervised learning tasks where unlabeled data may be abundant but labeled examples are expensive to obtain. The goal of active learning is to maximize the performance of a learning model using as few labeled training data as possible, thereby minimizing the cost of data annotation. So far, there is still very limited work on active learning for regression. In this paper, we propose a new active learning framework for regression called Expected Model Change Maximization (EMCM), which aims to choose the examples that lead to the largest change to the current model. The model change is measured as the difference between the current model parameters and the updated parameters after training with the enlarged training set. Inspired by the Stochastic Gradient Descent (SGD) update rule, the change is estimated as the gradient of the loss with respect to a candidate example for active learning. Under this framework, we derive novel active learning algorithms for both linear regression and nonlinear regression to select the most informative examples. Extensive experimental results on the benchmark data sets from UCI machine learning repository have demonstrated that the proposed algorithms are highly effective in choosing the most informative examples and robust to various types of data distributions.
[SGD update rule, active learning algorithms, nonlinear regression, Machine learning algorithms, supervised learning, regression analysis, labeled training data, Training, Training data, UCI machine learning repository, expected model change maximization, enlarged training set, stochastic gradient descent update rule, learning (artificial intelligence), stochastic processes, Expected Model Change Maximization, gradient methods, active learning framework, Regression tree analysis, Linear regression, Active learning, EMCM, Current measurement, data annotation, data distributions, Linear Regression, expectation-maximisation algorithm, Data models, unlabeled data, Nonlinear regression]
The Pairwise Gaussian Random Field for High-Dimensional Data Imputation
2013 IEEE 13th International Conference on Data Mining
None
2013
In this paper, we consider the problem of imputation (recovering missing values) in very high-dimensional data with an arbitrary covariance structure. The modern solution to this problem is the Gaussian Markov random field (GMRF). The problem with applying a GMRF to very high-dimensional data imputation is that while the GMRF model itself can be useful even for data having tens of thousands of dimensions, utilizing a GMRF requires access to a sparsified, inverse covariance matrix for the data. Computing this matrix using even state-of-the-art methods is very costly, as it typically requires first estimating the covariance matrix from the data (at a O(nm2) cost for m dimensions and n data points) and then performing a regularized inversion of the estimated covariance matrix, which is also very expensive. This is impractical for even moderately-sized, high-dimensional data sets. In this paper, we propose a very simple alternative to the GMRF called the pair wise Gaussian random field or PGRF for short. The PGRF is a graphical, factor-based model. Unlike traditional Gaussian or GMRF models, a PGRF does not require a covariance or correlation matrix as input. Instead, a PGRF takes as input a set of p (dimension, dimension) pairs for which the user suspects there might be a strong correlation or anti-correlation. This set of pairs defines the graphical structure of the model, with a simple Gaussian factor associated with each of the p (dimension, dimension) pairs. Using this structure, it is easy to perform simultaneous inference and imputation of the model. The key benefit of the approach is that the time required for the PGRF to perform inference is approximately linear with respect to p, where p will typically be much smaller than the number of entries in a m&#x00D7;m covariance or precision matrix.
[Correlation, moderately-sized high-dimensional data sets, graphical factor-based model, Computational modeling, random processes, Gaussian distribution, arbitrary covariance structure, graphical structure, high-dimensional data imputation, classification, Covariance matrices, Gaussian factor, PGRF, pairwise Gaussian random field, Monte Carlo methods, imputation, Gaussian processes, Markov processes, Data models, computational complexity]
Controlling Attribute Effect in Linear Regression
2013 IEEE 13th International Conference on Data Mining
None
2013
In data mining we often have to learn from biased data, because, for instance, data comes from different batches or there was a gender or racial bias in the collection of social data. In some applications it may be necessary to explicitly control this bias in the models we learn from the data. This paper is the first to study learning linear regression models under constraints that control the biasing effect of a given attribute such as gender or batch number. We show how propensity modeling can be used for factoring out the part of the bias that can be justified by externally provided explanatory attributes. Then we analytically derive linear models that minimize squared error while controlling the bias by imposing constraints on the mean outcome or residuals of the models. Experiments with discrimination-aware crime prediction and batch effect normalization tasks show that the proposed techniques are successful in controlling attribute effects in linear regression models.
[gender, data mining, regression analysis, Predictive models, Data mining, Analytical models, learning (artificial intelligence), social data collection, Biological system modeling, Linear regression, Batch Effects, learning linear regression models, Vectors, batch number, attribute effect control, racial bias, Fair Data Mining, Propensity Score, attribute biasing effect, discrimination-aware crime prediction, Linear Regression, squared error minimization, Data models, batch effect normalization tasks, propensity modeling]
Active Matrix Completion
2013 IEEE 13th International Conference on Data Mining
None
2013
Recovering a matrix from a sampling of its entries is a problem of rapidly growing interest and has been studied under the name of matrix completion. It occurs in many areas of engineering and applied science. In most machine learning and data mining applications, it is possible to leverage the expertise of human oracles to improve the performance of the system. It is therefore natural to extend this idea of "human-in-the-loop" to the matrix completion problem. However, considering the enormity of data in the modern era, manually completing all the entries in a matrix will be an expensive process in terms of time, labor and human expertise, human oracles can only provide selective supervision to guide the solution process. Thus, appropriately identifying a subset of missing entries (for manual annotation) in an incomplete matrix is of paramount practical importance, this can potentially lead to better reconstructions of the incomplete matrix with minimal human effort. In this paper, we propose novel algorithms to address this issue. Since the query locations are actively selected by the algorithms, we refer to these methods as active matrix completion algorithms. The proposed techniques are generic and the same frameworks can be used in a wide variety of applications including recommendation systems, transductive / multi-label active learning, active learning in regression and active feature acquisition among others. Our extensive empirical analysis on several challenging real-world datasets certify the merit and versatility of the proposed frameworks in efficiently exploiting human intelligence in data mining / machine learning applications.
[incomplete matrix reconstruction, empirical analysis, Uncertainty, matrix recovery, query location, data mining, Manuals, active matrix completion, human-in-the-loop, Matrix decomposition, machine learning, Covariance matrices, Equations, matrix algebra, human intelligence, Collaboration, Prediction algorithms, learning (artificial intelligence), data mining applications]
Modeling Temporal Adoptions Using Dynamic Matrix Factorization
2013 IEEE 13th International Conference on Data Mining
None
2013
The problem of recommending items to users is relevant to many applications and the problem has often been solved using methods developed from Collaborative Filtering (CF). Collaborative Filtering model-based methods such as Matrix Factorization have been shown to produce good results for static rating-type data, but have not been applied to time-stamped item adoption data. In this paper, we adopted a Dynamic Matrix Factorization (DMF) technique to derive different temporal factorization models that can predict missing adoptions at different time steps in the users' adoption history. This DMF technique is an extension of the Non-negative Matrix Factorization (NMF) based on the well-known class of models called Linear Dynamical Systems (LDS). By evaluating our proposed models against NMF and TimeSVD++ on two real datasets extracted from ACM Digital Library and DBLP, we show empirically that DMF can predict adoptions more accurately than the NMF for several prediction tasks as well as outperforming TimeSVD++ in some of the prediction tasks. We further illustrate the ability of DMF to discover evolving research interests for a few author examples.
[collaborative filtering, DMF technique, Kalman Filter, Heuristic algorithms, State Space Models, collaborative filtering model-based method, Predictive models, matrix decomposition, ACM Digital Library, linear dynamical systems, Dynamic Matrix Factorization, Linear Dynamical Systems, LDS, Kalman filters, Mathematical model, DBLP, dynamic matrix factorization technique, NMF, Probabilistic logic, missing adoptions, Vectors, nonnegative matrix factorization, temporal adoption modeling, user adoption history, recommender systems, Data models, real dataset extraction, TimeSVD++]
Modeling Preferences with Availability Constraints
2013 IEEE 13th International Conference on Data Mining
None
2013
User preferences are commonly learned from historical data whereby users express preferences for items, e.g., through consumption of products or services. Most work assumes that a user is not constrained in their selection of items. This assumption does not take into account the availability constraint, whereby users could only access some items, but not others. For example, in subscription-based systems, we can observe only those historical preferences on subscribed (available) items. However, the objective is to predict preferences on unsubscribed (unavailable) items, which do not appear in the historical observations due to their (lack of) availability. To model preferences in a probabilistic manner and address the issue of availability constraint, we develop a graphical model, called Latent Transition Model (LTM) to discover users' latent interests. LTM is novel in incorporating transitions in interests when certain items are not available to the user. Experiments on a real-life implicit feedback dataset demonstrate that LTM is effective in discovering customers' latent interests, and it achieves significant improvements in prediction accuracy over baselines that do not model transitions.
[Availability, Industries, topic transition, latent interests, Cable TV, user preference modelling, LTM, topic model, unsubscribed items, probability, availability constraint, user preferences, Watches, consumer behaviour, Probability distribution, unavailable items, latent transition model, psychology, graphical model, customer latent interests, user latent interests, real-life implicit feedback dataset, Data models, data handling]
wRACOG: A Gibbs Sampling-Based Oversampling Technique
2013 IEEE 13th International Conference on Data Mining
None
2013
As machine learning techniques mature and are used to tackle complex scientific problems, challenges arise such as the imbalanced class distribution problem, where one of the target class labels is under-represented in comparison with other classes. Existing over sampling approaches for addressing this problem typically do not consider the probability distribution of the minority class while synthetically generating new samples. As a result, the minority class is not well represented which leads to high misclassification error. We introduce wRACOG, a Gibbs sampling-based over sampling approach to synthetically generating and strategically selecting new minority class samples. The Gibbs sampler uses the joint probability distribution of data attributes to generate new minority class samples in the form of a Markov chain. wRACOG iteratively learns a model by selecting samples from the Markov chain that have the highest probability of being misclassified. We validate the effectiveness of wRACOG using five UCI datasets and one new application domain dataset. A comparative study of wRACOG with three other well-known resampling methods provides evidence that wRACOG offers a definite improvement in classification accuracy for minority class samples over other methods.
[iterative methods, UCI datasets, wRACOG, Predictive models, application domain dataset, Probability distribution, statistical distributions, machine learning techniques, strategically selected minority class, Markov chain Monte Carlo (MCMC), Markov chain, target class labels, probability distribution, learning (artificial intelligence), Joints, Gibbs sampling-based oversampling technique, oversampling, pattern classification, sampling methods, imbalanced class distribution problem, iterative learning, classilication accuracy improvement, Gibbs sampling, Equations, misclassilication error, Sensitivity, synthetically generated minority class, Markov processes, Imbalanced class distribution, Random variables, data attributes]
Efficient Visualization of Large-Scale Data Tables through Reordering and Entropy Minimization
2013 IEEE 13th International Conference on Data Mining
None
2013
Visualization of data tables with n examples and m columns using heat maps provides a holistic view of the original data. As there are n! ways to order rows and m! ways to order columns, and data tables are typically ordered without regard to visual inspection, heat maps of the original data tables often appear as noisy images. However, if rows and columns of a data table are ordered such that similar rows and similar columns are grouped together, a heat map may provide a deep insight into the underlying data distribution. We propose an information-theoretic approach to produce a well-ordered data table. In particular, we search for ordering that minimizes entropy of residuals of predictive coding applied on the ordered data table. This formalization leads to a novel ordering procedure, EM-ordering, that can be applied separately on rows and columns. For ordering of rows, EM-ordering repeats until convergence the steps of (1) rescaling columns and (2) solving a Traveling Salesman Problem (TSP) where rows are treated as cities. To allow fast ordering of large data tables, we propose an efficient TSP heuristic with modest O(n log(n)) time complexity. When compared to the existing state-of-the-art reordering approaches, we show that the method often provides heat maps of higher visual quality, while being significantly more scalable. Moreover, analysis of real-world traffic and financial data sets using the proposed method, which allowed us to readily gain deeper insights about the data, further confirmed that EM-ordering can be a valuable tool for visual exploration of large-scale data sets.
[reordering, heatmap, information-theoretic approach, data distribution, TSP heuristic, large-scale data table visualization, financial data set analysis, Entropy, EM-ordering, real-world traffic analysis, travelling salesman problems, entropy, Clustering algorithms, data visualisation, Cities and towns, financial data processing, ordered data table, data visualization, entropy minimization, traveling salesman problem, heat map, data seriation, time complexity, Minimization, traffic engineering computing, predictive coding residual, Heating, rescaling columns, large-scale data set visual exploration, Data visualization, data reordering, large-scale data, Principal component analysis, computational complexity]
Local and Global Discriminative Learning for Unsupervised Feature Selection
2013 IEEE 13th International Conference on Data Mining
None
2013
In this paper, we consider the problem of feature selection in unsupervised learning scenario. Recently, spectral feature selection methods, which leverage both the graph Laplacian and the learning mechanism, have received considerable attention. However, when there are lots of irrelevant or noisy features, such graphs may not be reliable and then mislead the selection of features. In this paper, we propose the Local and Global Discriminative learning for unsupervised Feature Selection (LGDFS), which integrates a global and a set of locally linear regression model with weighted l<sub>2</sub>-norm regularization into a unified learning framework. By exploring the discriminative and geometrical information in the weighted feature space, which alleviates the effects of the irrelevant features, our approach can find the most representative features to well respect the cluster structure of the data. Experimental results on several benchmark data sets are provided to validate the effectiveness of the proposed approach.
[benchmark data sets, graph theory, regression analysis, Complexity theory, geometrical information, discriminative information, Manifolds, Clustering algorithms, Cost function, data cluster structure, global discriminative learning, graph Laplacian, feature selection, Laplace equations, Estimation, LGDFS, weighted l<sub>2</sub>-norm regularization, unsupervised feature selection, unsupervised learning scenario, unsupervised learning, pattern clustering, spectral feature selection method, locally linear regression model, local discriminative learning, weighted feature space]
Generative Maximum Entropy Learning for Multiclass Classification
2013 IEEE 13th International Conference on Data Mining
None
2013
Maximum entropy approach to classification is very well studied in applied statistics and machine learning and almost all the methods that exists in literature are discriminative in nature. In this paper, we introduce a maximum entropy classification method with feature selection for large dimensional data such as text datasets that is generative in nature. To tackle the curse of dimensionality of large data sets, we employ conditional independence assumption (Naive Bayes) and we perform feature selection simultaneously, by enforcing a 'maximum discrimination' between estimated class conditional densities. For two class problems, in the proposed method, we use Jeffreys (J) divergence to discriminate the class conditional densities. To extend our method to the multi-class case, we propose a completely new approach by considering a multi-distribution divergence: we replace Jeffreys divergence by Jensen-Shannon (JS) divergence to discriminate conditional densities of multiple classes. In order to reduce computational complexity, we employ a modified Jensen-Shannon divergence (JS_GM), based on AM-GM inequality. We show that the resulting divergence is a natural generalization of Jeffreys divergence to a multiple distributions case. As far as the theoretical justifications are concerned we show that when one intends to select the best features in a generative maximum entropy approach, maximum discrimination using J-divergence emerges naturally in binary classification. Performance and comparative study of the proposed algorithms have been demonstrated on large dimensional text and gene expression datasets that show our methods scale up very well with large dimensional datasets.
[text analysis, class conditional density discrimination, Jeffreys divergence, large dimensional text dataset, gene expression datasets, Entropy, Jefferys Divergence, class conditional density estimation, maximum discrimination, Training, biology computing, multiclass classification, Maximum Entropy, Training data, generative maximum entropy learning, Mathematical model, learning (artificial intelligence), binary classification, feature selection, computational complexity reduction, pattern classification, Computational modeling, large dimensional data, maximum entropy classification method, multidistribution divergence, Estimation, Jensen-Shannon Divergence, Text categorization, conditional independence assumption, AM-GM inequality, naive Bayes, maximum entropy methods, Jensen-Shannon divergence, Data models, Bayes methods, computational complexity, JS divergence]
A Parameter-Free Spatio-Temporal Pattern Mining Model to Catalog Global Ocean Dynamics
2013 IEEE 13th International Conference on Data Mining
None
2013
As spatio-temporal data have become ubiquitous, an increasing challenge facing computer scientists is that of identifying discrete patterns in continuous spatio-temporal fields. In this paper, we introduce a parameter-free pattern mining application that is able to identify dynamic anomalies in ocean data, known as ocean eddies. Despite ocean eddy monitoring being an active field of research, we provide one of the first quantitative analyses of the performance of the most used monitoring algorithms. We present an incomplete information validation technique, that uses the performance of two methods to construct an imperfect ground truth to test the significance of patterns discovered as well as the relative performance of pattern mining algorithms. These methods, in addition to the validation schemes discussed provide researchers new directions in analyzing large unlabeled climate datasets.
[incomplete information validation technique, Sea surface, parameter-free spatiotemporal pattern mining model, pattern mining, Noise, dynamic anomalies identify, ocean data, ocean eddies, data mining, ocean eddy monitoring, geophysics computing, global ocean dynamic cataloging, Data mining, Ocean temperature, unlabeled climate datasets, spatio-temporal data mining, Satellites, pattern mining algorithms, oceanography, Monitoring]
Transfer Learning across Networks for Collective Classification
2013 IEEE 13th International Conference on Data Mining
None
2013
This paper addresses the problem of transferring useful knowledge from a source network to predict node labels in a newly formed target network. While existing transfer learning research has primarily focused on vector-based data, in which the instances are assumed to be independent and identically distributed, how to effectively transfer knowledge across different information networks has not been well studied, mainly because networks may have their distinct node features and link relationships between nodes. In this paper, we propose a new transfer learning algorithm that attempts to transfer common latent structure features across the source and target networks. The proposed algorithm discovers these latent features by constructing label propagation matrices in the source and target networks, and mapping them into a shared latent feature space. The latent features capture common structure patterns shared by two networks, and serve as domain-independent features to be transferred between networks. Together with domain-dependent node features, we thereafter propose an iterative classification algorithm that leverages label correlations to predict node labels in the target network. Experiments on real-world networks demonstrate that our proposed algorithm can successfully achieve knowledge transfer between networks to help improve the accuracy of classifying nodes in the target network.
[Knowledge engineering, iterative methods, label correlations, Transfer learning, node features, network theory (graphs), transfer learning algorithm, Optimization, source network, Convergence, real-world networks, label propagation matrices, Network, target network, Prediction algorithms, node classification accuracy improvement, node label prediction, Iterative methods, learning (artificial intelligence), common latent structure feature transfer, pattern classification, collective classification, shared latent feature space, Subspace constraints, link relationships, information networks, Knowledge transfer, vector-based data, knowledge transfer, iterative classification algorithm, domain-dependent node features]
Distributed Column Subset Selection on MapReduce
2013 IEEE 13th International Conference on Data Mining
None
2013
Given a very large data set distributed over a cluster of several nodes, this paper addresses the problem of selecting a few data instances that best represent the entire data set. The solution to this problem is of a crucial importance in the big data era as it enables data analysts to understand the insights of the data and explore its hidden structure. The selected instances can also be used for data preprocessing tasks such as learning a low-dimensional embedding of the data points or computing a low-rank approximation of the corresponding matrix. The paper first formulates the problem as the selection of a few representative columns from a matrix whose columns are massively distributed, and it then proposes a MapReduce algorithm for selecting those representatives. The algorithm first learns a concise representation of all columns using random projection, and it then solves a generalized column subset selection problem at each machine in which a subset of columns are selected from the sub-matrix on that machine such that the reconstruction error of the concise representation is minimized. The paper then demonstrates the effectiveness and efficiency of the proposed algorithm through an empirical evaluation on benchmark data sets.
[data analysis, MapReduce algorithm, Data handling, greedy algorithms, Big Data, corresponding matrix, Column Subset Selection, Information management, Approximation methods, low-rank approximation, Data storage systems, MapReduce, distributed column subset selection, Greedy Algorithms, distributed algorithms, Distributed databases, data analysts, Approximation algorithms, Distributed Computing, generalized column subset selection problem, random projection, Cascading style sheets]
Compression-Based Graph Mining Exploiting Structure Primitives
2013 IEEE 13th International Conference on Data Mining
None
2013
How can we retrieve information from sparse graphs? Traditional graph mining approaches focus on discovering dense patterns inside complex networks, for example modularity-based or cut-based methods. However, most real world data sets are very sparse. Nevertheless, traditional approaches tend to omit interesting sparse patterns like stars. In this paper, we propose a novel graph mining technique modeling the transitivity and the hub ness of a graph using structure primitives. We exploit these structure primitives for effective graph compression using the Minimum Description Length Principle. The compression rate is an unbiased measure for the transitivity or hub ness and therefore provides interesting insights into the structure of even very sparse graphs. Since real graphs can be composed of sub graphs of different structures, we propose a novel algorithm CXprime (Compression-based exploiting Primitives) for clustering graphs using our coding scheme as an objective function. In contrast to traditional graph clustering methods, our algorithm automatically recognizes different types of sub graphs without requiring the user to specify input parameters. Additionally we propose a novel link prediction algorithm based on the detected substructures, which increases the quality of former methods. Extensive experiments evaluate our algorithms on synthetic and real data.
[Compression, Communities, graph theory, data mining, link prediction algorithm, compression-based graph mining technique, Entropy, star parse patterns, coding scheme, Data mining, structure primitives, dense pattern discovery, Clustering algorithms, example modularity-based method, Prediction algorithms, objective function, Graph mining, Link prediction, data compression, compression rate, Partition, sparse graphs, Receivers, information retrieval, Encoding, CXprime algorithm, pattern clustering, minimum description length principle, Minimum Description Length, cut-based methods, graph clustering methods]
Online Estimation of Discrete Densities
2013 IEEE 13th International Conference on Data Mining
None
2013
We address the problem of estimating a discrete joint density online, that is, the algorithm is only provided the current example and its current estimate. The proposed online estimator of discrete densities, EDDO (Estimation of Discrete Densities Online), uses classifier chains to model dependencies among features. Each classifier in the chain estimates the probability of one particular feature. Because a single chain may not provide a reliable estimate, we also consider ensembles of classifier chains and ensembles of weighted classifier chains. For all density estimators, we provide consistency proofs and propose algorithms to perform certain inference tasks. The empirical evaluation of the estimators is conducted in several experiments and on data sets of up to several million instances: We compare them to density estimates computed from Bayesian structure learners, evaluate them under the influence of noise, measure their ability to deal with concept drift, and measure the run-time performance. Our experiments demonstrate that, even though designed to work online, EDDO delivers estimators of competitive accuracy compared to batch Bayesian structure learners and batch variants of EDDO.
[probability estimation, pattern classification, weighted random classifier chains, Hoeffding trees, Density measurement, estimation theory, Bayesian structure learners, Radiation detectors, Estimation, consistency proofs, data streams, random processes, (ensembles of) classifier chains, EDDO batch variants, Noise measurement, estimation of discrete densities online, discrete joint density estimation, online estimator, density estimators, Inference algorithms, Bayes methods, learning (artificial intelligence), Joints, density estimation]
Extraction of Interpretable Multivariate Patterns for Early Diagnostics
2013 IEEE 13th International Conference on Data Mining
None
2013
Leveraging temporal observations to predict a patient's health state at a future period is a very challenging task. Providing such a prediction early and accurately allows for designing a more successful treatment that starts before a disease completely develops. Information for this kind of early diagnosis could be extracted by use of temporal data mining methods for handling complex multivariate time series. However, physicians usually prefer to use interpretable models that can be easily explained, rather than relying on more complex black-box approaches. In this study, a temporal data mining method is proposed for extracting interpretable patterns from multivariate time series data, which can be used to assist in providing interpretable early diagnosis. The problem is formulated as an optimization based binary classification task addressed in three steps. First, the time series data is transformed into a binary matrix representation suitable for application of classification methods. Second, a novel convex-concave optimization problem is defined to extract multivariate patterns from the constructed binary matrix. Then, a mixed integer discrete optimization formulation is provided to reduce the dimensionality and extract interpretable multivariate patterns. Finally, those interpretable multivariate patterns are used for early classification in challenging clinical applications. In the conducted experiments on two human viral infection datasets and a larger myocardial infarction dataset, the proposed method was more accurate and provided classifications earlier than three alternative state-of-the-art methods.
[integer programming, optimization based binary classification task, data mining, binary matrix representation, interpretable multivariate pattern extraction, Data mining, temporal observations, Optimization, data reduction, dimensionality reduction, temporal data mining methods, early classification methods, interpretability, patient health state prediction, concave programming, pattern classification, myocardial infarction dataset, Time series analysis, multivariate time series, convex programming, diseases, time series, Vectors, convex-concave optimization problem, mixed integer discrete optimization formulation, Diseases, matrix algebra, early diagnosis, early classification, pattern extraction, complex black-box approach, complex multivariate time series data, human viral infection datasets, medical computing, patient diagnosis, early diagnostics, Logistics]
Search Behavior Based Latent Semantic User Segmentation for Advertising Targeting
2013 IEEE 13th International Conference on Data Mining
None
2013
The popularity of internet usage greatly motivates the online advertising activities. Compared to advertising on traditional media, online advertising has rich information as well as necessary techniques to achieve precise user targeting. This rich information includes the search behaviors of a user, such as queries issued, or the ads clicked by the user. For popular websites with large number of active users, ad delivery targeting at individual users puts too much burden on the system. User segmentation is an alternative way to relieve this burden by grouping users of similar interests together, then the ad delivery system targets the user segments to display relevant ads, instead of individual users. Existing user segmentation work either adapts clustering methods without considering the hidden semantics embedded in the data, such as K-means, or treats users as data instance and clusters users indirectly even if the latent semantics is incorporated into the transformed data, such as PLSA or LDA. In this paper, we present a search behavior based latent semantic user segmentation method and validate its effectiveness on new ads. Instead of treating users as data instances, they are used as attributes of user issued queries or clicked ads which are considered to be data instances. LDA is then applied to this data set to directly obtain the user segments. Compared to popular K-means clustering, our approach achieves higher CTR values on new ads, with only simple search information.
[Clustering methods, Websites, Noise, behavioural sciences, Predictive models, LDA, advertising, online advertising activities, Optimization, user segmentation, user search behaviors, query processing, Semantics, Clustering algorithms, ad delivery targeting, CTR values, Advertising, search information, K-means clustering, precise user targeting, pattern clustering, ad delivery system, search behavior based latent semantic user segmentation method, Internet, Web sites, advertising targeting]
Mixed Membership Subspace Clustering
2013 IEEE 13th International Conference on Data Mining
None
2013
Clustering is one of the fundamental data mining tasks. While traditional clustering techniques assign each object to a single cluster only, in many applications it has been observed that objects might belong to multiple clusters with different degrees. In this work, we present a Bayesian framework to tackle the challenge of mixed membership clustering for vector data. We exploit the ideas of subspace clustering where the relevance of dimensions might be different for each cluster. Combining the relevance of the dimensions with the cluster membership degree of the objects, we propose a novel type of mixture model able to represent data containing mixed membership subspace clusters. For learning our model, we develop an efficient algorithm based on variational inference allowing easy parallelization. In our empirical study on synthetic and real data we show the strengths of our novel clustering technique.
[Adaptation models, real data, data mining, Bayesian framework, Vectors, mixed membership subspace clustering technique, Approximation methods, inference mechanisms, mixed membership clustering, Equations, pattern clustering, subspace clustering, vector data, synthetic data, variational techniques, model based clustering, Data models, Bayes methods, Random variables, object cluster membership degree, belief networks, variational inference]
Spectral Subspace Clustering for Graphs with Feature Vectors
2013 IEEE 13th International Conference on Data Mining
None
2013
Clustering graphs annotated with feature vectors has recently gained much attention. The goal is to detect groups of vertices that are densely connected in the graph as well as similar with respect to their feature values. While early approaches treated all dimensions of the feature space as equally important, more advanced techniques consider the varying relevance of dimensions for different groups. In this work, we propose a novel clustering method for graphs with feature vectors based on the principle of spectral clustering. Following the idea of subspace clustering, our method detects for each cluster an individual set of relevant features. Since spectral clustering is based on the eigendecomposition of the affinity matrix, which strongly depends on the choice of features, our method simultaneously learns the grouping of vertices and the affinity matrix. To tackle the fundamental challenge of comparing the clustering structures for different feature subsets, we define an objective function that is unbiased regarding the number of relevant features. We develop the algorithm SSCG and we show its application for multiple real-world datasets.
[feature values, Clustering methods, graph theory, learning, real-world datasets, networks, eigenvalues and eigenfunctions, Reactive power, graphs, attributed graphs, learning (artificial intelligence), Kernel, affinity matrix eigendecomposition, spectral clustering, vertex grouping, SSCG, feature space, Linear programming, Vectors, Equations, matrix algebra, graph clustering, feature vector, pattern clustering, subspace clustering, Feature extraction, spectral subspace clustering]
Permutation-Based Sequential Pattern Hiding
2013 IEEE 13th International Conference on Data Mining
None
2013
Sequence data are increasingly shared to enable mining applications, in various domains such as marketing, telecommunications, and healthcare. This, however, may expose sensitive sequential patterns, which lead to intrusive inferences about individuals or leak confidential information about organizations. This paper presents the first permutation-based approach to prevent this threat. Our approach hides sensitive patterns by replacing them with carefully selected permutations that avoid changes in the set of frequent nonsensitive patterns (side-effects) and in the ordering information of sequences (distortion). By doing so, it retains data utility in sequence mining and tasks based on item set properties, as permutation preserves the support of items, unlike deletion, which is used in existing works. To realize our approach, we develop an efficient and effective algorithm for generating permutations with minimal side-effects and distortion. This algorithm also avoids implausible symbol orderings that may exist in certain applications. In addition, we propose a method to hide sensitive patterns from a sequence dataset. Extensive experiments verify that our method allows significantly more accurate data analysis than the state-of the-art approach.
[Algorithm design and analysis, permutation-based sequential pattern hiding, sequence data, data analysis, data mining, Companies, sensitive sequential patterns, Data mining, minimal side-effects, item set property, Itemsets, sequence dataset, Insurance, intrusive inferences, data utility, implausible symbol orderings, data privacy, sequential pattern hiding, data encapsulation, Pattern matching, sequence mining, distortion, permutation]
Utilizing URLs Position to Estimate Intrinsic Query-URL Relevance
2013 IEEE 13th International Conference on Data Mining
None
2013
Query-URL relevance (QUR) is an important criterion to measure the quality of commercial search engines. However, the traditional way to collect high-quality QURs is time-consuming and labor-intensive since it is primarily based on human judges. To address these issues, numerous models have been studied to automatically infer the QURs. Unlike the prior studies in this literature, we first empirically analyze the correlation between multiple annotators' judgments on QURs and URL position in ranking lists. By doing so, we reveal and justify the potential impacts of URL position on inferring intrinsic QURs. Inspired by this finding, a position-sensitive model (PSM) is proposed to infer QURs more accurately. In contrast with most existing approaches that attempt to construct the direct relationship between QURs and the features characterizing query-URL pairs, PSM assumes that the QUR is connected with the features through URL position. We conducted the experiments in real search engine Baidu.com, and compared the experimental results to those of the typical methods used in similar tasks, reporting significant gains over click-through rate and the normalized discounted cumulative gains (NDCGs).
[click-through rate, Correlation, search engines, intrinsic query-URL relevance estimation, Electronic mail, normalized discounted cumulative gains, Training, query processing, QUR position, Relevance, Accuracy, Evaluation component, commercial search engines, feature extraction, Search engines, position-sensitive model, high-quality QURs, query-URL pairs, Educational institutions, Vectors, PSM, URL position, relevance feedback, Seach Engines, ranking lists, annotator judgments, NDCG]
Parameter-Free Audio Motif Discovery in Large Data Archives
2013 IEEE 13th International Conference on Data Mining
None
2013
The discovery of repeated structure, i.e. motifs/near-duplicates, is often the first step in exploratory data mining. As such, the last decade has seen extensive research efforts in motif discovery algorithms for text, DNA, time series, protein sequences, graphs, images, and video. Surprisingly, there has been less attention devoted to finding repeated patterns in audio sequences, in spite of their ubiquity in science and entertainment. While there is significant work for the special case of motifs in music, virtually all this work makes many assumptions about data (often to the point of being genre specific) and thus these algorithms do not generalize to audio sequences containing animal vocalizations, industrial processes, or a host of other domains that we may wish to explore. In this work we introduce a novel technique for finding audio motifs. Our method does not require any domain-specific tuning and is essentially parameter-free. We demonstrate our algorithm on very diverse domains, finding audio motifs in laboratory mice vocalizations, wild animal sounds, music, and human speech. Our experiments demonstrate that our ideas are effective in discovering objectively correct or subjectively plausible motifs. Moreover, we show our novel probabilistic early abandoning approach is efficient, being two to three orders of magnitude faster than brute-force search, and thus faster than real-time for most problems.
[Heuristic algorithms, brute-force search, data mining, Data mining, audio motif finding technique, Spectrogram, music, probabilistic early abandoning approach, industrial processes, wild animal sounds, domain-specific tuning, audio motif, animal vocalizations, parameter-free audio motif discovery, probability, repeated structure discovery, anytime algorithm, audio signal processing, laboratory mice vocalizations, spectrogram, Music, human speech, Feature extraction, Speech, large data archives, Mice, audio sequence repeated pattern finding, exploratory data mining]
Kernel Density Metric Learning
2013 IEEE 13th International Conference on Data Mining
None
2013
This paper introduces a supervised metric learning algorithm, called kernel density metric learning (KDML), which is easy to use and provides nonlinear, probability-based distance measures. KDML constructs a direct nonlinear mapping from the original input space into a feature space based on kernel density estimation. The nonlinear mapping in KDML embodies established distance measures between probability density functions, and leads to correct classification on datasets for which linear metric learning methods would fail. It addresses the severe challenge to kNN when features are from heterogeneous domains and, as a result, the Euclidean or Mahalanobis distance between original feature vectors is not meaningful. Existing metric learning algorithms can then be applied to the KDML features. We also propose an integrated optimization algorithm that learns not only the Mahalanobis matrix but also kernel bandwidths, the only hyper-parameters in the nonlinear mapping. KDML can naturally handle not only numerical features, but also categorical ones, which is rarely found in previous metric learning algorithms. Extensive experimental results on various datasets show that KDML significantly improves existing metric learning algorithms in terms of kNN classification accuracy.
[kNN classification accuracy, Density measurement, supervised metric learning algorithm, direct nonlinear mapping, kernel density metric learning, Optimization, Learning systems, optimisation, probability-based distance measures, heterogeneous domain, KDML, learning (artificial intelligence), input space, Kernel, Mahalanobis distance, pattern classification, kernel density estimation, feature space, kernel bandwidth, probability, Vectors, probability density functions, matrix algebra, feature vectors, Euclidean distance, integrated optimization algorithm, Mahalanobis matrix, dataset classification]
Classification of Multi-dimensional Streaming Time Series by Weighting Each Classifier's Track Record
2013 IEEE 13th International Conference on Data Mining
None
2013
Extensive research on time series classification in the last decade has produced fast and accurate algorithms for the single-dimensional case. However, the increasing prevalence of inexpensive sensors has reinforced the need for algorithms to handle multi-dimensional time series. For example, modern smartphones have at least a dozen sensors capable of producing streaming time series, and hospital-based (and increasingly, home-based) medical devices can produce time series streams from more than twenty sensors. The two most common ways to generalize from single to multi-dimensional data are to use all the streams or just the single best stream as determined at training time. However, as we show here, both approaches can be very brittle. Moreover, neither approach exploits the observation that different sensors may be considered "experts" on different classes. In this work, we introduce a novel framework for multi-dimensional time series classification that weights the class prediction from each time series stream. These weights are based not only on each stream's previous track record on the class it is currently predicting, but also on the distance from the unlabeled object. As we demonstrate with extensive experiments on real data, our method is more accurate than current approaches and particularly robust in the face of concept drift or sensor noise.
[pattern classification, Time series analysis, Footwear, time series, Classification algorithms, classification, class prediction, Training, Wrist, hospital-based medical devices, Training data, multidimensional streaming time series classification, Sensors, medical administrative data processing, multi-dimensional time series]
Identifying Transformative Scientific Research
2013 IEEE 13th International Conference on Data Mining
None
2013
Transformative research refers to research that shifts or disrupts established scientific paradigms. Notable examples include the discovery of high-temperature superconductivity that disrupted the theory established 30 years ago. Identifying potential transformative research early and accurately is important for funding agencies to maximize the impact of their investments. It also helps scientists identify and focus their attention on promising emerging works. This paper presents a data driven approach where citation patterns of scientific papers are analyzed to quantify how much a potential challenger idea shifts an established paradigm. The key idea is that transformative research creates an observable disruption in the structure of "information cascades," chains of references that can be traced back to the papers establishing some scientific paradigm. Such a disruption is visible soon after the challenger's introduction. We define a disruption score to quantify the disruption and develop an algorithm to compute it from a large citation network. Experimental results show that our approach can successfully identify transformative scientific papers that disrupt established paradigms in Physics and Computer Science, regardless of whether the challenger paradigm is an instant hit or a classic whose contribution is formally recognized with a Nobel Prize decades later.
[High-temperature superconductors, diffusion, information spread, Communities, transformative scientific research identification, information cascades, citation network, high-temperature superconductivity, Data mining, Physics, Computer science, cascades, data-driven approach, citation analysis, data handling, Reliability, large citation network]
Min-Max Hash for Jaccard Similarity
2013 IEEE 13th International Conference on Data Mining
None
2013
Min-wise hash is a widely-used hashing method for scalable similarity search in terms of Jaccard similarity, while in practice it is necessary to compute many such hash functions for certain precision, leading to expensive computational cost. In this paper, we introduce an effective method, i.e. the min-max hash method, which significantly reduces the hashing time by half, yet it has a provably slightly smaller variance in estimating pair wise Jaccard similarity. In addition, the estimator of min-max hash only contains pair wise equality checking, thus it is especially suitable for approximate nearest neighbor search. Since min-max hash is equally simple as min-wise hash, many extensions based on min-wise hash can be easily adapted to min-max hash, and we show how to combine it with b-bit minwise hash. Experiments show that with the same length of hash code, min-max hash reduces the hashing time to half as much as that of min-wise hash, while achieving smaller mean squared error (MSE) in estimating pair wise Jaccard similarity, and better best approximate ratio (BAR) in approximate nearest neighbor search.
[Laboratories, approximate nearest neighbor search, hash code, Approximation methods, minimax techniques, hashing method, hashing time reduction, mean squared error, pairwise equality checking, Jaccard similarity, Computational efficiency, mean square error methods, search problems, best approximate ratio, min-max hash method, Educational institutions, pairwise Jaccard similarity estimation, Nearest neighbor searches, min-max hash estimator, Computer science, pattern clustering, min-wise hash, scalable similarity search, b-bit minwise hash, Approximation algorithms, min-max hash]
GRIAS: An Entity-Relation Graph Based Framework for Discovering Entity Aliases
2013 IEEE 13th International Conference on Data Mining
None
2013
Recognizing the various aliases of an entity is a critical task for many applications, including Web search, recommendation system, and e-discovery. The goal of this paper is to accurately identify entity aliases, especially the long tail ones in the unstructured data. Our solution GRIAS (abbr. for a Graph-based framework for discovering entity Aliases) is motivated by the entity relationships collected from both the structured and unstructured data. These relationships help to build an entity-relation graph, and the graph-based similarity is calculated between an entity and its alias candidates which are first chosen by our proposed candidate selection method. Extensive experimental results on two real-world datasets demonstrate both the effectiveness and efficiency of the proposed framework.
[e-discovery, graph-based framework for discovering entity aliases, entity alias, Terminology, Earth Observing System, graph theory, data mining, candidate selection method, entity-relation graph, graph-based similarity, recommendation system, Graphics, Databases, structured data, Organizations, Cameras, unstructured data, GRIAS, Web search, alias similarity]
Focal-Test-Based Spatial Decision Tree Learning: A Summary of Results
2013 IEEE 13th International Conference on Data Mining
None
2013
Given a raster spatial framework, as well as training and test sets, the spatial decision tree learning (SDTL) problem aims to minimize classification errors as well as salt-and-pepper noise. The SDTL problem is important due to many societal applications such as land cover classification in remote sensing. However, the SDTL problem is challenging due to the spatial autocorrelation of class labels, and the potentially exponential number of candidate trees. Related work is limited due to the use of local-test-based decision nodes, which can not adequately model spatial autocorrelation during test phase, leading to high salt-and-pepper noise. In contrast, we propose a focal-test-based spatial decision tree (FTSDT) model, where the tree traversal direction for a location is based on not only local but also focal (i.e., neighborhood) properties of the location. Experimental results on real world remote sensing datasets show that the proposed approach reduces salt-and-pepper noise and improves classification accuracy.
[Correlation, image classification, Noise, land cover classification, societal applications, Remote sensing, Training, Prediction algorithms, real world remote sensing datasets, Decision trees, learning (artificial intelligence), spatial decision tree, focal-test-based spatial decision tree learning problem, FTSDT model, geophysical image processing, remote sensing, classification error minimization, spatial autocorrelation, candidate trees, focal test, Noise measurement, image denoising, tree traversal direction, decision trees, SDTL problem, spatial data mining, salt-and-pepper noise]
Conformal Prediction Using Decision Trees
2013 IEEE 13th International Conference on Data Mining
None
2013
Conformal prediction is a relatively new framework in which the predictive models output sets of predictions with a bound on the error rate, i.e., in a classification context, the probability of excluding the correct class label is lower than a predefined significance level. An investigation of the use of decision trees within the conformal prediction framework is presented, with the overall purpose to determine the effect of different algorithmic choices, including split criterion, pruning scheme and way to calculate the probability estimates. Since the error rate is bounded by the framework, the most important property of conformal predictors is efficiency, which concerns minimizing the number of elements in the output prediction sets. Results from one of the largest empirical investigations to date within the conformal prediction framework are presented, showing that in order to optimize efficiency, the decision trees should be induced using no pruning and with smoothed probability estimates. The choice of split criterion to use for the actual induction of the trees did not turn out to have any major impact on the efficiency. Finally, the experimentation also showed that when using decision trees, standard inductive conformal prediction was as efficient as the recently suggested method cross-conformal prediction. This is an encouraging results since cross-conformal prediction uses several decision trees, thus sacrificing the interpretability of a single decision tree.
[pattern classification, pruning scheme, class label, probability, Predictive models, Probability, Calibration, prediction theory, Standards, conformal prediction framework, probability estimates, Accuracy, output prediction sets, predictive models, split criterion, decision trees, Prediction algorithms, conformal mapping, Decision trees, cross-conformal prediction, Conformal prediction]
An Unsupervised Algorithm for Learning Blocking Schemes
2013 IEEE 13th International Conference on Data Mining
None
2013
A pair wise comparison of data objects is a requisite step in many data mining applications, but has quadratic complexity. In applications such as record linkage, blocking methods may be applied to reduce the cost. That is, the data is first partitioned into a set of blocks, and pair wise comparisons computed for pairs within each block. To date, blocking methods have required the blocking scheme be given, or the provision of training data enabling supervised learning algorithms to determine a blocking scheme. In either case, a domain expert is required. This paper develops an unsupervised method for learning a blocking scheme for tabular data sets. The method is divided into two phases. First, a weakly labeled training set is generated automatically in time linear in the number of records of the entire dataset. The second phase casts blocking key discovery as a Fisher feature selection problem. The approach is compared to a state-of-the-art supervised blocking key discovery algorithm on three real-world databases and achieves favorable results.
[Measurement, cost reduction, expert systems, Record Linkage, record linkage, learning blocking schemes, data objects, data mining, Blocking, domain expert, weakly labeled training set, Complexity theory, quadratic complexity, Personnel, unsupervised learning, Training, Couplings, tabular data sets, supervised blocking key discovery algorithm, supervised learning algorithms, unsupervised learning algorithm, Fisher feature selection problem, Indexing]
Semantic Frame-Based Document Representation for Comparable Corpora
2013 IEEE 13th International Conference on Data Mining
None
2013
Document representation is a fundamental problem for text mining. Many efforts have been done to generate concise yet semantic representation, such as bag-of-words, phrase, sentence and topic-level descriptions. Nevertheless, most existing techniques counter difficulties in handling monolingual comparable corpus, which is a collection of monolingual documents conveying the same topic. In this paper, we propose the use of frame, a high-level semantic unit, and construct frame-based representations to semantically describe documents by bags of frames, using an information network approach. One major challenge in this representation is that semantically similar frames may be of different forms. For example, "radiation leaked" in one news article can appear as "the level of radiation increased" in another article. To tackle the problem, a text-based information network is constructed among frames and words, and a link-based similarity measure called SynRank is proposed to calculate similarity between frames. As a result, different variations of the semantically similar frames are merged into a single descriptive frame using clustering, and a document can then be represented as a bag of representative frames. It turns out that frame-based document representation not only is more interpretable, but also can facilitate other text analysis tasks such as event tracking effectively. We conduct both qualitative and quantitative experiments on three comparable news corpora, to study the effectiveness of frame-based document representation and the similarity measure SynRank, respectively, and demonstrate that the superior performance of frame-based document representation on different real-world applications.
[text analysis, data mining, high-level semantic unit, Data mining, monolingual comparable corpus handling, Semantics, Earthquakes, semantic frame-based document representation, comparable corpora, Document Representation, data structures, text mining, Labeling, Context, SynRank, Tsunami, link-based similarity measure, text analysis tasks, Clustering, Equations, single descriptive frame, event tracking, pattern clustering, topic-level descriptions, Graph Similarity, text-based information network approach, bag-of-words]
Regularization Paths for Sparse Nonnegative Least Squares Problems with Applications to Life Cycle Assessment Tree Discovery
2013 IEEE 13th International Conference on Data Mining
None
2013
The nonnegative least squares problems are useful in applications where the physical nature of problem domain permits only additive linear combinations. We discuss the l<sub>1</sub>-regularized nonnegative least squares (L1-NLS) problem, where l1-regularization is used to induce sparsity. Although l<sub>1</sub>-regularization has been successfully used in least squares regression, when combined with nonnegativity constraints, developments of algorithms and their understandings have been limited. We propose an algorithm that generates the entire regularization paths of the L1-NLS problem. We prove the correctness of the proposed algorithm and illustrate a novel application in environmental sustainability. The application relates to life cycle assessment (LCA), a technique used to estimate environmental impact during the entire lifetime of a product. We address an inverse problem in LCA. Given environmental impact factors of a target product and of a large library of constituents, the goal is to reverse engineer an inventory tree for the product. Using real-world data sets, we demonstrate how our L1-NLS approach controls the size of discovered trees, and how the full regularization paths effectively illustrate the spectrum of discovered trees with varying sparsity and compositions.
[Electronic publishing, l<sub>1</sub>-regularized nonnegative least square problem, life cycle assessment tree discovery, Materials, production engineering computing, regression analysis, full regularization path, product lifetime, environmental sustainability, inverse problems, sparse nonnegative least square problem regularization path, Libraries, Books, least squares regression, life cycle assessment, environmental impact factors, inverse problem, least squares approximations, product life cycle management, l1-regularization, trees (mathematics), LCA technique, L1-NLS problem, reverse engineering, environmental impact estimation, Indexes, inventory tree, nonnegative least squares, sustainable development, nonnegative least squares problems, Vegetation, nonnegativity constraints, reverse engineer, data handling, inventory management, real-world data sets]
A High-Dimensional Set Top Box Ad Targeting Algorithm Including Experimental Comparisons to Traditional TV Algorithms
2013 IEEE 13th International Conference on Data Mining
None
2013
We present a method for targeting ads on television that works on today's TV systems. The method works by mining vast amounts of Set Top Box data, as well as advertiser customer data. From both sources the system builds demographic profiles, and then looks for media that have the highest match per dollar to the customer profile. The method was tested in four live television campaigns, comprising over 22,000 airings, and we present experimental results.
[high-dimensional set-top box ad targeting algorithm, TV, digital television, set-top box data mining, demographic profiles, live television campaigns, TV algorithms, data mining, television, Media, Educational institutions, Vectors, advertising, set-top boxes, advertiser customer data mining, Statistics, targeting, Sociology, set top box, customer profile, Advertising]
Efficient Algorithms for Selecting Features with Arbitrary Group Constraints via Group Lasso
2013 IEEE 13th International Conference on Data Mining
None
2013
Feature structure information plays an important role for regression and classification tasks. We consider a more generic problem: group lasso problem, where structures over feature space can be represented as a combination of features in a group. These groups can be either overlapped or non-overlapped, which are specified in different structures, e.g., structures over a line, a tree, a graph or even a forest. We propose a new approach to solve this generic group lasso problem, where certain features are selected in a group, and an arbitrary family of subset is allowed. We employ accelerated proximal gradient method to solve this problem, where a key step is solve the associated proximal operator. We propose a fast method to compute the proximal operator, where its convergence is rigorously proved. Experimental results on different structures (e.g., group, tree, graph structures) demonstrate the efficiency and effectiveness of the proposed algorithm.
[Gradient methods, pattern classification, feature structure information, feature space, Input variables, group lasso problem, lasso, arbitrary group constraints, regression analysis, accelerated proximal gradient method, Indexes, exclusive lasso, associated proximal operator, Standards, classification task, Convergence, Vegetation, regression task, feature group constraint, group lasso, Acceleration, gradient methods, feature selection]
BIG-ALIGN: Fast Bipartite Graph Alignment
2013 IEEE 13th International Conference on Data Mining
None
2013
How can we find the virtual twin (i.e., the same or similar user) on Linked In for a user on Facebook? How can we effectively link an information network with a social network to support cross-network search? Graph alignment - the task of finding the node correspondences between two given graphs - is a fundamental building block in numerous application domains, such as social networks analysis, bioinformatics, chemistry, pattern recognition. In this work, we focus on aligning bipartite graphs, a problem which has been largely ignored by the extensive existing work on graph matching, despite the ubiquity of those graphs (e.g., users-groups network). We introduce a new optimization formulation and propose an effective and fast algorithm to solve it. We also propose a fast generalization of our approach to align unipartite graphs. The extensive experimental evaluations show that our method outperforms the state-of-art graph matching algorithms in both alignment accuracy and running time, being up to 10x more accurate or 174x faster on real graphs.
[fast bipartite graph alignment, chemistry, Linked In, information network, pattern matching, graph theory, fast generalization, Probabilistic logic, graph matching algorithms, Sparse matrices, virtual twin, BIG-ALIGN, optimization formulation, social network, LinkedIn, optimisation, social network analysis, bioinformatics, Cost function, social networking (online), Bipartite graph, Facebook, cross-network search, pattern recognition]
Blocking Simple and Complex Contagion by Edge Removal
2013 IEEE 13th International Conference on Data Mining
None
2013
Eliminating interactions among individuals is an important means of blocking contagion spread, e.g., closing schools during an epidemic or shutting down electronic communication channels during social unrest. We study contagion blocking in networked populations by identifying edges to remove from a network, thus blocking contagion transmission pathways. We formulate various problems to minimize contagion spread and show that some are efficiently solvable while others are formally hard. We also compare our hardness results to those from node blocking problems and show interesting differences between the two. Our main problem is not only hard, but also has no approximation guarantee, unless P=NP. Therefore, we devise a heuristic for the problem and compare its performance to state-of-the-art heuristics from the literature. We show, through results of 12 (network, heuristic) combinations on three real social networks, that our method offers considerable improvement in the ability to block contagions in weighted and unweighted networks. We also conduct a parametric study to understand the limitations of our approach.
[Context, complex contagion, simple contagion, Social network services, social networks, network theory (graphs), Educational institutions, Approximation methods, social unrest, unweighted networks, social sciences, contagion blocking, weighted networks, node blocking problems, edge removal, networked populations, Integrated circuit modeling, Public healthcare, Contracts, computational complexity]
Guiding Autonomous Agents to Better Behaviors through Human Advice
2013 IEEE 13th International Conference on Data Mining
None
2013
Inverse Reinforcement Learning (IRL) is an approach for domain-reward discovery from demonstration, where an agent mines the reward function of a Markov decision process by observing an expert acting in the domain. In the standard setting, it is assumed that the expert acts (nearly) optimally, and a large number of trajectories, i.e., training examples are available for reward discovery (and consequently, learning domain behavior). These are not practical assumptions: trajectories are often noisy, and there can be a paucity of examples. Our novel approach incorporates advice-giving into the IRL framework to address these issues. Inspired by preference elicitation, a domain expert provides advice on states and actions (features) by stating preferences over them. We evaluate our approach on several domains and show that with small amounts of targeted preference advice, learning is possible from noisy demonstrations, and requires far fewer trajectories compared to simply learning from trajectories alone.
[human advice, decision theory, IRL framework, learning domain behavior, Educational institutions, domain expert, Noise measurement, Data mining, software agents, domain-reward discovery, Equations, Training, preference elicitation, Learning (artificial intelligence), Markov processes, Markov decision process, inverse reinforcement learning, Trajectory, reward function, learning (artificial intelligence), autonomous agents]
TL-PLSA: Transfer Learning between Domains with Different Classes
2013 IEEE 13th International Conference on Data Mining
None
2013
A new transfer learning method is presented in this paper, addressing a particularly hard transfer learning problem: the case where the target domain shares only a subset of its classes with the source domain and only unlabeled data are provided for the target domain. This is a situation that occurs frequently in real-world applications, such as the multiclass document classification problems that motivated our work. The proposed approach is a transfer learning variant of the Probabilistic Latent Semantic Analysis (PLSA) model that we name TL-PLSA. Unlike most approaches in the literature, TL-PLSA captures both the difference of the domains and the commonalities of the class sets, given no labelled data from the target domain. We perform experiments over three different datasets and show the difficulty of the task, as well as the promising results that we obtained with the new method.
[transfer learning, TL-PLSA, target domain, probability, multiclass document classification problems, Probabilistic logic, source domain, PLSA, Training, Graphical models, multiclass classification, Semantics, Training data, transfer learning method, Data models, unlabeled data, Mathematical model, learning (artificial intelligence), probabilistic latent semantic analysis model]
Learning, Analyzing and Predicting Object Roles on Dynamic Networks
2013 IEEE 13th International Conference on Data Mining
None
2013
Dynamic networks are structures with objects and links between the objects that vary in time. Temporal information in dynamic networks can be used to reveal many important phenomena such as bursts of activities in social networks and human communication patterns in email networks. In this area, one very important problem is to understand dynamic patterns of object roles. For instance, will a user become a peripheral node in a social network? Could a website become a hub on the Internet? Will a gene be highly expressed in gene-gene interaction networks in the later stage of a cancer? In this paper, we propose a novel approach that identifies the role of each object, tracks the changes of object roles over time, and predicts the evolving patterns of the object roles in dynamic networks. In particular, a probability model is proposed to extract latent features of object roles from dynamic networks. The extracted latent features are discriminative in learning object roles and are capable of characterizing network structures. The probability model is then extended to learn the dynamic patterns and make predictions on object roles. We assess our method on two data sets on the tasks of exploring how users' importance and political interests evolve as time progresses on dynamic networks. Overall, the extensive experimental evaluations confirm the effectiveness of our approach for identifying, analyzing and predicting object roles on dynamic networks.
[probability model, data mining, object role identification, Predictive models, network theory (graphs), Data mining, Analytical models, feature extraction, dynamic networks, latent feature extraction, object role analysis, Bismuth, learning (artificial intelligence), gene-gene interaction networks, Dynamic Network, Social network services, probability, social networks, object role patterns, dynamic patterns, object role learning, Object Role, object role prediction, Feature extraction, Bayes methods, Internet, Web site, network structures]
Tag-Weighted Dirichlet Allocation
2013 IEEE 13th International Conference on Data Mining
None
2013
In the past two decades, there has been a huge amount of document data with rich tag information during the evolution of the Internet, which can be called semi-structured data. These semi-structured data contain both unstructured features (e.g., plain text) and metadata, such as tags in html files or author and venue information in research articles. It's of great interest to model such kind of data. Most previous works focused on modeling the unstructured data. Some other methods have been proposed to model the unstructured data with specific tags. To build a general model for semi-structured documents remains an important problem in terms of both model fitness and efficiency. In this paper, we propose a novel method to model the tagged documents by a so-called Tag-Weighted Dirichlet Allocation (TWDA). TWDA is a framework that leverages both the tags and words in each document to infer the topic components for the documents. This allows not only to learn the document-topic and topic-word distributions, but also to infer the tag-topic distributions for text mining (e.g., classification, clustering, and recommendations). Moreover, TWDA can automatically infer the probabilistic weights of tags for each document, that can be used to predict the tags in one document. We present an efficient variational inference method with an EM algorithm for estimating the model parameters. The experimental results show the effectiveness, efficiency and robustness of our TWDA approach by comparing it with the state-of-the-art methods on four corpora in document modeling, tags prediction and text classification.
[Tag-Weighted, text analysis, venue information, metadata, text clustering, variational inference method, data mining, Predictive models, text classification, Analytical models, model fitness, TWDA framework, model parameter estimation, HTML files, parameter estimation, text mining, semistructured document data, Mathematical model, text recommendations, tag information, pattern classification, Dirichlet allocation, topic-word distribution learning, author information, topic model, model efficiency, probability, Probabilistic logic, Vectors, tagged document modelling, document tag leveraging, document word leveraging, inference mechanisms, plain text, document-topic distribution learning, recommender systems, pattern clustering, expectation-maximisation algorithm, topic components, Data models, tag-topic distribution inference, research articles, Resource management, tag-weighted Dirichlet allocation, variational inference, EM algorithm, probabilistic weights, tag prediction]
Mining Probabilistic Frequent Spatio-Temporal Sequential Patterns with Gap Constraints from Uncertain Databases
2013 IEEE 13th International Conference on Data Mining
None
2013
Uncertainty is common in real-world applications, for example, in sensor networks and moving object tracking, resulting in much interest in item set mining for uncertain transaction databases. In this paper, we focus on pattern mining for uncertain sequences and introduce probabilistic frequent spatial-temporal sequential patterns with gap constraints. Such patterns are important for the discovery of knowledge given uncertain trajectory data. We propose a dynamic programming approach for computing the frequentness probability of these patterns, which has linear time complexity, and we explore its embedding into pattern enumeration algorithms using both breadth-first search and depth-first search strategies. Our extensive empirical study shows the efficiency and effectiveness of our methods for synthetic and real-world datasets.
[Uncertainty, Sequential patterns, breadth-first search strategy, data mining, knowledge discovery, Data mining, uncertain databases, Spatial-temporal data, Databases, Trajectory, Dynamic programming, Mathematical model, linear time complexity, Uncertain databases, dynamic programming approach, dynamic programming, Probabilistic logic, Uncertain pattern mining, tree searching, uncertain trajectory data, pattern enumeration algorithms, probabilistic frequent spatio-temporal sequential pattern mining, gap constraints, depth-first search strategy, computational complexity]
Mining Following Relationships in Movement Data
2013 IEEE 13th International Conference on Data Mining
None
2013
Movement data have been widely collected from GPS and sensors, allowing us to analyze how moving objects interact in terms of space and time and to learn about the relationships that exist among the objects. In this paper, we investigate an interesting relationship that has not been adequately studied so far: the following relationship. Intuitively, a follower has similar trajectories as its leader but always arrives at a location with some time lag. The challenges in mining the following relationship are: (1) the following time lag is usually unknown and varying, (2) the trajectories of the follower and leader are not identical, and (3) the relationship is subtle and only occurs in a short period of time. In this paper, we propose a simple but practical method that addresses all these challenges. It requires only two intuitive parameters and is able to mine following time intervals between two trajectories in linear time. We conduct comprehensive experiments on both synthetic and real datasets to demonstrate the effectiveness of our method.
[Correlation, Animals, mining following relationships, Heuristic algorithms, movement data, Time series analysis, directed graphs, data mining, following time lag, Educational institutions, Trajectory, Data mining]
Linear Computation for Independent Social Influence
2013 IEEE 13th International Conference on Data Mining
None
2013
Recent years have witnessed the increased interests in exploiting influence in social networks for many applications. To the best of our knowledge, from the computational aspect of social influence analysis, most of existing work focus on either describing the influence propagation process or identifying the set of most influential seed nodes. However, these work usually do not distinguish the "independent influence" of each single seed node after removing other seeds. Since it is important to quickly figure out the real contribution of each seed, in this paper we propose to measure the seed's independent influence by a linear social influence model. Specifically, we first describe the linear social influence model, and then define the independent influence under this model for eliminating the "mutual enrichment" between seed nodes. Meanwhile, we find that the influence of a set of nodes is actually the sum of their independent influence, and we also give upper bounds for independent influence. Moreover, these findings are evaluated by two applications, i.e., ranking the seeds by their independent influence and identifying the Top-K influential ones. Finally, the experimental results on several real-world datasets validate the effectiveness and efficiency of the proposed independent social influence measures.
[Social influence, Computational modeling, independent social influence measures, independent social influence analysis, social networks, top-k influential seed identification, Vectors, influence propagation process, Equations, Linear Computation, Upper Bound, influential seed nodes, Upper bound, Independent, Ranking, linear social influence model, social networking (online), linear computation, Mathematical model, Integrated circuit modeling]
Learning Imbalanced Multi-class Data with Optimal Dichotomy Weights
2013 IEEE 13th International Conference on Data Mining
None
2013
Class-imbalance is very common in real data mining tasks. Previous studies focused on binary-class imbalance problem, whereas multi-class imbalance problem is more challenging. Error correcting output codes (ECOC) technique can be applied to class-imbalance problem, however, the standard ECOC aims at maximizing accuracy, ignoring the fact that, when class-imbalance is really a problem, the minority classes are more important than the majority classes. To enable ECOC to tackle multi-class imbalance, it is desired to have an appropriate code matrix, an effective learning strategy and a decoding strategy emphasizing the minority classes. In this paper, based on the aforementioned consideration, we propose the imECOC method which works on dichotomies to handle both the between-class imbalance and within-class imbalance. As the dichotomy classifiers contribute differently to the final prediction, imECOC assigns weights to dichotomies and uses weighted distance for decoding, where the optimal dichotomy weights are obtained by minimizing a weighted loss in favor of the minority classes. Experimental results on fourteen data sets show that, imECOC performs significantly better than many state-of-the-art multi-class imbalance learning methods, no matter whether multi-class F1, G-mean or AUC are used as evaluation measures.
[within-class imbalance, Class-imbalance learning, error correcting output codes, data mining, weighted loss minimization, Complexity theory, evaluation measures, AUC, Training, Learning systems, binary-class imbalance problem, optimal dichotomy weights, Accuracy, ECOC, minority classes, imECOC, Multi-class, learning (artificial intelligence), multiclass F1, pattern classification, data mining tasks, error correction codes, ECOC technique, code matrix, majority classes, Encoding, Decoding, decoding strategy, Standards, decoding, imbalanced multiclass data learning strategy, between-class imbalance, weighted distance]
Mining Statistically Significant Sequential Patterns
2013 IEEE 13th International Conference on Data Mining
None
2013
Recent developments in the frequent pattern mining framework uses additional measures of interest to reduce the set of discovered patterns. We introduce a rigorous and efficient approach to mine statistically significant, unexpected patterns in sequences of item sets. The proposed methodology is based on a null model for sequences and on a multiple testing procedure to extract patterns of interest. Experiments on sequences of replays of a video game demonstrate the scalability and the efficiency of the method to discover unexpected game strategies.
[frequent pattern mining framework, multiple testing procedure, Computational modeling, video game, game strategies, data mining, item set sequences, pattern discovery, Sequential pattern, Entropy, Data mining, statistically significant sequential pattern mining, Itemsets, null model, significance test, Hidden Markov models, computer games, interest pattern extraction, Random variables, sequences null model, statistical analysis]
Mining Summaries of Propagations
2013 IEEE 13th International Conference on Data Mining
None
2013
Analyzing the traces left by a meme of information propagating through a social network or by a user browsing a website can help to unveil the structure and dynamics of such complex networks. This may in turn open the door to concrete applications, such as finding influential users for a topic in a social network, or detecting the typical structure of a web browsing session that leads to a product purchase. In this paper we define the problem of mining summaries of propagations as a constrained pattern-mining problem. A propagation is a DAG where an entity (e.g., information exchanged in a social network, or a user browsing a website) flows following the underlying hierarchical structure of the nodes. A summary is a set of propagations that (i) involve a similar population of nodes, and (ii) exhibit a coherent hierarchical structure when merged altogether to form a single graph. The first constraint is defined based on the Jaccard coefficient, while the definition of the second one relies on the graph-theoretic concept of "agony" of a graph. It turns out that both constraints satisfy the downward closure property, thus enabling Apriori-like algorithms. However, motivated by the fact that computing agony is much more expensive than computing Jaccard, we devise two algorithms that explore the search space differently. The first algorithm is an Apriori-like, bottom-up method that checks both the constraints level-by-level. The second algorithm consists of a first phase where the search space is pruned as much as possible by exploiting the Jaccard constraint only, while involving the second constraint only afterwards, in a subsequent phase. We test our algorithms on four real-world datasets. Quantitative results reveal that the choice of the most efficient algorithm depends on the selectivity of the two constraints. Qualitative results show the relevance of the extracted summaries in a number of real-world scenarios.
[Lattices, data mining, complex networks, Data mining, web browsing, Jaccard constraint, summary mining, social network, graphs, Databases, agony, Sociology, search space, information propagation, constrained pattern mining problem, Social network services, pattern mining, hierarchical structure, social networks, Jaccard coefficient, DAG, Apriori, Statistics, graph-theoretic concept, directed graphs, downward-closure property, social networking (online), Web browsing session, influence maximization, Web site, Apriori-like algorithms, entity, Apriori-like bottom-up method, Periodic structures]
Active Density-Based Clustering
2013 IEEE 13th International Conference on Data Mining
None
2013
The density-based clustering algorithm DBSCAN is a fundamental technique for data clustering with many attractive properties and applications. However, DBSCAN requires specifying all pair wise (dis)similarities among objects that can be non-trivial to obtain in many applications. To tackle this problem, in this paper, we propose a novel active density-based clustering algorithm, named Act-DBSCAN, which works under a restricted number of used pair wise similarities. Act-DBSCAN exploits the pair wise lower-bounding (LB) similarities to initialize the cluster structure. Then, it adaptively selects the most informative pair wise LB similarities to update with the real ones in order to reconstruct the result until the budget limitation is reached. The goal is to approximate as much as possible the true clustering result with each update. Our Act-DBSCAN framework is built upon a proposed probabilistic model to score the impact of the update of each pair wise LB similarity on the change of the intermediate clustering structure. Deriving from this scoring system and the monotonicity and reduction property of our active clustering process, we propose the two efficient algorithms to iteratively select and update pair wise similarities and cluster structure. Experiments on real datasets show that Act-DBSCAN acquires good clustering results with only a few pair wise similarities, and requires only a small fraction of all pair wise similarities to reach the DBSCAN results. Act-DBSCAN also outperforms other related techniques such as active spectral clustering.
[Noise, Merging, monotonicity property, data clustering, intermediate clustering structure, active spectral clustering, scoring system, Training, Clustering algorithms, Kernel, informative pairwise LB similarity selection, Active learning, Estimation, probability, Probabilistic logic, pairwise lower-bounding similarities, true clustering, Act-DBSCAN framework, reduction property, Active clustering, pattern clustering, active density-based clustering algorithm, cluster structure, informative pairwise LB similarities, Density-based clustering]
Explaining Outliers by Subspace Separability
2013 IEEE 13th International Conference on Data Mining
None
2013
Outliers are extraordinary objects in a data collection. Depending on the domain, they may represent errors, fraudulent activities or rare events that are subject of our interest. Existing approaches focus on detection of outliers or degrees of outlierness (ranking), but do not provide a possible explanation of how these objects deviate from the rest of the data. Such explanations would help user to interpret or validate the detected outliers. The problem addressed in this paper is as follows: given an outlier detected by an existing algorithm, we propose a method that determines possible explanations for the outlier. These explanations are expressed in the form of subspaces in which the given outlier shows separability from the inliers. In this manner, our proposed method complements existing outlier detection algorithms by providing additional information about the outliers. Our method is designed to work with any existing outlier detection algorithm and it also includes a heuristic that gives a substantial speedup over the baseline strategy.
[outlier explanation, outlier detection algorithms, inliers, data analysis, subspace selection, Gaussian distribution, Extraterrestrial measurements, data collection, Accuracy, Databases, Data visualization, outlierness degrees, Feature extraction, data acquisition, subspace separability, data exploration, Detection algorithms]
Quantification Trees
2013 IEEE 13th International Conference on Data Mining
None
2013
In many applications there is a need to monitor how a population is distributed across different classes, and to track the changes in this distribution that derive from varying circumstances, an example such application is monitoring the percentage (or "prevalence") of unemployed people in a given region, or in a given age range, or at different time periods. When the membership of an individual in a class cannot be established deterministically, this monitoring activity requires classification. However, in the above applications the final goal is not determining which class each individual belongs to, but simply estimating the prevalence of each class in the unlabeled data. This task is called quantification. In a supervised learning framework we may estimate the distribution across the classes in a test set from a training set of labeled individuals. However, this may be sub optimal, since the distribution in the test set may be substantially different from that in the training set (a phenomenon called distribution drift). So far, quantification has mostly been addressed by learning a classifier optimized for individual classification and later adjusting the distribution it computes to compensate for its tendency to either under-or over-estimate the prevalence of the class. In this paper we propose instead to use a type of decision trees (quantification trees) optimized not for individual classification, but directly for quantification. Our experiments show that quantification trees are more accurate than existing state-of-the-art quantification methods, while retaining at the same time the simplicity and understandability of the decision tree framework.
[pattern classification, individual classification, classifier learning, labeled individual training set, Estimation, class prevalence estimation, quantification trees, Standards, Training, decision tree framework, Accuracy, distribution drift, supervised learning framework, Sociology, Vegetation, decision trees, test set, unlabeled data, Decision trees, learning (artificial intelligence)]
Mining Evolving Network Processes
2013 IEEE 13th International Conference on Data Mining
None
2013
Processes within real world networks evolve according to the underlying graph structure. A number of examples exists in diverse network genres: botnet communication growth, moving traffic jams [1], information foraging [2] in document networks (WWW and Wikipedia), and spread of viral memes or opinions in social networks. The network structure in all the above examples remains relatively fixed, while the shape, size and position of the affected network regions change gradually with time. Traffic jams grow, move, shrink and eventually disappear. Public attention shifts among current hot topics inducing a similar shift of highly accessed Wikipedia articles. Discovery of such smoothly evolving network processes has the potential to expose the intrinsic mechanisms of complex network dynamics, enable new data-driven models and improve network design. We introduce the novel problem of Mining smoothly evolving processes (MINESMOOTH) in networks with dynamic real-valued node/edge weights. We show that ensuring smooth transitions in the solution is NP-hard even on restricted network structures such as trees. We propose an efficient filtering based framework, called LEGATO. It achieves 3-7 times higher scores (i.e. larger and more significant processes) compared to alternatives on real networks, and above 80% accuracy in discovering realistic "embedded" processes in synthetic networks. In transportation networks, LEGATO discovers processes that conform to existing traffic jams models. Its results in Wikipedia reveal the temporal evolution of information seeking of Internet users.
[realistic embedded processes, Electronic publishing, synthetic networks, Heuristic algorithms, graph theory, Internet users, data mining, transportation networks, Encyclopedias, MINESMOOTH, Wikipedia, evolving network processes mining, Accuracy, optimisation, LEGATO, network processes, dynamic networks, dynamic real-valued node/edge weights, traffic jam models, traffic engineering computing, graph mining, Upper bound, NP-hard problem, information seeking, Internet, Web sites]
Enumeration of Time Series Motifs of All Lengths
2013 IEEE 13th International Conference on Data Mining
None
2013
Time series motifs are repeated patterns in long and noisy time series. Motifs are typically used to understand the dynamics of the source because repeated patterns with high similarity evidentially rule out the presence of noise. Recently, time series motifs have also been used for clustering, summarization, rule discovery and compression as features. For all such purposes, many high quality motifs of various lengths are desirable and thus, originates the problem of enumerating motifs for a wide range of lengths. Existing algorithms find motifs for a given length. A trivial way to enumerate motifs is to run one of the algorithms for the whole range of lengths. However, such parameter sweep is computationally infeasible for large real datasets. In this paper, we describe an exact algorithm, called MOEN, to enumerate motifs. The algorithm is an order of magnitude faster than the naive algorithm. The algorithm frees us from re-discovering the same motif at different lengths and tuning multiple data-dependent parameters. The speedup comes from using a novel bound on the similarity function across lengths and the algorithm uses only linear space unlike other motif discovery algorithms. We describe three case studies in entomology and activity recognition where MOEN enumerates several high quality motifs.
[Algorithm design and analysis, large real datasets, linear space, time series motif enumeration, Force, data mining, MOEN exact algorithm, Electroencephalography, rule discovery, motif discovery algorithms, Clustering algorithms, repeated patterns, multiple data-dependent parameter tuning, Distance bound, naive algorithm, Enumeration, Time series analysis, parameter sweep, Time series motif, time series, Noise measurement, high quality motifs, Upper bound, pattern clustering, entomology, similarity function, activity recognition]
Dominance Programming for Itemset Mining
2013 IEEE 13th International Conference on Data Mining
None
2013
Finding small sets of interesting patterns is an important challenge in pattern mining. In this paper, we argue that several well-known approaches that address this challenge are based on performing pair wise comparisons between patterns. Examples include finding closed patterns, free patterns, relevant subgroups and skyline patterns. Although progress has been made on each of these individual problems, a generic approach for solving these problems (and more) is still lacking. This paper tackles this challenge. It proposes a novel, generic approach for handling pattern mining problems that involve pair wise comparisons between patterns. Our key contributions are the following. First, we propose a novel algebra for programming pattern mining problems. This algebra extends relational algebras in a novel way towards pattern mining. It allows for the generic combination of constraints on individual patterns with dominance relations between patterns. Second, we introduce a modified generic constraint satisfaction system to evaluate these algebraic expressions. Experiments show that this generic approach can indeed effectively identify patterns expressed in the algebra.
[pattern pair wise comparison, relational algebra, generic constraint satisfaction system, data mining, dominance programming, Programming, Generators, Frequency measurement, Data mining, Itemsets, Algebra, generic approach, itemset mining, dominance relations, pattern mining problems]
Least Cost Influence in Multiplex Social Networks: Model Representation and Analysis
2013 IEEE 13th International Conference on Data Mining
None
2013
The least cost influence (LCI) problem, which asks to identify a minimum number of seed users who can eventually influence a large number of users, has become one of the central research topics recently in online social networks (OSNs). However, existing works mostly focused on a single network while users nowadays often join several OSNs. Thus, it is crucial to investigate the influence in multiplex networks, i.e. the influence is diffused across a set of networks via shared users, in order to obtain the best set of seed users.In this paper, we propose a unified framework to represent and analyze the influence diffusion in multiplex networks. More specifically, we tackle the LCI problem in multiplex OSNs by reducing multiplex networks to a single network via various coupling schemes while preserving the most influence propagation properties. Besides the coupling schemes to represent the diffusion process, the framework also includes the influence relay, a new metric to measure the flow of influence inside and between networks. The experiments on both real and synthesized datasets validate the effectiveness of the coupling schemes as well as provide some interesting insights into the process of influence propagation in multiplex networks.
[Multiplexing, multiplex social networks, LCI problem, model representation, model analysis, Stochastic processes, online social networks, shared users, Twitter, influence propagation properties, OSN, influence diffusion process, least cost influence problem, Couplings, coupling schemes, Logic gates, social networking (online), seed users, Facebook]
Bayesian Discovery of Multiple Bayesian Networks via Transfer Learning
2013 IEEE 13th International Conference on Data Mining
None
2013
Bayesian network structure learning algorithms with limited data are being used in domains such as systems biology and neuroscience to gain insight into the underlying processes that produce observed data. Learning reliable networks from limited data is difficult, therefore transfer learning can improve the robustness of learned networks by leveraging data from related tasks. Existing transfer learning algorithms for Bayesian network structure learning give a single maximum a posteriori estimate of network models. Yet, many other models may be equally likely, and so a more informative result is provided by Bayesian structure discovery. Bayesian structure discovery algorithms estimate posterior probabilities of structural features, such as edges. We present transfer learning for Bayesian structure discovery which allows us to explore the shared and unique structural features among related tasks. Efficient computation requires that our transfer learning objective factors into local calculations, which we prove is given by a broad class of transfer biases. Theoretically, we show the efficiency of our approach. Empirically, we show that compared to single task learning, transfer learning is better able to positively identify true edges. We apply the method to whole-brain neuroimaging data.
[data mining, posterior probability estimation, Approximation methods, maximum likelihood estimation, transfer learning algorithms, system biology, network models, Robustness, transfer biases, belief networks, learning (artificial intelligence), Joints, Bayesian structure discovery algorithms, true edge identification, probability, Bayesian network structure learning algorithms, neuroscience, multiple Bayesian networks, Bayesian network discovery, single task learning, single maximum a posteriori estimation, maximum a posteriori estimate, Approximation algorithms, limited data, Data models, Bayes methods, whole-brain neuroimaging data method, Periodic structures]
Forecasting Spatiotemporal Impact of Traffic Incidents on Road Networks
2013 IEEE 13th International Conference on Data Mining
None
2013
The advances in sensor technologies enable real-time collection of high-fidelity spatiotemporal data on transportation networks of major cities. In this paper, using two real-world transportation datasets: 1) incident data and 2) traffic data, we address the problem of predicting and quantifying the impact of traffic incidents. Traffic incidents include any non-recurring events on road networks, including accidents, weather hazard, road construction or work zone closures. By analyzing archived incident data, we classify incidents based on their features (e.g., time, location, type of incident). Subsequently, we model the impact of each incident class on its surrounding traffic by analyzing the archived traffic data at the time and location of the incidents. Consequently, in real-time, if we observe a similar incident (from real-time incident data), we can predict and quantify its impact on the surrounding traffic using our developed models. This information, in turn, can help drivers to effectively avoid impacted areas in real-time. To be useful for such real-time navigation application, and unlike current approaches, we study the dynamic behavior of incidents and model the impact as a quantitative time varying spatial span. In addition to utilizing incident features, we improve our classification approach further by analyzing traffic density around the incident area and the initial behavior of the incident. We evaluated our approach with very large traffic and incident datasets collected from the road networks of Los Angeles County and the results show that we can improve our baseline approach, which solely relies on incident features, by up to 45%.
[real-world transportation dataset, road construction, Roads, traffic density analysis, archived traffic data analysis, Los Angeles County, real-time navigation application, road networks, real-time incident data, Vehicles, road accidents, driver information systems, real-time high-fidelity spatiotemporal data collection, traffic forecast, Real-time systems, traffic data, weather hazard, pattern classification, road traffic, Navigation, data analysis, Time series analysis, traffic incidents, quantitative time varying spatial span, intelligent transportation, traffic engineering computing, accidents, spatiotemporal data, traffic incident classification, sensor technologies, transportation, dynamic incident behavior, impact analysis, spatiotemporal impact forecasting, incident datasets, work zone closures, forecasting theory, traffic datasets, incident features, Accidents]
Scaling Log-Linear Analysis to High-Dimensional Data
2013 IEEE 13th International Conference on Data Mining
None
2013
Association discovery is a fundamental data mining task. The primary statistical approach to association discovery between variables is log-linear analysis. Classical approaches to log-linear analysis do not scale beyond about ten variables. We develop an efficient approach to log-linear analysis that scales to hundreds of variables by melding the classical statistical machinery of log-linear analysis with advanced data mining techniques from association discovery and graphical modeling.
[Maximum likelihood estimation, log-linear analysis scaling, Computational modeling, Particle separators, Lattices, data mining, high-dimensional data, classical statistical machinery, association discovery, Entropy, Data mining, advanced data mining techniques, Analytical models, High-dimensional Data, graphical modeling, primary statistical approach, Data Modeling, statistical analysis, data mining task, Log-linear Analysis, Association Discovery]
Fast Pairwise Query Selection for Large-Scale Active Learning to Rank
2013 IEEE 13th International Conference on Data Mining
None
2013
Pair wise learning to rank algorithms (such as Rank SVM) teach a machine how to rank objects given a collection of ordered object pairs. However, their accuracy is highly dependent on the abundance of training data. To address this limitation and reduce annotation efforts, the framework of active pair wise learning to rank was introduced recently. However, in such a framework the number of possible query pairs increases quadratic ally with the number of instances. In this work, we present the first scalable pair wise query selection method using a layered (two-step) hashing framework. The first step relevance hashing aims to retrieve the strongly relevant or highly ranked points, and the second step uncertainty hashing is used to nominate pairs whose ranking is uncertain. The proposed framework aims to efficiently reduce the search space of pair wise queries and can be used with any pair wise learning to rank algorithm with a linear ranking function. We evaluate our approach on large-scale real problems and show it has comparable performance to exhaustive search. The experimental results demonstrate the effectiveness of our approach, and validate the efficiency of hashing in accelerating the search of massive pair wise queries.
[Uncertainty, large-scale active learning, fast pairwise query selection, Artificial neural networks, Search problems, scalable pair wise query selection method, Learning to Rank, Vectors, search space reduction, Active Learning, query processing, Accuracy, Databases, layered hashing framework, Training data, Hashing, file organisation, learning (artificial intelligence), uncertainty hashing, algorithm ranking, active pair wise learning]
Power to the Points: Validating Data Memberships in Clusterings
2013 IEEE 13th International Conference on Data Mining
None
2013
In this paper, we present a method to attach affinity scores to the implicit labels of individual points in a clustering. The affinity scores capture the confidence level of the cluster that claims to "own" the point. We demonstrate that these scores accurately capture the quality of the label assigned to the point. We also show further applications of these scores to estimate global measures of clustering quality, as well as accelerate clustering algorithms by orders of magnitude using active selection based on affinity. This method is very general and applies to clusterings derived from any geometric source. It lends itself to easy visualization and can prove useful as part of an interactive visual analytics framework. It is also efficient: assigning an affinity score to a point depends only polynomially on the number of clusters and is independent both of the size and dimensionality of the data. It is based on techniques from the theory of interpolation, coupled with sampling and estimation algorithms from high dimensional computational geometry.
[visualization, estimation theory, computational geometry, high dimensional computational geometry, clustering quality global measure estimation, affinity scores, estimation algorithm, data membership validation, Clustering algorithms, data visualisation, interpolation theory, sampling methods, label quality, Validating Clusterings, data analysis, clustering algorithm, Natural Neighbor Interpolation, Educational institutions, Probabilistic logic, Stability analysis, active selection, Standards, Power Diagrams, sampling algorithm, interpolation, pattern clustering, Data visualization, cluster confidence level, Data models, interactive visual analytics framework]
Weighted-Object Ensemble Clustering
2013 IEEE 13th International Conference on Data Mining
None
2013
Ensemble clustering, also known as consensus clustering, aims to generate a stable and robust clustering through the consolidation of multiple base clusterings. In recent years many ensemble clustering methods have been proposed, most of which treat each clustering and each object as equally important. Some approaches make use of weights associated with clusters, or with clusterings, when assembling the different base clusterings. Boosting algorithms developed for classification have also led to the idea of considering weighted objects during the clustering process. However, not much effort has been put towards incorporating weighted objects into the consensus process. To fill this gap, in this paper we propose an approach called Weighted-Object Ensemble Clustering (WOEC). We first estimate how difficult it is to cluster an object by constructing the co-association matrix that summarizes the base clustering results, and we then embed the corresponding information as weights associated to objects. We propose three different consensus techniques to leverage the weighted objects. All three reduce the ensemble clustering problem to a graph partitioning one. We present extensive experimental results which demonstrate that our WOEC approach outperforms state-of-the-art consensus clustering methods and is robust to parameter settings.
[graph partition, weighted-object ensemble clustering, weighted objects, Clustering methods, data mining, coassociation matrix, Boosting, Educational institutions, Vectors, Partitioning algorithms, matrix algebra, Ensemble clustering, consensus clustering, pattern clustering, Clustering algorithms, Bipartite graph, graph partitioning, WOEC]
Mining User Lifecycles from Online Community Platforms and their Application to Churn Prediction
2013 IEEE 13th International Conference on Data Mining
None
2013
Recent work has studied user development in the domains of both telecommunication and online community platforms, examining how users develop in terms of the company they keep (socially) and the language they use (lexically). Such works afford key insights into user changes along individual dimensions, yet they do not examine how users develop relative to their prior behaviour along multiple dimensions. In this paper we examine how users develop along various properties (in-degree, out-degree, posted terms) in three online community platforms (Facebook, SAP Community Network, and Server Fault) and using three models of user development: (i) isolated lifecycle periods, (ii) historical contrasts, and (iii) community contrasts. We present an approach to mine the lifecycle trajectories of users as a means to characterise user development along the different properties and development models, and demonstrate the utility of such trajectories in predicting churners. We find consistent effects with past work: users tend to reflect the behaviour of the community in early portions of their lifecycles, before then diverging from the community towards the end. We also find that users form sub-communities with whom they communicate and remain within.
[community contrasts, telecommunication community platforms, user development, Communities, data mining, social networks, churn prediction, online communities, Entropy, Probability distribution, Servers, isolated lifecycle periods, server fault, user lifecycles, user lifecycle mining, lifecycle trajectory mining, social networking (online), SAP community network, Trajectory, Internet, historical contrasts, Facebook, online community platforms]
Statistical Selection of Congruent Subspaces for Mining Attributed Graphs
2013 IEEE 13th International Conference on Data Mining
None
2013
Current mining algorithms for attributed graphs exploit dependencies between attribute information and edge structure, referred to as homophily. However, techniques fail if this assumption does not hold for the full attribute space. In multivariate spaces, some attributes have high dependency with the graph structure while others do not show any dependency. Hence, it is important to select congruent subspaces (i.e., subsets of the node attributes) showing dependencies with the graph structure. In this work, we propose a method for the statistical selection of such congruent subspaces. More specifically, we define a measure which assesses the degree of congruence between a set of attributes and the entire graph. We use it as the core of a statistical test, which congruent subspaces must pass. To illustrate its applicability to common graph mining tasks and in order to evaluate our selection scheme, we apply it to community outlier detection. Our selection of congruent subspaces enhances outlier detection by measuring outlier ness scores in selected subspaces only. Experiments on attributed graphs show that our approach outperforms traditional full space approaches and gives way to better outlier detection.
[attributed graph mining algorithm, Social network services, Image edge detection, Communities, graph theory, Estimation, data mining, subspace selection, statistical selection method, congruent subspaces, outlier ness scores, Vectors, full attribute space, Data mining, homophily, community outlier detection, Monte Carlo methods, attribute information, statistical test, attributed graphs, graph structure, statistical testing, multivariate spaces, edge structure]
Classifying Spam Emails Using Text and Readability Features
2013 IEEE 13th International Conference on Data Mining
None
2013
Supervised machine learning methods for classifying spam emails are long-established. Most of these methods use either header-based or content-based features. Spammers, however, can bypass these methods easily-especially the ones that deal with header features. In this paper, we report a novel spam classification method that uses features based on email content-language and readability combined with the previously used content-based task features. The features are extracted from four benchmark datasets viz. CSDMC2010, Spam Assassin, Ling Spam, and Enron-Spam. We use five well-known algorithms to induce our spam classifiers: Random Forest (RF), BAGGING, ADABOOSTM1, Support Vector Machine (SVM), and Nai&#x0308;ve Bayes (NB). We evaluate the classifier performances and find that BAGGING performs the best. Moreover, its performance surpasses that of a number of state-of-the-art methods proposed in previous studies. Although applied only to English language emails, the results indicate that our method may be an excellent means to classify spam emails in other languages, as well.
[text analysis, supervised machine learning methods, content-based task features, Ling spam, unsolicited e-mail, text categorization, HTML, random forest, SVM, Radio frequency, feature extraction, readability features, AdaboosTM1, header-based features, learning (artificial intelligence), benchmark datasets, pattern classification, email content-language, support vector machines, Unsolicited electronic mail, bagging, performance evaluation, spam assassin, Indexes, spam classifiers, Support vector machines, support vector machine, Nai&#x0308;ve Bayes, spam classification method, RF, NB, spam email classification, feature importance, spammers, CSDMC2010, English language emails, machine-learning application, Bayes methods, text features, anti-spam filter, Enron-spam, Spam classification, Bagging]
Most-Surely vs. Least-Surely Uncertain
2013 IEEE 13th International Conference on Data Mining
None
2013
Active learning methods aim to choose the most informative instances to effectively learn a good classifier. Uncertainty sampling, arguably the most frequently utilized active learning strategy, selects instances which are uncertain according to the model. In this paper, we propose a framework that distinguishes between two types of uncertainties: a model is uncertain about an instance due to strong and conflicting evidence (most-surely uncertain) vs. a model is uncertain about an instance because it does not have conclusive evidence (least-surely uncertain). We show that making a distinction between these uncertainties makes a huge difference to the performance of active learning. We provide a mathematical formulation to distinguish between these uncertainties for naive Bayes, logistic regression and support vector machines and empirically evaluate our methods on several real-world datasets.
[pattern classification, Uncertainty, support vector machines, active learning method, uncertain systems, Active learning, uncertainty sampling, active learning strategy, regression analysis, mathematical formulation, real-world datasets, Equations, Support vector machines, Learning systems, Measurement uncertainty, least-surely uncertain, naive Bayes method, most-surely uncertain, Bayes methods, Mathematical model, learning (artificial intelligence), logistic regression, Logistics]
On Pattern Preserving Graph Generation
2013 IEEE 13th International Conference on Data Mining
None
2013
Real datasets always play an essential role in graph mining and analysis. However, nowadays most available real datasets only support millions of nodes. Therefore, the literature on Big Data analysis utilizes statistical graph generators to generate a massive graph (e.g., billions of nodes) for evaluating the scalability of an algorithm. Nevertheless, current popular statistical graph generators are properly designed to preserve only the statistical metrics, such as the degree distribution, diameter, and clustering coefficient of the original social graphs. Recently, the importance of frequent graph patterns has been recognized in the various works on graph mining, but unfortunately this crucial criterion has not been noticed in the existing graph generators. To address this important need, we make the first attempt to design a Pattern Preserving Graph Generation (PPGG) algorithm to generate a graph including all frequent patterns and three most popular statistical parameters: degree distribution, clustering coefficient, and average vertex degree. The experimental results show that PPGG, which we have released as a free download, is efficient and able to generate a billion-node graph in approximately 10 minutes, much faster than the existing graph generators.
[Algorithm design and analysis, Big Data analysis, graph theory, data mining, Biology, Data mining, frequent graph pattern, Histograms, statistical graph generators, Databases, Clustering algorithms, pattern preserving graph generation, Graph Mining, average vertex degree, data analysis, clustering coefficient, statistical metrics, Big Data, graph analysis, Generators, real datasets, graph mining, Algorithms, pattern clustering, statistical analysis, degree distribution, PPGG algorithm]
Time Series Classification Using Compression Distance of Recurrence Plots
2013 IEEE 13th International Conference on Data Mining
None
2013
There is a huge increase of interest for time series methods and techniques. Virtually every piece of information collected from human, natural, and biological processes is susceptible to changes over time, and the study of how these changes occur is a central issue in fully understanding such processes. Among all time series mining tasks, classification is likely to be the most prominent one. In time series classification there is a significant body of empirical research that indicates that k-nearest neighbor rule in the time domain is very effective. However, certain time series features are not easily identified in this domain and a change in representation may reveal some significant and unknown features. In this work, we propose the use of recurrence plots as representation domain for time series classification. Our approach measures the similarity between recurrence plots using Campana-Keogh (CK-1) distance, a Kolmogorov complexity-based distance that uses video compression algorithms to estimate image similarity. We show that recurrence plots allied to CK-1 distance lead to significant improvements in accuracy rates compared to Euclidean distance and Dynamic Time Warping in several data sets. Although recurrence plots cannot provide the best accuracy rates for all data sets, we demonstrate that we can predict ahead of time that our method will outperform the time representation with Euclidean and Dynamic Time Warping distances.
[dynamic time warping, image classification, data mining, time series mining task, Complexity theory, Training, Accuracy, dstance measure, image similarity estimation, Mathematical model, k-nearest neighbor rule, CK-1 distance, time series classification, data compression, Time series analysis, Time series, time series, recurrence plot, video coding, classification, Equations, similarity measurement, Euclidean distance, Campana-Keogh distance, video compression algorithms, Kolmogorov complexity-based distance, recurrence plot compression distance]
Dynamic Pattern Detection with Temporal Consistency and Connectivity Constraints
2013 IEEE 13th International Conference on Data Mining
None
2013
We explore scalable and accurate dynamic pattern detection methods in graph-based data sets. We apply our proposed Dynamic Subset Scan method to the task of detecting, tracking, and source-tracing contaminant plumes spreading through a water distribution system equipped with noisy, binary sensors. While static patterns affect the same subset of data over a period of time, dynamic patterns may affect different subsets of the data at each time step. These dynamic patterns require a new approach to define and optimize penalized likelihood ratio statistics in the subset scan framework, as well as new computational techniques that scale to large, real-world networks. To address the first concern, we develop new subset scan methods that allow the detected subset of nodes to change over time, while incorporating temporal consistency constraints to reward patterns that do not dramatically change between adjacent time steps. Second, our Additive Graph Scan algorithm allows our novel scan statistic to process small graphs (500 nodes) in 4.1 seconds on average while maintaining an approximation ratio over 99% compared to an exact optimization method, and to scale to large graphs with over 12,000 nodes in 30 minutes on average. Evaluation results across multiple detection, tracking, and source-tracing tasks demonstrate substantial performance gains achieved by the Dynamic Subset Scan approach.
[water distribution system, noisy binary sensors, Additives, Heuristic algorithms, source-tracing contaminant plumes, graph theory, data mining, sensor fusion, Sensor systems, Optimization, water distribution systems, temporal consistency, spatial and subset scan statistics, graph-based data sets, Mathematical model, approximation ratio, penalized likelihood ratio statistics, dynamic subset scan method, exact optimization method, likelihood ratio statistics, connectivity constraints, Equations, contamination, static patterns, additive graph scan algorithm, environmental science computing, temporal consistency constraints, dynamic pattern detection method, multiple detection, statistical analysis]
Noise-Resistant Bicluster Recognition
2013 IEEE 13th International Conference on Data Mining
None
2013
Biclustering is crucial in finding co-expressed genes and their associated conditions in gene expression data. While various biclustering algorithms (e.g., combinatorial, probabilistic modelling, and matrix factorization) have been proposed and constantly improved in the past decade, data noise and bicluster overlaps make biclustering a still challenging task. It becomes difficult to further improve biclustering performance, without resorting to a new approach. Inspired by the recent progress in unsupervised feature learning using deep neural networks, in this work, we propose a novel model for biclustering, named Auto Decoder (AD), by relating biclusters to features and leveraging a neural network that is able to automatically learn features from the input data. To suppress severe noise present in gene expression data, we introduce a non-uniform signal recovery mechanism: Instead of reconstructing the whole input data to capture the bicluster patterns, AD weighs the zero and non-zero parts of the input data differently and is more flexible in dealing with different types of noise. AD is also properly regularized to deal with bicluster overlaps. To the best of our knowledge, this is the first biclustering algorithm that leverages neural network techniques to recover overlapped biclusters hidden in noisy gene expression data. We compared our approach with four state-of-the-art biclustering algorithms on both synthetic and real datasets. On three out of the four real datasets, AD significantly outperforms the other approaches. On controlled synthetic datasets, AD performs the best when noise level is beyond 15%.
[Algorithm design and analysis, noisy gene expression data, noise suppression, Noise, Noise level, co-expressed genes, genetics, biology computing, noise-resistant bicluster recognition, Robustness, AutoDecoder, deep neural networks, pattern recognition, nonuniform signal recovery mechanism, synthetic datasets, overlapped bicluster recovery, Gene Expression, AD, Biclustering, Neurons, bicluster patterns, biclustering algorithms, Gene expression, unsupervised feature learning, real datasets, unsupervised learning, Neural Network, pattern clustering, Neural networks, automatic feature learning, neural nets]
Itemsets for Real-Valued Datasets
2013 IEEE 13th International Conference on Data Mining
None
2013
Pattern mining is one of the most well-studied sub fields in exploratory data analysis. While there is a significant amount of literature on how to discover and rank item sets efficiently from binary data, there is surprisingly little research done in mining patterns from real-valued data. In this paper we propose a family of quality scores for real-valued item sets. We approach the problem by considering casting the dataset into a binary data and computing the support from this data. This naive approach requires us to select thresholds. To remedy this, instead of selecting one set of thresholds, we treat thresholds as random variables and compute the average support. We show that we can compute this support efficiently, and we also introduce two normalisations, namely comparing the support against the independence assumption and, more generally, against the partition assumption. Our experimental evaluation demonstrates that we can discover statistically significant patterns efficiently.
[normalisations, data analysis, pattern mining, data mining, naive approach, random processes, random variables, Vectors, Data mining, quality scores, Standards, Reactive power, binary data, real-valued datasets, Itemsets, exploratory data analysis, itemset discovery, statistically significant pattern discovery, itemset ranking, itemsets, real-valued itemsets, Random variables]
Collective Response Spike Prediction for Mutually Interacting Consumers
2013 IEEE 13th International Conference on Data Mining
None
2013
Modeling how marketing actions in various channels influence or cause consumer purchase decisions is crucial for marketing decision-making. Marketing campaigns stimulate consumer awareness, interest and help drive interactions such as the browsing of product web pages, ultimately impacting an individual's purchase decision. In addition, some successful campaigns stimulate word-of-mouth and social trends among consumers, and such collective behavior of consumers result in concurrent and correlated responses over a short term. Though each consumer's response should be attributed with both the same individual's experiences and the collective factors, unobservability of most word-of-mouth events makes the estimation challenging. The authors propose a new continuous-time predictive model for time-dependent response rates of each consumer, which can incorporate both the individual and the collective factors without explicit word-of-mouth observations. The individual factor is modeled as staircase functions associated with the experienced events by each consumer, and provides a clear psychological interpretation about how marketing advertising communications impact short-term and mid-term memories of consumers. The collective factor is modeled with aggregate response frequencies for mutually-interacting groups that are automatically estimated from data. The key idea to mine the mutually-interacting groups exists in a three-step estimator, which initially performs a Poisson regression without the collective factor, then does clustering of the residual time-series in the initial regression, and finally performs another Poisson regression involving the collective factor. The proposed collective factor robustly incorporates the underlying trends even when causality from one consumer's event spikes to another consumer's response is weak. High predictive accuracy of the proposed approach is empirically validated using real-world data provided by an online retailer in Europe.
[residual clustering, individual factor, consumer mid-term memory, purchasing, data mining, social trends, aggregate response frequency, Predictive models, time-dependent response rates, consumer short-term memory, advertising, Approximation methods, marketing action modelling, collective response spike prediction, consumer purchase decision, consumer event spikes, time-decaying curves, Market research, Prediction algorithms, marketing advertising communications, retailing, Computational modeling, Europe, online retailer, time series, Vectors, mutually interacting consumer, word-of-mouth, Poisson regression, mutually-interacting group mining, Continuous-time event prediction, Aggregates, pattern clustering, collective factor, residual time-series clustering, three-step estimator, decision making, marketing decision-making, continuous-time predictive model, staircase functions]
Price Information Patterns in Web Search Advertising: An Empirical Case Study on Accommodation Industry
2013 IEEE 13th International Conference on Data Mining
None
2013
Unlike advertising in traditional media, web search advertising content can be easily customized with little cost. In this paper, we apply content analysis and regression models on 11,818 unique ads related to the accommodation industry to empirically investigate how advertisers customize price information in their web search advertising content. To the best of our knowledge, our study is the first of this kind. We find that advertiser characteristics, such as website traffic, product quality, and position in the distribution chain, affect both the amount and forms of price information in its search advertising content. Moreover, the use of price information by an advertiser depends on query characteristics, such as search volume, cost per click ("CPC"), and specific words (e.g., trademark, location, price cue) in queries. Our empirical findings shed new light on how to effectively manage price information in search advertising, and suggest new research opportunities on web search advertising.
[Industries, distribution chain position, regression analysis, search advertising, cost per click, query processing, advertiser characteristics, product quality, price information patterns, Trademarks, Advertising, Web search advertising content analysis, Google, accommodation industry, advertising data processing, search volume, query, Educational institutions, price information management, content analysis, regression, CPC, Quality assessment, Internet, Website traffic, Web search, pricing, regression models]
Discovering Non-redundant Overlapping Biclusters on Gene Expression Data
2013 IEEE 13th International Conference on Data Mining
None
2013
Given a gene expression data matrix where each cell is the expression level of a gene under a certain condition, biclustering is the problem of searching for a subset of genes that co regulate and co express only under a subset of conditions. As some genes can belong to different functional categories, searching for non-redundant overlapping biclusters is an important problem in biclustering. However, most recent algorithms can only either produce disjoint biclusters or redundant biclusters with significant overlap. In other words, these algorithms do not allow users to specify the maximum overlap between the biclusters. In this paper, we propose a novel algorithm which can generate K overlapping biclusters where the maximum overlap between them is below a predefined threshold. Unlike the other approaches which often generate all biclusters at once, our algorithm produces the biclusters sequentially, where each newly generated bicluster is guaranteed to be different from the previous ones but can still overlap with them. The experiments on real datasets confirm that different meaningful overlapping biclusters are successfully discovered. Besides, under the same constraints, our algorithm returns much larger and higher-quality biclusters compared to those of the other state-of-the art algorithms.
[gene expression data, Biological system modeling, nonredundant overlapping bicluster discovery, data mining, Search problems, functional category, Complexity theory, gene expression data matrix, Gene expression, disjoint biclusters, biology computing, pattern clustering, Clustering algorithms, Coherence, non-redundant overlapping biclustering]
Cox Regression with Correlation Based Regularization for Electronic Health Records
2013 IEEE 13th International Conference on Data Mining
None
2013
Survival Regression models play a vital role in analyzing time-to-event data in many practical applications ranging from engineering to economics to healthcare. These models are ideal for prediction in complex data problems where the response is a time-to-event variable. An event is defined as the occurrence of a specific event of interest such as a chronic health condition. Cox regression is one of the most popular survival regression model used in such applications. However, these models have the tendency to over fit the data which is not desirable for healthcare applications because it limits their generalization to other hospital scenarios. In this paper, we address these challenges for the cox regression model. We combine two unique correlation based regularizers with cox regression to handle correlated and grouped features which are commonly seen in many practical problems. The proposed optimization problems are solved efficiently using cyclic coordinate descent and Alternate Direction Method of Multipliers algorithms. We conduct experimental analysis on the performance of these algorithms over several synthetic datasets and electronic health records (EHR) data about heart failure diagnosed patients from a hospital. We demonstrate through our experiments that these regularizers effectively enhance the ability of cox regression to handle correlated features. In addition, we extensively compare our results with other regularized linear and logistic regression algorithms. We validate the goodness of the features selected by these regularized cox regression models using the biomedical literature and different feature selection algorithms.
[Algorithm design and analysis, hospitals, correlation based regularization, chronic health condition, Medical services, regression analysis, regularization, EHR data, multipliers algorithm alternate direction method, healthcare, optimisation, biomedical literature, feature selection algorithm, heart failure diagnosed patients, time-to-event data analysis, hospital, Kernel, feature selection, synthetic datasets, data analysis, Biological system modeling, survival regression models, cyclic coordinate descent, logistic regression algorithm, regularized linear regression algorithm, Vectors, Hazards, electronic health records, Equations, cardiology, cox regression model, cox regression, patient diagnosis, optimization problems]
Constructing Topical Hierarchies in Heterogeneous Information Networks
2013 IEEE 13th International Conference on Data Mining
None
2013
A digital data collection (e.g., scientific publications, enterprise reports, news, and social media) can often be modeled as a heterogeneous information network, linking text with multiple types of entities. Constructing high-quality concept hierarchies that can represent topics at multiple granularities benefits tasks such as search, information browsing, and pattern mining. In this work we present an algorithm for recursively constructing multi-typed topical hierarchies. Contrary to traditional text-based topic modeling, our approach handles both textual phrases and multiple types of entities by a newly designed clustering and ranking algorithm for heterogeneous network data, as well as mining and ranking topical patterns of different types. Our experiments on datasets from two different domains demonstrate that our algorithm yields high quality, multi-typed topical hierarchies.
[Algorithm design and analysis, text analysis, data mining, network theory (graphs), Data mining, heterogeneous information network data, heterogeneous network, Clustering algorithms, Distributed databases, textual phrases, data acquisition, high quality multityped topical hierarchies, ranking algorithm, topical pattern ranking, information network, clustering algorithm, multityped topical hierarchies, digital data collection, Query processing, pattern clustering, topical pattern mining, high-quality concept hierarchies, Inference algorithms, Data models, topic hierarchy]
Binary Time-Series Query Framework for Efficient Quantitative Trait Association Study
2013 IEEE 13th International Conference on Data Mining
None
2013
Quantitative trait association study examines the association between quantitative traits and genetic variants. As a promising tool, it has been widely applied to dissect the genetic basis of complex diseases. However, such study usually involves testing trillions of variant-trait pairs and demands intensive computational resources. Recently, several algorithms have been developed to improve its efficiency. In this paper, we propose a framework, Fabrique, which models quantitative trait association study as querying binary time-series and bridges the two seemly different problems. Specifically, in the proposed framework, genetic variants are treated as a database consisting of binary time-series. Finding trait-associated variants is equivalent to finding the nearest neighbors of the trait. For efficient query process, Fabrique partitions and normalizes the binary time-series, and estimates a tight upper bound for each group of time-series to prune the search space. Extensive experimental results demonstrate that Fabrique only needs to search a very small portion of the database to locate the target variants and significantly outperforms the state-of-the-art method. We also show that Fabrique can be applied to other binary time-series query problem in addition to the genetic association study.
[Correlation, genetic association, Quantitative Trait Association Study, query processing, genetics, quantitative trait association study, Fabrique partitions, biology computing, Time-Series Query, query process, Pruning, Genetics, IP networks, search problems, search space pruning, Lower bound, Time series analysis, time series, Indexes, binary time-series query framework, Equations, genetic variants, Upper bound, complex diseases, trait-associated variants]
Communication-Efficient Distributed Multiple Reference Pattern Matching for M2M Systems
2013 IEEE 13th International Conference on Data Mining
None
2013
In M2M applications, it is very common to encounter the ad hoc snapshot query that requires fast responses from many local machines in which all the data are distributed. In the scenario when the query is more complex, the communication cost for sending it to all the local machines for processing can be very high. This paper aims to address this issue. Given a reference set of multiple and large-size patterns, we propose an approach to identifying its k nearest and farthest neighbors globally across all the local machines. By decomposing the reference patterns into a multi-resolution representation and using novel distance bound designs, our method guarantees the exact results in a communication-efficient manner. Analytical and empirical studies show that our method outperforms the state-of-the-art methods in saving significant bandwidth usage, especially for large numbers of machines and large-sized reference patterns.
[Wavelet transforms, pattern matching, communication-efficient distributed multiple reference pattern matching, M2M systems, k nearest neighbors, distance bound designs, machine-to-machine systems, Time series analysis, large-sized reference patterns, bandwidth usage, distributed processing, Servers, multiresolution representation, Couplings, query processing, Upper bound, M2M applications, Bandwidth, Vegetation, ad hoc snapshot query, local machines, k farthest neighbors, reference pattern decomposition]
Sparse K-Means with the l_q(0leq q&lt; 1) Constraint for High-Dimensional Data Clustering
2013 IEEE 13th International Conference on Data Mining
None
2013
Sparse clustering, which aims at finding a proper partition of extremely high dimensional data set with fewest relevant features, has been attracted more and more attention. Most researches model the problem through minimizing weighted feature contributions subject to a l<sub>1</sub> constraint. However, the l<sub>0</sub> constraint is the essential constraint for sparse modeling while the l<sub>1</sub> constraint is only a convex relaxation of it. In this article, we bridge the gap between the l<sub>0</sub> constraint and the l<sub>1</sub> constraint through development of two new sparse clustering models, which are the sparse k-means with the l<sub>q</sub>(0 &lt;; q &lt;; 1) constraint and the sparse k-means with the l<sub>0</sub> constraint. By proving the certain forms of the optimal solution of particular l<sub>q</sub>(0 = q &lt;; 1) non-convex optimizations, two efficient iterative algorithms are proposed. We conclude with experiments on both synthetic data and the Allen Developing on both synthetic data and the l<sub>q</sub>(0 = q &lt;; 1) models exhibit the advantages compared with the standard k-mans and sparse k-means with the l<sub>1</sub> constraint.
[iterative methods, Conferences, nonconvex optimizations, Data mining, l<sub>0</sub> constraint, High-Dimensional Clustering, l<sub>1</sub> constraint, pattern clustering, 0l_q(0 = q &lt; 1) Constraint, synthetic data, l<sub>q</sub> constraint, sparse clustering models, high-dimensional data clustering, convex relaxation, weighted feature minimization, concave programming, Sparse K-means, sparse k-means, iterative algorithms]
A Comparative Analysis of Ensemble Classifiers: Case Studies in Genomics
2013 IEEE 13th International Conference on Data Mining
None
2013
The combination of multiple classifiers using ensemble methods is increasingly important for making progress in a variety of difficult prediction problems. We present a comparative analysis of several ensemble methods through two case studies in genomics, namely the prediction of genetic interactions and protein functions, to demonstrate their efficacy on real-world datasets and draw useful conclusions about their behavior. These methods include simple aggregation, meta-learning, cluster-based meta-learning, and ensemble selection using heterogeneous classifiers trained on resampled data to improve the diversity of their predictions. We present a detailed analysis of these methods across 4 genomics datasets and find the best of these methods offer statistically significant improvements over the state of the art in their respective domains. In addition, we establish a novel connection between ensemble selection and meta-learning, demonstrating how both of these disparate methods establish a balance between ensemble diversity and performance.
[Ensemble selection, Stacking, Genomics, Ensemble methods, Proteins, comparative analysis, genetic interaction prediction, Accuracy, proteins, meta-learning, genomics, ensemble classifier, simple aggregation, Bioinformatics, pattern classification, ensemble diversity, ensemble performance, cluster-based meta-learning, Diversity reception, Supervised learning, protein functions, bioinformatics, ensemble methods, ensemble selection, heterogeneous classifiers]
Stochastic Blockmodel with Cluster Overlap, Relevance Selection, and Similarity-Based Smoothing
2013 IEEE 13th International Conference on Data Mining
None
2013
Stochastic block models provide a rich, probabilistic framework for modeling relational data by expressing the objects being modeled in terms of a latent vector representation. This representation can be a latent indicator vector denoting the cluster membership (hard clustering), a vector of cluster membership probabilities (soft clustering), or more generally a real-valued vector (latent space representation). Recently, a new class of overlapping stochastic block models has been proposed where the idea is to allow the objects to have hard memberships in multiple clusters (in form of a latent binary vector). This aspect captures the properties of many real-world networks in domains such as biology and social networks where objects can simultaneously have memberships in multiple clusters owing to the multiple roles they may have. In this paper, we improve upon this model in three key ways: (1) we extend the overlapping stochastic block model to the bipartite graph case which enables us to simultaneously learn the overlapping clustering of two different sets of objects in the graph, the unipartite graph is just a special case of our model, (2) we allow objects (in either set) to not have membership in any cluster by using a relevant object selection mechanism, and (3) we make use of additionally available object features (or a kernel matrix of pair wise object similarities) to further improve the overlapping clustering performance. We do this by explicitly encouraging similar objects to have similar cluster membership vectors. Moreover, using nonparametric Bayesian prior distributions on the key model parameters, we side-step the model selection issues such as selecting the number of clusters a priori. Our model is quite general and can be applied for both overlapping clustering and link prediction tasks in unipartite and bipartite networks (directed/undirected), or for overlapping co-clustering of general binary-valued data. Experiments on synthetic and real-world datasets from biology and social networks demonstrate that our model outperforms several state-of-the-art methods.
[graph theory, Stochastic processes, link prediction tasks, soft clustering, real-world networks, overlapping clustering learning, relevant object selection mechanism, nonparametric Bayesian, unipartite graph, biology computing, kernel matrix, link prediction, relational data modelling, latent space representation, Bipartite graph, learning (artificial intelligence), stochastic processes, latent indicator vector representation, Kernel, unipartite network, latent binary vector, biology, Social network services, probabilistic framework, real-valued vector, social networks, similarity-based smoothing, bipartite graph case, nonparametric Bayesian prior distributions, Vectors, matrix algebra, hard clustering, cluster membership vectors, vectors, pattern clustering, pair wise object similarities, relevance selection, general binary-valued data overlapping co-clustering, stochastic blockmodel, cluster membership probability vector, bipartite network, social networking (online), Data models, overlapping clustering, Bayes methods, overlapping stochastic block model]
Multi-instance Multi-graph Dual Embedding Learning
2013 IEEE 13th International Conference on Data Mining
None
2013
Multi-instance learning concerns about building learning models from a number of labeled instance bags, where each bag consists of instances with unknown labels. A bag is labeled positive if one or more multiple instances inside the bag is positive, and negative otherwise. For all existing multi-instance learning algorithms, they are only applicable to the setting where instances in each bag are represented by a set of well defined feature values. In this paper, we advance the problem to a multi-instance multi-graph setting, where a bag contains a number of instances and graphs in pairs, and the learning objective is to derive classification models from labeled bags, containing both instances and graphs, to predict previously unseen bags with maximum accuracy. To achieve the goal, the main challenge is to properly represent graphs inside each bag and further take advantage of complementary information between instance and graph pairs for learning. In the paper, we propose a Dual Embedding Multi-Instance Multi-Graph Learning (DE-MIMG) algorithm, which employs a dual embedding learning approach to (1) embed instance distributions into the informative sub graphs discovery process, and (2) embed discovered sub graphs into the instance feature selection process. The dual embedding process results in an optimal representation for each bag to provide combined instance and graph information for learning. Experiments and comparisons on real-world multi-instance multi-graph learning tasks demonstrate the algorithm performance.
[Multi-graph, graph theory, instance feature selection process, data mining, Embedding, labeled instance bags, Classification, Bismuth, learning (artificial intelligence), dual embedding process, Kernel, feature selection, multiinstance multigraph setting, DE-MIMG algorithm, pattern classification, Laplace equations, classification models, graph information, dual embedding learning approach, Linear programming, Educational institutions, Vectors, Multi-instance, Computer science, informative subgraph discovery process, Graph, dual embedding multiinstance multigraph learning algorithm]
TopicSketch: Real-Time Bursty Topic Detection from Twitter
2013 IEEE 13th International Conference on Data Mining
None
2013
Twitter has become one of the largest platforms for users around the world to share anything happening around them with friends and beyond. A bursty topic in Twitter is one that triggers a surge of relevant tweets within a short time, which often reflects important events of mass interest. How to leverage Twitter for early detection of bursty topics has therefore become an important research problem with immense practical value. Despite the wealth of research work on topic modeling and analysis in Twitter, it remains a huge challenge to detect bursty topics in real-time. As existing methods can hardly scale to handle the task with the tweet stream in real-time, we propose in this paper Topic Sketch, a novel sketch-based topic model together with a set of techniques to achieve real-time detection. We evaluate our solution on a tweet stream with over 30 million tweets. Our experiment results show both efficiency and effectiveness of our approach. Especially it is also demonstrated that Topic Sketch can potentially handle hundreds of millions tweets per day which is close to the total number of daily tweets in Twitter and present bursty event in finer-granularity.
[realtime, tweet stream, real-time bursty topic detection, Twitter, topic modeling, information analysis, Surges, bursty topic, Equations, Optimization, topic analysis, TopicSketch, social networking (online), Real-time systems, sketch-based topic model, Acceleration, Monitoring]
Efficient Learning on Point Sets
2013 IEEE 13th International Conference on Data Mining
None
2013
Recently several methods have been proposed to learn from data that are represented as sets of multidimensional vectors. Such algorithms usually suffer from the high demand of computational resources, making them impractical on large-scale problems. We propose to solve this problem by condensing i.e. reducing the sizes of the sets while maintaining the learning performance. Three methods are examined and evaluated with a wide spectrum of set learning algorithms on several large-scale image data sets. We discover that k-Means can successfully achieve the goal of condensing. In many cases, k-Means condensing can improve the algorithms' speed, space requirements, and surprisingly, learning performances simultaneously.
[point sets, image classification, efficient, data mining, kmeans, algorithm speed, Vectors, Complexity theory, Approximation methods, Training, k-means condensing, fast, algorithm space requirements, Approximation algorithms, Feature extraction, collective data, point set, learning (artificial intelligence), large-scale, Kernel, efficient learning]
Markov Blanket Feature Selection with Non-faithful Data Distributions
2013 IEEE 13th International Conference on Data Mining
None
2013
In faithful Bayesian networks, the Markov blanket of the class attribute is a unique and minimal feature subset for optimal feature selection. However, little attention has been paid to Markov blanket feature selection in a non-faithful environment which widely exists in the real world. To tackle this issue, in this paper, we deal with non-faithful data distributions and propose the concept of representative sets instead of Markov blankets. With a standard sparse group lasso for selection of features from the representative sets, we design an effective algorithm, SRS, for Markov blanket feature Selection via Representative Sets with non-faithful data distributions. Empirical studies demonstrate that SRS outperforms the state-of-the-art Markov blanket feature selectors and other well-established feature selection methods.
[Algorithm design and analysis, Feature selection, class attribute, representative sets, Redundancy, nonfaithful data distribution, Probability distribution, Faithful Bayesian networks, Standards, Representative sets, Markov blankets, standard sparse group lasso, SRS, Markov processes, Markov blanket feature selector, Bayes methods, Bayesian networks, Sparse group lasso, belief networks, Joints, feature selection, Markov blanket feature selection]
An Efficient Approach to Updating Closeness Centrality and Average Path Length in Dynamic Networks
2013 IEEE 13th International Conference on Data Mining
None
2013
Closeness centrality measures the communication efficiency of a specific vertex within a network while the average path length (APL) measures that of the whole network. Since the nature of these two measurements is based on the computation of all-pair shortest path distances, one can perform the breadth-first search method starting at every vertex and obtain the two measurements. However, as the edge counts in the real-world networks like Facebook increase over time, this naive way is obviously inefficient. In this paper, we proposed CENDY, an efficient approach to updating Closeness centrality and average path length in Dynamic networks when there is an edge insertion or deletion. In CENDY, we derived some theoretical properties to quickly identify a set of vertices whose shortest path changed after an edge update, and then update the closeness centralities of those vertices only as well as the APL of the graph by a few of single-source shortest path computations. We conducted extensive experiments to show that, when compared to the existing methods of computing exact or approximate values, CENDY outperformed others in significantly low update time while providing exact values of the two measurements on various real-world graph datasets.
[Algorithm design and analysis, updating closeness centrality, Heuristic algorithms, Length measurement, network theory (graphs), real-world graph datasets, specific vertex, Approximation methods, single-source shortest path computations, CENDY approach, update algorithm, breadth-first search method, edge counts, dynamic networks, Facebook, all-pair shortest path distances, Time measurement, edge insertion, tree searching, edge deletion, APL measures, closeness centrality, directed graphs, Approximation algorithms, communication efficiency, Joining processes, average path length, computational complexity]
Reconstructing Individual Mobility from Smart Card Transactions: A Space Alignment Approach
2013 IEEE 13th International Conference on Data Mining
None
2013
Smart card transactions capture rich information of human mobility and urban dynamics, therefore are of particular interest to urban planners and location-based service providers. However, since most transaction systems are only designated for billing purpose, typically, fine-grained location information, such as the exact boarding and alighting stops of a bus trip, is only partially or not available at all, which blocks deep exploitation of this rich and valuable data at individual level. This paper presents a "space alignment" framework to reconstruct individual mobility history from a large-scale smart card transaction dataset pertaining to a metropolitan city. Specifically, we show that by delicately aligning the monetary space and geospatial space with the temporal space, we are able to extrapolate a series of critical domain specific constraints. Later, these constraints are naturally incorporated into a semi-supervised conditional random field to infer the exact boarding and alighting stops of all transit routes with a surprisingly high accuracy, e.g., given only 10% trips with known alighting/boarding stops, we successfully inferred more than 78% alighting and boarding stops from all unlabeled trips. In addition, we demonstrated that the smart card data enriched by the proposed approach dramatically improved the performance of a conventional method for identifying users' home and work places (with 88% improvement on home detection and 35% improvement on work place detection). The proposed method offers the possibility to mine individual mobility from common public transit transactions, and showcases how uncertain data can be leveraged with domain knowledge and constraints, to support cross-application data mining tasks.
[Legged locomotion, Smart cards, location-based service providers, temporal space, Roads, large-scale smart card transaction dataset, metropolitan city, data mining, smart cards, urban planners, space alignment approach, Geospatial analysis, Data mining, monetary space, town and country planning, individual mobility reconstruction, geospatial space, cross-application data mining tasks, fine-grained location information, semisupervised conditional random field, Cities and towns, Labeling]
Feature Transformation with Class Conditional Decorrelation
2013 IEEE 13th International Conference on Data Mining
None
2013
The well-known feature transformation model of Fisher linear discriminant analysis (FDA) can be decomposed into an equivalent two-step approach: whitening followed by principal component analysis (PCA) in the whitened space. By proving that whitening is the optimal linear transformation to the Euclidean space in the sense of minimum log-determinant divergence, we propose a transformation model called class conditional decor relation (CCD). The objective of CCD is to diagonalize the covariance matrices of different classes simultaneously, which is efficiently optimized using a modified Jacobi method. CCD is effective to find the common principal components among multiple classes. After CCD, the variables become class conditionally uncorrelated, which will benefit the subsequent classification tasks. Combining CCD with the nearest class mean (NCM) classification model can significantly improve the classification accuracy. Experiments on 15 small-scale datasets and one large-scale dataset (with 3755 classes) demonstrate the scalability of CCD for different applications. We also discuss the potential applications of CCD for other problems such as Gaussian mixture models and classifier ensemble learning.
[Sparse matrices, Covariance matrices, Gaussian mixture model, minimum log-determinant divergence, Jacobian matrices, optimisation, optimal linear transformation, modified Jacobi method, decorrelation, Euclidean space, Decorrelation, class conditional decorrelation, Charge coupled devices, covariance matrices, pattern classification, feature transformation model, feature transformation, CCD, classifier ensemble learning, Indexes, equivalent two-step approach, large-scale dataset, Fisher linear discriminant analysis, principal component analysis, Principal component analysis, nearest class mean classification model, simultaneous diagonalization]
Efficient Learning for Models with DAG-Structured Parameter Constraints
2013 IEEE 13th International Conference on Data Mining
None
2013
In high-dimensional models, hierarchical and structural relationships among features are often used to constrain the search for the more important interactions. These relationships may come from prior knowledge or traditional design principles, such as that low-order effects should have larger contributions than higher-order ones and should be included into the model earlier. However, these structural constraints also make the optimization problem more challenging. In this paper, we propose the use of the alternating direction method of multipliers (ADMM) and accelerated gradient methods. In particular, we show that ADMM can be used to either directly solve the problem or serve as a key building block. Experimental results on a number of synthetic and real-world data sets demonstrate that the proposed algorithm is efficient and flexible. Moreover, the use of the hierarchical relationships consistently improves generalization performance and parameter estimation.
[DAG-structured parameter constraints, optimization problem, Alternating direction method of multipliers, generalization performance, Conferences, feature hierarchical relationships, learning, Data mining, optimisation, accelerated gradient methods, key building block, parameter estimation, low-order effects, learning (artificial intelligence), Accelerated gradient methods, gradient methods, ADMM, structural constraints, high-dimensional models, Heredity, generalisation (artificial intelligence), feature structural relationships, alternating direction method of multipliers, directed graphs, design principles, Structural sparsity]
UBLF: An Upper Bound Based Approach to Discover Influential Nodes in Social Networks
2013 IEEE 13th International Conference on Data Mining
None
2013
Influence maximization, defined as finding a small subset of nodes that maximizes spread of influence in social networks, is NP-hard under both Linear Threshold (LT) and Independent Cascade (IC) models, where a line of greedy/heuristic algorithms have been proposed. The simple greedy algorithm [14] achieves an approximation ratio of 1-1/e. The advanced CELF algorithm [16], by exploiting the sub modular property of the spread function, runs 700 times faster than the simple greedy algorithm on average. However, CELF is still inefficient [4], as the first iteration calls for N times of spread estimations (N is the number of nodes in networks), which is computationally expensive especially for large networks. To this end, in this paper we derive an upper bound function for the spread function. The bound can be used to reduce the number of Monte-Carlo simulation calls in greedy algorithms, especially in the first iteration of initialization. Based on the upper bound, we propose an efficient Upper Bound based Lazy Forward algorithm (UBLF in short), by incorporating the bound into the CELF algorithm. We test and compare our algorithm with prior algorithms on real-world data sets. Experimental results demonstrate that UBLF, compared with CELF, reduces more than 95% Monte-Carlo simulations and achieves at least 2-5 times speed-raising when the seed set is small.
[Greedy algorithms, submodular property, independent cascade model, upper bound function, upper bound based lazy forward algorithm, Monte Carlo methods, optimisation, Independent Cascade Model, Influence Maximization, spread estimations, spread function, greedy algorithm, UBLF, influential node discovery, approximation ratio, approximation theory, LT model, Monte-Carlo simulation calls, heuristic algorithms, Computational modeling, greedy algorithms, IC model, Estimation, social networks, CELF algorithm, Upper bound, Greedy Algorithms, Social Networks, Approximation algorithms, social networking (online), influence maximization, linear threshold model, Integrated circuit modeling]
Divide-and-Conquer Anchoring for Near-Separable Nonnegative Matrix Factorization and Completion in High Dimensions
2013 IEEE 13th International Conference on Data Mining
None
2013
Nonnegative matrix factorization (NMF) becomes tractable in polynomial time with unique solution under separability assumption, which postulates all the data points are contained in the conical hull of a few anchor data points. Recently developed linear programming and greedy pursuit methods can pick out the anchors from noisy data and results in a near-separable NMF. But their efficiency could be seriously weakened in high dimensions. In this paper, we show that the anchors can be precisely located from low-dimensional geometry of the data points even when their high dimensional features suffer from serious incompleteness. Our framework, entitled divide-and-conquer anchoring (DCA), divides the high-dimensional anchoring problem into a few cheaper sub-problems seeking anchors of data projections in low-dimensional random spaces, which can be solved in parallel by any near-separable NMF, and combines all the detected low-dimensional anchors via a fast hypothesis testing to identify the original anchors. We further develop two non-iterative anchoring algorithms in 1D and 2D spaces for data in convex hull and conical hull, respectively. These two rapid algorithms in the ultra low dimensions suffice to generate a robust and efficient near-separable NMF for high-dimensional or incomplete data via DCA. Compared to existing methods, two vital advantages of DCA are its scalability for big data, and capability of handling incomplete and high-dimensional noisy data. A rigorous analysis proves that DCA is able to find the correct anchors of a rank-k matrix by solving math cal O(klog k) sub-problems. Finally, we show DCA outperforms state-of-the-art methods on various datasets and tasks.
[iterative methods, separability assumption, divide and conquer methods, low- dimensional geometry, matrix completion, Noise, Nonnegative matrix factorization, near-separable nonnegative matrix factorization, anchor points, linear programming, conical hull, matrix decomposition, divide-and-conquer anchoring, Bismuth, Robustness, polynomial time, low-dimensional random spaces, big data, fast hypothesis testing, Testing, divide-and-conquer, Vectors, Noise measurement, convex hull, greedy pursuit methods, Geometry, geometry, data handling, random projection]
A Novel Relational Learning-to-Rank Approach for Topic-Focused Multi-document Summarization
2013 IEEE 13th International Conference on Data Mining
None
2013
Topic-focused multi-document summarization aims to produce a summary over a set of documents and conveys the most important aspects of a given topic. Most existing extractive methods view the task as a multi-criteria ranking problem over sentences, where relevance, salience and diversity are three typical requirements. However, diversity is a challenging problem as it involves modeling the relationship between sentences during ranking, where traditional methods usually tackle it in a heuristic or implicit way. In this paper, we propose a novel relational learning-to-rank approach (R-LTR) to solve this problem. Relational learning-to-rank is a new learning framework which further incorporates relationships into traditional learning-to-rank in an elegant way. Specifically, the ranking function is defined as the combination of content-based score of individual sentence, and relation-based score between the current sentence and those already selected. On this basis, we propose to learn the ranking function by minimizing the likelihood loss based on Plackett-Luce model, which can naturally model the sequential ranking procedure of candidate sentences. Stochastic gradient descent is then employed to conduct the learning process, and the summary is predicted by the greedy selection procedure based on the learned ranking function. Finally, we conduct extensive experiments on benchmark data sets TAC2008 and TAC2009. Experimental results show that our approach can significantly outperform the state-of-the-art methods from both quantitative and qualitative aspects.
[Measurement, multicriteria ranking problem, greedy selection procedure, Stochastic processes, TAC2009 benchmark data sets, relational learning-to-rank approach, Plackett-Luce model, candidate sentences, Training data, R-LTR, learning (artificial intelligence), stochastic processes, likelihood loss minimization, gradient methods, document handling, content-based score, Computational modeling, heuristic, relation-based score, Vectors, topic-focused multidocument summarization, stochastic gradient descent, TAC2008 benchmark data sets, Support vector machines, Hidden Markov models, sequential ranking procedure]
Cartification: A Neighborhood Preserving Transformation for Mining High Dimensional Data
2013 IEEE 13th International Conference on Data Mining
None
2013
The analysis of high dimensional data comes with many intrinsic challenges. In particular, cluster structures become increasingly hard to detect when the data includes dimensions irrelevant to the individual clusters. With increasing dimensionality, distances between pairs of objects become very similar, and hence, meaningless for knowledge discovery. In this paper we propose Cartification, a new transformation to circumvent this problem. We transform each object into an item set, which represents the neighborhood of the object. We do this for multiple views on the data, resulting in multiple neighborhoods per object. This transformation enables us to preserve the essential pair wise-similarities of objects over multiple views, and hence, to improve knowledge discovery in high dimensional data. Our experiments show that frequent item set mining on the certified data outperforms competing clustering approaches on the original data space, including traditional clustering, random projections, principle component analysis, subspace clustering, and clustering ensemble.
[Algorithm design and analysis, neighborhood preserving transformation, random projections, Noise, data mining, knowledge discovery, Data mining, Noise measurement, transformation, frequent itemset mining, high dimensional data mining, cartification, Itemsets, pattern clustering, subspace clustering, high dimensional data, Clustering algorithms, principle component analysis, subspace projections, clustering, clustering ensemble]
Improved Electricity Load Forecasting via Kernel Spectral Clustering of Smart Meters
2013 IEEE 13th International Conference on Data Mining
None
2013
This paper explores kernel spectral clustering methods to improve forecasts of aggregated electricity smart meter data. The objective is to cluster the data in such a way that building a forecasting models separately for each cluster and taking the sum of forecasts leads to a better accuracy than building one forecasting model for the total aggregate of all meters. To measure the similarity between time series, we consider wavelet feature extraction and several positive-definite kernels. To forecast the aggregated meter data, we use a periodic autoregressive model with calendar and temperature information as exogenous variable. The data used in the experiments are smart meter recordings from 6,000 residential customers and small-to-medium enterprises collected by the Irish Commission for Energy Regulation (CER). The results show a 20% improvement in forecasting accuracy, where the highest gain is obtained using a kernel with the Spearman's distance. The resulting clusters show distinctive patterns particularly during hours of peak demand.
[disaggregation, wavelet transforms, Predictive models, Reactive power, smart meters, feature extraction, Irish Commission for Energy Regulation, Kernel, Load modeling, smart meter data, positive-definite kernels, Time series analysis, kernel spectral clustering, small-to-medium enterprises, time series, wavelet feature extraction, Forecasting, power engineering computing, periodic autoregressive model, CER, residential customers, pattern clustering, load forecasting, improved electricity load forecasting, Data models, clustering]
Large-Scale Elastic Net Regularized Linear Classification SVMs and Logistic Regression
2013 IEEE 13th International Conference on Data Mining
None
2013
Elastic Net Regularizers have shown much promise in designing sparse classifiers for linear classification. In this work, we propose an alternating optimization approach to solve the dual problems of elastic net regularized linear classification Support Vector Machines (SVMs) and logistic regression (LR). One of the sub-problems turns out to be a simple projection. The other sub-problem can be solved using dual coordinate descent methods developed for non-sparse L2-regularized linear SVMs and LR, without altering their iteration complexity and convergence properties. Experiments on very large datasets indicate that the proposed dual coordinate descent - projection (DCD-P) methods are fast and achieve comparable generalization performance after the first pass through the data, with extremely sparse models.
[large-scale elastic net regularized linear classification, pattern classification, convergence properties, support vector machines, regression analysis, logistics, optimization approach, iteration complexity, Optimization, dual coordinate descent methods, Convergence, dual coordinate descent-projection method, Training, Support vector machines, Accuracy, optimisation, elastic net regularizers, nonsparse L2-regularized linear SVM, Convex functions, logistic regression, gradient methods, Logistics]
Influence-Based Network-Oblivious Community Detection
2013 IEEE 13th International Conference on Data Mining
None
2013
How can we detect communities when the social graphs is not available? We tackle this problem by modeling social contagion from a log of user activity, that is a dataset of tuples (u, i, t) recording the fact that user u "adopted" item i at time t. This is the only input to our problem. We propose a stochastic framework which assumes that item adoptions are governed by un underlying diffusion process over the unobserved social network, and that such diffusion model is based on community-level influence. By fitting the model parameters to the user activity log, we learn the community membership and the level of influence of each user in each community. This allows to identify for each community the "key" users, i.e., the leaders which are most likely to influence the rest of the community to adopt a certain item. The general framework can be instantiated with different diffusion models. In this paper we define two models: the extension to the community level of the classic (discrete time) Independent Cascade model, and a model that focuses on the time delay between adoptions. To the best of our knowledge, this is the first work studying community detection without the network.
[Adaptation models, community-level influence, Social network services, Communities, Stochastic processes, social contagion modelling, community membership learning, classic independent cascade model, item adoptions, Standards, stochastic framework, social network, diffusion model, user activity log, social networking (online), Peer-to-peer computing, Delays, learning (artificial intelligence), stochastic processes, diffusion process, social influence, community detection, influence-based network-oblivious community detection]
Beyond Boolean Matrix Decompositions: Toward Factor Analysis and Dimensionality Reduction of Ordinal Data
2013 IEEE 13th International Conference on Data Mining
None
2013
Boolean matrix factorization (BMF), or decomposition, received a considerable attention in data mining research. In this paper, we argue that research should extend beyond the Boolean case toward more general type of data such as ordinal data. Technically, such extension amounts to replacement of the two-element Boolean algebra utilized in BMF by more general structures, which brings non-trivial challenges. We first present the problem formulation, survey the existing literature, and provide an illustrative example. Second, we present new theorems regarding decompositions of matrices with ordinal data. Third, we propose a new algorithm based on these results along with an experimental evaluation.
[Algorithm design and analysis, ordinal data, Lattices, data mining, two-element Boolean algebra, Educational institutions, data mining research, Vectors, aggregation, Boolean algebra, matrix decomposition, Matrix decomposition, Data mining, data reduction, factor analysis, Boolean matrix factorization, Galois connection, concept lattice, Dogs, BMF, Boolean matrix decompositions, ordinal data dimensionality reduction]
Validating Network Value of Influencers by Means of Explanations
2013 IEEE 13th International Conference on Data Mining
None
2013
Recently, there has been significant interest in social influence analysis. One of the central problems in this area is the problem of identifying influencers, such that by convincing these users to perform a certain action (like buying a new product), a large number of other users get influenced to follow the action. The client of such an application is essentially a marketer who would target these influencers for marketing a given new product, say by providing free samples or discounts. It is natural that before committing resources for targeting an influencer the marketer would be interested in validating the influence (or network value) of influencers returned. This requires digging deeper into such analytical questions as: who are their followers, on what actions (or products) they are influential, etc. However, the current approaches to identifying influencers largely work as a black box in this respect. The goal of this paper is to open up the black box, address these questions and provide informative and crisp explanations for validating the network value of influencers. We formulate the problem of providing explanations (called PROXI) as a discrete optimization problem of feature selection. We show that PROXI is not only NP-hard to solve exactly, it is NP-hard to approximate within any reasonable factor. Nevertheless, we show interesting properties of the objective function and develop an intuitive greedy heuristic. We perform detailed experimental analysis on two real world datasets - Twitter and Flixster, and show that our approach is useful in generating concise and insightful explanations of the influence distribution of users and that our greedy algorithm is effective and efficient with respect to several baselines.
[Algorithm design and analysis, Greedy algorithms, TV, Communities, data mining, social influence analysis, Twitter, optimisation, heuristic programming, supermodular and submodular functions, viral marketing, Motion pictures, explanations, feature selection, objective function, Flixster datasets, network value, greedy algorithms, Blogs, intuitive greedy heuristic, social networks, marketing, PROXI, NP-hard problem, Twitter datasets, social networking (online), black box, discrete optimization problem, influence propagation analysis, computational complexity, influencer identification]
External Evaluation of Topic Models: A Graph Mining Approach
2013 IEEE 13th International Conference on Data Mining
None
2013
Given a topic and its top-k most relevant words generated by a topic model, how can we tell whether it is a low-quality or a high-quality topic? Topic models provide a low-dimensional representation of large document corpora, and drive many important applications such as summarization, document segmentation, word-sense disambiguation, etc. Evaluation of topic models is an important issue, since low-quality topics potentially degrade the performance of these applications. In this paper, we develop a graph mining and machine learning approach for the external evaluation of topic models. Based on the graph-centric features we extract from the projection of topic words on the Wikipedia page-links graph, we learn models that can predict the human-perceived quality of topics (based on human judgments), and classify them as high or low quality. Experiments on four real-world corpora show that our approach boosts the prediction performance up to 30% over three baselines of various complexities, and demonstrate the generality of our method to diverse domains. In addition, we provide an interpretation of our models and outline the discriminating characteristics of topic quality.
[Electronic publishing, text analysis, summarization, human-perceived quality, graph theory, data mining, Encyclopedias, Predictive models, topic quality, low-dimensional representation, feature extraction, Wikipedia page-links graph, word sense disambiguation, learning (artificial intelligence), top-k most relevant words, pattern classification, machine learning approach, graph-centric feature extraction, natural language processing, topic model, high-quality topic, topic word projection, graph mining, Digital signal processing, document segmentation, Feature extraction, topic models, human evaluation, Internet, large document corpora, Web sites, low-quality topic]
Multimedia LEGO: Learning Structured Model by Probabilistic Logic Ontology Tree
2013 IEEE 13th International Conference on Data Mining
None
2013
Recent advances in Multimedia research have generated a large collection of concept models, e.g., LSCOM and Media mill 101, which become accessible to other researchers. While most current research effort still focuses on building new concepts from scratch, little effort has been made on constructing new concepts upon the existing models already in the warehouse. To address this issue, we develop a new framework in this paper, termed LEGO, to seamlessly integrate both the new target training examples and the existing primitive concept models. LEGO treats the primitive concept models as a lego toy to potentially construct an unlimited vocabulary of new concepts. Specifically, LEGO first formulates the logic operations to be the lego connectors to combine existing concept models hierarchically in probabilistic logic ontology trees. LEGO then simultaneously incorporates new target training information to efficiently disambiguate the underlying logic tree and correct the error propagation. We present extensive experimental results on a large vehicle domain data set from Image Net, and demonstrate significantly superior performance over existing state-of-the-art approaches which build new concept models from scratch.
[logic operations, probabilistic logic, Ontologies, multimedia systems, Concept recycling, Multimedia communication, lego connectors, Probabilistic logic ontology tree, Training, Logical operations, Semantics, unlimited vocabulary, large vehicle domain data set, Mathematical model, lego toy, Computational modeling, trees (mathematics), multimedia LEGO, Probabilistic logic, Model warehouse, primitive concept models, structured model learning, Multimedia LEGO, probabilistic logic ontology tree, ontologies (artificial intelligence), ImageNet]
Discriminatively Enhanced Topic Models
2013 IEEE 13th International Conference on Data Mining
None
2013
This paper proposes a space-efficient, discriminatively enhanced topic model: a V structured topic model with an embedded log-linear component. The discriminative log-linear component reduces the number of parameters to be learnt while outperforming baseline generative models. At the same time, the explanatory power of the generative component is not compromised. We establish its superiority over a purely generative model by applying it to two different ranking tasks: (a) In the first task, we look at the problem of proposing alternative citations given textual and bibliographic evidence. We solve it as a ranking problem in itself and as a platform for further qualitative analysis of convergence of scientific phenomenon. (b) In the second task we address the problem of ranking potential email recipients based on email content and sender information.
[discriminatively enhanced topic models, alternative citations, bibliography recommendation, bibliographic evidence, Predictive models, Topic Models, Vectors, Electronic mail, Text Mining, Probabilistic ranking, textual evidence, recommender systems, embedded log-linear component, Hidden Markov models, ranking problem, citation recommendation, citation analysis, Hybrid power systems, Mathematical model, V structured topic model, Context modeling, Log linear models]
PerturBoost: Practical Confidential Classifier Learning in the Cloud
2013 IEEE 13th International Conference on Data Mining
None
2013
Mining large data requires intensive computing resources and data mining expertise, which might not be available for many users. With the development of cloud computing and services computing, data mining tasks can now be moved to the cloud or outsourced to third parties to save costs. In this new paradigm, data and model confidentiality becomes the major concern to the data owner. Meanwhile, users are also concerned about the potential tradeoff among costs, model quality, and confidentiality. In this paper, we propose the PerturBoost framework to address the problems in confidential cloud or outsourced learning. PerturBoost combined with the random space perturbation (RASP) method that was also developed by us can effectively protect data confidentiality, model confidentiality, and model quality with low client-side costs. Based on the boosting framework, we develop a number of base learner algorithms that can learn linear classifiers from the RASP-perturbed data. This approach has been evaluated with public datasets. The result shows that the RASP-based PerturBoost can provide model accuracy very close to the classifiers trained with the original data and the AdaBoost method, with high confidentiality guarantee and acceptable costs.
[PerturBoost framework, model confidentiality, Cloud computing, AdaBoost method, data mining, costing, Data mining, Servers, Accuracy, practical confidential classifier learning, cloud computing, learning (artificial intelligence), boosting framework, services computing, pattern classification, cost, Computational modeling, confidential cloud, outsourced learning, RASP-perturbed data, Vectors, data confidentiality, random space perturbation method, security of data, base learner algorithm, RASP method, Data models, model quality, linear classifier learning]
A Feature-Enhanced Ranking-Based Classifier for Multimodal Data and Heterogeneous Information Networks
2013 IEEE 13th International Conference on Data Mining
None
2013
We propose a heterogeneous information network mining algorithm: feature-enhanced Rank Class (F-Rank Class). F-Rank Class extends Rank Class to a unified classification framework that can be applied to binary or multiclass classification of unimodal or multimodal data. We experimented on a multimodal document dataset, 2008/9 Wikipedia Selection for Schools. For unimodal classification, F-Rank Class is compared to support vector machines (SVMs). F-Rank Class provides improvements up to 27.3% on the Wikipedia dataset. For multimodal document classification, F-Rank Class shows improvements up to 19.7% in accuracy when compared to SVM-based meta-classifiers. We also study 1) how the structure of the network and 2) how the choice of parameters affect the classification results.
[heterogeneous information network, data mining, Encyclopedias, unimodal data, multimodal data, Data mining, multimodal document dataset, Accuracy, F-RankClass, multiclass classification, heterogeneous information network mining algorithm, multimodal document classification, binary classification, Wikipedia dataset, document handling, pattern classification, Image edge detection, Educational institutions, unified classification framework, multimodal, feature-enhanced ranking-based classifier, classification, Support vector machines, Feature extraction, ranking, Web sites]
Nonlinear Causal Discovery for High Dimensional Data: A Kernelized Trace Method
2013 IEEE 13th International Conference on Data Mining
None
2013
Causal discovery for high-dimensional observations is a useful tool in many fields such as climate analysis and financial market analysis. A linear Trace method has been proposed to identify the causal direction between two linearly coupled high-dimensional observations X and Y. However, in reality, the relations between X and Y are usually nonlinear and consequently the linear Trace method may fail. In this paper, we propose a method to infer the nonlinear causal relations for two high-dimensional observations X and Y. The idea is to map the observations to high dimensional Reproducing Kernel Hilbert Space (RKHS) such that the nonlinear relations become simple linear ones. We show that the linear Trace condition holds for the causal direction but it is violated for the anti-causal direction in RKHS. Based on this theoretical result, we develop a simple algorithm to infer the causal direction for nonlinearly coupled causal pairs. Synthetic data and real world data experiments are conducted to show the effectiveness of our proposed method.
[RKHS, nonlinearly coupled causal pairs, anticausal direction, Hilbert spaces, Electronic mail, high dimensional reproducing kernel Hilbert space, Covariance matrices, linear Trace method, Accuracy, high dimensional data, synthetic data, kernel methods, Eigenvalues and eigenfunctions, Hilbert space, data handling, nonlinear causal discovery, Kernel, kernelized trace method, Meteorology, linearly coupled high-dimensional observations, real world data experiments]
Discriminative Link Prediction Using Local Links, Node Features and Community Structure
2013 IEEE 13th International Conference on Data Mining
None
2013
A link prediction (LP) algorithm is given a graph, and has to rank, for each node, other nodes that are candidates for new linkage. LP is strongly motivated by social search and recommendation applications. LP techniques often focus on global properties (graph conductance, hitting or commute times, Katz score) or local properties (Adamic-Adar and many variations, or node feature vectors), but rarely combine these signals. Furthermore, neither of these extremes exploit link densities at the intermediate level of communities. In this paper we describe a discriminative LP algorithm that exploits two new signals. First, a co-clustering algorithm provides community level link density estimates, which are used to qualify observed links with a surprise value. Second, links in the immediate neighborhood of the link to be predicted are interpreted %at face value, but through a local model of node feature similarities. These signals are combined into a discriminative link predictor. We evaluate the new predictor using five diverse data sets that are standard in the literature. We report on significant accuracy boosts compared to standard LP methods (including Adamic-Adar and random walk). Apart from the new predictor, another contribution is a rigorous protocol for benchmarking and reporting LP algorithms, which reveals the regions of strengths and weaknesses of all the predictors studied here, and establishes the new proposal as the most robust.
[Link prediction, Social network services, Communities, node features, discriminative link prediction, Social network, Vectors, Recommendation, community level link density estimates, Couplings, Accuracy, recommender systems, community structure, Prediction algorithms, Motion pictures, social networking (online), discriminative LP algorithm, coclustering algorithm, local links]
Most Clusters Can Be Retrieved with Short Disjunctive Queries
2013 IEEE 13th International Conference on Data Mining
None
2013
Simple keyword based searches are ubiquitous in today's internet age. It is hard to imagine an information system today that does not permit a simple keyword based search. This method of information retrieval has the obvious benefits of being highly interpretable, and having wide usage. However, a general perception is that keyword search may not be as powerful an information retrieval paradigm as those that utilize data mining technologies. At the same time, the tremendous growth in textual information in various domains has also given impetus to data mining technologies such as document clustering. Document clustering is a powerful technique, having wide applications in enterprise information management (EIM). However, there is a general perception that the clusters it produces are not always easily interpretable. This hampers its usage in certain settings. This leads us to the following question: can we retrieve a cluster (from a corpus) using a keyword search with precision and recall that are reasonable from the point of view of a retrieval system? What is the form of such a keyword search? How many keywords do we require? How do we arrive at these keywords? Not only are these questions natural, they have immediate use in several highly regulated applications in EIM such as eDiscovery and compliance, where document sets must be specified using keywords. In order to answer our question, we construct a framework that uses maximal frequent discriminative item sets. The novelty of our usage of these item sets is that although their definition as frequent item sets is conjunctive, we use them to form a disjunctive query upon the corpus. We then study the results of this query as an information retrieval problem whose target is the cluster. Our study yields a surprising result: most clusters can be retrieved, up to reasonable precision and recall, using a disjunctive query of only three terms. Among other ramifications, this gives us a readily interpretable description of a cluster in terms of the disjunctive query that returns it.
[Retrieval, text analysis, textual information, short disjunctive query, document clustering, Data mining, query processing, Itemsets, Clustering algorithms, information systems, information retrieval method, data mining technology, enterprise information management, Document Clustering, information system, Keyword Search, e-discovery, Disjunctive Queries, Keyword search, EIM, Standards, maximal frequent discriminative item sets, pattern clustering, compliance, Organizations, Internet, keyword based searches, Frequent Itemsets, document sets]
Integrity Verification of Outsourced Frequent Itemset Mining with Deterministic Guarantee
2013 IEEE 13th International Conference on Data Mining
None
2013
In this paper, we focus on the problem of result integrity verification for outsourcing of frequent item set mining. We design efficient cryptographic approaches that verify whether the returned frequent item set mining results are correct and complete with deterministic guarantee. The key of our solution is that the service provider constructs cryptographic proofs of the mining results. Both correctness and completeness of the mining results are measured against the proofs. We optimize the verification by minimizing the number of proofs. Our empirical study demonstrates the efficiency and effectiveness of the verification approaches.
[Cloud computing, Protocols, program verification, deterministic guarantee, service provider, data mining, Data-mining-as-a-service, cryptography, Complexity theory, Servers, Data mining, cryptographic proofs, frequent itemset mining, Optimization, Itemsets, outsourced frequent item set mining, Cryptography, integrity verification, integrity verification approach]
Progression Analysis of Community Strengths in Dynamic Networks
2013 IEEE 13th International Conference on Data Mining
None
2013
Community formation analysis of dynamic networks has been a hot topic in data mining which has attracted much attention. Recently, there are many studies which focus on discovering communities successively from each snapshot by considering both current and historical information. However, the detected communities are isolated at a certain snapshot, because these approaches ignore important historical or successive information. Different from previous studies which focus on community detection in dynamic networks, we define a new problem of tracking the progression of the community strength - a novel measure that reflects the community robustness and coherence throughout the entire observation period. The proposed community strength analysis provides significant insights into entity properties and relationships in a wide variety of applications. To tackle this problem, we propose a novel two-stage framework: we first identify communities via non-negative matrix factorization, and then calculate the strength of each detected community corresponding to each specific snapshot by solving an optimization problem. Experimental results show that the proposed approach is highly effective in discovering the progression of community strengths and detecting interesting communities.
[Symmetric matrices, Communities, data mining, Linear programming, Biology, Entropy, matrix decomposition, nonnegative matrix factorization, optimisation, community strengths, temporal community analysis, dynamic networks, Picture archiving and communication systems, community strength analysis, Joining processes, progression analysis]
Walk 'n' Merge: A Scalable Algorithm for Boolean Tensor Factorization
2013 IEEE 13th International Conference on Data Mining
None
2013
Tensors are becoming increasingly common in data mining, and consequently, tensor factorizations are becoming more important tools for data miners. When the data is binary, it is natural to ask if we can factorize it into binary factors while simultaneously making sure that the reconstructed tensor is still binary. Such factorizations, called Boolean tensor factorizations, can provide improved interpretability and find Boolean structure that is hard to express using normal factorizations. Unfortunately the algorithms for computing Boolean tensor factorizations do not usually scale well. In this paper we present a novel algorithm for finding Boolean CP and Tucker decompositions of large and sparse binary tensors. In our experimental evaluation we show that our algorithm can handle large tensors and accurately reconstructs the latent Boolean structure.
[Additive noise, Merging, data mining, scalable algorithm, latent Boolean structure, Encoding, tensors, Boolean algebra, matrix decomposition, Boolean tensors, Matrix decomposition, Tensor factorizations, sparse binary tensors, Tensile stress, Boolean tensor factorization, Random walks, reconstructed tensor, normal factorizations, MDL principle, Facebook]
Online Active Learning with Imbalanced Classes
2013 IEEE 13th International Conference on Data Mining
None
2013
This paper proposes an online algorithm for active learning that switches between different candidate instance selection strategies (ISS) for classification in imbalanced data sets. This is important for two reasons: 1) many real-world problems have imbalanced class distributions and 2) there is no ISS that always outperforms all the other techniques. We first empirically compare the performance of existing techniques on imbalanced data sets and show that different strategies work better on different data sets and some techniques even hurt compared to random selection. We then propose an unsupervised score to track and predict the performance of individual instance selection techniques, allowing us to select an effective technique without using a holdout set and wasting valuable labeled data. This score is used in a simple online learning approach that switches between different ISS at each iteration. The proposed approach performs better than the best individual strategy available to the online algorithm over data sets in this paper and provides a way to build practical and effective active learning system for imbalanced data sets.
[Measurement, Uncertainty, Correlation, data analysis, ISS, Educational institutions, instance selection strategies, imbalanced data sets classification, random selection, Training, Learning systems, unsupervised score, Prediction algorithms, learning (artificial intelligence), online active learning]
Efficient Invariant Search for Distributed Information Systems
2013 IEEE 13th International Conference on Data Mining
None
2013
In today's distributed information systems, a large amount of monitoring data such as log files have been collected. These monitoring data at various points of a distributed information system provide unparallel opportunities for us to characterize and track the information system via effectively correlating all monitoring data across the distributed system. Jiang1 proposed a concept named flow intensity to measure the intensity with which the monitoring data reacts to the volume of different user requests. The Autoregressive model with exogenous inputs (ARX) was used to quantify the relationship between each pair of flow intensity measured at various points across distributed systems. If such relationships hold all the time, they are considered as invariants of the underlying systems. Such invariants have been successfully used to characterize complex systems and support various system management tasks, such as system fault detection and localization. However, it is very time-consuming to search the complete set of invariants of large scale systems and existing algorithms are not scalable for thousands of flow intensity measurements. To this end, in this paper, we develop effective pruning techniques based on the identified upper bounds. Accordingly, two efficient algorithms are proposed to search the complete set of invariants based on the pruning techniques. Finally we demonstrate the efficiency and effectiveness of our algorithms with both real-world and synthetic data sets.
[Invariant, ARX Model, Distributed information systems, ARX, distributed processing, AutoRegressive Model, autoregressive model with exogenous inputs, information system tracking, information systems, Mathematical model, distributed information systems, Monitoring, flow intensity measurements, invariant search, Computational modeling, synthetic data set, Time series analysis, real-world data set, autoregressive processes, Equations, Efficient Search, pruning technique, Upper bound, monitoring data, information system characterization]
Coupled Heterogeneous Association Rule Mining (CHARM): Application Toward Inference of Modulatory Climate Relationships
2013 IEEE 13th International Conference on Data Mining
None
2013
The complex dynamic climate system often exhibits hierarchical modularity of its organization and function. Scientists have spent decades trying to discover and understand the driving mechanisms behind western African Sahel summer rainfall variability, mostly via hypothesis-driven and/or first-principles based research. Their work has furthered theory regarding the connections between various climate patterns, but the key relationships are still not fully understood. We present Coupled Heterogeneous Association Rule Mining (CHARM), a computationally efficient methodology that mines higher-order relationships between these subsystems' anomalous temporal phases with respect to their effect on the system's response. We apply this to climate science data, aiming to infer putative pathways/cascades of modulating events and the modulating signs that collectively define the network of pathways for the rainfall anomaly in the Sahel. Experimental results are consistent with fundamental theories of phenomena in climate science, especially physical processes that best describe sub-regional climate.
[Measurement, Oceans, data mining, CHARM, climate science data, geophysics computing, association rules, knowledge discovery, climate, Data mining, Couplings, modulatory climate relationships inference, Itemsets, climatology, higher-order relationships mining, coupled heterogeneous association rule mining, Meteorology, Manganese, rainfall anomaly, data coupling]
Leveraging Supervised Label Dependency Propagation for Multi-label Learning
2013 IEEE 13th International Conference on Data Mining
None
2013
Exploiting label dependency is a key challenge in multi-label learning, and current methods solve this problem mainly by training models on the combination of related labels and original features. However, label dependency cannot be exploited dynamically and mutually in this way. Therefore, we propose a novel paradigm of leveraging label dependency in an iterative way. Specifically, each label's prediction will be updated and also propagated to other labels via an random walk with restart process. Meanwhile, the label propagation is implemented as a supervised learning procedure via optimizing a loss function, thus more appropriate label dependency can be learned. Extensive experiments are conducted, and the results demonstrate that our method can achieve considerable improvements in terms of several evaluation metrics.
[Measurement, evaluation metrics, Predictive models, Educational institutions, loss function optimization, Vectors, multilabel learning, Training, random walk, supervised label dependency propagation, Random walk with restart, Label dependency, Multi-label learning, Feature extraction, Prediction algorithms, supervised learning procedure, learning (artificial intelligence), restart process]
A Mobility Simulation Framework Of Humans With Group Behavior Modeling
2013 IEEE 13th International Conference on Data Mining
None
2013
We present a mobility simulation framework that simulates the movement behaviors of people to generate spatiotemporal movement data. There is a growing interest in applications that make use of patterns mined from spatio-temporal data. However, since the availability of actual spatio-temporal movement data in the public domain is limited, it is useful to have simulation frameworks that generate data close to the real life behavior of people, so that data mining techniques can be tested. We argue that modeling group behavior effectively is a key element of any real-life simulation framework, because there are many applications that require the knowledge of groups and events. In this work, we propose generic models to represent individual and group movement behaviors. We present an algorithm that takes various behaviors created using the proposed models, and generates spatio-temporal movement data for as many individuals as needed. Experimental analysis shows the efficacy of the proposed framework handling a broad spectrum of behaviors with high scalability.
[mobility simulation, group movement behaviors, Computational modeling, Roads, spatiotemporal movement data, data mining, Educational institutions, real-life simulation framework, Probability distribution, Electronic mail, Data mining, group behavior, movement behavior simulation, mobility simulation framework, data mining techniques, Data models, behavioural sciences computing, group behavior modeling, spatio-temporal]
Spatio-Temporal Topic Modeling in Mobile Social Media for Location Recommendation
2013 IEEE 13th International Conference on Data Mining
None
2013
Mobile networks enable users to post on social media services (e.g., Twitter) from anywhere and anytime. This new phenomenon led to the emergence of a new line of work of mining the behavior of mobile users taking into account the spatio-temporal aspects of their engagement with online social media. In this paper, we address the problem of recommending the right locations to users at the right time. We claim to propose the first comprehensive model, called STT (Spatio-Temporal Topic), to capture the spatio-temporal aspects of user check-ins in a single probabilistic model for location recommendation. Our proposed generative model does not only captures spatio-temporal aspects of check-ins, but also profiles users. We conduct experiments on real life data sets from Twitter, Go Walla, and Bright kite. We evaluate the effectiveness of STT by evaluating the accuracy of location recommendation. The experimental results show that STT achieves better performance than the state-of-the-art models in the areas of recommender systems as well as topic modeling.
[mobile user behavior mining, topic model, data mining, Go Walla, Twitter, mobile social media, Entropy, Indexes, Bright kite, Accuracy, mobile computing, recommender systems, STT, user data check-ins, single probabilistic model, Cities and towns, social networking (online), spatio-temporal topic modeling, location recommendation, Data models, Random variables, spatio-temporal]
Active Query Driven by Uncertainty and Diversity for Incremental Multi-label Learning
2013 IEEE 13th International Conference on Data Mining
None
2013
In multi-label learning, it is rather expensive to label instances since they are simultaneously associated with multiple labels. Therefore, active learning, which reduces the labeling cost by actively querying the labels of the most valuable data, becomes particularly important for multi-label learning. A strong multi-label active learning algorithm usually consists of two crucial elements: a reasonable criterion to evaluate the gain of queried label, and an effective classification model, based on whose prediction the criterion can be accurately computed. In this paper, we first introduce an effective multi-label classification model by combining label ranking with threshold learning, which is incrementally trained to avoid retraining from scratch after every query. Based on this model, we then propose to exploit both uncertainty and diversity in the instance space as well as the label space, and actively query the instance-label pairs which can improve the classification model most. Experimental results demonstrate the superiority of the proposed approach to state-of-the-art methods.
[labeling cost reduction, Uncertainty, instance space, label space, reasonable criterion, uncertainty, query processing, diversity, active learning, Prediction algorithms, learning (artificial intelligence), multi-label learning, threshold learning, pattern classification, Computational modeling, multilabel active learning algorithm, label ranking, incremental multilabel learning, Vectors, label instances, active query, classification model, instance-label pairs, Measurement uncertainty, Diversity reception, Data models]
Accelerating Active Learning with Transfer Learning
2013 IEEE 13th International Conference on Data Mining
None
2013
Active learning, transfer learning, and related techniques are unified by a core theme: efficient and effective use of available data. Active learning offers scalable solutions for building effective supervised learning models while minimizing annotation effort. Transfer learning utilizes existing labeled data from one task to help learning related tasks for which limited labeled data are available. There has been limited research, however, on how to combine these two techniques. In this paper, we present a simple and principled transfer active learning framework that leverages pre-existing labeled data from related tasks to improve the performance of an active learner. We derive an intuitive bound on generalization error for the classifiers learned by this algorithm that provides insight into the algorithm's behavior and the problem in general. Experimental results using several well-known transfer learning data sets confirm our theoretical analysis and demonstrate the effectiveness of our approach.
[Algorithm design and analysis, pattern classification, transfer learning, Transfer Learning, supervised learning models, Machine Learning, Active Learning, Training, Upper bound, active learning, Query processing, Supervised learning, labeled data, Learning Theory, classifiers generalization error, Labeling, Acceleration, learning (artificial intelligence)]
Statistical Inference of Protein "LEGO Bricks"
2013 IEEE 13th International Conference on Data Mining
None
2013
Proteins are biomolecules of life. They fold into a great variety of three-dimensional (3D) shapes. Underlying these folding patterns are many recurrent structural fragments or building blocks (analogous to 'LEGO® bricks'). This paper reports an innovative statistical inference approach to discover a comprehensive dictionary of protein structural building blocks from a large corpus of experimentally determined protein structures. Our approach is built on the Bayesian and information theoretic criterion of minimum message length. To the best of our knowledge, this work is the first systematic and rigorous treatment of a very important data mining problem that arises in the cross-disciplinary area of structural bioinformatics. The quality of the dictionary we find is demonstrated by its explanatory power - any protein within the corpus of known 3D structures can be dissected into successive regions assigned to fragments from this dictionary. This induces a novel one-dimensional representation of three-dimensional protein folding patterns, suitable for application of the rich repertoire of character-string processing algorithms, for rapid identification of folding patterns of newly determined structures. This paper presents the details of the methodology used to infer the dictionary of building blocks, and is supported by illustrative examples to demonstrate its effectiveness and utility.
[Dictionaries, data mining, 3D structures, one-dimensional representation, Amino acids, Proteins, protein structure, Three-dimensional displays, three-dimensional shape, proteins, 3D shape, protein structural building block dictionary, information theoretic criterion, information theory, structural bioinformatics, information theoretic inference, data mining problem, recurrent structural fragments, character-string processing algorithm, minimum message length, three-dimensional protein folding patterns, Encoding, Bayesian criterion, biomolecules, bioinformatics, protein LEGO bricks, statistical inference, Bayes methods, statistical analysis, dictionaries]
A Probabilistic Behavior Model for Discovering Unrecognized Knowledge
2013 IEEE 13th International Conference on Data Mining
None
2013
Discovering interesting behavior patterns and profiles of users as they interact with E-commerce (EC) sites is an important task for site managers. We propose a probabilistic behavior model for extracting latent classes of items that impact the users' item selections but cannot be inferred from the current knowledge of the managers. The proposed model assumes that the current knowledge is represented by categories of items that are defined in the EC site, and a user selects items depending on both of their categories and latent classes. By estimating latent classes, each of which shows items accessed by users with common interests, we can find interesting factors for explaining user behavior. We evaluate our proposed model using item-access log data observed in an EC site. The results show that our model can accurately predict users' item selection, and actually discover latent classes of items having similar latent characteristic such as "colored design" and "impression" by using item categories such as "coat" and "hat" as the current knowledge of the managers.
[probabilistic behavior model, user profile, topic model, item latent class extraction, data mining, probability, Footwear, Predictive models, Probabilistic logic, Entropy, Data mining, behavior pattern discovery, unrecognized knowledge discovery, user item selection, EC sites, Data models, e-commerce sites, behavir model, item-access log data, Kernel, user behavior, electronic commerce]
Prominent Features of Rumor Propagation in Online Social Media
2013 IEEE 13th International Conference on Data Mining
None
2013
The problem of identifying rumors is of practical importance especially in online social networks, since information can diffuse more rapidly and widely than the offline counterpart. In this paper, we identify characteristics of rumors by examining the following three aspects of diffusion: temporal, structural, and linguistic. For the temporal characteristics, we propose a new periodic time series model that considers daily and external shock cycles, where the model demonstrates that rumor likely have fluctuations over time. We also identify key structural and linguistic differences in the spread of rumors and non-rumors. Our selected features classify rumors with high precision and recall in the range of 87% to 92%, that is higher than other states of the arts on rumor classification.
[Adaptation models, Electric shock, Time series analysis, Social Media, Psychology, online social networks, linguistic differences, Twitter, time series, Diffusion Network, rumor propagation, Sentiment Analysis, periodic time series model, temporal characteristics, Pragmatics, external shock cycles, daily shock cycles, Rumor, structural differences, Time Series, social networking (online), rumor classification, Mathematical model, online social media]
Group Feature Selection with Streaming Features
2013 IEEE 13th International Conference on Data Mining
None
2013
Group feature selection makes use of structural information among features to discover a meaningful subset of features. Existing group feature selection algorithms only deal with pre-given candidate feature sets and they are incapable of handling streaming features. On the other hand, feature selection algorithms targeted for streaming features can only perform at the individual feature level without considering intrinsic group structures of the features. In this paper, we perform group feature selection with streaming features. We propose to perform feature selection at the group and individual feature levels simultaneously in a manner of a feature stream rather than a pre-given candidate feature set. In our approach, the group structures are fully utilized to reduce the cost of evaluating streaming features. We have extensively evaluated the proposed method. Experimental results have demonstrated that our proposed algorithms statistically outperform state-of-the-art methods of feature selection in terms of classification accuracy.
[pattern classification, Uncertainty, Redundancy, group structures, streaming feature evaluation, Entropy, Standards, GFSSF, Training, Accuracy, streaming feature handling, group feature selection with streaming features, structural information, intrinsic group structures, statistical analysis, Mutual information, feature selection]
Bayesian Multi-Task Relationship Learning with Link Structure
2013 IEEE 13th International Conference on Data Mining
None
2013
In this paper, we study the multi-task learning problem with a new perspective of considering the link structure of data and task relationship modeling simultaneously. In particular, we first introduce the Matrix Generalized Inverse Gaussian (MGIG) distribution and define a Matrix Gaussian Matrix Generalized Inverse Gaussian (MG-MGIG) prior. Based on this prior, we propose a novel multi-task learning algorithm, the Bayesian Multi-task Relationship Learning (BMTRL) algorithm. To incorporate the link structure into the framework of BMTRL, we propose link constraints between samples. Through combining the BMTRL algorithm with the link constraints, we propose the Bayesian Multi-task Relationship Learning with Link Constraints (BMTRL-LC) algorithm. To make the computation tractable, we simultaneously use a convex optimization method and sampling techniques. In particular, we adopt two stochastic EM algorithms for BMTRL and BMTRL-LC, respectively. The experimental results on Cora dataset demonstrate the promise of the proposed algorithms.
[Algorithm design and analysis, Machine learning algorithms, task relationship modeling, data mining, data link structure, Multi-task Learning, Gaussian distribution, Covariance matrices, MG-MGIG, Optimization, sampling techniques, inverse problems, data structures, learning (artificial intelligence), Kernel, multitask learning problem, BMTRL-LC algorithm, sampling methods, stochastic EM algorithms, Cora dataset, matrix Gaussian matrix generalized inverse Gaussian, convex programming, Bayesian multitask relationship learning with link constraints, matrix algebra, convex optimization method, Task Relationship Modeling, Link Structure, Bayes methods]
Quantitative Prediction of Glaucomatous Visual Field Loss from Few Measurements
2013 IEEE 13th International Conference on Data Mining
None
2013
We propose database-aware regression methods for extrapolation from few measurements in the context of quantitative prognosis. The idea is to leverage a database of patients with similar conditions to increase the effective number of samples when we train a predictive model. Applying the proposed method to a database of glaucoma patients, we were able to predict the disease condition at a future time point significantly more accurately than the conventional patient-wise linear regression approach. In fact, our prediction was 50% more accurate than the conventional approach when three or less measurements were available and with only two measurements at least as accurate as the conventional approach with six measurements. Moreover, the proposed method can provide spatially localized prediction and also the (localized) speed of progression, which are valuable for doctors in making decisions.
[glaucoma patients database, Visualization, Quantitative prognosis, Linear regression, regression analysis, Predictive models, quantitative prognosis, diseases, Spatio-temporal data, Vectors, Clustering, Multi-task learning, Diseases, database-aware regression method, Extrapolation, extrapolation, patient treatment, decision making, glaucomatous visual field loss quantitative prediction, Principal component analysis]
On the Feature Discovery for App Usage Prediction in Smartphones
2013 IEEE 13th International Conference on Data Mining
None
2013
With the increasing number of mobile Apps developed, they are now closely integrated into daily life. In this paper, we develop a framework to predict mobile Apps that are most likely to be used regarding the current device status of a smartphone. Such an Apps usage prediction framework is a crucial prerequisite for fast App launching, intelligent user experience, and power management of smartphones. By analyzing real App usage log data, we discover two kinds of features: The Explicit Feature (EF) from sensing readings of built-in sensors, and the Implicit Feature (IF) from App usage relations. The IF feature is derived by constructing the proposed App Usage Graph (abbreviated as AUG) that models App usage transitions. In light of AUG, we are able to discover usage relations among Apps. Since users may have different usage behaviors on their smartphones, we further propose one personalized feature selection algorithm. We explore minimum description length (MDL) from the training data and select those features which need less length to describe the training data. The personalized feature selection can successfully reduce the log size and the prediction time. Finally, we adopt the kNN classification model to predict Apps usage. Note that through the features selected by the proposed personalized feature selection algorithm, we only need to keep these features, which in turn reduces the prediction time and avoids the curse of dimensionality when using the kNN classifier. The results based on a real dataset demonstrate the effectiveness of the proposed framework and show the predictive capability for App usage prediction.
[graph theory, App usage log data analysis, fast App launching, prediction time reduction, feature discovery, Training, App usage graph, mobile computing, power aware computing, AUG, intelligent user experience, MDL, Training data, training data, Prediction algorithms, smartphones, Sensors, minimum description length, IF feature, feature selection, Testing, log size reduction, pattern classification, kNN classification model, curse of dimensionality, explicit feature, mobile App usage prediction framework, smart phones, Usage prediction, built-in sensors, App usage transitions, classification, Equations, Mobile Application, mobile Apps, personalized feature selection algorithm, kNN classifier, implicit feature, Smart phones]
How Many Zombies Around You?
2013 IEEE 13th International Conference on Data Mining
None
2013
Recent years have witnessed the explosive growth of online social media. Weibo, a famous "Chinese Twitter\
[bogus users, Social Media, data mining, Predictive models, predicting process, ProZombie, active common users, Read only memory, Training, Chinese Twitter, Accuracy, Weibo user panoramic description, labeled Weibo users, Economics, training set labeling, intelligent software, Computational modeling, Zombie, Cascading Model, Media, training process, two-stage cascading model, Weibo, zombie user recognition, user profiling, social networking (online), online social media]
Hibernating Process: Modelling Mobile Calls at Multiple Scales
2013 IEEE 13th International Conference on Data Mining
None
2013
Do mobile phone calls at larger granularities behave in the same pattern as in smaller ones? How can we forecast the distribution of a whole month's phone calls with only one day's observation? There are many models developed to interpret large scale social graphs. However, all of the existing models focus on graph at one time scale. Many dynamical behaviors were either ignored, or handled at one scale. In particular new users might join or current users quit social networks at any time. In this paper, we propose HiP, a novel model to capture longitudinal behaviors in modeling degree distribution of evolving social graphs. We analyze a large scale phone call dataset using HiP, and compare with several previous models in literature. Our model is able to fit phone call distribution at multiple scales with 30% to 75% improvement over the best existing method on each scale.
[hibernating process, Social network services, graph theory, social networks, social graphs, Mobile communication, churning behavior, Mobile handsets, Parametric statistics, Hip, large scale social graphs, mobile computing, large scale phone call dataset, Mobile phone call graph, Data models, social sciences computing, heavy tailed distribution, non-parametric model, Mobile computing]
On Good and Fair Paper-Reviewer Assignment
2013 IEEE 13th International Conference on Data Mining
None
2013
Peer review has become the most common practice for judging papers submitted to a conference for decades. An extremely important task involved in peer review is to assign submitted papers to reviewers with appropriate expertise which is referred to as paper-reviewer assignment. In this paper, we study the paper-reviewer assignment problem from both the goodness aspect and the fairness aspect. For the goodness aspect, we propose to maximize the topic coverage of the paper-reviewer assignment. This objective is new and the problem based on this objective is shown to be NP-hard. To solve this problem efficiently, we design an approximate algorithm which gives a 1/3-approximation. For the fairness aspect, we perform a detailed study on conflict-of-interest (COI) types and discuss several issues related to using COI, which, we hope, can raise some open discussions among researchers on the COI study. Finally, we conducted experiments on real datasets which verified the effectiveness of our algorithm and also revealed some interesting results of COI.
[Greedy algorithms, Algorithm design and analysis, text analysis, conflict-of-interest types, query, topic coverage, Linear programming, Approximation methods, Data mining, 1/3-approximation, query processing, optimisation, NP-hard problem, fair paper-reviewer assignment paper, text document, COI study, COI types, peer review, approximate algorithm, Paper reviewer assignment, Approximation algorithms, Bipartite graph, good paper-reviewer assignment, computational complexity]
Community Detection in Networks with Node Attributes
2013 IEEE 13th International Conference on Data Mining
None
2013
Community detection algorithms are fundamental tools that allow us to uncover organizational principles in networks. When detecting communities, there are two possible sources of information one can use: the network structure, and the features and attributes of nodes. Even though communities form around nodes that have common edges and common attributes, typically, algorithms have only focused on one of these two data modalities: community detection algorithms traditionally focus only on the network structure, while clustering algorithms mostly consider only node attributes. In this paper, we develop Communities from Edge Structure and Node Attributes (CESNA), an accurate and scalable algorithm for detecting overlapping communities in networks with node attributes. CESNA statistically models the interaction between the network structure and the node attributes, which leads to more accurate community detection as well as improved robustness in the presence of noise in the network structure. CESNA has a linear runtime in the network size and is able to process networks an order of magnitude larger than comparable approaches. Last, CESNA also helps with the interpretation of detected communities by finding relevant node attributes for each community.
[Electronic publishing, Network communities, Image edge detection, Communities, data modalities, overlapping community detection, community detection algorithms, Encyclopedias, network theory (graphs), Community detection, organizational principles, network size, Overlapping community detection, pattern clustering, CESNA, clustering algorithms, community from edge structure and node attributes, Node attributes, social networking (online), statistical modelling, statistical analysis, Facebook, network structure, Logistics]
Multitask Learning with Feature Selection for Groups of Related Tasks
2013 IEEE 13th International Conference on Data Mining
None
2013
Multitask learning has been thoroughly proven to improve the generalization performance given a set of related tasks. Most multitask learning algorithm assume that all tasks are related. However, if all the tasks are not related, negative transfer of information occurs amongst the tasks, and the performance of traditional multitask learning algorithm worsens. Thus, we design an algorithm that simultaneously groups the related tasks and trains only the related task together. There are different approaches to train the related tasks in multi-task learning based on which information is shared across the tasks. These approaches either assume that the parameters of each of the tasks are situated close together, or assume that there is a common underlying latent space in the features of the tasks that is related. Most multi-task learning algorithm use either regularization method or matrix-variate priors. In our algorithm, the related tasks are tied together by a set of common features selected by each tasks. Thus, to train the related tasks together, we use spike and slab prior to select a common set of features for the related tasks, and a mixture of gaussians prior to select the set of related tasks. For validation, the developed algorithm is tested on toxicity prediction and hand written digit recognition data sets. The results show a significant improvement over multitask learning with feature selection for larger number of tasks. Further, the developed algorithm is also compared against another state of the art algorithm that similarly groups the related tasks together and proven to be better and more accurate.
[generalization performance, information sharing, Multi-task Learning, spike, slab, Training, multitask learning algorithm, feature extraction, Gaussian mixture, Mathematical model, learning (artificial intelligence), Slabs, matrix-variate prior, feature selection, Mixture of Gaussian Prior, toxicology, pattern classification, handwritten character recognition, hand written digit recognition data sets, toxicity prediction, Vectors, Chemicals, Equations, Groups of Tasks, regularization method, Expectation Propagation, Gaussian processes, Spike and Slab Prior, Bayes methods, mixture models]
Network Hypothesis Testing Using Mixed Kronecker Product Graph Models
2013 IEEE 13th International Conference on Data Mining
None
2013
The recent interest in networks-social, physical, communication, information, etc.-has fueled a great deal of research on the analysis and modeling of graphs. However, many of the analyses have focused on a single large network (e.g., a sub network sampled from Facebook). Although several studies have compared networks from different domains or samples, they largely focus on empirical exploration of network similarities rather than explicit tests of hypotheses. This is in part due to a lack of statistical methods to determine whether two large networks are likely to have been drawn from the same underlying graph distribution. Research on across-network hypothesis testing methods has been limited by (i) difficulties associated with obtaining a set of networks to reason about the underlying graph distribution, and (ii) limitations of current statistical models of graphs that make it difficult to represent variations across networks. In this paper, we exploit the recent development of mixed-Kronecker Product Graph Models, which accurately capture the natural variation in real world graphs, to develop a model-based approach for hypothesis testing in networks.
[hypothesis testing, graph theory, network theory (graphs), graph analysis, Electronic mail, graph distribution, Statistics, large networks, Network science, Training, mixed-Kronecker product graph models, Sociology, Data models, graph modeling, across-network hypothesis testing methods, Facebook, statistical testing, statistical methods, graph models, Testing]
Graph Partitioning Change Detection Using Tree-Based Clustering
2013 IEEE 13th International Conference on Data Mining
None
2013
We are concerned with the issue of detecting changes of graph partitioning structures from a graph sequence. We call this issue GPCD(graph partitioning change detection). The graph partitioning structures may represent network communities. Hence GPCD is important in that it leads to discovery of important events which cause changes of network communities. We introduce a new algorithm for GPCD, denoted as TREE. The key ideas are: 1) we employ probabilistic trees to represent probabilistic models of graph partitioning structures. 2) We then reduce GPCD into the issue of detecting changes of trees on the basis of the minimum description length (MDL) principle. 3) By taking the cost of changes into consideration, we realize significantly less false alarm rates for change detection than the baseline method called Graph Scope. We empirically demonstrate that TREE is able to detect changes more accurately than Graph Scope.
[graph partitioning structures, event discovery, Communities, graph theory, Complexity theory, graph sequence, dynamic model selection, tree-based clustering, GraphScope, Bipartite graph, MDL principle, minimum description length, probability, trees (mathematics), Receivers, Educational institutions, Encoding, Partitioning algorithms, GPCD, probabilistic models, graph partitioning change detection, pattern clustering, tree structures, minimum description length principle, network communities, baseline method, false alarm rates, probabilistic trees]
SAX-VSM: Interpretable Time Series Classification Using SAX and Vector Space Model
2013 IEEE 13th International Conference on Data Mining
None
2013
In this paper, we propose a novel method for discovering characteristic patterns in a time series called SAX-VSM. This method is based on two existing techniques - Symbolic Aggregate approximation and Vector Space Model. SAX-VSM automatically discovers and ranks time series patterns by their "importance" to the class, which not only facilitates well-performing classification procedure, but also provides an interpretable class generalization. The accuracy of the method, as shown through experimental evaluation, is at the level of the current state of the art. While being relatively computationally expensive within a learning phase, our method provides fast, precise, and interpretable classification.
[time series analysis, classification algorithms, pattern classification, interpretable time series classification, Time series analysis, data mining, vector space model, time series, Vectors, automatic time series pattern discovery, Approximation methods, automatic time series pattern ranking, Training, Accuracy, symbolic aggregate approximation, SAX-VSM, interpretable class generalization, time series pattern ranking, Euclidean distance, symbol manipulation, Approximation algorithms, learning (artificial intelligence), characteristic pattern discovery, computational complexity]
Clustering on Multiple Incomplete Datasets via Collective Kernel Learning
2013 IEEE 13th International Conference on Data Mining
None
2013
Multiple datasets containing different types of features may be available for a given task. For instance, users' profiles can be used to group users for recommendation systems. In addition, a model can also use users' historical behaviors and credit history to group users. Each dataset contains different information and suffices for learning. A number of clustering algorithms on multiple datasets were proposed during the past few years. These algorithms assume that at least one dataset is complete. So far as we know, all the previous methods will not be applicable if there is no complete dataset available. However, in reality, there are many situations where no dataset is complete. As in building a recommendation system, some new users may not have profiles or historical behaviors, while some may not have credit history. Hence, no available dataset is complete. In order to solve this problem, we propose an approach called Collective Kernel Learning to infer hidden sample similarity from multiple incomplete datasets. The idea is to collectively completes the kernel matrices of incomplete datasets by optimizing the alignment of shared instances of the datasets. Furthermore, a clustering algorithm is proposed based on the kernel matrix. The experiments on both synthetic and real datasets demonstrate the effectiveness of the proposed approach. The proposed clustering algorithm outperforms the comparison algorithms by as much as two times in normalized mutual information.
[Algorithm design and analysis, Correlation, Laplace equations, collective kernel learning, real dataset, incomplete dataset kernel matrices, synthetic dataset, Equations, Optimization, multiple incomplete dataset clustering, user profiles, recommendation systems, normalized mutual information, recommender systems, pattern clustering, Clustering algorithms, user historical behavior, learning (artificial intelligence), credit history, Kernel]
MLI: An API for Distributed Machine Learning
2013 IEEE 13th International Conference on Data Mining
None
2013
MLI is an Application Programming Interface designed to address the challenges of building Machine Learning algorithms in a distributed setting based on data-centric computing. Its primary goal is to simplify the development of high-performance, scalable, distributed algorithms. Our initial results show that, relative to existing systems, this interface can be used to build distributed implementations of a wide variety of common Machine Learning algorithms with minimal complexity and highly competitive performance and scalability.
[application program interfaces, Computational modeling, application programming interface, high-performance algorithm, Vectors, MLI, Sparks, machine learning, MATLAB, distributed computing, data-centric computing, distributed algorithm, distributed algorithms, API, distributed machine learning, Mathematical model, learning (artificial intelligence), programming interface, Logistics]
Co-ClusterD: A Distributed Framework for Data Co-Clustering with Sequential Updates
2013 IEEE 13th International Conference on Data Mining
None
2013
Co-clustering is a powerful data mining tool for co-occurrence and dyadic data. As data sets become increasingly large, the scalability of co-clustering becomes more and more important. In this paper, we propose two approaches to parallelize co-clustering with sequential updates in a distributed environment. Based on these two approaches, we present a new distributed framework, Co-ClusterD, that supports efficient implementations of co-clustering algorithms with sequential updates. We design and implement Co-ClusterD, and show its efficiency through two co-clustering algorithms: fast nonnegative matrix tri-factorization (FNMTF) and information theoretic co-clustering (ITCC). We evaluate our framework on both a local cluster of machines and the Amazon EC2 cloud. Our evaluation shows that co-clustering algorithms implemented in Co-ClusterD can achieve better results and run faster than their traditional concurrent counterparts.
[Algorithm design and analysis, Co-Clustering, Scalability, Sequential Updates, data mining, dyadic data, distributed processing, FNMTF, distributed environment, matrix decomposition, Convergence, coclustering parallelization approach, Cloud Computing, Clustering algorithms, Distributed databases, data coclustering algorithm, sequential updates, information theory, information theoretic coclustering, distributed framework, Linear programming, ITCC, Synchronization, Amazon EC2 cloud, Co-ClusterD, fast nonnegative matrix trifactorization, pattern clustering, Distributed Framework, Concurrent Updates]
Non-negative Multiple Tensor Factorization
2013 IEEE 13th International Conference on Data Mining
None
2013
Non-negative Tensor Factorization (NTF) is a widely used technique for decomposing a non-negative value tensor into sparse and reasonably interpretable factors. However, NTF performs poorly when the tensor is extremely sparse, which is often the case with real-world data and higher-order tensors. In this paper, we propose Non-negative Multiple Tensor Factorization (NMTF), which factorizes the target tensor and auxiliary tensors simultaneously. Auxiliary data tensors compensate for the sparseness of the target data tensor. The factors of the auxiliary tensors also allow us to examine the target data from several different aspects. We experimentally confirm that NMTF performs better than NTF in terms of reconstructing the given data. Furthermore, we demonstrate that the proposed NMTF can successfully extract spatio-temporal patterns of people's daily life such as leisure, drinking, and shopping activity by analyzing several tensors extracted from online review data sets.
[nonnegative multiple tensor factorization, data analysis, Geology, target data tensor sparseness, Probabilistic logic, tensors, matrix decomposition, Sparse matrices, Matrix decomposition, Machine Learning, auxiliary data tensors, Tensile stress, higher-order tensors, Motion pictures, Non-negative Tensor Factorization, Spatio-Temporal Pattern, Business]
Multiclass Semi-Supervised Boosting Using Similarity Learning
2013 IEEE 13th International Conference on Data Mining
None
2013
In this paper, we consider the multiclass semi-supervised classification problem. A boosting algorithm is proposed to solve the multiclass problem directly. The proposed multiclass approach uses a new multiclass loss function, which includes two terms. The first term is the cost of the multiclass margin and the second term is a regularization term on unlabeled data. The regularization term is used to minimize the inconsistency between the pair wise similarity and the classifier predictions. It assigns the soft labels weighted with the similarity between unlabeled and labeled examples. We then derive a boosting algorithm, named CD-MSSBoost, from the proposed loss function using coordinate gradient descent. The derived algorithm is further used for learning optimal similarity function for a given data. Our experiments on a number of UCI datasets show that CD-MSSBoost outperforms the state-of-the-art methods to multiclass semi-supervised learning.
[Algorithm design and analysis, coordinate gradient descent, pattern classification, multiclass loss function, optimal similarity function learning, multiclass semisupervised classification problem, UCI datasets, Semi-Supervised Learning, Boosting, multiclass margin, multiclass semisupervised learning, Multiclass classification, Optimization, multiclass semisupervised boosting algorithm, Training, regularization term, Semisupervised learning, Prediction algorithms, unlabeled data, learning (artificial intelligence), CD-MSSBoost, Similarity learning]
Mining Dependent Frequent Serial Episodes from Uncertain Sequence Data
2013 IEEE 13th International Conference on Data Mining
None
2013
In this paper, we focus on the problem of mining Probabilistic Dependent Frequent Serial Episodes (P-DFSEs) from uncertain sequence data. By observing that the frequentness probability of an episode in an uncertain sequence is a Markov Chain imbeddable variable, we first propose an Embeded Markov Chain-based algorithm that efficiently computes the frequentness probability of an episode by projecting the probability space into a set of limited partitions. To further improve the computation efficiency, we devise an optimized approach that prunes candidate episodes early by estimating the upper bound of their frequentness probabilities.
[Heuristic algorithms, data mining, Probabilistic logic, Electromagnetic compatibility, probabilistic dependent frequent serial episodes mining, uncertain sequence data, Data mining, probability space, Automata, Markov processes, embeded Markov chain-based algorithm, P-DFSE mining, episode frequentness probability]
Efficient and Scalable Information Geometry Metric Learning
2013 IEEE 13th International Conference on Data Mining
None
2013
Information Geometry Metric Learning (IGML) is shown to be an effective algorithm for distance metric learning. In this paper, we attempt to alleviate two limitations of IGML: (A) the time complexity of IGML increases rapidly for high dimensional data, (B) IGML has to transform the input low rank kernel into a full-rank one since it is undefined for singular matrices. To this end, two novel algorithms, referred to as Efficient Information Geometry Metric Learning (EIGML) and Scalable Information Geometry Metric Learning (SIGML), are proposed. EIGML scales linearly with the dimensionality, resulting in significantly reduced computational complexity. As for SIGML, it is proven to have a range-space preserving property. Following this property, SIGML is found to be capable of handling both full-rank and low-rank kernels. Additionally, the geometric information from data is further exploited in SIGML. In contrast to most existing metric learning methods, both EIGML and SIGML have closed-form solutions and can be efficiently optimized. Experimental results on various data sets demonstrate that the proposed methods outperform the state-of-the-art metric learning algorithms.
[Measurement, Covariance matrices, low rank kernel, Learning systems, Information geometry, Mahalanobis distance learning, high dimensional data, range-space preserving property, Eigenvalues and eigenfunctions, scalable information geometry metric learning, learning (artificial intelligence), Kernel, computational complexity reduction, Information Geometry Metric Learning, SIGML, distance metric learning, time complexity, efficient information geometry metric learning, singular matrices, EIGML, closed-form solution, range-space preserving, Computational complexity, matrix algebra, full-rank kernel, computational complexity]
Exploring Patient Risk Groups with Incomplete Knowledge
2013 IEEE 13th International Conference on Data Mining
None
2013
Patient risk stratification, which aims to stratify a patient cohort into a set of homogeneous groups according to some risk evaluation criteria, is an important task in modern medical informatics. Good risk stratification is the key to good personalized care plan design and delivery. The typical procedure for risk stratification is to first identify a set of risk-relevant medical features (also called risk factors), and then construct a predictive model to estimate the risk scores for individual patients. However, due to the heterogeneity of patients' clinical conditions, the risk factors and their importance vary across different patient groups. Therefore a better approach is to first segment the patient cohort into a set of homogeneous groups with consistent clinical conditions, namely risk groups, and then develop group-specific risk prediction models. In this paper, we propose RISGAL (RISk Group Analysis), a novel semi-supervised learning framework for patient risk group exploration. Our method segments a patient similarity graph into a set of risk groups such that some risk groups are in alignment with (incomplete) prior knowledge from the domain experts while the remaining groups reveal new knowledge from the data. Our method is validated on public benchmark datasets as well as a real electronic medical record database to identify risk groups from a set of potential Congestive Heart Failure (CHF) patients.
[Heart, patient risk stratification, CHF, Electronic Medical Records, graph theory, Semi-Supervised Learning, Risk Group Analysis, Accuracy, medical informatics, Clustering algorithms, patient cohort, patient similarity graph, Benchmark testing, patient risk group exploration, Patient Risk Stratification, learning (artificial intelligence), risk-relevant medical features, risk factors, patient clinical condition heterogeneity, congestive heart failure patients, semisupervised learning framework, risk group analysis, electronic medical record database, electronic health records, patient care, risk scores, RISGAL, Diseases, risk analysis, personalized care plan design, incomplete knowledge, domain experts, homogeneous groups, Semisupervised learning, group-specific risk prediction models, medical computing, risk evaluation criteria, Medical diagnostic imaging]
Classification-Based Clustering Evaluation
2013 IEEE 13th International Conference on Data Mining
None
2013
The evaluation of clustering quality has proven to be a difficult task. While it is generally agreed that application specific human assessment can provide a reasonable gold standard for clustering evaluation, the use of human assessors is not practical in many real situations. As a result, machine computable internal clustering quality measures (CQMs) are often used in the evaluation process. However, CQMs have their own drawbacks. Despite their extensive use in clustering research and applications, many CQMs have been shown to lack generality. In this paper we present a new CQM with general applicability. The basis of our CQM is a pattern recognition view of clustering's purpose: the unsupervised prediction of behavior from populations. This purpose translates naturally into our new classifier based CQM which we refer to as in formativeness. We show that in formativeness can satisfy core CQM axioms defined in prior research. Additionally, we provide experimental support, showing that in formativeness can outperform many established CQMs by detecting a larger variety of meaningful structures across a range of synthetic datasets, while at the same time exhibiting good performance on each individual dataset. Our results indicate that in formativeness provides a highly general and effective CQM.
[pattern classification, application-specific human assessment, clustering method, Estimation, human factors, human assessors, classification-based clustering evaluation, Statistics, Radio frequency, CQM axioms, machine computable internal clustering quality measures, pattern clustering, clustering quality evaluation, Sociology, Clustering algorithms, unsupervised behavior prediction, Euclidean distance, Labeling, pattern recognition, synthetic datasets]
Efficient Online Sequence Prediction with Side Information
2013 IEEE 13th International Conference on Data Mining
None
2013
Sequence prediction is a key task in machine learning and data mining. It involves predicting the next symbol in a sequence given its previous symbols. Our motivating application is predicting the execution path of a process on an operating system in real-time. In this case, each symbol in the sequence represents a system call accompanied with arguments and a return value. We propose a novel online algorithm for predicting the next system call by leveraging both context and side information. The online update of our algorithm is efficient in terms of time cost and memory consumption. Experiments on real-world data sets showed that our method outperforms state-of-the-art online sequence prediction methods in both accuracy and efficiency, and incorporation of side information does significantly improve the predictive accuracy.
[Error analysis, memory consumption, data mining, side information, Predictive models, scalability, system call, Accuracy, context information, Prediction algorithms, learning (artificial intelligence), return value, process execution path, Context, system trace, operating system, online algorithm, sequence predictio, Vectors, machine learning, time cost, online sequence prediction, Memory management, operating systems (computers), arguments, online learning]
Multilabel Consensus Classification
2013 IEEE 13th International Conference on Data Mining
None
2013
In the era of big data, a large amount of noisy and incomplete data can be collected from multiple sources for prediction tasks. Combining multiple models or data sources helps to counteract the effects of low data quality and the bias of any single model or data source, and thus can improve the robustness and the performance of predictive models. Out of privacy, storage and bandwidth considerations, in certain circumstances one has to combine the predictions from multiple models or data sources without accessing the raw data. Consensus-based prediction combination algorithms are effective for such situations. However, current research on prediction combination focuses on the single label setting, where an instance can have one and only one label. Nonetheless, data nowadays are usually multilabeled, such that more than one label have to be predicted at the same time. Direct applications of existing prediction combination methods to multilabel settings can lead to degenerated performance. In this paper, we address the challenges of combining predictions from multiple multilabel classifiers and propose two novel algorithms, MLCM-r (MultiLabel Consensus Maximization for ranking) and MLCM-a (MLCM for microAUC). These algorithms can capture label correlations that are common in multilabel classifications, and optimize corresponding performance metrics. Experimental results on popular multilabel classification tasks verify the theoretical analysis and effectiveness of the proposed methods.
[Measurement, Algorithm design and analysis, Correlation, consensus-based prediction combination algorithms, label correlations, Predictive models, ensemble, multilabel classifiers, optimisation, multilabel consensus classification, Prediction algorithms, Bipartite graph, learning (artificial intelligence), big data, MLCM-r, pattern classification, data sources, storage considerations, Big Data, prediction tasks, MLCM for microAUC, MLCM-a, multilabel consensus maximization for ranking, data quality, noisy data, predictive model robustness, bandwidth considerations, performance metrics, multilabel classification, Data models, single label setting, incomplete data]
Sampling Heterogeneous Networks
2013 IEEE 13th International Conference on Data Mining
None
2013
Online social networks are mainly characterized by large-scale and heterogeneous semantic relationships. Unfortunately, for online social network services such as Facebook or Twitter, it is very difficult to obtain the fully observed network without privilege to access the data internally. To address the above needs, social network sampling is a means that aims at identifying a representative subgraph that preserves certain properties of the network, given the information of any instance in the network is unknown before being sampled. This study tackles heterogeneous network sampling by considering the conditional dependency of node types and link types, where we design a property, Relational Profile, to account such characterization. We further propose a sampling method to preserve this property. Lastly, we propose to evaluate our model from three different angles. First, we show that the proposed sampling method can more faithfully preserve the Relational Profile. Second, we evaluate the usefulness of the Relational Profile showing such information is beneficial for link prediction tasks. Finally, we evaluate whether the networks sampled by our method can be used to train more accurate prediction models comparing to networks produced by other methods.
[heterogeneous networks, graph theory, Predictive models, online social network services, link prediction tasks, Twitter, social network sampling, large-scale semantic relationships, node type conditional dependency, Semantics, network prediction, Mathematical model, Facebook, network sampling, Patents, sampling methods, Social network services, heterogeneous network sampling, link type conditional dependency, relational profile, Equations, heterogeneous semantic relationships, representative subgraph identification, Sampling methods, social networking (online)]
A Model for Discovering Correlations of Ubiquitous Things
2013 IEEE 13th International Conference on Data Mining
None
2013
With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web services, physical things are becoming an integral part of the emerging ubiquitous Web. Correlation discovery for ubiquitous things is critical for many important applications such as things search, recommendation, annotation, classification, clustering, composition, and management. In this paper, we propose a novel approach for discovering things correlation based on user, temporal, and spatial information captured from usage events of things. In particular, we use a spatio-temporal graph and a social graph to model things usage contextual information and user-thing relationships respectively. Then, we apply random walks with restart on these graphs to compute correlations among things. This correlation analysis lays a solid foundation and contributes to improved effectiveness in things management. To demonstrate the utility of our approach, we perform a systematic case study and comprehensive experiments on things annotation.
[Correlation, radiofrequency identification, wireless sensor networks, random walk with restart, graph theory, correlation discovery, social graph, Educational institutions, RFID, ubiquitous Web, spatio-temporal graph, ubiquitous things, ubiquitous computing, Equations, Ubiquitous things, correlation analysis, Web services, Feature extraction, Radiofrequency identification, Testing]
Efficiently Mining Top-K High Utility Sequential Patterns
2013 IEEE 13th International Conference on Data Mining
None
2013
High utility sequential pattern mining is an emerging topic in the data mining community. Compared to the classic frequent sequence mining, the utility framework provides more informative and actionable knowledge since the utility of a sequence indicates business value and impact. However, the introduction of "utility" makes the problem fundamentally different from the frequency-based pattern mining framework and brings about dramatic challenges. Although the existing high utility sequential pattern mining algorithms can discover all the patterns satisfying a given minimum utility, it is often difficult for users to set a proper minimum utility. A too small value may produce thousands of patterns, whereas a too big one may lead to no findings. In this paper, we propose a novel framework called top-k high utility sequential pattern mining to tackle this critical problem. Accordingly, an efficient algorithm, Top-k high Utility Sequence (TUS for short) mining, is designed to identify top-k high utility sequential patterns without minimum utility. In addition, three effective features are introduced to handle the efficiency problem, including two strategies for raising the threshold and one pruning for filtering unpromising items. Our experiments are conducted on both synthetic and real datasets. The results show that TUS incorporating the efficiency-enhanced strategies demonstrates impressive performance without missing any high utility sequential patterns.
[Algorithm design and analysis, Top-K sequential pattern mining, Sequences, data mining, TUS mining, information filtering, Data mining, real datasets, Sorting, High utility sequential pattern mining, Itemsets, data mining community, top-k high utility sequential pattern mining, business value, unpromising item filtering, business data processing, Business, utility theory, synthetic datasets]
Efficient Proper Length Time Series Motif Discovery
2013 IEEE 13th International Conference on Data Mining
None
2013
As one of the most essential data mining tasks, finding frequently occurring patterns, i.e., motif discovery, has drawn a lot of attention in the past decade. Despite successes in speedup of motif discovery algorithms, most of the existing algorithms still require predefined parameters. The critical and most cumbersome one is time series motif length since it is difficult to manually determine the proper length of the motifs-even for the domain experts. In addition, with variability in the motif lengths, ranking among these motifs becomes another major problem. In this work, we propose a novel algorithm using compression ratio as a heuristic to discover meaningful motifs in proper lengths. The ranking of these various length motifs relies on an ability to compress time series by its own motif as a hypothesis. Furthermore, other than being an anytime algorithm, our experimental evaluation also demonstrates that our proposed method outperforms existing works in various domains both in terms of speed and accuracy.
[Algorithm design and analysis, motif ranking, Time series analysis, data mining, time series, Data mining, compression ratio, anytime algorithm, Accuracy, Clustering algorithms, proper length motif, proper length time series motif discovery, motif discovery, time series mining, Data models, data mining task, Time complexity]
On Anomalous Hotspot Discovery in Graph Streams
2013 IEEE 13th International Conference on Data Mining
None
2013
Network streams have become ubiquitous in recent years because of many dynamic applications. Such streams may show localized regions of activity and evolution because of anomalous events. This paper will present methods for dynamically determining anomalous hot spots from network streams. These are localized regions of sudden activity or change in the underlying network. We will design a localized principal component analysis algorithm, which can continuously maintain the information about the changes in the different neighborhoods of the network. We will use a fast incremental eigenvector update algorithm based on von Mises iterations in a lazy way in order to efficiently maintain local correlation information. This is used to discover local change hotspots in dynamic streams. We will finally present an experimental study to demonstrate the effectiveness and efficiency of our approach.
[Algorithm design and analysis, Time-frequency analysis, Correlation, graph streams, localized principal component analysis algorithm, anomalous events, Image edge detection, dynamic streams, graph theory, network theory (graphs), Vectors, anomaly detection, network streams, dynamic applications, anomalous hotspot discovery, fast incremental eigenvector update algorithm, von Mises iterations, eigenvalues and eigenfunctions, local correlation information, Motion pictures, Eigenvalues and eigenfunctions, local change hotspot discovery, principal component analysis]
From Social User Activities to People Affiliation
2013 IEEE 13th International Conference on Data Mining
None
2013
This study addresses the problem of inferring users' employment affiliation information from social activities. It is motivated by the applications which need to monitoring and analyzing the social activities of the employees from a given company, especially their social tracks related to the work and business. It definitely helps to better understand their needs and opinions towards certain business area, so that the account sales targeting these customers in the given company can adjust the sales strategies accordingly. Specifically, in this task we are given a snapshot of a social network and some labeled social users who are the employees of a given company. Our goal is to identify more users from the same company. We formulate this problem as a task of classifying nodes over a graph, and develop a Supervised Label Propagation model. It naturally incorporates the rich set of features for social activities, models the networking effect by label propagation, and learns the feature weights so that the labels are propagated to the right users. To validate its effectiveness, we show our case studies on identifying the employees of "China Telecom" and "China Unicom" from Sina Weibo. The experimental results show that our method significantly outperforms the compared baseline ones.
[Social network services, China Telecom, Companies, supervised label propagation model, Media, China Unicom, Vectors, Telecommunications, Equations, social network, social networking (online), learning (artificial intelligence), social user activities]
Transfer Learning across Cancers on DNA Copy Number Variation Analysis
2013 IEEE 13th International Conference on Data Mining
None
2013
DNA copy number variations (CNVs) are prevalent in all types of tumors. It is still a challenge to study how CNVs play a role in driving tumorgenic mechanisms that are either universal or specific in different cancer types. To address the problem, we introduce a transfer learning framework to discover common CNVs shared across different tumor types as well as CNVs specific to each tumor type from genome-wide CNV data measured by array CGH and SNP genotyping array. The proposed model, namely Transfer Learning with Fused LASSO (TLFL), detects latent CNV components from multiple CNV datasets of different tumor types to distinguish the CNVs that are common across the datasets and those that are specific in each dataset. Both the common and type-specific CNVs are detected as latent components in matrix factorization coupled with fused LASSO on adjacent CNV probe features. TLFL considers the common latent components underlying the multiple datasets to transfer knowledge across different tumor types. In simulations and experiments on real cancer CNV datasets, TLFL detected better latent components that can be used as features to improve classification of patient samples in each individual dataset compared with the model without the knowledge transfer. In cross-dataset analysis on bladder cancer and cross-domain analysis on breast cancer and ovarian cancer, TLFL also learned latent CNV components that are both predictive of tumor stages and correlate with known cancer genes.
[DNA Copy Number, transfer learning with fused LASSO, Transfer Learning, adjacent CNV probe features, patient sample classification, Genomics, genome-wide CNV data, common latent components, matrix decomposition, cross-domain analysis, Biological cells, cross-dataset analysis, bladder cancer, CNVs, latent components, TLFL, learning (artificial intelligence), Probes, tumours, SNP genotyping array, pattern classification, matrix factorization, array CGH, ovarian cancer, breast cancer, Fused LASSO Components, tumorgenic mechanisms, cancer genes, Cancer Genomics, Hidden Markov models, DNA, cancer, DNA copy number variation analysis, Arrays, medical computing, Cancer, Tumors]
Predicting Social Links for New Users across Aligned Heterogeneous Social Networks
2013 IEEE 13th International Conference on Data Mining
None
2013
Nowadsys, many new users are keeping joining in the online social networks every day and these new users usually have very few social connections and very sparse auxiliary information in the network. Prediction social links for new users is very important. Different from conventional link prediction problems, link prediction for new users is more challenging due to the lack of information from the new users in the network. Meanwhile, in recent years, users are usually involved in multiple social networks simultaneously to enjoy the specific services offered by different social networks. The shared users of multiple networks can act as the "anchors" aligned the networks they participate in. In this paper, we propose a link prediction method called SCAN-PS (Supervised Cross Aligned Networks link prediction with Personalized Sampling), to solve the social link prediction problem for new users. SCAN-PS can use information transferred from both the existing active users in the target network and other source networks through aligned accounts. In addition, SCAN-PS could solve the cold start problem when information of these new users is total absent in the target network. Extensive experiments conducted on two real-world aligned heterogeneous social networks demonstrate that SCAN-PS can perform well in predicting social links for new users.
[SCAN-PS, data mining, social links, Prediction methods, online social networks, Twitter, Data Mining, Vectors, Cultural differences, aligned heterogeneous social networks, Link Prediction, Diversity reception, Feature extraction, social networking (online), supervised cross aligned networks link prediction with personalized sampling]
Combating Sub-Clusters Effect in Imbalanced Classification
2013 IEEE 13th International Conference on Data Mining
None
2013
Approaches to imbalanced classification problem usually focus on rebalancing the class sizes, neglecting the effect of hidden structure within the majority class. The aim of this paper is to highlight the effect of sub-clusters within the majority class on detecting minority class instances, and handle imbalanced classification by learning the structure in the data. We propose a decomposition based approach to two-class imbalanced classification problem. This approach works by first learning the hidden structure of the majority class using an unsupervised learning algorithm. Thus, transforming the classification problem into several classification sub-problems. The base classifier is constructed on each sub-problem. The ensemble is tuned to increase its sensitivity towards minority class. The proposed approach overcomes the limitations of conventional classifiers on imbalanced problem, and combats imbalance by learning hidden structure in the majority class, which is neglected in most existing works. We demonstrate the performance of the proposed approach on several datasets.
[majority class, latent structure, pattern classification, Correlation, within-class imbalance, supervised learning, Classification algorithms, unsupervised learning, decomposition-based approach, Support vector machines, Training, ensemble learning, two-class imbalanced classification problem, Sensitivity, Accuracy, pattern clustering, unsupervised learning algorithm, hidden structure learning, Clustering algorithms, minority class instance detection, class size rebalancing, base classifier, clustering, subcluster effect]
Influence and Profit: Two Sides of the Coin
2013 IEEE 13th International Conference on Data Mining
None
2013
Influence maximization problem is to find a set of seeds in social networks such that the cascade influence is maximized. Traditional models assume all nodes are willing to spread the influence once they are influenced, and they ignore the disparity between influence and profit of a product. In this paper by considering the role that price plays in viral marketing, we propose price related (PR) frame that contains PR-I and PR-L models for classic IC and LT models respectively, which is a pioneer work. We find that influence and profit are like two sides of the coin, high price hinders the influence propagation and to enlarge the influence some sacrifice on profit is inevitable. We propose Balanced Influence and Profit (BIP) maximization problem. We prove the NP-hardness of BIP maximization under PR-I and PR-L model. Unlike influence maximization, the BIP objective function is not monotone. Despite the non-monotony, we show BIP objective function is sub modular under certain conditions. Two unbudgeted greedy algorithms separately are devised. We conduct simulations on real-world datasets and evaluate the superiority of our algorithms over existing ones.
[profitability, profit maximization, balanced influence and profit maximization problem, LT models, independent cascade model, BIP objective function, optimisation, viral marketing, PR-L models, Manufacturing, NP-hardness problem, Influence maximization, LT model, Social network services, Computational modeling, greedy algorithms, IC model, social networks, unbudgeted greedy algorithms, Linear programming, Educational institutions, marketing data processing, PR-I model, IC models, price related frame, cascade influence maximization problem, influence propagation, social networking (online), linear threshold model, Integrated circuit modeling, pricing, computational complexity, coin]
Constrained Clustering: Effective Constraint Propagation with Imperfect Oracles
2013 IEEE 13th International Conference on Data Mining
None
2013
While spectral clustering is usually an unsupervised operation, there are circumstances in which we have prior belief that pairs of samples should (or should not) be assigned with the same cluster. Constrained spectral clustering aims to exploit this prior belief as constraint (or weak supervision) to influence the cluster formation so as to obtain a structure more closely resembling human perception. Two important issues remain open: (1) how to propagate sparse constraints effectively, (2) how to handle ill-conditioned/noisy constraints generated by imperfect oracles. In this paper we present a unified framework to address the above issues. Specifically, in contrast to existing constrained spectral clustering approaches that blindly rely on all features for constructing the spectral, our approach searches for neighbours driven by discriminative feature selection for more effective constraint diffusion. Crucially, we formulate a novel data-driven filtering approach to handle the noisy constraint problem, which has been unrealistically ignored in constrained spectral clustering literature.
[constrained spectral clustering, Clustering methods, data-driven filtering approach, Constrained clustering, constraint propagation, information filtering, Noise measurement, imperfect oracles, discriminative feature selection, Equations, Optimization, weak supervision, Training, sparse constraint propagation, cluster formation, ill-conditioned-noisy constraint handling, pattern clustering, constraint diffusion, Vegetation, Mathematical model, constraint handling, feature selection, spectral clustering]
Influence Maximization in Dynamic Social Networks
2013 IEEE 13th International Conference on Data Mining
None
2013
Social influence and influence diffusion has been widely studied in online social networks. However, most existing works on influence diffusion focus on static networks. In this paper, we study the problem of maximizing influence diffusion in a dynamic social network. Specifically, the network changes over time and the changes can be only observed by periodically probing some nodes for the update of their connections. Our goal then is to probe a subset of nodes in a social network so that the actual influence diffusion process in the network can be best uncovered with the probing nodes. We propose a novel algorithm to approximate the optimal solution. The algorithm, through probing a small portion of the network, minimizes the possible error between the observed network and the real network. We evaluate the proposed algorithm on both synthetic and real large networks. Experimental results show that our proposed algorithm achieves a better performance than several alternative algorithms.
[Algorithm design and analysis, influence diffusion maximization, Heuristic algorithms, Estimation, human factors, online social networks, Twitter, directed graphs, static networks, Approximation algorithms, social networking (online), probing nodes, influence maximization, dynamic social networks, Probes, social influence]
Adaptive Model Tree for Streaming Data
2013 IEEE 13th International Conference on Data Mining
None
2013
With an ever-growing availability of data streams the interest in and need for efficient techniques dealing with such data increases. A major challenge in this context is the accurate online prediction of continuous values in the presence of concept drift. In this paper, we introduce a new adaptive model tree (AMT), designed to incrementally learn from the data stream, adapt to the changes, and to perform real time accurate predictions at anytime. To deal with sub models lying in different subspaces, we propose a new model clustering algorithm able to identify subspace models, and use it for computing splits in the input space. Compared to state of the art, our AMT allows for oblique splits, delivering more compact and accurate models.
[Adaptation models, data streaming, regression tree, streaming data, Computational modeling, subspace model identification, trees (mathematics), Predictive models, AMT, model clustering algorithm, Support vector machines, adaptive model tree, online prediction, Impurities, pattern clustering, concept drift, oblique splits, Clustering algorithms, prediction, Data models, learning (artificial intelligence)]
Structural-Context Similarities for Uncertain Graphs
2013 IEEE 13th International Conference on Data Mining
None
2013
Structural-context similarities between vertices in graphs, such as the Jaccard similarity, the Dice similarity, and the cosine similarity, play important roles in a number of graph data analysis techniques. However, uncertainty is inherent in massive graph data, and therefore the classical definitions of structural-context similarities on exact graphs don't make sense on uncertain graphs. In this paper, we propose a generic definition of structural-context similarity for uncertain graphs. Since it is computationally prohibitive to compute the similarity between two vertices of an uncertain graph directly by its definition, we investigate two efficient approaches to computing similarities, namely the polynomial-time exact algorithms and the linear-time approximation algorithms. The experimental results on real uncertain graphs verify the effectiveness of the proposed structural-context similarities as well as the accuracy and efficiency of the proposed evaluation algorithms.
[approximation theory, Uncertainty, Data analysis, data analysis, graph theory, Approximation methods, cosine similarity, linear-time approximation algorithm, graph data analysis techniques, uncertain graph, Dice similarity, Accuracy, Databases, structural-context similarities, structural-context similarity, Approximation algorithms, Jaccard similarity, Joints, polynomial-time exact algorithm, computational complexity]
Message from the Conference Chairs
2014 IEEE International Conference on Data Mining
None
2014
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Program Co-Chairs
2014 IEEE International Conference on Data Mining
None
2014
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2014 IEEE International Conference on Data Mining
None
2014
Provides a listing of current committee members and society officers.
[]
Program Committee
2014 IEEE International Conference on Data Mining
None
2014
Provides a listing of current committee members and society officers.
[]
Keynotes
2014 IEEE International Conference on Data Mining
None
2014
These keynote discusses the following: Towards Mobile Visual Search; Ten Research Challenges in Data Science; and Understanding Global Change: Opportunities and Challenges for Data Driven Research.
[data driven research, mobile visual search, data science, global change understanding, data mining, globalisation]
Tutorials
2014 IEEE International Conference on Data Mining
None
2014
These keynote discusses the following: Social Multimedia as Sensors; Node and Graph Similarity: Theory and Applications; Finding Repeated Structure in Time Series: Algorithms and Applications; and A Tutorial on Online Learning Methods for Big Data Analytics.
[data analysis, node similarity, graph theory, data mining, Tutorials, Big Data, Educational institutions, time series, Classification algorithms, repeated structure finding, Multimedia communication, multimedia computing, graph similarity, online learning methods, Big Data analytics, Clustering algorithms, social networking (online), social multimedia, Big data, Sensors, computer aided instruction]
Discriminative Learning on Exemplary Patterns of Sequential Numerical Data
2014 IEEE International Conference on Data Mining
None
2014
One of the effective methodologies for time series classification is to identify informative subsequence patterns in time series and exploit them as discriminative features. Previous studies on this methodology have achieved promising results using a small number of individually selected patterns. However, there remain difficulties in finding a set of related patterns or patterns of a minor class, which can be critical in real-world applications. In this paper, we exploit the sparse learning technique for the support vector machine (SVM) to identify informative and exemplary patterns. We first present a representation of time series as a vector of distances to exemplary patterns. It allows a structural SVM to handle distance space data and function as the nearest neighbor classifier, the combination of which is known to be highly competitive in time series classification. We then extend the zero-norm approximation method for the structural SVM, which can eliminate non-essential patterns from the classification model. The resulting model makes predictions by a simple modified nearest neighbor rule, yet has a strong mathematical support for empirical risk minimization and feature selection. We conduct an empirical study on real-world behavior and sequential data to evaluate the effectiveness of the proposed method and graphically examine the exemplary patterns.
[sparse learning technique, structural SVM, Transforms, Approximation methods, Training, exemplary patterns, Mathematical model, learning (artificial intelligence), Sequence template transform, feature selection, time series classification, approximation theory, pattern classification, zero-norm approximation method, support vector machines, Time series analysis, Time series classification, risk minimization, sequential numerical data, nearest neighbor classifier, time series, Vectors, discriminative learning, Nearest neighbor classifier, Structural SVM, classification model, Support vector machines, discriminative features, informative subsequence patterns, support vector machine, nearest neighbor rule, Data cleaning, data handling, distance space data handling]
Orthogonal Matching Pursuit for Sparse Quantile Regression
2014 IEEE International Conference on Data Mining
None
2014
We consider new formulations and methods for sparse quantile regression in the high-dimensional setting. Quantile regression plays an important role in many data mining applications, including outlier-robust exploratory analysis in gene selection. In addition, the sparsity consideration in quantile regression enables the exploration of the entire conditional distribution of the response variable given the predictors and therefore yields a more comprehensive view of the important predictors. We propose a generalized Orthogonal Matching Pursuit algorithm for variable selection, taking the misfit loss to be either the traditional quantile loss or a smooth version we call quantile Huber, and compare the resulting greedy approaches with convex sparsity-regularized formulations. We apply a recently proposed interior point methodology to efficiently solve all formulations, provide theoretical guarantees of consistent estimation, and demonstrate the performance of our approach using empirical studies of simulated and genomic datasets.
[genomic datasets, pattern matching, gene selection, greedy algorithms, Matching pursuit algorithms, data mining, regression analysis, Vectors, interior point methodology, Sparse matrices, Servers, MATLAB, simulated datasets, greedy approaches, Convergence, conditional distribution, quantile Huber, sparse quantile regression, outlier-robust exploratory analysis, variable selection, IP networks, generalized orthogonal matching pursuit algorithm]
Inferring Uncertain Trajectories from Partial Observations
2014 IEEE International Conference on Data Mining
None
2014
The explosion in the availability of GPS-enabled devices has resulted in an abundance of trajectory data. In reality, however, majority of these trajectories are collected at a low sampling rate and only provide partial observations on their actually traversed routes. Consequently, they are mired with uncertainty. In this paper, we develop a technique called Infer Tra to infer uncertain trajectories from network-constrained partial observations. Rather than predicting the most likely route, the inferred uncertain trajectory takes the form of an edge-weighted graph and summarizes all probable routes in a holistic manner. For trajectory inference, Infer Tra employs Gibbs sampling by learning a Network Mobility Model (NMM) from a database of historical trajectories. Extensive experiments on real trajectory databases show that the graph-based approach of Infer Tra is up to 50% more accurate, 20 times faster, and immensely more versatile than state-of-the-art techniques.
[network mobility model, Uncertainty, partial observations, Roads, edge weighted graph, trajectory inference, uncertain trajectory, network theory (graphs), uncertainty handling, GPS-enabled device, network constrained partial observation, mobile computing, Databases, Trajectory, learning (artificial intelligence), Joints, InferTra, inference mechanisms, Gibbs sampling, NMM, uncertain trajectory data, directed graphs, road network, Markov processes, Random variables, Joining processes]
Tensor-Based Multi-view Feature Selection with Applications to Brain Diseases
2014 IEEE International Conference on Data Mining
None
2014
In the era of big data, we can easily access information from multiple views which may be obtained from different sources or feature subsets. Generally, different views provide complementary information for learning tasks. Thus, multi-view learning can facilitate the learning process and is prevalent in a wide range of application domains. For example, in medical science, measurements from a series of medical examinations are documented for each subject, including clinical, imaging, immunologic, serologic and cognitive measures which are obtained from multiple sources. Specifically, for brain diagnosis, we can have different quantitative analysis which can be seen as different feature subsets of a subject. It is desirable to combine all these features in an effective way for disease diagnosis. However, some measurements from less relevant medical examinations can introduce irrelevant information which can even be exaggerated after view combinations. Feature selection should therefore be incorporated in the process of multi-view learning. In this paper, we explore tensor product to bring different views together in a joint space, and present a dual method of tensor-based multi-view feature selection DUAL-TMFS based on the idea of support vector machine recursive feature elimination. Experiments conducted on datasets derived from neurological disorder demonstrate the features selected by our proposed method yield better classification performance and are relevant to disease diagnosis.
[medical science, Correlation, immunologic measures, multiview learning, DUAL-TMFS, multi-view learning, tensor-based multiview feature selection, tensor-based multi-view feature selection, brain diagnosis, imaging measures, medical administrative data processing, learning (artificial intelligence), big data, feature selection, neurological disorder, support vector machines, Big Data, diseases, Vectors, brain, clinical measures, tensor, cognitive measures, serologic measures, support vector machine recursive feature elimination, Diseases, Support vector machines, Tensile stress, brain diseases, feature subsets, disease diagnosis, medical examinations, neurophysiology, medical computing, Medical diagnostic imaging, patient diagnosis]
Collective Prediction of Multiple Types of Links in Heterogeneous Information Networks
2014 IEEE International Conference on Data Mining
None
2014
Link prediction has become an important and active research topic in recent years, which is prevalent in many real-world applications. Current research on link prediction focuses on predicting one single type of links, such as friendship links in social networks, or predicting multiple types of links independently. However, many real-world networks involve more than one type of links, and different types of links are not independent, but related with complex dependencies among them. In such networks, the prediction tasks for different types of links are also correlated and the links of different types should be predicted collectively. In this paper, we study the problem of collective prediction of multiple types of links in heterogeneous information networks. To address this problem, we introduce the linkage homophily principle and design a relatedness measure, called RM, between different types of objects to compute the existence probability of a link. We also extend conventional proximity measures to heterogeneous links. Furthermore, we propose an iterative framework for heterogeneous collective link prediction, called HCLP, to predict multiple types of links collectively by exploiting diverse and complex linkage information in heterogeneous information networks. Empirical studies on real-world tasks demonstrate that the proposed collective link prediction approach can effectively boost link prediction performances in heterogeneous information networks.
[Drugs, Correlation, probability, social networks, heterogeneous information networks, meta path, Chemical compounds, information networks, proximity measures, Chemicals, Diseases, Couplings, collective link prediction, Semantics, friendship links, linkage homophily principle, relatedness measure, link existence probability, RM]
Factorized Similarity Learning in Networks
2014 IEEE International Conference on Data Mining
None
2014
The problem of similarity learning is relevant to many data mining applications, such as recommender systems, classification, and retrieval. This problem is particularly challenging in the context of networks, which contain different aspects such as the topological structure, content, and user supervision. These different aspects need to be combined effectively, in order to create a holistic similarity function. In particular, while most similarity learning methods in networks such as Sim Rank utilize the topological structure, the user supervision and content are rarely considered. In this paper, a Factorized Similarity Learning (FSL) is proposed to integrate the link, node content, and user supervision into an uniform framework. This is learned by using matrix factorization, and the final similarities are approximated by the span of low rank matrices. The proposed framework is further extended to a noise-tolerant version by adopting a hinge-loss alternatively. To facilitate efficient computation on large scale data, a parallel extension is developed. Experiments are conducted on the DBLP and CoRA datasets. The results show that FSL is robust, efficient, and outperforms the state-of-the-art.
[data mining, Supervised matrix factorization, matrix decomposition, Sparse matrices, Optimization, factorized similarity learning, similarity learning method, low rank matrices, holistic similarity function, sim rank, Silicon, DBLP dataset, learning (artificial intelligence), FSL, Supervision, pattern classification, matrix factorization, information retrieval, Network similarity, Linear programming, CoRA dataset, Noise measurement, Equations, recommender systems, Content, parallel extension, data mining application, Link]
Learning Local Semantic Distances with Limited Supervision
2014 IEEE International Conference on Data Mining
None
2014
Recent advances in distance function learning have demonstrated that learning a good distance metric can greatly improve the performance in a wide variety of tasks in data mining and web search. A major problem in such scenarios is the limited labeled knowledge available for learning the user intentions. Furthermore, distances are inherently local, where a single global distance function may not capture the distance structure well. A challenge here is that local distance learning is even harder when the labeled information available is limited, because the distance function varies with data locality. To address these issues, we propose a local metric learning algorithm termed Local Semantic Sensing (LSS), which augments the small amount of labeled data with unlabeled data in order to learn the semantic information in the manifold structure, and then integrated with supervised intentional knowledge in a local way. We present results in a retrieval application, which show that the approach significantly outperforms other state-of-the-art methods in the literature.
[Measurement, local metric learning algorithm, search engines, retrieval application, manifold structure, Instance based, Semantic Aware, data mining, user intentions, Data mining, Manifolds, Semantics, data locality, Semi-supervised, learning (artificial intelligence), Similarity learning, Context, Symmetric matrices, distance metric, information retrieval, Vectors, single global distance function, LSS, local semantic distance learning, supervised intentional knowledge, distance function learning, local semantic sensing, Internet, Web search, Metric learning]
Road Traffic Congestion Monitoring in Social Media with Hinge-Loss Markov Random Fields
2014 IEEE International Conference on Data Mining
None
2014
Real-time road traffic congestion monitoring is an important and challenging problem. Most existing monitoring approaches require the deployment of infrastructure sensors or large-scale probe vehicles. Their installation is often expensive and temporal-spatial coverage is limited. Probe vehicle data are oftentimes noisy on urban arterials, and therefore insufficient to provide accurate congestion estimation. This paper presents a novel social-media based approach to traffic congestion monitoring, in which pedestrians, drivers, and passengers a retreated as human sensors and their posted tweets in Twitter as observations of nearby ongoing traffic conditions. There are three technical challenges for road traffic monitoring based on Twitter, namely: 1) language ambiguity in the usage of traffic related terms, 2) uncertainty and low resolution of geographic location mentions, and 3) interactions between traffic-related events such as accidents and congestion. We propose a topic modeling based language model to address the first challenge and a collaborative inference model based on probabilistic soft logic (PSL) to address the second and third challenges. We present a unified statistical framework that combines those two models based on hinge loss Markov random fields (HLMRFs). In order to address the computational challenges incurred by the non-analytical integral of latent variables (factors) and the MAP estimation of a large number of location-dependent traffic congestion variables, we propose a fast approximate inference algorithm based on maximization expectation (ME) and the alternating directed method of multipliers (ADMM). Extensive evaluations over a variety of metrics on real world Twitter and INRIX probe speed datasets in two U.S. Major cities demonstrate the efficiency and effectiveness of our proposed approach.
[Traffic Congestion Monitoring, Roads, probabilistic logic, approximate inference algorithm, Social Media, INRIX probe speed datasets, collaborative inference model, probabilistic soft logic, Twitter, geographic location mention uncertainty, large-scale probe vehicles, Vehicles, latent variables nonanalytical integral, social-media based approach, ME, road traffic congestion monitoring, Sensors, Monitoring, MAP estimation, road traffic, location-dependent traffic congestion variables, passengers, Markov Random Fields, natural language processing, unified statistical framework, language ambiguity, HLMRF, Media, pedestrians, PSL, hinge-loss Markov random fields, low resolution geographic location mention, traffic engineering computing, inference mechanisms, alternating directed method of multipliers, US major cities, urban arterials, human sensors, maximization expectation, traffic-related events, topic modeling based language model, traffic related terms, Markov processes, social networking (online), statistical analysis, drivers, infrastructure sensors, Accidents]
LorSLIM: Low Rank Sparse Linear Methods for Top-N Recommendations
2014 IEEE International Conference on Data Mining
None
2014
In this paper, we notice that sparse and low-rank structures arise in the context of many collaborative filtering applications where the underlying graphs have block-diagonal adjacency matrices. Therefore, we propose a novel Sparse and Low-Rank Linear Method (Lor SLIM) to capture such structures and apply this model to improve the accuracy of the Top-N recommendation. Precisely, a sparse and low-rank aggregation coefficient matrix W is learned from Lor SLIM by solving an l1-norm and nuclear norm regularized optimization problem. We also develop an efficient alternating augmented Lagrangian method (ADMM) to solve the optimization problem. A comprehensive set of experiments is conducted to evaluate the performance of Lor SLIM. The experimental results demonstrate the superior recommendation quality of the proposed algorithm in comparison with current state-of-the-art methods.
[collaborative filtering, alternating augmented Lagrangian method, top-n recommendations, Sparse matrices, Optimization, low-rank aggregation coefficient matrix, Lor SLIM, optimisation, nuclear norm regularized optimization problem, collaborative filtering applications, L_1-norm regularization, Mathematical model, LorSLIM, low rank sparse linear methods, ADMM, Recommender systems, low-rank linear method, Top-N Recommender Systems, block-diagonal adjacency matrices, Vectors, Equations, matrix algebra, Sparse and Low-Rank Linear Method, recommender systems, l<sub>1</sub>-norm, nuclear norm regularization, Collaboration, low-rank structures]
Detecting Flow Anomalies in Distributed Systems
2014 IEEE International Conference on Data Mining
None
2014
Deep within the networks of distributed systems, one often finds anomalies that affect their efficiency and performance. These anomalies are difficult to detect because the distributed systems may not have sufficient sensors to monitor the flow of traffic within the interconnected nodes of the networks. Without early detection and making corrections, these anomalies may aggravate over time and could possibly cause disastrous outcomes in the system in the unforeseeable future. Using only coarse-grained information from the two end points of network flows, we propose a network transmission model and a localization algorithm, to detect the location of anomalies and rank them using a proposed metric within distributed systems. We evaluate our approach on passengers' records of an urbanized city's public transportation system and correlate our findings with passengers' postings on social media micro blogs. Our experiments show that the metric derived using our localization algorithm gives a better ranking of anomalies as compared to standard deviation measures from statistical models. Our case studies also demonstrate that transportation events reported in social media micro blogs matches the locations of our detect anomalies, suggesting that our algorithm performs well in locating the anomalies within distributed systems.
[network transmission model, Transportation, distributed processing, anomaly location detection, urbanized city public transportation system, traffic flow monitoring, localization algorithm, Histograms, public transport, coarse-grained information, social networking (online), distributed systems, Computer networks, Data models, Sensors, flow anomaly detection, social media microblogs, Mathematical model, Joining processes]
Low-Rank Common Subspace for Multi-view Learning
2014 IEEE International Conference on Data Mining
None
2014
Multi-view data is very popular in real-world applications, as different view-points and various types of sensors help to better represent data when fused across views or modalities. Samples from different views of the same class are less similar than those with the same view but different class. We consider a more general case that prior view information of testing data is inaccessible in multi-view learning. Traditional multi-view learning algorithms were designed to obtain multiple view-specific linear projections and would fail without this prior information available. That was because they assumed the probe and gallery views were known in advance, so the correct view-specific projections were to be applied in order to better learn low-dimensional features. To address this, we propose a Low-Rank Common Subspace (LRCS) for multi-view data analysis, which seeks a common low-rank linear projection to mitigate the semantic gap among different views. The low-rank common projection is able to capture compatible intrinsic information across different views and also well-align the within-class samples from different views. Furthermore, with a low-rank constraint on the view-specific projected data and that transformed by the common subspace, the within-class samples from multiple views would concentrate together. Different from the traditional supervised multi-view algorithms, our LRCS works in a weakly supervised way, where only the view information gets observed. Such a common projection can make our model more flexible when dealing with the problem of lacking prior view information of testing data. Two scenarios of experiments, robust subspace learning and transfer learning, are conducted to evaluate our algorithm. Experimental results on several multi-view datasets reveal that our proposed method outperforms state-of-the-art, even when compared with some supervised learning methods.
[Algorithm design and analysis, common subspace, data analysis, Noise, Linear programming, multiview data analysis, multiview learning, Multi-view, LRCS, low-rank, Robustness, Face, learning (artificial intelligence), Probes, low-rank common subspace, Testing]
Sparse Real Estate Ranking with Online User Reviews and Offline Moving Behaviors
2014 IEEE International Conference on Data Mining
None
2014
Ranking residential real estates based on investment values can provide decision making support for home buyers and thus plays an important role in estate marketplace. In this paper, we aim to develop methods for ranking estates based on investment values by mining users' opinions about estates from online user reviews and offline moving behaviors (e.g., Taxi traces, smart card transactions, check-ins). While a variety of features could be extracted from these data, these features are Interco related and redundant. Thus, selecting good features and integrating the feature selection into the fitting of a ranking model are essential. To this end, in this paper, we first strategically mine the fine-grained discrminative features from user reviews and moving behaviors, and then propose a probabilistic sparse pair wise ranking method for estates. Specifically, we first extract the explicit features from online user reviews which express users' opinions about point of interests (POIs) near an estate. We also mine the implicit features from offline moving behaviors from multiple perspectives (e.g., Direction, volume, velocity, heterogeneity, topic, popularity, etc.). Then we learn an estate ranking predictor by combining a pair wise ranking objective and a sparsity regularization in a unified probabilistic framework. And we develop an effective solution for the optimization problem. Finally, we conduct a comprehensive performance evaluation with real world estate related data, and the experimental results demonstrate the competitive performance of both features and the proposed model.
[opinion mining, Smart cards, optimization problem, data mining, Mobile communication, sparsity regularization, Data mining, probabilistic sparse pairwise ranking method, Online User Reviews, optimisation, real estate data processing, online user review, estate marketplace, Trajectory, feature selection, probability, pairwise ranking objective, Offline Moving Behaviors, investment, sparse real estate ranking, investment value, estate ranking predictor, Feature extraction, Residential Real Estate, offline moving behavior, residential real estate, Sparse Ranking, Investment]
Finding the Optimal Subspace for Clustering
2014 IEEE International Conference on Data Mining
None
2014
The ability to simplify and categorize things is one of the most important elements of human thought, understanding, and learning. The corresponding explorative data analysis techniques -- dimensionality reduction and clustering -- have initially been studied by our community as two separate research topics. Later algorithms like CLIQUE, ORCLUS, 4C, etc. Performed clustering and dimensionality reduction in a joint, alternating process to find clusters residing in low-dimensional subspaces. Such a low-dimensional representation is extremely useful, because it allows us to visualize the relationships between the various objects of a cluster. However, previous methods of subspace, correlation or projected clustering determine an individual subspace for each cluster. In this paper, we demonstrate that it is even much more valuable to find clusters in one common low-dimensional subspace, because then we can study not only the intra-cluster but also the inter-cluster relationships of objects, and the relationships of the whole clusters to each other. We develop the mathematical foundation ORT (Optimal Rigid Transform) to determine an arbitrarily-oriented subspace, suitable for a given cluster structure. Based on ORT, we propose FOSSCLU (Finding the Optimal Sub Space for Clustering), a new iterative clustering algorithm. Our extensive experiments demonstrate that FOSSCLU outperforms the previous methods even in both aspects: clustering and dimensionality reduction.
[iterative methods, data analysis, iterative clustering algorithm, finding the optimal sub space for clustering, Noise, Transforms, mathematical foundation, Matrix decomposition, Covariance matrices, ORT, dimensionality reduction, low-dimensional representation, pattern clustering, Clustering algorithms, optimal subspace, optimal rigid transform, FOSSCLU, Eigenvalues and eigenfunctions, Joint Subspace Clustering, Principal component analysis]
A Collaborative Kalman Filter for Time-Evolving Dyadic Processes
2014 IEEE International Conference on Data Mining
None
2014
We present the collaborative Kalman filter (CKF), a dynamic model for collaborative filtering and related factorization models. Using the matrix factorization approach to collaborative filtering, the CKF accounts for time evolution by modeling each low-dimensional latent embedding as a multidimensional Brownian motion. Each observation is a random variable whose distribution is parameterized by the dot product of the relevant Brownian motions at that moment in time. This is naturally interpreted as a Kalman filter with multiple interacting state space vectors. We also present a method for learning a dynamically evolving drift parameter for each location by modeling it as a geometric Brownian motion. We handle posterior intractability via a mean-field variational approximation, which also preserves tractability for downstream calculations in a manner similar to the Kalman filter. We evaluate the model on several large datasets, providing quantitative evaluation on the 10 million Movie lens and 100 million Netflix datasets and qualitative evaluation on a set of 39 million stock returns divided across roughly 6,500 companies from the years 1962-2014.
[collaborative filtering, geometric Brownian motion, Movie lens datasets, matrix factorization approach, random variable, Netflix datasets, matrix decomposition, Approximation methods, multiple interacting state space vectors, posterior intractability, Kalman filters, Mathematical model, learning (artificial intelligence), CKF, approximation theory, collaborative Kalman filter, multidimensional Brownian motion, Vectors, Equations, Brownian motion, Collaboration, time-evolving dyadic processes, low-dimensional latent modelling, Data models, dot product, mean-field variational approximation, drift parameter]
SNOC: Streaming Network Node Classification
2014 IEEE International Conference on Data Mining
None
2014
Many real-world networks are featured with dynamic changes, such as new nodes and edges, and modification of the node content. Because changes are continuously introduced to the network in a streaming fashion, we refer to such dynamic networks as streaming networks. In this paper, we propose a new classification method for streaming networks, namely streaming network node classification (SNOC). For streaming networks, the essential challenge is to properly capture the dynamic changes of the node content and node interactions to support node classification. While streaming networks are dynamically evolving, for a short temporal period, a subset of salient features are essentially tied to the network content and structures, and therefore can be used to characterize the network for classification. To achieve this goal, we propose to carry out streaming network feature selection (SNF) from the network, and use selected features as gauge to classify unlabeled nodes. A Laplacian based quality criterion is proposed to guide the node classification, where the Laplacian matrix is generated based on node labels and structures. Node classification is achieved by finding the class that results in the minimal gauging value with respect to the selected features. By frequently updating the features selected from the network, node classification can quickly adapt to the changes in the network for maximal performance gain. Experiments demonstrate that SNOC is able to capture changes in network structures and node content, and outperforms baseline approaches with significant performance gain.
[Dynamic, salient feature, SNOC, SNF, Data mining, streaming fashion, Optimization, Feature Selection, Accuracy, network content, Network, Classification, Laplacian based quality criterion, Laplacian matrix, streaming network node classification, feature selection, real-world network, pattern classification, Laplace equations, Linear programming, Educational institutions, Vectors, dynamic network, matrix algebra, streaming network feature selection]
Dual-Domain Hierarchical Classification of Phonetic Time Series
2014 IEEE International Conference on Data Mining
None
2014
Phonemes are the smallest units of sound produced by a human being. Automatic classification of phonemes is a well-researched topic in linguistics due to its potential for robust speech recognition. With the recent advancement of phonetic segmentation algorithms, it is now possible to generate datasets of millions of phonemes automatically. Phoneme classification on such datasets is a challenging data mining task because of the large number of classes (over a hundred) and complexities of the existing methods. In this paper, we introduce the phoneme classification problem as a data mining task. We propose a dual-domain (time and frequency) hierarchical classification algorithm. Our method uses a Dynamic Time Warping (DTW) based classifier in the top layers and time-frequency features in the lower layer. We cross-validate our method on phonemes from three online dictionaries and achieved up to 35% improvement in classification compared to existing techniques. We provide case studies on classifying accented phonemes and speaker invariant phoneme classification.
[dual-domain hierarchical classification, dynamic time warping, Dictionaries, linguistics, robust speech recognition, data mining, datasets, frequency hierarchical classification algorithm, Accuracy, DTW based classifier, speech recognition, sound units, Phoneme classification, Robustness, Big data, phonetic segmentation algorithms, online dictionaries, Time series analysis, time series, time hierarchical classification algorithm, speaker invariant phoneme classification, Standards, signal classification, automatic classification, time-frequency features, phonetic time series, Speech recognition, Speech, time series mining]
Sequence Classification Based on Delta-Free Sequential Patterns
2014 IEEE International Conference on Data Mining
None
2014
Sequential pattern mining is one of the most studied and challenging tasks in data mining. However, the extension of well-known methods from many other classical patterns to sequences is not a trivial task. In this paper we study the notion of &#x03B4;-freeness for sequences. While this notion has extensively been discussed for itemsets, this work is the first to extend it to sequences. We define an efficient algorithm devoted to the extraction of &#x03B4;-free sequential patterns. Furthermore, we show the advantage of the &#x03B4;-free sequences and highlight their importance when building sequence classifiers, and we show how they can be used to address the feature selection problem in statistical classifiers, as well as to build symbolic classifiers which optimizes both accuracy and earliness of predictions.
[pattern classification, free patterns, feature selection problem, data mining, sequence classification, &#x03B4;-freeness, statistical classifiers, Generators, text classification, Data mining, early classification, Accuracy, Runtime, Itemsets, delta-free sequential patterns, symbolic classifiers, Feature extraction, sequential pattern mining, itemsets, statistical analysis, &#x03B4;-free sequential patterns, feature selection, sequence mining]
Social Spammer Detection with Sentiment Information
2014 IEEE International Conference on Data Mining
None
2014
Social media is a popular platform for spammers to unfairly overwhelm normal users with unwanted or fake content via social networking. The spammers significantly hinder the use of social media systems for effective information dissemination and sharing. Different from the spammers in traditional platforms such as email and the Web, spammers in social media can easily connect with each other, sometimes without mutual consent. They collude with each other to imitate normal users by quickly accumulating a large number of "human" friends. In addition, content information in social media is noisy and unstructured. It is infeasible to directly apply traditional spammer detection methods in social media. Understanding and detecting deception has been extensively studied in traditional sociology and social sciences. Motivated by psychological findings in physical world, we investigate whether sentiment analysis can help spammer detection in online social media. In particular, we first conduct an exploratory study to analyze the sentiment differences between spammers and normal users, and then present an optimization formulation that incorporates sentiment information into a novel social spammer detection framework. Experimental results on real-world social media datasets show the superior performance of the proposed framework by harnessing sentiment analysis for social spammer detection.
[Sentiment analysis, Laplace equations, information sharing, Computational modeling, information dissemination, sociology, Media, psychological findings, Twitter, unsolicited e-mail, social networking, optimization formulation, optimisation, psychology, social spammer detection, sentiment information, social sciences, social networking (online), Data models, online social media]
TINA: Cross-Modal Correlation Learning by Adaptive Hierarchical Semantic Aggregation
2014 IEEE International Conference on Data Mining
None
2014
With the explosive growth of web data, effective and efficient technologies are in urgent needs for retrieving semantically relevant contents of heterogeneous modalities. Previous studies construct global transformations to project the heterogeneous data into a measurable subspace. However, global projections cannot appropriately adapt to diverse contents, and the naturally existing multi-level semantic relation in web data is ignored. We study the problem of semantic coherent retrieval, where documents from different modalities should be ranked by the semantic relevance to the queries. Accordingly, we propose TINA, a correlation learning method by Adaptive Hierarchical Semantic Aggregation. First, by joint modeling of content and ontology similarities, we build a semantic hierarchy to measure multi-level semantic relevance. Second, with a set of local linear projections aggregated by gating functions, we optimize the structure risk objective function that involves semantic coherence measurement, local projection consistency and the complexity penalty of local projections. Therefore, semantic coherence and a better bias-variance trade-off can be achieved by TINA. Extensive experiments on widely used NUS-WIDE and ICML-Challenge datasets demonstrate that TINA outperforms state-of-the-art, and achieves better adaptation to the multi-level semantic relation and content divergence.
[Visualization, Adaptation models, structure risk objective function, Correlation, Cross-modal retrieval, Local correlation learning, local linear projection, heterogeneous modality, Ontologies, gating function, TINA, cross-modal correlation learning, heterogeneous data, Training, Semantics, joint modeling, learning (artificial intelligence), multilevel semantic relation, semantic hierarchy, NUS-WIDE dataset, Web data, local projection consistency, global transformation, semantic coherent retrieval, content divergence, content similarity, ontology similarity, multilevel semantic relevance, correlation learning method, bias-variance trade-off, adaptive hierarchical semantic aggregation, content-based retrieval, semantically relevant content retrieval, complexity penalty, Semantic hierarchy, relevance feedback, ICML-challenge dataset, Coherence, Internet, semantic coherence measurement]
Diverse Power Iteration Embeddings and Its Applications
2014 IEEE International Conference on Data Mining
None
2014
Spectral Embedding is one of the most effective dimension reduction algorithms in data mining. However, its computation complexity has to be mitigated in order to apply it for real-world large scale data analysis. Many researches have been focusing on developing approximate spectral embeddings which are more efficient, but meanwhile far less effective. This paper proposes Diverse Power Iteration Embeddings (DPIE), which not only retains the similar efficiency of power iteration methods but also produces a series of diverse and more effective embedding vectors. We test this novel method by applying it to various data mining applications (e.g. Clustering, anomaly detection and feature selection) and evaluating their performance improvements. The experimental results show our proposed DPIE is more effective than popular spectral approximation methods, and obtains the similar quality of classic spectral embedding derived from eigen-decompositions. Moreover it is extremely fast on big data applications. For example in terms of clustering result, DPIE achieves as good as 95% of classic spectral clustering on the complex datasets but 4000+ times faster in limited memory environment.
[data analysis, data mining, eigendecompositions, Big Data, DPIE, Vectors, diverse power iteration embeddings, Complexity theory, Data mining, Equations, approximate spectral embeddings, eigenvalues and eigenfunctions, vectors, computation complexity, large scale data analysis, big data applications, dimension reduction algorithms, pattern clustering, Clustering algorithms, Approximation algorithms, embedding vectors, Eigenvalues and eigenfunctions, spectral clustering, computational complexity]
Noise-Resistant Unsupervised Feature Selection via Multi-perspective Correlations
2014 IEEE International Conference on Data Mining
None
2014
Unsupervised feature selection is an important issue for high dimensional dataset analysis. However popular methods are susceptible to noisy instances (observations) or noisy features. We propose a noise-resistant feature selection algorithm by capturing multi-perspective correlations. Our proposed approach, called Noise-Resistant Unsupervised Feature Selection (NRFS), is based on multi-perspective correlation that reflects the importance of feature with respect to noise-resistant representative instances and various global trends from spectral decomposition. In this way, the model concisely captures a wide variety of local patterns. Experimental results demonstrate the effectiveness of our algorithm.
[spectral decomposition, Correlation, data analysis, multiperspective correlations, Noise measurement, Equations, Manifolds, local patterns, noise-resistant representative instances, NRFS, Clustering algorithms, high dimensional dataset analysis, Bismuth, noise-resistant unsupervised feature selection, Mathematical model, noise-resistant feature selection algorithm, feature selection, global trends]
Technology Prospecting for High Tech Companies through Patent Mining
2014 IEEE International Conference on Data Mining
None
2014
Technology prospecting is a process to evaluate the potential business values of high tech companies from the technology perspective. In this paper, we provide a new view-angle to understand technology prospecting by studying the evolving distributions of technologies in the companies. Specifically, we first exploit topic models to learn technological context in the form of probabilistic distributions of assignees and locations from large-scale patent documents. Then, we develop a matching solution to measure the relationships between patent topics and the description documents of technology terms. In this way, we can obtain the distribution of technologies for each company. In addition, we are able to assess the technology prospecting of a company by a designed indicator, which allows to compare the levels of discrepancies between the emerging technology distributions available as Garner Hype Cycles and the distribution of technologies of the company. Finally, experimental results on real-world patent data show the effectiveness of our approach for technology prospecting.
[Patents, Electronic publishing, pattern matching, data mining, Companies, Encyclopedias, business values, large-scale patent documents, Technology Prospecting, patent topics, Garner Hype Cycles, statistical distributions, probabilistic distributions, Topic Modeling, technology prospecting, Hype Cycle, technology management, high tech companies, real-world patent data mining, patents, innovation management, Internet, Patent Mining, technology distribution]
News Credibility Evaluation on Microblog with a Hierarchical Propagation Model
2014 IEEE International Conference on Data Mining
None
2014
Benefiting from its openness, collaboration and real-time features, Micro blog has become one of the most important news communication media in modern society. However, it is also filled with fake news. Without verification, such information could spread promptly through social network and result in serious consequences. To evaluate news credibility on Micro blog, we propose a hierarchical propagation model. We detect sub-events within a news event to describe its detailed aspects. Thus, for a news event, a three-layer credibility network consisting of event, sub-events and messages can represent it from different scale and reveal vital information for credibility evaluation. After linking these entities with their semantic and social associations, the credibility value of each entity is propagated on this network to achieve the final evaluation result. By formulating this propagation process as a graph optimization problem, we provide a globally optimal solution with an iterative algorithm. Experiments conducted on two real-world datasets show that the proposed model boosts the accuracy by more than 6% and the F-score by more than 16% over a baseline method.
[iterative methods, Symmetric matrices, graph optimization problem, rumor detection, news communication media, Media, hierarchical propagation model, Vectors, information analysis, Microblog, iterative algorithm, Optimization, social network, optimisation, news credibility, Semantics, Clustering algorithms, three-layer credibility network, Feature extraction, social networking (online), Social media credibility, microblog, news credibility evaluation]
Neural Conditional Energy Models for Multi-label Classification
2014 IEEE International Conference on Data Mining
None
2014
Multi-label classification (MLC) is a type of structured output prediction problems where a given instance can be associated to more than one labels at a time. From the probabilistic point of view, a model predicts a set of labels y given an input vector v by learning a conditional distribution p(y|v). This paper presents a powerful model called a Neural Conditional Energy Model (NCEM) to solve MLC. The model can be viewed as a hybrid deterministic-stochastic network of which we use a deterministic neural network to transform the input data, before contributing to the energy landscape of v, y, and a single stochastic hidden layer h. Non-linear transformation given by the neural network makes our model more expressive and more capable of capturing complex relations between input and output, and using deterministic neurons facilitates exact inference. We present an efficient learning algorithm that is simple to implement. We conduct extensive experiments on 15 real-world datasets from wide variety of domains with various evaluation metrics to confirm that NCEM is significantly superior to current state-of-the-art models most of the time based on pair-wise t-test at 5% significance level. The MATLAB source code to replicate our experiments are available at https://github.com/Kublai-Jing/NCEM.
[Correlation, nonlinear transformation, mathematics computing, MLC, probabilistic point of view, Stochastic processes, stochastic hidden layer, Multi-Label Classification, neural conditional energy models, deterministic neural network, Training, vector, Mathematical model, learning (artificial intelligence), stochastic processes, pattern classification, Computational modeling, probability, Probabilistic Modeling, Vectors, conditional distribution, vectors, multilabel classification, NCEM, Data models, Matlab source code, neural nets]
A Transfer Probabilistic Collective Factorization Model to Handle Sparse Data in Collaborative Filtering
2014 IEEE International Conference on Data Mining
None
2014
Data Sparsity incurs serious concern in collaborative filtering (CF). This issue is especially critical for newly launched CF applications where observed ratings are too scarce to learn a good model to predict missing values. There could be, however, information from other related domains which are with relatively denser data that can be utilized. This paper proposes a transfer-learning based approach that exploits probabilistic matrix factorization model trained with variational expectation-maximization (VIM) to resolve data sparsity by using information from multiple auxiliary domains. We conduct experiments on several data combination and report significant improvements over state-of-the-art transfer-based models for collaborative filtering. The results also show that our framework is the only solution that can achieve acceptable performance when each user has only one single rating. The code of our model is available at https://github.com/Kublai-Jing/TIC https://github.com/Kublai-Jing/TIC.
[Adaptation models, collaborative filtering, sparse data handling, transfer-learning based approach, Data Sparsity, Collaborative Filtering, Probabilistic Modeling, transfer probabilistic collective factorization model, Probabilistic logic, matrix decomposition, variational expectation-maximization, Equations, Collaboration, variational techniques, expectation-maximisation algorithm, Motion pictures, Data models, data handling, data sparsity, VIM, probabilistic matrix factorization model, Mathematical model, learning (artificial intelligence)]
An Examination of Multivariate Time Series Hashing with Applications to Health Care
2014 IEEE International Conference on Data Mining
None
2014
As large-scale multivariate time series data become increasingly common in application domains, such as health care and traffic analysis, researchers are challenged to build efficient tools to analyze it and provide useful insights. Similarity search, as a basic operator for many machine learning and data mining algorithms, has been extensively studied before, leading to several efficient solutions. However, similarity search for multivariate time series data is intrinsically challenging because (1) there is no conclusive agreement on what is a good similarity metric for multivariate time series data and (2) calculating similarity scores between two time series is often computationally expensive. In this paper, we address this problem by applying a generalized hashing framework, namely kernelized locality sensitive hashing, to accelerate time series similarity search with a series of representative similarity metrics. Experiment results on three large-scale clinical data sets demonstrate the effectiveness of the proposed approach.
[dynamic time warping, data mining algorithms, data mining, hashing, large-scale clinical data sets, search, Databases, nearest neighbor, large-scale multivariate time series data, representative similarity metrics, similarity, multivariate time series hashing, learning (artificial intelligence), Kernel, health care, time series similarity search, Time series analysis, time series, Time measurement, Vectors, machine learning, kernelized locality sensitive hashing, Euclidean distance, kernel methods, alignment]
Probabilistic Latent Document Network Embedding
2014 IEEE International Conference on Data Mining
None
2014
A document network refers to a data type that can be represented as a graph of vertices, where each vertex is associated with a text document. Examples of such a data type include hyperlinked Web pages, academic publications with citations, and user profiles in social networks. Such data have very high-dimensional representations, in terms of text as well as network connectivity. In this paper, we study the problem of embedding, or finding a low-dimensional representation of a document network that "preserves" the data as much as possible. These embedded representations are useful for various applications driven by dimensionality reduction, such as visualization or feature selection. While previous works in embedding have mostly focused on either the textual aspect or the network aspect, we advocate a holistic approach by finding a unified low-rank representation for both aspects. Moreover, to lend semantic interpretability to the low-rank representation, we further propose to integrate topic modeling and embedding within a joint model. The gist is to join the various representations of a document (words, links, topics, and coordinates) within a generative model, and to estimate the hidden representations through MAP estimation. We validate our model on real-life document networks, showing that it outperforms comparable baselines comprehensively on objective evaluation metrics.
[Visualization, semantic interpretability, visualization, graph theory, topic modeling, data preservation, low-dimensional representation, dimensionality reduction, document network, unified low-rank representation, Semantics, embedded systems, embedding, Mathematical model, probabilistic latent document network embedding, Joints, feature selection, embedding problem, document handling, MAP estimation, real-life document networks, probability, Educational institutions, Data visualization, Nickel, generative model]
Locating POS Terminals from Credit Card Transactions
2014 IEEE International Conference on Data Mining
None
2014
Credit card is a popular payment method and the transaction data keeps track of purchasing activities in people's daily lives. Extracting location of people's activities is an important task in many data mining problems because it may greatly help improve user experience and the service provided to people. Locating people from credit card transactions is equivalent to determining the location of every POS terminal where a payment takes place. This is however not an easy task because the locations of terminals are not usually provided to the credit card issuing companies and only a few terminals can be unambiguously located through map service by providing the merchants' names. In this paper, we propose a system to infer the locations of POS terminals using transaction data and map service. We first construct a transaction graph where the nodes are POS terminals. We then propose a two phase algorithm to find out uncertain and unknown locations of the terminals. In the first phase, we try to eliminate the uncertainty of POS terminals with multiple candidate locations. We show this problem is NP-hard and then give an effective heuristic algorithm to solve it. In the second phase, we compute the locations of unknown POS terminals by propagating the locations of known ones with spatial-temporal constraints. The algorithm is evaluated using a real-world credit card transaction data set and the result is promising for business applications.
[payment method, Uncertainty, NP-hard, graph theory, purchasing, data mining, Companies, Electronic mail, Data mining, credit card transaction, credit card transactions, transaction graph, Trajectory, business applications, spatial-temporal constraints, Credit cards, credit transactions, location extraction, POS terminal locating, transaction data, POS, location, purchasing activities, map service, two phase algorithm, computational complexity]
Detecting Campaign Promoters on Twitter Using Markov Random Fields
2014 IEEE International Conference on Data Mining
None
2014
As social media is becoming an increasingly important source of public information, companies, organizations and individuals are actively using social media platforms to promote their products, services, ideas and ideologies. Unlike promotional campaigns on TV or other traditional mass media platforms, campaigns on social media often appear in stealth modes. Campaign promoters often try to influence people's behaviors/opinions/decisions in a latent manner such that the readers are not aware that the messages they see are strategic campaign posts aimed at persuading them to buy target products/services. Readers take such campaign posts as just organic posts from the general public. It is thus important to discover such campaigns, their promoter accounts and how the campaigns are organized and executed as it can uncover the dynamics of Internet marketing. This discovery is clearly useful for competitors and also the general public. However, so far little work has been done to solve this problem. In this paper, we study this important problem in the context of the Twitter platform. Given a set of tweets streamed from Twitter based on a set of keywords representing a particular topic, the proposed technique aims to identify user accounts that are involved in promotion. We formulate the problem as a relational classification problem and solve it using typed Markov Random Fields (T-MRF), which is proposed as a generalization of the classic Markov Random Fields. Our experiments are carried out using three real-life datasets from the health science domain related to smoking. Such campaigns are interesting to health scientists, government health agencies and related businesses for obvious reasons. Our results show that the proposed method is highly effective.
[Context, campaign promoter detection, pattern classification, Markov Random Fields, random processes, Media, Twitter, Internet marketing, marketing data processing, typed Markov random fields, Markov random fields, relational classification problem, Uniform resource locators, strategic campaign posts, Campaign Promoter, social media platforms, Markov processes, social networking (online), Random variables, Internet, T-MRF, public information, Belief propagation, health science domain]
LRBM: A Restricted Boltzmann Machine Based Approach for Representation Learning on Linked Data
2014 IEEE International Conference on Data Mining
None
2014
Linked data consist of both node attributes, e.g., Preferences, posts and degrees, and links which describe the connections between nodes. They have been widely used to represent various network systems, such as social networks, biological networks and etc. Knowledge discovery on linked data is of great importance to many real applications. One of the major challenges of learning linked data is how to effectively and efficiently extract useful information from both node attributes and links in linked data. Current studies on this topic either use selected topological statistics to represent network structures, or linearly map node attributes and network structures to a shared latent feature space. However, while approaches based on statistics may miss critical patterns in network structure, approaches based on linear mappings may not be sufficient to capture the non-linear characteristics of nodes and links. To handle the challenge, we propose, to our knowledge, the first deep learning method to learn from linked data. A restricted Boltzmann machine model named LRBM is developed for representation learning on linked data. In LRBM, we aim to extract the latent feature representation of each node from both node attributes and network structures, non-linearly map each pair of nodes to the links, and use hidden units to control the mapping. The details of how to adapt LRBM for link prediction and node classification on linked data have also been presented. In the experiments, we test the performance of LRBM as well as other baselines on link prediction and node classification. Overall, the extensive experimental evaluations confirm the effectiveness of the proposed LRBM model in mining linked data.
[restricted Boltzmann machine model, Social network services, semantic Web, representation learning, data mining, Receivers, Probability distribution, knowledge discovery, Representation Learning, Data mining, Boltzmann machines, Deep Learning, Tensile stress, LRBM, Linked Data mining, knowledge representation, latent feature representation, Linked Data, Restricted Boltzmann Machine, Feature extraction, Data models, learning (artificial intelligence), deep learning method]
Early Classification of Ongoing Observation
2014 IEEE International Conference on Data Mining
None
2014
This work focuses on early classification of ongoing observation of the object, which is beneficial for a number of applications that require time-critical decision making. We propose an approach for discovering two key aspects of multivariate time series (m.t.s.) observation, (1) Temporal Dynamics and (2) Sequential Cues. The key idea is that m.t.s. Observation can be represented as an instantiation of a Multivariate Marked Point-Process (Multi-MPP). Each variable characterizes the temporal dynamics of a particular feature event of an object, where both timing and strength information of that feature event are preserved. To make this model computationally practical, we introduce the Multilevel-Discretized Marked Point-Process (MD-MPP) model which can ensure a good piece-wise stationary property both in the time-domain and mark-space while preserving dynamics as much as possible. Based on this model, another important temporal patterns of early classification, sequential cues among variables, becomes formalizable. We construct a probabilistic suffix tree to represent sequential patterns among features in terms of Variable order Markov Model (VMM). The effectiveness of our approach is evaluated on three experimental scenarios. Our method achieves superior performance for early classification of ongoing m.t.s. Observation data.
[variable order Markov model, Correlation, Heuristic algorithms, mathematics computing, Stochastic processes, data mining, multilevel-discretized marked point-process, temporal dynamics, Training, Detectors, sequential cues, learning (artificial intelligence), multivariate marked point-process, pattern classification, Computational modeling, Time series analysis, time series, Temporal Dynamics, machine learning, MD-MPP, early classification, multivariate time series observation, Multi-MPP, decision making, VMM, Markov processes, Time Series, Sequential Cue, Early Classification, time-critical decision making]
Identifying Recurrent and Unknown Performance Issues
2014 IEEE International Conference on Data Mining
None
2014
For a large-scale software system, especially an online service system, when a performance issue occurs, it is desirable to check whether this issue has occurred before. If there are past similar issues, a known remedy could be applied. Otherwise, a new troubleshooting process may have to be initiated. The symptom of a performance issue can be characterized by a set of metrics. Due to the sophisticated nature of software systems, manual diagnosis of performance issues based on metric data is typically expensive and laborious. In this paper, we propose a Hidden Markov Random Field (HMRF) based approach to automatic identification of recurrent and unknown performance issues. We formulate the problem of issue identification as a HMRF-based clustering problem. Our approach incorporates the learning of metric discretization thresholds and the optimization of issue clustering. Based on the learned thresholds and cluster centroids, we can achieve accurate identification of recurrent issues and unknown issues. Experimental evaluations on an open benchmark and a large-scale industrial production system show that our approach is effective and outperforms the related state-of-the-art approaches.
[Measurement, Production systems, metric discretization thresholds, Fingerprint recognition, cluster centroids, large-scale software system, hidden Markov models, online service system, Clustering algorithms, unknown performance issues, issue identification, Monitoring, Issue identification, issue clustering, Vectors, metric data, large-scale industrial production system, duplication detection, performance, pattern clustering, Hidden Markov models, HMRF-based clustering problem, troubleshooting process, metrics, automated diagnosis, hidden Markov random field, automatic identification]
Steering Information Diffusion Dynamically against User Attention Limitation
2014 IEEE International Conference on Data Mining
None
2014
As viral marketing in online social networks flourishes recently, a lot of attention has been drawn to the study of influence maximization in social networks. However, most works in influence maximization have overlooked the important role that social network providers (websites) play in the diffusion processes. Viral marketing campaigns are usually sold by websites as services to their clients. The websites can not only select initial sets of users to start diffusion processes, but can also have impacts throughout the diffusion processes by deciding when the information should be brought to the attention of individual users. This is especially true when user attention is limited, and the websites have to notify users about an item to bring it into the attention of users. In this paper, we study the diffusion of information from the perspective of social network websites. We propose a novel push-driven cascade (PDC) model, which emphasizes the role of websites during the diffusion of information. In the PDC model, the website "pushes" items to bring them to the attention of users, and whether a user is interested in an item is decided by her preference and the social influence from her friends. Analogous to the influence maximization problem on the traditional information diffusion models, we propose a dynamic influence maximization problem on the PDC model, which is defined as a sequential decision making problem for the website. We show that the problem can be formalized as a Markov sequential decision problem, and there exists a deterministic Markovian policy that is an optimal solution for the problem. We develop an AO algorithm that finds the optimal solution for the problem, and a heuristic online search algorithm, which has similar effectiveness, but is significantly more efficient. We evaluate the proposed algorithms on various real-world datasets, and find them significantly outperform the baselines.
[user attention limitation, Heuristic algorithms, online social networks, social network Web sites, Twitter, social network providers, History, Markov sequential decision problem, optimisation, influence maximization problem, sequential decision making problem, push-driven cascade model, PDC model, Mathematical model, information diffusion, search problems, information dissemination, Diffusion processes, marketing data processing, optimal solution, Markovian policy, AO* algorithm, viral marketing campaigns, user attention, Markov processes, social networking (online), influence maximization, heuristic online search algorithm, push-driven cascade]
UnTangle: Visual Mining for Data with Uncertain Multi-labels via Triangle Map
2014 IEEE International Conference on Data Mining
None
2014
Data with multiple uncertain labels are common in many situations. For examples, a movie may be associated with multiple genres with different levels of confidence, and a protein sequence may be probabilistically assigned to several structural subcategories. Despite their ubiquity, the problem of visualizing uncertain labels has not been adequately addressed. Existing approaches often either discard the uncertainty information, or map the data to a low-dimensional subspace where their associations with multiple labels are obscured. In this paper, we propose a novel visual mining technique, UnTangle, for visualizing uncertain multi-labels. In our proposed visualization, data items are placed inside a web of connected triangles, with labels assigned to the triangle vertices such that nearby labels are more relevant to each other. The positions of the data items are determined based on the probabilistic associations between items and labels. UnTangle provides both (a) an automatic label placement algorithm, and (b) adaptive interaction mechanisms that allow users to control the label positioning for different visual queries. Our work makes a unique contribution by providing an effective way to investigate the relationship between data items and their uncertain labels, as well as the relationships among labels. Our user study suggests that the visualization effectively helps users discover emergent patterns and compare the nuances of uncertainty information in the data labels.
[Visualization, Uncertainty, multiple uncertain labels, data mining, adaptive interaction mechanisms, Data mining, visual mining, uncertain multilabels, emergent pattern discovery, Distributed databases, data visualisation, probabilistic associations, uncertainty data, label positioning control, multi-labels, Motion pictures, ternary plot, probablistic labels, multiple genres, Probabilistic logic, visual queries, UnTangle, low-dimensional subspace, visualizing uncertain labels, triangle map, Data visualization, data items, uncertainty information, automatic label placement algorithm, protein sequence, uncertain multilabel visualization]
Social Marketing Meets Targeted Customers: A Typical User Selection and Coverage Perspective
2014 IEEE International Conference on Data Mining
None
2014
The emergence of social networks has provided opportunities for both targeted marketing and viral marketing. By concentrating the efforts on a few key customers, targeted marketing could make the promotion of the items (products) much easier and more cost-effective. On the other hand, viral marketing aims at finding a set of individuals (seeds) to maximize the word-of-mouth propagation of an item. However, these two marketing strategies can only exploit some specific characteristics of the social networks, and the problem of how to combine them together to build a better, stronger business is still open. To that end, in this paper, we propose a general approach for integrated marketing. Specifically, to market a given item, we first generate the item-specific candidate users by a recommendation algorithm, and then select the typical users who have the best balanced utility scores and consumption/social entropy. Next, treating typical users as targeted customers, we study the problem of maximizing information awareness in viral marketing with these constrained targets. Along this line, we define it as a constrained coverage maximization problem, and propose three solutions: GMIC, LMIC and QMIC. Finally, extensive experimental results on real-world datasets demonstrate that our integrated marketing approach could outperform the methods that consider only targeted marketing or viral marketing.
[Greedy algorithms, Social Marketing, Entropy, Targeted Marketing, social marketing, social network, optimisation, QMIC, viral marketing, recommendation algorithm, Viral Marketing, constrained coverage maximization problem, feature selection, GMIC, Social network services, Computational modeling, Linear programming, marketing data processing, Cultural differences, Recommendation, recommender systems, targeted marketing, Collaboration, LMIC, social networking (online), user selection]
Exploiting Heterogeneous Human Mobility Patterns for Intelligent Bus Routing
2014 IEEE International Conference on Data Mining
None
2014
Optimal planning for public transportation is one of the keys to sustainable development and better quality of life in urban areas. Compared to private transportation, public transportation uses road space more efficiently and produces fewer accidents and emissions. In this paper, we focus on the identification and optimization of flawed bus routes to improve utilization efficiency of public transportation services, according to people's real demand for public transportation. To this end, we first provide an integrated mobility pattern analysis between the location traces of taxicabs and the mobility records in bus transactions. Based on mobility patterns, we propose a localized transportation mode choice model, with which we can accurately predict the bus travel demand for different bus routing. This model is then used for bus routing optimization which aims to convert as many people from private transportation to public transportation as possible given budget constraints on the bus route modification. We also leverage the model to identify region pairs with flawed bus routes, which are effectively optimized using our approach. To validate the effectiveness of the proposed methods, extensive studies are performed on real world data collected in Beijing which contains 19 million taxi trips and 10 million bus trips.
[optimal planning, road space, Roads, budget constraint, taxicabs, Optimization, utilization efficiency, localized transportation mode choice model, optimisation, mobility record, bus route modification, road vehicles, bus travel demand, Cities and towns, bus routing optimization, heterogeneous human mobility pattern, Routing, intelligent transportation systems, Vectors, private transportation, public transportation services, transportation, Bridges, sustainable development, human mobility pattern, bus routing, bus transaction, integrated mobility pattern analysis, intelligent bus routing, vehicle routing, public transport, quality of life, flawed bus route]
Anomaly Detection Using the Poisson Process Limit for Extremes
2014 IEEE International Conference on Data Mining
None
2014
Anomaly detection starts from a model of normal behavior and classifies departures from this model as anomalies. This paper introduces a statistical non-parametric approach for anomaly detection that is based on a multivariate extension of the Poisson point process model for univariate extremes. The method is demonstrated on both a synthetic and a real-world data set, the latter being an unbalanced data set of acceleration data collected from movements of 7 pediatric patients suffering from epilepsy that is previously studied in [1]. The positive predictive values could be improved with an increase up to 12.9% (and a mean of 7%) while the sensitivity scores stayed unaltered. The proposed method was also shown to outperform an one-class SVM classifier. Because the Poisson point process model of extremes is able to combine information on the number of excesses over a fixed threshold with that on the excess values, a powerful model to detect anomalies is obtained that can be of high value in many applications.
[epilepsy, Poisson point process model, multivariate extension, anomaly detection, unbalanced data, pediatric patients, statistical nonparametric approach, univariate extremes, stochastic processes, Kernel, Testing, semi-supervised, synthetic data set, Estimation, real-world data set, sensitivity scores, Vectors, one-class SVM classifier, positive predictive values, security of data, fixed threshold, Poisson point process, Hidden Markov models, extreme value statistics, Brain modeling, Data models, nonparametric statistics, unbalanced data set]
Ratable Aspects over Sentiments: Predicting Ratings for Unrated Reviews
2014 IEEE International Conference on Data Mining
None
2014
Most existing rat able aspect generating methods for aspect mining focus on identifying and rating aspects of reviews with overall ratings, while huge amount of unrated reviews are beyond their ability. This drawback motivates the research problem in this paper: predicting aspect ratings and overall ratings for unrated reviews. To solve this problem, we novelly propose a topic model based on Latent Dirichlet Allocation with indirect supervision. Compared with the previous bag-of-words representation of review documents, we utilize the quad-tuples of (head, modifier, rating, entity) to explicitly model the associations between modifiers and ratings. Specifically, our solution for aspect mining in unrated reviews is decomposed into three steps. Firstly, rat able aspects are generated over sentiments from training reviews with overall ratings. Afterwards, inference of aspect identification and rating for unrated reviews are provided. Finally, overall ratings are predicted for unrated reviews. Under this framework, aspect and sentiment associations are captured in the form of joint probabilities through a generative process. The effectiveness of our approach is testified on a real-world dataset crawled from Trip Advisor http://www.tripadvisor.com/, and extensive experiments show that our method significantly outperforms state-of-the-art methods.
[document handling, latent Dirichlet allocation, aspect mining, data mining, Gaussian distribution, Aspect Identification, ratable aspect generating methods, Equations, Aspect Rating Prediction, Training, Overall Rating Prediction, bag-of-words representation, Feature extraction, sentiment associations, Inference algorithms, Numerical models, Mathematical model, review documents]
Fast and Exact Monitoring of Co-Evolving Data Streams
2014 IEEE International Conference on Data Mining
None
2014
Given a huge stream of multiple co-evolving sequences, such as motion capture and web-click logs, how can we find meaningful patterns and spot anomalies? Our aim is to monitor data streams statistically, and find sub sequences that have the characteristics of a given hidden Markov model (HMM). For example, consider an online web-click stream, where massive amounts of access logs of millions of users are continuously generated every second. So how can we find meaningful building blocks and typical access patterns such as weekday/weekend patterns, and also, detect anomalies and intrusions? In this paper, we propose Stream Scan, a fast and exact algorithm for monitoring multiple co-evolving data streams. Our method has the following advantages: (a) it is effective, leading to novel discoveries and surprising outliers, (b) it is exact, and we theoretically prove that Stream Scan guarantees the exactness of the output, (c) it is fast, and requires O (1) time and space per time-tick. Our experiments on 67GB of real data illustrate that Stream Scan does indeed detect the qualifying subsequence patterns correctly and that it can offer great improvements in speed (up to 479,000 times) over its competitors.
[Algorithm design and analysis, Legged locomotion, access pattern detection, Heuristic algorithms, hidden Markov model, StreamScan, HMM, Co-evolving data streams, co-evolving data stream monitoring, hidden Markov models, Viterbi algorithm, Hidden Markov models, media streaming, Monitoring, Periodic structures, computational complexity]
Ternary Matrix Factorization
2014 IEEE International Conference on Data Mining
None
2014
Can we learn from the unknown? Logical data sets of the ternary kind are often found in information systems. They contain unknown as well as true/false values. An unknown value may represent a missing entry (lost or indeterminable) or something with meaning, like a "Don't Know" response in a questionnaire. In this paper we introduce an effectively- and efficiently-superior algorithm for reducing the dimensionality of logical data (categorical data in general) in the context of a new data mining challenge: Ternary Matrix Factorization (TMF). For a ternary data matrix, TMF exploits ternary logic to produce a basis matrix (which holds the major patterns in the data) and a usage matrix (which maps patterns to original observations). Both matrices are interpretable, and their ternary matrix product approximates the original matrix. TMF has applications in 1) finding targeted structure in ternary data, 2) imputing values through pattern-discovery in highly-incomplete categorical data sets, and 3) solving instances of its encapsulated Binary Matrix Factorization (BMF) problem. Our elegant algorithm Faster (Fast Ternary Matrix Factorization) has linear run-time complexity with respect to the dimensions of the data set and is parameter-robust. Experiments on synthetic and real-world data sets show that we are able to efficiently and effectively outperform state-of-the-art techniques in all three TMF applications.
[Uncertainty, Three-valued logic, fast ternary matrix factorization, ternary data matrix, data mining, Multivalued logic, Complexity theory, matrix decomposition, Optimization, dimensionality reduction, logical data dimensionality reduction, imputation, usage matrix, ternary data, basis matrix, missing values, Context, matrix factorization, encapsulated binary matrix factorization problem, Vectors, Matrix decomposition, TMF, highly-incomplete categorical data sets, pattern-discovery, FasTer, ternary logic, encapsulated BMF problem]
Classification by CUT: Clearance under Threshold
2014 IEEE International Conference on Data Mining
None
2014
Identifying bad objects hidden amidst many good objects is important for public safety and decision-making. These problems are complicated in that the cost of leaving a bad object unidentified may not be specified easily, making it difficult to apply existing cost-sensitive classification that depends on knowing a cost matrix or cost distribution. A compelling case for this "illusive cost" issue is presented in our project of identifying contaminated transformers with an industrial partner. To address this problem, we present an alternative formulation of cost-sensitive classification, Clearance Under Threshold (CUT) Classification. Given a training set, CUT classification is to partition the attribute space such that a partition is cleared if the probability of a future object in this partition being bad is less than a user-specified threshold. The goal is to clear many low-risk objects so that users can more effectively target high-risk objects. We present a solution to this problem and evaluate it on a case study for clearing contaminated transformers and on public benchmarks from UC Irvine's Machine Learning Repository. According to the experiments, our algorithms performed far better than the baselines derived from previous classification approaches.
[object recognition, Transmission line matrix methods, Oil insulation, CUT classification, Training, cost matrix, illusive cost issue, Sociology, public safety, Classification, Decision trees, learning (artificial intelligence), hidden object identification, cost distribution, pattern classification, clearance under threshold classification, cost-sensitive classification, probability, UC Irvine machine learning repository, Educational institutions, Classification for Imbalanced Data, future object probability, contaminated transformers, public benchmarks, high-risk objects, decision making, Power transformer insulation]
Modeling Adoptions and the Stages of the Diffusion of Innovations
2014 IEEE International Conference on Data Mining
None
2014
We study the data mining problem of modeling adoptions and the stages of the diffusion of an innovation. For our aim we propose a stochastic model which decomposes a diffusion trace (sequence of adoptions) in an ordered sequence of stages, where each stage is intuitively built around two dimensions: users and relative speed at which adoptions happen. Each stage is characterized by a specific rate of adoption and it involves different users to different extent, while the sequentiality in the diffusion is guaranteed by constraining the transition probabilities among stages. An empirical evaluation on synthetic and real-world adoption logs shows the effectiveness of the proposed framework in summarizing the adoption process, enabling several analysis tasks such as the identification of adopter categories, clustering and characterization of diffusion traces, and prediction of which users will adopt an item in the next future.
[innovation diffusion, Technological innovation, transition probability, Social network services, data mining, probability, modeling adoption, diffusion trace, task analysis, Statistics, adoption process, Sociology, Hidden Markov models, synthetic adoption, adopter category, Data models, real-world adoption, Mathematical model, stochastic processes, data mining problem, stochastic model]
Stock Market Prediction from WSJ: Text Mining via Sparse Matrix Factorization
2014 IEEE International Conference on Data Mining
None
2014
We revisit the problem of predicting directional movements of stock prices based on news articles: here our algorithm uses daily articles from The Wall Street Journal to predict the closing stock prices on the same day. We propose a unified latent space model to characterize the "co-movements" between stock prices and news articles. Unlike many existing approaches, our new model is able to simultaneously leverage the correlations: (a) among stock prices, (b) among news articles, and (c) between stock prices and news articles. Thus, our model is able to make daily predictions on more than 500 stocks (most of which are not even mentioned in any news article) while having low complexity. We carry out extensive back testing on trading strategies based on our algorithm. The result shows that our model has substantially better accuracy rate (55.7%) compared to many widely used algorithms. The return (56%) and Sharpe ratio due to a trading strategy based on our model are also much higher than baseline indices.
[text analysis, Correlation, trading strategy, data mining, stock price, news article, Predictive models, sparse matrix factorization, matrix decomposition, Optimization, unified latent space model, Accuracy, stock market prediction, Prediction algorithms, The Wall Street Journal, text mining, stock markets, electronic trading, Educational institutions, Vectors, directional movement, WSJ, Sharpe ratio, sparse optimization, computational finance, sparse matrices]
A Scalable Method for Exact Sampling from Kronecker Family Models
2014 IEEE International Conference on Data Mining
None
2014
The recent interest in modeling complex networks has fueled the development of generative graph models, such as Kronecker Product Graph Model (KPGM) and mixed KPGM (mKPGM). The Kronecker family of models are appealing because of their elegant fractal structure, as well as their ability to capture important network characteristics such as degree, diameter, and (in the case of mKPGM) clustering and population variance. In addition, scalable sampling algorithms for KPGMs made the analysis of large-scale, sparse networks feasible for the first time. In this work, we show that the scalable sampling methods, in contrast to prior belief, do not in fact sample from the underlying KPGM distribution and often result in sampling graphs that are very unlikely. To address this issue, we develop a new representation that exploits the structure of Kronecker models and facilitates the development of novel grouped sampling methods that are provably correct. In this paper, we outline efficient algorithms to sample from mKPGMs and KPGMs based on these ideas. Notably, our mKPGM algorithm is the first available scalable sampling method for this model and our KPGM algorithm is both faster and more accurate than previous scalable methods. We conduct both theoretical analysis and empirical evaluation to demonstrate the strengths of our algorithms and show that we can sample a network with 75 million edges in 87 seconds on a single processor.
[Algorithm design and analysis, sampling methods, fractal structure, graph theory, Kronecker family models, Probability distribution, Fractals, exact sampling, complex networks, Indexes, generative graph models, Kronecker product graph model, mixed KPGM, scalable sampling methods, mKPGM, Time complexity]
Time Series Join on Subsequence Correlation
2014 IEEE International Conference on Data Mining
None
2014
We consider the problem of joining two long time series based on their most correlated segments. Two time series can be joined at any locations and for arbitrary length. Such join locations and length provide useful knowledge about the synchrony of the two time series and have applications in many domains including environmental monitoring, patient monitoring and power monitoring. However, join on correlation is a computationally expensive task, specially when the time series are large. The naive algorithm requires O (n4) computation where n is the length of the time series. We propose an algorithm, named Jocor, that uses two algorithmic techniques to tackle the complexity. First, the algorithm reuses the computation by caching sufficient statistics and second, the algorithm prunes unnecessary correlation computation by admissible heuristics. The algorithm runs orders of magnitude faster than the naive algorithm and enables us to join long time series as well as many small time series. We propose a variant of Jocor for fast approximation and an extension to a GPU-based parallel method to bring down the running-time to interactive level for analytics applications. We show three independent uses of time series join on correlation which are made possible by our algorithm.
[Correlation, environmental monitoring, Similarity, mathematics computing, Jocor, parallel processing, patient monitoring, Matching, arbitrary length, Alignment, power monitoring, naive algorithm, approximation theory, approximation, Time series analysis, time series joining, Educational institutions, time series, graphics processing units, Equations, Standards, Computer science, GPU-based parallel method, Euclidean distance, Time Series, statistical analysis, subsequence correlation, statistics]
Locally Estimating Core Numbers
2014 IEEE International Conference on Data Mining
None
2014
Graphs are a powerful way to model interactions and relationships in data from a wide variety of application domains. In this setting, entities represented by vertices at the 'center' of the graph are often more important than those associated with vertices on the 'fringes'. For example, central nodes tend to be more critical in the spread of information or disease and play an important role in clustering/community formation. Identifying such 'core' vertices has recently received additional attention in the context of network experiments, which analyze the response when a random subset of vertices are exposed to a treatment (e.g. Inoculation, free product samples, etc). Specifically, the likelihood of having many central vertices in any exposure subset can have a significant impact on the experiment. We focus on using k-cores and core numbers to measure the extent to which a vertex is central in a graph. Existing algorithms for computing the core number of a vertex require the entire graph as input, an unrealistic scenario in many real world applications. Moreover, in the context of network experiments, the sub graph induced by the treated vertices is only known in a probabilistic sense. We introduce a new method for estimating the core number based only on the properties of the graph within a region of radius &#x03B4; around the vertex, and prove an asymptotic error bound of our estimator on random graphs. Further, we empirically validate the accuracy of our estimator for small values of &#x03B4; on a representative corpus of real data sets. Finally, we evaluate the impact of improved local estimation on an open problem in network experimentation posed by Ugander et al.
[application domains, empirical analysis, Communities, graph theory, network theory (graphs), real data sets, random graphs, set theory, real world applications, asymptotic error bound, central nodes, Accuracy, graph properties, core vertices, graph vertices, random vertex subset, radius region, unrealistic scenario, network experiments, community formation, graph algorithms, subgraphs, local core number estimation, Estimation, probability, clustering formation, time complexity, central vertices, Computational complexity, Equations, core numbers, Upper bound, data interactions, response analysis, Data models, exposure subset, graph fringes, data relationships, k-cores, computational complexity]
Dynamic Time Warping Averaging of Time Series Allows Faster and More Accurate Classification
2014 IEEE International Conference on Data Mining
None
2014
Recent years have seen significant progress in improving both the efficiency and effectiveness of time series classification. However, because the best solution is typically the Nearest Neighbor algorithm with the relatively expensive Dynamic Time Warping as the distance measure, successful deployments on resource constrained devices remain elusive. Moreover, the recent explosion of interest in wearable devices, which typically have limited computational resources, has created a growing need for very efficient classification algorithms. A commonly used technique to glean the benefits of the Nearest Neighbor algorithm, without inheriting its undesirable time complexity, is to use the Nearest Centroid algorithm. However, because of the unique properties of (most) time series data, the centroid typically does not resemble any of the instances, an unintuitive and underappreciated fact. In this work we show that we can exploit a recent result to allow meaningful averaging of 'warped' times series, and that this result allows us to create ultra-efficient Nearest 'Centroid' classifiers that are at least as accurate as their more lethargic Nearest Neighbor cousins.
[pattern classification, wearable devices, Nearest centroid, nearest centroid classifiers, Heuristic algorithms, Time series analysis, nearest neighbor algorithm, Time series classification, Artificial neural networks, time complexity, time series, Classification algorithms, resource constrained devices, Training, Accuracy, Nearest Neighbor, dynamic time warping averaging, Prototypes, warped time series averaging, Dynamic Time Warping, Time series averaging, computational complexity, time series classification]
A Statistically Efficient and Scalable Method for Log-Linear Analysis of High-Dimensional Data
2014 IEEE International Conference on Data Mining
None
2014
Log-linear analysis is the primary statistical approach to discovering conditional dependencies between the variables of a dataset. A good log-linear analysis method requires both high precision and statistical efficiency. High precision means that the risk of false discoveries should be kept very low. Statistical efficiency means that the method should discover actual associations with as few samples as possible. Classical approaches to log-linear analysis make use of &#x03C7;2 tests to control this balance between quality and complexity. We present an information-theoretic approach to log-linear analysis. We show that our approach 1) requires significantly fewer samples to discover the true associations than statistical approaches -- statistical efficiency -- 2) controls for the risk of false discoveries as well as statistical approaches -- high precision - and 3) can perform the discovery on datasets with hundreds of variables on a standard desktop computer -- computational efficiency.
[High-dimensional data, Maximum likelihood estimation, Head, information-theoretic approach, Computational modeling, Particle separators, data mining, high-dimensional data, Association discovery, Encoding, log-linear analysis, Standards, statistical efficiency, Graphical models, Data modeling, Data models, Statistical inference, statistical analysis, Log-linear Analysis, Information theory]
Composite Likelihood Data Augmentation for Within-Network Statistical Relational Learning
2014 IEEE International Conference on Data Mining
None
2014
The prevalence of datasets that can be represented as networks has recently fueled a great deal of work in the area of Relational Machine Learning (RML). Due to the statistical correlations between linked nodes in the network, many RML methods focus on predicting node features (i.e., labels) using the network relationships. However, many domains are comprised of a single, partially-labeled network. Thus, relational versions of Expectation Maximization (i.e., R-EM), which jointly learn parameters and infer the missing labels, can outperform methods that learn parameters from the labeled data and apply them for inference on the unlabeled nodes. Although R-EM methods can significantly improve predictive performance in networks that are densely labeled, they do not achieve the same gains in sparsely labeled networks and can perform worse than RML methods. In this work, we show the fixed-point methods that R-EM uses for approximate learning and inference result in errors that prevent convergence in sparsely labeled networks. We then propose two methods that do not experience this problem. First, we develop a Relational Stochastic EM (R-SEM) method, which uses stochastic parameters that are not as susceptible to approximation errors. Then we develop a Relational Data Augmentation (R-DA) method, which integrates over a range of stochastic parameter values for inference. R-SEM and R-DA can use any collective RML algorithm for learning and inference in partially labeled networks. We analyze their performance with two RML learners over four real world datasets, and show that they outperform independent learning, RML and R-EM -- particularly in sparsely labeled networks.
[RML methods, R-EM methods, partially-labeled network, statistical correlations, network relationships, network theory (graphs), Nonlinear Dynamical Systems, Mixing Rate, Approximation methods, relational stochastic EM method, stochastic parameters, collective RML algorithm, approximate learning, relational data augmentation method, sparsely labeled networks, learning (artificial intelligence), stochastic processes, Joints, Statistical Relational Learning, relational machine learning, approximation theory, linked nodes, fixed-point methods, R-SEM method, Estimation, predictive performance, within-network statistical relational learning, relational versions, Data Augmentation, composite likelihood data augmentation, node feature prediction, Markov processes, expectation-maximisation algorithm, R-DA method, relational inference, Approximation algorithms, Inference algorithms, expectation maximization, data handling, approximation errors]
Structural Bregman Distance Functions Learning to Rank with Self-Reinforcement
2014 IEEE International Conference on Data Mining
None
2014
Learning to rank is an important task for many data mining applications. Essentially, the goal of learning to rank is to learn an appropriate similarity or distance metric to determine the relevance relationships among data points. However, most of the existing approaches for distance metric learning are limited in three aspects. First, they often assume a fixed form of distance metric for the entire input space. Second, the assumed distance functions are often computationally expensive or even intractable to learn for high dimensional data, such as Mahalanobis distance. Third, most of these approaches lack robustness to noisily labeled data, which is pervasive in many real-world applications. In this paper, we study learning to rank as a problem of distance metric learning to address the above three problems. We choose Bregman distance as the target distance function, due to its general functional form as a generalization of a wide class of distance functions, and its capacity of exploiting complicated nonlinear patterns underlying the data. Under the framework of structural SVM, we formulate the problem of learning Bregman distance functions for ranking as a QP problem by a nonparametric approach, and present an effective algorithm. Furthermore, we propose a self-reinforcement scheme that adaptively differentiates each data point in the role of learning to secure the robustness. We emphasize that the proposed method SBLR-S (Structural Bregman distance functions Learning to Rank with Self-reinforcement) is more general than the conventional distance metric learning approaches, and is able to handle high dimensional data as well as noisily labeled data. The experiments of data ranking on real-world datasets show the superiority of this method to the state-of-the-art literature.
[Measurement, structural SVM, data mining, Optimization, QP problem, SBLR-S method, Training, distance metric learning approach, structural learning, nonparametric approach, self-reinforcement, Robustness, learning (artificial intelligence), distance functions, Kernel, data mining applications, Mahalanobis distance, Bregman distance function learning problem, support vector machines, distance metric learning, structural Bregman distance function learning, data points, self-reinforcement scheme, generalisation (artificial intelligence), Bregman distance, Support vector machines, complicated nonlinear patterns, structural Bregman distance functions learning to rank with self-reinforcement method, Data models, nonparametric statistics, learning to rank]
Metric Factorization for Exploratory Analysis of Complex Data
2014 IEEE International Conference on Data Mining
None
2014
How to explore complex data? Often, several representations for each data object are available, the data are described by attributes of heterogeneous data type and/or each data object is characterized by many features. It is difficult to choose a suitable similarity measure and an appropriate data mining technique to get an unbiased overview on the information contained in complex data. In this paper, we introduce Metric Factorization as a novel data mining task. The goal of Metric Factorization is to discover the major alternative views of complex data. Our novel algorithm MF extends matrix factorization techniques to support metric data. We do not need to choose a single similarity measure but can just input any available metric. Metric Factorization builds automatically interesting basis spaces from a large variety of input metrics. Due to metric properties, the basis spaces can be further explored with standard techniques like Multidimensional Scaling. We relate the Metric Factorization task to data compression and demonstrate how ideas from information theory (Minimum Description Length principle) make the parametrization of MF optional. We further introduce the idea of landmark points to effectively compress and thus support large data sets. Extensive experiments demonstrate the benefits of our approach.
[Measurement, similarity measure, data compression, data analysis, metric factorization, data mining, Encoding, Matrix factorization, matrix decomposition, metric data, Data mining, MF optional parametrization, Optimization, Standards, Image color analysis, heterogeneous data type, MDL, minimum description length principle, Feature extraction, information theory, data mining technique, multidimensional scaling]
Online Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction
2014 IEEE International Conference on Data Mining
None
2014
Max-margin matrix factorization (M3F) has been popularly applied to collaborative filtering for personalized recommendations. The nonparametric M3F model represents the latest progress of the M3F methods, which can auto-select the number of factors by using nonparametric techniques. However, existing non-parametric M3F methods assume a collection of user rating data can be fully obtained before training, and they are inapplicable for on-the-fly recommender systems where user rating data arrive continuously. In this paper, we present a new efficient online nonparametric 3F model for flexible recommendation. Specifically, we design an online nonparametric M3F model (OnM3F) based on the online Passive-Aggressive learning and solve the corresponding optimization problem by using the online stochastic gradient descent. Empirical studies on two large real-world data sets verify the effectiveness of the proposed method.
[online nonparametric max-margin matrix factorization, Adaptation models, collaborative filtering, Computational modeling, Nonparametric Max-Margin Matrix Factorization, online passive-aggressive learning, online stochastic gradient descent, Online Learning, matrix decomposition, collaborative prediction, Collaborative Prediction, Optimization, Learning systems, user rating data collection, recommender systems, Collaboration, nonparametric M3F methods, personalized recommendation system, Data models, Bayes methods, learning (artificial intelligence), OnM3F model, M3F model]
Diffusion Archaeology for Diffusion Progression History Reconstruction
2014 IEEE International Conference on Data Mining
None
2014
Diffusion through graphs can be used to model many real-world process, such as the spread of diseases, social network memes, computer viruses, or water contaminants. Often, a real-world diffusion cannot be directly observed while it is occurring -- perhaps it is not noticed until some time has passed, continuous monitoring is too costly, or privacy concerns limit data access. This leads to the need to reconstruct how the present state of the diffusion came to be from partial diffusion data. Here, we tackle the problem of reconstructing a diffusion history from one or more snapshots of the diffusion state. This ability can be invaluable to learn when certain computer nodes are infected or which people are the initial disease spreaders to control future diffusions. We formulate this problem over discrete-time SEIRS-type diffusion models in terms of maximum likelihood. We design methods that are based on sub modularity and a novel prize-collecting dominating-set vertex cover (PCDSVC) relaxation that can identify likely diffusion steps with some provable performance guarantees. Our methods are the first to be able to reconstruct complete diffusion histories accurately in real and simulated situations. As a special case, they can also identify the initial spreaders better than existing methods for that problem. Our results for both meme and contaminant diffusion show that the partial diffusion data problem can be overcome with proper modeling and methods, and that hidden temporal characteristics of diffusion can be predicted from limited data.
[Computers, diffusion, diffusion state, disease spreader, graph theory, History, Approximation methods, maximum likelihood estimation, temporal characteristics, real-world process, contaminant diffusion, real-world diffusion, maximum likelihood, performance guarantee, Silicon, Mathematical model, diffusion progression history reconstruction, diffusion history reconstruction, continuous monitoring, Computational modeling, discrete time systems, diffusion archaeology, history, graph, epidemics, discrete-time SEIRS-type diffusion model, data access, PCDSVC relaxation, prize-collecting dominating-set vertex cover relaxation, data handling, partial diffusion data problem, Integrated circuit modeling]
Privacy-Preserving Personalized Recommendation: An Instance-Based Approach via Differential Privacy
2014 IEEE International Conference on Data Mining
None
2014
Recommender systems become increasingly popular and widely applied nowadays. The release of users' private data is required to provide users accurate recommendations, yet this has been shown to put users at risk. Unfortunately, existing privacy-preserving methods are either developed under trusted server settings with impractical private recommender systems or lack of strong privacy guarantees. In this paper, we develop the first lightweight and provably private solution for personalized recommendation, under untrusted server settings. In this novel setting, users' private data is obfuscated before leaving their private devices, giving users greater control on their data and service providers less responsibility on privacy protections. More importantly, our approach enables the existing recommender systems (with no changes needed) to directly use perturbed data, rendering our solution very desirable in practice. We develop our data perturbation approach on differential privacy, the state-of-the-art privacy model with lightweight computation and strong but provable privacy guarantees. In order to achieve useful and feasible perturbations, we first design a novel relaxed admissible mechanism enabling the injection of flexible instance-based noises. Using this novel mechanism, our data perturbation approach, incorporating the noise calibration and learning techniques, obtains perturbed user data with both theoretical privacy and utility guarantees. Our empirical evaluation on large-scale real-world datasets not only shows its high recommendation accuracy but also illustrates the negligible computational overhead on both personal computers and smart phones. As such, we are able to meet two contradictory goals, privacy preservation and recommendation accuracy. This practical technology helps to gain user adoption with strong privacy protection and benefit companies with high-quality personalized services on perturbed user data.
[Data privacy, Noise, privacy preservation, Learning and Optimization, theoretical privacy, data perturbation, Servers, private recommender systems, perturbed user data, Privacy, computational overhead, high quality personalized services, user private data, Recommender System, calibration, noise calibration, personal computing, privacy protections, Vectors, privacy-preserving methods, smart phones, privacy-preserving personalized recommendation, Probabilistic Analysis, Sensitivity, recommender systems, Aggregates, recommendation accuracy, provable privacy guarantees, Differential Privacy, data privacy, user adoption, strong privacy protection, Data Perturbation, untrusted server settings, utility guarantees, trusted computing, differential privacy]
Mining Contentious Documents Using an Unsupervised Topic Model Based Approach
2014 IEEE International Conference on Data Mining
None
2014
This work proposes an unsupervised method intended to enhance the quality of opinion mining in contentious text. It presents a Joint Topic Viewpoint (JTV) probabilistic model to analyse the underlying divergent arguing expressions that may be present in a collection of contentious documents. It extends the original Latent Dirichlet Allocation (LDA), which makes it domain and thesaurus-independent, e.g., does not rely on Word Net coverage. The conceived JTV has the potential of automatically carrying the tasks of extracting associated terms denoting an arguing expression, according to the hidden topics it discusses and the embedded viewpoint it voices. Furthermore, JTV's structure enables the unsupervised grouping of obtained arguing expressions according to their viewpoints, using a constrained clustering approach. Experiments are conducted on three types of contentious documents: polls, online debates and editorials. The qualitative and quantitative analysis of the experimental results show the effectiveness of our model to handle six different contentious issues when compared to a state-of-the-art method. Moreover, the ability to automatically generate distinctive and informative patterns of arguing expressions is demonstrated.
[opinion mining, latent Dirichlet allocation, joint topic viewpoint probabilistic model, data mining, arguing expression, Medical services, LDA, editorials, Word Net coverage, polls, Data mining, online debates, quantitative analysis, Joints, unsupervised topic model based approach, document handling, Government, probability, unsupervised method, unsupervised grouping, JTV probabilistic model, contentious document mining, Insurance, qualitative analysis, Data models, Editorials]
Efficient Integrity Verification for Outsourced Collaborative Filtering
2014 IEEE International Conference on Data Mining
None
2014
Collaborative filtering (CF) over large datasets requires significant computing power. Due to this data owning organizations often outsource the computation of CF (including some abstraction of the data itself) to a public cloud infrastructure. However, this leads to the question of how to verify the integrity of the outsourced computation. In this paper, we develop verification mechanisms for two popular item based collaborative filtering techniques. We further analyze the cheating behavior of the cloud from the game-theoretic perspective. Coupled with the right incentives, we can ensure that the computation is incentive compatible thus ensuring that a rational adversary will not cheat. Leveraging this, we can develop efficient and effective mechanisms to address the problem of integrity in outsourcing.
[Measurement, collaborative filtering, Collaborative Filtering, outsourced computation, game-theoretic perspective, game theory, item based collaborative filtering technique, Integrity Verification, Educational institutions, Electronic mail, Servers, outsourced collaborative filtering, public cloud infrastructure, verification mechanism, formal verification, outsourcing, cheating behavior, Collaboration, Prediction algorithms, Outsourcing, data owning organization, cloud computing, integrity verification]
PGT: Measuring Mobility Relationship Using Personal, Global and Temporal Factors
2014 IEEE International Conference on Data Mining
None
2014
Rich location data of mobile users collected from smart phones and location-based social networking services enable us to measure the mobility relationship strength based on their interactions in the physical world. A commonly-used measure for such relationship is the frequency of meeting events (i.e., Co-locate at the same time). That is, the more frequently two persons meet, the stronger their mobility relationship is. However, we argue that not all the meeting events are equally important in measuring the mobility relationship and propose to consider personal and global factors to differentiate meeting events. Personal factor models the probability for an individual user to visit a certain location, whereas the global factor models the popularity of a location based on the behavior of general public. In addition, we introduce the temporal factor to further consider the time gaps between meeting events. Accordingly, we propose a unified framework, called PGT, that considers personal, global, and temporal factors to measure the strength of the relationship between two given mobile users. Extensive experiments on real datasets validate our ideas and show that our method significantly outperforms the state-of-the-art methods.
[time gaps, unified framework, mobility, Conferences, probability, mobile user location data collection, smart phones, Data mining, general public behavior, relationship strength, social computing, personal-global-and-temporal factors, mobility relationship strength measurement, mobile computing, physical world, spatiotemporal, meeting event frequency, social networking (online), location-based social networking services, PGT]
MASCOT: Fast and Highly Scalable SVM Cross-Validation Using GPUs and SSDs
2014 IEEE International Conference on Data Mining
None
2014
Cross-validation is a commonly used method for evaluating the effectiveness of Support Vector Machines (SVMs). However, existing SVM cross-validation algorithms are not scalable to large datasets because they have to (i) hold the whole dataset in memory and/or (ii) perform a very large number of kernel value computation. In this paper, we propose a scheme to dramatically improve the scalability and efficiency of SVM cross-validation through the following key ideas. (i) To avoid holding the whole dataset in the memory and avoid performing repeated kernel value computation, we precompute the kernel values and reuse them. (ii) We store the precomputed kernel values to a high-speed storage framework, consisting of CPU memory extended by solid state drives (SSDs) and GPU memory as a cache, so that reusing (i.e., Reading) kernel values takes much lesser time than computing them on-the-fly. (iii) To further improve the efficiency of the SVM training, we apply a number of techniques for the extreme example search algorithm, design a parallel kernel value read algorithm, propose a caching strategy well-suited to the characteristics of the storage framework, and parallelize the tasks on the GPU and the CPU. For datasets of sizes that existing algorithms can handle, our scheme achieves several orders of magnitude of speedup. More importantly, our scheme enables SVM cross-validation on datasets of very large scale that existing algorithms are unable to handle.
[Algorithm design and analysis, Instruction sets, Graphics processing units, GPUs, cache storage, SVM, Training, storage management, scalable SVM cross-validation, MASCOT, Cross-validation, GPU memory, solid state drive, Kernel, support vector machines, caching strategy, SSD, SSDs, graphics processing units, Equations, Support vector machines, CPU memory, support vector machine, parallel kernel value read algorithm, disc drives, high-speed storage framework]
Multi-graph-view Learning for Graph Classification
2014 IEEE International Conference on Data Mining
None
2014
Graph classification has traditionally focused on graphs generated from a single feature view. In many applications, it is common to have useful information from different channels/views to describe objects, which naturally results in a new representation with multiple graphs generated from different feature views being used to describe one object. In this paper, we formulate a new Multi-Graph-View learning task for graph classification, where each object to be classified contains graphs from multiple graph-views. This problem setting is essentially different from traditional single-graph-view graph classification, where graphs are from one single feature view. To solve the problem, we propose a Cross Graph-View Sub graph Feature based Learning (gCGVFL) algorithm that explores an optimal set of sub graphs, across multiple graph-views, as features to represent graphs. Specifically, we derive an evaluation criterion to estimate the discriminative power and the redundancy of sub graph features across all views, and assign proper weight values to each view to indicate its importance for graph classification. The iterative cross graph-view sub graph scoring and graph-view weight updating form a closed loop to find optimal sub graphs to represent graphs for multi-graph-view learning. Experiments and comparisons on real-world tasks demonstrate the algorithm's performance.
[iterative methods, multigraph-view learning, Subgraph Mining, Graph Classification, Redundancy, graph theory, Educational institutions, cross graph-view subgraph feature based learning, Vectors, graph-view weight updating, iterative cross graph-view subgraph scoring, Optimization, Training, Feature Selection, Histograms, Image color analysis, graph classification, learning (artificial intelligence), Multi-Graph-View]
RS-Forest: A Rapid Density Estimator for Streaming Anomaly Detection
2014 IEEE International Conference on Data Mining
None
2014
Anomaly detection in streaming data is of high interest in numerous application domains. In this paper, we propose a novel one-class semi-supervised algorithm to detect anomalies in streaming data. Underlying the algorithm is a fast and accurate density estimator implemented by multiple fully randomized space trees (RS-Trees), named RS-Forest. The piecewise constant density estimate of each RS-tree is defined on the tree node into which an instance falls. Each incoming instance in a data stream is scored by the density estimates averaged over all trees in the forest. Two strategies, statistical attribute range estimation of high probability guarantee and dual node profiles for rapid model update, are seamlessly integrated into RS Forestto systematically address the ever-evolving nature of data streams. We derive the theoretical upper bound for the proposed algorithm and analyze its asymptotic properties via bias-variance decomposition. Empirical comparisons to the state-of-the-art methods on multiple benchmark datasets demonstrate that the proposed method features high detection rate, fast response, and insensitivity to most of the parameter settings. Algorithm implementations and datasets are available upon request.
[bias-variance decomposition, data mining, data streams, Predictive models, anomaly detection, randomized space trees, one-class semisupervised algorithm, dual node profile, asymptotic property, Detectors, Benchmark testing, learning (artificial intelligence), density estimation, streaming data, rapid density estimator, piecewise constant density, Estimation, trees (mathematics), RS-forest, RS-trees, Anomaly detection, Upper bound, security of data, Vegetation, statistical attribute range estimation, Data models, statistical analysis, ensembles]
Heterogeneous Metric Learning with Content-Based Regularization for Software Artifact Retrieval
2014 IEEE International Conference on Data Mining
None
2014
The problem of software artifact retrieval has the goal to effectively locate software artifacts, such as a piece of source code, in a large code repository. This problem has been traditionally addressed through the textual query. In other words, information retrieval techniques will be exploited based on the textual similarity between queries and textual representation of software artifacts, which is generated by collecting words from comments, identifiers, and descriptions of programs. However, in addition to these semantic information, there are rich information embedded in source codes themselves. These source codes, if analyzed properly, can be a rich source for enhancing the efforts of software artifact retrieval. To this end, in this paper, we develop a feature extraction method on source codes. Specifically, this method can capture both the inherent information in the source codes and the semantic information hidden in the comments, descriptions, and identifiers of the source codes. Moreover, we design a heterogeneous metric learning approach, which allows to integrate code features and text features into the same latent semantic space. This, in turn, can help to measure the artifact similarity by exploiting the joint power of both code and text features. Finally, extensive experiments on real-world data show that the proposed method can help to improve the performances of software artifact retrieval with a significant margin.
[Measurement, source code (software), text analysis, heterogeneous metric learning approach, feature extraction method, Electronic mail, Optimization, textual query, artifact similarity, query processing, information retrieval technique, Semantics, feature extraction, code feature, content-based regularization, learning (artificial intelligence), source code, Information retrieval, text feature, latent semantic space, textual representation, Feature extraction, Software, code repository, textual similarity, semantic information, software artifact retrieval]
A Fast Inference Algorithm for Stochastic Blockmodel
2014 IEEE International Conference on Data Mining
None
2014
Stochastic block model is a widely used statistical tool for modeling graphs and networks. Despite its popularity, the development on efficient inference algorithms for this model is surprisingly inadequate. The existing solutions are either too slow to handle large networks, or suffer from convergence issues. In this paper, we propose a fast and principled inference algorithm for stochastic block model. The algorithm is based on the variational Bayesian framework, and deploys the natural conjugate gradient method to accelerate the optimization of the variational bound. Leveraging upon the power of both conjugate and natural gradients, it converges super linearly and produces high quality solutions in practice. In particular, we apply our algorithm to the community detection task and compare it with the state-of-the-art variational Bayesian algorithms. We show that it can achieve up to two orders of magnitude speedup without significantly compromising the quality of solutions.
[Gradient methods, fast inference algorithm, natural gradients, natural conjugate gradient method, variational Bayesian algorithms, variational bound, inference mechanisms, community detection task, Equations, principled inference algorithm, Convergence, Manifolds, stochastic block model, graphs, variational Bayesian framework, modeling graphs, Inference algorithms, Bayes methods, stochastic processes, gradient methods, statistical tool]
Modeling and Mining Spatiotemporal Social Contact of Metapopulation from Heterogeneous Data
2014 IEEE International Conference on Data Mining
None
2014
During an epidemic, the spatial, temporal and demographical patterns of disease transmission are determined by multiple factors. Besides the physiological properties of pathogenes and hosts, the social contacts of host population, which characterize individuals' reciprocal exposures of infection in view of demographical structures and various social activities, are also pivotal to understand and further predict the prevalence of infectious diseases. The means of measuring social contacts will dominate the extent how precisely we can forecast the dynamics of infections in the real world. Most current works focus their efforts on modeling the spatial patterns of static social contacts. In this work, we address the problem on how to characterize and measure dynamical social contacts during an epidemic from a novel perspective. We propose an epidemic-model-based tensor deconvolution framework to address this issue, in which the spatiotemporal patterns of social contacts are represented by the factors of tensors, which can be discovered by a tensor deconvolution procedure with an integration of epidemic models from rich types of data, mainly including heterogeneous outbreak surveillance, social-demographic census and physiological data from medical reports. Taking SIR model as a case study, the efficacy of the proposed method is theoretically analyzed and empirically validated through a set of rigorous experiments on both synthetic and real-world data.
[data mining, heterogeneous data, spatial pattern, healthcare, tensor deconvolution, Sociology, heterogeneous outbreak surveillance, demographical pattern, epidemic modeling, spatiotemporal social contact mining, disease transmission, pattern recognition, epidemic, temporal pattern, social activities, Educational institutions, diseases, Statistics, Diseases, spatiotemporal social contact, demographical structures, Tensile stress, Surveillance, infectious disease, SIR model, metapopulation, epidemic-model-based tensor deconvolution framework, Data models, medical computing, social-demographic census, physiological data, multiple source data mining]
Continuous KNN Join Processing for Real-Time Recommendation
2014 IEEE International Conference on Data Mining
None
2014
The explosive growth of user-generated contents in social networking websites necessitates the recommendation functionality that can push to the user the content that he/she is most likely to be interested in. Such recommendation should happen in real-time as new contents become available, because "freshness" is an important consideration in people's content-consumption behavior. Representing users and contents as feature vectors in a high-dimensional space, we can essentially cast the problem of real-time recommendations as the problem of computing the list of k nearest neighbors of each user, which we call kNN join. Given the vast volume of contents and users, the biggest challenge is how to continuously update the kNN join results as new contents arrive. Existing methods for incremental kNN join on data streams suffer from the "curse of dimensionality" and high in-memory search cost. In this paper, we present a solution that first identifies the users whose kNN's might be affected by the newly arrived content, and then update their kNN's respectively. We propose a new index structure named HDR-tree in order to support the efficient search of affected users. HDR-tree performs dimensionality reduction through clustering and principle component analysis (PCA) in order to improve the search effectiveness. To further reduce response time, we propose a variant of HDR-tree, called HDR-tree, that supports more efficient but approximate solutions. The results of extensive experiments show that our methods significantly outperform baseline methods.
[k nearest neighbors, data streams, high-dimensional data, social networking, real-time recommendation, curse of dimensionality reduction, data reduction, HDR-tree index structure, Clustering algorithms, principle component analysis, Real-time systems, tree data structures, people content-consumption behavior, clustering analysis, high in-memory search cost, k nearest neighbor join, Educational institutions, Indexes, user-generated contents, PCA, recommender systems, pattern clustering, Collaboration, continuous KNN join processing, Vegetation, social networking (online), Web sites, principal component analysis, Principal component analysis]
Scalable SVM-Based Classification in Dynamic Graphs
2014 IEEE International Conference on Data Mining
None
2014
With the emergence of networked data, graph classification has received considerable interest during the past years. Most approaches to graph classification focus on designing effective kernels to compute similarities for static graphs. However, they become computationally intractable in terms of time and space when a graph is presented in a incremental fashion with continuous updates, i.e., Insertions of nodes and edges. In this paper, we examine the problem of classification in large-scale and incrementally changing graphs. To this end, a framework combining an incremental Support Vector Machine (SVM) with the Weisfeiler-Lehman (W-L) graph kernel has been proposed to study this problem. By retaining the support vectors from each learning step, the classification model is incrementally updated whenever new changes are made to the subject graph. Furthermore, we design an entropy-based sub graph extraction strategy to select informative neighbor nodes and discard those with less discriminative power, to facilitate an effective classification process. We demonstrate the advantages of our learning techniques by conducting an empirical evaluation on two large-scale real-world graph datasets. The experimental results also validate the benefits of our sub graph extraction method when combined with the incremental learning techniques.
[pattern classification, support vector machines, graph theory, mathematics computing, Predictive models, entropy-based subgraph extraction, Entropy, SVM, Data mining, Support vector machines, Training, entropy, support vector machine, graph classification, W-L graph kernel, Weisfeiler-Lehman graph kernel, Prediction algorithms, learning technique, learning (artificial intelligence), Kernel]
Towards Scalable and Accurate Online Feature Selection for Big Data
2014 IEEE International Conference on Data Mining
None
2014
Feature selection is important in many big data applications. There are at least two critical challenges. Firstly, in many applications, the dimensionality is extremely high, in millions, and keeps growing. Secondly, feature selection has to be highly scalable, preferably in an online manner such that each feature can be processed in a sequential scan. In this paper, we develop SAOLA, a Scalable and Accurate On Line Approach for feature selection. With a theoretical analysis on a low bound on the pair wise correlations between features in the currently selected feature subset, SAOLA employs novel online pair wise comparison techniques to address the two challenges and maintain a parsimonious model over time in an online manner. An empirical study using a series of benchmark real data sets shows that SAOLA is scalable on data sets of extremely high dimensionality, and has superior performance over the state-of-the-art feature selection methods.
[Correlation, pairwise correlation, Feature redundancy, Redundancy, data mining, SAOLA, Big Data, Search problems, Training, Accuracy, Markov processes, Online feature selection, Big data, Extremely high dimensionality, scalable and accurate online approach, feature selection]
Learning Fine-Grained Spatial Models for Dynamic Sports Play Prediction
2014 IEEE International Conference on Data Mining
None
2014
We consider the problem of learning predictive models for in-game sports play prediction. Focusing on basketball, we develop models for anticipating near-future events given the current game state. We employ a latent factor modeling approach, which leads to a compact data representation that enables efficient prediction given raw spatiotemporal tracking data. We validate our approach using tracking data from the 2012-2013 NBA season, and show that our model can make accurate in-game predictions. We provide a detailed inspection of our learned factors, and show that our model is interpretable and corresponds to known intuitions of basketball game play.
[in-game predictions, in-game sports play prediction, representation learning, Predictive models, raw spatiotemporal tracking data, Training, Analytical models, near-future events, predictive models, dynamic sports play prediction, basketball game play, data structures, learning (artificial intelligence), Computational modeling, NBA season, compact data representation, spatiotemporal reasoning, prediction theory, factor modeling, sports analytics, predictive modeling, Games, Gaussian processes, Data models, data handling, sport, fine-grained spatial model learning]
Output Feature Augmented Lasso
2014 IEEE International Conference on Data Mining
None
2014
Lasso simultaneously conducts variable selection and supervised regression. In this paper, we extend Lasso to multiple output prediction, which belongs to the categories of structured learning. Though structured learning makes use of both input and output simultaneously, the joint feature mapping in current framework of structured learning is usually application-specific. As a result, ad hoc heuristics have to be employed to design different joint feature mapping functions for different applications, which results in the lackness of generalization ability for multiple output prediction. To address this limitation, in this paper, we propose to augment Lasso with output by decoupling the joint feature mapping function of traditional structured learning. The contribution of this paper is three-fold: 1) The augmented Lasso conducts regression and variable selection on both the input and output features, and thus the learned model could fit an output with both the selected input variables and the other correlated outputs. 2) To be more general, we set up nonlinear dependencies among output variables by generalized Lasso. 3) Moreover, the Augmented Lagrangian Method (ALM) with Alternating Direction Minimizing (ADM) strategy is used to find the optimal model parameters. The extensive experimental results demonstrate the effectiveness of the proposed method.
[Correlation, ALM, output feature augmented Lasso, augmented Lagrangian method, Input variables, ADM strategy, Lasso, regression analysis, joint feature mapping functions, Linear programming, Vectors, alternating direction minimizing, multiple output prediction, alternating direction minimizing strategy, Training, supervised regression, structured learning, variable selection, learning (artificial intelligence), Kernel, Joints, feature selection]
Multi-touch Attribution in Online Advertising with Survival Theory
2014 IEEE International Conference on Data Mining
None
2014
Multi-touch attribution, which allows distributing the credit to all related advertisements based on their corresponding contributions, has recently become an important research topic in digital advertising. Traditionally, rule-based attribution models have been used in practice. The drawback of such rule-based models lies in the fact that the rules are not derived form the data but only based on simple intuition. With the ever enhanced capability to tracking advertisement and users' interaction with the advertisement, data-driven multi-touch attribution models, which attempt to infer the contribution from user interaction data, become an important research direction. We here propose a new data-driven attribution model based on survival theory. By adopting a probabilistic framework, one key advantage of the proposed model is that it is able to remove the presentation biases inherit to most of the other attribution models. In addition to model the attribution, the proposed model is also able to predict user's 'conversion' probability. We validate the proposed method with a real-world data set obtained from a operational commercial advertising monitoring company. Experiment results have shown that the proposed method is quite promising in both conversion prediction and attribution.
[Gold, Online Advertising, Multi-touch attribution, advertising data processing, probabilistic framework, data-driven multitouch attribution models, probability, survival theory, Predictive models, Hazards, digital advertising, rule-based attribution models, Survival theory, Hidden Markov models, commercial advertising monitoring company, user interaction data, Data models, user conversion probability prediction, data handling, Internet, Advertising, Kernel, online advertising]
Tracking the Evolution of Social Emotions: A Time-Aware Topic Modeling Perspective
2014 IEEE International Conference on Data Mining
None
2014
Many of today's online news websites have enabled users to specify different types of emotions (e.g., Angry and shocked) they have after reading news. Compared with traditional user feedbacks such as comments and ratings, these specific emotion annotations are more accurate for expressing users' personal emotions. In this paper, we propose to exploit these users' emotion annotations for online news in order to track the evolution of emotions, which plays an important role in various online services. A critical challenge is how to model emotions with respect to time spans. To this end, we propose a time-aware topic modeling perspective for solving this problem. Specifically, we first develop a model named emotion-Topic over Time (eToT), in which we represent the topics of news as a Beta distribution over time and a multinomial distribution over emotions. Whilee ToT can uncover the latent relationship among news, emotion and time directly, it cannot capture the dynamics of topics. Therefore, we further develop another model named emotion based Dynamic Topic Model (eDTM), where we explore the state space model for tracking the dynamics of topics. In addition, we demonstrate that both eToT and eDTM could enable several potential applications, such as emotion prediction, emotion-based news recommendations and emotion anomaly detections. Finally, we validate the proposed models with extensive experiments with a real-world data set.
[Sentiment analysis, user feedbacks, Predictive models, social emotion evolution tracking, multinomial distribution over emotion, statistical distributions, user emotion annotations, Analytical models, Semantics, eDTM, social sciences computing, Joints, online services, emotion based dynamic topic model, online news Web sites, emotion-topic over time model, Vectors, user personal emotions, state space model, time-aware topic modeling, Beta distribution over time, eToT model, social networking (online), Data models, Internet]
Mp-Dissimilarity: A Data Dependent Dissimilarity Measure
2014 IEEE International Conference on Data Mining
None
2014
Nearest neighbour search is a core process in many data mining algorithms. Finding reliable closest matches of a query in a high dimensional space is still a challenging task. This is because the effectiveness of many dissimilarity measures, that are based on a geometric model, such as lp-norm, decreases as the number of dimensions increases. In this paper, we examine how the data distribution can be exploited to measure dissimilarity between two instances and propose a new data dependent dissimilarity measure called 'mp-dissimilarity'. Rather than relying on geometric distance, it measures the dissimilarity between two instances in each dimension as a probability mass in a region that encloses the two instances. It deems the two instances in a sparse region to be more similar than two instances in a dense region, though these two pairs of instances have the same geometric distance. Our empirical results show that the proposed dissimilarity measure indeed provides a reliable nearest neighbour search in high dimensional spaces, particularly in sparse data. Mp-dissimilarity produced better task specific performance than lp-norm and cosine distance in classification and information retrieval tasks.
[high dimensional space, data distribution, data mining algorithms, data mining, nearest neighbour search, cosine distance, lp-norm, Electronic mail, Approximation methods, Data mining, query processing, reliable nearest neighbour search, Accuracy, mp-dissimilarity, search problems, geometric model, geometric distance, probability, l<sub>p</sub>-norm, Information retrieval, Educational institutions, Vectors, m<sub>p</sub>-dissimilarity measures, probability mass, information retrieval tasks, distance measure, data dependent dissimilarity measure]
Check-in Location Prediction Using Wavelets and Conditional Random Fields
2014 IEEE International Conference on Data Mining
None
2014
The widespread adoption of ubiquitous devices does not only facilitate the connection of billions of people, but has also fuelled a culture of sharing rich, high resolution locations through check-ins. Despite the profusion of GPS and WiFi driven location prediction techniques, the sparse and random nature of check-in data generation have ushered diverse problems, which have prompted the prediction of future check-ins to be very challenging. In this paper, we propose a novel enhanced location predictor for check-in data that is crafted using Poisson distribution, Wavelets and Conditional Random Fields (CRF). Specifically, we show that check-in generation is governed by the Poisson distribution. In addition, among others, we utilize wavelets to rigorously analyze social influence and learn elusive underlying patterns, as well as human mobility behaviors embedded in check-in data. We utilize this knowledge to institute CRF features, which capture latent trends that govern users' mobility. These CRF features are employed to build a robust predictive model that predicts future locations with enhanced accuracy. We demonstrate the effectiveness of our predictive model on two real datasets. Furthermore, our experiments reveal that our approach outperforms a state-of-the-art work with an accuracy of 36%.
[wavelet transforms, Time series analysis, wavelet transform, Prediction, Predictive models, CRF features, Data Mining, Poisson distribution, ubiquitous computing, information services, Equations, Multiresolution analysis, ubiquitous devices, WiFi driven location prediction techniques, check-in location prediction, Global Positioning System, conditional random field, GPS driven location prediction techniques, Accuracy, Location Based Services, Hidden Markov models, Wireless Fidelity, Mathematical model]
Janus -- Analytics-Driven Transition Planner
2014 IEEE International Conference on Data Mining
None
2014
In this paper, we address the problem of transition of IT operations from one service provider to another. We present analytics-driven solutions to generate a transition plan while addressing various aspects such as coverage, risk, time, and cost. We model the IT operations through graphs and use the well defined problems in graph theory to build solutions for transition planner. We demonstrate the proof-of-concept of proposed ideas using a real-world case-study.
[Data center management, Communities, graph theory, service provider, Graph theory, IT Operations, Databases, Janus, analytics-driven transition planner, Transition, Software, Hardware, IT operations, IEEE Potentials, business data processing, Business]
Large-Scale Analysis of Soccer Matches Using Spatiotemporal Tracking Data
2014 IEEE International Conference on Data Mining
None
2014
Although the collection of player and ball tracking data is fast becoming the norm in professional sports, large-scale mining of such spatiotemporal data has yet to surface. In this paper, given an entire season's worth of player and ball tracking data from a professional soccer league (&#x2248;400,000,000 data points), we present a method which can conduct both individual player and team analysis. Due to the dynamic, continuous and multi-player nature of team sports like soccer, a major issue is aligning player positions over time. We present a "role-based" representation that dynamically updates each player's relative role at each frame and demonstrate how this captures the short-term context to enable both individual player and team analysis. We discover role directly from data by utilizing a minimum entropy data partitioning method and show how this can be used to accurately detect and visualize formations, as well as analyze individual player behavior.
[Context, large-scale mining, soccer match analysis, data analysis, Spatiotemporal Tracking Data, sports, data mining, Entropy, Role, spatiotemporal tracking data, minimum entropy data partitioning method, Data visualization, large-scale analysis, Games, Probability density function, ball tracking data, Trajectory, Spatiotemporal phenomena, Sports Analytics, sport, Formation]
Graded Multilabel Classification by Pairwise Comparisons
2014 IEEE International Conference on Data Mining
None
2014
The task in multilabel classification is to predict for a given set of labels whether each individual label should be attached to an instance or not. Graded multilabel classification generalizes this setting by allowing to specify for each label a degree of membership on an ordinal scale. This setting can be frequently found in practice, for example when movies or books are assessed on a one-to-five star rating in multiple categories. In this paper, we propose to reformulate the problem in terms of preferences between the labels and their scales, which can then be tackled by learning from pair wise comparisons. We present three different approaches which make use of this decomposition and show on three datasets that we are able to outperform baseline approaches. In particular, we show that our solution, which is able to model pair wise preferences across multiple scales, outperforms a straight-forward approach which considers the problem as a set of independent ordinal regression tasks.
[pattern classification, TV, graded multilabel classification, regression analysis, Loss measurement, learning, Calibration, Training, independent ordinal regression tasks, ordinal classification, pairwise comparisons, Motion pictures, Robustness, learning by pairwise comparisons, learning (artificial intelligence), Medical diagnostic imaging]
TRIBAC: Discovering Interpretable Clusters and Latent Structures in Graphs
2014 IEEE International Conference on Data Mining
None
2014
Graphs are a powerful representation of relational data, such as social and biological networks. Often, these entities form groups and are organised according to a latent structure. However, these groupings and structures are generally unknown and it can be difficult to identify them. Graph clustering is an important type of approach used to discover these vertex groups and the latent structure within graphs. One type of approach for graph clustering is non-negative matrix factorisation However, the formulations of existing factorisation approaches can be overly relaxed and their groupings and results consequently difficult to interpret, may fail to discover the true latent structure and groupings, and converge to extreme solutions. In this paper, we propose a new formulation of the graph clustering problem that results in clusterings that are easy to interpret. Combined with a novel algorithm, the clusterings are also more accurate than state-of-the-art algorithms for both synthetic and real datasets.
[interpretable clusters discovery, non-negative matrix factorisation, Image edge detection, Communities, graph theory, TRIBAC, Airports, matrix decomposition, Matrix decomposition, Optimization, Equations, graph clustering, latent structures, graph clustering problem, nonnegative matrix factorisation, pattern clustering, Clustering algorithms, blockmodelling, interpretability]
Stream Mining Using Statistical Relational Learning
2014 IEEE International Conference on Data Mining
None
2014
Stream mining has gained popularity in recent years due to the availability of numerous data streams from sources such as social media and sensor networks. Data mining on such continuous streams possess a variety of challenges including concept drift and unbounded stream length. Traditional data mining approaches to these problems have difficulty incorporating relational domain knowledge and feature relationships, which can be used to improve the accuracy of a classifier. In this work, we model large data streams using statistical relational learning techniques for classification, in particular, we use a Markov Logic Network to capture relational features in structured data and show that this approach performs better for supervised learning than current state-of-the-art approaches. Additionally, we evaluate our approach with semi-supervised learning scenarios, where class labels are only partially available during training.
[unbounded stream length, data mining, supervised learning, sensor networks, Data mining, relational domain knowledge, stream mining, Training, feature relationships, Accuracy, structured data, relational features, Classification, class labels, learning (artificial intelligence), social media, Markov logic network, Statistical Relational Learning, pattern classification, Grounding, statistical relational learning, classification, Markov random fields, data models, Stream Mining, concept drift, Markov processes, Data models, large data streams model]
Ups and Downs in Buzzes: Life Cycle Modeling for Temporal Pattern Discovery
2014 IEEE International Conference on Data Mining
None
2014
In social media analysis, one critical task is detecting burst of topics or buzz, which is reflected by extremely frequent mentions of certain key words in a short time interval. Detecting buzz not only provides useful insights into the information propagation mechanism, but also plays an essential role in preventing malicious rumors. However, buzz modeling is a challenging task because a buzz time-series usually exhibits sudden spikes and heavy tails, which fails most existing time-series models. To deal with buzz time-series sequences, we propose a novel time-series modeling approach which captures the rise and fade temporal patterns via Product Life Cycle (PLC) models, a classical concept in economics. More specifically, we propose a mixture of PLC models to capture the multiple peaks in buzz time-series and furthermore develop a probabilistic graphical model (K-MPLC) to automatically discover inherent life cycle patterns within a collection of buzzes. Our experiment results show that our proposed method significantly outperforms existing state-of-the-art approaches on buzzes clustering.
[Measurement, text analysis, K-MPLC, time-series modeling, malicious rumors prevention, life cycle patterns, Optimization, Graphical models, temporal pattern discovery, key words frequent mentions, Time-Series Modeling, Clustering algorithms, product life cycle modeling, Biological system modeling, product life cycle management, PLC models, buzz time-series sequences, Media, Probabilistic logic, time series, information propagation mechanism, pattern clustering, buzz modeling, Time-Series Clustering, social networking (online), buzzes clustering, time series clustering, buzz detection, probabilistic graphical model, social media analysis]
Flu Gone Viral: Syndromic Surveillance of Flu on Twitter Using Temporal Topic Models
2014 IEEE International Conference on Data Mining
None
2014
Surveillance of epidemic outbreaks and spread from social media is an important tool for governments and public health authorities. Machine learning techniques for now casting the flu have made significant inroads into correlating social media trends to case counts and prevalence of epidemics in a population. There is a disconnect between data-driven methods for forecasting flu incidence and epidemiological models that adopt a state based understanding of transitions, that can lead to sub-optimal predictions. Furthermore, models for epidemiological activity and social activity like on Twitter predict different shapes and have important differences. We propose a temporal topic model to capture hidden states of a user from his tweets and aggregate states in a geographical region for better estimation of trends. We show that our approach helps fill the gap between phenomenological methods for disease surveillance and epidemiological models. We validate this approach by modeling the flu using Twitter in multiple countries of South America. We demonstrate that our model can consistently outperform plain vocabulary assessment in flu case-count predictions, and at the same time get better flu-peak predictions than competitors. We also show that our fine-grained modeling can reconcile some contrasting behaviors between epidemiological and social models.
[Syndromic Surveillance, Vocabulary, Google, epidemic outbreak surveillance, machine learning technique, Biological system modeling, Social Media, Switches, Predictive models, temporal topic model, Twitter, Data Mining, public health authority, Epidemiology, epidemics, Market research, social networking (online), Data models, Topic Model, learning (artificial intelligence), medical computing, social media, health care]
Mining Personal Health Index from Annual Geriatric Medical Examinations
2014 IEEE International Conference on Data Mining
None
2014
People take regular medical examinations mostly not for discovering diseases but for having a peace of mind regarding their health status. Therefore, it is important to give them an overall feedback with respect to all the health indicators that have been ranked against the whole population. In this paper, we propose a framework of mining Personal Health Index (PHI) from a large and comprehensive geriatric medical examination (GME) dataset. We define PHI as an overall score of personal health status based on a complement probability of health risks. The health risks are calculated using the information from the cause of death (COD) dataset that is linked to the GME dataset. Especially, the highest health risk is revealed in the cases of people who had been taking GME for some years and then passed away for medical reasons. The proposed framework consists of methods in data pre-processing, feature extraction and selection, and model selection. The effectiveness of the proposed framework is validated by a set of comprehensive experiments based on the records of 102,258 participants. As the first of this kind, our work provides a baseline for further research.
[health indicators, data mining, personal health index mining, Data mining, comprehensive geriatric medical examination dataset, feature extraction, GME dataset, complement probability, Cities and towns, cause of death dataset, feature selection, health care, personal health status, data preprocessing, probability, model selection, Educational institutions, diseases, Indexes, Support vector machines, Personal Health Index, Feature extraction, annual geriatric medical examinations, health risks, geriatric medical examination, medical computing, Geriatrics]
Social Role Identification via Dual Uncertainty Minimization Regularization
2014 IEEE International Conference on Data Mining
None
2014
In this paper, we study a challenging problem of inferring individuals' role and statuses in a professional social network, which is of central importance in workforce optimization and human capital management. Realizing the natural setting of social nodes associated with dual view information, i.e., The local node characteristics and the global network influence, we present a novel model that explores graph regularization techniques and integrates such information to achieve improved prediction performance. In particular, our prediction model is built upon the graph transductive learning framework that encodes an uncertainty regularization term in the conventional empirical risk minimization principle. Through taking advantage of the information from both the local profile and the global network characteristics, the final inference of the role or statues achieves minimum an empirical loss on the labeled set, as well as a minimum uncertainty on the unlabeled social nodes. We perform extensive empirical study using real-world data and compare with representative peer approaches. The experimental results on three real social network data sets show that the proposed model greatly outperforms a number of baseline models and is able to effectively infer in a wide range of scenarios.
[social nodes, Uncertainty, graph theory, Graph Regularization, dual uncertainty minimization regularization, global network influence, Social Role Identification, Electronic mail, professional social network, human capital management, local profile, social role identification, learning (artificial intelligence), risk management, prediction model, graph regularization techniques, workforce optimization, Dual Uncertainty Minimization, Minimization, graph transductive learning framework, dual view information, LinkedIn, Feature extraction, social networking (online), empirical risk minimization principle, Data models, local node characteristics]
A Joint Model for Topic-Sentiment Evolution over Time
2014 IEEE International Conference on Data Mining
None
2014
Most existing topic models focus either on extracting static topic-sentiment conjunctions or topic-wise evolution over time leaving out topic-sentiment dynamics and missing the opportunity to provide a more in-depth analysis of textual data. In this paper, we propose an LDA-based topic model for analyzing topic-sentiment evolution over time by modeling time jointly with topics and sentiments. We derive inference algorithm based on Gibbs Sampling process. Finally, we present results on reviews and news datasets showing interpretable trends and strong correlation with ground truth in particular for topic-sentiment evolution over time.
[opinion mining, text analysis, Correlation, topic-sentiment evolution analysis, news datasets, Data mining, textual data in-depth analysis, Analytical models, Accuracy, Monte Carlo methods, topic-wise evolution, static topic-sentiment conjunction extraction, Mathematical model, Joints, trend analysis, topic-sentiment dynamics, joint topic sentiment models, information resources, sentiment analysis, time series, inference mechanisms, joint model, LDA-based topic model, inference algorithm, Markov processes, topic models, Data models, Gibbs sampling process]
Hidden Conditional Random Fields with Deep User Embeddings for Ad Targeting
2014 IEEE International Conference on Data Mining
None
2014
Estimating a user's propensity to click on a display ad or purchase a particular item is a critical task in targeted advertising, a burgeoning online industry worth billions of dollars. Better and more accurate estimation methods result in improved online user experience, as only relevant and interesting ads are shown, and may also lead to large benefits for advertisers, as targeted users are more likely to click or make a purchase. In this paper we address this important problem, and propose an approach for improved estimation of ad click or conversion probability based on a sequence of user's online actions, modeled using Hidden Conditional Random Fields (HCRF) model. In addition, in order to address the sparsity issue at the input side of the HCRF model, we propose to learn distributed, low-dimensional representations of user actions through a directed skip-gram, a neural architecture suitable for sequential data. Experimental results on a real-world data set comprising thousands of user sessions collected at Yahoo servers clearly indicate the benefits and the potential of the proposed approach, which outperformed competing state-of-the-art algorithms and obtained significant improvements in terms of retrieval measures.
[hidden CRF, ad targeting, click modeling, Predictive models, targeted advertising, user propensity estimation, HCRF model, retrieval measure, Advertising, directed skip-gram architecture, Context, advertising data processing, probability, information retrieval, deep user embeddings, neural architecture, Browsers, Support vector machines, hidden conditional random fields, Hidden Markov models, ad click estimation, neural nets, purchase prediction, Context modeling, conversion probability]
Learning to Grade Student Programs in a Massive Open Online Course
2014 IEEE International Conference on Data Mining
None
2014
We study the problem of automatically evaluating the quality of computer programs produced by students in a very large, online, interactive programming course (or "MOOC"). Automatically evaluating interactive programs (such as computer games) is not easy because such programs lack any sort of well-defined logical specification. As an alternative, we devise some simple statistical approaches to assigning a score to a student-produced code.
[Measurement, computer science education, computer programs, Computational modeling, Programming, well-defined logical specification, educational administrative data processing, formal specification, statistical approaches, quality evaluation, Prototypes, computer games, educational courses, Games, massive open online course, interactive programming course, student-produced code, Libraries, Peer-to-peer computing, computer aided instruction, statistical analysis, programming, student program grading]
Senders, Receivers and Authors in Document Classification
2014 IEEE International Conference on Data Mining
None
2014
In many document classification problems, sets of people will be associated with the document. These sets might include document authors, or people who have read the document, or the sender of an electronic message, or the recipients of the message, or those carbon copied, or those blind carbon copied. It is obvious that these sets of people can constitute important information that can help to classify the document. In this paper, we propose a simple method for mapping the set of people in a sender or receiver category to a single, low dimensional vector in a latent space. There are many ways that this vector can be used to help with the document classification task, and in the paper we consider three distinct possibilities in detail. We find that mapping a set of senders or receivers to a latent space in this way and incorporating this mapping into a classifier can greatly boost classification accuracy on several real electronic discovery tasks.
[document handling, pattern classification, electronic message sender, electronic messaging, Receivers, Vectors, Encoding, document author, classification accuracy, Electronic mail, document classification, Carbon, Support vector machines, Bayes methods, message recipient, electronic discovery task]
Shell Miner: Mining Organizational Phrases in Argumentative Texts in Social Media
2014 IEEE International Conference on Data Mining
None
2014
Threaded debate forums have become one of the major social media platforms. Usually people argue with one another using not only claims and evidences about the topic under discussion but also language used to organize them, which we refer to as shell. In this paper, we study how to separate shell from topical contents using unsupervised methods. Along this line, we develop a latent variable model named Shell Topic Model (STM) to jointly model both topics and shell. Experiments on real online debate data show that our model can find both meaningful shell and topics. The results also show the effectiveness of our model by comparing it with several baselines in shell phrases extraction and document modeling.
[shell phrase extraction, text analysis, shell topic model, data mining, organizational phrase mining, Media, Educational institutions, topic modeling, latent variable model, Data mining, Noise measurement, STM, Training, shell miner, topical content, Hidden Markov models, organizational phrases, argumentative text, social networking (online), Data models, document modeling, learning (artificial intelligence), social media]
Topic Models with Topic Ordering Regularities for Topic Segmentation
2014 IEEE International Conference on Data Mining
None
2014
Documents from the same domain usually discuss similar topics in a similar order. In this paper we present new ordering-based topic models that use generalised Mallows models to capture this regularity to constrain topic assignments. Specifically, these new models assume that there is a canonical topic ordering shared amongst documents from the same domain, and each document-specific topic ordering is allowed to vary from the canonical topic ordering. Instead of full orderings over a set of all possible topics covered by a domain, we make use of top-t orderings via a multistage ranking process. We show how to reformulate the new models so that a point-wise sampling algorithm from the Bayesian word segmentation literature can be used for posterior inference. Experimental results on several document collections with different properties show that our model performs much better than the other topic ordering-based models, and competitively with other state-of-the-art topic segmentation models.
[Adaptation models, Electronic publishing, topic segmentation, topic assignments, Encyclopedias, multistage ranking process, document-specific topic ordering, ordering-based topic models, top-t ordering, belief networks, top-t orderings, Topic model, point-wise sampling algorithm, Bayesian word segmentation literature, permutation, document handling, pattern classification, topic ordering regularities, sampling methods, Biological system modeling, canonical topic ordering, GMM, generalised Mallows models, Hidden Markov models, posterior inference, Internet]
Understanding Where Your Classifier Does (Not) Work -- The SCaPE Model Class for EMM
2014 IEEE International Conference on Data Mining
None
2014
FACT, the First G-APD Cherenkov Telescope, detects air showers induced by high-energetic cosmic particles. It is desirable to classify a shower as being induced by a gamma ray or a background particle. Generally, it is nontrivial to get any feedback on the real-life training task, but we can attempt to understand how our classifier works by investigating its performance on Monte Carlo simulated data. To this end, in this paper we develop the SCaPE (Soft Classifier Performance Evaluation) model class for Exceptional Model Mining, which is a Local Pattern Mining framework devoted to highlighting unusual interplay between multiple targets. In our Monte Carlo simulated data, we take as targets the computed classifier probabilities and the binary column containing the ground truth: which kind of particle induced the corresponding shower. Using a newly developed quality measure based on ranking loss, the SCaPE model class highlights subspaces of the search space where the classifier performs particularly well or poorly. These subspaces arrive in terms of conditions on attributes of the data, hence they come in a language a domain expert understands, which should aid him in understanding where his/her classifier does (not) work. Found subgroups highlight subspaces whose difficulty for classification is corroborated by astrophysical interpretation, as well as subspaces that warrant further investigation.
[Protons, FACT telescope, air shower detection, Astrophysics, Monte Carlo simulated data, data mining, Exceptional Model Mining, classifier probabilities, soft classifier performance evaluation model class, Loss measurement, shower classification, Data mining, Radio frequency, Monte Carlo methods, local pattern mining framework, astronomy computing, search space, search problems, binary column, exceptional model mining, pattern classification, ranking loss, Atmospheric modeling, EMM, G-APD Cherenkov telescope, probability, soft classifier, background particle, high-energetic cosmic particles, Cherenkov radiation, Terrestrial atmosphere, astronomical telescopes, SCaPE, Telescopes, gamma ray, astrophysical interpretation]
Ring-Shaped Hotspot Detection: A Summary of Results
2014 IEEE International Conference on Data Mining
None
2014
Given a collection of geo-located activities (e.g., Crime reports), ring-shaped hotspot detection (RHD) finds rings, where concentration of activities inside the ring is much higher than outside. RHD is important for the applications such as crime analysis, where it may focus the search for crime source's location, e.g. The home of a serial criminal. RHD is challenging because of the large number of candidate rings and the high computational cost of the statistical significance test. Previous statistically significant hotspot detection techniques (e.g., Sat Scan) identify circular/rectangular areas, but can not discover rings. This paper proposes a dual grid based pruning (DGP) approach to detect ring-shaped hotspots. A case study on real crime data confirms that DGP detects novel ring-shaped regions, regions that go undetected by Sat Scan. Experiments show that DGP improves the computational cost of a naive approach substantially.
[Geology, grid computing, geographic information systems, Biology, Equations, RHD, dual grid based pruning, Monte Carlo methods, Upper bound, ring-shaped hotspot detection, DGP approach, Computational efficiency, Mathematical model, statistical analysis, geo-located activities, statistical significance test]
Tensor Regression Based on Linked Multiway Parameter Analysis
2014 IEEE International Conference on Data Mining
None
2014
Classical regression methods take vectors as covariates and estimate the corresponding vectors of regression parameters. When addressing regression problems on covariates of more complex form such as multi-dimensional arrays (i.e. Tensors), traditional computational models can be severely compromised by ultrahigh dimensionality as well as complex structure. By exploiting the special structure of tensor covariates, the tensor regression model provides a promising solution to reduce the model's dimensionality to a manageable level, thus leading to efficient estimation. Most of the existing tensor-based methods independently estimate each individual regression problem based on tensor decomposition which allows the simultaneous projections of an input tensor to more than one direction along each mode. As a matter of fact, multi-dimensional data are collected under the same or very similar conditions, so that data share some common latent components but can also have their own independent parameters for each regression task. Therefore, it is beneficial to analyse regression parameters among all the regressions in a linked way. In this paper, we propose a tensor regression model based on Tucker Decomposition, which identifies not only the common components of parameters across all the regression tasks, but also independent factors contributing to each particular regression task simultaneously. Under this paradigm, the number of independent parameters along each mode is constrained by a sparsity-preserving regulariser. Linked multiway parameter analysis and sparsity modeling further reduce the total number of parameters, with lower memory cost than their tensor-based counterparts. The effectiveness of the new method is demonstrated on real data sets.
[multidimensional arrays, multidimensional data, tensor regression model, regression analysis, sparsity-preserving regulariser, Educational institutions, Vectors, tensors, sparse coding, sparsity modeling, tensor regression, Training, Tensile stress, vectors, linked multiway parameter analysis, parameter estimation, Data models, Nickel, data acquisition, Tucker decomposition, memory cost, tensor decomposition]
Heavyweight Pattern Mining in Attributed Flow Graphs
2014 IEEE International Conference on Data Mining
None
2014
This paper defines a new problem - heavyweight pattern mining in attributed flow graphs. The problem can be described as the discovery of patterns in flow graphs that have sets of attributes associated with their nodes. A connection between nodes is represented as a directed edge. The amount of load that goes through a path between nodes, or the frequency of transmission of such load between nodes, is represented as edge weights. A heavyweight pattern is a sub-set of attributes, found in a dataset of attributed flow graphs, that are connected by edges and have a computed weight higher than an user-defined threshold. A new algorithm called AFG Miner is introduced, the first one to our knowledge that finds heavyweight patterns in a dataset of attributed flow graphs and associates each pattern with its occurrences. The paper also describes a new tool for compiler engineers, HEP Miner, that applies the AFG Miner algorithm to Profile-based Program Analysis modeled as a heavyweight pattern mining problem.
[Algorithm design and analysis, data mining, attributed flow graphs, data flow graphs, profile-based program analysis, Electronic mail, Complexity theory, Data mining, Servers, user-defined threshold, program analysis, Labeling, directed edge, HEP Miner, AFG Miner algorithm, sub-graph mining, pattern mining, program profiling, flow graphs, Flow graphs, compiler engineers, graph mining, heavyweight pattern mining, directed graphs, software analysis]
Online Spectral Learning on a Graph with Bandit Feedback
2014 IEEE International Conference on Data Mining
None
2014
Online learning on a graph is appealing due to its efficiency. However, existing online learning algorithms on a graph are limited to binary classification. Moreover, they require accessing the full label information, where the label oracle needs to return the true class label after the learner makes classification of each node. In many application scenarios, we only have access to partial label information, where the label oracle will return a single bit indicating whether the prediction is correct or not, instead of the true class label. This is also known as bandit feedback. In this paper, to overcome the above limitations of existing online learning algorithms on a graph, we study online learning on a graph for multi-class node classification, in both the full information setting and the partial information setting. First, we present an online multi-class classification algorithm in the full information setting. It is based on function learning on a graph using the spectral information of the graph Laplacian. We show that it attains O (cd log T) regret bound, where T is the number of rounds in online learning, c is the number of classes, and d is the number of eigenvectors of the graph Laplacian used for learning. Second, we present an online multi-class classification algorithm with bandit feedback. We use upper-confidence bound technique to trade off the exploration and exploitation of label information. We show that it attains O (cd &#x221A;T log T) regret bound, which is only a &#x221A;T factor worse than the proposed algorithm in the full information setting. Experiments on several benchmark graph datasets show that the proposed online multi-class classification algorithm beats the state-of-art baseline, and the proposed bandit algorithm is also much better than the bandit version of the baseline.
[Algorithm design and analysis, online learning on a graph, Error analysis, graph theory, online multiclass classification algorithm, Data mining, Spectral Learning, eigenvalues and eigenfunctions, feedback, multiclass node classification, online spectral learning, function learning on a graph, Prediction algorithms, Eigenvalues and eigenfunctions, Bandit Feedback, learning (artificial intelligence), graph Laplacian, pattern classification, Laplace equations, Online Learning on a Graph, upper-confidence bound technique, Regret Bound, Vectors, Partial Information Setting, Graph Cut Size, bandit feedback, eigenvectors, computational complexity]
Low-Density Cut Based Tree Decomposition for Large-Scale SVM Problems
2014 IEEE International Conference on Data Mining
None
2014
The current trend of growth of information reveals that it is inevitable that large-scale learning problems become the norm. In this paper, we propose and analyze a novel Low-density Cut based tree Decomposition method for large-scale SVM problems, called LCD-SVM. The basic idea here is divide and conquer: use a decision tree to decompose the data space and train SVMs on the decomposed regions. Specifically, we demonstrate the application of low density separation principle to devise a splitting criterion for rapidly generating a high-quality tree, thus maximizing the benefits of SVMs training. Extensive experiments on 14 real-world datasets show that our approach can provide a significant improvement in training time over state-of-the-art methods while keeps comparable test accuracy with other methods, especially for very large-scale datasets.
[pattern classification, support vector machines, splitting criterion, data space decomposition, LCD-SVM, Educational institutions, low-density cut based tree decomposition, large-scale learning problems, Computational complexity, Training, Support vector machines, large scale, Histograms, SVM training, Accuracy, decision tree, decision trees, SVM problems, Decision trees, learning (artificial intelligence), low density separation principle]
Social Topic Modeling for Point-of-Interest Recommendation in Location-Based Social Networks
2014 IEEE International Conference on Data Mining
None
2014
In this paper, we address the problem of recommending Point-of-Interests (POIs) to users in a location-based social network. To the best of our knowledge, we are the first to propose the ST (Social Topic) model capturing both the social and topic aspects of user check-ins. We conduct experiments on real life data sets from Foursquare and Yelp. We evaluate the effectiveness of ST by evaluating the accuracy of top-k POI recommendation. The experimental results show that ST achieves better performance than the state-of-the-art models in the areas of social network-based recommender systems, and exploits the power of the location-based social network that has never been utilized before.
[Social network services, real life data sets, point-of-interest recommendation, topic aspects, social network-based recommender systems, Indexes, location-based social network, social topic modeling, user check-ins, Accuracy, recommender systems, social aspects, ST model, Cities and towns, Motion pictures, social networking (online), Yelp, Data models, Recommender systems, Foursquare, top-k POI recommendation]
Bayesian Heteroskedastic Choice Modeling on Non-identically Distributed Linkages
2014 IEEE International Conference on Data Mining
None
2014
Choice modeling (CM) aims to describe and predict choices according to attributes of subjects and options. If we presume each choice making as the formation of link between subjects and options, immediately CM can be bridged to link analysis and prediction (LAP) problem. However, such a mapping is often not trivial and straightforward. In LAP problems, the only available observations are links among objects but their attributes are often inaccessible. Therefore, we extend CM into a latent feature space to avoid the need of explicit attributes. Moreover, LAP is usually based on binary linkage assumption that models observed links as positive instances and unobserved links as negative instances. Instead, we use a weaker assumption that treats unobserved links as pseudo negative instances. Furthermore, most subjects or options may be quite heterogeneous due to the long-tail distribution, which is failed to capture by conventional LAP approaches. To address above challenges, we propose a Bayesian heteroskedastic choice model to represent the non-identically distributed linkages in the LAP problems. Finally, the empirical evaluation on real-world datasets proves the superiority of our approach.
[Adaptation models, LAP problems, data analysis, Biological system modeling, non-IID Bayesian analysis, Predictive models, Vectors, CM, heteroskedastic choice model, statistical distributions, long-tail distribution, Couplings, pseudo negative instances, Bayesian heteroskedastic choice modeling, nonidentically distributed linkages, Data models, Bayes methods, belief networks, link analysis and prediction, parallel Gibbs sampling]
Robust Dynamic Trajectory Regression on Road Networks: A Multi-task Learning Framework
2014 IEEE International Conference on Data Mining
None
2014
Trajectory regression, which aims to predict the travel time of arbitrary trajectories on road networks, attracts significant attention in various applications of traffic systems these years. In this paper, we tackle this problem with a multitask learning (MTL) framework. To take the temporal nature of the problem into consideration, we divide the regression problem into a set of sub-tasks of distinct time periods, then the problem can be treated in a multi-task learning framework. Further, we propose a novel regularization term in which we exploit the block sparse structure to augment the robustness of the model. In addition, we incorporate the spatial smoothness over road links and thus achieve a spatial-temporal framework. An accelerated proximal algorithm is adopted to solve the convex but non-smooth problem, which will converge to the global optimum. Experiments on both synthetic and real data sets demonstrate the effectiveness of the proposed method.
[trajectory regression, Roads, regression analysis, robust dynamic trajectory regression, road networks, regression problem, Optimization, Training, trajectory control, Robustness, dynamic, Trajectory, learning (artificial intelligence), block sparse structure, arbitrary trajectories, multitask learning framework, travel time prediction, traffic engineering computing, traffic systems, convex nonsmooth problem, accelerated proximal algorithm, multi-task learning, MTL framework, spatial-temporal framework, Data models, Acceleration, structured sparsity]
Detecting Volatility Shift in Data Streams
2014 IEEE International Conference on Data Mining
None
2014
Current drift detection techniques detect a change in distribution within a stream. However, there are no current techniques that analyze the change in the rate of these detected changes. We coin the term stream volatility, to describe the rate of changes in a stream. A stream has a high volatility if changes are detected frequently and has a low volatility if changes are detected infrequently. We are particularly interested in a volatility shift which is a change in the rate of change (e.g. From high volatility to low volatility). We introduce and define the concept of stream volatility, and propose a novel technique to detect volatility on data streams in the presence of concept drifts. In the experiments we show our algorithm to be both fast and efficient. We also propose a new algorithm for drift detection called SEED that is faster and more memory efficient than the existing state-of-the-art drift detection approach. A faster drift detection algorithm has a flow-on benefit to the subsequent volatility detection stage because both algorithms run concurrently on the data stream.
[Algorithm design and analysis, drift detection techniques, data mining, data streams, Volatility Detection, stream change rate, stream volatility, Drift Detection, Memory management, Detectors, SEED, Reservoirs, volatility shift detection, Delays, Detection algorithms, Testing, Data Stream]
Latent Ranking Analysis Using Pairwise Comparisons
2014 IEEE International Conference on Data Mining
None
2014
Ranking objects is an essential problem in recommendation systems. Since comparing two objects is the simplest type of queries in order to measure the relevance of objects, the problem of aggregating pair wise comparisons to obtain a global ranking has been widely studied. In order to learn a ranking model, a training set of queries as well as their correct labels are supplied and a machine learning algorithm is used to find the appropriate parameters of the ranking model with respect to the labels. In this paper, we propose a probabilistic model for learning multiple latent rankings using pair wise comparisons. Our novel model can capture multiple hidden rankings underlying the pair wise comparisons. Based on the model, we develop an efficient inference algorithm to learn multiple latent rankings. The performance study with synthetic and real-life data sets confirms the effectiveness of our model and inference algorithm.
[ranking objects, supervised learning, Probabilistic logic, Educational institutions, Vectors, inference mechanisms, Equations, Standards, probabilistic model, recommendation systems, Accuracy, inference algorithm, multiple hidden rankings, pairwise comparisons, real-life data sets, latent ranking analysis, Data models, Learning to rank, learning (artificial intelligence), ranking model, machine learning algorithm, multiple latent rankings]
Bus Travel Time Predictions Using Additive Models
2014 IEEE International Conference on Data Mining
None
2014
Many factors can affect the predictability of public bus services such as traffic, weather, day of week, and hour of day. However, the exact nature of such relationships between travel times and predictor variables is, in most situations, not known. In this paper we develop a framework that allows for flexible modeling of bus travel times through the use of Additive Models. The proposed class of models provides a principled statistical framework that is highly flexible in terms of model building. The experimental results demonstrate uniformly superior performance of our best model as compared to previous prediction methods when applied to a very large GPS data set obtained from buses operating in the city of Rio de Janeiro.
[Additives, public bus service predictability, Trajectory Data, GPS data set, Predictive models, principled statistical framework, model building, Rio de Janeiro, Global Positioning System, transportation, Support vector machines, Traffic Modeling, Arrival Time Prediction, predictor variables, additive models, Brazil, Tensor Product Basis, Basis Function, Mixed Models, Data models, Trajectory, statistical analysis, Kernel, bus travel time prediction]
Explicit Versus Implicit Graph Feature Maps: A Computational Phase Transition for Walk Kernels
2014 IEEE International Conference on Data Mining
None
2014
As many real-world data can elegantly be represented as graphs, various graph kernels and methods for computing them have been proposed. Surprisingly, many of the recent graph kernels do not employ the kernel trick anymore but rather compute an explicit feature map and report higher efficiency. So, is there really no benefit of the kernel trick when it comes to graphs? Triggered by this question, we investigate under which conditions it is possible to compute a graph kernel explicitly and for which graph properties this computation is actually more efficient. We give a sufficient condition for R-convolution kernels that enables kernel computation by explicit mapping. We theoretically and experimentally analyze efficiency and flexibility of implicit kernel functions and dot products of explicitly computed feature maps for widely used graph kernels such as random walk kernels, sub graph matching kernels, and shortest-path kernels. For walk kernels we observe a phase transition when comparing runtime with respect to label diversity and walk lengths leading to the conclusion that explicit computations are only favourable for smaller label sets and walk lengths whereas implicit computation is superior for longer walk lengths and data sets with larger label diversity.
[explicit graph feature maps, implicit graph feature maps, graph theory, walk lengths, implicit kernel functions, real-world data, label sets, Data mining, flexibility analysis, Runtime, Convolution, dot products, graph properties, graph kernels, phase transition, sufficient condition, Kernel, subgraph matching kernels, Java, Symmetric matrices, label diversity, random processes, time complexity, Vectors, efficiency analysis, random walk kernels, shortest-path kernels, R-convolution kernels, runtime analysis, data handling, kernel computation, computational phase transition, computational complexity]
Spectral Clustering for Medical Imaging
2014 IEEE International Conference on Data Mining
None
2014
Spectral clustering is often reported in the literature as successfully being applied to applications from image segmentation to community detection. However, what is not reported is that great time and effort are required to construct a graph Laplacian to achieve these successes. This problem which we call Laplacian construction is critical for the success of spectral clustering but is not well studied by the community. Instead the best Laplacian is typically learnt for each domain from trial and error. This is problematic for areas such as medical imaging since: (i) the same images can be segmented in multiple ways depending on the application focus and (ii) we don't wish to construct one Laplacian, rather we wish to create a method to construct a Laplacian for each patient's scan. In this paper we attempt to automate the process of Laplacian creation with the help of guidance towards the application focus. In most domains creating a basic Laplacian is plausible, so we propose adjusting this given Laplacian by discovering important nodes. We formulate this problem as an integer linear program with a precise geometric interpretation which is globally minimized using large scale solvers such as Gurobi. We show the usefulness on a real world problem in the area of fMRI scan segmentation where methods using standard Laplacians perform poorly.
[biomedical MRI, integer programming, graph theory, linear programming, automatic Laplacian creation process, global minimization, Sociology, image segmentation, geometric interpretation, fMRI scan segmentation, large scale solvers, Gurobi, community detection, medical image processing, spectral clustering, graph Laplacian construction, Biomedical imaging, patient scan, Laplace equations, node discovery, Image edge detection, Vectors, Statistics, integer linear program, pattern clustering, Senior citizens, real world problem, minimisation, medical imaging]
Fast Algorithms for Frequent Itemset Mining from Uncertain Data
2014 IEEE International Conference on Data Mining
None
2014
The majority of existing data mining algorithms mine frequent item sets from precise databases. A well-known algorithm is FP-growth, which builds a compact FP-tree structure to capture important contents of the database and mines frequent item sets from the FP-tree. However, there are situations in which data are uncertain. In recent years, researchers have paid attention to frequent item set mining from uncertain databases. UFP-growth is one of the frequently cited algorithms for mining uncertain data. However, the corresponding UFP-tree structure can be large. Other tree structures for handling uncertain data may achieve compactness at the expense of looser upper bounds on expected supports. To solve this problem, we propose two compact tree structures which capture uncertain data with tighter upper bounds than existing tree structures. We also designed two algorithms that mine frequent item sets from our proposed trees. Our experimental results show the tightness of bounds to expected supports provided by these algorithms.
[Algorithm design and analysis, FP-growth, UF-growth algorithm, UF-tree structure, fast algorithms, data mining algorithms, frequent patterns, data mining, Association analysis, uncertain data handling, Electron tubes, tightened upper bounds, Data mining, frequent itemset mining, Integrated circuits, Upper bound, Itemsets, compact FP-tree structure, tree structures, uncertain data, data handling, tree data structures, looser upper bounds]
Spotting Fake Reviews via Collective Positive-Unlabeled Learning
2014 IEEE International Conference on Data Mining
None
2014
Online reviews have become an increasingly important resource for decision making and product designing. But reviews systems are often targeted by opinion spamming. Although fake review detection has been studied by researchers for years using supervised learning, ground truth of large scale datasets is still unavailable and most of existing approaches of supervised learning are based on pseudo fake reviews rather than real fake reviews. Working with Dianping, the largest Chinese review hosting site, we present the first reported work on fake review detection in Chinese with filtered reviews from Dianping's fake review detection system. Dianping's algorithm has a very high precision, but the recall is hard to know. This means that all fake reviews detected by the system are almost certainly fake but the remaining reviews (unknown set) may not be all genuine. Since the unknown set may contain many fake reviews, it is more appropriate to treat it as an unlabeled set. This calls for the model of learning from positive and unlabeled examples (PU learning). By leveraging the intricate dependencies among reviews, users and IP addresses, we first propose a collective classification algorithm called Multi-typed Heterogeneous Collective Classification (MHCC) and then extend it to Collective Positive and Unlabeled learning (CPU). Our experiments are conducted on real-life reviews of 500 restaurants in Shanghai, China. Results show that our proposed models can markedly improve the F1 scores of strong baselines in both PU and non-PU learning settings. Since our models only use language independent features, they can be easily generalized to other languages.
[pattern classification, opinion spamming, data mining, supervised learning, Dianping fake review detection system, CPU learning, Collective PU Learning, Training, online review, Training data, collective positive and unlabeled learning, multityped heterogeneous collective classification, Data models, Nickel, Internet, IP networks, Reliability, learning (artificial intelligence), Spam Detection, Testing, MHCC]
Discovering Temporal Retweeting Patterns for Social Media Marketing Campaigns
2014 IEEE International Conference on Data Mining
None
2014
Social media has become one of the most popular marketing channels for many companies, which aims at maximizing their influence by various marketing campaigns conducted from their official accounts on social networks. However, most of these marketing accounts merely focus on the contents of their tweets. Less effort has been made on understanding tweeting time, which is a major contributing factor in terms of attracting customers' attention and maximizing the influence of a social marketing campaign. To that end, in this paper, we provide a focused study of temporal retweeting patterns and their influence on social media marketing campaigns. Specifically, we investigate the users' retweeting patterns by modeling their retweeting behaviors as a generative process, which considers temporal, social, and topical factors. Moreover, we validate the predictive power of the model on the dataset collected from Sina Weibo, the most popular micro blog platform in China. By discovering the temporal retweeting patterns, we analyze the temporal popular topics and recommend tweets to users in a time-aware manner. Finally, experimental results show that the proposed algorithm outperforms other baseline methods. This model is applicable for companies to conduct their marketing campaigns at the right time on social media.
[Context, microblog platform, generative process, Sina Weibo, retweeting behaviors, Companies, Media, Predictive models, Educational institutions, marketing data processing, History, social media marketing campaigns, temporal retweeting patterns, topical factor, China, temporal factor, social networking (online), social factor, Context modeling]
A Framework to Recommend Interventions for 30-Day Heart Failure Readmission Risk
2014 IEEE International Conference on Data Mining
None
2014
In this paper, we describe a novel framework to recommend personalized intervention strategies to minimize 30-day readmission risk for heart failure (HF) patients, as they move through the provider's cardiac care protocol. We design principled solutions by learning the structure and parameters of a multi-layer hierarchical Bayesian network from underlying high-dimensional patient data. Next, we generate and summarize the rules leading to personalized interventions which can be applied to individual patients as they progress from admit to discharge. We present comprehensive experimental results as well as interesting case studies to demonstrate the effectiveness of our proposed framework using large real-world patient datasets on Microsoft Azure for Research platform.
[risk management, personalized intervention strategy recommendation, heart failure, bayesian network, Conferences, heart failure readmission risk, Data mining, parameter learning, cardiology, HF patients, structure learning, research platform, risk of readmission, time 30 day, recommender systems, multilayer hierarchical Bayesian network, high-dimensional patient data, Microsoft Azure, intervention recommendation, provider cardiac care protocol, belief networks, learning (artificial intelligence), medical computing]
Hete-CF: Social-Based Collaborative Filtering Recommendation Using Heterogeneous Relations
2014 IEEE International Conference on Data Mining
None
2014
In this paper, we investigate the social-based recommendation algorithms on heterogeneous social networks and proposed Hete-CF, a social collaborative filtering algorithm using heterogeneous relations. Distinct from the exiting methods, Hete-CF can effectively utilise multiple types of relations in a heterogeneous social network. More importantly, Hete-CF is a general approach and can be used in arbitrary social networks, including event based social networks, location based social networks, and any other types of heterogeneous information networks associated with social information. The experimental results on a real-world dataset DBLP (a typical heterogeneous information network)demonstrate the effectiveness of our algorithm.
[collaborative filtering, social collaborative filtering algorithm, Social network services, social-based recommendation algorithms, heterogeneous information networks, Hete-CF, Vectors, social information, Sparse matrices, Equations, heterogeneous social networks, heterogeneous relations, social-based collaborative filtering recommendation, location based social networks, Collaboration, Prediction algorithms, social networking (online), event based social networks, Mathematical model, real-world dataset DBLP]
Hierarchical Incident Ticket Classification with Minimal Supervision
2014 IEEE International Conference on Data Mining
None
2014
In this paper, we introduce a novel approach for incident ticket classification that aims at minimizing the manual labelling effort while achieving good-quality predictions. To accomplish this, we devise a two-stage technique that employs hierarchical clustering using a combination of graph clustering (community finding) and topic modelling as first stage, followed by either another round of hierarchical clustering or an active learning approach as second stage. We evaluate the performance of our method in terms of manual labelling effort, prediction quality and efficiency on three real-world datasets and demonstrate that classical approaches to text classification are not well suited for incident ticket texts.
[text analysis, Communities, Manuals, Servers, real-world datasets, Clustering algorithms, text mining, Labeling, Monitoring, pattern classification, active learning approach, multi-class classification, manual labelling, community finding, performance evaluation, prediction efficiency, two-stage technique, graph clustering, good-quality predictions, prediction quality, minimal supervision, pattern clustering, topic modelling, Feature extraction, hierarchical clustering, hierarchical incident ticket text classification]
A Gaussian Process Model for Knowledge Propagation in Web Ontologies
2014 IEEE International Conference on Data Mining
None
2014
We consider the problem of predicting missing class-memberships and property values of individual resources in Web ontologies. We first identify which relations tend to link similar individuals by means of a finite-set Gaussian Process regression model, and then efficiently propagate knowledge about individuals across their relations. Our experimental evaluation demonstrates the effectiveness of the proposed method.
[class-memberships, Symmetric matrices, semi-supervised, Web ontologies, semantic web, regression analysis, Ontologies, Training, transductive, Gaussian processes, finite-set Gaussian process regression model, ontologies (artificial intelligence), Internet, property values, Labeling, knowledge propagation, Kernel, Portals, gaussian process]
Mutual Information Based Output Dimensionality Reduction
2014 IEEE International Conference on Data Mining
None
2014
Given a large dimensional input and output space, even simple regression is prohibitively costly. Dimensionality reduction in the output space is important for efficient learning and prediction as modern paradigms, e.g. Topic modelling, image classification, etc., have extremely large output spaces. In contrast to input dimensionality reduction, dimension reduction in output side is complicated. We propose, mutual information based output dimensionality reduction, that takes into account the relationship between the input and the output which is essential for regression and classification problems. Our method selects those labels to form the compressed label space that typically have the maximum mutual information with the input. Selecting the best subset is computationally hard, but we provide a polynomial time algorithm with provable approximation guarantee. We conduct experiments on seven multi-label classification datasets. Results show our method performs better than existing methods on some datasets.
[Greedy algorithms, regression problems, submodular function, regression analysis, output dimension reduction, learning, Approximation methods, dimension reduction, multi-label, mutual information based output dimensionality reduction, mutual information, learning (artificial intelligence), approximation theory, pattern classification, subset selection, dimensional output space, Educational institutions, Vectors, Decoding, compressed label space, polynomial time algorithm, approximation guarantee, multilabel classification datasets, dimensional input space, prediction, classification problems, Mutual information, Compressed sensing, computational complexity]
Multi-label Classification with Meta-Labels
2014 IEEE International Conference on Data Mining
None
2014
The area of multi-label classification has rapidly developed in recent years. It has become widely known that the baseline binary relevance approach can easily be outperformed by methods which learn labels together. A number of methods have grown around the label power set approach, which models label combinations together as class values in a multi-class problem. We describe the label-power set-based solutions under a general framework of meta-labels and provide some theoretical justification for this framework which has been lacking, explaining how meta-labels essentially allow a random projection into a space where non-linearities can easily be tackled with established linear learning algorithms. The proposed framework enables comparison and combination of related approaches to different multi-label problems. We present a novel model in the framework and evaluate it empirically against several high-performing methods, with respect to predictive performance and scalability, on a number of datasets and evaluation metrics. This deployment obtains competitive accuracy for a fraction of the computation required by the current meta-label methods for multi-label classification.
[pattern classification, evaluation metrics, Scalability, Predictive models, label-powerset-based solutions, label combinations, linear learning algorithms, Vectors, Indexes, classification, multi-label, Training, meta-labels, Accuracy, Neural networks, multilabel classification, label powerset approach, learning (artificial intelligence), baseline binary relevance approach]
Graph Summarization with Quality Guarantees
2014 IEEE International Conference on Data Mining
None
2014
We study the problem of graph summarization. Given a large graph we aim at producing a concise lossy representation that can be stored in main memory and used to approximately answer queries about the original graph much faster than by using the exact representation. In this paper we study a very natural type of summary: the original set of vertices is partitioned into a small number of super nodes connected by super edges to form a complete weighted graph. The super edge weights are the edge densities between vertices in the corresponding super nodes. The goal is to produce a summary that minimizes the reconstruction error w.r.t. The original graph. By exposing a connection between graph summarization and geometric clustering problems (i.e., k-means and k-median), we develop the first polynomial-time approximation algorithm to compute the best possible summary of a given size.
[Algorithm design and analysis, graph summarization, approximation theory, Smoothing methods, Symmetric matrices, graph theory, geometric clustering problems, Approximation methods, complete weighted graph, pattern clustering, Clustering algorithms, Approximation algorithms, Silicon, geometry, polynomial-time approximation algorithm, computational complexity]
Data Fusion Using Restricted Boltzmann Machines
2014 IEEE International Conference on Data Mining
None
2014
We address the issue of data fusion. Suppose that we are given two datasets, where some variables are different each other and others are the same. The goal of data fusion is to complement the missing unique variables in each dataset using the common variables. Data fusion facilitates inference over multiple independent and different datasets, which is an important data mining issues that affect many applications, such as recommendation, image reconstruction, or market analysis. In this paper, we propose a novel approach to data fusion using restricted Boltzmann machines (RBMs). In applying to data fusion, RBMs are able to model hidden patterns lying behind the two datasets with bipartite graph structure between hidden variables and observable ones. There is a bottleneck in the application of RBMs to data fusion: It is computationally expensive to learn RBMs from data with missing values. Therefore, we propose a new efficient algorithm for learning RBMs from missing data. This algorithm maximizes the lower bound on the observation likelihood, which can efficiently be computed. With benchmark datasets, we empirically demonstrate that our RBM-based data fusion method significantly outperforms existing methods in terms of complement accuracy. These results demonstrate an advantage of data fusion based on latent-variable modeling.
[data fusion, Maximum likelihood estimation, Computational modeling, observation likelihood, data mining, sensor fusion, Vectors, maximum likelihood estimation, Boltzmann machines, Data integration, restricted Boltzmann machine, Data models, RBM, Joints, Time complexity, latent-variable modelling]
Spotting Suspicious Link Behavior with fBox: An Adversarial Perspective
2014 IEEE International Conference on Data Mining
None
2014
How can we detect suspicious users in large online networks? Online popularity of a user or product (via follows, page-likes, etc.) can be monetized on the premise of higher ad click-through rates or increased sales. Web services and social networks which incentivize popularity thus suffer from a major problem of fake connections from link fraudsters looking to make a quick buck. Typical methods of catching this suspicious behavior use spectral techniques to spot large groups of often blatantly fraudulent (but sometimes honest) users. However, small-scale, stealthy attacks may go unnoticed due to the nature of low-rank Eigen analysis used in practice. In this work, we take an adversarial approach to find and prove claims about the weaknesses of modern, state-of-the-art spectral methods and propose fBox, an algorithm designed to catch small-scale, stealth attacks that slip below the radar. Our algorithm has the following desirable properties: (a) it has theoretical underpinnings, (b) it is shown to be highly effective on real data and (c) it is scalable (linear on the input size). We evaluate fBox on a large, public 41.7 million node, 1.5 billion edge who-follows-whom social graph from Twitter in 2010 and with high precision identify many suspicious accounts which have persisted without suspension even to this day.
[Algorithm design and analysis, Fans, suspicious link behavior spotting, spectral techniques, social networks, social graph, Encyclopedias, ad click-through rate, Twitter, adversarial perspective, link fraud, Vectors, fBox algorithm, Matrix decomposition, security of data, Web services, social networking (online)]
Recovering Low-Rank and Sparse Matrices via Robust Bilateral Factorization
2014 IEEE International Conference on Data Mining
None
2014
Recovering low-rank and sparse matrices from partial, incomplete or corrupted observations is an important problem in many areas of science and engineering. In this paper, we propose a scalable robust bilateral factorization (RBF) method to recover both structured matrices from missing and grossly corrupted data such as robust matrix completion (RMC), or incomplete and grossly corrupted measurements such as compressive principal component pursuit (CPCP). With the unified framework, we first present two robust trace norm regularized bilateral factorization models for RMC and CPCP problems, which can achieve an orthogonal dictionary and a robust data representation, simultaneously. Then, we apply the alternating direction method of multipliers to efficiently solve the RMC problems. Finally, we provide the convergence analysis of our algorithm, and extend it to address general CPCP problems. Experimental results verified both the efficiency and effectiveness of our RBF method compared with the state-of-the-art methods.
[Algorithm design and analysis, structured matrices, compressive principal component pursuit, robust trace norm regularized bilateral factorization models, matrix decomposition, Sparse matrices, robust data representation, Image reconstruction, Convergence, CPCP problems, low-rank, Robustness, Face, RBF method, RMC, orthogonal dictionary, Matrix decomposition, sparse matrices recovery, low-rank matrices recovery, alternating direction method of multipliers, convergence analysis, RPCA, robust matrix completion, principal component analysis, sparse matrices]
A Parallel and Efficient Algorithm for Learning to Match
2014 IEEE International Conference on Data Mining
None
2014
Many tasks in data mining and related fields can be formalized as matching between objects in two heterogeneous domains, including collaborative filtering, link prediction, image tagging, and web search. Machine learning techniques, referred to as learning-to-match in this paper, have been successfully applied to the problems. Among them, a class of state-of-the-art methods, named feature-based matrix factorization, formalize the task as an extension to matrix factorization by incorporating auxiliary features into the model. Unfortunately, making those algorithms scale to real world problems is challenging, and simple parallelization strategies fail due to the complex cross talking patterns between sub-tasks. In this paper, we tackle this challenge with a novel parallel and efficient algorithm. Our algorithm, based on coordinate descent, can easily handle hundreds of millions of instances and features on a single machine. The key recipe of this algorithm is an iterative relaxation of the objective to facilitate parallel updates of parameters, with guaranteed convergence on minimizing the original objective function. Experimental results demonstrate that the proposed method is effective on a wide range of matching problems, with efficiency significantly improved upon the baselines while accuracy retained unchanged.
[Algorithm design and analysis, iterative methods, collaborative filtering, pattern matching, efficient algorithm, convergence, parallel algorithm, data mining, coordinate descent, matrix decomposition, machine learning techniques, Parallel algorithms, feature-based matrix factorization, Convergence, Training, state-of-the-art method, link prediction, Prediction algorithms, heterogeneous domain, learning (artificial intelligence), image tagging, parallel algorithms, auxiliary feature, learning to match, parallelization strategy, complex cross talking patterns, learning-to-match, Collaboration, parallel matrix factorization, matching problems, Time complexity, iterative relaxation, Web search]
Robust Spectral Learning for Unsupervised Feature Selection
2014 IEEE International Conference on Data Mining
None
2014
In this paper, we consider the problem of unsupervised feature selection. Recently, spectral feature selection algorithms, which leverage both graph Laplacian and spectral regression, have received increasing attention. However, existing spectral feature selection algorithms suffer from two major problems: 1) since the graph Laplacian is constructed from the original feature space, noisy and irrelevant features may have adverse effect on the estimated graph Laplacian and hence degenerate the quality of the induced graph embedding, 2) since the cluster labels are discrete in natural, relaxing and approximating these labels into a continuous embedding can inevitably introduce noise into the estimated cluster labels. Without considering the noise in the cluster labels, the feature selection process may be misguided. In this paper, we propose a Robust Spectral learning framework for unsupervised Feature Selection (RSFS), which jointly improves the robustness of graph embedding and sparse spectral regression. Compared with existing methods which are sensitive to noisy features, our proposed method utilizes a robust local learning method to construct the graph Laplacian and a robust spectral regression method to handle the noise on the learned cluster labels. In order to solve the proposed optimization problem, an efficient iterative algorithm is proposed. We also show the close connection between the proposed robust spectral regression and robust Huber M-estimator. Experimental results on different datasets show the superiority of RSFS.
[iterative methods, optimization problem, Noise, graph theory, robust Huber M-estimator, regression analysis, sparse spectral regression, Optimization, Accuracy, robust local learning method, Clustering algorithms, Robustness, graph Laplacian, robust spectral learning framework for unsupervised feature selection, feature selection, estimated cluster labels, Laplace equations, induced graph embedding, RSFS, spectral feature selection algorithms, iterative algorithm, unsupervised learning, pattern clustering, robust spectral regression method, Mutual information]
Flow-Based Influence Graph Visual Summarization
2014 IEEE International Conference on Data Mining
None
2014
Visually mining a large influence graph is appealing yet challenging. Existing summarization methods enhance the visualization with blocked views, but have adverse effect on the latent influence structure. How can we visually summarize a large graph to maximize influence flows? In particular, how can we illustrate the impact of an individual node through the summarization? Can we maintain the appealing graph metaphor while preserving both the overall influence pattern and fine readability? To answer these questions, we first formally define the influence graph summarization problem. Second, we propose an end-to-end framework to solve the new problem. Last, we report our experiment results. Evidences demonstrate that our framework can effectively approximate the proposed influence graph summarization objective while outperforming previous methods in a typical scenario of visually mining academic citation networks.
[Visualization, influence graph summarization problem, visualization, academic citation networks, flow-based influence graph visual summarization, Pipelines, summarization methods, Linear programming, Topology, Matrix decomposition, Data mining, flow visualisation, appealing graph metaphor, graphs, Clustering algorithms, data visualisation, large influence graph, influence graph summarization objective, influence flow, influence graph]
Distributed Methods for High-Dimensional and Large-Scale Tensor Factorization
2014 IEEE International Conference on Data Mining
None
2014
Given a high-dimensional and large-scale tensor, how can we decompose it into latent factors? Can we process it on commodity computers with limited memory? These questions are closely related to recommendation systems exploiting context information such as time and location. They require tensor factorization methods scalable with both the dimension and size of a tensor. In this paper, we propose two distributed tensor factorization methods, SALS and CDTF. Both methods are scalable with all aspects of data, and they show an interesting trade-off between convergence speed and memory requirements. SALS updates a subset of the columns of a factor matrix at a time, and CDTF, a special case of SALS, updates one column at a time. On our experiment, only our methods factorize a 5-dimensional tensor with 1B observable entries, 10M mode length, and 1K rank, while all other state-of-the-art methods fail. Moreover, our methods require several orders of magnitude less memory than the competitors. We implement our methods on MapReduce with two widely applicable optimization techniques: local disk caching and greedy row assignment.
[large-scale tensor factorization, local disk caching, Scalability, matrix decomposition, CDTF, parallel processing, high-dimensional tensor factorization, Distributed computing, Optimization, MapReduce, distributed tensor factorization methods, greedy row assignment, Distributed databases, Tensor factorization, subset alternating least square, Recommender system, least squares approximations, coordinate descent for tensor factorization, Matrix decomposition, factor matrix, Tensile stress, SALS, Memory management, optimization techniques, Tin, data handling]
Parallel Corpus Approach for Name Matching in Record Linkage
2014 IEEE International Conference on Data Mining
None
2014
Record linkage, or entity resolution, is an important area of data mining. Name matching is a key component of systems for record linkage. Alternative spellings of the same name are a common occurrence in many applications. We use the largest collection of genealogy person records in the world together with user search query logs to build name-matching models. The procedure for building a crowd-sourced training set is outlined together with the presentation of our method. We cast the problem of learning alternative spellings as a machine translation problem at the character level. We use information retrieval evaluation methodology to show that this method substantially outperforms on our data a number of standard well known phonetic and string similarity methods in terms of precision and recall. Our result can lead to a significant practical impact in entity resolution applications.
[Record Linkage, Crowd Sourcing, record linkage, parallel corpus approach, name matching, string similarity methods, data mining, Machine Translation, Data mining, parallel processing, Training, query processing, user search query logs, character level, Databases, information retrieval evaluation methodology, phonetic method, machine translation problem, Computational modeling, Buildings, Probability, entity resolution, Couplings, crowd-sourced training set, string matching, genealogy person records, language translation]
Metric Ranking of Invariant Networks with Belief Propagation
2014 IEEE International Conference on Data Mining
None
2014
The management of large-scale distributed information systems relies on the effective use and modeling of monitoring data collected at various points in the distributed information systems. A promising approach is to discover invariant relationships among the monitoring data and generate invariant networks, where a node is a monitoring data source (metric) and a link indicates an invariant relationship between two monitoring data. Such an invariant network representation can help system experts to localize and diagnose the system faults by examining those broken invariant relationships and their related metrics, because system faults usually propagate among the monitoring data and eventually lead to some broken invariant relationships. However, at one time, there are usually a lot of broken links (invariant relationships) within an invariant network. Without proper guidance, it is difficult for system experts to manually inspect this large number of broken links. Thus, a critical challenge is how to effectively and efficiently rank metrics (nodes) of invariant networks according to the anomaly levels of metrics. The ranked list of metrics will provide system experts with useful guidance for them to localize and diagnose the system faults. To this end, we propose to model the nodes and the broken links as a Markov Random Field (MRF), and develop an iteration algorithm to infer the anomaly of each node based on belief propagation (BP). Finally, we validate the proposed algorithm on both real-world and synthetic data sets to illustrate its effectiveness.
[Measurement, MRF, Invariant Networks, Invariant, ARX Model, invariant networks, belief propagation, data source, Time series analysis, large-scale distributed information systems, distributed processing, Belief Propogation, Markov random field, Information systems, monitoring data, Benchmark testing, Markov processes, metric ranking, Data models, information systems, belief networks, Monitoring, Belief propagation]
High-Dimensional Data Stream Classification via Sparse Online Learning
2014 IEEE International Conference on Data Mining
None
2014
The amount of data in our society has been exploding in the era of big data today. In this paper, we address several open challenges of big data stream classification, including high volume, high velocity, high dimensionality, and high sparsity. Many existing studies in data mining literature solve data stream classification tasks in a batch learning setting, which suffers from poor efficiency and scalability when dealing with big data. To overcome the limitations, this paper investigates an online learning framework for big data stream classification tasks. Unlike some existing online data stream classification techniques that are often based on first-order online learning, we propose a framework of Sparse Online Classification (SOC) for data stream classification, which includes some state-of-the-art first-order sparse online learning algorithms as special cases and allows us to derive a new effective second-order online learning algorithm for data stream classification. We conduct an extensive set of experiments, in which encouraging results validate the efficacy of the proposed algorithms in comparison to a family of state-of-the-art techniques on a variety of data stream classification tasks.
[Algorithm design and analysis, pattern classification, high-dimensional data stream classification, Machine learning algorithms, data stream classification, Error analysis, data analysis, SOC, first-order sparse online learning algorithms, Big Data, Electronic mail, Training, online data stream classification techniques, second-order online learning algorithm, data stream classification tasks, sparse, Prediction algorithms, Big data, sparse online classification, learning (artificial intelligence), Big Data stream classification, online learning]
On Sparse Feature Attacks in Adversarial Learning
2014 IEEE International Conference on Data Mining
None
2014
Adversarial learning is the study of machine learning techniques deployed in non-benign environments. Example applications include classifications for detecting spam email, network intrusion detection and credit card scoring. In fact as the gamut of application domains of machine learning grows, the possibility and opportunity for adversarial behavior will only increase. Till now, the standard assumption about modeling adversarial behavior has been to empower an adversary to change all features of the classifiers at will. The adversary pays a cost proportional to the size of "attack". We refer to this form of adversarial behavior as a dense feature attack. However, the aim of an adversary is not just to subvert a classifier but carry out data transformation in a way such that spam continues to appear like spam to the user as much as possible. We demonstrate that an adversary achieves this objective by carrying out a sparse feature attack. We design an algorithm to show how a classifier should be designed to be robust against sparse adversarial attacks. Our main insight is that sparse feature attacks are best defended by designing classifiers which use &#x2113;<sub>1</sub> regularizers.
[attack size, adversarial learning, nonbenign environments, unsolicited e-mail, &#x2113;<sub>1</sub> regularizers, Electronic mail, machine learning techniques, computer crime, classifier, Robustness, data transformation, learning (artificial intelligence), adversarial behavior modeling, pattern classification, l1 regularizer, sparse feature attack, Vectors, Sparse modelling, Game theory, sparse adversarial attacks, Games, Data models, dense feature attack, spam, Adversarial learning, Logistics]
Stability-Based Stopping Criterion for Active Learning
2014 IEEE International Conference on Data Mining
None
2014
While active learning has drawn broad attention in recent years, there are relatively few studies on stopping criterion for active learning. We here propose a novel model stability based stopping criterion, which considers the potential of each unlabeled examples to change the model once added to the training set. The underlying motivation is that active learning should terminate when the model does not change much by adding remaining examples. Inspired by the widely used stochastic gradient update rule, we use the gradient of the loss at each candidate example to measure its capability to change the classifier. Under the model change rule, we stop active learning when the changing ability of all remaining unlabeled examples is less than a given threshold. We apply the stability-based stopping criterion to two popular classifiers: logistic regression and support vector machines (SVMs). It can be generalized to a wide spectrum of learning models. Substantial experimental results on various UCI benchmark data sets have demonstrated that the proposed approach outperforms state-of-art methods in most cases.
[support vector machines, Stability, Stopping criterion, Active learning, stability-based stopping criterion, regression analysis, SVM, Training, Support vector machines, active learning, support vector machine, Stability criteria, Benchmark testing, stochastic gradient update rule, Data models, learning (artificial intelligence), stochastic processes, logistic regression, gradient methods, stability, Logistics]
Hashtag Graph Based Topic Model for Tweet Mining
2014 IEEE International Conference on Data Mining
None
2014
Mining topics in Twitter is increasingly attracting more attention. However, the shortness and informality of tweets leads to extreme sparse vector representation with a large vocabulary, which makes the conventional topic models (e.g., Latent Dirichlet Allocation) often fail to achieve high quality underlying topics. Luckily, tweets always show up with rich user-generated hash tags as keywords. In this paper, we propose a novel topic model to handle such semi-structured tweets, denoted as Hash tag Graph based Topic Model (HGTM). By utilizing relation information between hash tags in our hash tag graph, HGTM establishes word semantic relations, even if they haven't co-occurred within a specific tweet. In addition, we enhance the dependencies of both multiple words and hash tags via latent variables (topics) modeled by HGTM. We illustrate that the user-contributed hash tags could serve as weakly-supervised information for topic modeling, and hash tag relation could reveal the semantic relation between tweets. Experiments on a real-world twitter data set show that our model provides an effective solution to discover more distinct and coherent topics than the state-of-the-art baselines and has a strong ability to control sparseness and noise in tweets.
[keywords, extreme sparse vector representation, data mining, Twitter, Educational institutions, topic modeling, Vectors, tweet mining, twitter data set, weakly supervised information, Analytical models, user-generated hash tags, hash tag relation, latent variables, Semantics, topics mining, social networking (online), semistructured tweets, HGTM, word semantic relations, Data models, Mathematical model, hash tag graph based topic model]
Contrary to Popular Belief Incremental Discretization can be Sound, Computationally Efficient and Extremely Useful for Streaming Data
2014 IEEE International Conference on Data Mining
None
2014
Discretization of streaming data has received surprisingly little attention. This might be because streaming data require incremental discretization with cut points that may vary over time and this is perceived as undesirable. We argue, to the contrary, that it can be desirable for a discretization to evolve in synchronization with an evolving data stream, even when the learner assumes that attribute values' meanings remain invariant over time. We examine the issues associated with discretization in the context of distribution drift and develop computationally efficient incremental discretization algorithms. We show that discretization can reduce the error of a classical incremental learner and that allowing a discretization to drift in synchronization with distribution drift can further reduce error.
[Context, Time-frequency analysis, data streaming, incremental discretization, Vectors, Synchronization, synchronisation, Histograms, distribution drift, Electricity, Approximation algorithms, synchronization, data handling]
Scalable Multi-instance Learning
2014 IEEE International Conference on Data Mining
None
2014
Multi-instance learning (MIL) has been widely applied to diverse applications involving complicated data objects such as images and genes. However, most existing MIL algorithms can only handle small-or moderate-sized data. In order to deal with the large scale problems in MIL, we propose an efficient and scalable MIL algorithm named miFV. Our algorithm maps the original MIL bags into a new feature vector representation, which can obtain bag-level information, and meanwhile lead to excellent performances even with linear classifiers. In consequence, thanks to the low computational cost in the mapping step and the scalability of linear classifiers, miFV can handle large scale MIL data efficiently and effectively. Experiments show that miFV not only achieves comparable accuracy rates with state-of-the-art MIL algorithms, but has hundreds of times faster speed than other MIL algorithms.
[pattern classification, linear classifiers, efficiency, large scale data, Scalability, miFV MIL algorithm, multi-instance learning, data objects, low computational cost, Vectors, feature vector representation, scalability, Training, Accuracy, Clustering algorithms, bag-level information, scalable multiinstance learning, learning (artificial intelligence), Kernel, Principal component analysis]
Exploring Social Influence on Location-Based Social Networks
2014 IEEE International Conference on Data Mining
None
2014
Recently, with the advent of location-based social networking services (LBSNs), travel planning and location-aware information recommendation based on LBSNs have attracted much research attention. In this paper, we study the impact of social relations hidden in LBSNs, i.e., The social influence of friends. We propose a new social influence-based user recommender framework (SIR) to discover the potential value from reliable users (i.e., Close friends and travel experts). Explicitly, our SIR framework is able to infer influential users from an LBSN. We claim to capture the interactions among virtual communities, physical mobility activities and time effects to infer the social influence between user pairs. Furthermore, we intend to model the propagation of influence using diffusion-based mechanism. Moreover, we have designed a dynamic fusion framework to integrate the features mined into a united follow probability score. Finally, our SIR framework provides personalized top-k user recommendations for individuals. To evaluate the recommendation results, we have conducted extensive experiments on real datasets (i.e., The Go Walla dataset). The experimental results show that the performance of our SIR framework is better than the state-of the-art user recommendation mechanisms in terms of accuracy and reliability.
[user recommendation mechanisms, reliability, user interfaces, travel planning, location-based social networks, virtual communities, diffusion-based mechanism, dynamic fusion framework, Cities and towns, location-based social networking services, Mathematical model, location-aware information recommendation, Social network services, Educational institutions, probability score, personalized top-k user recommendations, Equations, Tuning, recommender systems, LBSN, social relations, Heating, mobility activities, social networking (online), SIR framework, influential users, social influence-based user recommender framework]
On Spectral Analysis of Signed and Dispute Graphs
2014 IEEE International Conference on Data Mining
None
2014
This paper presents a study of signed networks from both theoretical and practical aspects. On the theoretical aspect, we conduct theoretical study based on matrix perturbation theorem for analyzing community structures of complex signed networks and show how the negative edges affect distributions and patterns of node spectral coordinates in the spectral space. We prove and demonstrate cluster orthogonality for two types of signed networks: graph with dense inter-community mixed sign edges and k-dispute graph. We show why the line orthogonality pattern does not hold in the spectral space for these two types of networks. On the practical aspect, we have developed a clustering method to study signed networks and k-dispute networks. Empirical evaluations on both synthetic and real networks show our algorithm outperforms existing clustering methods on signed networks in terms of accuracy.
[dense intercommunity mixed sign edges, Spectral graph analysis, line orthogonality pattern, Communities, network theory (graphs), spectral analysis, complex signed network, Partitioning algorithms, complex networks, k-dispute graph, cluster orthogonality, Equations, Spectral analysis, matrix algebra, matrix perturbation, Clustering algorithms, node spectral coordinate, social network analysis, Eigenvalues and eigenfunctions, signed graph, matrix perturbation theorem, Erbium]
Document-Specific Keyphrase Extraction Using Sequential Patterns with Wildcards
2014 IEEE International Conference on Data Mining
None
2014
Finding good keyphrases for a document is beneficial for many applications, such as text summarization, browsing, and indexing. In this paper, we propose a sequential pattern mining based document-specific keyphrase extraction method. Our key innovation is to use wildcards (or gap constraints) to help extract sequential patterns, where the flexible wildcard constraints within a pattern can capture semantic relationships between words. To achieve this goal, we regard each single document as a sequential dataset, and propose an efficient algorithm to mine sequential patterns with wildcard and one-off conditions that allows important keyphrases to be captured during the mining process. For each extracted keyphrase candidate, we use some statistical pattern features to characterize it. A supervised learning classifier is trained to identify keyphrases from a test document. Comparisons on keyphrase benchmark datasets confirm that our document-specific keyphrase extraction method is effective in improving the quality of extracted keyphrases.
[text analysis, wildcard constraints, statistical pattern features, data mining, mining process, wildcards, Data mining, keyphrases identification, keyphrase benchmark datasets, Databases, Semantics, keyphrase extraction, document-specific keyphrase extraction, learning (artificial intelligence), pattern classification, Educational institutions, supervised learning classifier, classification, sequential patterns extraction, gap constraints, sequential dataset, Feature extraction, sequential pattern mining, statistical analysis, Time complexity, semantic relationships, Microprogramming]
ORION: Online Regularized Multi-task Regression and Its Application to Ensemble Forecasting
2014 IEEE International Conference on Data Mining
None
2014
Ensemble forecasting is a well-known numerical prediction technique for modeling the evolution of nonlinear dynamic systems. The ensemble member forecasts are generated from multiple runs of a computer model, where each run is obtained by perturbing the starting condition or using a different model representation of the dynamic system. The ensemble mean or median is typically chosen as the consensus point estimate of the aggregated forecasts for decision making purposes. These approaches are limited in that they assume each ensemble member is equally skill ful and do not consider their inherent correlations. In this paper, we cast the ensemble forecasting task as an online, multi-task regression problem and present a framework called ORION to estimate the optimal weights for combining the ensemble members. The weights are updated using a novel online learning with restart strategy as new observation data become available. Experimental results on seasonal soil moisture predictions from 12 major river basins in North America demonstrate the superiority of the proposed approach compared to the ensemble median and other baseline methods.
[Computational modeling, Online Multi-task Learning, regression analysis, Predictive models, geophysics computing, Forecasting, Equations, Ensemble Forecasting, ensemble forecasting task, weather forecasting, nonlinear dynamic system, Prediction algorithms, Data models, ORION, online regularized multitask regression, learning (artificial intelligence), Manganese, online learning]
Learning Low-Rank Label Correlations for Multi-label Classification with Missing Labels
2014 IEEE International Conference on Data Mining
None
2014
Multi-label learning deals with the problem where each training example is associated with a set of labels simultaneously, with the set of labels corresponding to multiple concepts or semantic meanings. Intuitively, the multiple labels are usually correlated in some semantic space while sharing the same input space. As a consequence, the multi-label learning process can be augmented significantly by exploiting the label correlations effectively. Most of the existing approaches share the limitations in that the label correlations are typically taken as prior knowledge, which may not depict the true dependencies among labels correctly, or they do not adequately address the issue of missing labels. In this paper, we propose an integrated framework that learns the correlations among labels while training the multi-label model simultaneously. Specifically, a low rank structure is adopted to capture the complex correlations among labels. In addition, we incorporate a supplementary label matrix which augments the possibly incomplete label matrix by exploiting the label correlations. An alternating algorithm is then developed to solve the optimization problem. Extensive experiments are conducted on a number of image and text data sets to demonstrate the effectiveness of the proposed approach.
[low-rank label correlation learning, Adaptation models, text analysis, optimization problem, Correlation, image classification, supplementary label matrix, label correlation, multilabel learning, Training, optimisation, Semantics, image data sets, missing labels, semantic space, learning (artificial intelligence), multi-label learning, low rank, Oceans, Birds, Vectors, alternating algorithm, matrix algebra, text data sets, multilabel classification]
Learning Sparse Gaussian Bayesian Network Structure by Variable Grouping
2014 IEEE International Conference on Data Mining
None
2014
Bayesian networks (BNs) are popular for modeling conditional distributions of variables and causal relationships, especially in biological settings such as protein interactions, gene regulatory networks and microbial interactions. Previous BN structure learning algorithms treat variables with similar tendency separately. In this paper, we propose a grouped sparse Gaussian BN (GSGBN) structure learning algorithm which creates BN based on three assumptions: (i) variables follow a multivariate Gaussian distribution, (ii) the network only contains a few edges (sparse), (iii) similar variables have less-divergent sets of parents, while not-so-similar ones should have divergent sets of parents (variable grouping). We use L<sub>1</sub> regularization to make the learned network sparse, and another term to incorporate shared information among variables. For similar variables, GSGBN tends to penalize the differences of similar variables' parent sets more, compared to those not-so-similar variables' parent sets. The similarity of variables is learned from the data by alternating optimization, without prior domain knowledge. Based on this new definition of the optimal BN, a coordinate descent algorithm and a projected gradient descent algorithm are developed to obtain edges of the network and also similarity of variables. Experimental results on both simulated and real datasets show that GSGBN has substantially superior prediction performance for structure learning when compared to several existing algorithms.
[not-so-similar variable parent sets, coordinate descent algorithm, Gaussian distribution, microbial interactions, Probability distribution, sparsity, Optimization, optimisation, biology computing, similar variable parent sets, multivariate Gaussian distribution, Benchmark testing, Bismuth, causal relationships, alternating optimization, belief networks, learning (artificial intelligence), gradient methods, Bayesian network, biological settings, grouped sparse Gaussian BN, Linear regression, L<sub>1</sub> regularization, variable conditional distributions, GSGBN structure learning algorithm, variable grouping, Sensitivity, sparse Gaussian Bayesian network structure learning, projected gradient descent algorithm]
Learning from Label and Feature Heterogeneity
2014 IEEE International Conference on Data Mining
None
2014
Multiple types of heterogeneity, such as label heterogeneity and feature heterogeneity, often co-exist in many real-world data mining applications, such as news article categorization, gene functionality prediction. To effectively leverage such heterogeneity, in this paper, we propose a novel graph-based framework for Learning with both Label and Feature heterogeneities, namely L2F. It models the label correlation by requiring that any two label-specific classifiers behave similarly on the same views if the associated labels are similar, and imposes the view consistency by requiring that view-based classifiers generate similar predictions on the same examples. To solve the resulting optimization problem, we propose an iterative algorithm, which is guaranteed to converge to the global optimum. Furthermore, we analyze its generalization performance based on Rademacher complexity, which sheds light on the benefits of jointly modeling the label and feature heterogeneity. Experimental results on various data sets show the effectiveness of the proposed approach.
[iterative methods, pattern classification, Correlation, graph theory, data mining, label heterogeneity, Linear programming, real-world data mining applications, Vectors, Loss measurement, multilabel learning, Complexity theory, iterative algorithm, Optimization, label-specific classifiers, heterogeneity, optimisation, feature heterogeneity, multi-view learning, optimization, Rademacher complexity, Diabetes, learning (artificial intelligence), multi-label learning]
Learning from Imbalanced Data in Relational Domains: A Soft Margin Approach
2014 IEEE International Conference on Data Mining
None
2014
We consider the problem of learning probabilistic models from relational data. One of the key issues with relational data is class imbalance where the number of negative examples far outnumbers the number of positive examples. The common approach for dealing with this problem is the use of sub-sampling of negative examples. We, on the other hand, consider a soft margin approach that explicitly trades off between the false positives and false negatives. We apply this approach to the recently successful formalism of relational functional gradient boosting. Specifically, we modify the objective function of the learning problem to explicitly include the trade-off between false positives and negatives. We show empirically that this approach is more successful in handling the class imbalance problem than the original framework that weighed all the examples equally.
[Measurement, sampling methods, imbalanced data, relational functional gradient boosting, Computational modeling, data mining, probability, Boosting, Probabilistic logic, Electronic mail, Standards, probabilistic model, relational data, class imbalance, Cost function, soft margin approach, gradient methods, subsampling method]
K-MEAP: Generating Specified K Clusters with Multiple Exemplars by Efficient Affinity Propagation
2014 IEEE International Conference on Data Mining
None
2014
Recently, an attractive clustering approach named multi-exemplar affinity propagation (MEAP) has been proposed as an extension to the single exemplar based Affinity Propagation (AP). MEAP is able to automatically identify multiple exemplars for each cluster associated with a super exemplar. However, if the cluster number is a prior knowledge and can be specified by the user, MEAP is unable to make use of such knowledge directly in its learning process. Instead it has to rely on re-running the process as many times as it takes by tuning parameters until it generates the desired number of clusters. The process of MEAP re-running may be very time consuming. In this paper, we propose a new clustering algorithm called KMEAP which is able to generate specified K clusters directly while retaining the advantages of MEAP. Two kinds of new additional messages are introduced in MEAP in order to control the number of clusters in the process of message passing. The detailed problem formulation, the derived updating rules for passing messages, and the in-depth analysis of the proposed K-MEAP are provided. Experimental studies demonstrated that K-MEAP not only generates K clusters directly and efficiently without tuning parameters, but also outperforms related approaches in terms of clustering accuracy.
[message passing, multiple exemplars, specified K clusters, multiexemplar affinity propagation, Linear programming, attractive clustering approach, Tuning, Couplings, K-MEAP, Accuracy, Message passing, pattern clustering, Clustering algorithms, clustering, affinity propagation, Time complexity]
Naive-Bayes Inspired Effective Pre-Conditioner for Speeding-Up Logistic Regression
2014 IEEE International Conference on Data Mining
None
2014
We propose an alternative parameterization of Logistic Regression (LR) for the categorical data, multi-class setting. LR optimizes the conditional log-likelihood over the training data and is based on an iterative optimization procedure to tune this objective function. The optimization procedure employed may be sensitive to scale and hence an effective pre-conditioning method is recommended. Many problems in machine learning involve arbitrary scales or categorical data (where simple standardization of features is not applicable). The problem can be alleviated by using optimization routines that are invariant to scale such as (second-order) Newton methods. However, computing and inverting the Hessian is a costly procedure and not feasible for big data. Thus one must often rely on first-order methods such as gradient descent (GD), stochastic gradient descent (SGD) or approximate second-order such as quasi-Newton (QN) routines, which are not invariant to scale. This paper proposes a simple yet effective pre-conditioner for speeding-up LR based on naive Bayes conditional probability estimates. The idea is to scale each attribute by the log of the conditional probability of that attribute given the class. This formulation substantially speeds-up LR's convergence. It also provides a weighted naive Bayes formulation which yields an effective framework for hybrid generative-discriminative classification.
[optimization routines, parameterisation, second-order Newton methods, convergence, regression analysis, Niobium, Optimization, naive-Bayes inspired effective preconditioner, Convergence, Training, optimisation, categorical data, discriminative-generative learning, hybrid generative-discriminative classification, Mathematical model, learning (artificial intelligence), logistic regression, Newton method, pattern classification, naive Bayes conditional probability estimates, machine learning, classification, weighted naive Bayes, stochastic gradient descent, Equations, LR convergence, weighted naive Bayes formulation, pre-conditioning, iterative optimization, conditional log-likelihood, Bayes methods, multiclass setting, preconditioning method, Logistics]
Multi-view Clustering via Multi-manifold Regularized Nonnegative Matrix Factorization
2014 IEEE International Conference on Data Mining
None
2014
Multi-view clustering integrates complementary information from multiple views to gain better clustering performance rather than relying on a single view. NMF based multi-view clustering algorithms have shown their competitiveness among different multi-view clustering algorithms. However, NMF fails to preserve the locally geometrical structure of the data space. In this paper, we propose a multi-manifold regularized nonnegative matrix factorization framework (MMNMF) which can preserve the locally geometrical structure of the manifolds for multi-view clustering. MMNMF regards that the intrinsic manifold of the dataset is embedded in a convex hull of all the views' manifolds, and incorporates such an intrinsic manifold and an intrinsic (consistent) coefficient matrix with a multi-manifold regularizer to preserve the locally geometrical structure of the multi-view data space. We use linear combination to construct the intrinsic manifold, and propose two strategies to find the intrinsic coefficient matrix, which lead to two instances of the framework. Experimental results show that the proposed algorithms outperform existing NMF based algorithms for multi-view clustering.
[MMNMF, linear combination, Linear programming, Educational institutions, matrix decomposition, Matrix decomposition, Approximation methods, multimanifold regularized nonnegative matrix factorization, multiview clustering, Convergence, Manifolds, intrinsic coefficient matrix, geometrical structure, pattern clustering, Clustering algorithms]
Investment Recommendation in P2P Lending: A Portfolio Perspective with Risk Management
2014 IEEE International Conference on Data Mining
None
2014
P2P lending is an online platform to make borrowing and investment transactions. A central question on these platforms is how to align the right products with the right investors, thus helping investors to make better decisions. Along this line, tremendous efforts have been devoted to modeling the credits of products and borrowers from an economic perspective. However, these global models are only exploratory in nature and are not practical. In this paper, we focus on the personalized investment recommendation by reconstructing the two steps for investment decision making: what to buy and how much money to pay. Specifically, we first generate a candidate investment recommendation list for each investor that tackles "what to buy" problem. In this process, we consider various unique properties of investment recommendation. Furthermore, according to the portfolio theory, we optimize the shares of each recommended candidate by incorporating the investments an investor currently holds, thus solving the "how much money to pay" problem. Finally, extensive experimental results on a large-scale real world dataset show the effectiveness of our model under various evaluation metrics.
[Measurement, investment transaction, evaluation metrics, P2P Lending, investment decision making, portfolio perspective, borrowing transaction, Investment Recommendation, Portfolio Perspective, financial data processing, Mathematical model, Portfolios, Context, risk management, how much money to pay problem, peer-to-peer computing, Biological system modeling, investment, personalized investment recommendation, Equations, P2P lending, recommender systems, economic perspective, decision making, global models, Investment]
Predicting the Geographical Origin of Music
2014 IEEE International Conference on Data Mining
None
2014
Traditional research into the arts has almost always been based around the subjective judgment of human critics. The use of data mining tools to understand art has great promise as it is objective and operational. We investigate the distribution of music from around the world: geographical ethnomusicology. We cast the problem as training a machine learning program to predict the geographical origin of pieces of music. This is a technically interesting problem as it has features of both classification and regression, and because of the spherical geometry of the surface of the Earth. Because of these characteristics of the representation of geographical positions, most standard classification/regression methods cannot be directly used. Two applicable methods are K-Nearest Neighbors and Random forest regression, which are robust to the non-standard structure of data. We also investigated improving performance through use of bagging. We collected 1,142 pieces of music from 73 countries/areas, and described them using 2 different sets of standard audio descriptors using MARSYAS. 10-fold cross validation was used in all experiments. The experimental results indicate that Random forest regression produces significantly better results than KNN, and the use of bagging improves the performance of KNN. The best performing algorithm achieved a mean great circle distance error of 3,113 km.
[machine learning program, pattern classification, Art, k-nearest neighbor method, data mining, regression analysis, geographical ethnomusicology, Educational institutions, kNN, music geographical origin, Standards, Earth, Training, music, random forest regression, Training data, geography, regression, Feature extraction, learning (artificial intelligence), random forest regression method]
Co-Clustering Structural Temporal Data with Applications to Semiconductor Manufacturing
2014 IEEE International Conference on Data Mining
None
2014
Recent years have witnessed data explosion in semiconductor manufacturing due to advances in instrumentation and storage techniques. In particular, following the same recipe for a certain IC device, multiple tools and chambers can be deployed for the production of this device, during which multiple time series can be collected, such as temperature, impedance, gas flow, electric bias, etc. These time series naturally fit into a two-dimensional array (matrix), i.e., Each element in this array corresponds to a time series for one process variable from one chamber. To leverage the rich structural information in such temporal data, in this paper, we propose a novel framework named C-Struts to simultaneously cluster on the two dimensions of this array. In this framework, we interpret the structural information as a set of constraints on the cluster membership, introduce an auxiliary probability distribution accordingly, and design an iterative algorithm to assign each time series to a certain cluster on each dimension. To the best of our knowledge, we are the first to address this problem. Extensive experiments on benchmark and manufacturing data sets demonstrate the effectiveness of the proposed method.
[electric bias, 2D array, coclustering structural temporal data, Probability distribution, semiconductor technology, statistical distributions, storage techniques, co-clustering, auxiliary probability distribution, structural, Clustering algorithms, Prototypes, semiconductor industry, structural information, semiconductor manufacturing, Manufacturing, instrumentation, data explosion, temporal, Time series analysis, Process control, time series, iterative algorithm, manufacturing data processing, cluster membership, pattern clustering, IC device, manufacturing data sets, Arrays]
Mining Query-Based Subnetwork Outliers in Heterogeneous Information Networks
2014 IEEE International Conference on Data Mining
None
2014
Mining outliers in a heterogeneous information network is a challenging problem: It is even unclear what should be outliers in a large heterogeneous network (e.g., Outliers in the entire bibliographic network consisting of authors, titles, papers and venues). In this study, we propose an interesting class of outliers, query-based sub network outliers: Given a heterogeneous network, a user raises a query to retrieve a set of task-relevant sub networks, among which, sub network outliers are those that significantly deviate from others (e.g., Outliers of author groups among those studying "topic modeling"). We formalize this problem and propose a general framework, where one can query for finding sub network outliers with respect to different semantics. We introduce the notion of sub network similarity that captures the proximity between two sub networks by their membership distributions. We propose an outlier detection algorithm to rank all the sub networks according to their outlierness without tuning parameters. Our quantitative and qualitative experiments on both synthetic and real data sets show that the proposed method outperforms other baselines.
[Patents, tuning parameters, heterogeneous information network, data mining, task-relevant subnetworks, heterogeneous information networks, Linear programming, topic modeling, Vectors, bibliographic systems, Data mining, information networks, outlier detection, membership distributions, Computer science, query processing, bibliographic network, subnetwork similarity, outlier detection algorithm, query-based, Silicon, Mathematical model, query-based subnetwork outlier mining]
Automated Essay Evaluation Augmented with Semantic Coherence Measures
2014 IEEE International Conference on Data Mining
None
2014
Manual grading of students' essays is a time-consuming, labor-intensive and expensive activity for educational institutions. It is nevertheless necessary since essays are considered to be the most useful tool to assess learning outcomes. Automated essay evaluation represents a practical solution to this task, however, its main weakness is predominant focus on vocabulary and text syntax, and limited consideration of text semantics. In this work, we propose an extension to existing automated essay evaluation systems that incorporates additional semantic attributes. We design the novel attributes by transforming sequential parts of an essay into the semantic space and measuring changes between them to estimate coherence of the text. The resulting system (called SAGE - Semantic Automated Grader for Essays) achieves significantly higher grading accuracy compared with 8 other state-of-the-art automated essay evaluation systems.
[Semantic Attributes, text analysis, Correlation, Automated Scoring, automated essay evaluation, SAGE, Essay Evaluation, manual grading, text semantics, vocabulary, Semantics, semantic automated grader for essay, semantic coherence measures, student essay, Weight measurement, natural language processing, Natural Language Processing, Extraterrestrial measurements, Dispersion, text syntax, Pragmatics, Coherence, computer aided instruction, educational institutions, educational institution]
[Publishers information]
2014 IEEE International Conference on Data Mining
None
2014
Provides a listing of current committee members and society officers.
[]
Message from the Conference Chairs
2015 IEEE International Conference on Data Mining
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Program Co-Chairs
2015 IEEE International Conference on Data Mining
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organization
2015 IEEE International Conference on Data Mining
None
2015
Provides a listing of current committee members and society officers.
[]
Program Committee
2015 IEEE International Conference on Data Mining
None
2015
Provides a listing of current committee members and society officers.
[]
Diamond Sampling for Approximate Maximum All-Pairs Dot-Product (MAD) Search
2015 IEEE International Conference on Data Mining
None
2015
Given two sets of vectors, A = {a1&#x2192;, . . . , am&#x2192;} and B = {b1&#x2192;, . . . , bn&#x2192;}, our problem is to find the top-t dot products, i.e., the largest |ai&#x2192; &#x00B7; bj&#x2192;| among all possible pairs. This is a fundamental mathematical problem that appears in numerous data applications involving similarity search, link prediction, and collaborative filtering. We propose a sampling-based approach that avoids direct computation of all mn dot products. We select diamonds (i.e., four-cycles) from the weighted tripartite representation of A and B. The probability of selecting a diamond corresponding to pair (i, j) is proportional to (ai&#x2192; &#x00B7; bj&#x2192;)2, amplifying the focus on the largest-magnitude entries. Experimental results indicate that diamond sampling is orders of magnitude faster than direct computation and requires far fewer samples than any competing approach. We also apply diamond sampling to the special case of maximum inner product search, and get significantly better results than the state-of-theart hashing methods.
[sampling methods, weighted tripartite representation, diamond sampling, Diamonds, approximate MAD search, Search problems, Indexes, Sparse matrices, Data mining, maximum all-pairs dot-product, vectors, mn dot products, Collaboration, search problems, Manganese]
Information Source Detection via Maximum A Posteriori Estimation
2015 IEEE International Conference on Data Mining
None
2015
The problem of information source detection, whose goal is to identify the source of a piece of information from a diffusion process (e.g., computer virus, rumor, epidemic, and so on), has attracted ever-increasing attention from research community in recent years. Although various methods have been proposed, such as those based on centrality, spectral and belief propagation, the existing solutions still suffer from high time complexity and inadequate effectiveness. To this end, we revisit this problem in the paper and present a comprehensive study from the perspective of likelihood approximation. Different from many previous works, we consider both infected and uninfected nodes to estimate the likelihood for the detection. Specifically, we propose a Maximum A Posteriori (MAP) estimator to detect the information source for general graphs with rumor centrality as the prior. To further improve the efficiency, we design two approximate estimators, namely Brute Force Search Approximation (BFSA) and Greedy Search Bound Approximation (GSBA). BFSA tries to traverse the permitted permutations and directly computes the likelihood, while GSBA exploits a strategy of greedy search to find a surrogate upper bound of the probabilities of permitted permutations for a given node, and derives an approximate MAP estimator. Extensive experiments on several network data sets clearly demonstrate the effectiveness of our methods in detecting the single information source.
[MAP estimator, Conferences, greedy algorithms, information source detection, GSBA, brute force search approximation, Data mining, information networks, maximum likelihood estimation, BFSA, likelihood approximation, maximum a posteriori estimation, greedy search, search problems, greedy search bound approximation, maximum a posteriori]
Influential Sustainability on Social Networks
2015 IEEE International Conference on Data Mining
None
2015
In this paper, we study a novel paradigm of viral marketing with the goal to sustain the influential effectiveness in the network. We study from real cases such as the Ice Bucket Challenges for the ALS awareness, and figure out the "easy come and easy go" phenomenon in the marketing promotion. Such a natural property is fully unexplored in the literature, but it will violate the need of many marketing applications which attempt to receive the perpetual attention and support. We thus highlight the problem of Influential Sustainability, to pursue the long-term and effective influence on the network. Given the set of initial seeds S and a threshold &#x03C1;, the goal of Influential Sustainability is to best decide the timing to activate each seed in S so as to maximize the number of iterations in which each iteration will activate the number of inactive nodes more than &#x03C1;. The Influential Sustainability problem is challenging due to its #P-hard nature. In addition to the greedy idea, we further present three strategies to heuristically decide the activating timing for each seed. As demonstrated in the empirical study on real data, instead of only providing the flexibility of striking a compromise between the execution efficiency and the resulting quality, these heuristic algorithms can be executed highly efficiently and meanwhile it is able to sustain the longer period which can continuously activate inactive nodes effectively. The results demonstrate their prominent advantage to be practical algorithms for the promising viral marketing paradigm.
[Algorithm design and analysis, Influential Sustainability, heuristic algorithms, Social network services, Heuristic algorithms, social networks, inactive nodes, marketing promotion, ALS awareness, Network Diffusion, marketing, ice bucket challenges, Social Networks, viral marketing, marketing applications, Approximation algorithms, Market research, social networking (online), Timing, Integrated circuit modeling, influential sustainability problem]
Towards Frequent Subgraph Mining on Single Large Uncertain Graphs
2015 IEEE International Conference on Data Mining
None
2015
Uncertainty is intrinsic to a wide spectrum of real-life applications, which inevitably applies to graph data. Representative uncertain graphs are seen in bio-informatics, social networks, etc. This paper motivates the problem of frequent subgraph mining on single uncertain graphs. We present an enumeration-evaluation algorithm to solve the problem. By showing support computation on an uncertain graph is #P-hard, we develop an approximation algorithm with accuracy guarantee for this purpose. To enhance the solution, we devise optimization techniques to achieve better mining performance. Experiment results on real-life data confirm the usability of the algorithm.
[enumeration-evaluation algorithm, approximation theory, FSM, #P-hard, Conferences, graph theory, single uncertain graph, data mining, frequent subgraph mining, Data mining, optimization techniques, approximation algorithm, single uncertain graphs, computational complexity]
Ensemble of Diverse Sparsifications for Link Prediction in Large-Scale Networks
2015 IEEE International Conference on Data Mining
None
2015
Previous research has aimed to lower the cost of handling large networks by reducing the network size via sparsification. However, when many edges are removed from the network, the information that can be used for link prediction becomes rather limited, and the prediction accuracy thereby drops significantly. To address this issue, we propose a framework called Diverse Ensemble of Drastic Sparsification (DEDS), which constructs ensemble classifiers with good accuracy while keeping the prediction time short. DEDS includes various sparsification methods that are designed to preserve different measures of a network. Therefore, DEDS can generate sparsified networks with significant structural differences and increase the diversity of the ensemble classifier, which is key to improving prediction performance. When a network is drastically sparsified to 0.1% of the original one, DEDS effectively relieves the drop in prediction accuracy and raises the AUC value from 0.52 to 0.70. With a larger sparsification ratio, DEDS is even able to outperform the classifier trained from the original network. As for the efficiency, more than 95% prediction cost can be saved when the network is sparsified to 1% of the original one. If the original network is disk-resident but can fit into main memory after being sparsified, as much as 99.94% of the prediction cost can be saved.
[Algorithm design and analysis, graph theory, large-scale network, network theory (graphs), Size measurement, Approximation methods, DEDS, Clustering algorithms, link prediction, Prediction algorithms, Approximation algorithms, network analysis, ensemble classifier, Computational efficiency, network sparsification, diverse ensemble of drastic sparsification]
Modeling Emerging, Evolving and Fading Topics Using Dynamic Soft Orthogonal NMF with Sparse Representation
2015 IEEE International Conference on Data Mining
None
2015
Dynamic topic models (DTM) are of great use to analyze the evolution of unobserved topics of a text collection over time. Recent years have witnessed the explosive growth of streaming text data emerging from online media, which creates an unprecedented need for DTMs for timely event analysis. While there have been some matrix factorization methods in the literature for dynamic topic modeling, further study is still in great need to model emerging, evolving and fading topics in a more natural and effective way. In light of this, we first propose a matrix factorization model called SONMFSR (Soft Orthogonal NMF with Sparse Representation), which makes full use of soft orthogonal and sparsity constraints for static topic modeling. Furthermore, by introducing the constraints of emerging, evolving and fading topics to SONMFSR, we easily obtain a novel DTM called SONMFSR<sub>d</sub> for dynamic event analysis. Extensive experiments on two public corpora demonstrate the superiority of SONMFSR<sub>d</sub> to some state-of-the-art DTMs in both topic detection and tracking. In particular, SONMFSR<sub>d</sub> shows great potential in real-world applications, where popular topics in Two Sessions 2015 are captured and traced dynamically for possible insights.
[Vocabulary, text analysis, Topic Detection and Tracking (TDT), soft orthogonal NMF with sparse representation, data mining, static topic modeling, SONMFSR<sub>d</sub>, matrix decomposition, topic tracking, Data mining, Sparse matrices, Dynamic Topic Model (DTM), Soft Orthogonality, soft orthogonal constraints, matrix factorization model, topic detection, Fading, static topic mining, Biological system modeling, dynamic event analysis, evolving topics, Probabilistic logic, Non-negative Matrix Factorization (NMF), fading topics, emerging topics, dynamic topic models, sparsity constraints, Sparse Representation, Data models, DTM]
Unobtrusive Sensing Incremental Social Contexts Using Fuzzy Class Incremental Learning
2015 IEEE International Conference on Data Mining
None
2015
By utilizing captured characteristics of surrounding contexts through widely used Bluetooth sensor, user-centric social contexts can be effectively sensed and discovered by dynamic Bluetooth information. At present, state-of-the-art approaches for building classifiers can basically recognize limited classes trained in the learning phase; however, due to the complex diversity of social contextual behavior, the built classifier seldom deals with newly appeared contexts, which results in degrading the recognition performance greatly. To address this problem, we propose, an OSELM (online sequential extreme learning machine) based class incremental learning method for continuous and unobtrusive sensing new classes of social contexts from dynamic Bluetooth data alone. We integrate fuzzy clustering technique and OSELM to discover and recognize social contextual behaviors by real-world Bluetooth sensor data. Experimental results show that our method can automatically cope with incremental classes of social contexts that appear unpredictably in the real-world. Further, our proposed method have the effective recognition capability for both original known classes and newly appeared unknown classes, respectively.
[Bluetooth, Social Contextual behavior, Online Sequential Extreme Learning Machine (OSELM), user-centric social contexts, Mobile handsets, Data mining, ubiquitous computing, Learning systems, online sequential extreme learning machine, unobtrusive sensing incremental social contexts, social contextual behaviors, learning phase, Sensors, learning (artificial intelligence), real-world Bluetooth sensor data, Context, social aspects of automation, class incremental learning method, Class incremental learning, dynamic Bluetooth data alone, fuzzy clustering technique, pattern clustering, dynamic Bluetooth information, Context-aware, Feature extraction, captured characteristics]
Learning Predictive Substructures with Regularization for Network Data
2015 IEEE International Conference on Data Mining
None
2015
Learning a succinct set of substructures that predicts global network properties plays a key role in understanding complex network data. Existing approaches address this problem by sampling the exponential space of all possible subnetworks to find ones of high prediction accuracy. In this paper, we develop a novel framework that avoids sampling by formulating the problem of predictive subnetwork learning as node selection, subject to network-constrained regularization. Our framework involves two steps: (i) subspace learning, and (ii) predictive substructures discovery with network regularization. The framework is developed based upon two mathematically sound techniques of spectral graph learning and gradient descent optimization, and we show that their solutions converge to a global optimum solution - a desired property that cannot be guaranteed by sampling approaches. Through experimental analysis on a number of real world datasets, we demonstrate the performance of our framework against state-of-the-art algorithms, not only based on prediction accuracy but also in terms of domain relevance of the discovered substructures.
[Algorithm design and analysis, global network properties, complex network data, predictive subnetwork learning, gradient descent optimization, graph theory, Electronic mail, node selection, Optimization, spectral graph learning, Diseases, subspace learning, mathematically sound techniques, Prediction algorithms, convex optimization, Silicon, network data regularization, data handling, network-constrained regularization, Yttrium, learning (artificial intelligence), gradient methods, learning predictive substructures]
Jackknifing Documents and Additive Smoothing for Naive Bayes with Scarce Data
2015 IEEE International Conference on Data Mining
None
2015
Nai&#x0308;ve Bayes (NB) classifiers are well-suited to several applications owing to their easy interpretability and maintainability. However, text classification is often hampered by the lack of adequate training data. This motivates the question: how can we train NB more effectively whentraining data is very scarce?In this paper, we introduce an established subsampling techniquefrom statistics -- the jackknife -- into machine learning. Our approachjackknifes documents themselves to create new "pseudo-documents." Theunderlying idea is that although these pseudo-documents do not havesemantic meaning, they are equally representative of the underlyingdistribution of terms. Therefore, they could be used to train any classifierthat learns this underlying distribution, namely, any parametric classifiersuch as NB (but not, for example, non-parametric classifiers such as SVMand k-NN). Furthermore, the marginal value of this additional trainingdata should be the highest precisely when the original data is inadequate. We then show that our jackknife technique is related to the questionof additively smoothing NB via an appropriately defined notion of"adjointness." This relation is surprising since it connects a statisticaltechnique for handling scarce data to a question about the NB model. Accordingly, we are able to shed light on optimal values of the smoothingparameter for NB in the very scarce data regime. We validate our approach on a wide array of standard benchmarks -- both binary and multi-class -- for two event models of multinomial NB. Weshow that the jackknife technique can dramatically improve the accuracyfor both event models of NB in the regime of very scarce training data. Inparticular, our experiments show that the jackknife can make NB moreaccurate than SVM for binary problems in the very scarce training dataregime. We also provide a comprehensive characterization of the accuracyof these important classifiers (for both binary and multiclass) in the veryscarce data regime for benchmark text datasets, without feature selectionand class imbalance.
[Scarce Data, Additives, subsampling technique, parametric classifier, binary problems, text classification, SVM, Niobium, Training, Multinomial Event Models, multinomial NB model, class imbalance, Training data, learning (artificial intelligence), feature selection, document handling, pattern classification, Smoothing methods, sampling methods, statistical technique, scarce data, Naive Bayes classifiers, machine learning, Jackknife Subsampling, Support vector machines, additive smoothing, Naive Bayes, Comparison between Naive Bayes and SVM, Data models, Bayes methods, statistical analysis, NB training, document jackknife technique, NB classifiers]
Network Clustering via Maximizing Modularity: Approximation Algorithms and Theoretical Limits
2015 IEEE International Conference on Data Mining
None
2015
Many social networks and complex systems are found to be naturally divided into clusters of densely connected nodes, known as community structure (CS). Finding CS is one of fundamental yet challenging topics in network science. One of the most popular classes of methods for this problem is to maximize Newman's modularity. However, there is a little understood on how well we can approximate the maximum modularity as well as the implications of finding community structure with provable guarantees. In this paper, we settle definitely the approximability of modularity clustering, proving that approximating the problem within any (multiplicative) positive factor is intractable, unless P = NP. Yet we propose the first additive approximation algorithm for modularity clustering with a constant factor. Moreover, we provide a rigorous proof that a CS with modularity arbitrary close to maximum modularity QOPT might bear no similarity to the optimal CS of maximum modularity. Thus even when CS with near-optimal modularity are found, other verification methods are needed to confirm the significance of the structure.
[Algorithm design and analysis, additive approximation algorithm, QOPT, network theory (graphs), inapproximability, Electronic mail, approximation algorithms, complex networks, Approximation methods, verification methods, near-optimal modularity, complex systems, Clustering algorithms, approximation algorithm, network science, Cascading style sheets, theoretical limits, Community structure, approximation theory, social networks, Partitioning algorithms, network clustering, modularity clustering approximability, CS, provable guarantees, maximizing modularity, pattern clustering, community structure, Newman's modularity, multiplicative positive factor, Approximation algorithms, computational complexity, maximum modularity]
Exceptionally Monotone Models -- The Rank Correlation Model Class for Exceptional Model Mining
2015 IEEE International Conference on Data Mining
None
2015
Exceptional Model Mining strives to find coherent subgroups of the dataset where multiple target attributes interact in an unusual way. One instance of such an investigated form of interaction is Pearson's correlation coefficient between two targets. EMM then finds subgroups with an exceptionally linear relation between the targets. In this paper, we enrich the EMM toolbox by developing the more general rank correlation model class. We find subgroups with an exceptionally monotone relation between the targets. Apart from catering for this richer set of relations, the rank correlation model class does not necessarily require the assumption of target normality, which is implicitly invoked in the Pearson's correlation model class. Furthermore, it is less sensitive to outliers.
[exceptional model mining, Correlation, data mining, general rank correlation model, Data mining, coherent subgroups, Microwave integrated circuits, Analytical models, exceptionally-monotone models, Lungs, multiple target attributes, EMM toolbox, Numerical models, linear relation, Cancer, Pearson correlation coefficient]
Knowing an Object by the Company it Keeps: A Domain-Agnostic Scheme for Similarity Discovery
2015 IEEE International Conference on Data Mining
None
2015
Appropriately defining and then efficiently calculating similarities from large data sets are often essential in data mining, both for building tractable representations and for gaining understanding of data and generating processes. Here we rely on the premise that given a set of objects and their correlations, each object is characterized by its context, i.e. its correlations to the other objects, and that the similarity between two objects therefore can be expressed in terms of the similarity between their respective contexts. Resting on this principle, we propose a data-driven and highly scalable approach for discovering similarities from large data sets by representing objects and their relations as a correlation graph that is transformed to a similarity graph. Together these graphs can express rich structural properties among objects. Specifically, we show that concepts -- representations of abstract ideas and notions -- are constituted by groups of similar objects that can be identified by clustering the objects in the similarity graph. These principles and methods are applicable in a wide range of domains, and will here be demonstrated for three distinct types of objects: codons, artists and words, where the numbers of objects and correlations range from small to very large.
[Context, Correlation, graph theory, data mining, correlation graph, similarity discovery, Electronic mail, Data mining, Computer science, Semantics, similarity graph, domain-agnostic scheme, Computational linguistics]
Accurate Estimation of Generalization Performance for Active Learning
2015 IEEE International Conference on Data Mining
None
2015
Active learning is a crucial method in settings where a human labeling of instances is challenging to obtain. The typical active learning loop builds a model from a few labeled instances, chooses informative unlabeled instances, asks an Oracle (i.e. a human) to label them and then rebuilds the model. Active learning is widely used with much research attention focused on determining which instances to ask the human to label. However, an understudied problem is estimating the accuracy of the learner when instances are added actively. This is a problem because regular cross validation methods may not work well due to the bias in selecting instances to label. We show that existing methods to address the issue of estimating performance are not suitable for practitioners since the scaling coefficients can have high variance, the estimators can produce nonsensical results and the estimates are empirically inaccurate in the classification setting. We propose a new general active learning method which more accurately estimates generalization performance through a sampling step and a new weighted cross validation estimator. Our method can be used with a variety of query strategies and learners. We empirically illustrate the benefits of our method to the practitioner by showing it is more accurate than the standard weighted cross validation estimator and, when used as part of a termination criterion, obtains more accurate estimates of generalization error while having comparable generalization performance.
[sampling methods, Conferences, general active learning method, weighted cross validation estimator, generalisation (artificial intelligence), Standards, Active Learning, Training, Learning systems, generalization performance estimation, sampling step, Training data, Labeling, learning (artificial intelligence), Cross Validation, Logistics]
Discovery of College Students in Financial Hardship
2015 IEEE International Conference on Data Mining
None
2015
College students with financial difficulties refer to those whose families can hardly afford their high tuition in universities, and should be supported by modern funding system. Indeed, students' economic plight negatively impact their mental health, academic performance, as well as their personal and social life. While funding students in financial hardship is widely accepted, there is limited understanding and research on effectively identification of the qualifying students. Traditional approaches relying on advisers' personal assessments are inefficient, and such subjective judgements may not reflect the truth. To this end, in this paper, we explore the data mining techniques for identifying students who are qualified for financial support. Specifically, we investigate students' complex behaviors on campus from multiple perspectives, and develop a learning framework, named Dis-HARD, by jointly incorporating the heterogeneous features to predict the portfolio of stipends a given student should be awarded. Our framework formalizes the above problem as a multi-label learning problem. Along this line, we first extract discriminative features from three perspectives: (i) smartcard usage behavior, (ii) internet usage behavior and (iii) trajectory on campus. Then, we develop a linear loss function with regularization to solve this multi-label classification problem. In addition, to effectively exploit the students' similarity and label dependency, we incorporate the graph Laplacian and composite l2,1-norm into the regularization of our model, and develop are-weighted algorithm to achieve effective optimization. Finally, experiments on real-world data demonstrate that our method consistently provides better performance compared to the existing state-of-the-art methods.
[financial difficulties, mental health, smart cards, linear loss function, model regularization, student similarity, optimisation, universities, Trajectory, 1</sub>-norm, campus, reweighted algorithm, social life, Feature extraction, Internet, stipends, student complex behaviors, Correlation, multilabel learning problem, graph theory, data mining, financial hardship, college student discovery, label dependency, multi-label classification, Data mining, composite &#x2113;<sub>2, smartcard usage behavior, Student behavior analysis, financial support, psychology, Web and internet services, learning framework, graph Laplacian, Portfolios, pattern classification, discriminative feature extraction, tuition, heterogeneous features, portfolio predict, Internet usage behavior, personal life, financial management, sequential pattern mining, personal assessments, educational institutions, academic performance, multilabel classification problem, Dis-HARD]
Monitoring Stealthy Diffusion
2015 IEEE International Conference on Data Mining
None
2015
Starting with the seminal work by Kempe et al., a broad variety of problems, such as targeted marketing and the spread of viruses and malware, have been modeled as selecting a subset of nodes to maximize diffusion through a network. In cyber-security applications, however, a key consideration largely ignored in this literature is stealth. In particular, an attacker often has a specific target in mind, but succeeds only if the target is reached (e.g., by malware) before the malicious payload is detected and corresponding countermeasures deployed. The dual side of this problem is deployment of a limited number of monitoring units, such as cyber-forensics specialists, so as to limit the likelihood of such targeted and stealthy diffusion processes reaching their intended targets. We investigate the problem of optimal monitoring of targeted stealthy diffusion processes, and show that a number of natural variants of this problem are NP-hard to approximate. On the positive side, we show that if stealthy diffusion starts from randomly selected nodes, the defender's objective is submodular, and a fast greedy algorithm has provable approximation guarantees. In addition, we present approximation algorithms for the setting in which an attacker optimally responds to the placement of monitoring nodes by adaptively selecting the starting nodes for the diffusion process. Our experimental results show that the proposed algorithms are highly effective and scalable.
[invasive software, Security Games, network theory (graphs), Monitoring Diffusion, Approximation methods, monitoring units, cyber-security applications, malicious payload detection, malware spreading, virus spreading, Malware, stealthy diffusion process, Monitoring, Computational modeling, greedy algorithms, fast greedy algorithm, Diffusion processes, optimal stealthy diffusion process monitoring, Grippers, NP-hard problem, Monitoring influence maximization, cyber-forensics specialists, Approximation algorithms, social networking (online), randomly selected nodes, computational complexity]
Time Series Segmentation to Discover Behavior Switching in Complex Physical Systems
2015 IEEE International Conference on Data Mining
None
2015
An accurate and automated identification of operational behavior switching is critical to the autonomic management of complex systems. In this paper, we collect sensor readings from those systems, which are treated as time series, and propose a solution to discover switching behaviors by inferring the relationship changes among massive time series. The method first learns a sequence of local relationship models that can best fit the time series data, and then combines the changes of local relationships to identify the system level behavior switching. In the local relationship modeling, we formulate the underlying switching identification as a segmentation problem, and propose a sophisticated optimization algorithm to accurately discover different segments in time series. In addition, we develop a hierarchical optimization strategy to further improve the efficiency of segmentation. To unveil the system level behavior switching, we present a density estimation and mode search algorithm to effectively aggregate the segmented local relationships so that the global switch points can be captured. Our method has been evaluated on both synthetic data and datasets from real systems. Experimental results demonstrate that it can successfully discover behavior switching in different systems.
[switching identification, Switches, segmentation problem, Complexity theory, mode search algorithm, Optimization, optimisation, optimization, hierarchical optimization, segmentation, complex physical systems, time series data, system level behavior switching, Mathematical model, density estimation, ADMM, search problems, pattern classification, Time series analysis, time series segmentation, time series, global switch points, Complex systems, large-scale systems, Aggregates, Physical systems]
Finding Multiple Stable Clusterings
2015 IEEE International Conference on Data Mining
None
2015
Multi-clustering, which tries to find multiple independent ways to partition a data set into groups, has enjoyed many applications, such as customer relationship management, bioinformatics and healthcare informatics. This paper addresses two fundamental questions in multi-clustering: how to model the quality of clusterings and how to find multiple stable clusterings. We introduce to multi-clustering the notion of clustering stability based on Laplacian eigengap, which was originally used in the regularized spectral learning method for similarity matrix learning. We mathematically prove that the larger the eigengap, the more stable the clustering. Consequently, we propose a novel multi-clustering method MSC (for Multiple Stable Clustering). An advantage of our method comparing to the existing multi-clustering methods is that our method does not need any parameter about the number of alternative clusterings in the data set. Our method can heuristically estimate the number of meaningful clusterings in a data set, which is infeasible in the existing multi-clustering methods. We report an empirical study that clearly demonstrates the effectiveness of our method.
[eigengap, Laplace equations, Clustering methods, regularized spectral learning method, MSC method, Laplacian eigengap, multiple stable clustering method, multi-clustering, Stability analysis, clustering stability, clustering quality model, similarity matrix learning, Image color analysis, pattern clustering, Clustering algorithms, Customer relationship management, multiple stable clusterings, Eigenvalues and eigenfunctions, feature subspace]
Learning Label Specific Features for Multi-label Classification
2015 IEEE International Conference on Data Mining
None
2015
Binary relevance (BR) is a well-known framework for multi-label classification. It decomposes multi-label classification into binary (one-vs-rest) classification subproblems, one for each label. The BR approach is a simple and straightforward way for multi-label classification, but it still has several drawbacks. First, it does not consider label correlations. Second, each binary classifier may suffer from the issue of class-imbalance. Third, it can become computationally unaffordable for data sets with many labels. Several remedies have been proposed to solve these problems by exploiting label correlations between labels and performing label space dimension reduction. Meanwhile, inconsistency, another potential drawback of BR, is often ignored by researchers when they construct multi-label classification models. Inconsistency refers to the phenomenon that if an example belongs to more than one class label, then during the binary training stage, it can be considered as both positive and negative example simultaneously. This will mislead binary classifiers to learn suboptimal decision boundaries. In this paper, we seek to solve this problem by learning label specific features for each label. We assume that each label is only associated with a subset of features from the original feature set, and any two strongly correlated class labels can share more features with each other than two uncorrelated or weakly correlated ones. The proposed method can be applied as a feature selection method for multi-label learning and a general strategy to improve multi-label classification algorithms comprising a number of binary classifiers. Comparison with the state-of-the-art approaches manifests competitive performance of our proposed method.
[Algorithm design and analysis, Computers, pattern classification, Correlation, binary classifier, feature selection method, Data mining, learning label specific feature, Training, space dimension reduction, BR approach, multilabel classification model, Prediction algorithms, binary training stage, suboptimal decision boundary, Bayes methods, learning (artificial intelligence), binary classification, feature selection, Binary relevance]
Informative Prediction Based on Ordinal Questionnaire Data
2015 IEEE International Conference on Data Mining
None
2015
Supporting human decision making is a major goal of data mining. The more decision making is critical, the more interpretability is required in the predictive model. This paper proposes a new framework to build a fully interpretable predictive model for questionnaire data, while maintaining high prediction accuracy with regards to the final outcome. Such a model has applications in project risk assessment, in health care, in sentiment analysis and presumably in any real world application that relies on questionnaire data for informative and accurate prediction. Our framework is inspired by models in Item Response Theory (IRT), which were originally developed in psychometrics with applications to standardized tests such as SAT. We first extend these models, which are essentially unsupervised, to the supervised setting. We then derive a distance metric from the trained model to define the informativeness of individual question items. On real-world questionnaire data obtained from information technology projects, we demonstrate the power of this approach in terms of interpretability as well as predictability. To the best of our knowledge, this is the first work that leverages the IRT framework to provide informative and accurate prediction on ordinal questionnaire data.
[Measurement, Sentiment analysis, human decision making, data mining, Medical services, SAT, information technology project, Predictive models, computability, IRT, Data mining, questionnaire data, ordinal questionnaire data, informative prediction, interpretability, prediction accuracy, health care, predictive model, Decision making, sentiment analysis, psychometrics, predictability, accurate prediction, Standards, project risk assessment, metric learning, decision making, item response theory, psychometric testing]
Traveling Salesman in Reverse: Conditional Markov Entropy for Trajectory Segmentation
2015 IEEE International Conference on Data Mining
None
2015
We are interested in inferring the set of waypoints (or intermediate destinations) of a mobility trajectory in the absence of timing information. We find that, by mining a dataset of real mobility traces, computing the entropy of conditional Markov trajectory enables us to uncover waypoints, even though no timing information nor absolute geographic location is provided. We build on this observation and design an efficient algorithm for trajectory segmentation. Our empirical evaluation demonstrates that the entropy-based heuristic used by our segmentation algorithm outperforms alternative approaches as it is 43% more accurate than a geometric approach and 20% more accurate than path-stretch based approach. We further explore the link between trajectory entropy, mobility predictability and the nature of intermediate locations using a route choice model on real city maps.
[Uncertainty, conditional Markov trajectory, Computational modeling, trajectory segmentation, Mobility, conditional entropy, Entropy, travelling salesman problems, conditional Markov entropy, route choice model, entropy, mobility predictability, Markov trajectories, Markov processes, traveling salesman, mobility trajectory, Trajectory, Random variables, Timing, trajectory entropy]
Robust PCA Via Nonconvex Rank Approximation
2015 IEEE International Conference on Data Mining
None
2015
Numerous applications in data mining and machine learning require recovering a matrix of minimal rank. Robust principal component analysis (RPCA) is a general framework for handling this kind of problems. Nuclear norm based convex surrogate of the rank function in RPCA is widely investigated. Under certain assumptions, it can recover the underlying true low rank matrix with high probability. However, those assumptions may not hold in real-world applications. Since the nuclear norm approximates the rank by adding all singular values together, which is essentially a l<sub>1</sub>-norm of the singular values, the resulting approximation erroris not trivial and thus the resulting matrix estimator can be significantly biased. To seek a closer approximation and to alleviate the above-mentioned limitations of the nuclear norm, we propose a nonconvex rank approximation. This approximation to the matrix rank is tighter than the nuclear norm. To solve the associated nonconvex minimization problem, we develop an efficient augmented Lagrange multiplier based optimization algorithm. Experimental results demonstrate that our method outperforms current state-of-the-art algorithms in both accuracy and efficiency.
[approximation theory, pattern classification, nonconvex rank approximation, Conferences, robust principal component analysis, optimization algorithm, Data mining, nuclear norm based convex surrogate, PCA, augmented Lagrange multiplier, nonconvex minimization problem, RPCA, minimisation, principal component analysis]
Automatic Taxonomy Extraction from Bipartite Graphs
2015 IEEE International Conference on Data Mining
None
2015
Given a large bipartite graph that represents objects and their properties, how can we automatically extract semantic information that provides an overview of the data and -- at the same time -- enables us to drill down to specific parts for an in-depth analysis? In this work, we propose extracting a taxonomy that models the relation between the properties via an is a hierarchy. The extracted taxonomy arranges the properties from general to specific providing different levels of abstraction. Our proposed method has the following desirable properties: (a) it requires no user-defined parameters, by exploiting the principle of minimum description length, (b) it is effective, by utilizing the inheritance of objects when representing the hierarchy, and (c) it is scalable, being linear in the number of edges. We demonstrate the effectiveness and scalability of our method on a broad spectrum of real, publicly available graphs from drug-property graphs to social networks with up to 22 million vertices and 286 million edges.
[Taxonomy, graph theory, social networks, Encoding, semantic information extraction, drug-property graphs, Data mining, Physics, automatic taxonomy extraction, Chemistry, bipartite graphs, graphs, Animals, MDL, feature extraction, taxonomies, social networking (online), Bipartite graph]
Point-of-Interest Recommender Systems: A Separate-Space Perspective
2015 IEEE International Conference on Data Mining
None
2015
With the rapid development of Location-based Social Network (LBSN) services, a large number of Point-Of-Interests (POIs) have been available, which consequently raises a great demand of building personalized POI recommender systems. A personalized POI recommender system can significantly assist users to find their preferred POIs and help POI owners to attract more customers. However, it is very challenging to develop a personalized POI recommender system because a user's checkin decision making process is very complex and could be influenced by many factors such as social network and geographical distance. In the literature, a variety of methods have been proposed to tackle this problem. Most of these methods model user's preference for POIs with integrated approaches and consider all candidate POIs as a whole space. However, by carefully examining a longitudinal real-world checkin data, we find that the whole space of users' checkins actually consists of two parts: social friend space and user interest space. The social friend space denotes the set of POI candidates that users' friends have checked-in before and the user interest space refers to the set of POI candidates that are similar to users' historical checkins, but are not visited by their friends yet. Along this line, we develop separate models for the both spaces to recommend POIs. Specifically, in social friend space, we assume users would repeat their friends' historical POIs due to the preference propagation through social networks, and propose a new Social Friend Probabilistic Matrix Factorization (SFPMF) model. In user interest space, we propose a new User Interest Probabilistic Matrix Factorization (UIPMF) model to capture the correlations between a new POI and one user's historical POIs. To evaluate the proposed models, we conduct extensive experiments with many state-of-the-art baseline methods and evaluation metrics on the real-world data set. The experimental results firmly demonstrate the effectiveness of our proposed models.
[Correlation, social friend probabilistic matrix factorization model, evaluation metrics, SFPMF, UIPMF, decision making process, matrix decomposition, Matrix Factorization, personalized POI recommender system, user interest space, Mathematical model, location-based social network service, Recommender systems, UIPMF model, LBSN service, Social network services, Computational modeling, SFPMF model, probability, Probabilistic logic, social friend space, POI Recommendation, recommender systems, user interest probabilistic matrix factorization model, decision making, social networking (online), Data models, point-of-interest recommender system]
Generative Models for Mining Latent Aspects and Their Ratings from Short Reviews
2015 IEEE International Conference on Data Mining
None
2015
A large number of online reviews have been accumulated on the Web, such as Amazon.com and Cnet.com. It is increasingly challenging to digest these reviews for both consumers and firms as the volume of reviews increases. A promising direction to ease such a burden is to automatically identify aspects of a product and reveal each individual's ratings on them from these reviews. The identified and rated aspects can help consumers understand the pros and cons of a product and make their purchase decisions, and help firms learn user feedbacks and improve their products and marketing strategy. While different methods have been introduced to tackle this problem in the past, few of them successfully model the intrinsic connection between aspect and aspect rating particularly in short reviews. To this end, in this paper, we first propose the Aspect Identification and Rating (AIR) model to model observed textual reviews and overall ratings in a generative way, where the sampled aspect rating influences the sampling of sentimental words on this aspect. Furthermore, we enhance AIR model to particularly address one unique characteristic of short reviews that aspects mentioned in reviews may be quite unbalanced, and develop another model namely AIRS. Within AIRS model, we allow an aspect to directly affect the sampling of a latent rating on this aspect in order to capture the mutual influence between aspect and aspect rating through the whole generative process. Finally, we examine our two models and compare them with other methods based on multiple real world data sets, including hotel reviews, beer reviews and app reviews. Experimental results clearly demonstrate the effectiveness and improvement of our models. Other potential applications driven by our results are also shown in the experiments.
[user feedbacks, data mining, individual ratings, Rating, Predictive models, textual reviews, Data mining, AIRS model, beer reviews, Histograms, aspect identification and rating model, reviews, sampled aspect rating, hotel reviews, marketing strategy, Atmospheric modeling, sentiment analysis, sentimental word sampling, latent aspect mining, generative models, marketing data processing, Aspect Identification, Indexes, aspect rating, app reviews, Reviews, online reviews, Web reviews, Cameras, Data models, Web sites]
Leveraging Implicit Relative Labeling-Importance Information for Effective Multi-label Learning
2015 IEEE International Conference on Data Mining
None
2015
In multi-label learning, each training example is represented by a single instance while associated with multiple labels, and the task is to predict a set of relevant labels for the unseen instance. Existing approaches learn from multi-label data by assuming equal labeling-importance, i.e. all the associated labels are regarded to be relevant while their relative importance for the training example are not differentiated. Nonetheless, this assumption fails to reflect the fact that the importance degree of each associated label is generally different, though the importance information is not explicitly accessible from the training examples. In this paper, we show that effective multi-label learning can be achieved by leveraging the implicit relative labeling-importance (RLI) information. Specifically, RLI degrees are formalized as multinomial distribution over the label space, which are estimated by adapting an iterative label propagation procedure. After that, the multi-label prediction model is learned by fitting the estimated multinomial distribution as regularized with popular multi-label empirical loss. Comprehensive experiments clearly validate the usefulness of leveraging implicit RLI information to learn from multi-label data.
[iterative methods, Symmetric matrices, data analysis, Estimation, Predictive models, implicit relative labeling-importance information, multinomial distribution, multilabel learning, multilabel prediction model, Training, Semantics, RLI degrees, label distribution, Yttrium, Reliability, learning (artificial intelligence), iterative label propagation, relative labeling-importance, multi-label learning]
Content-Aware Collaborative Filtering for Location Recommendation Based on Human Mobility Data
2015 IEEE International Conference on Data Mining
None
2015
Location recommendation plays an essential role in helping people find places they are likely to enjoy. Though some recent research has studied how to recommend locations with the presence of social network and geographical information, few of them addressed the cold-start problem, specifically, recommending locations for new users. Because the visits to locations are often shared on social networks, rich semantics (e.g., tweets) that reveal a person's interests can be leveraged to tackle this challenge. A typical way is to feed them into traditional explicit-feedback content-aware recommendation methods (e.g., LibFM). As a user's negative preferences are not explicitly observable in most human mobility data, these methods need draw negative samples for better learning performance. However, prior studies have empirically shown that sampling-based methods don't perform as well as a method that considers all unvisited locations as negative but assigns them a lower confidence. To this end, we propose an Implicit-feedback based Content-aware Collaborative Filtering (ICCF) framework to incorporate semantic content and steer clear of negative sampling. For efficient parameter learning, we develop a scalable optimization algorithm, scaling linearly with the data size and the feature size. Furthermore, we offer a good explanation to ICCF, such that the semantic content is actually used to refine user similarity based on mobility. Finally, we evaluate ICCF with a large-scale LBSN dataset where users have profiles and text content. The results show that ICCF outperforms LibFM of the best configuration, and that user profiles and text content are not only effective at improving recommendation but also helpful for coping with the cold-start problem.
[cold-start problem, collaborative filtering, large-scale LBSN dataset, text content, Implicit feedback, Twitter, semantic content, Sparse matrices, parameter learning, Content aware, Optimization, person interests, Location recommendation, social network, mobile computing, optimisation, Semantics, implicit-feedback based content-aware collaborative filtering framework, geographical information, location recommendation, user similarity, ICCF, Filtering, data size, user negative preferences, feature size, recommender systems, Collaboration, sampling-based methods, social networking (online), scalable optimization algorithm, negative sampling, learning performance, human mobility data]
Community Detection Based on Structure and Content: A Content Propagation Perspective
2015 IEEE International Conference on Data Mining
None
2015
With the recent advances in information networks, the problem of identifying group structure or communities has received a significant amount of attention. Most of the existing principles of community detection or clustering mainly focus on either the topological structure of a network or the node attributes separately, while both of the two aspects provide valuable information to characterize the nature of communities. In this paper we combine the topological structure of a network as well as the content information of nodes in the task of detecting communities in information networks. Specifically, we treat a network as a dynamic system and consider its community structure as a consequence of interactions among nodes. To model the interactions we introduce the principle of content propagation and integrate the aspects of structure and content in a network naturally. We further describe the interactions among nodes in two different ways, including a linear model to approximate influence propagation, and modeling the interactions directly with random walk. Based on interaction modeling, the nature of communities is described by analyzing the stable status of the dynamic system. Extensive experimental results on benchmark datasets demonstrate the superiority of the proposed framework over the state of the art.
[information network, Computational modeling, graph theory, Probability, interaction modeling, Probabilistic logic, Data mining, information networks, content propagation, Analytical models, topological structure, community structure, Benchmark testing, Integrated circuit modeling, community detection]
Mining Indecisiveness in Customer Behaviors
2015 IEEE International Conference on Data Mining
None
2015
In the retail market, the consumers' indecisiveness refers to the inability to make quick and assertive decisions when they choose among competing product options. Indeed, indecisiveness has been investigated in a number of fields, such as economics and psychology. However, these studies are usually based on the subjective customer survey data with some manually defined questions. Instead, in this paper, we provide a focused study on automatically mining indecisiveness in massive customer behaviors in online stores. Specifically, we first give a general definition to measure the observed indecisiveness in each behavior session. From these observed indecisiveness, we can learn the latent factors/reasons by a probabilistic factor-based model. These two factors are the indecisive indexes of the customers and the product bundles, respectively. Next, we demonstrate that this indecisiveness mining process could be useful in several potential applications, such as the competitive product detection and personalized product bundles recommendation. Finally, we perform extensive experiments on a large-scale behavioral logs of online customers in a distributed environment. The results reveal that our measurement of indecisiveness agrees with the common sense assessment, and the discoveries are useful in predicting customer behaviors and providing better recommendation services for both customers and online retailers.
[online retailers, latent factors, Psychology, data mining, online stores, Item Bundle, Manuals, Competition detection, consumer behaviour, distributed environment, Data mining, probabilistic factor-based model, subjective customer survey data, Customer Behaviors, customer behavior indecisiveness mining, consumer indecisiveness, Indecisiveness, retail data processing, large-scale behavioral logs, Decision making, probability, personalized product bundle recommendation, Probabilistic logic, online customers, Indexes, competing product options, Recommendation, retail market, competitive product detection, common sense assessment, Internet, Data-driven]
Robust Multi-Network Clustering via Joint Cross-Domain Cluster Alignment
2015 IEEE International Conference on Data Mining
None
2015
Network clustering is an important problem thathas recently drawn a lot of attentions. Most existing workfocuses on clustering nodes within a single network. In manyapplications, however, there exist multiple related networks, inwhich each network may be constructed from a different domainand instances in one domain may be related to instances in otherdomains. In this paper, we propose a robust algorithm, MCA, formulti-network clustering that takes into account cross-domain relationshipsbetween instances. MCA has several advantages overthe existing single network clustering methods. First, it is ableto detect associations between clusters from different domains, which, however, is not addressed by any existing methods. Second, it achieves more consistent clustering results on multiple networksby leveraging the duality between clustering individual networksand inferring cross-network cluster alignment. Finally, it providesa multi-network clustering solution that is more robust to noiseand errors. We perform extensive experiments on a variety ofreal and synthetic networks to demonstrate the effectiveness andefficiency of MCA.
[Algorithm design and analysis, multinetwork clustering via cluster alignment, Clustering methods, cross-domain relationships, Linear programming, MCA, duality, Optimization, Computer science, pattern clustering, joint cross-domain cluster alignment, Clustering algorithms, Graph Clustering, cross-network cluster alignment, duality (mathematics), Multi-network, Robustness]
A Unified Gradient Regularization Family for Adversarial Examples
2015 IEEE International Conference on Data Mining
None
2015
Adversarial examples are augmented data points generated by imperceptible perturbation of input samples. They have recently drawn much attention with the machine learning and data mining community. Being difficult to distinguish from real examples, such adversarial examples could change the prediction of many of the best learning models including the state-of-the-art deep learning models. Recent attempts have been made to build robust models that take into account adversarial examples. However, these methods can either lead to performance drops or lack mathematical motivations. In this paper, we propose a unified framework to build robust machine learning models against adversarial examples. More specifically, using the unified framework, we develop a family of gradient regularization methods that effectively penalize the gradient of loss function w.r.t. inputs. Our proposed framework is appealing in that it offers a unified view to deal with adversarial examples. It incorporates another recently-proposed perturbation based approach as a special case. In addition, we present some visual effects that reveals semantic meaning in those perturbations, and thus support our regularization method and provide another explanation for generalizability of adversarial examples. By applying this technique to Maxout networks, we conduct a series of experiments and achieve encouraging results on two benchmark datasets. In particular, we attain the best accuracy on MNIST data (without data augmentation) and competitive performance on CIFAR-10 data.
[CIFAR-10 data, data augmentation, Maxout networks, data mining, perturbation based approach, Predictive models, unified gradient regularization family, imperceptible perturbation, Approximation methods, Data mining, Optimization, Training, Deep learning, gradient regularization methods, loss function, MNIST data, machine learning models, Robustness, Mathematical model, learning (artificial intelligence), unified framework, Adversarial examples, augmented data points, deep learning models, mathematical motivations, data mining community, Robust classification, Regularization]
Parallel Hierarchical Clustering in Linearithmic Time for Large-Scale Sequence Analysis
2015 IEEE International Conference on Data Mining
None
2015
The rapid development of sequencing technology has led to an explosive accumulation of genomics data. Clustering is often the first step to perform in sequence analysis, and hierarchical clustering is one of the most commonly used approaches for this purpose. However, the standard hierarchical clustering method scales poorly due to its quadratic time and space complexities stemming mainly from the need of computing and storing a pairwise distance matrix. It is thus necessary to minimize the number of pairwise distances computed without degrading clustering performance. On the other hand, as high-performance computing systems are becoming widely accessible, it is highly desirable that a clustering method can be easily adapted to parallel computing environments for further speedup, which is not a trivial task for hierarchical clustering. We proposed a new hierarchical clustering method that achieves good clustering performance and high scalability on large sequence datasets. It consists of two stages. In the first stage, a new landmark-based active hierarchical divisive clustering method was proposed that partitions a large-scale sequence dataset into groups, and in the second stage, a fast hierarchical agglomerative clustering method is applied to each group. By assembling hierarchies from both stages, the hierarchy of the data can be easily recovered. Theoretical results showed that our method can recover the true hierarchy with a high probability under some mild conditions and has a linearithmic time complexity with respect to the number of input sequences. The proposed method also facilitates an efficient parallel implementation. Empirical results on various datasets showed that our method achieved clustering accuracy comparable to ESPRIT-Tree and ran faster than greedy heuristic methods.
[Algorithm design and analysis, fast hierarchical agglomerative clustering method, Clustering methods, large-scale sequence analysis, landmark-based active hierarchical divisive clustering method, parallel programming, parallel hierarchical clustering, Clustering algorithms, sequencing technology, Parallel processing, genomics, linearithmic time complexity, pairwise distance matrix, data hierarchy recovery, genomics data, Sequences, data analysis, probability, ESPRIT-Tree, parallel implementation, pattern clustering, bioinformatics, quadratic time complexity, large-scale sequence dataset partitioning, Time complexity, parallel computing environment, space complexity, computational complexity]
Accelerating Exact Similarity Search on CPU-GPU Systems
2015 IEEE International Conference on Data Mining
None
2015
In recent years, the use of Graphics Processing Units (GPUs) for data mining tasks has become popular. With modern processors integrating both CPUs and GPUs, it is also important to consider what tasks benefit from GPU processing and which do not, and apply a heterogeneous processing approach to improve the efficiency where applicable. Similarity search, also known as k-nearest neighbor search, is a key part of data mining applications and is used also extensively in applications such as multimedia search, where only a small subset of possible results are used. Our contribution is a new exact kNN algorithm with a compressed partial heapsort that outperforms other state-of-the-art exact kNN algorithms by leveraging both the GPU and CPU.
[similarity search, Instruction sets, Force, Graphics processing units, data mining, heterogeneous processing approach, kNN algorithm, Data mining, Approximation methods, GPU, k-nearest neighbor search, nearest neighbor, parallel, GPU processing, learning (artificial intelligence), search problems, data mining applications, pattern classification, data mining tasks, multiprocessing systems, multimedia search, CPU-GPU systems, heterogeneous processing, compressed partial heapsort, graphics processing units, Sorting, modern processors, heapsort]
Ensemble Kernel Mean Matching
2015 IEEE International Conference on Data Mining
None
2015
The Kernel Mean Matching (KMM) is an elegant algorithm that produces density ratios between training and test data by minimizing their maximum mean discrepancy in a kernel space. The applicability of KMM to large-scale problems is however hindered by the quadratic complexity of calculating and storing the kernel matrices over training and test data. To address this problem, this paper proposes a novel ensemble algorithm for KMM, which divides test samples into smaller partitions, estimates a density ratio for each partition and then fuses these local estimates with a weighted sum. Our theoretical analysis shows that the ensemble KMM has a lower error bound than the centralized KMM, which uses all the test data at once to estimate the density ratio. Considering its suitability for distributed implementation, the proposed algorithm is also favorable in terms of time and space complexities. Experiments on benchmark datasets confirm the superiority of the proposed algorithm in terms of estimation accuracy and running time.
[Algorithm design and analysis, pattern matching, Density ratio estimation, Estimation, data mining, time complexity, KMM, Partitioning algorithms, Complexity theory, kernel mean matching, quadratic complexity, theoretical analysis, Training, density ratio, Kernel mean matching, Distributed algorithm, elegant algorithm, Density functional theory, large-scale problem, Ensemble method, Kernel, space complexity, computational complexity]
Predicting Sports Scoring Dynamics with Restoration and Anti-Persistence
2015 IEEE International Conference on Data Mining
None
2015
Professional team sports provide an excellent domain for studying the dynamics of social competitions. These games are constructed with simple, well-defined rules and payoffs that admit a high-dimensional set of possible actions and nontrivial scoring dynamics. The resulting gameplay and efforts to predict its evolution are the object of great interest to both sports professionals and enthusiasts. In this paper, we consider two online prediction problems for team sports: given a partially observed game Who will score next? and ultimately Who will win? We present novel interpretable generative models of within-game scoring that allow for dependence on lead size (restoration) and on the last team to score (anti-persistence). We then apply these models to comprehensive within-game scoring data for four sports leagues over a ten-year period. By assessing these models' relative goodness-of-fit we shed new light on the underlying mechanisms driving the observed scoring dynamics of each sport. Furthermore, in both predictive tasks, the performance of our models consistently outperforms baselines models, and our models make quantitative assessments of the latent team skill, over time.
[social competition dynamics, Computational modeling, within-game scoring data, Predictive models, Probabilistic logic, professional team sports, goodness-of-fit, History, online prediction problems, sports scoring dynamics prediction, Games, Lead, quantitative assessment, data handling, statistical analysis, sport, Clocks]
Domain-Specific Knowledge Base Enrichment Using Wikipedia Tables
2015 IEEE International Conference on Data Mining
None
2015
The knowledge base is a machine-readable set of knowledge. More and more multi-domain and large-scale knowledge bases have emerged in recent years, and they play an essential role in many information systems and semantic annotation tasks. However we do not have a perfect knowledge base yet and maybe we will never have a perfect one, because all the knowledge bases have limited coverage while new knowledge continues to emerge. Therefore populating and enriching the existing knowledge base become important tasks. Traditional knowledge base population task usually leverages the information embedded in the unstructured free text. Recently researchers found that massive structured tables on the Web are high-quality relational data and easier to be utilized than the unstructured text. Our goal of this paper is to enrich the knowledge base using Wikipedia tables. Here, knowledge means binary relations between entities and we focus on the relations in some specific domains. There are two basic types of information can be used in this task: the existing relation instances and the connection between types and relations. We firstly propose two basic probabilistic models based on two types of information respectively. Then we propose a light-weight aggregated model to combine the advantages of basic models. The experimental results show that our method is an effective approach to enriching the knowledge base with both high precision and recall.
[Electronic publishing, text analysis, high-quality relational data, unstructured free text, Knowledge based systems, probability, Encyclopedias, Wikipedia tables, binary relations, domain-specific knowledge base enrichment, probabilistic models, Relation extraction, Knowledge base enrichment, Databases, Semantics, knowledge based systems, Web tables, Internet, Web sites, light-weight aggregated model]
Fast Parallel Mining of Maximally Informative k-Itemsets in Big Data
2015 IEEE International Conference on Data Mining
None
2015
The discovery of informative itemsets is a fundamental building block in data analytics and information retrieval. While the problem has been widely studied, only few solutions scale. This is particularly the case when i) the data set is massive, calling for large-scale distribution, and/or ii) the length k of the informative itemset to be discovered is high. In this paper, we address the problem of parallel mining of maximally informative k-itemsets (miki) based on joint entropy. We propose PHIKS (Parallel Highly Informative K-ItemSet) a highly scalable, parallel miki mining algorithm. PHIKS renders the mining process of large scale databases (up to terabytes of data) succinct and effective. Its mining process is made up of only two efficient parallel jobs. With PHIKS, we provide a set of significant optimizations for calculating the joint entropies of miki having different sizes, which drastically reduces the execution time of the mining process. PHIKS has been extensively evaluated using massive real-world data sets. Our experimental results confirm the effectiveness of our proposal by the significant scale-up obtained with high itemsets length and over very large databases.
[data mining, maximally informative-itemset discovery, Entropy, Data mining, parallel processing, Optimization, Itemsets, entropy, spark, parallel mining process, parallel maximally informative k-itemset mining, big data, data analysis, data analytics, information retrieval, Big Data, Information retrieval, massive distribution, PHIKS, mapreduce, joint entropy, parallel miki mining algorithm, informative itemsets, large-scale distribution, real-world data sets]
Online Model Evaluation in a Large-Scale Computational Advertising Platform
2015 IEEE International Conference on Data Mining
None
2015
Online media provides opportunities for marketers through which they can deliver effective brand messages to a wide range of audiences at scale. Advertising technology platforms enable advertisers to reach their target audience by delivering ad impressions to online users in real time. In order to identify the best marketing message for a user and to purchase impressions at the right price, we rely heavily on bid prediction and optimization models. Even though the bid prediction models are well studied in the literature, the equally important subject of model evaluation is usually overlooked or not discussed in detail. Effective and reliable evaluation of an online bidding model is crucial for making faster model improvements as well as for utilizing the marketing budgets more efficiently. In this paper, we present an experimentation framework for bid prediction models where our focus is on the practical aspects of model evaluation. Specifically, we outline the unique challenges we encounter in our platform due to a variety of factors such as heterogeneous goal definitions, varying budget requirements across different campaigns, high seasonality and the auction-based environment for inventory purchasing. Then, we introduce return on investment (ROI) as a unified model performance (i.e., success) metric and explain its merits over more traditional metrics such as click-through rate (CTR) or conversion rate (CVR). Most importantly, we discuss commonly used evaluation and metric summarization approaches in detail and propose a more accurate method for online evaluation of new experimental models against the baseline. Our meta-analysis-based approach addresses various shortcomings of other methods and yields statistically robust conclusions that allow us to conclude experiments more quickly in a reliable manner. We demonstrate the effectiveness of our evaluation strategy on real campaign data through some experiments.
[Measurement, marketing budget, Adaptation models, RTB, Predictive models, metric summarization approach, Statistical Test, Analytical models, optimisation, bid prediction model, cost-benefit analysis, Multiple Statistical Test, Real-time systems, Advertising, advertising technology platform, Computational Advertising, tendering, advertising data processing, online model evaluation, brand message, online bidding model, investment, bid optimization model, marketing message, large-scale computational advertising platform, meta-analysis-based approach, ROI, auction-based environment, return-on-investment, statistical analysis, inventory purchasing]
BrainQuest: Perception-Guided Brain Network Comparison
2015 IEEE International Conference on Data Mining
None
2015
Why are some people more creative than others? How do human brain networks evolve over time? A key stepping stone to both mysteries and many more is to compare weighted brain networks. In contrast to networks arising from other application domains, the brain network exhibits its own characteristics (e.g., high density, indistinguishability), which makes any off-the-shelf data mining algorithm as well as visualization tool sub-optimal or even mis-leading. In this paper, we propose a shift from the current mining-then-visualization paradigm, to jointly model these two core building blocks (i.e., mining and visualization) for brain network comparisons. The key idea is to integrate the human perception constraint into the mining block earlier so as to guide the analysis process. We formulate this as a multi-objective feature selection problem, and propose an integrated framework, BrainQuest, to solve it. We perform extensive empirical evaluations, both quantitatively and qualitatively, to demonstrate the effectiveness and efficiency of our approach.
[Visualization, Correlation, perception-guided brain network comparison, visual perception, data mining, BrainQuest, Topology, Data mining, Indexes, data mining algorithm, multiobjective feature selection problem, Network topology, data visualisation, Brain modeling, medical computing, feature selection, mining-then-visualization paradigm]
Top-k Link Recommendation in Social Networks
2015 IEEE International Conference on Data Mining
None
2015
Inferring potential links is a fundamental problem in social networks. In the link recommendation problem, the aim is to suggest a list of potential people to each user, ordered by the preferences of the user. Although various approaches have been developed to solve this problem, the difficulty of producing a ranking list with high precision at the top -- the most important consideration for real world applications -- remains largely an open problem. In this work, we propose two top-k link recommendation algorithms which focus on optimizing the top ranked links. For this purpose, we define a cost-sensitive ranking loss which penalizes the mistakes at the top of a ranked list more than the mistakes at the bottom. In particular, we propose a log loss, derive its surrogate, and formulate a top-k link recommendation model by optimizing this surrogate loss function based upon latent features. Moreover, we extend this top-k link recommendation model by incorporating both the latent features and explicit features of the network. Finally, an efficient learning scheme to learn the model parameters is provided. We conduct empirical studies based upon four real world datasets, i.e., Wikipedia, CondMat, Epinions, and MovieLens 1M, of which the largest network contains more than 70 thousand nodes and over one million links. Our experiments demonstrate that the proposed algorithms outperform several state-of-the-art methods.
[Electronic publishing, Link recommendation, Social network services, social networks, Encyclopedias, Wikipedia, Electronic mail, Epinions, top-k learning to rank, social network, recommender systems, top-k link recommendation algorithm, CondMat, Feature extraction, social networking (online), Internet, learning (artificial intelligence), cost-sensitive ranking, MovieLens 1M, cost-sensitive ranking loss]
The Impact of Patent Activities on Stock Dynamics in the High-Tech Sector
2015 IEEE International Conference on Data Mining
None
2015
Patent data has been used for generating patent-based indicators for tracking the technology development of high-tech companies. In this paper, we further show the promises of exploiting patent data for the analysis and prospecting of high-tech companies in the stock market. Specifically, we aim at investigating the relationship between the patent activities of high-tech companies and the dynamics of their stock price movement. While stock forecasting is a topic of general interest and has been studied extensively in the literature, the most popular forecasting models do not facilitate the discovery of the patent-activity impact all essential characteristics of the market performance of a given stock. To this end, we propose a new approach to analyze the relationships between patent activities and the statistical characteristics of stock prices. To the best of our knowledge, we are the first to propose a model of this nature, relating patent data mining and financial modeling. Also, we demonstrate the relationships of the market-adjusted stock returns and the number of patent applications as well as the diversity of the corresponding patent categories. Moreover, we establish relationships between the monthly drift and volatility of the market-adjusted stock returns and those patent activity indicators. Here, we exploit a widely accepted diffusion model of the stock returns and estimate its parameters. By adopting the moving window technique, we create fitted models by introducing various lagged terms of patent activity characteristics. For each company, we consider the coefficients of each significant term over the entire time horizon and perform further statistical testing on the overall significance of the corresponding indicator. The analysis has been performed on real-world stock trading data as well as patent data. The results confirm the significant impact of patent activity on stock movement and on its essential statistical characteristics of drift and volatility.
[patent applications, data mining, Companies, patent activity characteristics, Data mining, diffusion model, stock forecasting, patents, parameter estimation, monthly drift, financial data processing, stock markets, Stock markets, technology development, stock returns, patent activity indicators, Patents, Technological innovation, high-tech companies, monthly volatility, Patent Data Mining, data analysis, Biological system modeling, Factor Analysis, real-world stock trading data, patent-activity impact discovery, market-adjusted stock returns, stock market performance, financial modeling, patent data mining, economic indicators, stock price movement, moving window technique, statistical characteristics, patent categories, Data models, statistical analysis, Time-series Analysis]
Modeling Adoption and Usage of Competing Products
2015 IEEE International Conference on Data Mining
None
2015
The emergence and wide-spread use of online social networks has led to a dramatic increase on the availability of social activity data. Importantly, this data can be exploited to investigate, at a microscopic level, some of the problems that have captured the attention of economists, marketers and sociologists for decades, such as, e.g., product adoption, usage and competition. In this paper, we propose a continuous-time probabilistic model, based on temporal point processes, for the adoption and frequency of use of competing products, where the frequency of use of one product can be modulated by those of others. This model allows us to efficiently simulate the adoption and recurrent usages of competing products, and generate traces in which we can easily recognize the effect of social influence, recency and competition. We then develop an inference method to efficiently fit the model parameters by solving a convex program. The problem decouples into a collection of smaller subproblems, thus scaling easily to networks with hundred of thousands of nodes. We validate our model over synthetic and real diffusion data gathered from Twitter, and show that the proposed model does not only provides a good fit to the data and more accurate predictions than alternatives but also provides interpretable model parameters, which allow us to gain insights into some of the factors driving product adoption and frequency of use.
[Social network services, Computational modeling, online social networks, Predictive models, modeling adoption, product adoption, Twitter, real diffusion data, History, temporal point processes, microscopic level, continuous-time probabilistic model, competing products, Microscopy, social networking (online), Data models, Mathematical model, social influence, convex program, social activity data]
Experimental Design with Multiple Kernels
2015 IEEE International Conference on Data Mining
None
2015
In classification tasks, labeled data is a necessity but sometimes difficult or expensive to obtain. On the contrary, unlabeled data is usually abundant. Recently, different active learning algorithms are proposed to alleviate this issue by selecting the most informative data points to label. One family of active learning methods comes from Optimum Experimental Design (OED) in statistics. Instead of selecting data points one by one iteratively, OED-based approaches select data in a one-shot manner, that is, a fixed-sized subset is selected from the unlabeled dataset for manually labeling. These methods usually use kernels to represent pair-wise similarities between different data points. It is well known that choosing optimal kernel types (e.g. Gaussian kernel) and kernel parameters (e.g. kernel width) is tricky, and a common way to resolve it is by Multiple Kernel Learning (MKL), i.e., to construct a few candidate kernels and merge them to form a consensus kernel. There would be different ways to combine multiple kernels, one of which, called the the globalised approach is to assign a weight to each candidate kernel. In practice different data points in the same candidate kernel may not have the same contribution in the consensus kernel, this requires assigning different weights to different data points in the same candidate kernel, leading to the localized approach. In this paper, we introduce MKL to OED-based active learning, specifically we propose globalised and localized multiple kernel active learning methods, respectively. Our experiments on six benchmark datasets demonstrate that the proposed methods have better performance than existing OED-based active learning methods.
[Algorithm design and analysis, Uncertainty, kernel width, optimal kernel type, pair-wise similarity representation, Data mining, optimum experimental design, Learning systems, Training, kernel parameter, candidate kernel weight assignment, multiple kernel learning, OED-based active learning, Robustness, informative data points, consensus kernel, learning (artificial intelligence), Kernel, pattern classification, Gaussian kernel, Active learning, active learning algorithm, Multiple Kernels, classification task, MKL, labeled data, Gaussian processes, data handling, design of experiments, Experimental Design, statistics]
Mining Multi-aspect Reflection of News Events in Twitter: Discovery, Linking and Presentation
2015 IEEE International Conference on Data Mining
None
2015
A major event often has repercussions on both news media and microblogging sites such as Twitter. Reports from mainstream news agencies and discussions from Twitter complement each other to form a complete picture. An event can have multiple aspects (sub-events) describing it from multiple angles, each of which attracts opinions/comments posted on Twitter. Mining such reflections is interesting to both policy makers and ordinary people seeking information. In this paper, we propose a unified framework to mine multi-aspect reflections of news events in Twitter. We propose a novel and efficient dynamic hierarchical entity-aware event discovery model to learn news events and their multiple aspects. The aspects of an event are linked to their reflections in Twitter by a bootstrapped dataless classification scheme, which elegantly handles the challenges of selecting informative tweets under overwhelming noise and bridging the vocabularies of news and tweets. In addition, we demonstrate that our framework naturally generates an informative presentation of each event with entity graphs, time spans, news summaries and tweet highlights to facilitate user digestion.
[news summaries, Vocabulary, electronic publishing, news events, event discovery, graph theory, data mining, Twitter, Data mining, twitter, Computer hacking, vocabulary, multiaspect reflection mining, user digestion, news vocabularies, bootstrapped dataless classification scheme, news, Twitter complement, time spans, tweet highlights, news media, Xenon, Media, microblogging sites, informative presentation, entity graphs, policy makers, social networking (online), dynamic hierarchical entity-aware event discovery model, Joining processes]
Multi-level Approximate Spectral Clustering
2015 IEEE International Conference on Data Mining
None
2015
Clustering is a task of finding natural groups in datasets based on measured or perceived similarity between data points. Spectral clustering is a well-known graph-theoretic approach, which is capable of capturing non-convex geometries of datasets. However, it generally becomes infeasible for analyzing large datasets due to relatively high time and space complexity. In this paper, we propose Multi-level Approximate Spectral (MAS) clustering to enable efficient analysis of large datasets. By integrating a series of low-rank matrix approximations (i.e., approximations to the affinity matrix and its subspace, as well as those for the Laplacian matrix and the Laplacian subspace), MAS achieves great computational and spacial efficiency. MAS provides a general framework for fast and accurate spectral clustering, which works with any kernels, various fast sampling strategies and different low-rank approximation algorithms. In addition, it can be easily extended for distributed computing. From a theoretical perspective, we provide rigorous analysis of its approximation error in addition to its correctness and computational complexity. Through extensive experiments we demonstrate superior performance of the proposed method relative to several well-known approximate spectral clustering algorithms.
[Algorithm design and analysis, low-rank matrix approximations, graph theory, low-rank approximation algorithms, Complexity theory, dataset analysis, Approximation methods, graph-theoretic approach, approximation error, distributed computing, multilevel approximate spectral clustering, MAS clustering, Clustering algorithms, Kernel, approximation theory, Laplace equations, data analysis, sampling strategies, time complexity, group theory, pattern clustering, natural groups, Approximation algorithms, nonconvex geometries, space complexity, computational complexity]
KSTR: Keyword-Aware Skyline Travel Route Recommendation
2015 IEEE International Conference on Data Mining
None
2015
With the popularity of social media (e.g., Facebook and Flicker), users could easily share their check-in records and photos during their trips. In view of the huge amount of check-in data and photos in social media, we intend to discover travel experiences to facilitate trip planning. Prior works have been elaborated on mining and ranking existing travel routes from check-in data. We observe that when planning a trip, users may have some keywords about preference on his/her trips. Moreover, a diverse set of travel routes is needed. To provide a diverse set of travel routes, we claim that more features of Places of Interests (POIs) should be extracted. Therefore, in this paper, we propose a Keyword-aware Skyline Travel Route (KSTR) framework that use knowledge extraction from historical mobility records and the user's social interactions. Explicitly, we model the "Where, When, Who" issues by featurizing the geographical mobility pattern, temporal influence and social influence. Then we propose a keyword extraction module to classify the POI-related tags automatically into different types, for effective matching with query keywords. We further design a route reconstruction algorithm to construct route candidates that fulfill the query inputs. To provide diverse query results, we explore Skyline concepts to rank routes. To evaluate the effectiveness and efficiency of the proposed algorithms, we have conducted extensive experiments on real location-based social network datasets, and the experimental results show that KSTR does indeed demonstrate good performance compared to state-of-the-art works.
[who issue, route ranking, real location-based social network datasets, KSTR, mobility management (mobile radio), Radio frequency, query processing, POI-related tag classification, mobile computing, travel experience discovery, Semantics, feature extraction, route reconstruction algorithm, temporal influence, Trajectory, trip planning, social media, Facebook, social influence, keyword-aware skyline travel route recommendation, Social network services, geographical mobility pattern, Media, query keywords, classification, Flicker, places-of-interests, check-in records, user social interactions, travel industry, knowledge extraction, where issue, Feature extraction, social networking (online), Planning, historical mobility records, when issue]
Collaborative Multi-domain Sentiment Classification
2015 IEEE International Conference on Data Mining
None
2015
Sentiment classification is a hot research topic in both industrial and academic fields. The mainstream sentiment classification methods are based on machine learning and treat sentiment classification as a text classification problem. However, sentiment classification is widely recognized as a highly domain-dependent task. The sentiment classifier trained in one domain may not perform well in another domain. A simple solution to this problem is training a domain-specific sentiment classifier for each domain. However, it is difficult to label enough data for every domain since they are in a large quantity. In addition, this method omits the sentiment information in other domains. In this paper, we propose to train sentiment classifiers for multiple domains in a collaborative way based on multi-task learning. Specifically, we decompose the sentiment classifier in each domain into two components, a general one and a domain-specific one. The general sentiment classifier can capture the global sentiment information and is trained across various domains to obtain better generalization ability. The domain-specific sentiment classifier is trained using the labeled data in one domain to capture the domain-specific sentiment information. In addition, we explore two kinds of relations between domains, one based on textual content and the other one based on sentiment word distribution. We build a domain similarity graph using domain relations and encode it into our approach as regularization over the domain-specific sentiment classifiers. Besides, we incorporate the sentiment knowledge extracted from sentiment lexicons to help train the general sentiment classifier more accurately. Moreover, we introduce an accelerated optimization algorithm to train the sentiment classifiers efficiently. Experimental results on two benchmark sentiment datasets show that our method can outperform baseline methods significantly and consistently.
[Adaptation models, text analysis, sentiment lexicons, graph theory, generalization ability, textual content, collaborative multidomain sentiment classification, Data mining, global sentiment information, Learning systems, Training, domain-specific sentiment classifier, sentiment information, Benchmark testing, Motion pictures, sentiment word distribution, learning (artificial intelligence), sentiment knowledge extraction, pattern classification, text classification problem, sentiment analysis, sentiment classification, knowledge acquisition, mainstream sentiment classification method, domain-specific sentiment information, generalisation (artificial intelligence), machine learning, multitask learning, domain similarity graph, multi-task learning, Collaboration, multi-domain, accelerated optimization algorithm]
R2FP: Rich and Robust Feature Pooling for Mining Visual Data
2015 IEEE International Conference on Data Mining
None
2015
The human visual system proves smart in extracting both global and local features. Can we design a similar way for unsupervised feature learning? In this paper, we propose anovel pooling method within an unsupervised feature learningframework, named Rich and Robust Feature Pooling (R2FP), to better explore rich and robust representation from sparsefeature maps of the input data. Both local and global poolingstrategies are further considered to instantiate such a methodand intensively studied. The former selects the most conductivefeatures in the sub-region and summarizes the joint distributionof the selected features, while the latter is utilized to extractmultiple resolutions of features and fuse the features witha feature balancing kernel for rich representation. Extensiveexperiments on several image recognition tasks demonstratethe superiority of the proposed techniques.
[human visual system, autoencoder, representation learning, data mining, Electronic mail, Data mining, R2FP, unsupervised feature learning, rich and robust feature pooling, pooling, feature extraction, data visualisation, visual data mining, Feature extraction, Robustness, learning (artificial intelligence), Kernel, Spatial resolution, image recognition]
Convex Approximation to the Integral Mixture Models Using Step Functions
2015 IEEE International Conference on Data Mining
None
2015
The parameter estimation to mixture models has been shown as a local optimal solution for decades. In this paper, we propose a functional estimation to mixture models using step functions. We show that the proposed functional inference yields a convex formulation and consequently the mixture models are feasible for a global optimum inference. The proposed approach further unifies the existing isolated exemplar-based clustering techniques at a higher level of generality, e.g. it provides a theoretical justification for the heuristics of the clustering by affinity propagation Frey &amp; Dueck (2007), it reproduces Lashkari &amp; Golland (2007)'s's convex formulation as a special case under this step function construction. Empirical studies also verify the theoretic justifications.
[approximation theory, functional inference, Estimation, isolated exemplar-based clustering techniques, convex programming, Electronic mail, Function approximation, inference mechanisms, pattern clustering, Convex Approximation, step functions, Mixture models, convex approximation, step function, parameter estimation, Inference algorithms, affinity propagation, Bayes methods, integral mixture models, mixture models]
Infinite Author Topic Model Based on Mixed Gamma-Negative Binomial Process
2015 IEEE International Conference on Data Mining
None
2015
Incorporating the side information of text corpus, i.e., authors, time stamps, and emotional tags, into the traditional text mining models has gained significant interests in the area of information retrieval, statistical natural language processing, and machine learning. One branch of these works is the so-called Author Topic Model (ATM), which incorporates the authors's interests as side information into the classical topic model. However, the existing ATM needs to predefine the number of topics, which is difficult and inappropriate in many real-world settings. In this paper, we propose an Infinite Author Topic (IAT) model to resolve this issue. Instead of assigning a discrete probability on fixed number of topics, we use a stochastic process to determine the number of topics from the data itself. To be specific, we extend a gamma-negative binomial process to three levels in orderto capture the author-document-keyword hierarchical structure. Furthermore, each document is assigned a mixed gamma process that accounts for the multi-author's contribution towards this document. An efficient Gibbs sampling inference algorithm witheach conditional distribution being closed-form is developed for the IAT model. Experiments on several real-world datasets show the capabilities of our IAT model to learn the hidden topics, authors' interests on these topics and the number of topics simultaneously.
[text analysis, Bayesian nonparametric learning, text mining models, Stochastic processes, data mining, Probability distribution, text corpus side information, author-document-keyword hierarchical structure, gamma distribution, Topic models, discrete probability, Weight measurement, Text mining, sampling methods, statistical natural language processing, information retrieval, inference mechanisms, machine learning, infinite author topic model, Gibbs sampling inference algorithm, Hidden Markov models, Data models, ATM, Bayes methods, mixed gamma-negative binomial process]
Exploiting Temporal and Social Factors for B2B Marketing Campaign Recommendations
2015 IEEE International Conference on Data Mining
None
2015
Business to Business (B2B) marketing aims at meeting the needs of other businesses instead of individual consumers. In B2B markets, the buying processes usually involve series of different marketing campaigns providing necessary information to multiple decision makers with different interests and motivations. The dynamic and complex nature of these processes imposes significant challenges to analyze the process logs for improving the B2B marketing practice. Indeed, most of the existing studies only focus on the individual consumers in the markets, such as movie/product recommender systems. In this paper, we exploit the temporal behavior patterns in the buying processes of the business customers and develop a B2B marketing campaign recommender system. Specifically, we first propose the temporal graph as the temporal knowledge representation of the buying process of each business customer. The key idea is to extract and integrate the campaign order preferences of the customer using the temporal graph. We then develop the low-rank graph reconstruction framework to identify the common graph patterns and predict the missing edges in the temporal graphs. We show that the prediction of the missing edges is effective to recommend the marketing campaigns to the business customers during their buying processes. Moreover, we also exploit the community relationships of the business customers to improve the performances of the graph edge predictions and the marketing campaign recommendations. Finally, we have performed extensive empirical studies on real-world B2B marketing data sets and the results show that the proposed method can effectively improve the quality of the campaign recommendations for challenging B2B marketing tasks.
[graph theory, missing graph edge predictions, Companies, B2B marketing campaign recommendations, B2B marketing campaign recommender system, B2B marketing data sets, Electronic mail, business to business marketing, temporal behavior patterns, Temporal Patterns, B2B marketing practice, temporal knowledge representation, Graph Reconstruction, graph pattern identification, Recommender systems, Recommender Systems, social factors, Knowledge representation, marketing data processing, buying processes, business customers, campaign order preferences, Community Networks, recommender systems, temporal graph, knowledge representation, Temporal Graph, Markov processes, Webinars, low-rank graph reconstruction framework]
An Aggressive Graph-Based Selective Sampling Algorithm for Classification
2015 IEEE International Conference on Data Mining
None
2015
Traditional online learning algorithms are designed for vector data only, which assume that the labels of all the training examples are provided. In this paper, we study graph classification where only limited nodes are chosen for labelling by selective sampling. Particularly, we first adapt a spectral-based graph regularization technique to derive a novel online learning linear algorithm which can handle graph data, although it still queries the labels of all nodes and thus is not preferred, as labelling is typically time-consuming. To address this issue, we then propose a new confidence-based query method for selective sampling. The theoretical result shows that our online learning algorithm with a fraction of queried labels can achieve a mistake bound comparable with the one learning on all labels of the nodes. In addition, the algorithm based on our proposed query strategy can achieve a mistake bound better than the one based on other query methods. However, our algorithm is conservative to update the model whenever error happens, which obviously wastes training labels that are valuable for the model. To take advantage of these labels, we further propose a novel aggressive algorithm, which can update the model aggressively even if no error occurs. The theoretical analysis shows that our aggressive approach can achieve a mistake bound better than its conservative and fully-supervised counterpart, with substantially fewer queried times. We empirically evaluate our algorithm on several real-world graph datasets and the experimental results demonstrate that our method is highly effective.
[Algorithm design and analysis, confidence-based query method, Graph Node Classification, pattern classification, Laplace equations, Uncertainty, sampling methods, graph theory, graph data handling, Predictive models, Online Learning, online learning linear algorithm, Training, query processing, aggressive graph-based selective sampling algorithm, online learning algorithms, Selective Sampling, vector data, graph classification, spectral-based graph regularization technique, Prediction algorithms, learning (artificial intelligence), Kernel, aggressive algorithm]
Beyond Query: Interactive User Intention Understanding
2015 IEEE International Conference on Data Mining
None
2015
Users often fail to find the right keywords to precisely describe their queries in the information seeking process. Techniques such as user intention predictions and personalized recommendations are designed to help the users figure out how to formalize their queries. In this work, we aim to help users identify their search targets using a new approach called Interactive User Intention Understanding. In particular, we construct an automatic questioner that generates yes-or-no questions for the user. Then we infer user intention according to the corresponding answers. In order to generate "smart" questions in an optimal sequence, we propose the IHS algorithm based on heuristic search. We prove an error bound for the proposed algorithm on the ranking of target items given the questions and answers. We conduct experiments on three datasets and compare our result with two baseline methods. Experimental results show that IHS outperforms the baseline methods by 27.83% and 25.98% respectively.
[Algorithm design and analysis, Companies, interactive, Mobile communication, Engines, user intention, query processing, heuristic algorithm, information seeking process, target item ranking, Search engines, interactive systems, yes-or-no questions, search problems, user intention predictions, optimal sequence, personalized recommendations, error bound, interactive user intention understanding, Partitioning algorithms, heuristic search, recommender systems, automatic questioner, Games, IHS algorithm, question answering (information retrieval)]
Sparse Online Relative Similarity Learning
2015 IEEE International Conference on Data Mining
None
2015
For many data mining and machine learning tasks, the quality of a similarity measure is the key for their performance. To automatically find a good similarity measure from datasets, metric learning and similarity learning are proposed and studied extensively. Metric learning will learn a Mahalanobis distance based on positive semi-definite (PSD) matrix, to measure the distances between objectives, while similarity learning aims to directly learn a similarity function without PSD constraint so that it is more attractive. Most of the existing similarity learning algorithms are online similarity learning method, since online learning is more scalable than offline learning. However, most existing online similarity learning algorithms learn a full matrix with d2 parameters, where d is the dimension of the instances. This is clearly inefficient for high dimensional tasks due to its high memory and computational complexity. To solve this issue, we introduce several Sparse Online Relative Similarity (SORS) learning algorithms, which learn a sparse model during the learning process, so that the memory and computational cost can be significantly reduced. We theoretically analyze the proposed algorithms, and evaluate them on some real-world high dimensional datasets. Encouraging empirical results demonstrate the advantages of our approach in terms of efficiency and efficacy.
[Measurement, Algorithm design and analysis, Machine learning algorithms, Correlation, Sparse Online learning, learning process, Image retrieval, data mining, sparse online relative similarity learning, Sparse matrices, Data mining, High-dimensional similarity learning, Data stream mining, SORS learning algorithms, learning (artificial intelligence), sparse model, machine learning tasks]
Fast Low-Rank Matrix Learning with Nonconvex Regularization
2015 IEEE International Conference on Data Mining
None
2015
Low-rank modeling has a lot of important applications in machine learning, computer vision and social network analysis. While the matrix rank is often approximated by the convex nuclear norm, the use of nonconvex low-rank regularizers has demonstrated better recovery performance. However, the resultant optimization problem is much more challenging. A very recent state-of-the-art is based on the proximal gradient algorithm. However, it requires an expensive full SVD in each proximal step. In this paper, we show that for many commonly-used nonconvex low-rank regularizers, a cutoff can be derived to automatically threshold the singular values obtained from the proximal operator. This allows the use of power method to approximate the SVD efficiently. Besides, the proximal operator can be reduced to that of a much smaller matrix projected onto this leading subspace. Convergence, with a rate of O(1/T) where T is the number of iterations, can be guaranteed. Extensive experiments are performed on matrix completion and robust principal component analysis. The proposed method achieves significant speedup over the state-of-the-art. Moreover, the matrix solution obtained is more accurate and has a lower rank than that of the traditional nuclear norm regularizer.
[SVD, Closed-form solutions, Nonconvex optimization, matrix completion, convergence, mathematics computing, nonconvex low-rank regularizers, Minimization, Low-rank matrix, robust principal component analysis, Matrix decomposition, Sparse matrices, low-rank matrix learning, machine learning, Optimization, Robust PCA, proximal operator, Proximal gradient, Approximation algorithms, Robustness, learning (artificial intelligence), singular value decomposition, Matrix completion]
Weighted Spectral Cluster Ensemble
2015 IEEE International Conference on Data Mining
None
2015
Clustering explores meaningful patterns in the non-labeled data sets. Cluster Ensemble Selection (CES) is a new approach, which can combine individual clustering results for increasing the performance of the final results. Although CES can achieve better final results in comparison with individual clustering algorithms and cluster ensemble methods, its performance can be dramatically affected by its consensus diversity metric and thresholding procedure. There are two problems in CES: 1) most of the diversity metrics is based on heuristic Shannon's entropy and 2) estimating threshold values are really hard in practice. The main goal of this paper is proposing a robust approach for solving the above mentioned problems. Accordingly, this paper develops a novel framework for clustering problems, which is called Weighted Spectral Cluster Ensemble (WSCE), by exploiting some concepts from community detection arena and graph based clustering. Under this framework, a new version of spectral clustering, which is called Two Kernels Spectral Clustering, is used for generating graphs based individual clustering results. Further, by using modularity, which is a famous metric in the community detection, on the transformed graph representation of individual clustering results, our approach provides an effective diversity estimation for individual clustering results. Moreover, this paper introduces a new approach for combining the evaluated individual clustering results without the procedure of thresholding. Experimental study on varied data sets demonstrates that the prosed approach achieves superior performance to state-of-the-art methods.
[Measurement, Correlation, graph theory, graph based clustering, weighted evidence accumulation clustering, Entropy, nonlabeled data sets, two-kernels spectral clustering, heuristic Shannon's entropy, CES, normalized modularity, graph based individual clustering, pattern clustering, Diversity reception, Clustering algorithms, WSCE, data sets, weighted spectral cluster ensemble selection, Mathematical model, Yttrium, cluster ensemble, spectral clustering, diversity metrics, community detection arena]
From Micro to Macro: Uncovering and Predicting Information Cascading Process with Behavioral Dynamics
2015 IEEE International Conference on Data Mining
None
2015
Cascades are ubiquitous in various network environments. How to predict these cascades is highly nontrivial in several vital applications, such as viral marketing, epidemic prevention and traffic management. Most previous works mainly focus on predicting the final cascade sizes. As cascades are typical dynamic processes, it is always interesting and important to predict the cascade size at any time, or predict the time when a cascade will reach a certain size (e.g. an threshold for outbreak). In this paper, we unify all these tasks into a fundamental problem: cascading process prediction. That is, given the early stage of a cascade, how to predict its cumulative cascade size of any later time? For such a challenging problem, how to understand the micro mechanism that drives and generates the macro phenomena (i.e. cascading process) is essential. Here we introduce behavioral dynamics as the micro mechanism to describe the dynamic process of a node's neighbors getting infected by a cascade after this node getting infected (i.e. one-hop subcascades). Through data-driven analysis, we find out the common principles and patterns lying in behavioral dynamics and propose a novel Networked Weibull Regression model for behavioral dynamics modeling. After that we propose a novel method for predicting cascading processes by effectively aggregating behavioral dynamics, and present a scalable solution to approximate the cascading process with a theoretical guarantee. We extensively evaluate the proposed method on a large scale social network dataset. The results demonstrate that the proposed method can significantly outperform other state-of-the-art baselines in multiple tasks including cascade size prediction, outbreak time prediction and cascading process prediction.
[micromechanism, dynamic process, Additives, cumulative cascade size, networked Weibull regression model, Predictive models, behavioral dynamics modeling, traffic management, Dynamic Processes Prediction, outbreak time prediction, Analytical models, Social Network, network environments, viral marketing, Social network services, Computational modeling, Information Cascades, epidemic prevention, Computer science, marketing, social network dataset, data-driven analysis, social networking (online), information cascading process prediction, Data models, behavioural sciences computing]
Max-Intensity: Detecting Competitive Advertiser Communities in Sponsored Search Market
2015 IEEE International Conference on Data Mining
None
2015
In a sponsored search market, the problem of measuring the intensity of competition among advertisers is increasingly gaining prominence today. Usually, search providers want to monitor the advertiser communities that share common bidding keywords, so that they can intervene when competition slackens. However, to the best of our knowledge, not much research has been conducted in identifying advertiser communities and understanding competition within these communities. In this paper we introduce a novel approach to detect competitive communities in a weighted bi-partite network formed by advertisers and their bidding keywords. The proposed approach is based on an advertiser vertex metric called intensity score, which takes the following two factors into consideration: the competitors that bid on the same keywords, and the advertisers' consumption proportion within the community. Evidence shows that when market competition rises, the revenue for a search provider also increases. Our community detection algorithm Max-Intensity is designed to detect communities which have the maximum intensity score. In this paper, we conduct experiments and validate the performance of Max-Intensity on sponsored search advertising data. Compared to baseline methods, the communities detected by our algorithm have low Herfindahl-Hirschman index (HHI) and comprehensive concentration index (CCI), which demonstrates that the communities given by Max-Intensity can capture the structure of the competitive communities.
[Measurement, Max-Intensity, Correlation, search engines, network theory (graphs), HHI, search providers, Competition Coefficient, comprehensive concentration index, competitive advertiser community detection, Search engines, Monitoring, advertising data processing, weighted bi-partite network, intensity score, CCI, Sponsored Search, sponsored search market, Indexes, Herfindahl-Hirschman index, Competition Community Detection, Data models, advertiser vertex metric, Max-Intensity detection algorithm, Detection algorithms, bidding keywords]
Deep Convolutional Neural Networks for Multi-instance Multi-task Learning
2015 IEEE International Conference on Data Mining
None
2015
Multi-instance learning studies problems in which labels are assigned to bags that contain multiple instances. In these settings, the relations between instances and labels are usually ambiguous. In contrast, multi-task learning focuses on the output space in which an input sample is associated with multiple labels. In real world, a sample may be associated with multiple labels that are derived from observing multiple aspects of the problem. Thus many real world applications are naturally formulated as multi-instance multi-task (MIMT) problems. A common approach to MIMT is to solve it task-by-task independently under the multi-instance learning framework. On the other hand, convolutional neural networks (CNN) have demonstrated promising performance in single-instance single-label image classification tasks. However, how CNN deals with multi-instance multi-label tasks still remains an open problem. This is mainly due to the complex multiple-to-multiple relations between the input and output space. In this work, we propose a deep leaning model, known as multi-instance multi-task convolutional neural networks (MIMT-CNN), where a number of images representing a multi-task problem is taken as the inputs. Then a shared sub-CNN is connected with each input image to form instance representations. Those sub-CNN outputs are subsequently aggregated as inputs to additional convolutional layers and full connection layers to produce the ultimate multi-label predictions. This CNN model, through transfer learning from other domains, enables transfer of prior knowledge at image level learned from large single-label single-task data sets. The bag level representations in this model are hierarchically abstracted by multiple layers from instance level representations. Experimental results on mouse brain gene expression pattern annotation data show that the proposed MIMT-CNN model achieves superior performance.
[mouse brain gene expression pattern annotation data, image classification, single-label single-task data sets, deep leaning model, Deep learning, complex multiple-to-multiple relations, ultimate multilabel predictions, deep convolutional neural networks, learning (artificial intelligence), MIMT-CNN problem, transfer learning, Biological system modeling, single-instance single-label image classification tasks, multi-instance learning, multiinstance multitask learning, multiinstance multilabel tasks, Brain models, Gene expression, Standards, multi-task learning, image representation, bioinformatics, multiinstance multitask convolutional neural networks, Data models, bag level representations, neural nets]
A Bayesian Hierarchical Model for Comparing Average F1 Scores
2015 IEEE International Conference on Data Mining
None
2015
In multi-class text classification, the performance (effectiveness) of a classifier is usually measured by micro-averaged and macro-averaged F1 scores. However, the scores themselves do not tell us how reliable they are in terms of forecasting the classifier's future performance on unseen data. In this paper, we propose a novel approach to explicitly modelling the uncertainty of average F1 scores through Bayesian reasoning, and demonstrate that it can provide much more comprehensive performance comparison between text classifiers than the traditional frequentist null hypothesis significance testing (NHST).
[pattern classification, text analysis, Bayesian hierarchical model, Uncertainty, text classifiers, Computational modeling, average F<sub>1</sub> scores comparison, hypothesis testing, model comparison, Estimation, performance evaluation, Electronic mail, text classification, inference mechanisms, frequentist NHST, Data models, Bayes methods, Bayesian reasoning, Bayesian inference, null hypothesis significance testing, Testing]
Multiple Anonymized Social Networks Alignment
2015 IEEE International Conference on Data Mining
None
2015
Users nowadays are normally involved in multiple (usually more than two) online social networks simultaneously to enjoy more social network services. Some of the networks that users are involved in can share common structures either due to the analogous network construction purposes or because of the similar social network features. However, the social network datasets available in research are usually pre-anonymized and accounts of the shared users in different networks are mostly isolated without any known connections. In this paper, we want to identify such connections between the shared users' accounts in multiple social networks (i.e., the anchor links), which is formally defined as the M-NASA (Multiple Anonymized Social Networks Alignment) problem. M-NASA is very challenging to address due to (1) the lack of known anchor links to build models, (2) the studied networks are anonymized, where no users' personal profile or attribute information is available, and (3) the "transitivity law" and the "one-to-one property" based constraints on anchor links. To resolve these challenges, a novel two-phase network alignment framework UMA (Unsupervised Multi-network Alignment) is proposed in this paper. Extensive experiments conducted on multiple real-world partially aligned social networks demonstrate that UMA can perform very well in solving the M-NASA problem.
[M-NASA, Social network services, Conferences, graph theory, network theory (graphs), UMA, Data Mining, transitivity law, Data mining, Indexes, one-to-one property, unsupervised multinetwork alignment, Partial Network Alignment, Optimization, unsupervised learning, Multiple Heterogeneous Social Networks, multiple anonymized social networks alignment, social networking (online), anchor link, Joining processes, online social network]
Modeling Social Attention for Stock Analysis: An Influence Propagation Perspective
2015 IEEE International Conference on Data Mining
None
2015
With the rapid growth of usage of social network, the patterns, the scales, and the rate of information exchange have brought profound impacts on research and practice in finance. One important topic is the stock market efficiency analysis. Traditional schemes in finance focus on identifying significant abnormal returns triggered by important events. However, those events are merely identified by regular financial announcements such as mergers, equity issuances, and financial reports. Related data-driven approaches mainly focus on developing trading strategies using social media data, while the results are usually lack of theoretical explanations. In this paper, we fill the gap between the usage of social media data and financial theories. We propose a Degree of Social Attention (DSA) framework for stock analysis based on influence propagation model. Specifically, we define the self-influence for users in a social network and the DSA for stocks. A recursive process is also designed for dynamic value updating. Furthermore, we provide two modified approaches to reduce the computational cost. Our testing results from the Chinese stock market suggest that the proposed framework effectively captures stock abnormal returns based on the related social media data, and DSA is verified to be a key factor to link social media activities to the stock market.
[Influence Propagation, Social network services, Computational modeling, financial theories, Market Efficiency, Finance, Stock Social Attention, Media, degree of social attention, stock market efficiency analysis, social network, Analytical models, Social Network, information exchange, DSA, data-driven approach, influence propagation, social media data, social networking (online), stock markets, Stock markets, Integrated circuit modeling, finance]
Controlling Propagation at Group Scale on Networks
2015 IEEE International Conference on Data Mining
None
2015
Given a network with groups, such as a contact-network grouped by ages, which are the best groups to immunize to control the epidemic? Equivalently, how to best choose communities in social networks like Facebook to stop rumors from spreading? Immunization is an important problem in multiple different domains like epidemiology, public health, cyber security and social media. Additionally, clearly immunization at group scale (like schools and communities) is more realistic due to constraints in implementations and compliance (e.g., it is hard to ensure specific individuals take the adequate vaccine). Hence efficient algorithms for such a "group-based" problem can help public-health experts take more practical decisions. However most prior work has looked into individual-scale immunization. In this paper, we study the problem of controlling propagation at group scale. We formulate novel so-called Group Immunization problems for multiple natural settings (for both threshold and cascade-based contagion models under both node-level and edge-level interventions) and develop multiple efficient algorithms, including provably approximate solutions. Finally, we show the effectiveness of our methods via extensive experiments on real and synthetic datasets.
[cyber security, Diffusion processes, social networks, group-based problem, contact network, Vaccines, group scale, individual-scale immunization, epidemiology, public health experts, social networking (online), Resource management, controlling propagation, Integrated circuit modeling, Facebook, Public healthcare, social media, group immunization problems, Immune system, synthetic datasets]
Parallel Multi-task Learning
2015 IEEE International Conference on Data Mining
None
2015
In this paper, we develop parallel algorithms for a family of regularized multi-task methods which can model task relations under the regularization framework. Since those multi-task methods cannot be parallelized directly, we use the FISTA algorithm, which in each iteration constructs a surrogate function of the original problem by utilizing the Lipschitz structure of the objective function based on the solution in the last iteration, to solve it. Specifically, we investigate the dual form of the objective function in those methods by adopting the hinge, e-insensitive, and square losses to deal with multi-task classification and regression problems, and then utilize the Lipschitz structure to construct the surrogate function for the dual forms. The surrogate functions constructed in the FISTA algorithm are founded to be decomposable, leading to parallel designs for those multi-task methods. Experiments on several benchmark datasets show that the convergence of the proposed algorithms is as fast as that of SMO-style algorithms and the parallel design can speedup the computation.
[Algorithm design and analysis, multitask classification, parallel algorithms, parallel multitask learning, iteration constructs, regression problems, Lipschitz structure, Multi-Task Learning, Fasteners, surrogate functions, Linear programming, Covariance matrices, Parallel algorithms, regularized multitask methods, Convergence, Parallel Algorithm, SMO-style algorithms, regularization framework, learning (artificial intelligence), Kernel, FISTA algorithm]
SimNest: Social Media Nested Epidemic Simulation via Online Semi-Supervised Deep Learning
2015 IEEE International Conference on Data Mining
None
2015
Infectious disease epidemics such as influenza and Ebola pose a serious threat to global public health. It is crucial to characterize the disease and the evolution of the ongoing epidemic efficiently and accurately. Computational epidemiology can model the disease progress and underlying contact network, but suffers from the lack of real-time and fine-grained surveillance data. Social media, on the other hand, provides timely and detailed disease surveillance, but is insensible to the underlying contact network and disease model. This paper proposes a novel semi-supervised deep learning framework that integrates the strengths of computational epidemiology and social media mining techniques. Specifically, this framework learns the social media users' health states and intervention actions in real time, which are regularized by the underlying disease model and contact network. Conversely, the learned knowledge from social media can be fed into computational epidemic model to improve the efficiency and accuracy of disease diffusion modeling. We propose an online optimization algorithm to substantialize the above interactive learning process iteratively to achieve a consistent stage of the integration. The extensive experimental results demonstrated that our approach can effectively characterize the spatiotemporal disease diffusion, outperforming competing methods by a substantial margin on multiple metrics.
[online semisupervised-deep learning, influenza, efficiency improvement, data mining, contact network, Twitter, spatiotemporal disease diffusion, social media user health states, disease diffusion modeling, deep learning, computational epidemiology, accuracy improvement, interactive learning process, Sociology, SimNest, learning (artificial intelligence), social media nested epidemic simulation, social media mining techniques, Computational modeling, disease progress, epidemic simulation, infectious disease epidemics, Media, diseases, Statistics, Diseases, Surveillance, global public health, intervention actions, social networking (online), Data models, online optimization algorithm, Ebola]
Cost-Sensitive Online Classification with Adaptive Regularization and Its Applications
2015 IEEE International Conference on Data Mining
None
2015
Cost-Sensitive Online Classification is recently proposed to directly online optimize two well-known cost-sensitive measures: (i) maximization of weighted sum of sensitivity and specificity, and (ii) minimization of weighted misclassification cost. However, the previous existing learning algorithms only utilized the first order information of the data stream. This is insufficient, as recent studies have proved that incorporating second order information could yield significant improvements on the prediction model. Hence, we propose a novel cost-sensitive online classification algorithm with adaptive regularization. We theoretically analyzed the proposed algorithm and empirically validated its effectiveness with extensive experiments. We also demonstrate the application of the proposed technique for solving several online anomaly detection tasks, showing that the proposed technique could be an effective tool to tackle cost-sensitive online classification tasks in various application domains.
[Adaptation models, pattern classification, Machine learning algorithms, sensitivity weighted sum maximization, Online Learning, Classification algorithms, Data mining, cost-sensitive online classification algorithm, machine learning, weighted misclassification cost minimization, specificity weighted sum maximization, Adaptive Regularization, Training, Sensitivity, ARCSOGD, Prediction algorithms, learning (artificial intelligence), adaptively regularized cost-sensitive online gradient descent algorithm, gradient methods, online anomaly detection, Cost-Sensitive Classification]
Top-k Reliability Search on Uncertain Graphs
2015 IEEE International Conference on Data Mining
None
2015
Uncertain graphs have been widely used to represent graph data with inherent uncertainty in structures. Reliability search is a fundamental problem in uncertain graph analytics. This paper studies a new problem, the top-k reliability search problem on uncertain graphs, that is, finding k vertices v with the highest reliabilities of connections from a source vertex s to v. Note that the existing algorithm for the threshold-based reliability search problem is inefficient for the top-k reliability search problem. We propose a new algorithm to efficiently solve the top-k reliability search problem. The algorithm adopts two important techniques, namely the BFS sharing technique and the offline sampling technique. The BFS sharing technique exploits overlaps among different sampled possible worlds of the input uncertain graph and performs a single BFS on all possible worlds simultaneously. The offline sampling technique samples possible worlds offline and stored them using a compact structure. The algorithm also takes advantages of bit vectors and bitwise operations to improve efficiency. Moreover, we generalize the top-k reliability search problem to the multi-source case and show that the multi-source case of the problem can be equivalently converted to the single-source case of the problem. Extensive experiments carried out on both real and synthetic datasets verify that the optimized algorithm outperforms the baselines by 1 - 2 orders of magnitude in execution time while achieving comparable accuracy. Meanwhile, the optimized algorithm exhibits linear scalability with respect to the size of the input uncertain graph.
[Uncertainty, graph theory, Search problems, bit vectors, single-source case, Approximation methods, Proteins, Monte Carlo methods, input uncertain graph, multisource case, data structures, linear scalability, top-k reliability search problem, source vertex, search problems, synthetic datasets, bitwise operations, sampling methods, data analysis, uncertain graph analytics, real datasets, vectors, threshold-based reliability search problem, Approximation algorithms, BFS sharing technique, Reliability, offline sampling technique samples, graph data representation]
Complementary Aspect-Based Opinion Mining Across Asymmetric Collections
2015 IEEE International Conference on Data Mining
None
2015
Aspect-based opinion mining is to find elaborate opinions towards an underlying theme, perspective or viewpoint as to a subject such as a product or an event. Nowadays, with rapid growing of opinionated text on the Web, mining aspect-level opinions has become a promising means for online public opinion analysis. In particular, the booming of various types of online media provide diverse yet complementary information, bringing unprecedented opportunities for public opinion analysis across different populations. Along this line, in this paper, we propose CAMEL, a novel topic model for complementary aspect-based opinion mining across asymmetric collections. CAMEL gains complementarity by modeling both common and specific aspects across different collections, and keeping all the corresponding opinions for contrastive study. To further boost CAMEL, we propose AME, an automatic labeling scheme for maximum entropy model, to help discriminate aspect and opinion words without heavy human labeling. Extensive experiments on synthetic multicollection data sets demonstrate the superiority of CAMEL to baseline methods, in leveraging cross-collection complementarity to find higher-quality aspects and more coherent opinions as well as aspect-opinion relationships. This is particularly true when the collections get seriously imbalanced. Experimental results also show that the AME model indeed outperforms manual labeling in suggesting true opinion words. Finally, case study on two public events further demonstrates the practical value of CAMEL for real-world public opinion analysis.
[Aspect-based Opinion Mining, online public opinion analysis, topic model, Topic Detection and Tracking, synthetic multicollection data sets, data mining, Switches, Manuals, Media, Entropy, cross-collection complementarity, Data mining, maximum entropy model, Maximum Entropy Model, complementary aspect-based opinion mining, LDA Model, Analytical models, entropy, aspect-opinion relationships, aspect-level opinion mining, asymmetric collections, CAMEL, aspect-oriented programming, Labeling]
Simultaneous Semi-NMF and PCA for Clustering
2015 IEEE International Conference on Data Mining
None
2015
Cluster analysis is often carried out in combination with dimension reduction. The Semi-Non-negative Matrix Factorization (Semi-NMF) that learns a low-dimensional representation of a data set lends itself to a clustering interpretation. In this work we propose a novel approach to finding an optimal subspace of multi-dimensional variables for identifying a partition of the set of objects. The use of a low-dimensional representation can be of help in providing simpler and more interpretable solutions. We show that by doing so, our model is able to learn low-dimensional representations that are better suited for clustering, outperforming not only Semi-NMF, but also other NMF variants.
[Laplace equations, Dimension Reduction, seminonnegative matrix factorization, Linear programming, matrix decomposition, Matrix decomposition, Clustering, dimension reduction, Optimization, PCA, Manifolds, data representation learning, data reduction, pattern clustering, Clustering algorithms, SemiNMF, data structures, learning (artificial intelligence), cluster analysis, Semi-NMF, principal component analysis, Principal component analysis]
Learning a Macroscopic Model of Cultural Dynamics
2015 IEEE International Conference on Data Mining
None
2015
A fundamental open question that has been studied by sociologists since the 70s and recently started being addressed by the computer-science community is the understanding of the role that influence and selection play in shaping the evolution of socio-cultural systems. Quantifying these forces in real settings is still a big challenge, especially in the large-scale case in which the entire social network between the users may not be known, and only longitudinal data in terms of masses of cultural groups (e.g., political affiliation, product adoption, market share, cultural tastes) may be available. We propose an influence and selection model encompassing an explicit characterization of the feature space for the different cultural groups in the form of a natural equation-based macroscopic model, following the approach of Kempe et al. [EC 2013]. Our main goal is to estimate edge influence strengths and selection parameters from an observed time series. To do an experimental evaluation on real data, we perform learning on real datasets from Last. FM and Wikipedia.
[feature space, Computational modeling, sociocultural systems, cultural aspects, time series, Wikipedia, Cultural differences, Statistics, longitudinal data, social network, computer-science community, humanities, natural equation-based macroscopic model, cultural dynamics, Microscopy, Sociology, Last.FM, feature selection model, edge influence strengths, Data models, social sciences computing, cultural groups, Mathematical model, feature selection]
Learning Set Cardinality in Distance Nearest Neighbours
2015 IEEE International Conference on Data Mining
None
2015
Distance-based nearest neighbours (dNN) queries and aggregations over their answer sets are important for exploratory data analytics. We focus on the Set Cardinality Prediction (SCP) problem for the answer set of dNN queries. We contribute a novel, query-driven perspective for this problem, whereby answers to previous dNN queries are used to learn the answers to incoming dNN queries. The proposed novel machine learning (ML) model learns the dynamically changing query patterns space and thus it can focus only on the portion of the data being queried. The model enjoys several comparative advantages in prediction error and space requirements. This is in addition to being applicable in environments with sensitive data and/or environments where data accesses are too costly to execute, where the data-centric state-of-the-art is inapplicable and/or too costly. A comprehensive performance evaluation of our model is conducted, evaluating its comparative advantages versus acclaimed methods (i.e., different self-tuning histograms, sampling, multidimensional histograms, and the power-method).
[Adaptation models, Solid modeling, data analysis, set cardinality prediction problem, local regression vector quantization, Estimation, hetero-associative competitive learning, exploratory data analytics, dNN query, ML model, distance nearest neighbors analytics, SCP problem, query processing, Histograms, Query-driven set cardinality prediction, Quantization (signal), query pattern space, Prototypes, learning set cardinality, distance-based nearest neighbour query, data access, Yttrium, learning (artificial intelligence), machine learning model]
Are You Going to the Party: Depends, Who Else is Coming?: [Learning Hidden Group Dynamics via Conditional Latent Tree Models]
2015 IEEE International Conference on Data Mining
None
2015
Scalable probabilistic modeling and prediction in high dimensional multivariate time-series, such as dynamic social networks with co-evolving nodes and edges, is a challenging problem, particularly for systems with hidden sources of dependence and/or homogeneity. Here, we address this problem through the discovery of hierarchical latent groups. We introduce a family of Conditional Latent Tree Models (CLTM), in which tree-structured latent variables incorporate the unknown groups. The latent tree itself is conditioned on observed covariates such as seasonality, historical activity, and node attributes. We propose a statistically efficient framework for learning both the hierarchical tree structure and the parameters of the CLTM. We demonstrate competitive performance on two real world datasets, one from the students' attempts at answering questions in a psychology MOOC and the other from Twitter users participating in an emergency management discussion and interacting with one another. In addition, our modeling framework provides valuable and interpretable information about the hidden group structures and their effect on the evolution of the time series.
[student attempts, Predictive models, Twitter, emergency management discussion, hierarchical tree structure, tree-structured latent variables, observed covariates, scalable probabilistic prediction, Multivariate time series, psychology, scalable probabilistic modeling, dynamic networks, tree data structures, Mathematical model, Maximum likelihood estimation, Time series analysis, high dimensional multivariate time-series, probability, question answering, emergency management, time series, psychology MOOC, conditional latent tree models, hierarchical latent groups, hidden group structures, educational courses, CLTM parameters, hierarchical latent group discovery, social networking (online), conditional latent tree model, Random variables, computer aided instruction, real world datasets, question answering (information retrieval), Twitter users, Context modeling]
Automated Feature Learning: Mining Unstructured Data for Useful Abstractions
2015 IEEE International Conference on Data Mining
None
2015
When the amount of training data is limited, the successful application of machine learning techniques typically hinges on the ability to identify useful features or abstractions. Expert knowledge often plays a crucial role in this feature engineering process. However, manual creation of such abstractions can be labor intensive and expensive. In this paper, we propose a feature learning framework that takes advantage of the vast amount of expert knowledge available in unstructured form on the Web. We explore the use of unsupervised learning techniques and non-Euclidean distance measures to automatically incorporate such expert knowledge when building feature representations. We demonstrate the utility of our proposed approach on the task of learning useful abstractions from a list of over two thousand patient medications. Applied to three clinically relevant patient risk stratification tasks, the classifiers built using the learned abstractions outperform several baselines including one based on a manually curated feature space.
[Knowledge engineering, Correlation, feature representation, Taxonomy, Buildings, patient medication, data mining, abstraction identification, machine learning techniques, unstructured data mining, automated feature learning, Hospitals, Data models, non-Euclidean distance measures, learning (artificial intelligence), Kernel, unsupervised learning techniques]
Mining Brain Networks Using Multiple Side Views for Neurological Disorder Identification
2015 IEEE International Conference on Data Mining
None
2015
Mining discriminative subgraph patterns from graph data has attracted great interest in recent years. It has a wide variety of applications in disease diagnosis, neuroimaging, etc. Most research on subgraph mining focuses on the graph representation alone. However, in many real-world applications, the side information is available along with the graph data. For example, for neurological disorder identification, in addition to the brain networks derived from neuroimaging data, hundreds of clinical, immunologic, serologic and cognitive measures may also be documented for each subject. These measures compose multiple side views encoding a tremendous amount of supplemental information for diagnostic purposes, yet are often ignored. In this paper, we study the problem of discriminative subgraph selection using multiple side views and propose a novel solution to find an optimal set of subgraph features for graph classification by exploring a plurality of side views. We derive a feature evaluation criterion, named gSide, to estimate the usefulness of subgraph patterns based upon side views. Then we develop a branch-and-bound algorithm, called gMSV, to efficiently search for optimal subgraph features by integrating the subgraph mining process and the procedure of discriminative feature selection. Empirical studies on graph classification tasks for neurological disorders using brain networks demonstrate that subgraph patterns selected by the multi-side-view guided subgraph selection approach can effectively boost graph classification performances and are relevant to disease diagnosis.
[Neuroimaging, feature evaluation criterion, discriminative subgraph pattern mining, immunologic measures, graph theory, data mining, gMSV, graph data, side information, Data mining, gSide, discriminative feature selection, supplemental information, discriminative subgraph selection, branch-and-bound algorithm, subgraph pattern usefulness estimation, graph representation, Kernel, Testing, pattern classification, subgraph pattern, diseases, brain, clinical measures, cognitive measures, serologic measures, Diffusion tensor imaging, Diseases, Computer science, medical disorders, graph mining, neuroimaging data, brain network mining, subgraph features, graph classification, neurological disorders, disease diagnosis, neurophysiology, brain network, patient diagnosis, neurological disorder identification]
On the Connectivity of Multi-layered Networks: Models, Measures and Optimal Control
2015 IEEE International Conference on Data Mining
None
2015
Networks appear naturally in many high-impact real-world applications. In an increasingly connected and coupled world, the networks arising from many application domains are often collected from different channels, forming the so-called multi-layered networks, such as cyber-physical systems, organization-level collaboration platforms, critical infrastructure networks and many more. Compared with single-layered networks, multi-layered networks are more vulnerable as even a small disturbance on one supporting layer/network might cause a ripple effect to all the dependent layers, leading to a catastrophic/cascading failure of the entire system. The state-of-the-art has been largely focusing on modeling and manipulating the cascading effect of two-layered interdependent network systems for some specific type of network connectivity measure. This paper generalizes the challenge to multiple dimensions. First, we propose a new data model for multi-layered networks MULAN, which admits an arbitrary number of layers with a much more flexible dependency structure among different layers, beyond the current pair-wise dependency. Second, we unify a wide range of classic network connectivity measures SUBLINE. Third, we show that for any connectivity measure in the SUBLINE family, it enjoys the diminishing returns property which in turn lends itself to a family of provable near-optimal control algorithms with linear complexity. Finally, we conduct extensive empirical evaluations on real network data, to validate the effectiveness of the proposed algorithms.
[linear complexity, graph theory, connectivity control, cascading failure, SUBLINE family, network theory (graphs), network connectivity measure, organization-level collaboration platforms, single-layered networks, critical infrastructure networks, catastrophic failure, Silicon, multi-layered networks, Atmospheric modeling, near-optimal control algorithms, Physical layer, optimal control, two-layered interdependent network systems, cyber-physical systems, Current measurement, MULAN, Optimal control, Collaboration, network connectivity measures, Data models, critical infrastructures, pair-wise dependency, multilayered networks]
Constructing Disease Network and Temporal Progression Model via Context-Sensitive Hawkes Process
2015 IEEE International Conference on Data Mining
None
2015
Modeling disease relationships and temporal progression are two key problems in health analytics, which have not been studied together due to data and technical challenges. Thanks to the increasing adoption of Electronic Health Records (EHR), rich patient information is being collected over time. Using EHR data as input, we propose a multivariate context-sensitive Hawkes process or cHawkes, which simultaneously infers the disease relationship network and models temporal progression of patients. Besides learning disease network and temporal progression model, cHawkes is able to predict when a specific patient might have other related diseases in future given the patient history, which in turn can have many potential applications in predictive health analytics, public health policy development and customized patient care. Extensive experiments on real EHR data demonstrate that cHawkes not only can uncover meaningful disease relations and model accurate temporal progression of patients, but also has significantly better predictive performance compared to several baseline models.
[public health policy development, Health Analytics, Predictive models, disease relationship network inference, EHR data, cHawkes, health analytics, patient history, EHR, Disease Relation, Hawkes Process, Point Process, health care, disease network construction, temporal progression modeling, Hypertension, Computational modeling, diseases, disease relationships modeling, predictive health analytics, disease network learning, electronic health records, inference mechanisms, temporal patient progression, Disease Prediction, Diseases, customized patient care, patient information, disease relations, Hidden Markov models, Data models, multivariate context-sensitive Hawkes process, Context modeling]
Efficient Entity Resolution with Adaptive and Interactive Training Data Selection
2015 IEEE International Conference on Data Mining
None
2015
Entity resolution (ER) is the task of deciding which records in one or more databases refer to the same real-world entities. A crucial step in ER is the accurate classification of pairs of records into matches and non-matches. In most practical ER applications, obtaining training data %of high quality is costly and time consuming. Various techniques have been proposed for ER to interactively generate training data and learn an accurate classifier. We propose an approach for training data selection for ER that exploits the cluster structure of the weight vectors (similarities) calculated from compared record pairs. Our approach adaptively selects an optimal number of informative training examples for manual labeling based on a user defined sampling error margin, and recursively splits the set of weight vectors to find pure enough subsets for training. We consider two aspects of ER that are highly significant in practice: a limited budget for the number of manual labeling that can be done, and a noisy oracle where manual labels might be incorrect. Experiments on four real public data sets show that our approach can significantly reduce manual labeling efforts for training an ER classifier while achieving matching quality comparative to fully supervised classifiers.
[Data matching, pattern matching, record linkage, Manuals, fully supervised classifiers, user defined sampling error margin, Training, interactive training data selection, active learning, manual labeling, interactive labeling, Training data, Clustering algorithms, interactive systems, Silicon, Labeling, ER applications, Erbium, noisy oracle, pattern classification, adaptive training data selection, databases, weight vectors, entity resolution, manual labels, public data sets, ER classifier, matching quality, cluster structure, hierarchical clustering]
The ABACOC Algorithm: A Novel Approach for Nonparametric Classification of Data Streams
2015 IEEE International Conference on Data Mining
None
2015
Stream mining poses unique challenges to machine learning: predictive models are required to be scalable, incrementally trainable, must remain bounded in size, and benon parametric in order to achieve high accuracy even in complex and dynamic environments. Moreover, the learning system must be parameterless - traditional tuning methods are problematic in streaming settings - and avoid requiring prior knowledge of the number of distinct class labels occurring in the stream. In this paper, we introduce a new algorithmic approach for nonparametric learning in data streams. Our approach addresses all above mentioned challenges by learning a model that covers the input space using simple local classifiers. The distribution of these classifiers dynamically adapts to the local (unknown) complexity of the classification problem, thus achieving a good balance between model complexity and predictive accuracy. By means of an extensive empirical evaluation against standard nonparametric baselines, we show state-of-the-art results in terms of accuracy versus model size. Our empirical analysis is complemented by a theoretical performance guarantee which does not rely on any stochastic assumption on the source generating the stream.
[Measurement, Algorithm design and analysis, Adaptation models, data mining, Predictive models, nonparametric learning, Data mining, local classifiers, model complexity, Learning systems, predictive models, Prediction algorithms, learning (artificial intelligence), input space, Data Stream, pattern classification, Nonparametric Classification, stochastic assumption, Constant Budget Model Size, machine learning, data stream mining, nonparametric classification problem, High-Speed Data, predictive accuracy, nonparametric statistics]
Dissecting Regional Weather-Traffic Sensitivity Throughout a City
2015 IEEE International Conference on Data Mining
None
2015
The impact of inclement weather to urban traffic has been widely observed and studied for many years, with focus primarily on individual road segments by analyzing data from roadside deployed monitors. However, two fundamental questions are still open: (i) how to identify regional weather-traffic sensitivity index throughout a city, that indicates the degree to which the region traffic in a city is impacted by weather changes, (ii) among complex regional features, such as road structure and population density, how to dissect the most influential regional features that drive the urban region traffic to be more vulnerable to weather changes. Answering these questions is unprecedentedly important for urban planners to understand the functional characteristics of various urban regions throughout a city, and to improve traffic prediction and learn the key factors in urban planning. However, these two questions are nontrivial to answer, because urban traffic changes dynamically over time and is essentially affected by many other factors, which may dominate the overall impact. In this work, we make the first study on these questions, by developing a weather-traffic index (WTI) system. The system includes two main components: WTI establishment and key factor analysis. Using the proposed system, we conducted comprehensive empirical study in Shanghai, and the WTI extracted have been validated to be surprisingly consistent with real world observations. Further regional key factor analysis yields interesting results. For example, house age has significant impact on WTI, which sheds light on future urban planning and reconstruction.
[WTI system, Correlation, Roads, sensitivity analysis, complex regional features, urban planners, regional key factor analysis, urban planning, weather forecasting, population density, road segments, regional weather-traffic sensitivity index, weather-traffic index, Cities and towns, city dynamics, Meteorology, traffic prediction, road traffic, data analysis, Shanghai, Trajectories, traffic engineering computing, regional weather-traffic sensitivity dissection, Indexes, Public transportation, town and country planning, Sensitivity, city region traffic, urban region traffic, urban reconstruction, road structure, urban computing]
A Parameter-Free Approach for Mining Robust Sequential Classification Rules
2015 IEEE International Conference on Data Mining
None
2015
Sequential data is generated in many domains of science and technology. Although many studies have been carried out for sequence classification in the past decade, the problem is still a challenge, particularly for pattern-based methods. We identify two important issues related to pattern-based sequence classification which motivate the present work: the curse of parameter tuning and the instability of common interestingness measures. To alleviate these issues, we suggest a new approach and framework for mining sequential rule patterns for classification purpose. We introduce a space of rule pattern models and a prior distribution defined on this model space. From this model space, we define a Bayesian criterion for evaluating the interest of sequential patterns. We also develop a parameter-free algorithm to efficiently mine sequential patterns from the model space. Extensive experiments show that (i) the new criterion identifies interesting and robust patterns, (ii) the direct use of the mined rules as new features in a classification process demonstrates higher inductive performance than the state-of-the-art sequential pattern based classifiers.
[pattern classification, pattern-based sequence classification, a prior distribution, pattern-based methods, data mining, Data Mining, parameter tuning curse, Frequency measurement, Data mining, statistical distributions, Tuning, parameter-free approach, Bayesian criterion, sequential data, Sequential Classification Rules, robust sequential classification rule mining, Feature extraction, Data models, Robustness, common interestingness measures instability, rule pattern models, Bayes methods]
Patent Citation Recommendation for Examiners
2015 IEEE International Conference on Data Mining
None
2015
There is a consensus that U. S. patent examiners, who are responsible for identifying prior art relevant to adjudication of patentability of patent applications, often lack the time, resources and/or experience necessary to conduct a dequateprior art search. This study aims to build an automatic and effective system of patent citation recommendation for patent examiners. In addition to focusing on content and bibliographic information, our proposed system considers another important piece of information that is known by patent examiners, namely, applicant citations. We integrate applicant citations and bibliographic information of patents into a heterogeneous citation bibliographic network. Based on this network, we explore metapaths based relationships between a query patent application and a candidate prior patent and classify them into two categories:(1) Bibliographic meta-paths, (2) Applicant Bibliographic metapaths. We propose a framework based on a two-phase ranking approach: the first phase involves selection of a candidate subset from the whole U. S. patent data, and the second phase uses supervised learning models to rank prior patents in the candidate subset. The results show that both bibliographic informationand applicant citation information are very useful for examiner citation recommendation, and that our approach significantly outperforms a search engine.
[patent applications, Art, US patent examiners, patent citation recommendation, United States, content information, applicant citations, Search problems, Data mining, Search engines, citation analysis, patents, learning (artificial intelligence), two-phase ranking approach, bibliographic meta-path, Patents, search engine, ssupervised learning models, Patent mining, bibliographic information, Recommendation, applicant bibliographic metapath, Computer science, recommender systems, Supervised learning]
CNL: Collective Network Linkage Across Heterogeneous Social Platforms
2015 IEEE International Conference on Data Mining
None
2015
The popularity of social media has led many users to create accounts with different online social networks. Identifying these multiple accounts belonging to same user is of critical importance to user profiling, community detection, user behavior understanding and product recommendation. Nevertheless, linking users across heterogeneous social networks is challenging due to large network sizes, heterogeneous user attributes and behaviors in different networks, and noises in user generated data. In this paper, we propose an unsupervised method, Collective Network Linkage (CNL), to link users across heterogeneous social networks. CNL incorporates heterogeneous attributes and social features unique to social network users, handles missing data, and performs in a collective manner. CNL is highly accurate and efficient even without training data. We evaluate CNL on linking users across different social networks. Our experiment results on a Twitter network and another Foursquare network demonstrate that CNL performs very well and its accuracy is superior than the supervised Mobius approach.
[CNL, heterogeneous user behaviors, Foursquare network, user behavior understanding, online social networks, heterogeneous user attributes, Probability distribution, Twitter network, network sizes, Prediction algorithms, Numerical models, social media, community detection, Social network services, product recommendation, Media, unsupervised method, missing data handling, Couplings, heterogeneous social networks, social features, collective network linkage, user profiling, social networking (online), Joining processes]
Population Synthesis via k-Nearest Neighbor Crossover Kernel
2015 IEEE International Conference on Data Mining
None
2015
The recent development of multi-agent simulations brings about a need for population synthesis. It is a task of reconstructing the entire population from a sampling survey of limited size (1% or so), supplying the initial conditions from which simulations begin. This paper presents a new kernel density estimator for this task. Our method is an analogue of the classical Breiman-Meisel-Purcell estimator, but employs novel techniques that harness the huge degree of freedom which is required to model high-dimensional nonlinearly correlated datasets: the crossover kernel, the k-nearest neighbor restriction of the kernel construction set and the bagging of kernels. The performance as a statistical estimator is examined through real and synthetic datasets. We provide an "optimization-free" parameter selection rule for our method, a theory of how our method works and a computational cost analysis. To demonstrate the usefulness as a population synthesizer, our method is applied to a household synthesis task for an urban micro-simulator.
[multi-agent systems, kernel density estimation, multi-agent simulation, Computational modeling, computational cost analysis, Estimation, k-nearest neighbor, Statistics, town and country planning, k-nearest neighbor crossover kernel, urban microsimulator, multiagent simulation, crossover kernel, Sociology, Bandwidth, optimization-free parameter selection rule, population synthesis, k-nearest neighbor restriction, Breiman-Meisel-Purcell estimator, statistical analysis, Kernel, Bagging, statistical estimator]
Detecting Overlapping Communities from Local Spectral Subspaces
2015 IEEE International Conference on Data Mining
None
2015
Based on the definition of local spectral subspace, we propose a novel approach called LOSP for local overlapping community detection. Using the power method for a few steps, LOSP finds an approximate invariant subspace, which depicts the embedding of the local neighborhood structure around the seeds of interest. LOSP then identifies the local community expanded from the given seeds by seeking a sparse indicator vector in the subspace where the seeds are in its support. We provide a systematic investigation on LOSP, and thoroughly evaluate it on large real world networks across multiple domains. With the prior information of very few seed members, LOSP can detect the remaining members of a target community with high accuracy. Experiments demonstrate that LOSP outperforms the Heat Kernel and PageRank diffusions. Using LOSP as a subroutine, we further address the problem of multiple membership identification, which aims to find all the communities a single vertex belongs to. High F1 scores are achieved in detecting multiple local communities with respect to arbitrary single seed for various large real world networks.
[local neighborhood structure, sparse indicator vector, approximate invariant subspace, graph theory, multiple membership identification, network theory (graphs), Community detection, local spectral subspace, Electronic mail, Approximation methods, LOSP approach, Clustering, Seed set expansion, F1 scores, Systematics, local overlapping community detection, Algorithms, Heating, target community, Web sites, real world networks, Local spectral subspace, Kernel]
Scalable Hypergraph Learning and Processing
2015 IEEE International Conference on Data Mining
None
2015
A hypergraph allows a hyperedge to connect more than two vertices, using which to capture the high-order relationships, many hypergraph learning algorithms are shown highly effective in various applications. When learning large hypergraphs, converting them to graphs to employ the distributed graph frameworks is a common approach, yet it results in major efficiency drawbacks including an inflated problem size, the excessive replicas, and the unbalanced workloads. To avoid such drawbacks, we take a different approach and propose HyperX, which is a thin layer built upon Spark. To preserve the problem size, HyperX directly operates on a distributed hypergraph. To reduce the replicas, HyperX replicates the vertices but not the hyperedges. To balance the workloads, we investigate the hypergraph partitioning problem aiming at minimizing the space and the communication cost subject to two separate constraints on the hyperedge and the vertex workloads. With experiments on both real and synthetic datasets, we verify that HyperX significantly improves the efficiency of the learning algorithms when compared with the graph conversion approach.
[Algorithm design and analysis, Machine learning algorithms, distributed graph frameworks, Distributed Processing, graph conversion, Scalability, graph theory, scalable hypergraph learning, vertices, Partitioning algorithms, Hypergraph Learning, Sparks, hyperedge, Optimization, Proteins, HyperX, Approximation algorithms, Spark, learning (artificial intelligence)]
A General Suspiciousness Metric for Dense Blocks in Multimodal Data
2015 IEEE International Conference on Data Mining
None
2015
Which seems more suspicious: 5,000 tweets from 200 users on 5 IP addresses, or 10,000 tweets from 500 users on 500 IP addresses but all with the same trending topic and all in 10 minutes? The literature has many methods that try to find dense blocks in matrices, and, recently, tensors, but no method gives a principled way to score the suspiciouness of dense blocks with different numbers of modes and rank them to draw human attention accordingly. Dense blocks are worth inspecting, typically indicating fraud, emerging trends, or some other noteworthy deviation from the usual. Our main contribution is that we show how to unify these methods and how to give a principled answer to questions like the above. Specifically, (a) we give a list of axioms that any metric of suspicousness should satisfy, (b) we propose an intuitive, principled metric that satisfies the axioms, and is fast to compute, (c) we propose CROSSSPOT, an algorithm to spot dense regions, and sort them in importance ("suspiciousness") order. Finally, we apply CROSSSPOT to real data, where it improves the F1 score over previous techniques by 68% and finds retweet-boosting in a real social dataset spanning 0.3 billion posts.
[Measurement, retweet-boosting, Inspection, Twitter, multimodal data, social dataset, dense block, suspicious behavior, Data mining, dense blocks, general suspiciousness metric, Tensile stress, CROSSSPOT, human attention, data handling, IP networks, Facebook, trending topic]
Adaptive Heterogeneous Ensemble Learning Using the Context of Test Instances
2015 IEEE International Conference on Data Mining
None
2015
We consider binary classification problems where each of the two classes shows a multi-modal distribution in the feature space, and the classification has to be performed over different test scenarios, where every test scenario only involves a subset of the positive and negative modes in the data. In such conditions, there may exist certain pairs of positive and negative modes, termed as pairs of confusing modes, which may not appear together in the same test scenario but can be highly overlapping in the feature space. Determining the class labels at such pairs of confusing modes is challenging as the labeling decisions depend not only on the feature values but also on the context of the test scenario. To overcome this challenge, we present the Adaptive Heterogeneous Ensemble Learning (AHEL) algorithm, which constructs an ensemble of classifiers in accordance with the multi-modality within the classes, and further assigns adaptive weights to classifiers based on their relevance in the context of a test scenario. We demonstrate the effectiveness of our approach in comparison with baseline approaches on a synthetic dataset and a real-world application involving global water monitoring.
[Algorithm design and analysis, binary classification problems, multimodal distribution, baseline approaches, adaptive learning, Training, Earth, confusing modes, AHEL algorithm, real-world application, adaptive heterogeneous ensemble learning, Training data, multi-modality, Labeling, learning (artificial intelligence), binary classification, Monitoring, Context, negative modes, data analysis, feature space, synthetic dataset, adaptive weights, positive modes, data heterogeneity, test instances, global water monitoring]
Supervised Topic Models for Microblog Classification
2015 IEEE International Conference on Data Mining
None
2015
In this paper we present a topic model based approach for classifying micro-blog posts into a given topics of interests. The short nature of micro-blog posts make them challenging for directly learning a classification model. To overcome this limitation, we use content of the links embedded in these posts to improve the topic learning. The hypothesis is that since the link content is far richer than the content of the post itself, using link content along with the content of the post will help learning. However, how this link content can be used to construct features for classification remains a challenging issue. Furthermore, in previous methods, user based information is utilized in an ad-hoc manner that only work for certain type of classification, such as characterizing content of microblogs. In this paper, we propose supervised topic model, User-Labeled-LDA and its nonparametric variant that can avoid the ad-hoc feature construction task and model the topics in a discriminative way. Our experiments on a Twitter dataset shows that modeling user interests and link information helps in learning quality topics for sparse tweets as well as helps significantly in classification task. Our experiments further show that modeling this information in a principled way through topic models helps more than simply adding this information through features.
[latent Dirichlet allocation, Electronic publishing, pattern classification, topic classification, Blogs, Encyclopedias, Twitter, Data mining, tweets, microblog classification, supervised topic model, user-labeled-LDA, social networking (online), Data models, Internet]
Post Classification Label Refinement Using Implicit Ordering Constraint Among Data Instances
2015 IEEE International Conference on Data Mining
None
2015
Classification of instances into different categories in various real world applications suffer from inaccuracies due to lack of representative training data, limitations of classification models, noise and outliers in the input data etc. In this paper we propose a new post classification label refinement method for the scenarios where data instances have an inherent ordering among them that can be leveraged to correct inconsistencies in class labels. We show that by using the ordering constraint, more robust algorithms can be developed than traditional methods. Moreover in most applications where this ordering among instances exists, it is not directly observed. The proposed approach simultaneously estimates the latent ordering among instances and corrects the class labels. We demonstrate the utility of the approach for the application of monitoring the dynamics of lakes and reservoirs. The proposed approach has been evaluated on synthetic datasets with different noise structures and noise levels.
[pattern classification, classification models, robust algorithms, reservoirs, preference based ordering, rank aggregation, Noise measurement, representative training data, Earth, post classification label refinement, data instances, Image color analysis, Training data, implicit ordering constraint, Lakes, class labels, data handling, lakes, Colored noise, Monitoring, noise levels, synthetic datasets, noise structures]
Variable Selection for Efficient Nonnegative Tensor Factorization
2015 IEEE International Conference on Data Mining
None
2015
Nonnegative Tensor Factorization (NTF) has become a popular tool for extracting informative patterns from tensor data. However, NTF has high computational cost both in space and in time, mostly in iterative calculation of the gradient. In this paper, we consider variable selection to reduce the cost, assuming sparsity of the factor matrices. In fact, it is known that the factor matrices are often very sparse in many applications such as network analysis, text analysis and image analysis. We update only a small subset of important variables in each iterative step. We show the effectiveness of the algorithm analytically and experimentally in comparison with conventional NTF algorithms. The algorithm was five times faster than the naive algorithm in the best case and required one to five hundred times less memory while keeping the approximation accuracy as the same.
[Algorithm design and analysis, Input variables, tensors, matrix decomposition, Sparse matrices, Indexes, CANDECOMP/PARAFAC, Tensile stress, NTF algorithm, iterative calculation, Approximation algorithms, nonnegative tensor factorization, factor matrices, Tensor Factorization, Yttrium, naive algorithm, variable selection algorithm, Nonnegative Tensor Factorization]
Transfer Learning via Relational Type Matching
2015 IEEE International Conference on Data Mining
None
2015
Transfer learning is typically performed between problem instances within the same domain. We consider the problem of transferring across domains. To this effect, we adopt a probabilistic logic approach. First, our approach automatically identifies predicates in the target domain that are similar in their relational structure to predicates in the source domain. Second, it transfers the logic rules and learns the parameters of the transferred rules using target data. Finally, it refines the rules as necessary using theory refinement. Our experimental evidence supports that this transfer method finds models as good or better than those found with state-of-the-art methods, with and without transfer, and in a fraction of the time.
[transfer learning, Uncertainty, Transfer Learning, probabilistic logic, target domain, data mining, Probabilistic logic, Probability distribution, problem instances, automatic predicate identification, Data mining, relational type matching, source domain, relational structure, logic rules, Relational Learning, transferred rule parameter learning, Proteins, Learning systems, Markov processes, learning (artificial intelligence), probabilistic logic approach]
Theoretical and Empirical Criteria for the Edited Nearest Neighbour Classifier
2015 IEEE International Conference on Data Mining
None
2015
We aim to dispel the blind faith in theoretical criteria for optimisation of the edited nearest neighbour classifier and its version called the Voronoi classifier. Three criteria from past and recent literature are considered: two bounds using Vapnik-Chervonenkis (VC) dimension and a probabilistic criterion derived by a Bayesian approach. We demonstrate the shortcomings of these criteria for selecting the best reference set, and summarise alternative empirical criteria found in the literature.
[pattern classification, edited nearest neighbour classifier, Voronoi classifier, computational geometry, Probabilistic logic, Electronic mail, Data mining, probabilistic criterion, Training, Upper bound, Bayesian approach, theoretical criteria, Vapnik-Chervonenkis dimension, Prototypes, empirical criteria, Bayes methods]
LambdaMF: Learning Nonsmooth Ranking Functions in Matrix Factorization Using Lambda
2015 IEEE International Conference on Data Mining
None
2015
This paper emphasizes optimizing ranking measures in a recommendation problem. Since ranking measures are non-differentiable, previous works have been proposed to deal with this problem via approximations or lower/upper bounding of the loss. However, such mismatch between ranking measures and approximations/bounds can lead to non-optimal ranking results. To solve this problem, we propose to model the gradient of non-differentiable ranking measure based on the idea of virtual gradient, which is called lambda in learning to rank. In addition, noticing the difference between learning to rank and recommendation models, we prove that under certain circumstance the existence of popular items can lead to unlimited norm growing of the latent factors in a matrix factorization model. We further create a novel regularization term to remedy such concern. Finally, we demonstrate that our model, LambdaMF, outperforms several state-of-the-art methods. We further show in experiments that in all cases our model achieves global optimum of normalized discount cumulative gain during training. Detailed implementation and supplementary material can be found at (http://www.csie.ntu.edu.tw/~b00902055/).
[pattern classification, latent factors, nonsmooth ranking functions learning, Predictive models, LambdaMF, Loss measurement, matrix decomposition, Approximation methods, Matrix decomposition, Optimization, recommender systems, virtual gradient, nondifferentiable ranking measure gradient, regularization term, normalized discount cumulative gain, Collaboration, lambda matrix factorization, Data models, learning (artificial intelligence), recommendation problem]
Measuring Large-Scale Dynamic Graph Similarity by RICom: RWR with Intergraph Compression
2015 IEEE International Conference on Data Mining
None
2015
By how much is a large-scale graph transformed over time or by a significant event?' or 'how structurally similar are two large-scale graphs?' are the two questions that this paper attempts to address. The proposed method efficiently calculates and accurately produces graph similarity. Our approach is based on the well-known random walk with restart (RWR) algorithm, which quantifies relevance between nodes to express the structural and connection characteristics of graphs. Intergraph compression, which is inspired by interframe compression, merges two input graphs and reorders their nodes contributing to improved process-data storage efficiency and processing convenience. This is a boon to the RWR algorithm for large-scale graphs. The representation of a graph transformed via intergraph compression can be used to accurately show similarity because sub-matrix blocks are reordered to concentrate nonzero elements. In performing the RWR algorithm, which quantifies inter-node relevance, transformed representation of graph with Intergraph compression is efficient in space requirement and produces results more quickly and accurately over conventional graph transformation schemes. We demonstrate the validity of our method through experiments and apply it to the usage data of public transportation SmartCard in a large metropolitan area to suggest usefulness of the proposed algorithm.
[Algorithm design and analysis, graph structural characteristics, Heuristic algorithms, graph theory, random walk with restart (RWR), RWR algorithm, data processing improvement, graph similarity, SmartCard, random walk-with-restart algorithm, Image coding, input graph merging, interframe compression, large metropolitan area, structurally-similar graphs, data compression, intergraph compression, graph connection characteristics, random processes, Time measurement, dynamic graph, Public transportation, RICom, submatrix blocks, public transportation, Atmospheric measurements, large-scale dynamic graph similarity measurement, node reordering, Particle measurements, process-data storage efficiency improvement, internode relevance, space requirement]
Fast Matrix-Vector Multiplications for Large-Scale Logistic Regression on Shared-Memory Systems
2015 IEEE International Conference on Data Mining
None
2015
Shared-memory systems such as regular desktops now possess enough memory to store large data. However, the training process for data classification can still be slow if we do not fully utilize the power of multi-core CPUs. Many existing works proposed parallel machine learning algorithms by modifying serial ones, but convergence analysis may be complicated. Instead, we do not modify machine learning algorithms, but consider those that can take the advantage of parallel matrix operations. We particularly investigate the use of parallel sparse matrix-vector multiplications in a Newton method for large scale logistic regression. Various implementations from easy to sophisticated ones are analyzed and compared. Results indicate that under suitable settings excellent speedup can be achieved.
[Algorithm design and analysis, pattern classification, Machine learning algorithms, parallel matrix-vector multiplication, Instruction sets, large-scale logistic regression, parallel sparse matrix-vector multiplications, Sparse matrices, classification, large data classification, Training, convergence analysis, parallel machine learning algorithms, sparse matrix, shared memory systems, parallel matrix operations, learning (artificial intelligence), Newton method, multicore CPU, Logistics]
Iterative Classification for Sanitizing Large-Scale Datasets
2015 IEEE International Conference on Data Mining
None
2015
Cheap ubiquitous computing enables the collection of massive amounts of personal data in a wide variety of domains. Many organizations aim to share such data while obscuring features that could disclose identities or other sensitive information. Much of the data now collected exhibits weak structure (e.g., natural language text) and machine learning approaches have been developed to identify and remove sensitive entities in such data. Learning-based approaches are never perfect and relying upon them to sanitize data can leak sensitive information as a consequence. However, a small amount of risk is permissible in practice, and, thus, our goal is to balance the value of data published and the risk of an adversary discovering leaked sensitive information. We model data sanitization as a game between 1) a publisher who chooses a set of classifiers to apply to data and publishes only instances predicted to be non-sensitive and 2) an attacker who combines machine learning and manual inspection to uncover leaked sensitive entities (e.g., personal names). We introduce an iterative greedy algorithm for the publisher that provably executes no more than a linear number of iterations, and ensures a low utility for a resource-limited adversary. Moreover, using several real world natural language corpora, we illustrate that our greedy algorithm leaves virtually no automatically identifiable sensitive instances for a state-of-the-art learning algorithm, while sharing over 93% of the original data, and completes after at most 5 iterations.
[iterative methods, leaked sensitive entities, Manuals, Predictive models, natural language corpora, ubiquitous computing, iterative classification, data sanitization, Publishing, weak structured data sanitization, learning (artificial intelligence), resource-limited adversary, iterative greedy algorithm, pattern classification, learning algorithm, greedy algorithms, natural language processing, Natural languages, game theory, Inspection, Privacy preserving, machine learning, personal data, leaked sensitive information, large scale datasets, Data models, Yttrium]
Analysis of Spectral Space Properties of Directed Graphs Using Matrix Perturbation Theory with Application in Graph Partition
2015 IEEE International Conference on Data Mining
None
2015
The eigenspace of the adjacency matrix of a graph possesses important information about the network structure. However, analyzing the spectral space properties for directed graphs is challenging due to complex valued decompositions. In this paper, we explore the adjacency eigenspaces of directed graphs. With the aid of the graph perturbation theory, we emphasize on deriving rigorous mathematical results to explain several phenomena related to the eigenspace projection patterns that are unique for directed graphs. Furthermore, we relax the community structure assumption and generalize the theories to the perturbed Perron-Frobenius simple invariant subspace so that the theories can adapt to a much broader range of network structural types. We also develop a graph partitioning algorithm and conduct evaluations to demonstrate its potential.
[Symmetric matrices, Directed graphs, network theory (graphs), Linear programming, spectral space property, Partitioning algorithms, Matrix perturbation, matrix perturbation theory, complex valued decomposition, Perron-Frobenius subspace, matrix algebra, adjacency matrix eigenspace, Asymmetric adjacency matrices, Graph partition, perturbation theory, directed graphs, Clustering algorithms, directed graph, Eigenvalues and eigenfunctions, Spectral projection, Yttrium, Mathematical model, network structure, graph partitioning algorithm]
The Convergence Behavior of Naive Bayes on Large Sparse Datasets
2015 IEEE International Conference on Data Mining
None
2015
Large and sparse datasets with a lot of missing values are common in the big data era. Naive Bayes is a good classification algorithm for such datasets, as its time and space complexity scales well with the size of non-missing values. However, several important questions about the behavior of naive Bayes are yet to be answered. For example, how different mechanisms of missing, data sparseness and the number of attributes systematically affect the learning curves and convergence? Recent work in classifying large and sparse real-world datasets still could not address these questions mainly because the data missing mechanisms of these datasets are not taken into account. In this paper, we propose two novel data missing and expansion mechanisms to answer these questions. We use the data missing mechanisms to generate large and sparse data with various properties, and study the entire learning curve and convergence behavior of naive Bayes. We made several observations, which are verified through detailed theoretical study. Our results are useful for learning large sparse data in practice.
[pattern classification, space complexity scales, convergence, time complexity scales, data sparseness, Big Data, data expansion mechanisms, Complexity theory, Convergence, Training, naive Bayes behavior, Upper bound, learning curves, Prototypes, large real-world datasets classification, Motion pictures, Big data, sparse real-world datasets classification, Bayes methods, learning (artificial intelligence), convergence behavior, computational complexity, data missing mechanisms]
DRN: Bringing Greedy Layer-Wise Training into Time Dimension
2015 IEEE International Conference on Data Mining
None
2015
Sequential data modeling has received growing interests due to its impact on real world problems. Sequential data is ubiquitous - financial transactions, advertise conversions and disease evolution are examples of sequential data. A long-standing challenge in sequential data modeling is how to capture the strong hidden correlations among complex features in high volumes. The sparsity and skewness in the features extracted from sequential data also add to the complexity of the problem. In this paper, we address these challenges from both discriminative and generative perspectives, and propose novel stochastic learning algorithms to model nonlinear variances from static time frames and their transitions. The proposed model, Deep Recurrent Network (DRN), can be trained in an unsupervised fashion to capture transitions, or in a discriminative fashion to conduct sequential labeling. We analyze the conditional independence of each functional module and tackle the diminishing gradient problem by developing a two-pass training algorithm. Extensive experiments on both simulated and real-world dynamic networks show that the trained DRN outperforms all baselines in the sequential classification task and obtains excellent performance in the regression task.
[advertise conversions, Heuristic algorithms, two-pass training algorithm, regression analysis, complex feature extraction, Data mining, Training, feature extraction, unsupervised training, regression task, sequential data modeling, Mathematical model, simulated dynamic networks, nonlinear variances, real-world dynamic networks, pattern classification, discriminative perspectives, Computational modeling, deep recurrent network, recurrent neural nets, static time frames, financial transactions, generative perspectives, sequential classification task, stochastic learning algorithms, unsupervised learning, sequential labeling, trained DRN, Hidden Markov models, Data models, disease evolution]
Learning User Preferences across Multiple Aspects for Merchant Recommendation
2015 IEEE International Conference on Data Mining
None
2015
With the pervasive use of mobile devices, Location Based Social Networks(LBSNs) have emerged in past years. These LBSNs, allowing their users to share personal experiences and opinions on visited merchants, have very rich and useful information which enables a new breed of location-based services, namely, Merchant Recommendation. Existing techniques for merchant recommendation simply treat each merchant as an item and apply conventional recommendation algorithms, e.g., Collaborative Filtering, to recommend merchants to a target user. However, they do not differentiate the user's real preferences on various aspects, and thus can only achieve limited success. In this paper, we aim to address this problem by utilizing and analyzing user reviews to discover user preferences in different aspects. Following the intuition that a user rating represents a personalized rational choice, we propose a novel utility-based approach by combining collaborative and individual views to estimate user preference (i.e., rating). An optimization algorithm based on a Gaussian model is developed to train our merchant recommendation approach. Lastly we evaluate the proposed approach in terms of effectiveness, efficiency and cold-start using two real-world datasets. The experimental results show that our approach outperforms the state-of-the-art methods. Meanwhile, a real mobile application is implemented to demonstrate the practicability of our method.
[Economics, learning algorithm, Social network services, optimization algorithm, Mobile applications, user preference learning, user review analysis, utility-based approach, Analytical models, mobile computing, recommender systems, recommender system, merchant recommendation, recommendation algorithms, Collaboration, Gaussian processes, Gaussian model, location-based services, learning (artificial intelligence), Recommender systems, business data processing, Context modeling, utility theory, Location based social network]
Logdet Divergence Based Sparse Non-Negative Matrix Factorization for Stable Representation
2015 IEEE International Conference on Data Mining
None
2015
Non-negative matrix factorization (NMF) decomposes any non-negative matrix into the product of two low dimensional non-negative matrices. Since NMF learns effective parts-based representation, it has been widely applied in computer vision and data mining. However, traditional NMF has the riskrisk learning rank-deficient basis learning rank-deficient basis on high-dimensional dataset with few examples especially when some examples are heavily corrupted by outliers. In this paper, we propose a Logdet divergence based sparse NMF method (LDS-NMF) to deal with the rank-deficiency problem. In particular, LDS-NMF reduces the risk of rank deficiency by minimizing the Logdet divergence between the product of basis matrix with its transpose and the identity matrix, meanwhile penalizing the density of the coefficients. Since the objective function of LDS-NMF is nonconvex, it is difficult to optimize. In this paper, we develop a multiplicative update rule to optimize LDS-NMF in the frame of block coordinate descent, and theoretically prove its convergence. Experimental results on popular datasets show that LDS-NMF can learn more stable representations than those learned by representative NMF methods.
[Non-negative matrix factorization, Logdet divergence, NMF, data mining, Linear programming, transpose matrix, LDS-NMF method, matrix decomposition, Sparse matrices, Data mining, identity matrix, Training, Computer science, block coordinate descent, robust matrix decomposition, risk learning rank-deficiency problem, image representation, computer vision, Nuclear magnetic resonance, parts-based representation, sparse nonnegative matrix factorization, Principal component analysis]
Clustering with Partition Level Side Information
2015 IEEE International Conference on Data Mining
None
2015
Constrained clustering uses pre-given knowledge to improve the clustering performance. Among existing literature, researchers usually focus on Must-Link and Cannot-Link pairwise constraints. However, pairwise constraints not only disobey the way we make decisions, but also suffer from the vulnerability of noisy constraints and the order of constraints. In light of this, we use partition level side information instead of pairwise constraints to guide the process of clustering. Compared with pairwise constraints, partition level side information keeps the consistency within partial structure and avoids self-contradictory and the impact of constraints order. Generally speaking, only small part of the data instances are given labels by human workers, which are used to supervise the procedure of clustering. Inspired by the success of ensemble clustering, we aim to find a clustering solution which captures the intrinsic structure from the data itself, and agrees with the partition level side information as much as possible. Then we derive the objective function and equivalently transfer it into a K-mean-like optimization problem. Extensive experiments on several real-world datasets demonstrate the effectiveness and efficiency of our method compared to pairwise constrained clustering and consensus clustering, which verifies the superiority of partition level side information to pairwise constraints. Besides, our method has high robustness to noisy side information.
[ensemble clustering, k-mean-like optimization problem, Utility function, pairwise constrained clustering, Optimization, cannot-link pairwise constraint, optimisation, noisy side information, Clustering algorithms, noisy constraint, Robustness, Labeling, data instance, human worker, Partition level side information, objective function, K-means, Symmetric matrices, partition level side information, Linear programming, Noise measurement, Clustering, self-contradictory, consensus clustering, pattern clustering, clustering performance, clustering solution, must-link pairwise constraint]
Station Site Optimization in Bike Sharing Systems
2015 IEEE International Conference on Data Mining
None
2015
Bike sharing systems, aiming at providing the missing links in the public transportation systems, are becoming popular in urban cities. In an ideal bike sharing network, the station locations are usually selected in a way that there are balanced pick-ups and drop-offs among stations. This can help avoid expensive re-balancing operations and maintain high user satisfaction. However, it is a challenging task to develop such an efficient bike sharing system with appropriate station locations. Indeed, the bike station demand is influenced by multiple factors of surrounding environment and complex public transportation networks. Limited efforts have been made to develop demand-and-balance prediction models for bike sharing systems by considering all these factors. To this end, in this paper, we propose a bike sharing network optimization approach by considering multiple influential factors. The goal is to enhance the quality and efficiency of the bike sharing service by selecting the right station locations. Along this line, we first extract fine-grained discriminative features from human mobility data, point of interests (POI), as well as station network structures. Then, prediction models based on Artificial Neural Networks (ANN) are developed for predicting station demand and balance. In addition, based on the learned patterns of station demand and balance, a genetic algorithm based optimization model is built to choose a set of stations from a large number of candidates in a way such that the station usage is maximized and the number of unbalanced stations is minimized. Finally, the extensive experimental results on the NYC CitiBike sharing system show the advantages of our approach for optimizing the station site allocation in terms of the bike usage as well as the required re-balancing efforts.
[ANN, bike usage, bike sharing network optimization model, Predictive models, complex public transportation networks, Site location optimization, Optimization, demand-and-balance prediction models, genetic algorithm, NYC CitiBike sharing system, bike sharing systems, station locations, station usage, Bike Sharing System, urban cities, balanced pick-ups, unbalanced stations, public transportation systems, bike station demand, Neural Network Prediction, genetic algorithms, POI, Public transportation, re-balancing operations, transportation, point of interests, fine-grained discriminative features, station site allocation, artificial neural networks, station site optimization, Neural networks, Bicycles, Feature extraction, bike sharing service, bicycles, station network structures, neural nets, user satisfaction, human mobility data]
Spatio-Temporal Topic Models for Check-in Data
2015 IEEE International Conference on Data Mining
None
2015
Twitter, together with other online social networks, such as Facebook, and Gowalla have begun to collect hundreds of millions of check-ins. Check-in data captures the spatial and temporal information of user movements and interests. To model and analyze the spatio-temporal aspect of check-in data and discover temporal topics and regions, we propose two spatio-temporal topic models: Downstream Spatio-Temporal Topic Model (DSTTM) and Upstream Spatio-Temporal Topic Model (USTTM). Both models can discover temporal topics and regions. We use continuous time to model check-in data, rather than discretized time, avoiding the loss of information through discretization. In order to capture the property that user's interests and activity space will change over time, we propose the USTTM, where users have different region and topic distributions at different times. We conduct experiments on Twitter and Gowalla data sets. In our quantitative analysis, we evaluate the effectiveness of our models by the perplexity, the accuracy of POI recommendations, and user prediction, demonstrating that our models achieve better performance than the state-of-the-art models.
[Spatio-temporal, temporal topic discovery, online social networks, user prediction, Predictive models, Gaussian distribution, Twitter, downstream spatiotemporal topic model, Probability distribution, user movements, user activity space, Cities and towns, Twitter data sets, data acquisition, spatiotemporal aspect analysis, temporal region discovery, quantitative analysis, Facebook, Check-in, spatiotemporal topic model, POI recommendations, temporal information, Gowalla data sets, upstream spatiotemporal topic model, Probabilistic logic, check-in data model, spatiotemporal phenomena, social networking (online), Data models, DSTTM, USTTM, Topic Model, spatial information, user interests]
Missing Value Estimation for Hierarchical Time Series: A Study of Hierarchical Web Traffic
2015 IEEE International Conference on Data Mining
None
2015
Hierarchical time series (HTS) is a special class of multivariate time series where many related time series are organized in a hierarchical tree structure and they are consistent across hierarchy levels. HTS modeling is crucial and serves as the basis for business planning and management in many areas such as manufacturing inventory, energy and traffic management. However, due to machine failures, network disturbances or human maloperation, HTS data suffer from missing values across different hierarchical levels. In this paper, we study the missing value estimation problem under hierarchical web traffic settings, where the user-visit traffic are organized in various hierarchical structures, such as geographical structure and website structure. We develop an efficient algorithm, HTSImpute, to accurately estimate the missing value in multivariate noisy web traffic time series with specific hierarchical consistency in HTS settings. Our HTSImpute is able to (1) utilize the temporal dependence information within each individual time series, (2) exploit the intra-relations between time series through hierarchy, (3) guarantee the satisfaction of hierarchical consistency constraints. Results on three synthetic HTS datasets and three real-world hierarchical web traffic datasets demonstrate that our approach is able to provide more accurate and hierarchically consistent estimations than other baselines.
[High-temperature superconductors, hierarchical Web traffic settings, Time series analysis, multivariate time series, Estimation, user-visit traffic, Predictive models, time series, HTSImpute, Forecasting, missing value estimation problem, multivariate noisy Web traffic time series, Time Series Analysis, Hierarchical Time Series, hierarchical consistency constraints, Missing Value Estimation, Internet, Yttrium, hierarchical time series, telecommunication traffic, Business]
Absorbing Random-Walk Centrality: Theory and Algorithms
2015 IEEE International Conference on Data Mining
None
2015
We study a new notion of graph centrality based on absorbing random walks. Given a graph G = (V, E) and a set of query nodes Q &#x2286; V, we aim to identify the k most central nodes in G with respect to Q. Specifically, we consider central nodes to be absorbing for random walks that start at the query nodes Q. The goal is to find the set of k central nodes that minimizes the expected length of a random walk until absorption. The proposed measure, which we call k absorbing random-walk centrality, favors diverse sets, as it is beneficial to place the k absorbing nodes in different parts of the graph so as to &#x201C;intercept&#x201D; random walks that start from different query nodes. Although similar problem definitions have been considered in the literature, e.g., in information-retrieval settings where the goal is to diversify web-search results, in this paper we study the problem formally and prove some of its properties. We find that the problem is NP-hard, while the objective function is monotone and supermodular, implying that a greedy algorithm provides solutions with an approximation guarantee. On the other hand, the greedy algorithm involves expensive matrix operations that make it prohibitive to employ on large datasets. To confront this challenge, we explore the performance of efficient heuristics.
[Greedy algorithms, approximation theory, random walks, random-walk centrality, greedy algorithms, graph theory, query node, Length measurement, Linear programming, Probability distribution, Web-search result, node centrality, Computer science, approximation guarantee, graph mining, Q measurement, NP-hard problem, matrix operation, Approximation algorithms, graph centrality, greedy algorithm, computational complexity, objective function]
Personalized Grade Prediction: A Data Mining Approach
2015 IEEE International Conference on Data Mining
None
2015
To increase efficacy in traditional classroom courses as well as in Massive Open Online Courses (MOOCs), automated systems supporting the instructor are needed. One important problem is to automatically detect students that are going to do poorly in a course early enough to be able to take remedial actions. This paper proposes an algorithm that predicts the final grade of each student in a class. It issues a prediction for each student individually, when the expected accuracy of the prediction is sufficient. The algorithm learns online what is the optimal prediction and time to issue a prediction based on past history of students' performance in a course. We derive demonstrate the performance of our algorithm on a dataset obtained based on the performance of approximately 700 undergraduate students who have taken an introductory digital signal processing over the past 7 years. Using data obtained from a pilot course, our methodology suggests that it is effective to perform early in-class assessments such as quizzes, which result in timely performance prediction for each student, thereby enabling timely interventions by the instructor (at the student or class level) when necessary.
[Algorithm design and analysis, Measurement, data mining, undergraduate students, data mining approach, Data mining, personalized grade prediction, MOOC, Education, Prediction algorithms, digital signal processing education, Forecasting algorithms, massive open online courses, early in-class assessments, further education, student final grade prediction, traditional classroom courses, introductory digital signal processing, Signal processing algorithms, educational courses, grade prediction, computer aided instruction, Yttrium, student performance, online learning]
CrowdTC: Crowdsourced Taxonomy Construction
2015 IEEE International Conference on Data Mining
None
2015
Recently, taxonomy has attracted much attention. Both automatic construction solutions and human-based computation approaches have been proposed. The automatic methods suffer from the problem of either low precision or low recall and human computation, on the other hand, is not suitable for large scale tasks. Motivated by the shortcomings of both approaches, we present a hybrid framework, which combines the power of machine-based approaches and human computation (the crowd) to construct a more complete and accurate taxonomy. Specifically, our framework consists of two steps: we first construct a complete but noisy taxonomy automatically, then crowd is introduced to adjust the entity positions in the constructed taxonomy. However, the adjustment is challenging as the budget (money) for asking the crowd is often limited. In our work, we formulate the problem of finding the optimal adjustment as an entity selection optimization (ESO) problem, which is proved to be NP-hard. We then propose an exact algorithm and a more efficient approximation algorithm with an approximation ratio of 1/2(1-1/e). We conduct extensive experiments on real datasets, the results show that our hybrid approach largely improves the recall of the taxonomy with little impairment for precision.
[Crowdsourcing, approximation theory, CrowdTC, Uncertainty, NP-hard, information management, ESO problem, Taxonomy, human-based computation approaches, Noise measurement, Approximation methods, crowdsourced taxonomy construction, entity selection optimization, Optimization, automatic construction solutions, optimisation, Taxonomy Construction, Syntactics, human computation, approximation algorithm, Approximation algorithms, hybrid framework, machine-based approaches, computational complexity]
Outcomes Prediction via Time Intervals Related Patterns
2015 IEEE International Conference on Data Mining
None
2015
The increasing availability of multivariate temporal data in many domains, such as biomedical, security and more, provides exceptional opportunities for temporal knowledge discovery, classification and prediction, but also challenges. Temporal variables are often sparse and in many domains, such as in biomedical data, they have huge number of variables. In recent decades in the biomedical domain events, such as conditions, drugs and procedures, are stored as time intervals, which enables to discover Time Intervals Related Patterns (TIRPs) and use for classification or prediction. In this study we present a framework for outcome events prediction, called Maitreya, which includes an algorithm for TIRPs discovery called KarmaLegoD, designed to handle huge number of symbols. Three indexing strategies for pairs of symbolic time intervals are proposed and compared, showing that the use of FullyHashed indexing is only slightly slower but consumes minimal memory. We evaluated Maitreya on eight real datasets for the prediction of clinical procedures as outcome events. The use of TIRPs outperform the use of symbols, especially with horizontal support (number of instances) as TIRPs feature representation.
[Algorithm design and analysis, temporal knowledge discovery, data mining, temporal knowledge classification, clinical procedure prediction, temporal knowledge prediction, Time Intervals Mining, biomedical domain events, Data mining, biomedical data, TIRP, Prediction algorithms, pattern classification, Maitreya, indexing, Time series analysis, drugs, Prediction, time intervals related patterns, multivariate temporal data availability, KarmaLegoD, temporal variables, bioinformatics, outcome events prediction, medical computing, Indexing]
Experience-Aware Item Recommendation in Evolving Review Communities
2015 IEEE International Conference on Data Mining
None
2015
Current recommender systems exploit user and item similarities by collaborative filtering. Some advanced methods also consider the temporal evolution of item ratings as a global background process. However, all prior methods disregard the individual evolution of a user's experience level and how this is expressed in the user's writing in a review community. In this paper, we model the joint evolution of user experience, interest in specific item facets, writing style, and rating behavior. This way we can generate individual recommendations that take into account the user's maturity level (e.g., recommending art movies rather than blockbusters for a cinematography expert). As only item ratings and review texts are observables, we capture the user's experience and interests in a latent model learned from her reviews, vocabulary and writing style. We develop a generative HMM-LDA model to trace user evolution, where the Hidden Markov Model (HMM) traces her latent experience progressing over time -- with solely user reviews and ratings as observables over time. The facets of a user's interest are drawn from a Latent Dirichlet Allocation (LDA) model derived from her reviews, as a function of her (again latent) experience level. In experiments with four realworld datasets, we show that our model improves the rating prediction over state-of-the-art baselines, by a substantial margin. In addition, our model can also give some interpretations for the user experience level.
[Vocabulary, collaborative filtering, user evolution, hidden Markov model, Predictive models, evolving review community, item ratings, user interfaces, hidden Markov models, user experience level, Recommender, Motion pictures, global background process, User Model, user maturity level, experience-aware item recommendation, Language Model, latent dirichlet allocation, recommender systems, Experience, Hidden Markov models, item similarities, latent model, Writing, generative HMM-LDA model, Resource management, Lenses]
Two-Step Heterogeneous Finite Mixture Model Clustering for Mining Healthcare Databases
2015 IEEE International Conference on Data Mining
None
2015
Dealing with real-life databases often implies handling sets of heterogeneous variables. We are proposing in this paper a methodology for exploring and analyzing such databases, with an application in the specific domain of healthcare data analytics. We are thus proposing a two-step heterogeneous finite mixture model, with a first step involving a joint mixture of Gaussian and multinomial distribution to handle numerical (i.e., real and integer numbers) and categorical variables (i.e., discrete values), and a second step featuring a mixture of hidden Markov models to handle sequences of categorical values (e.g., series of events). This approach is evaluated on a real-world application, the clustering of administrative healthcare databases from Que&#x0301;bec, with results illustrating the good performances of the proposed method.
[data mining, Medical services, Administrative health care databases, hidden Markov models, Mixed attributes, Databases, Clustering algorithms, Mixture models, Numerical models, medical administrative data processing, health care, healthcare data analytics, data analysis, administrative healthcare database, heterogeneous finite mixture model clustering, multinomial distribution, Multivalued categorical variables, Partitioning algorithms, Clustering, pattern clustering, Hidden Markov models, Gaussian processes, Gaussian, healthcare database mining, heterogeneous variables, Finite mixture model, mixture models]
Quality Control for Crowdsourced Hierarchical Classification
2015 IEEE International Conference on Data Mining
None
2015
Repeated labeling is a widely adopted quality control method in crowdsourcing. This method is based on selecting one reliable label from multiple labels collected by workers because a single label from only one worker has a wide variance of accuracy. Hierarchical classification, where each class has a hierarchical relationship, is a typical task in crowdsourcing. However, direct applications of existing methods designed for multi-class classification have the disadvantage of discriminating among a large number of classes. In this paper, we propose a label aggregation method for hierarchical classification tasks. Our method takes the hierarchical structure into account to handle a large number of classes and estimate worker abilities more precisely. Our method is inspired by the steps model based on item response theory, which models responses of examinees to sequentially dependent questions. We considered hierarchical classification to be a question consisting of a sequence of subquestions and built a worker response model for hierarchical classification. We conducted experiments using real crowdsourced hierarchical classification tasks and demonstrated the benefit of incorporating a hierarchical structure to improve the label aggregation accuracy.
[Crowdsourcing, pattern classification, quality control, hierarchical structure, hierarchical classification, crowdsourced hierarchical classification tasks, Probabilistic logic, Electronic mail, worker abilities, worker response model, quality control method, hierarchical relationship, multiclass classification, label aggregation method accuracy, Quality control, item response theory, Labeling, Reliability, crowdsoucring, Pharmaceuticals]
Sparse Hierarchical Tucker Factorization and Its Application to Healthcare
2015 IEEE International Conference on Data Mining
None
2015
We propose a new tensor factorization method, called the Sparse Hierarchical-Tucker (Sparse H-Tucker), for sparse and high-order data tensors. Sparse H-Tucker is inspired by its namesake, the classical Hierarchical Tucker method, which aims to compute a tree-structured factorization of an input data set that may be readily interpreted by a domain expert. However, Sparse H-Tucker uses a nested sampling technique to overcome a key scalability problem in Hierarchical Tucker, which is the creation of an unwieldy intermediate dense core tensor, the result of our approach is a faster, more space-efficient, and more accurate method. We test our method on a real healthcare dataset, which is collected from 30K patients and results in an 18th order sparse data tensor. Unlike competing methods, Sparse H-Tucker can analyze the full data set on a single multi-threaded machine. It can also do so more accurately and in less time than the state-of-the-art: on a 12th order subset of the input data, Sparse H-Tucker is 18x more accurate and 7.5x faster than a previously state-of-the-art method. Moreover, we observe that Sparse H-Tucker scales nearly linearly in the number of non-zero tensor elements. The resulting model also provides an interpretable disease hierarchy, which is confirmed by a clinical expert.
[intermediate dense core tensors, sparse hierarchical tucker factorization, tensors, matrix decomposition, Approximation methods, Sparse matrices, healthcare dataset, clinical expert, interpretable disease hierarchy, medical administrative data processing, Mathematical model, health care, input data set, multi-threading, tensor factorization method, Computational modeling, tree-structured factorization, 18th order sparse data tensor, multithreaded machine, diseases, domain expert, Diseases, Tensile stress, key scalability problem, Approximation algorithms, sampling technique, sparse H-Tucker, sparse high-order data tensors]
Task Assignment Optimization in Collaborative Crowdsourcing
2015 IEEE International Conference on Data Mining
None
2015
A number of emerging applications, such as, collaborative document editing, sentence translation, and citizen journalism require workers with complementary skills and expertise to form groups and collaborate on complex tasks. While existing research has investigated task assignment for knowledge intensive crowdsourcing, they often ignore the aspect of collaboration among workers, that is central to the success of such tasks. Research in behavioral psychology has indicated that large groups hinder successful collaboration. Taking that into consideration, our work is one of the first to investigate and formalize the notion of collaboration among workers and present theoretical analyses to understand the hardness of optimizing task assignment. We propose efficient approximation algorithms with provable theoretical guarantees and demonstrate the superiority of our algorithms through a comprehensive set of experiments using real-world and synthetic datasets. Finally, we conduct a real world collaborative sentence translation application using Amazon Mechanical Turk that we hope provides a template for evaluating collaborative crowdsourcing tasks in micro-task based crowdsourcing platforms.
[Algorithm design and analysis, algorithms, Human factors, complementary skills, approximation algorithms, Approximation methods, real-world datasets, Optimization, knowledge intensive crowdsourcing, psychology, optimization, collaborative document editing, task assignment, collaborative crowdsourcing tasks, synthetic datasets, Crowdsourcing, approximation theory, crowdsourcing, citizen journalism, collaborative sentence translation application, microtask based crowdsourcing platforms, sentence translation, behavioral psychology, Collaboration, Amazon Mechanical Turk, Approximation algorithms, collaborative crowdsourcing, language translation]
Differentially Private Random Forest with High Utility
2015 IEEE International Conference on Data Mining
None
2015
Privacy-preserving data mining has become an active focus of the research community in the domains where data are sensitive and personal in nature. For example, highly sensitive digital repositories of medical or financial records offer enormous values for risk prediction and decision making. However, prediction models derived from such repositories should maintain strict privacy of individuals. We propose a novel random forest algorithm under the framework of differential privacy. Unlike previous works that strictly follow differential privacy and keep the complete data distribution approximately invariant to change in one data instance, we only keep the necessary statistics (e.g. variance of the estimate) invariant. This relaxation results in significantly higher utility. To realize our approach, we propose a novel differentially private decision tree induction algorithm and use them to create an ensemble of decision trees. We also propose feasible adversary models to infer about the attribute and class label of unknown data in presence of the knowledge of all other data. Under these adversary models, we derive bounds on the maximum number of trees that are allowed in the ensemble while maintaining privacy. We focus on binary classification problem and demonstrate our approach on four real-world datasets. Compared to the existing privacy preserving approaches we achieve significantly higher utility.
[Data privacy, decision tree ensemble, data mining, privacy-preserving data mining, random forest, real-world datasets, Privacy, Decision trees, learning (artificial intelligence), data instance, privacy preserving data mining, pattern classification, binary classification problem, unknown data class label, differential privacy framework, Standards, differentially private random forest, attribute inference, Sensitivity, complete data distribution, differentially-private decision tree induction algorithm, Vegetation, decision trees, adversary models, data privacy, differential privacy]
Finding Time-Critical Responses for Information Seeking in Social Media
2015 IEEE International Conference on Data Mining
None
2015
Social media is being increasingly used to request information and help in situations like natural disasters, where time is a critical commodity. However, generic social media platforms are not explicitly designed for timely information seeking, making it difficult for users to obtain prompt responses. Algorithms to ensure prompt responders for questions in social media have to understand the factors affecting their response time. In this paper, we draw from sociological studies on information seeking and organizational behavior to model the future availability and past response behavior of the candidate responders. We integrate these criteria with their interests to identify users who can provide timely and relevant responses to questions posted in social media. We propose a learning algorithm to derive optimal rankings of responders for a given question. We present questions posted on Twitter as a form of information seeking activity in social media. Our experiments demonstrate that the proposed framework is useful in identifying timely and relevant responders for questions in social media.
[Algorithm design and analysis, learning algorithm, time-critical responses, optimal rankings, Media, Twitter, natural disasters, Hurricanes, generic social media platforms, organizational behavior, Timely Information, information seeking activity, Q&amp;A, Situational Awareness, Training, prompt responders, social networking (online), Real-time systems, Internet, critical commodity, Time factors, learning (artificial intelligence)]
Nonparametric Poisson Factorization Machine
2015 IEEE International Conference on Data Mining
None
2015
Factorization Machine (FM) provides a generic framework that combines the prediction quality of factorization models with the flexibility of feature engineering that discriminative models like SVM offer. The Bayesian Factorization Machine [11], with its impressive predictive performance and the convenience of automatic tuning of parameters, has been one of the most successful and efficient approaches within this framework. However, this model has two major drawbacks. Firstly, it assumes that the data is generated from Gaussian distributions that may not be the best assumption for count data such as integer-valued ratings. Secondly, to get the best performance, one needs to cross-validate over the number of latent factors used for modeling the pairwise interaction in FM, a process that is computationally intensive. This paper introduces the Nonparametric Poisson Factorization Machine (NPFM), which models count data using the Poisson distribution, which provides both modeling and computational advantages for sparse data. The ideal number of latent factors is estimated from the data itself, thereby addressing a key limitation of existing approaches to FM. Additionally, NPFM has linear time complexity with respect to the number of non-zero observations.
[matrix factorization, Frequency modulation, factorization prediction quality, FM, Predictive models, parameter tuning, matrix decomposition, factorization machine, Sparse matrices, Poisson distribution, NPFM, Standards, nonparametric Poisson factorization machine, recommender systems, feature engineering, Data models, Bayes methods, gamma process, learning (artificial intelligence), tensor factorization, Bayesian factorization machine, linear time complexity, Recommender systems]
From 0.5 Million to 2.5 Million: Efficiently Scaling up Real-Time Bidding
2015 IEEE International Conference on Data Mining
None
2015
Real-Time Bidding allows an advertiser to purchase media inventory through an auction system that unfolds in the order of milliseconds. Media providers are increasingly being integrated into such programmatic buying platforms. It is typical for a contemporary Real-Time Bidding system to receive millions of bid requests per second at peak time, and have a large portion of these to be irrelevant to any advertiser. Meanwhile, given a valuable bid request, tens of thousands of advertisements might be qualified for scoring. We present our efforts in building selection models for both bid requests and advertisements to handle this scalability challenge. Our bid request model treats the system load as a hierarchical resource allocation problem and directs traffic based on the estimated quality of bid requests. Next, our exploration/exploitation advertisement model selects a limited number of qualified advertisements for thorough scoring based on the expected value of a bid request to the advertiser given its features. Our combined bid request and advertisement model is able to win more auctions and bring more value to clients by stabilizing the bidding pipeline. We empirically show that our deployed system is capable of handling 5x more bid requests.
[advertiser, tendering, advertising data processing, auction system, modeling, programmatic buying platforms, hierarchical resource allocation problem, Media, Servers, Data mining, scalability, bid request model, advertisements, resource allocation, real-time bidding, contemporary real-time bidding system, Digital signal processing, Real-time systems, Online advertising, Resource management, media inventory, Load modeling, electronic commerce]
Catching the Head, Tail, and Everything in Between: A Streaming Algorithm for the Degree Distribution
2015 IEEE International Conference on Data Mining
None
2015
The degree distribution is one of the most fundamental graph properties of interest for real-world graphs. It has been widely observed in numerous domains that graphs typically have a tailed or scale-free degree distribution. While the average degree is usually quite small, the variance is quite high and there are vertices with degrees at all scales. We focus on the problem of approximating the degree distribution of a large streaming graph, with small storage. We design an algorithm headtail, whose main novelty is a new estimator of infrequent degrees using truncated geometric random variables. We give a mathematical analysis of headtail and show that it has excellent behavior in practice. We can process streams will millions of edges with storage less than 1% and get extremely accurate approximations for all scales in the degree distribution. We also introduce a new notion of Relative Hausdorff distance between tailed histograms. Existing notions of distances between distributions are not suitable, since they ignore infrequent degrees in the tail. The Relative Hausdorff distance measures deviations at all scales, and is a more suitable distance for comparing degree distributions. By tracking this new measure, we are able to give strong empirical evidence of the convergence of headtail.
[Algorithm design and analysis, large streaming graph, graph theory, truncated geometric random variables, Frequency estimation, mathematical analysis, Approximation methods, streaming algorithm, Standards, graph sampling, Degree distribution, Histograms, Streaming algorithms, Mathematical analysis, graph properties, algorithm theory, Approximation algorithms, relative Hausdorff distance measures deviations, degree distribution, headtail, small-space algorithms]
Geo-Social Clustering of Places from Check-in Data
2015 IEEE International Conference on Data Mining
None
2015
In this paper, we develop an algorithm to cluster places not only based on their locations but also their semantics. Specifically, two places are considered similar if they are spatially close and visited by people of similar communities. With the explosion in the availability of location-tracking technologies, it has become easy to track locations and movements of users through user "check-ins". These check-ins provide insights into the community structure of people visiting a place, which is leveraged and integrated into the proposed geo-social clustering framework called GeoScop. While community detection is typically done on social networks, in our problem, we lack any network data. Rather, two people belong to the same community if they visit similar geo-social clusters. We tackle this chicken-and-egg problem through an iterative procedure of expectation maximization and DBSCAN. Extensive experiments on real check-in data demonstrate that GeoScop mines semantically meaningful clusters that cannot be found by using any of the existing clustering techniques. Furthermore, GeoScop is up to 6 times more pure in social quality than the state-of-the-art technique. The executables for the tool are available at http://www.cse.iitm.ac.in/ ~simsayan/software.html.
[iterative methods, dbscan, Pipelines, data mining, social network, GeoScop framework, Semantics, Clustering algorithms, check-ins, Libraries, community detection, Business, chicken-and-egg problem, clustering techniques, Social network services, social clustering framework, location-tracking technologies, social networks, DBSCAN, Standards, pattern clustering, expectation-maximisation algorithm, social networking (online), places geo-social clustering, iterative procedure, expectation maximization, check-in data, spatio-temporal]
Hierarchies in Directed Networks
2015 IEEE International Conference on Data Mining
None
2015
Interactions in many real-world phenomena can be explained by a stronghierarchical structure. Typically, this structure or ranking is not known, instead we only have observed outcomes of the interactions, and the goal is toinfer the hierarchy from these observations. Discovering a hierarchy in the context of directed networks can be formulated asfollows: given a graph, partition vertices into levels such that, ideally, there are only edges from upper levels to lower levels. The ideal case can onlyhappen if the graph is acyclic. Consequently, in practice we have to introducea penalty function that penalizes edges violating the hierarchy. A practicalvariant for such penalty is agony, where each violating edge is penalized basedon the severity of the violation. Hierarchy minimizing agony can be discoveredin O(m^2) time, and much faster in practice. In this paper we introduce severalextensions to agony. We extend the definition for weighted graphs and allow acardinality constraint that limits the number of levels. While, these areconceptually trivial extensions, current algorithms cannot handle them, northey can be easily extended. We provide an exact algorithm of O(m^2 log n) time by showing the connection of agony to the capacitated circulation problem. Wealso show that this bound is in fact pessimistic and we can compute agony forlarge datasets. In addition, we show that we can compute agony in polynomialtime for any convex penalty, and, to complete the picture, we show that minimizinghierarchy with any concave penalty is an NP-hard problem.
[Context, Hierarchy discovery, Conferences, graph theory, hierarchical structure, Transforms, partition vertices, Partitioning algorithms, hierarchical systems, Data mining, directed networks, Optimization, optimisation, NP-hard problem, agony, capacitated circulation, acyclic graph]
A Generative Spatial Clustering Model for Random Data through Spanning Trees
2015 IEEE International Conference on Data Mining
None
2015
When performing analysis of spatial data, there is often the need to aggregate geographical areas into larger regions, a process called regionalization or spatially constrained clustering. These algorithms assume that the items to be clustered are non-stochastic, an assumption not held in many applications. In this work, we present a new probabilistic regionalization algorithm that allows spatially varying random variables as features. Hence, an area highly different from its neighbors can still be considered a member of their cluster if it has a large variance. Our proposal is based on a Bayesian generative spatial product partition model. We build an effective Markov Chain Monte Carlo algorithm to carry out a random walk on the space of all trees and their induced spatial partitions by edges' deletion. We evaluate our algorithm using synthetic data and with one problem of municipalities regionalization based on cancer incidence rates. We are able to better accommodate the natural variation of the data and to diminish the effect of outliers, producing better results than state-of-art approaches.
[Stochastic processes, Probability distribution, spanning trees, spatial graphical model, random walk, Monte Carlo methods, Markov chain Monte Carlo algorithm, probabilistic regionalization algorithm, Clustering algorithms, geography, synthetic data, Space exploration, cancer incidence rates, municipalities regionalization, spatially constrained clustering, data analysis, generative spatial clustering model, trees (mathematics), Bayesian generative spatial product partition model, random processes, random data, Partitioning algorithms, edge deletion, Bayesian spatial model, spatial data analysis, geographical areas, pattern clustering, Markov processes, Data models, cancer, Bayes methods, Yttrium]
Having a Blast: Meta-Learning and Heterogeneous Ensembles for Data Streams
2015 IEEE International Conference on Data Mining
None
2015
Ensembles of classifiers are among the best performing classifiers available in many data mining applications. However, most ensembles developed specifically for the dynamic data stream setting rely on only one type of base-level classifier, most often Hoeffding Trees. In this paper, we study the use of heterogeneous ensembles, comprised of fundamentally different model types. Heterogeneous ensembles have proven successful in the classical batch data setting, however they do not easily transfer to the data stream setting. We therefore introduce the Online Performance Estimation framework, which can be used in data stream ensembles to weight the votes of (heterogeneous) ensemble members differently across the stream. Experiments over a wide range of data streams show performance that is competitive with state of the art ensemble techniques, including Online Bagging and Leveraging Bagging. All experimental results from this work are easily reproducible and publicly available on OpenML for further analysis.
[Hoeffding trees, Data Streams, classical batch data setting, Stacking, ensemble members, data mining, Predictive models, heterogeneous ensembles, Data mining, Meta-Learning, Ensembles, Training, meta-learning, learning (artificial intelligence), data mining applications, pattern classification, OpenML, dynamic data stream setting, data stream ensembles, Estimation, trees (mathematics), online performance estimation framework, base-level classifiers, online bagging, Data models, leveraging bagging, Bagging]
Efficient Approximate Solutions to Mutual Information Based Global Feature Selection
2015 IEEE International Conference on Data Mining
None
2015
Mutual Information (MI) is often used for feature selection when developing classifier models. Estimating the MI for a subset of features is often intractable. We demonstrate, that under the assumptions of conditional independence, MI between a subset of features can be expressed as the Conditional Mutual Information (CMI) between pairs of features. But selecting features with the highest CMI turns out to be a hard combinatorial problem. In this work, we have applied two unique global methods, Truncated Power Method (TPower) and Low Rank Bilinear Approximation (LowRank), to solve the feature selection problem. These algorithms provide very good approximations to the NP-hard CMI based feature selection problem. We experimentally demonstrate the effectiveness of these procedures across multiple datasets and compare them with existing MI based global and iterative feature selection procedures.
[approximation theory, conditional mutual information based global feature selection, combinatorial mathematics, TPower, Computational modeling, Estimation, classifier models, Entropy, Approximation methods, truncated power method, hard combinatorial problem, LowRank, Random variables, NP-hard CMI based feature selection problem, Yttrium, low rank bilinear approximation, Mutual information, feature selection, computational complexity]
KnowSim: A Document Similarity Measure on Structured Heterogeneous Information Networks
2015 IEEE International Conference on Data Mining
None
2015
As a fundamental task, document similarity measure has broad impact to document-based classification, clustering and ranking. Traditional approaches represent documents as bag-of-words and compute document similarities using measures like cosine, Jaccard, and dice. However, entity phrases rather than single words in documents can be critical for evaluating document relatedness. Moreover, types of entities and links between entities/words are also informative. We propose a method to represent a document as a typed heterogeneous information network (HIN), where the entities and relations are annotated with types. Multiple documents can be linked by the words and entities in the HIN. Consequently, we convert the document similarity problem to a graph distance problem. Intuitively, there could be multiple paths between a pair of documents. We propose to use the meta-path defined in HIN to compute distance between documents. Instead of burdening user to define meaningful meta paths, an automatic method is proposed to rank the meta-paths. Given the meta-paths associated with ranking scores, an HIN-based similarity measure, KnowSim, is proposed to compute document similarities. Using Freebase, a well-known world knowledge base, to conduct semantic parsing and construct HIN for documents, our experiments on 20Newsgroups and RCV1 datasets show that KnowSim generates impressive high-quality document clustering.
[document-based ranking, knowledge base, heterogeneous information network, structured heterogeneous information networks, Document similarity, Approximation methods, semantic parsing, meta-paths, 20Newsgroups dataset, deductive databases, document relatedness evaluation, document represent, Semantics, RCV1 dataset, KnowSim, world knowledge base, HIN-based similarity measure, document handling, document similarity measure, pattern classification, Laplace equations, structured text similarity, Knowledge based systems, document-based classification, graph distance problem, ranking scores, knowledge graph, Freebase, pattern clustering, Organizations, Approximation algorithms, Feature extraction, document-based clustering]
A Hierarchical Pattern Learning Framework for Forecasting Extreme Weather Events
2015 IEEE International Conference on Data Mining
None
2015
Extreme weather events, like extreme rainfalls, are severe weather hazards and also the triggers for other natural disasters like floods and tornadoes. Accurate forecasting of such events relies on the understanding of the spatiotemporal evolution processes in climate system. Learning from climate science data has been a challenging task, because the variations among spatial, temporal and multivariate spaces have created a huge amount of features and complex regularities within the data. In this study we developed a framework for learning patterns from the spatiotemporal system and forecasting extreme weather events. In this framework, we learned patterns in a hierarchical manner: in each level, new features were learned from data and used as the input for the next level. Firstly, we summarized the temporal evolution process of individual variables by learning the location-based patterns. Secondly, we developed an optimization algorithm for summarizing the spatial regularities, SCOT, by growing spatial clusters from the location-based patterns. Finally, we developed an instance-based algorithm, SPC, to forecast the extreme events through classification. We applied this framework to forecasting extreme rainfall events in the eastern Central Andes area. Our experiments show that this method was able to find climatic process patterns similar to those found in domain studies, and our forecasting results outperformed the state-of-art model.
[severe weather hazard, floods, spatiotemporal system, spatial regularity summarization, extreme weather event forecasting, SPC, Predictive models, spatiotemporal evolution process, location-based pattern learning, optimization algorithm, Spatiotemporal, Data mining, extreme rainfall event forecasting, Optimization, weather forecasting, climate system, SCOT, natural disaster, feature extraction, instance-based algorithm, Clustering algorithms, climatic process pattern, learning (artificial intelligence), eastern Central Andes area, Meteorology, pattern classification, rain, data analysis, tornadoes, geophysics computing, classification, Forecasting, pattern clustering, climate science data learning, atmospheric techniques, spatial clusters, climatology, data feature learning, Spatiotemporal phenomena, hierarchical pattern learning framework, Hierarchical Learning]
GS-Orthogonalization Based "Basis Feature" Selection from Word Co-occurrence Matrix
2015 IEEE International Conference on Data Mining
None
2015
Feature selection plays an important role in machinelearning applications. Especially for text data, the highdimensionaland sparse characteristics will affect the performanceof feature selction. In this paper, an unsupervised feature selection algorithm through Random Projection and Gram-Schmidt Orthogonalization (RP-GSO) from the word co-occurrence matrix is proposed. The RP-GSO has three advantages: (1) it takes as input dense word co-occurrence matrix, avoiding the sparseness of original document-term matrix, (2) it selects "basis features" by Gram-Schmidt process, guaranteeing the orthogonalization of feature space, and (3) it adopts random projection to speed upGS process. We did extensive experiments on two real-world textcorpora, and observed that RP-GSO achieves better performancecomparing against supervised and unsupervised methods in textclassification and clustering tasks.
[text analysis, basis feature selection, text clustering, text classification, Sparse matrices, MATLAB, basis feature, GS-Orthogonalization, Training, machine learning applications, Clustering algorithms, feature selection, pattern classification, word cooccurrence matrix, Gram-Schmidt orthogonalization, random processes, GS-orthogonalization, unsupervised feature selection algorithm, Matrix decomposition, matrix algebra, unsupervised learning, Computer science, feature space orthogonalization, pattern clustering, Feature extraction, text corpora, random projection, RP-GSO]
Sequential Model-Free Hyperparameter Tuning
2015 IEEE International Conference on Data Mining
None
2015
Hyperparameter tuning is often done manually but current research has proven that automatic tuning yields effective hyperparameter configurations even faster and does not require any expertise. To further improve the search, recent publications propose transferring knowledge from previous experiments to new experiments. We adapt the sequential model-based optimization by replacing its surrogate model and acquisition function with one policy that is optimized for the task of hyperparameter tuning. This policy generalizes over previous experiments but neither uses a model nor uses meta-features, nevertheless, outperforms the state of the art. We show that a static ranking of hyperparameter combinations yields competitive results and substantially outperforms a random hyperparameter search. Thus, it is a fast and easy alternative to complex hyperparameter tuning strategies and allows practitioners to tune their hyperparameters by simply using a look-up table. We made look-up tables for two classifiers publicly available: SVM and AdaBoost. Furthermore, we propose a similarity measure for data sets that yields more comprehensible results than those using meta-features. We show how this similarity measure can be applied to surrogate models in the SMBO framework and empirically show that this change leads to better hyperparameter configurations in less trials.
[Adaptation models, Machine learning algorithms, complex hyperparameter tuning strategies, sequential model-based optimization, surrogate models, Loss measurement, SVM, meta-features, hyperparameter optimization, Optimization, hyperparameter configurations, meta-learning, classifiers, data sets, sequential model-free hyperparameter tuning, pattern classification, acquisition function, transfer learning, support vector machines, SMBO framework, look-up tables, Tuning, AdaBoost, random hyperparameter search, Data models, automatic tuning, static ranking, hyperparameter combinations]
Spammers Detection from Product Reviews: A Hybrid Model
2015 IEEE International Conference on Data Mining
None
2015
Driven by profits, spam reviews for product promotion or suppression become increasingly rampant in online shopping platforms. This paper focuses on detecting hidden spam users based on product reviews. In the literature, there have been tremendous studies suggesting diversified methods for spammer detection, but whether these methods can be combined effectively for higher performance remains unclear. Along this line, a hybrid PU-learning-based Spammer Detection (hPSD) model is proposed in this paper. On one hand, hPSD can detect multi-type spammers by injecting or recognizing only a small portion of positive samples, which meets particularly real-world application scenarios. More importantly, hPSD can leverage both user features and user relations to build a spammer classifier via a semi-supervised hybrid learning framework. Experimental results on movie data sets with shilling injection show that hPSD outperforms several state-of-the-art baseline methods. In particular, hPSD shows great potential in detecting hidden spammers as well as their underlying employers from a real-life Amazon data set. These demonstrate the effectiveness and practical value of hPSD for real-life applications.
[semi-supervised hybrid learning framework, hybrid PU-learning-based spammer detection model, unsolicited e-mail, hPSD, shilling injection, spam reviews, online shopping platforms, real-life Amazon data set, Motion pictures, Labeling, multitype spammers, spammers detection, retail data processing, Economics, profits, hidden spammer detection, product suppression, Couplings, product reviews, hybrid model, security of data, movie data sets, Feature extraction, Data models, Internet, Reliability, product promotion]
A Data Driven Approach to Uncover Deficiencies in Online Reputation Systems
2015 IEEE International Conference on Data Mining
None
2015
Online reputation systems serve as core building blocks in various Internet services such as E-commerce (e.g. eBay) and crowdsourcing (e.g., oDesk). The flaws of real-world online reputation systems were reported extensively. Users who are frustrated about the system will eventually abandon such service. However, no formal studies have explored such flaws. This paper presents the first attempt, which develops a novel data analytical framework to uncover online reputation system deficiencies from data. We develop a novel measure to quantify the efficiency of online reputation systems, i.e., ramp up time of a new service provider. We first show that inherent preferences or personal biases in assigning feedbacks (or ratings) cause the computational infeasibility in evaluating online reputation systems from data. We develop a computationally efficient randomized algorithm with theoretical performance guarantees to address this computational challenge. We apply our methodology to real-life datasets (from eBay and Google Helpouts), we discover that the ramp up time in eBay and Google Helpouts are around 791 and 1,327 days respectively. Around 78.7% sellers have ramped up in eBay and only 1.5% workers have ramped up in Google Helpouts. This small fraction and the long ramp up time (1,327 days) explain why Google Helpouts was eventually shut down in April 2015.
[Crowdsourcing, Internet services, Google, data driven approach, data analysis, service provider, Companies, Google Helpouts, Online reputation, Deficiencies, Algorithms, online reputation systems, Web and internet services, data analytical framework, Delays, Internet, eBay]
Towards Collusive Fraud Detection in Online Reviews
2015 IEEE International Conference on Data Mining
None
2015
Online review fraud has evolved in sophistication by launching intelligent campaigns where a group of coordinated participants work together to deliver deceptive reviews for the designated targets. Such collusive fraud is considered much harder to defend against as these campaign participants are capable of evading detection by shaping their behaviors collectively so as not to appear suspicious. The present work complements existing studies by exploring more subtle behavioral trails connected with collusive review fraud. A novel statistical model is proposed to further characterize, recognize, and forecast collusive fraud in online reviews. The proposed model is completely unsupervised, which bypasses the difficulty of manual annotation required for supervised modeling. It is also highly flexible to incorporate collusion characteristics available for better modeling and prediction. Experiments on two real-world datasets demonstrate the effectiveness of the proposed method and the improvements in learning and predictive abilities.
[Computers, Computational modeling, Predictive models, Probabilistic logic, statistical model, Synchronization, Collusive Review Fraud, Opinion Spam, security of data, reviews, online reviews, fraud, collusive fraud detection, Data models, statistical analysis, collusive review fraud, Business]
Learning Career Mobility and Human Activity Patterns for Job Change Analysis
2015 IEEE International Conference on Data Mining
None
2015
Discovering the determinants of job change and predicting the individual job change occasion are essential approaches for understanding the professional careers of human. However, with the evolution of labor division and globalization, modern careers become more self-directed and dynamic, which makes job change occasion difficult to predict. Fortunately, the emerging online professional networks and location-based social networks provide a large amount of work experience and daily activity records of individuals around the world, which open a venue for the accurate job change analysis. Discovering the determinants of job change and predicting the individual job change occasion are essential approaches for understanding the professional careers of human. However, with the evolution of labor division and globalization, modern careers become more self-directed and dynamic, which makes job change occasion difficult to predict. Fortunately, the emerging online professional networks and location-based social networks provide a large amount of work experience and daily activity records of individuals around the world, which open a venue for the accurate job change analysis. In this paper, we aggregate the work experiences and check-in records of individuals to model the job change motivations and correlations between professional and daily life. Specifically, we attempt to reveal to what extent the job change occasion can be predicted based on the career mobility and daily activity patterns at the individual level. Following the classical theory of job mobility determinants, we extract and quantify the environmental conditions and personal preference of careers from the perspective of industrial/regional constraints and personal interests/demands. Besides, we investigate the factors of activity patterns which may be correlated with job change as cause and effect results. First, we quantify the consumption diversity, sentiment fluctuation and geographic movement from the check-in records as indicators. Then, we leverage the center-bias level assignment and multi-point snapshot mechanism to capture historical and parallel migration. Finally, experimental results based on a large real-world dataset show that the job change occasions can be accurately predicted with the aggregated factors.
[Industries, Correlation, job change occasion prediction, consumption diversity, job change determinant discovery, job mobility determinants, human activity pattern learning, online professional networks, location-based social networks, job change analysis, dynamic careers, sentiment fluctuation, regional constraints, environmental conditions, globalisation, Economics, Engineering profession, sentiment analysis, globalization, human professional careers, multipoint snapshot mechanism, employment, personal interests, job change motivations, center-bias level assignment, LinkedIn, self-directed careers, professional aspects, check-in records, career mobility learning, Organizations, social networking (online), geographic movement, labor division]
Feature Selection with Integrated Relevance and Redundancy Optimization
2015 IEEE International Conference on Data Mining
None
2015
The task of feature selection is to select a subset of the original features according to certain predefined criterion with the goal to remove irrelevant and redundant features, improve the prediction performance and reduce the computational costs of data mining algorithms. In this paper, we integrate feature relevance and redundancy explicitly in the feature selection criterion. Spectral feature analysis is applied here which can fit into both supervised and unsupervised learning problems. Specifically, we formulate the problem into a combinatorial problem to maximize the relevance and minimize the redundancy of the selected subset of features at the same time. The problem can be relaxed and solved with an efficient extended power method with global convergence guaranteed. Extensive experiments demonstrate the advantages of the proposed technique in terms of improving the prediction performance and reducing redundancy in data.
[Correlation, data redundancy reduction, convergence, data mining algorithms, data mining, combinatorial problem, predefined criterion, spectral feature selection, feature selection subset, set theory, Data mining, prediction performance, Optimization, Convergence, feature relevance, Prediction algorithms, redundancy, unsupervised learning problems, feature selection, Laplace equations, Redundancy, global convergence, eigen-optimization, relevance, unsupervised learning, supervised learning problems, supervised/unsupervised learning, feature redundancy, feature selection criterion, spectral feature analysis, feature selection task, minimisation, extended power method]
Forensic Style Analysis with Survival Trajectories
2015 IEEE International Conference on Data Mining
None
2015
Electronic Health Records (EHRs) consists of patient information such as demographics, medications, laboratory test results, diagnosis codes and procedures. Mining EHRs could lead to improvement in patient healthcare management as EHRs contain detailed information related to disease prognosis for large patient populations. We hypothesize that a patient's condition does not deteriorate at random, the trajectories, sequences in which diseases appear in a patient, are determined by a finite number of underlying disease mechanisms. In this work, we exploit this idea by predicting a patient's risk of mortality in the context of the metabolic syndrome by assessing which of many available trajectories a patient is following and progression along this trajectory. Implementing this idea required innovative enhancements both for the study design and also for the fitting algorithm. We propose a forensic-style study design, which aligns patients on last follow-up and measures time backwards. We modify the time-dependent covariate Cox proportional hazards model to better capture coefficients of covariate that follow a particular temporal sequence, such as trajectories. Knowledge extracted from such analysis can lead to personalized treatments, thereby forming the basis for future trajectory-centered guidelines.
[digital forensics, data mining, disease mechanisms, Hazards, Time measurement, electronic health records, survival trajectories, time-dependent covariate Cox proportional hazard model, EHR mining, fitting algorithm, Diseases, patient mortality risk prediction, metabolic syndrome, patient information, covariate coefficients, disease prognosis, particular temporal sequence, patient healthcare management improvement, knowledge extraction, forensic-style analysis, patient condition, Trajectory, Diabetes, Medical diagnostic imaging]
Semantic-Based Recommendation Across Heterogeneous Domains
2015 IEEE International Conference on Data Mining
None
2015
Cross-domain recommendation has attracted wide research interest which generally aims at improving the recommendation performance by alleviating the cold start problem in collaborative filtering based recommendation or generating a more comprehensive user profiles from multiple domains. In most previous cross-domain recommendation settings, explicit or implicit relationships can be easily established across different domains. However, many real applications belong to a more challenging setting: recommendation across heterogeneous domains without explicit relationships, where neither explicit user-item relations nor overlapping features exist between different domains. In this new setting, we need to (1) enrich the sparse data to characterize users or items and (2) bridge the gap caused by the heterogenous features in different domains. To overcome the first challenge, we proposed an optimized local tag propagation algorithm to generate descriptive tags for user profiling. For the second challenge, we proposed a semantic relatedness metric by mapping the heterogenous features onto their concept space derived from online encyclopedias. We conducted extensive experiments on two real datasets to justify the effectiveness of our solution.
[collaborative filtering, item characterization, heterogeneous feature mapping, semantic matching, Twitter, Electronic mail, cross-domain recommendation, cold start problem, Semantics, concept space, Motion pictures, descriptive tag generation, heterogenous domains, sparse data, online encyclopedias, semantic-based recommendation, Media, semantic relatedness metric, Computer science, Bridges, user profiles, recommender systems, user profiling, collaborative filtering based recommendation, user characterization, optimized local tag propagation algorithm]
A Graph-Based Hybrid Framework for Modeling Complex Heterogeneity
2015 IEEE International Conference on Data Mining
None
2015
Data heterogeneity is an intrinsic property of many high impact applications, such as insider threat detection, traffic prediction, brain image analysis, quality control in manufacturing processes, etc. Furthermore, multiple types of heterogeneity (e.g., task/view/instance heterogeneity) often co-exist in these applications, thus pose new challenges to existing techniques, most of which are tailored for a single or dual types of heterogeneity. To address this problem, in this paper, we propose a novel graph-based hybrid approach to simultaneously model multiple types of heterogeneity in a principled framework. The objective is to maximize the smoothness consistency of the neighboring nodes, bag-instance correlation together with task relatedness on the hybrid graphs, and simultaneously minimize the empirical classification loss. Furthermore, we analyze its performance based on Rademacher complexity, which sheds light on the benefits of jointly modeling multiple types of heterogeneity. To solve the resulting non-convex non-smooth problem, we propose an iterative algorithm named M3 Learning, which combines block coordinate descent and the bundle method for optimization. Experimental results on various data sets show the effectiveness of the proposed algorithm.
[iterative methods, Correlation, TV, graph theory, data mining, brain image analysis, manufacturing processes, bag-instance correlation, Loss measurement, Optimization, Analytical models, multi-view learning, heterogeneous learning, Bipartite graph, learning (artificial intelligence), concave programming, traffic prediction, task relatedness, smoothness consistency maximization, empirical classification loss minimization, quality control, multi-instance learning, iterative algorithm, bundle method, neighboring nodes, block coordinate descent, multi-task learning, resulting nonconvex nonsmooth problem, insider threat detection, Rademacher complexity, M3 learning, data heterogeneity, Quality control, complex heterogeneity modeling, data handling, minimisation, graph-based hybrid framework]
Freedom: Online Activity Recognition via Dictionary-Based Sparse Representation of RFID Sensing Data
2015 IEEE International Conference on Data Mining
None
2015
Understanding and recognizing the activities performed by people is a fundamental research topic for a wide range of important applications such as fall detection of elderly people. In this paper, we present the technical details behind Freedom, a low-cost, unobtrusive system that supports independent livingof the older people. The Freedom system interprets what aperson is doing by leveraging machine learning algorithmsand radio-frequency identification (RFID) technology. To dealwith noisy, streaming, unstable RFID signals, we particularlydevelop a dictionary-based approach that can learn dictionariesfor activities using an unsupervised sparse coding algorithm. Our approach achieves efficient and robust activity recognitionvia a more compact representation of the activities. Extensiveexperiments conducted in a real-life residential environmentdemonstrate that our proposed system offers a good overallperformance (e.g., achieving over 96% accuracy in recognizing23 activities) and has the potential to be further developed tosupport the independent living of elderly people.
[Dictionaries, Correlation, radiofrequency identification, online activity recognition, elderly people independent living, assisted living, RFID sensing data, Activity recognition, Silicon, data structures, unsupervised sparse coding algorithm, learning (artificial intelligence), machine learning algorithms, feature selection, pattern recognition, Legged locomotion, data analysis, RFID, sparse coding, sensing data, dictionary-based sparse representation, geriatrics, dictionary, Senior citizens, radio-frequency identification, Freedom system, Feature extraction, Radiofrequency identification]
A Multi-label Ensemble Method Based on Minimum Ranking Margin Maximization
2015 IEEE International Conference on Data Mining
None
2015
Multi-label classification is a learning task of predicting a set of target labels for a given example. In this paper, we propose an ensemble method for multi-label classification, which is designed to optimize a novel minimum ranking margin objective function. Moreover, a boosting-type strategy is adopted to construct an accurate multi-label ensemble from multiple weak base classifiers. Experiments on different real-world multi-label classification tasks show that better performance can be achieved compared to other well-established methods.
[pattern classification, Correlation, minimum ranking margin maximization, learning task, boosting-type strategy, Boosting, Turning, Linear programming, multi-label classification, multiple-weak-base classifiers, Training, ensemble learning, optimisation, real-world multilabel classification tasks, Prediction algorithms, target label prediction, minimum ranking margin objective function optimization, Yttrium, learning (artificial intelligence), multilabel ensemble method]
Collaborated Online Change-Point Detection in Sparse Time Series for Online Advertising
2015 IEEE International Conference on Data Mining
None
2015
Online advertising delivers promotional marketing messages to consumers through online media. Advertisers often have the desire to optimize their advertising spending and strategies in order to maximize their KPI (Key performance indicator). To build accurate ad performance predictive models, it is crucial to detect the change-points in historical data and therefore apply appropriate strategies to address the data pattern shift. However, with sparse data, which is common in online advertising, online change-point detection often becomes challenging. We propose a novel collaborated online change-point detection method in this paper. Through efficiently leveraging and coordinating with auxiliary time series, it can quickly and accurately identify the change-points in sparse and noisy time series. Simulation studies as well as real data applications have demonstrated its effectiveness in detecting change-point in sparse time series and therefore improving the accuracy of predictive models.
[advertising data processing, Time series analysis, KPI, Predictive models, Media, change-point detection, online change-point detection, time series, Noise measurement, data pattern shift, feature extraction, sparse time series, Data models, promotional marketing message, key performance indicator, Internet, Advertising, online advertising]
MMFE: Multitask Multiview Feature Embedding
2015 IEEE International Conference on Data Mining
None
2015
In data mining and pattern recognition area, the learned objects are often represented by the multiple features from various of views. How to learn an efficient and effective feature embedding for the subsequent learning tasks? In this paper, we address this issue by providing a novel multi-task multiview feature embedding (MMFE) framework. The MMFE algorithm is based on the idea of low-rank approximation, which suggests that the observed multiview feature matrix is approximately represented by the low-dimensional feature embedding multiplied by a projection matrix. In order to fully consider the particular role of each view to the multiview feature embedding, we simultaneously suggest the multitask learning scheme and ensemble manifold regularization into the MMFE algorithm to seek the optimal projection. Since the objection function of MMFE is multi-variable and non-convex, we further provide an iterative optimization procedure to find the available solution. Two real world experiments show that the proposed method outperforms single-task-based as well as state-of-the-art multiview feature embedding methods for the classification problem.
[Algorithm design and analysis, iterative methods, data mining, ensemble manifold regularization, Approximation methods, Optimization, Manifolds, Multitask, optimisation, multitask learning scheme, low-dimensional feature embedding, projection matrix, Classification, multitask multiview feature embedding, MMFE algorithm, pattern recognition, approximation theory, Linear programming, multiview feature matrix, low-rank approximation, Multiview, matrix algebra, iterative optimization procedure, classification problem, Dimension reduction, objection function, Yttrium, Principal component analysis]
Towards Mining Trapezoidal Data Streams
2015 IEEE International Conference on Data Mining
None
2015
We study a new problem of learning from doubly-streaming data where both data volume and feature space increase over time. We refer to the problem as mining trapezoidal data streams. The problem is challenging because both data volume and feature space are increasing, to which existing online learning, online feature selection and streaming feature selection algorithms are inapplicable. We propose a new Sparse Trapezoidal Streaming Data mining algorithm (STSD) and its two variants which combine online learning and online feature selection to enable learning trapezoidal data streams with infinite training instances and features. Specifically, when new training instances carrying new features arrive, the classifier updates the existing features by following the passive-aggressive update rule used in online learning and updates the new features with the structural risk minimization principle. Feature sparsity is also introduced using the projected truncation techniques. Extensive experiments on the demonstrated UCI data sets show the performance of the proposed algorithms.
[Algorithm design and analysis, Machine learning algorithms, truncation technique, Heuristic algorithms, data mining, streaming feature selection algorithm, online feature selection, Electronic mail, doubly-streaming data, Data mining, Training, Feature Selection, structural risk minimization principle, sparse trapezoidal streaming data mining algorithm, STSD, Prediction algorithms, feature sparsity, learning (artificial intelligence), feature selection, UCI data, data feature space, Online Learning, data volume space, Trapezoidal Data Streams, Sparsity, online learning]
A Cure Time Model for Joint Prediction of Outcome and Time-to-Outcome
2015 IEEE International Conference on Data Mining
None
2015
The Cox model has been widely used in time-to-outcome predictions, particularly in studies of medical patients, where prediction of the time of death is desired. In addition, the cure model has been proposed to model times of death for discharged patients. However, neither the Cox model nor the cure model allow explicit cure information and prediction of patient cure times (discharge times). In this paper we propose a new model, the "cure time model\
[cure information, Conferences, patient cure time, regression analysis, Predictive models, time-to-outcome prediction, clinical data, Analytical models, discharged patient, dying patient, trauma patient dataset, survival analysis, static data, logistic regression, trauma, medical patient, censored data, Hazards, medical information systems, cure time model, UCSF/San Francisco General Hospital, Hospitals, Cox model, surviving patient, death/cure time, time-to-mortality, prediction, Data models, joint prediction, Logistics]
Part-Level Regularized Semi-Nonnegative Coding for Semi-Supervised Learning
2015 IEEE International Conference on Data Mining
None
2015
Graph-based semi-supervised learning method has been influential in the data mining and machine learning fields. The key is to construct an effective graph to capture the intrinsic data structure, which further benefits for propagating the unlabeled data over the graph. The existing methods have shown the effectiveness of a graph regularization term on measuring the similarities among samples, which further uncovers the data structure. However, all the existing graph-based methods are on the sample-level, i.e. calculate the similarity based on sample-level representation coefficients, inevitably overlooking the underlying part-level structure within sample. Inspired by the strong interpretability of Non-negative Matrix Factorization (NMF) method, we design a more robust and discriminative graph, by integrating low-rank factorization and graph regularizer into a unified framework. Specifically, a novel low-rank factorization through Semi-Non-negative Matrix Factorization (SNMF) is proposed to extract the semantically part-level representation. Moreover, instead of incorporating a graph regularization on sample-level, we propose a sparse graph regularization term built on the decomposed part-level representation. This practice results in a more accurate measurement among samples, generating a more discriminative graph for semi-supervised learning. As a non-trivial contribution, we also provide an optimization solution to the proposed method. Comprehensive experimental evaluations show that our proposed method is able to achieve superior performance compared with the state-of-the-art semi-supervised classification baselines in both transductive and inductive scenarios.
[part-level regularized seminonnegative coding, graph regularizer, semisupervised learning, seminonnegative matrix factorization, graph theory, Linear programming, discriminative graph, Encoding, matrix decomposition, Matrix decomposition, Sparse matrices, Data mining, encoding, low-rank factorization, SNMF, semantic part-level representation extraction, Semisupervised learning, Face, learning (artificial intelligence), sparse graph regularization term]
Domain Induced Dirichlet Mixture of Gaussian Processes: An Application to Predicting Disease Progression in Multiple Sclerosis Patients
2015 IEEE International Conference on Data Mining
None
2015
Predicting disease course is critical in chronic progressive diseases such as multiple sclerosis (MS) for determining treatment. Forming an accurate predictive model based on clinical data is particularly challenging when data is gathered from multiple clinics/physicians as the labels vary with physicians' subjective judgment about clinical tests and further we have no a priori knowledge of the various types of physician subjectivity. At the same time, we often have some (limited) domain knowledge on how to group patients into disease progression subgroups. In this paper, we first present our rationale for choosing a Dirichlet mixture of Gaussian processes (DPMGP) model to address the subjectivity in our data. We then introduce a new approach to incorporating domain knowledge into the non-parametric mixture model. We demonstrate the efficacy of our model by applying it to two medical datasets to predict disease progression in MS patients and disability levels in early Parkinson's patients.
[domain knowledge, disability levels, physician subjectivity, Predictive models, medical datasets, DPMGP model, clinical data, Baysian non-parametric, patient treatment, Parkinson patients, Clustering algorithms, MS patients, predictive medicine, Mathematical model, disease progression subgroups, semi-supervised learning and application, predictive model, disease course, diseases, chronic progressive diseases, multiple sclerosis patients, Diseases, clinical tests, nonparametric mixture model, Gaussian process, Gaussian processes, Data models, mixture models, medical computing, domain induced dirichlet mixture]
Rare Category Detection on Time-Evolving Graphs
2015 IEEE International Conference on Data Mining
None
2015
Rare category detection(RCD) is an important topicin data mining, focusing on identifying the initial examples fromrare classes in imbalanced data sets. This problem becomes more challenging when the data is presented as time-evolving graphs, as used in synthetic ID detection and insider threat detection. Most existing techniques for RCD are designed for static data sets, thus not suitable for time-evolving RCD applications. To address this challenge, in this paper, we first proposetwo incremental RCD algorithms, SIRD and BIRD. They arebuilt upon existing density-based techniques for RCD, andincrementally update the detection models, which provide 'timeflexible' RCD. Furthermore, based on BIRD, we propose amodified version named BIRD-LI to deal with the cases wherethe exact priors of the minority classes are not available. Wealso identify a critical task in RCD named query distribution. Itaims to allocate the limited budget into multiple time steps, suchthat the initial examples from the rare classes are detected asearly as possible with the minimum labeling cost. The proposedincremental RCD algorithms and various query distributionstrategies are evaluated empirically on both synthetic and real data.
[incremental RCD algorithms, BIRD-LI, Heuristic algorithms, minimum labeling cost, graph theory, data mining, Data mining, Rare Category Detection, query processing, empirical evaluation, imbalanced data sets, rare classes, synthetic data, time-evolving graphs, minority classes, query distribution, synthetic ID detection, Labeling, Mathematical model, learning (artificial intelligence), Distribution strategy, static data sets, critical task, time-flexible RCD, real data, Birds, density-based techniques, Time-evolving Graph Mining, Matrix decomposition, incremental detection model update, rare category detection, SIRD, insider threat detection, Incremental Learning]
Representation Learning via Semi-Supervised Autoencoder for Multi-task Learning
2015 IEEE International Conference on Data Mining
None
2015
Multi-task learning aims at learning multiple related but different tasks. In general, there are two ways for multi-task learning. One is to exploit the small set of labeled data from all tasks to learn a shared feature space for knowledge sharing. In this way, the focus is on the labeled training samples while the large amount of unlabeled data is not sufficiently considered. Another way has a focus on how to share model parameters among multiple tasks based on the original features space. Here, the question is whether it is possible to combine the advantages of both approaches and develop a method, which can simultaneously learn a shared subspace for multiple tasks and learn the prediction models in this subspace? To this end, in this paper, we propose a feature representation learning framework, which has the ability in combining the autoencoders, an effective way to learn good representation by using large amount of unlabeled data, and model parameter regularization methods into a unified model for multi-task learning. Specifically, all the tasks share the same encoding and decoding weights to find their latent feature representations, based on which a regularized multi-task softmax regression method is used to find a distinct prediction model for each task. Also, some commonalities are considered in the prediction models according to the relatedness of multiple tasks. There are several advantages of the proposed model: 1) it can make full use of large amount of unlabeled data from all the tasks to learn satisfying representations, 2) the learning of distinct prediction models can benefit from the success of autoencoder, 3) since we incorporate the labeled information into the softmax regression method, so the learning of feature representation is indeed in a semi-supervised manner. Therefore, our model is a semi-supervised autoencoder for multi-task learning (SAML for short). Finally, extensive experiments on three real-world data sets demonstrate the effectiveness of the proposed framework. Moreover, the feature representation obtained in this model can be used by other methods to obtain improved results.
[feature representation learning framework, knowledge sharing, regression analysis, Predictive models, Encoding, model parameter regularization methods, Representation Learning, Decoding, SAML, multitask learning, Multi-task learning, Training, regularized multitask softmax regression method, Semi-supervised Learning, semisupervised autoencoder, knowledge representation, Autoencoder, Organizations, Data models, learning (artificial intelligence), Logistics]
[Publisher's information]
2015 IEEE International Conference on Data Mining
None
2015
Provides a listing of current committee members and society officers.
[]
Message from the Conference General Chairs
2016 IEEE 16th International Conference on Data Mining
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Program Chairs
2016 IEEE 16th International Conference on Data Mining
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2016 IEEE 16th International Conference on Data Mining
None
2016
Provides a listing of current committee members and society officers.
[]
Program Committee
2016 IEEE 16th International Conference on Data Mining
None
2016
Provides a listing of current committee members and society officers.
[]
Keynotes
2016 IEEE 16th International Conference on Data Mining
None
2016
Provides an abstract for each of the keynote presentations and may include a brief professional biography of each
[]
Auditing Black-Box Models for Indirect Influence
2016 IEEE 16th International Conference on Data Mining
None
2016
Data-trained predictive models see widespread use, but for the most part they are used as black boxes which output a prediction or score. It is therefore hard to acquire a deeper understanding of model behavior, and in particular how different features influence the model prediction. This is important when interpreting the behavior of complex models, or asserting that certain problematic attributes (like race or gender) are not unduly influencing decisions. In this paper, we present a technique for auditing black-box models, which lets us study the extent to which existing models take advantage of particular features in the dataset, without knowing how the models work. Our work focuses on the problem of indirect influence: how some features might indirectly influence outcomes via other, related features. As a result, we can find attribute influences even in cases where, upon further direct examination of the model, the attribute is not referred to by the model at all. Our approach does not require the black-box model to be retrained. This is important if (for example) the model is only accessible via an API, and contrasts our work with other methods that investigate feature influence like feature selection. We present experimental evidence for the effectiveness of our procedure using a variety of publicly available datasets and models. We also validate our procedure using techniques from interpretable learning and feature selection, as well as against other black-box auditing procedures.
[Context, fairness, Computational modeling, data mining, Predictive models, attribute influences, auditing, black-box, Standards, Computer science, black-box models, predictive models, Data models, black-box auditing, dataset, indirect influence, Testing]
Asynchronous Multi-task Learning
2016 IEEE 16th International Conference on Data Mining
None
2016
Many real-world machine learning applications involve several learning tasks which are inter-related. For example, in healthcare domain, we need to learn a predictive model of a certain disease for many hospitals. The models for each hospital may be different because of the inherent differences in the distributions of the patient populations. However, the models are also closely related because of the nature of the learning tasks modeling the same disease. By simultaneously learning all the tasks, multi-task learning (MTL) paradigm performs inductive knowledge transfer among tasks to improve the generalization performance. When datasets for the learning tasks are stored at different locations, it may not always be feasible to transfer the data to provide a data centralized computing environment due to various practical issues such as high data volume and privacy. In this paper, we propose a principled MTL framework for distributed and asynchronous optimization to address the aforementioned challenges. In our framework, gradient update does not wait for collecting the gradient information from all the tasks. Therefore, the proposed method is very efficient when the communication delay is too high for some task nodes. We show that many regularized MTL formulations can benefit from this framework, including the low-rank MTL for shared subspace learning. Empirical studies on both synthetic and real-world datasets demonstrate the efficiency and effectiveness of the proposed framework.
[inductive knowledge transfer, generalization performance, distributed optimization, MTL paradigm, data centralized computing environment, Predictive models, asynchronous optimization, healthcare domain, Servers, machine learning, Statistics, communication delay, Optimization, multi-task learning, optimisation, shared subspace learning, Distributed optimization, Hospitals, asynchronous multitask learning, Sociology, Data models, low-rank MTL, learning (artificial intelligence), distributed multi-task learning]
Unsupervised Exceptional Attributed Sub-Graph Mining in Urban Data
2016 IEEE 16th International Conference on Data Mining
None
2016
Geo-located social media provide a wealth of information that describes urban areas based on user descriptions and comments. Such data makes possible to identify meaningful city neighborhoods on the basis of the footprints left by a large and diverse population that uses this type of media. In this paper, we present some methods to exhibit the predominant activities and their associated urban areas to automatically describe a whole city. Based on a suitable attributed graph model, our approach identifies neighborhoods with homogeneous and exceptional characteristics. We introduce the novel problem of exceptional sub-graph mining in attributed graphs and propose a complete algorithm that takes benefits from new upper bounds and pruning properties. We also propose an approach to sample the space of exceptional sub-graphs within a given time-budget. Experiments performed on 10 real datasets are reported and demonstrate the relevancy and the limits of both approaches.
[Algorithm design and analysis, Weight measurement, geo-located social media, Social network services, Urban areas, graph theory, data mining, constraint-based pattern mining, Data mining, non redundancy issues, unsupervised learning, unsupervised exceptional attributed sub-graph mining, Upper bound, Attributed graphs, geography, instant mining, urban data, social networking (online), pattern space sampling, Space exploration, meaningful city neighborhoods]
Causal Inference by Compression
2016 IEEE 16th International Conference on Data Mining
None
2016
Causal inference is one of the fundamental problems in science. In recent years, several methods have been proposed for discovering causal structure from observational data. These methods, however, focus specifically on numeric data, and are not applicable on nominal or binary data. In this work, we focus on causal inference for binary data. Simply put, we propose causal inference by compression. To this end we propose an inference framework based on solid information theoretic foundations, i.e. Kolmogorov complexity. However, Kolmogorov complexity is not computable, and hence we propose a practical and computable instantiation based on the Minimum Description Length (MDL) principle. To apply the framework in practice, we propose ORIGO, an efficient method for inferring the causal direction from binary data. ORIGO employs the lossless PACK compressor, works directly on the data and does not require assumptions about neither distributions nor the type of causal relations. Extensive evaluation on synthetic, benchmark, and real-world data shows that ORIGO discovers meaningful causal relations, and outperforms state-of-the-art methods by a wide margin.
[Drugs, data compression, causal structure, lossless PACK compressor, Complexity theory, inference mechanisms, observational data, information theoretic foundations, binary data compression, ORIGO, mdl, minimum description length principle, Benchmark testing, Inference algorithms, information theory, MDL principle, Decision trees, causal inference, Informatics, Information theory]
On Dense Subgraphs in Signed Network Streams
2016 IEEE 16th International Conference on Data Mining
None
2016
Signed networks remain relatively under explored despite the fact that many real networks are of this kind. Here, we study the problem of subgraph density in signed networks and show connections to the event detection task. Notions of density have been used in prior studies on anomaly detection, but all existing methods have been developed for unsigned networks. We develop the first algorithms for finding dense subgraphs in signed networks using semi-definite programming based rounding. We give rigorous guarantees for our algorithms, and develop a heuristic EGOSCAN which is significantly faster. We evaluate the performance of EGOSCAN for different notions of density, and observe that it performs significantly better than natural adaptations of prior algorithms for unsigned networks. In particular, the improvement in edge density over previous methods is as much as 85% and usually over 50%. These results are consistent across signed and unsigned networks in different domains. The improvement in performance is even more significant for a constrained version of the problem involving finding subgraphs containing a subset of query nodes. We also develop an event detection method for signed and unsigned networks based on subgraph density. We apply this to three different temporal datasets, and show that our method based on EGOSCAN significantly outperforms existing approaches and baseline methods in terms of the precision-recall tradeoff (by as much as 25-50% in some instances).
[query nodes, Event detection, Image edge detection, Social network services, Conferences, graph theory, subgraph density, Programming, signed network streams, anomaly detection, Quadratic programming, heuristic EGOSCAN, computer network security, mathematical programming, event detection task, edge density, graphs, unsigned networks, Approximation algorithms, semidefinite programming based rounding, dense subgraphs, temporal datasets]
Relief of Spatiotemporal Accessibility Overloading with Optimal Resource Placement
2016 IEEE 16th International Conference on Data Mining
None
2016
With the effects of global warming, some epidemic diseases via mosquito (e.g. mosquito-borne diseases) become more serious, such as dengue fever and zika virus. It is reported that the epidemic disease may cause many challenges to the hospital management due to the unexpected burst with uncertain reasons. Furthermore, the imperfect cares during the propagation of epidemic diseases, such as dengue fever (so far the appropriate treatment is not well established), may lead to the increasing mortality rate which should be avoided. In this paper, a novel paradigm for optimizing the placement of medical resource is proposed in pursuit of reducing the overloading cases in hospitals during the epidemic outbreak in the urban area. In this paper we explore the first paper to explore two important issues, including the strategy to evaluate the service quality and the solution to dynamically dispatch the medical resource, along with the spatial variation of epidemic outbreak. As validated in our experimental results in real data of dengue outbreak happening in Tainan (2015), we present the feasibility of our framework to deploy a dynamic placement strategy for medical resource assignment.
[Tainan, hospitals, NP-hard multiple-choice knapsack problem, Urban areas, Data mining, knapsack problems, Diseases, Graphical models, Hospitals, epidemics, resource allocation, dengue outbreak, service quality evaluation, spatiotemporal accessibility overloading, medical resource assignment, Resource management, medical resource placement, epidemic outbreak, health care, Distribution functions, computational complexity, MCKP]
Mining Graphlet Counts in Online Social Networks
2016 IEEE 16th International Conference on Data Mining
None
2016
Counting subgraphs is a fundamental analysis task for online social networks (OSNs). Given the sheer size and restricted access of online social network data, efficient computation of subgraph counts is highly challenging. Although a number of algorithms have been proposed to estimate the relative counts of subgraphs in OSNs with restricted access, there are only few works which try to solve a more general problem, i.e., counting subgraph frequencies. In this paper, we propose an efficient random walk-based framework to estimate the subgraph counts. Our framework generates samples by leveraging consecutive steps of the random walk as well as by observing neighbors of visited nodes. Using the importance sampling technique, we derive unbiased estimators of the subgraph counts. To make better use of the degree information of visited nodes, we also design an improved estimator, which increases the efficiency of the estimate at no additional cost. We conduct extensive experimental evaluation on real-world OSNs to confirm our theoretical claims. The experiment results show that our estimators are unbiased, accurate, efficient and better than the state-of-the-art algorithm. For the Weibo graph with more than 58 million nodes, our method produces estimate of triangle count with an error less than 5% using only 20 thousands sampled nodes. Detailed comparison with the state-of-the-art method demonstrates that our algorithm is 4 to 5 times more accurate.
[Algorithm design and analysis, subgraph count computation, estimation theory, Computational modeling, graph theory, subgraph frequency counting, data mining, Random walk, online social networks, importance sampling technique, Twitter, Graphlet counting, importance sampling, Data mining, OSNs, random walk-based framework, graphlet count mining, Weibo graph, Monte Carlo methods, unbiased estimators, Markov processes, social networking (online), Online social networks]
Differentially Private Regression Diagnostics
2016 IEEE 16th International Conference on Data Mining
None
2016
Linear and logistic regression are popular statistical techniques for analyzing multi-variate data. Typically, analysts do not simply posit a particular form of the regression model, estimate its parameters, and use the results for inference orprediction. Instead, they first use a variety of diagnostic techniques to assess how well the model fits the relationships in the data and how well it can be expected to predict outcomes for out-of-sample records, revising the model as necessary to improve fit and predictive power. In this article, we develop &#x03B5;-differentially private diagnostics for regression, beginning to fill a gap in privacy-preserving data analysis. Specifically, we create differentially private versions of residual plots for linear regression and of receiver operating characteristic (ROC) curves for logistic regression. The former helps determine whether or not the data satisfy the assumptions underlying the linear regression model, and the latter is used to assess the predictive power of the logistic regression model. These diagnostics improve the usefulness of algorithms for computing differentially private regression output, which alone does not allow analysts to assess the quality of the posited model. Our empirical studies show that these algorithms are adequate for diagnosing the fit and predictive power of regression models on representative datasets when the size of the dataset times the privacy parameter (&#x03B5;) is at least 1000.
[Algorithm design and analysis, Data privacy, data analysis, Computational modeling, statistical techniques, sensitivity analysis, receiver operating characteristic curves, regression analysis, Predictive models, privacy parameter, differentially private regression diagnostics, &#x03B5;-differentially private diagnostics, privacy-preserving data analysis, ROC curves, Privacy, Analytical models, linear regression model, multivariate data analysis, Data models, data privacy, logistic regression]
Vote-and-Comment: Modeling the Coevolution of User Interactions in Social Voting Web Sites
2016 IEEE 16th International Conference on Data Mining
None
2016
In social voting Web sites, how do the user actions - up-votes, down-votes and comments - evolve over time? Are there relationships between votes and comments? What is normal and what is suspicious? These are the questions we focus on. We analyzed over 20,000 submissions corresponding to more than 100 million user interactions from three social voting Web sites: Reddit, Imgur and Digg. Our first contribution is two discoveries: (i) the number of comments grows as a power-law on the number of votes and (ii) the time between a submission creation and a user's reaction obeys a log-logistic distribution. Based on these patterns, we propose VnC (Vote-and-Comment), a parsimonious but accurate and scalable model that models the coevolution of user activities. In our experiments on real data, VnC outperformed state-of-the-art baselines on accuracy. Additionally, we illustrate VnC usefulness for forecasting and outlier detection.
[user interaction coevolution, Social network services, Computational modeling, Blogs, user activities coevolution, Forecasting, outlier detection, vote-and-comment model, social networking (online), Data models, Trajectory, social voting Web sites, VnC model, Web sites]
On Efficient External-Memory Triangle Listing
2016 IEEE 16th International Conference on Data Mining
None
2016
Discovering triangles in large graphs is a well-studied area, however, both external-memory performance of existing methods and our understanding of the complexity involved leave much room for improvement. To shed light on this problem, we first generalize the existing in-memory algorithms into a single framework of 18 triangle-search techniques. We then develop a novel external-memory approach, which we call Pruned Companion Files (PCF), that supports operation of all 18 algorithms, while significantly reducing I/O compared to the common methods in this area. After finding the best node-traversal order, we build an implementation around it using SIMD instructions for list intersection and PCF for I/O. This method runs 5-10 times faster than the best available implementation and exhibits orders of magnitude less I/O. In one of our graphs, the program finds 1 trillion triangles in 237 seconds using a desktop CPU.
[SIMD instructions, algorithms, triangle-search techniques, external-memory triangle listing, Redundancy, Taxonomy, graph theory, mathematics computing, in-memory algorithms, pruned companion files, Random access memory, large graphs, Complexity theory, Partitioning algorithms, Data mining, external-memory performance, storage management, triangle listing, node-traversal order, PCF, I/O reduction, Nickel, search problems, computational complexity]
Efficient Distributed SGD with Variance Reduction
2016 IEEE 16th International Conference on Data Mining
None
2016
Stochastic Gradient Descent (SGD) has become one of the most popular optimization methods for training machine learning models on massive datasets. However, SGD suffers from two main drawbacks: (i) The noisy gradient updates have high variance, which slows down convergence as the iterates approach the optimum, and (ii) SGD scales poorly in distributed settings, typically experiencing rapidly decreasing marginal benefits as the number of workers increases. In this paper, we propose a highly parallel method, CentralVR, that uses error corrections to reduce the variance of SGD gradient updates, and scales linearly with the number of worker nodes. CentralVR enjoys low iteration complexity, provably linear convergence rates, and exhibits linear performance gains up to hundreds of cores for massive datasets. We compare CentralVR to state-of-the-art parallel stochastic optimization methods on a variety of models and datasets, and find that our proposed methods exhibit stronger scaling than other SGD variants.
[CentralVR, Stochastic processes, stochastic programming, machine learning model training, Servers, massive datasets, parallel processing, Convergence, variance reduction, noisy gradient updates, learning (artificial intelligence), gradient methods, error correction, distributed optimization, Computational modeling, parallel method, distributed SGD, optimization methods, Indexes, stochastic gradient descent, stochastic gradient descent (SGD), linear convergence rates, low iteration complexity, error corrections, SGD gradient update variance, Approximation algorithms, Error correction, parallel stochastic optimization methods, computational complexity]
Triply Stochastic Variational Inference for Non-linear Beta Process Factor Analysis
2016 IEEE 16th International Conference on Data Mining
None
2016
We propose a non-linear extension to factor analysis with beta process priors for improved data representation ability. This non-linear Beta Process Factor Analysis (nBPFA) allows data to be represented as a non-linear transformation of a standard sparse factor decomposition. We develop a scalable variational inference framework, which builds upon the ideas of the variational auto-encoder, by allowing latent variables of the model to be sparse. Our framework can be readily used for real-valued, binary and count data. We show theoretically and with experiments that our training scheme, with additive or multiplicative noise on observations, improves performance and prevents overfitting. We benchmark our algorithms on image, text and collaborative filtering datasets. We demonstrate faster convergence rates and competitive performance compared to standard gradient-based approaches.
[variational auto-encoder, data representation, nonlinear transformation, Computational modeling, Stochastic processes, triply stochastic variational inference, Gaussian distribution, Decoding, collaborative filtering datasets, multiplicative noise, Standards, Convergence, Stochastic Variational Inference, Analytical models, standard sparse factor decomposition, Non-linear Factor Analysis, additive noise, data structures, nonlinear beta process factor analysis, nBPFA, convergence rates, stochastic processes, nonlinear extension, scalable variational inference framework]
Beyond Points and Paths: Counting Private Bodies
2016 IEEE 16th International Conference on Data Mining
None
2016
Mining of spatial data is an enabling technology for mobile services, Internet-connected cars, and the Internet of Things. But the very distinctiveness of spatial data that drives utility, comes at the cost of user privacy. In this work, we continue the tradition of privacy-preserving spatial analytics, focusing not on point or path data, but on planar spatial regions. Such data represents the area of a user's most frequent visitation-such as "around home and nearby shops". Specifically we consider the differentially-private release of data structures that support range queries for counting users' spatial regions. Counting planar regions leads to unique challenges not faced in existing work. A user's spatial region that straddles multiple data structure cells can lead to duplicate counting at query time. We provably avoid this pitfall by leveraging the Euler characteristic. To address the increased sensitivity of range queries to spatial region data, we calibrate privacy-preserving noise using bounded user region size and a constrained inference that uses robust least absolute deviations. Our novel constrained inference reduces noise and introduces covertness by (privately) imposing consistency. We provide a full end-to-end theoretical analysis of both differential privacy and high-probability utility for our approach using concentration bounds. A comprehensive experimental study on several real-world datasets establishes practical validity.
[point data, Data privacy, planar spatial regions, data mining, high-probability utility, privacy-preserving noise, mobile services, multiple data structure cells, Histograms, Privacy, constrained inference, spatial regions counting, Internet-connected cars, Euler characteristic, data structures, Trajectory, Face, private bodies, bounded user region size, Internet-of-things, query time, range queries, Data structures, privacy-preserving spatial analytics, Spatial databases, inference mechanisms, planar regions counting, security of data, user privacy, data privacy, path data, spatial data mining, differential privacy]
Efficient Rectangular Maximal-Volume Algorithm for Rating Elicitation in Collaborative Filtering
2016 IEEE 16th International Conference on Data Mining
None
2016
Cold start problem in Collaborative Filtering can be solved by asking new users to rate a small seed set of representative items or by asking representative users to rate a new item. The question is how to build a seed set that can give enough preference information for making good recommendations. One of the most successful approaches, called Representative Based Matrix Factorization, is based on Maxvol algorithm. Unfortunately, this approach has one important limitation - a seed set of a particular size requires a rating matrix factorization of fixed rank that should coincide with that size. This is not necessarily optimal in the general case. In the current paper, we introduce a fast algorithm for an analytical generalization of this approach that we call Rectangular Maxvol. It allows the rank of factorization to be lower than the required size of the seed set. Moreover, the paper includes the theoretical analysis of the method's error, the complexity analysis of the existing methods and the comparison to the state-of-the-art approaches.
[Algorithm design and analysis, rectangular maximal-volume algorithm, collaborative filtering, Filtering, rectangular Maxvol algorithm, complexity analysis, matrix decomposition, rating elicitation, Matrix decomposition, Optimization, cold start problem, seed set, rating matrix factorization, Collaboration, Prediction algorithms, Mathematical model, representative based matrix factorization]
New Robust Clustering Model for Identifying Cancer Genome Landscapes
2016 IEEE 16th International Conference on Data Mining
None
2016
In recent decades, the availability of comprehensive genomic data has facilitated the insight of molecular portraits of cancer. Specifically, by conducting cancer clustering, cancer samples can be divided into several groups according to their differences and similarities in molecular characteristics. Traditional cancer clustering usually analyzes cancer samples from a single tissue, but such analysis cannot reveal the connections among different types of cancer. Landscape analysis across human cancers can help discover molecular signatures shared across cancer tissues, providing an opportunity to design new gene therapy tailored for different cancer patients. However, the noise level in genomic data is high. The robust clustering method is crucial to tackle this problem. In this paper, we propose a new robust clustering method to approach the landscape analysis for TCGA cancer data from a novel view, which is to eliminate the noise and then perform clustering on the cleaned data rather than weaken the effect of noise as existing noise-resistant norm methods. Extensive experiments on both genomic datasets and clustering benchmark datasets confirm the effectiveness and correctness of our proposed method.
[Cancer Genome Landscapes, Clustering methods, TCGA cancer data, Denoised Clustering Model, Genomics, Matrix decomposition, Robust Clustering, landscape analysis, The Cancer Genome Atlas, pattern clustering, clustering model, bioinformatics, genomics, Robustness, cancer, Bioinformatics, cancer genome landscape identification, Cancer, Tumors]
Event Series Prediction via Non-Homogeneous Poisson Process Modelling
2016 IEEE 16th International Conference on Data Mining
None
2016
Data streams whose events occur at random arrival times rather than at the regular, tick-tock intervals of traditional time series are increasingly prevalent. Event series are continuous, irregular and often highly sparse, differing greatly in nature to the regularly sampled time series traditionally the concern of hard sciences. As mass sets of such data have become more common, so interest in predicting future events in them has grown. Yet repurposing of traditional forecasting approaches has proven ineffective, in part due to issues such as sparsity, but often due to inapplicable underpinning assumptions such as stationarity and ergodicity. In this paper we derive a principled new approach to forecasting event series that avoids such assumptions, based upon: 1. The processing of event series datasets in order to produce a first parameterized mixture model of non-homogeneous Poisson processes, and 2. Application of a technique called parallel forecasting that uses these processes' rate functions to directly generate accurate temporal predictions for new query realizations. This approach uses forerunners of a stochastic process to shed light on the distribution of future events, not for themselves, but for realizations that subsequently follow in their footsteps.
[nonhomogeneous Poisson processes, parallel forecasting, data streams, Predictive models, Windows, parallel processing, event series prediction, query processing, Expectation Maximization, event series dataset processing, Mixture models, parameterized mixture model, temporal predictions, stochastic processes, NHPP, Time series analysis, time series, stochastic process, Forecasting, event series forecasting, query realizations, Event Series Prediction, forecasting theory, Markov processes, Parallel Forecasting, data handling, mixture models, Poisson Processes]
Model Accuracy and Runtime Tradeoff in Distributed Deep Learning: A Systematic Study
2016 IEEE 16th International Conference on Data Mining
None
2016
Deep learning with a large number of parameters requires distributed training, where model accuracy and runtime are two important factors to be considered. However, there has been no systematic study of the tradeoff between these two factors during the model training process. This paper presents Rudra, a parameter server based distributed computing framework tuned for training large-scale deep neural networks. Using variants of the asynchronous stochastic gradient descent algorithm we study the impact of synchronization protocol, stale gradient updates, mini batch size, learning rates, and number of learners on run time performance and model accuracy. We introduce a new learning rate modulation strategy to counter the effect of stale gradients and propose a new synchronization protocol that can effectively bound the staleness in gradients, improve runtime performance and achieve good model accuracy. Our empirical investigation reveals a principled approach for distributed training of neural networks: the mini-batch size per learner should be reduced as more learners are added to the system to preserve the model accuracy. We validate this approach using commonly-used image classification benchmarks: CIFAR10 and ImageNet.
[synchronization protocol, distributed neural network training, Protocols, image classification, mini-batch size per learner, model accuracy, stale gradients, Servers, Training, stale gradient updates, asynchronous stochastic gradient descent algorithm, Mathematical model, learning (artificial intelligence), gradient methods, mini batch size, runtime tradeoff, Computational modeling, distributed deep learning, distributed training, Rudra, Synchronization, learning rates, learning rate modulation strategy, Neural networks, large-scale deep neural networks, commonly-used image classification benchmarks, parameter server based distributed computing framework, model training process, CIFAR10, neural nets, ImageNet]
Waddling Random Walk: Fast and Accurate Mining of Motif Statistics in Large Graphs
2016 IEEE 16th International Conference on Data Mining
None
2016
Algorithms for mining very large graphs, such as those representing online social networks, to discover the relative frequency of small subgraphs within them are of high interest to sociologists, computer scientists and marketeers alike. However, the computation of these network motif statistics via naive enumeration is infeasible for either its prohibitive computational costs or access restrictions on the full graph data. Methods to estimate the motif statistics based on random walks by sampling only a small fraction of the subgraphs in the large graph address both of these challenges. In this paper, we present a new algorithm, called the Waddling Random Walk (WRW), which estimates the concentration of motifs of any size. It derives its name from the fact that it sways a little to the left and to the right, thus also sampling nodes not directly on the path of the random walk. The WRW algorithm achieves its computational efficiency by not trying to enumerate subgraphs around the random walk but instead using a randomized protocol to sample subgraphs in the neighborhood of the nodes visited by the walk. In addition, WRW achieves significantly higher accuracy (measured by the closeness of its estimate to the correct value) and higher precision (measured by the low variance in its estimations) than the current state-of-the-art algorithms for mining subgraph statistics. We illustrate these advantages in speed, accuracy and precision using simulations on well-known and widely used graph datasets representing real networks.
[Computers, Protocols, sampling methods, motif statistics, Social network services, graph theory, Estimation, data mining, random processes, Probability, computational efficiency, graph datasets, Data mining, graph sampling, graph mining, very large graph mining, random walk, subgraph, subgraph sampling, Microstructure, randomized protocol, WRW, waddling random walk, network motif]
Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation
2016 IEEE 16th International Conference on Data Mining
None
2016
Predicting personalized sequential behavior is a key task for recommender systems. In order to predict user actions such as the next product to purchase, movie to watch, or place to visit, it is essential to take into account both long-term user preferences and sequential patterns (i.e., short-term dynamics). Matrix Factorization and Markov Chain methods have emerged as two separate but powerful paradigms for modeling the two respectively. Combining these ideas has led to unified methods that accommodate long-and short-term dynamics simultaneously by modeling pairwise user-item and item-item interactions. In spite of the success of such methods for tackling dense data, they are challenged by sparsity issues, which are prevalent in real-world datasets. In recent years, similarity-based methods have been proposed for (sequentially-unaware) item recommendation with promising results on sparse datasets. In this paper, we propose to fuse such methods with Markov Chains to make personalized sequential recommendations. We evaluate our method, Fossil, on a variety of large, real-world datasets. We show quantitatively that Fossil outperforms alternative algorithms, especially on sparse datasets, and qualitatively that it captures personalized dynamics and is able to make meaningful recommendations.
[long-term user preferences, Portable computers, Markov Chains, Heuristic algorithms, Predictive models, matrix decomposition, Sparse matrices, Sequential Prediction, sequentially-unaware item recommendation, personalized sequential behavior prediction, item-item interaction modeling, Motion pictures, Prediction algorithms, pairwise user-item modelling, Recommender systems, matrix factorization, similarity model fusion, Fossil method, sequential patterns, Markov chain methods, sparse sequential recommendation, recommender systems, sparse datasets, Markov processes, personalized sequential recommendations]
Adaptive Neighborhood Propagation by Joint L2,1-Norm Regularized Sparse Coding for Representation and Classification
2016 IEEE 16th International Conference on Data Mining
None
2016
We propose a new transductive label propagation method, termed Adaptive Neighborhood Propagation (Adaptive-NP) by joint L2,1-norm regularized sparse coding, for semi-supervised classification. To make the predicted soft labels more accurate for predicting the labels of samples and to avoid the tricky process of choosing the optimal neighborhood size or kernel width for graph construction, Adaptive-NP seamlessly integrates sparse coding and neighborhood propagation into a unified framework. That is, the sparse reconstruction error and classification error are combined for joint minimization, which clearly differs from traditional methods that explicitly separate graph construction and label propagation into independent steps, which may result in inaccurate predictions. Note that our Adaptive-NP alternately optimize the sparse codes and soft labels matrices, where the sparse codes are used as adaptive weights for neighborhood propagation at each iteration, so the tricky process of determining neighborhood size or kernel width is avoided. Besides, for enhancing sparse coding, we use the L2,1-norm constraint on the sparse coding coefficients and the reconstruction error at the same time for delivering more accurate and robust representations. Extensive simulations show that our model can deliver state-of-the-art performances on several public datasets for classification.
[sparse codes, kernel width, Sparse matrices, semisupervised classification, regularized sparse coding, Manifolds, 1-norm regularized sparse coding, linear neighborhood propagation, Robustness, Kernel, transductive label propagation method, classification error, Weight measurement, pattern classification, sparse coding coefficients, joint minimization, L2, adaptive weights, Transductive learning, Minimization, Encoding, soft labels matrices, neighborhood size, classification, robust representations, matrix algebra, sparse reconstruction error, adaptive neighborhood propagation, Adaptive-NP, public datasets, graph construction]
From Sets of Good Redescriptions to Good Sets of Redescriptions
2016 IEEE 16th International Conference on Data Mining
None
2016
Redescription mining aims at finding pairs of queries over data variables that describe roughly the same set of observations. These redescriptions can be used to obtain different views on the same set of entities. So far, redescription mining methods have aimed at listing all redescriptions supported by the data. Such an approach can result in many redundant redescriptions and hinder the user's ability to understand the overall characteristics of the data. In this work, we present an approach to find a good set of redescriptions, instead of finding a set of good redescriptions. That is, we present a way to remove the redundant redescriptions from a given set of redescriptions. We measure the redundancy using a framework inspired by the subjective interestingness based on maximum-entropy distributions as proposed by De Bie in 2011. Redescriptions, however, raise their unique requirements on the framework, and our solution differs significantly from the existing ones. Notably, our approach can handle disjunctions and conjunctions in the queries, whereas the existing approaches are limited only to conjunctive queries. The framework also reduces the redundancy in the redescription mining results, as we show in our empirical evaluation.
[Pattern Set Mining, Computational modeling, Biological system modeling, data mining, Redescription Mining, Entropy, maximum-entropy distributions, Data mining, Optimization, query processing, redescription mining methods, Maximum Entropy Principle, data variables, conjunctive query, maximum entropy methods, Data models, redundancy, good redescription set, redundant redescription removal, Informatics]
Edge Weight Prediction in Weighted Signed Networks
2016 IEEE 16th International Conference on Data Mining
None
2016
Weighted signed networks (WSNs) are networks in which edges are labeled with positive and negative weights. WSNs can capture like/dislike, trust/distrust, and other social relationships between people. In this paper, we consider the problem of predicting the weights of edges in such networks. We propose two novel measures of node behavior: the goodness of a node intuitively captures how much this node is liked/trusted by other nodes, while the fairness of a node captures how fair the node is in rating other nodes' likeability or trust level. We provide axioms that these two notions need to satisfy and show that past work does not meet these requirements for WSNs. We provide a mutually recursive definition of these two concepts and prove that they converge to a unique solution in linear time. We use the two measures to predict the edge weight in WSNs. Furthermore, we show that when compared against several individual algorithms from both the signed and unsigned social network literature, our fairness and goodness metrics almost always have the best predictive power. We then use these as features in different multiple regression models and show that we can predict edge weights on 2 Bitcoin WSNs, an Epinions WSN, 2 WSNs derived from Wikipedia, and a WSN derived from Twitter with more accurate results than past work. Moreover, fairness and goodness metrics form the most significant feature for prediction in most (but not all) cases.
[Weight measurement, Correlation, social network literature, Online banking, Social network services, regression analysis, Wikipedia, negative edge, multiple regression models, edge weight prediction, Wireless sensor networks, WSN, signed network, prediction, Prediction algorithms, social networking (online), edge weight, weighted signed networks]
Transfer Learning for Survival Analysis via Efficient L2,1-Norm Regularized Cox Regression
2016 IEEE 16th International Conference on Data Mining
None
2016
In survival analysis, the primary goal is to monitor several entities and model the occurrence of a particular event of interest. In such applications, it is quite often the case that the event of interest may not always be observed during the study period and this gives rise to the problem of censoring which cannot be easily handled in the standard regression approaches. In addition, obtaining sufficient labeled training instances for learning a robust prediction model is a very time consuming process and can be extremely difficult in practice. In this paper, we propose a transfer learning based Cox method, called Transfer-Cox, which uses auxiliary data to augment learning when there are insufficient amount of training examples. The proposed method aims to extract "useful" knowledge from the source domain and transfer it to the target domain, thus potentially improving the prediction performance in such time-to-event data. The proposed method uses the l<sub>2,1</sub>-norm penalty to encourage multiple predictors to share similar sparsity patterns, thus learns a shared representation across source and target domains, potentially improving the model performance on the target task. To speedup the computation, we apply the screening approach and extend the strong rule to sparse survival analysis models in multiple high-dimensional censored datasets. We demonstrate the performance of the proposed transfer learning method using several synthetic and high-dimensional microarray gene expression benchmark datasets and compare with other related competing state-of-the-art methods. Our results show that the proposed screening approach significantly improves the computational efficiency of the proposed algorithm without compromising the prediction performance. We also demonstrate the scalability of the proposed approach and show that the time taken to obtain the results is linear with respect to both the number of instances and features.
[Transfer learning, regularization, high-dimensional data, Predictive models, computational efficiency, patient monitoring, high-dimensional microarray gene expression benchmark datasets, Learning systems, Analytical models, multiple high-dimensional censored datasets, genetics, sparsity patterns, biology computing, auxiliary data, survival analysis, robust prediction model, transfer learning method, Prediction algorithms, sparse survival analysis models, labeled training instances, learning (artificial intelligence), Transfer-Cox, Hazards, transfer learning based Cox method, Standards, regression, time-to-event data, Data models, data handling, regularized Cox regression, target task, standard regression]
Interactive Multi-task Relationship Learning
2016 IEEE 16th International Conference on Data Mining
None
2016
Multi-task learning (MTL) is a learning paradigm that provides a principled way to improve the generalization performance of a set of related machine learning tasks by transferring knowledge among the tasks. The past decade has witnessed many successful applications of MTL in different domains. In the center of MTL algorithms is how the relatedness of tasks are modeled and encoded in learning formulations to facilitate knowledge transfer. Among the MTL algorithms, the multi-task relationship learning (MTRL) attracted much attention in the community because it learns task relationship from data to guide knowledge transfer, instead of imposing a prior task relatedness assumption. However, this method heavily depends on the quality of training data. When there is insufficient training data or the data is too noisy, the algorithm could learn an inaccurate task relationship that misleads the learning towards suboptimal models. To address the aforementioned challenge, in this paper we propose a novel interactive multi-task relationship learning (iMTRL) framework that efficiently solicits partial order knowledge of task relationship from human experts, effectively incorporates the knowledge in a proposed knowledge-aware MTRL formulation. We propose an efficient optimization algorithm for kMTRL and comprehensively study query strategies that identify the critical pairs that are most influential to the learning. We present extensive empirical studies on both synthetic and real datasets to demonstrate the effectiveness of proposed framework.
[iMTRL, knowledge-aware MTRL formulation, Machine learning algorithms, kMTRL, interactive multitask relationship learning, Predictive models, optimization algorithm, query strategies, Covariance matrices, Knowledge transfer, Diseases, Training, optimisation, task relationship, interactive systems, Data models, learning (artificial intelligence), partial order knowledge]
Augmented LSTM Framework to Construct Medical Self-Diagnosis Android
2016 IEEE 16th International Conference on Data Mining
None
2016
Given a health-related question (such as "I have a bad stomach ache. What should I do?"), a medical self-diagnosis Android inquires further information from the user, diagnoses the disease, and ultimately recommend best solutions. One practical challenge to build such an Android is to ask correct questions and obtain most relevant information, in order to correctly pinpoint the most likely causes of health conditions. In this paper, we tackle this challenge, named "relevant symptom question generation": Given a limited set of patient described symptoms in the initial question (e.g., "stomach ache"), what are the most critical symptoms to further ask the patient, in order to correctly diagnose their potential problems? We propose an augmented long short-term memory (LSTM) framework, where the network architecture can naturally incorporate the inputs from embedding vectors of patient described symptoms and an initial disease hypothesis given by a predictive model. Then the proposed framework generates the most important symptom questions. The generation process essentially models the conditional probability to observe a new and undisclosed symptom, given a set of symptoms from a patient as well as an initial disease hypothesis. Experimental results show that the proposed model obtains improvements over alternative methods by over 30% (both precision and mean ordinal distance).
[Correlation, relevant symptom question generation, Humanoid robots, Predictive models, initial disease hypothesis, Electronic mail, Deep learning, Android (operating system), health conditions, augmented long short-term memory, health-related question, LSTM, Medical self-diagnosis Android, probability, diseases, network architecture, medical self-diagnosis Android, patient described symptoms, Diseases, disease hypothesis, Relevant symptom generation, conditional probability, disease diagnosis, Androids, medical computing, Medical diagnostic imaging, patient diagnosis, augmented LSTM, mean ordinal distance]
The Optimal Distribution of Electric-Vehicle Chargers across a City
2016 IEEE 16th International Conference on Data Mining
None
2016
It has been estimated that the cumulative sales of Electric Vehicles (EVs) will be up to 5.9 million and the stock of EVs will be up to 20 million by 2020 [1]. As the number of EVs is expanding, there is a growing need for widely distributed, publicly accessible, EV charging facilities. The public EV Chargers (EVCs) are expected to be found and will be needed where there is on-street parking, at taxi stands, in parking lots at places of employment, hotels, airports, shopping centres, convenience shops, fast food restaurants, and coffee houses, etc. In this work, we aim to optimize the distribution of public EVCs across the city such that (i) the overall revenue generated by the EVCs is maximized, subject to (ii) the overall driver discomfort (e.g., queueing time) for EV charging is minimized. This is the first study on EVC distribution where EVCs are assumed to be installed in almost all regions across a city. The problem is formulated using a bilevel optimization model. We propose an alternating framework to solve it and have proved that a local minima is achievable. Moreover, this work introduces novel methods to extract information to understand the discomfort of petroleum car drivers, EV charging demands, parking time and parking fees across the city. The source data explored include the trajectories of taxis, the distribution of petroleum stations and various local features. The empirical study uses the real data sets from Shenzhen City, one of the largest cities in China. The extensive tests verify the superiority of the proposed bilevel optimization model in all aspects.
[electric vehicles, Charging stations, Data Mining, bilevel optimization, Optimization, driver discomfort, electric vehicle chargers, sales management, optimisation, electric vehicle charging, optimal distribution, driver information systems, China, on-street parking, Charging Station, Urban areas, Shenzhen City, taxi stands, Electrical Vehicle, automobile industry, petroleum car drivers, Automobiles, Petroleum, Public transportation, cumulative sales, EVC, Bilevel Optimization]
Streaming Model Selection via Online Factorized Asymptotic Bayesian Inference
2016 IEEE 16th International Conference on Data Mining
None
2016
Recent growing needs for real time data analytics have increased importance of streaming model selection. Real-world streaming observations are often obtained by dynamically-changing or heterogeneous data sources, and learning machines must identify the complexities of the data generation processes on the fly without prior knowledge. This paper proposes online FAB (OFAB) inference as a general framework for streaming model selection of latent variable models. The key idea in OFAB inference is degeneration, i.e. it intentionally considers a "redundant" latent space anddynamically derives a "non-redundant" latent sub-space using a FAB-unique shrinkage mechanism on demand. By integrating the idea of stochastic variational inference, OFAB automatically and dynamically selects the best dimensionality of latent variables in a streaming and Bayesian principled manner. Empirical results on two applications, density estimation and abnormal detection, show that online FAB (OFAB) outperformed the state-of-the-art online inference methods.
[heterogeneous data sources, Stochastic processes, Complexity theory, data generation process, Analytical models, OFAB inference, online factorized asymptotic Bayesian inference, variational techniques, streaming model selection, latent variable models, learning (artificial intelligence), Bayesian inference, density estimation, abnormal detection, data analysis, stochastic variational inference, FAB-unique shrinkage mechanism, real-world streaming observations, inference mechanisms, machine learning, Online learning, redundant latent space, nonredundant latent subspace, Hidden Markov models, Data models, Inference algorithms, Bayes methods, real time data analytics, Stream learning]
Robust Multi-View Feature Selection
2016 IEEE 16th International Conference on Data Mining
None
2016
High-throughput technologies have enabled us to rapidly accumulate a wealth of diverse data types. These multi-view data contain much more information to uncover the cluster structure than single-view data, which draws raising attention in data mining and machine learning areas. On one hand, many features are extracted to provide enough information for better representations, on the other hand, such abundant features might result in noisy, redundant and irrelevant information, which harms the performance of the learning algorithms. In this paper, we focus on a new topic, multi-view unsupervised feature selection, which aims to discover the discriminative features in each view for better explanation and representation. Although there are some exploratory studies along this direction, most of them employ the traditional feature selection by putting the features in different views together and fail to evaluate the performance in the multi-view setting. The features selected in this way are difficult to explain due to the meaning of different views, which disobeys the goal of feature selection as well. In light of this, we intend to give a correct understanding of multi-view feature selection. Different from the existing work, which either incorrectly concatenates the features from different views, or takes huge time complexity to learn the pseudo labels, we propose a novel algorithm, Robust Multi-view Feature Selection (RMFS), which applies robust multi-view K-means to obtain the robust and high quality pseudo labels for sparse feature selection in an efficient way. Nontrivially we give the solution by taking the derivatives and further provide a K-means-like optimization to update several variables in a unified framework with the convergence guarantee. We demonstrate extensive experiments on three real-world multi-view data sets, which illustrate the effectiveness and efficiency of RMFS in terms of both single-view and multi-view evaluations by a large margin.
[Algorithm design and analysis, data mining, high-throughput technologies, Data mining, Optimization, Feature Selection, optimisation, feature extraction, Clustering algorithms, Robustness, learning (artificial intelligence), K-means-like optimization, feature selection, Multi-view Learning, learning algorithms, performance evaluation, robust multiview feature selection, multiview unsupervised feature selection, machine learning, multiview data, RMFS, Robust Clustering, single-view evaluation, pattern clustering, cluster structure, Feature extraction, Periodic structures, discriminative feature discovery, multiview evaluation]
KNN Classifier with Self Adjusting Memory for Heterogeneous Concept Drift
2016 IEEE 16th International Conference on Data Mining
None
2016
Data Mining in non-stationary data streams is gaining more attention recently, especially in the context of Internet of Things and Big Data. It is a highly challenging task, since the fundamentally different types of possibly occurring drift undermine classical assumptions such as i.i.d. data or stationary distributions. Available algorithms are either struggling with certain forms of drift or require a priori knowledge in terms of a task specific setting. We propose the Self Adjusting Memory (SAM) model for the k Nearest Neighbor (kNN) algorithm since kNN constitutes a proven classifier within the streaming setting. SAM-kNN can deal with heterogeneous concept drift, i.e different drift types and rates, using biologically inspired memory models and their coordination. It can be easily applied in practice since an optimization of the meta parameters is not necessary. The basic idea is to construct dedicated models for the current and former concepts and apply them according to the demands of the given situation. An extensive evaluation on various benchmarks, consisting of artificial streams with known drift characteristics as well as real world datasets is conducted. Thereby, we explicitly add new benchmarks enabling a precise performance evaluation on multiple types of drift. The highly competitive results throughout all experiments underline the robustness of SAM-kNN as well as its capability to handle heterogeneous concept drift.
[Adaptation models, pattern classification, KNN classifier, Biological system modeling, Heuristic algorithms, data mining, Predictive models, nonstationary data streams, Big Data, performance evaluation, kNN, Data mining, Internet of Things, optimisation, concept drift, Benchmark testing, Prediction algorithms, Data streams, self adjusting memory model, k nearest neighbor algorithm, SAM-kNN robustness, metaparameter optimization]
New Probabilistic Multi-graph Decomposition Model to Identify Consistent Human Brain Network Modules
2016 IEEE 16th International Conference on Data Mining
None
2016
Many recent scientific efforts have been devoted to constructing the human connectome using Diffusion Tensor Imaging (DTI) data for understanding large-scale brain networks that underlie higher-level cognition in human. However, suitable network analysis computational tools are still lacking in human brain connectivity research. To address this problem, we propose a novel probabilistic multi-graph decomposition model to identify consistent network modules from the brain connectivity networks of the studied subjects. At first, we propose a new probabilistic graph decomposition model to address the high computational complexity issue in existing stochastic block models. After that, we further extend our new probabilistic graph decomposition model for multiple networks/graphs to identify the shared modules cross multiple brain networks by simultaneously incorporating multiple networks and predicting the hidden block state variables. We also derive an efficient optimization algorithm to solve the proposed objective and estimate the model parameters. We validate our method by analyzing both the weighted fiber connectivity networks constructed from DTI images and the standard human face image clustering benchmark data sets. The promising empirical results demonstrate the superior performance of our proposed method.
[biomedical MRI, graph theory, multiple networks, Stochastic processes, multiple graphs, weighted fiber connectivity networks, optimization algorithm, Optimization, biodiffusion, optimisation, network analysis computational tools, Multi-Graph Decomposition, Human Connectome, parameter estimation, large-scale brain networks, medical image processing, human face image clustering, probabilistic multigraph decomposition model, Computational modeling, Probabilistic Graph Decomposition, Probabilistic logic, human cognition, DTI data, Diffusion tensor imaging, consistent human brain network modules, DTI images, pattern clustering, diffusion tensor imaging data, probabilistic graph decomposition model, Brain modeling, human brain connectivity research, Data models, stochastic block models, human connectome, computational complexity]
Efficient Extraction of Non-negative Latent Factors from High-Dimensional and Sparse Matrices in Industrial Applications
2016 IEEE 16th International Conference on Data Mining
None
2016
High-dimensional and sparse (HiDS) matrices are commonly encountered in many big data-related industrial applications like recommender systems. When acquiring useful patterns from them, non-negative matrix factorization (NMF) models have proven to be highly effective because of their fine representativeness of non-negative data. However, current NMF techniques suffer from a) inefficiency in addressing HiDS matrices, and b) constrained training schemes lack of flexibility, extensibility and adaptability. To address these issues, this work proposes to factorize industrial-size sparse matrices via a novel Inherently Non-negative Latent Factor (INLF) model. It connects the output factors and decision variables via a single-element-dependent sigmoid function, thereby innovatively removing the non-negativity constraints from its training process without impacting the solution accuracy. Hence, its training process is unconstrained, highly flexible and compatible with general learning schemes. Experimental results on five HiDS matrices generated by industrial applications indicate that INLF is able to acquire non-negative latent factors from them in a more efficient manner than any existing method does.
[Conferences, Estimation, nonnegative latent factors extraction, Big Data, matrix decomposition, Data mining, Sparse matrices, Big-Data, unconstrained, HiDS matrices, single-element-dependent sigmoid function, non-negative factorization, missing-data estimation, feature extraction, high-dimensional and sparse matrices, inherently nonnegative latent factor, big data, INLF, inherently non-negative, sparse matrices, Manganese, latent factor]
Robust Graph-Theoretic Clustering Approaches Using Node-Based Resilience Measures
2016 IEEE 16th International Conference on Data Mining
None
2016
This paper examines a schema for graph-theoretic clustering using node-based resilience measures. Node-based resilience measures optimize an objective based on a critical set of nodes whose removal causes some severity of disconnection in the network. Beyond presenting a general framework for the usage of node based resilience measures for variations of clustering problems, we emphasize the unique potential of such methods to accomplish the following properties: (i) clustering a graph in one step without knowing the number of clusters a priori, and (ii) removing noise from noisy data. We first present results of clustering experiments using a &#x03B2;-parametrized generalization of vertex attack tolerance, showing high clustering accuracy for both real datasets and equal density synthetic data sets, as well as successful removal of noise nodes. It is shown that arbitrarily increasing &#x03B2; increases the number of noise nodes removed in some cases, and that internal validation measures can be used to determine the correct number of clusters in a class of datasets. Further results are presented using five different resilience measures with a general node-based resilience clustering technique. In a subset of cases a resilience measure, such as integrity, is able to cluster to high accuracy in one step, giving the correct clustering while also determining the correct number of clusters. Integrity is also shown to be promising with respect to noise removal, removing up to 80% of noise on some datasets.
[Context, graph theory, Scattering, node-based resilience measures, Partitioning algorithms, Noise measurement, Resilience, robust graph-theoretic clustering, &#x03B2;-parametrized generalization, Image color analysis, noisy data, pattern clustering, Clustering algorithms, vertex attack tolerance, noise, node-based resilience clustering, clustering, resilience measures, equal density synthetic data sets, noise removal]
Hyperbolae are No Hyperbole: Modelling Communities That are Not Cliques
2016 IEEE 16th International Conference on Data Mining
None
2016
Cliques are frequently used to model communities: a community is a set of nodes where each pair is equally likely to be connected. But studying real-world communities reveals that they have more structure than that. In particular, the nodes can be ordered in such a way that (almost) all edges in the community lie below a hyperbola. In this paper we present three new models for communities that capture this phenomenon. Our models explain the structure of the communities differently, but we also prove that they are identical in their expressive power. Our models fit to real-world data much better than traditional block models or previously-proposed hyperbolic models, both of which are a special case of our model. Our models also allow for intuitive interpretation of the parameters, enabling us to summarize the shapes of the communities in graphs effectively.
[Shape, cliques, no hyperbole, Computational modeling, graph theory, Probabilistic logic, Data models, Peer-to-peer computing, Informatics, YouTube, hyperbolae]
Learning Hierarchically Decomposable Concepts with Active Over-Labeling
2016 IEEE 16th International Conference on Data Mining
None
2016
Many classification tasks target high-level concepts that can be decomposed into a hierarchy of finer-grained sub-concepts. For example, some string entities that are Locations are also Attractions, some Attractions are Museums, etc. Such hierarchies are common in named entity recognition (NER), document classification, and biological sequence analysis. We present a new approach for learning hierarchically decomposable concepts. The approach learns a high-level classifier (e.g., location vs. non-location) by seperately learning multiple finer-grained classifiers (e.g., museum vs. non-museum), and then combining the results. Soliciting labels at a finer level of granularity than that of the target concept is a new approach to active learning, which we term active over-labeling. In experiments in NER and document classification tasks, we show that active over-labeling substantially improves area under the precision-recall curve when compared with standard passive or active learning. Finally, because finer-grained labels may be more expensive to obtain, we also present a cost-sensitive active learner that uses a multi-armed bandit approach to dynamically choose the label granularity to target, and show that the bandit-based learner is robust to differences in label cost and labeling budget.
[classification tasks, bandit-based learner, named entity recognition, Biology, Electronic mail, text classification, task analysis, active over-labeling, biological sequence analysis, Training, active learning, Picture archiving and communication systems, Labeling, learning (artificial intelligence), document handling, passive learning, learning hierarchically decomposable concepts, Diamond, document classification, classification, Standards, hierarchical labeling, NER, semi-supervised learning, cost analysis]
AWarp: Fast Warping Distance for Sparse Time Series
2016 IEEE 16th International Conference on Data Mining
None
2016
Dynamic Time Warping (DTW) distance has been effectively used in mining time series data in a multitude of domains. However, in its original formulation DTW is extremely inefficient in comparing long sparse time series, containing mostly zeros and some unevenly spaced non-zero observations. Original DTW distance does not take advantage of this sparsity, leading to redundant calculations and a prohibitively large computational cost for long time series. We derive a new time warping similarity measure (AWarp) for sparse time series that works on the run-length encoded representation of sparse time series. The complexity of AWarp is quadratic on the number of observations as opposed to the range of time of the time series. Therefore, AWarp can be several orders of magnitude faster than DTW on sparse time series. AWarp is exact for binary-valued time series and a close approximation of the original DTW distance for any-valued series. We discuss useful variants of AWarp: bounded (both upper and lower), constrained, and multidimensional. We show applications of AWarp to three data mining tasks including clustering, classification, and outlier detection, which are otherwise not feasible using classic DTW, while producing equivalent results. Potential areas of application include bot detection, human activity classification, and unusual review pattern mining.
[DTW distance, run-length encoding, Heuristic algorithms, data mining, dynamic time warping distance, bot detection, Data mining, Sparse matrices, outlier detection, run-length encoded representation, AWarp, binary-valued time series, time warping similarity measure, human activity classification, pattern classification, Time series analysis, time series, Encoding, Time measurement, review pattern mining, pattern clustering, sparse time series, Signal processing algorithms, any-valued series, clustering, time series data mining]
Binary Classifier Calibration Using an Ensemble of Near Isotonic Regression Models
2016 IEEE 16th International Conference on Data Mining
None
2016
Learning accurate probabilistic models from data is crucial in many practical tasks in data mining. In this paper we present a new non-parametric calibration method called Ensemble of Near Isotonic Regression (ENIR). The method can be considered as an extension of BBQ, a recently proposed calibration method, as well as the commonly used calibration method based on isotonic regression (IsoRegC). ENIR is designed to address the key limitation of IsoRegC which is the monotonicity assumption of the predictions. Similar to BBQ, the method post-processes the output of a binary classifier to obtain calibrated probabilities. Thus it can be used with many existing classification models to generate accurate probabilistic predictions. We demonstrate the performance of ENIR on synthetic and real datasets for commonly applied binary classification models. Experimental results show that the method outperforms several common binary classifier calibration methods. In particular on the real data, ENIR commonly performs statistically significantly better than the other methods, and never worse. It is able to improve the calibration power of classifiers, while retaining their discrimination power. The method is also computationally tractable for large scale datasets, as it is O(N log N) time, where N is the number of samples.
[pattern classification, Computational modeling, regression analysis, Predictive models, Probabilistic logic, binary classifier calibration, Calibration, Data mining, ensemble of near isotonic regression, binary classification models, Histograms, ENIR, ensemble models, near isotonic regression, classifier calibration, Data models, calibration, accurate probability]
A Scalable and Generic Framework to Mine Top-k Representative Subgraph Patterns
2016 IEEE 16th International Conference on Data Mining
None
2016
Mining subgraph patterns is an active area of research. Existing research has primarily focused on mining all subgraph patterns in the database. However, due to the exponential subgraph search space, the number of patterns mined, typically, is too large for any human mediated analysis. Consequently, deriving insights from the mined patterns is hard for domain scientists. In addition, subgraph pattern mining is posed in multiple forms: the function that models if a subgraph is a pattern varies based on the application and the database could be over multiple graphs or a single, large graph. In this paper, we ask the following question: Given a subgraph importance function and a budget k, which are the k subgraph patterns that best represent all other patterns of interest? We show that the problem is NP-hard, and propose a generic framework called RESLING that adapts to arbitrary subgraph importance functions and generalizable to both transactional graph databases as well as single, large graphs. Experiments show that RESLING is up to 20 times more representative of the pattern space and 2 orders of magnitude faster than the state-of-the-art techniques. The executables for the tool are available at http://www.cse.iitm.ac.in/~sayan/software.html.
[scalable, Roads, Conferences, RESLING, graph theory, subgraph importance function, data mining, Data mining, database management systems, subgraph pattern mining, network, Proteins, Databases, representative, Social network services, Redundancy, top-k, patterns, mining, exponential subgraph search space, graph, generic, NP-hard problem, subgraph, generic framework, computational complexity]
What You Will Gain By Rounding: Theory and Algorithms for Rounding Rank
2016 IEEE 16th International Conference on Data Mining
None
2016
When factorizing binary matrices, we often have to make a choice between using expensive combinatorial methods that retain the discrete nature of the data and using continuous methods that can be more efficient but destroy the discrete structure. Alternatively, we can first compute a continuous factorization and subsequently apply a rounding procedure to obtain a discrete representation. But what will we gain by rounding? Will this yield lower reconstruction errors? Is it easy to find a low-rank matrix that rounds to a given binary matrix? Does it matter which threshold we use for rounding? Does it matter if we allow for only non-negative factorizations? In this paper, we approach these and further questions by presenting and studying the concept of rounding rank. We show that rounding rank is related to linear classification, dimensionality reduction, and nested matrices. We also report on an extensive experimental study that compares different algorithms for finding good factorizations under the rounding rank model.
[pattern classification, nested matrices, Symmetric matrices, data analysis, rounding rank model, Light rail systems, Probabilistic logic, Complexity theory, Matrix decomposition, Data mining, matrix algebra, data reduction, dimensionality reduction, linear classification, Informatics]
A Fast Iterative Algorithm for Improved Unsupervised Feature Selection
2016 IEEE 16th International Conference on Data Mining
None
2016
Dimensionality reduction is often a crucial step for the successful application of machine learning and data mining methods. One way to achieve said reduction is feature selection. Due to the impossibility of labelling many data sets, unsupervised approaches are frequently the only option. The column subset selection problem translates naturally to this purpose, and has received consider able attention over the last few years, as it provides simple linear models for data reconstruction. Existing methods, however, often achieve approximation errors that are far from the optimum. In this paper we present a novel algorithm for column subset selection that consistently outperforms state-of-the-art methods in approximation error. We present a series of key derivations that allow an efficient implementation, making it comparable in speed and in some cases faster than other algorithms. We also prove results that make it possible to deal with huge matrices, which has strong implications for other algorithms of this type in the big data field. We validate our claimsthrough experiments on a wide variety of well-known data sets.
[Algorithm design and analysis, fast iterative algorithm, iterative methods, column subset selection problem, data mining, data reconstruction, Big Data, Linear programming, set theory, Data mining, Matrix decomposition, Proposals, machine learning, linear models, approximation error, dimensionality reduction, improved unsupervised feature selection, Prototypes, Approximation algorithms, learning (artificial intelligence), big data, feature selection]
Sparse Factorization Machines for Click-through Rate Prediction
2016 IEEE 16th International Conference on Data Mining
None
2016
With the rapid development of E-commerce, recent years have witnessed the booming of online advertising industry, which raises extensive concerns of both academic and business circles. Among all the issues, the task of Click-through rates (CTR) prediction plays a central role, as it may influence the ranking and pricing of online ads. To deal with this task, the Factorization Machines (FM) model is designed for better revealing proper combinations of basic features. However, the sparsity of ads transaction data, i.e., a large proportion of zero elements, may severely disturb the performance of FM models. To address this problem, in this paper, we propose a novel Sparse Factorization Machines (SFM) model, in which the Laplace distribution is introduced instead of traditional Gaussian distribution to model the parameters, as Laplace distribution could better fit the sparse data with higher ratio of zero elements. Along this line, it will be beneficial to select the most important features or conjunctions with the proposed SFM model. Furthermore, we develop a distributed implementation of our SFM model on Spark platform to support the prediction task on mass dataset in practice. Comprehensive experiments on two large-scale real-world datasets clearly validate both the effectiveness and efficiency of our SFM model compared with several state-of-the-art baselines, which also proves our assumption that Laplace distribution could be more suitable to describe the online ads transaction data.
[CTR, Frequency modulation, online advertising industry, advertising data processing, Predictive models, Gaussian distribution, Laplace transforms, matrix decomposition, Laplace distribution, SFM model, click-through rate prediction, sparse factorization machine model, click-through rates prediction, Data models, e-commerce, Bayes methods, Mathematical model, Advertising, electronic commerce]
Unsupervised Feature Selection for Outlier Detection by Modelling Hierarchical Value-Feature Couplings
2016 IEEE 16th International Conference on Data Mining
None
2016
Proper feature selection for unsupervised outlier detection can improve detection performance but is very challenging due to complex feature interactions, the mixture of relevant features with noisy/redundant features in imbalanced data, and the unavailability of class labels. Little work has been done on this challenge. This paper proposes a novel Coupled Unsupervised Feature Selection framework (CUFS for short) to filter out noisy or redundant features for subsequent outlier detection in categorical data. CUFS quantifies the outlierness (or relevance) of features by learning and integrating both the feature value couplings and feature couplings. Such value-to-feature couplings capture intrinsic data characteristics and distinguish relevant features from those noisy/redundant features. CUFS is further instantiated into a parameter-free Dense Subgraph-based Feature Selection method, called DSFS. We prove that DSFS retains a 2-approximation feature subset to the optimal subset. Extensive evaluation results on 15 real-world data sets show that DSFS obtains an average 48% feature reduction rate, and enables three different types of pattern-based outlier detection methods to achieve substantially better AUC improvements and/or perform orders of magnitude faster than on the original feature set. Compared to its feature selection contender, on average, all three DSFS-based detectors achieve more than 20% AUC improvement.
[2-approximation feature subset, feature reduction rate, coupled unsupervised feature selection, Search problems, set theory, Data mining, value-to-feature couplings, Coupling Learning, Detectors, pattern-based outlier detection methods, unsupervised outlier detection, feature selection, CUFS, complex feature interactions, feature selection contender, Noise measurement, intrinsic data characteristics, unsupervised learning, Couplings, DSFS-based detectors, Outlying Feature Selection, Feature extraction, Data models, feature value couplings, AUC improvements, hierarchical value-feature coupling modelling, Non-IID Outlier Detection, parameter-free dense subgraph-based feature selection method]
Partition Aware Connected Component Computation in Distributed Systems
2016 IEEE 16th International Conference on Data Mining
None
2016
How can we find all connected components in an enormous graph with billions of nodes and edges?Finding connected components is a fundamental operation for various graph computation tasks such as pattern recognition, reachability, graph compression, etc. Many algorithms have been proposed for decades, but most of them are not scalable enough to process recent web scale graphs. Recently, a MapReduce algorithm was proposed to handle such large graphs. However, the algorithm repeatedly reads and writes numerous intermediate data that cause network overload and prolong the running time. In this paper, we propose PACC (Partition-Aware Connected Components), a new distributed algorithm based on graph partitioning for load-balancing and edge-filtering. Experimental results show that PACC significantly reduces the intermediate data, and provides up to 10 times faster performance than the current state-of-the-art MapReduce algorithm on real world graphs.
[Algorithm design and analysis, scalable, load balancing, connected component, parallel processing, MapReduce, edge filtering, resource allocation, enormous graph, parallel, Distributed databases, graph compression, distributed systems, reachability, graph partitioning, Distributed algorithms, pattern recognition, algorithm, reachability analysis, Computational modeling, partition aware connected component computation, Phase change random access memory, distributed, Partitioning algorithms, graph, PACC, mapreduce, Memory management]
Iteratively Reweighted Least Squares Algorithms for L1-Norm Principal Component Analysis
2016 IEEE 16th International Conference on Data Mining
None
2016
Principal component analysis (PCA) is often used to reduce the dimension of data by selecting a few orthonormal vectors that explain most of the variance structure of the data. L1 PCA uses the L1 norm to measure error, whereas the conventional PCA uses the L2 norm. For the L1 PCA problem minimizing the fitting error of the reconstructed data, we propose an exact reweighted and an approximate algorithm based on iteratively reweighted least squares. We provide convergence analyses, and compare their performance against benchmark algorithms in the literature. The computational experiment shows that the proposed algorithms consistently perform best.
[Algorithm design and analysis, data dimension reduction, iterative methods, least squares approximations, L1-norm principal component analysis, Iteratively Reweighted Least Squares, orthonormal vectors, L1 Norm, Linear programming, data variance structure, L1 PCA problem, exact reweightedalgorithm, L2 norm, convergence analysis, Loading, approximate algorithm, Benchmark testing, Approximation algorithms, fitting error minimization, Iterative methods, principal component analysis, Principal component analysis, iteratively reweighted least squares algorithms]
Convolutional MKL Based Multimodal Emotion Recognition and Sentiment Analysis
2016 IEEE 16th International Conference on Data Mining
None
2016
Technology has enabled anyone with an Internet connection to easily create and share their ideas, opinions and content with millions of other people around the world. Much of the content being posted and consumed online is multimodal. With billions of phones, tablets and PCs shipping today with built-in cameras and a host of new video-equipped wearables like Google Glass on the horizon, the amount of video on the Internet will only continue to increase. It has become increasingly difficult for researchers to keep up with this deluge of multimodal content, let alone organize or make sense of it. Mining useful knowledge from video is a critical need that will grow exponentially, in pace with the global growth of content. This is particularly important in sentiment analysis, as both service and product reviews are gradually shifting from unimodal to multimodal. We present a novel method to extract features from visual and textual modalities using deep convolutional neural networks. By feeding such features to a multiple kernel learning classifier, we significantly outperform the state of the art of multimodal emotion recognition and sentiment analysis on different datasets.
[convolutional MKL based multimodal emotion recognition, Sentiment analysis, Emotion recognition, Visualization, Multiple kernel learning, Neurons, sentiment analysis, emotion recognition, Biological neural networks, textual modalities, Deep learning, multiple kernel learning classifier, feature extraction, visual modalities, Feature extraction, deep convolutional neural networks, learning (artificial intelligence), Convolutional neural networks, Kernel, Multimodal sentiment analysis, neural nets]
Recommending Packages to Groups
2016 IEEE 16th International Conference on Data Mining
None
2016
The success of recommender systems has made them the focus of a massive research effort in both industry and academia. Recent work has generalized recommendations to suggest packages of items to single users, or single items to groups of users. However, to the best of our knowledge, the interesting problem of recommending a package to a group of users (P2G) has not been studied to date. This is a problem with several practical applications, such as recommending vacation packages to tourist groups, entertainment packages to groups of friends, or sets of courses to groups of students. In this paper, we formulate the P2G problem, and we propose probabilistic models that capture the preference of a group towards a package, incorporating factors such as user impact and package viability. We also investigate the issue of recommendation fairness. This is a novel consideration that arises in our setting, where we require that no user is consistently slighted by the item selection in the package. We present aggregation algorithms for finding the best packages and compare our suggested models with baseline approaches stemming from previous work. The results show that our models find packages of high quality which consider all special requirements of P2G recommendation.
[Algorithm design and analysis, entertainment packages, aggregation algorithms, P2G recommendation, Computational modeling, probability, vacation packages, Probabilistic logic, user impact, probabilistic models, recommender systems, educational courses, Motion pictures, Mathematical model, Recommender systems, package viability, Bars, package recommendation]
Subspace Outlier Detection in Linear Time with Randomized Hashing
2016 IEEE 16th International Conference on Data Mining
None
2016
Outlier detection algorithms are often computationally intensive because of their need to score each point in the data. Even simple distance-based algorithms have quadratic complexity. High-dimensional outlier detection algorithms such as subspace methods are often even more computationally intensive because of their need to explore different subspaces of the data. In this paper, we propose an exceedingly simple subspace outlier detection algorithm, which can be implemented in a few lines of code, and whose complexity is linear in the size of the data set and the space requirement is constant. We show that this outlier detection algorithm is much faster than both conventional and high-dimensional algorithms and also provides more accurate results. The approach uses randomized hashing to score data points and has a neat subspace interpretation. Furthermore, the approach can be easily generalized to data streams. We present experimental results showing the effectiveness of the approach over other state-of-the-art methods.
[data analysis, linear complexity, Complexity theory, Electronic mail, subspace outlier detection algorithm, Training, randomized hashing, Detectors, file organisation, Data models, Robustness, Detection algorithms, computational complexity]
CoreScope: Graph Mining Using k-Core Analysis &#x2014; Patterns, Anomalies and Algorithms
2016 IEEE 16th International Conference on Data Mining
None
2016
How do the k-core structures of real-world graphs look like? What are the common patterns and the anomalies? How can we use them for algorithm design and applications? A k-core is the maximal subgraph where all vertices have degree at least k. This concept has been applied to such diverse areas as hierarchical structure analysis, graph visualization, and graph clustering. Here, we explore pervasive patterns that are related to k-cores and emerging in graphs from several diverse domains. Our discoveries are as follows: (1) Mirror Pattern: coreness of vertices (i.e., maximum k such that each vertex belongs to the k-core) is strongly correlated to their degree. (2) Core-Triangle Pattern: degeneracy of a graph (i.e., maximum k such that the k-core exists in the graph) obeys a 3-to-1 power law with respect to the count of triangles. (3) Structured Core Pattern: degeneracy-cores are not cliques but have non-trivial structures such as core-periphery and communities. Our algorithmic contributions show the usefulness of these patterns. (1) Core-A, which measures the deviation from Mirror Pattern, successfully finds anomalies in real-world graphs complementing densest-subgraph based anomaly detection methods. (2) Core-D, a single-pass streaming algorithm based on Core-Triangle Pattern, accurately estimates the degeneracy of billion-scale graphs up to 7&#x00D7; faster than a recent multipass algorithm.(3) Core-S, inspired by Structured Core Pattern, identifies influential spreaders up to 17&#x00D7; faster than top competitors with comparable accuracy.
[Algorithm design and analysis, mirror pattern, Correlation, graph theory, data mining, Companies, 3-to-1 power law, Twitter, anomaly detection, Electronic mail, structured core pattern, graph degeneracy, multipass algorithm, maximal subgraph, Graphs, Mirrors, single-pass streaming algorithm, degeneracy-cores, degeneracy, hierarchical structure analysis, graph visualization, k-core analysis, graph clustering, graph mining, pervasive patterns, anomaly detection methods, influential nodes, core-triangle pattern, CoreScope, k-cores, algorithm design]
L-EnsNMF: Boosted Local Topic Discovery via Ensemble of Nonnegative Matrix Factorization
2016 IEEE 16th International Conference on Data Mining
None
2016
Nonnegative matrix factorization (NMF) has been widely applied in many domains. In document analysis, it has been increasingly used in topic modeling applications, where a set of underlying topics are revealed by a low-rank factor matrix from NMF. However, it is often the case that the resulting topics give only general topic information in the data, which tends not to convey much information. To tackle this problem, we propose a novel ensemble model of nonnegative matrix factorization for discovering high-quality local topics. Our method leverages the idea of an ensemble model, which has been successful in supervised learning, into an unsupervised topic modeling context. That is, our model successively performs NMF given a residual matrix obtained from previous stages and generates a sequence of topic sets. Our algorithm for updating the input matrix has novelty in two aspects. The first lies in utilizing the residual matrix inspired by a state-of-the-art gradient boosting model, and the second stems from applying a sophisticated local weighting scheme on the given matrix to enhance the locality of topics, which in turn delivers high-quality, focused topics of interest to users. We evaluate our proposed method by comparing it against other topic modeling methods, such as a few variants of NMF and latent Dirichlet allocation, in terms of various evaluation measures representing topic coherence, diversity, coverage, computing time, and so on. We also present qualitative evaluation on the topics discovered by our method using several real-world data sets.
[latent Dirichlet allocation, text analysis, supervised learning, low-rank factor matrix, topic modeling, matrix decomposition, Data mining, local weighting, unsupervised topic modeling, topic coherence, boosted local topic discovery-via-ensemble-of-nonnegative matrix factorization, topic diversity, gradient methods, Context, matrix factorization, topic computing time, qualitative evaluation, topic coverage, Boosting, residual matrix, Matrix decomposition, L-EnsNMF, Standards, unsupervised learning, ensemble learning, gradient boosting, high-quality local topic discovery, Data models, document analysis, local weighting scheme, Context modeling, gradient boosting model]
Modeling Ambiguity, Subjectivity, and Diverging Viewpoints in Opinion Question Answering Systems
2016 IEEE 16th International Conference on Data Mining
None
2016
Product review websites provide an incredible lens into the wide variety of opinions and experiences of different people, and play a critical role in helping users discover products that match their personal needs and preferences. To help address questions that can't easily be answered by reading others' reviews, some review websites also allow users to pose questions to the community via a question-answering (QA) system. As one would expect, just as opinions diverge among different reviewers, answers to such questions may also be subjective, opinionated, and divergent. This means that answering such questions automatically is quite different from traditional QA tasks, where it is assumed that a single 'correct' answer is available. While recent work introduced the idea of question-answering using product reviews, it did not account for two aspects that we consider in this paper: (1) Questions have multiple, often divergent, answers, and this full spectrum of answers should somehow be used to train the system, and (2) What makes a 'good' answer depends on the asker and the answerer, and these factors should be incorporated in order for the system to be more personalized. Here we build a new QA dataset with 800 thousand questions-and over 3.1 million answers-and show that explicitly accounting for personalization and ambiguity leads both to quantitatively better answers, but also a more nuanced view of the range of supporting, but subjective, opinions.
[QA dataset, QA tasks, Predictive models, QA system, Information retrieval, Data mining, Standards, personalization, opinion question answering systems, product review Web sites, Knowledge discovery, Cameras, question answering (information retrieval), Web sites, Lenses]
Traffic Speed Prediction and Congestion Source Exploration: A Deep Learning Method
2016 IEEE 16th International Conference on Data Mining
None
2016
Traffic speed prediction is a long-standing and critically important topic in the area of Intelligent Transportation Systems (ITS). Recent years have witnessed the encouraging potentials of deep neural networks for real-life applications of various domains. Traffic speed prediction, however, is still in its initial stage without making full use of spatio-temporal traffic information. In light of this, in this paper, we propose a deep learning method with an Error-feedback Recurrent Convolutional Neural Network structure (eRCNN) for continuous traffic speed prediction. By integrating the spatio-temporal traffic speeds of contiguous road segments as an input matrix, eRCNN explicitly leverages the implicit correlations among nearby segments to improve the predictive accuracy. By further introducing separate error feedback neurons to the recurrent layer, eRCNN learns from prediction errors so as to meet predictive challenges rising from abrupt traffic events such as morning peaks and traffic accidents. Extensive experiments on real-life speed data of taxis running on the 2nd and 3rd ring roads of Beijing city demonstrate the strong predictive power of eRCNN in comparison to some state-of-the-art competitors. The necessity of weight pre-training using a transfer learning notion has also been testified. More interestingly, we design a novel influence function based on the deep learning model, and showcase how to leverage it to recognize the congestion sources of the ring roads in Beijing.
[Spatio-temporal, Roads, error-feedback recurrent convolutional neural network structure, ITS, Intelligent transportation systems, Training, spatio-temporal traffic information, Deep learning, Convolution, learning (artificial intelligence), traffic speed prediction, road traffic, transfer learning, eRCNN, Neurons, recurrent neural nets, Beijing City, Convolutional neural network, intelligent transportation systems, Time series prediction, traffic engineering computing, Biological neural networks, congestion source exploration, Machine learning, Feature extraction, deep learning method]
Blind Men and The Elephant: Thurstonian Pairwise Preference for Ranking in Crowdsourcing
2016 IEEE 16th International Conference on Data Mining
None
2016
Crowdsourcing services make it possible to collect huge amount of annotations from less trained crowd workers in an inexpensive and efficient manner. However, unlike making binary or pairwise judgements, labeling complex structures such as ranked lists by crowd workers is subject to large variance and low efficiency, mainly due to the huge labeling space and the annotators' non-expert nature. Yet ranked lists offer the most informative knowledge for training and testing in various data mining and information retrieval tasks such as learning to rank. In this paper, we propose a novel generative model called "Thurstonian Pairwise Preference" (TPP) to infer the true ranked list out of a collection of crowdsourced pairwise annotations. The key challenges that TPP addresses are to resolve the inevitable incompleteness and inconsistency of judgements, as well as to model variable query difficulty and different labeling quality resulting from workers' domain expertise and truthfulness. Experimental results on both synthetic and real-world datasets demonstrate that TPP can effectively bind pairwise preferences of the crowd into rankings and substantially outperforms previously published methods.
[Crowdsourcing, crowdsourcing, Thurstonian pairwise preference, complex structure labeling, judgement completeness, data mining, information retrieval, Noise measurement, Data mining, informative knowledge, query difficulty, Thurstonian Pairwise Preference, Training, query processing, ranked lists, crowdsourced pairwise annotations, judgement inconsistency, Ranking, Transmission line measurements, Labeling, TPP addresses, Testing, crowdsourcing services, labeling quality]
Regularizing Deep Convolutional Neural Networks with a Structured Decorrelation Constraint
2016 IEEE 16th International Conference on Data Mining
None
2016
Deep convolutional networks have achieved successful performance in data mining field. However, training large networks still remains a challenge, as the training data may be insufficient and the model can easily get overfitted. Hence the training process is usually combined with a model regularization. Typical regularizers include weight decay, Dropout, etc. In this paper, we propose a novel regularizer, named Structured Decorrelation Constraint (SDC), which is applied to the activations of the hidden layers to prevent overfitting and achieve better generalization. SDC impels the network to learn structured representations by grouping the hidden units and encouraging the units within the same group to have strong connections during the training procedure. Meanwhile, it forces the units in different groups to learn non-redundant representations by minimizing the cross-covariance between them. Compared with Dropout, SDC reduces the co-adaptions between the hidden units in an explicit way. Besides, we propose a novel approach called Reg-Conv that can help SDC to regularize the complex convolutional layers. Experiments on extensive datasets show that SDC significantly reduces overfitting and yields very meaningful improvements on classification performance (on CIFAR-10 6.22% accuracy promotion and on CIFAR-100 9.63% promotion).
[training procedure, Correlation, Neurons, data mining, structured representation learning, cross-covariance minimization, Electronic mail, Biological neural networks, Training, model regularization, SDC, data mining field, deep convolutional neural network regularization, dropout, structured decorrelation constraint, Data models, Convolutional Networks, Decorrelation, learning (artificial intelligence), weight decay, Reg-Conv, complex convolutional layers regularization, feedforward neural nets, Overfitting]
Aligned Matrix Completion: Integrating Consistency and Independency in Multiple Domains
2016 IEEE 16th International Conference on Data Mining
None
2016
Matrix completion is the task of recovering a data matrix from a sample of entries, and has received significant attention in theory and practice. Normally, matrix completion considers a single matrix, which can be a noisy image or a rating matrix in recommendation. In practice however, data is often obtained from multiple domains rather than a single domain. For example, in recommendation, multiple matrices may exist as user x movie and user x book, while correlations among the multiple domains can be reasonably exploited to improve the quality of matrix completion. In this paper, we consider the problem of aligned matrix completion, where multiple matrices are recovered that correspond to different representations of the same group of objects. In the proposed model, we maintain consistency of multiple domains with a shared latent structure, while allowing independent patterns for each separate domain. In addition, we impose the low-rank structure of a matrix with a novel regularizer which provides better approximation than the standard nuclear norm relaxation.
[Algorithm design and analysis, Correlation, data analysis, data matrix, aligned matrix completion, Minimization, low-rank structure, Electronic mail, Noise measurement, task analysis, Optimization, Convergence, matrix algebra, recommender systems, noisy image, rating matrix, multidomain recommendation task]
Heterogeneous Representation Learning with Structured Sparsity Regularization
2016 IEEE 16th International Conference on Data Mining
None
2016
Motivated by real applications, heterogeneous learning has emerged as an important research area, which aims to model the co-existence of multiple types of heterogeneity. In this paper, we propose a HEterogeneous REpresentation learning model with structured Sparsity regularization (HERES) to learn from multiple types of heterogeneity. HERES aims to leverage two kinds of information to build a robust learning system. One is the rich correlations among heterogeneous data such as task relatedness, view consistency, and label correlation. The other is the prior knowledge of the data in the form of, e.g., the soft-clustering of the tasks. HERES is a generic framework for heterogeneous learning, which integrates multi-task, multi-view, and multi-label learning into a principled framework based on representation learning. The objective of HERES is to minimize the reconstruction loss of using the factor matrices to recover the input matrix for heterogeneous data, regularized by the structured sparsity constraint. The resulting optimization problem is challenging due to the non-smoothness and non-separability of structured sparsity. We develop an iterative updating method to solve the problem. Furthermore, we prove that the reformulation of structured sparsity is separable, which leads to a family of efficient and scalable algorithms for solving structured sparsity penalized problems. The experimental results in comparison with state-of-the-art methods demonstrate the effectiveness of the proposed approach.
[iterative methods, optimization problem, Correlation, heterogeneous data input matrix, label correlation, multilabel learning, reconstruction loss minimization, task soft-clustering, structured sparsity constraint, Optimization, multiview learning, Learning systems, multi-view learning, factor matrices, Robustness, learning (artificial intelligence), heterogeneous representation learning, task relatedness, multi-label learning, iterative updating method, Encoding, Matrix decomposition, multitask learning, matrix algebra, view consistency, multi-task learning, HERES, structured sparsity regularization, data knowledge, Data models, minimisation, Heterogeneous learning]
POI Recommendation: A Temporal Matching between POI Popularity and User Regularity
2016 IEEE 16th International Conference on Data Mining
None
2016
Point of interest (POI) recommendation, which provides personalized recommendation of places to mobile users, is an important task in location-based social networks (LBSNs). However, quite different from traditional interest-oriented merchandise recommendation, POI recommendation is more complex due to the timing effects: we need to examine whether the POI fits a user's availability. While there are some prior studies which included the temporal effect into POI recommendations, they overlooked the compatibility between time-varying popularity of POIs and regular availability of users, which we believe has a non-negligible impact on user decision-making. To this end, in this paper, we present a novel method which incorporates the degree of temporal matching between users and POIs into personalized POI recommendations. Specifically, we first profile the temporal popularity of POIs to show when a POI is popular for visit by mining the spatio-temporal human mobility and POI category data. Secondly, we propose latent user regularities to characterize when a user is regularly available for exploring POIs, which is learned with a user-POI temporal matching function. Finally, results of extensive experiments with real-world POI check-in and human mobility data demonstrate that our proposed user-POI temporal matching method delivers substantial advantages over baseline models for POI recommendation tasks.
[POI category data, POI recommendation tasks, pattern matching, point-of-interest recommendation, Mobile communication, Data mining, History, location-based social networks, mobile computing, spatio-temporal human mobility, mobile users, user decision-making, personalized POI recommendations, user-POI temporal matching function, Urban areas, real-world POI check-in, personalized recommendation, user regularity, Public transportation, recommender systems, LBSN, decision making, social networking (online), Pattern matching, Mobile computing]
College Student Scholarships and Subsidies Granting: A Multi-modal Multi-label Approach
2016 IEEE 16th International Conference on Data Mining
None
2016
Scholarships and financial aids in modern universities are the basic administrative plans to ensure and promote the completion of academic training and studies for students. Traditional grants allocation procedures are based on manual determination, which costs lots of human resources. In this paper, we investigate an assistance model for helping improve the scheme of granting. We first collect students information from multi-modal channels, including their behaviors of campus consumption, internet usage, daily trajectory together with their enrollment information. The approval status and amount of funds granted are converted as labels. We propose the College Student Scholarships and Subsidies Granting (CS3G) approach to address the concrete problem. CS3G approach overcomes 3 obstacles, i.e., complicated multi-label influences, private modal information protection and difficulties in label collection. In detail, based on the facts that scholarships mainly depend on academic achievements, subsidies granting is generally based on students financial hardships as well as credits, and there are implicit influences among scholarships and subsidies, the CS3G approach handles types of interactions between multiple labels, it is notable that data from different modalities are collected by different divisions of a university, privacy protection is considered in CS3G, i.e., no interaction between features from different modalities in the model training phase. Besides, due to the confidentiality of the concrete types/amounts of granting, only a portion of labels is collected in this application, CS3G is trained in a semi-supervised style. Empirical investigations show good generalization ability of CS3G on benchmark datasets, and a real assessment of a university also validates the power of our approach for tackling this type of problem well.
[CS3G approach, Scholarships, Multi-Modal Learning, college student scholarships and subsidies granting approach, university, educational administrative data processing, semisupervised training, Training, privacy protection, Privacy-Preserving, Feature extraction, data protection, Multi-Label Learning, Concrete, multimodal multilabel approach, Internet, Trajectory, educational institutions, Resource management, learning (artificial intelligence), Student Scholarships and Subsidies Granting]
Generalized Independent Subspace Clustering
2016 IEEE 16th International Conference on Data Mining
None
2016
Data can encapsulate different object groupings in subspaces of arbitrary dimension and orientation. Finding such subspaces and the groupings within them is the goal of generalized subspace clustering. In this work we present a generalized subspace clustering technique capable of finding multiple non-redundant clusterings in arbitrarily-oriented subspaces. We use Independent Subspace Analysis (ISA) to find the subspace collection that minimizes the statistical dependency (redundancy) between clusterings. We then cluster in the arbitrarily-oriented subspaces identified by ISA. Our algorithm ISAAC (Independent Subspace Analysis and Clustering) uses the Minimum Description Length principle to automatically choose parameters that are otherwise difficult to set. We comprehensively demonstrate the effectiveness of our approach on synthetic and real-world data.
[multiple nonredundant clusterings, subspace collection, Redundancy, statistical dependency minimization, independent subspace analysis, pattern clustering, Clustering algorithms, generalized independent subspace clustering technique, minimum description length principle, Probability density function, Data models, ISAAC algorithm, Random variables, minimisation, statistical analysis, Kernel, independent subspace analysis and clustering, arbitrarily-oriented subspaces, Principal component analysis]
Matrix Profile III: The Matrix Profile Allows Visualization of Salient Subsequences in Massive Time Series
2016 IEEE 16th International Conference on Data Mining
None
2016
Multidimensional Scaling (MDS) is one of the most versatile tools used for exploratory data mining. It allows a first glimpse of possible structure in the data, which can inform the choice of analyses used. Its uses are multiple. It can give the user an idea as to the cluster ability or linear separability of the data. It can help spot outliers, or can hint at the intrinsic dimensionality of the data. Moreover, it can sometimes reveal unexpected latent dimensions in the data. With all these uses, MDS is increasingly used in areas as diverse as marketing, medicine, genetics, music and linguistics. One of the strengths of MDS is that it is essentially agnostic to data type, as we can use any distance measure to create the distance matrix, which is the only required input to the MDS algorithm. In spite of this generality, we make the following claim. MDS is not (well) defined for an increasingly important data type, time series subsequences. In this work we explain why this is the case, and we propose a scalable solution. We demonstrate the utility of our ideas on several diverse real-world datasets. At the core of our approach is a novel Minimum Description Length (MDL) subsequence extraction algorithm. Beyond MDS visualization, this subsequence extraction subroutine may be a useful tool in its own right.
[Algorithm design and analysis, cluster ability, linear separability, Visualization, minimum description length subsequence extraction algorithm, data mining, Feature Extraction, Data mining, Heart beat, MDL, Clustering algorithms, data visualisation, intrinsic data dimensionality, matrix profile, MDS, Time series analysis, time series, matrix algebra, pattern clustering, Multidimensional Scaling, Data visualization, Time Series, salient subsequences visualization, multidimensional scaling, Heart rate variability, exploratory data mining]
Random Walk with Restart over Dynamic Graphs
2016 IEEE 16th International Conference on Data Mining
None
2016
Random Walk with Restart (RWR) is an appealing measure of proximity between nodes based on graph structures. Since real graphs are often large and subject to minor changes, it is prohibitively expensive to recompute proximities from scratch. Previous methods use LU decomposition and degree reordering heuristics, entailing O(|&#x03BD;|3) time and O(|&#x03BD;|2) memory to compute all (|&#x03BD;|2) pairs of node proximities in a static graph. In this paper, a dynamic scheme to assess RWR proximities is proposed: (1) For unit update, we characterize the changes to all-pairs proximities as the outer product of two vectors. We notice that the multiplication of an RWR matrix and its transition matrix, unlike traditional matrix multiplications, is commutative. This can greatly reduce the computation of all-pairs proximities from O(|&#x03BD;|3) to O(|&#x0394;|) time for each update without loss of accuracy, where |&#x0394;| (&#x226A;|V|2) is the number of affected proximities. (2) To avoid O(|V|2) memory for all pairs of outputs, we also devise efficient partitioning techniques for our dynamic model, which can compute all pairs of proximities segment-wisely within O(I|V|) memory and O([|V|/l]) I/O costs, where 1 &#x2264; I &#x2264; |V| is a user-controlled trade-off between memory and I/O costs. (3) For bulk updates, we also devise aggregation and hashing methods, which can discard many unnecessary updates further and handle chunks of unit updates simultaneously. Our experimental results on various datasets demonstrate that our methods can be 1-2 orders of magnitude faster than other competitors while securing scalability and exactness.
[partitioning techniques, Symmetric matrices, random walk with restart, proximity search, Computational modeling, graph theory, RWR matrix, Time measurement, Loss measurement, Noise measurement, Matrix decomposition, matrix algebra, hashing method, dynamic graphs, graphs, transition matrix, RWR proximities, aggregation method, Matrix converters, computational complexity]
Communities in Preference Networks: Refined Axioms and Beyond
2016 IEEE 16th International Conference on Data Mining
None
2016
Borgs et al. [2016] investigated essential requirements for communities in preference networks. They defined six axioms on community functions, i.e., community detection rules. Though having elegant properties, the practicality of this axiomsystem is compromised by the intractability of checking twocritical axioms, so no nontrivial consistent community functionwas reported in [Borgs et al., 2016]. By adapting the two axioms in a natural way, we propose two new axioms that are efficiently-checkable. We show that most of the desirable properties of the original axiom system are preserved. More importantly, the new axioms provide a general approach to constructing consistent community functions. We further find a natural consistent community function that is also enumerable and samplable, answering an open problem in the literature.
[Context, Social network services, Conferences, Laboratories, Lattices, consistent community function construction, network theory (graphs), preference networks, axiomazation, efficiently-checkable axioms, community, Clustering algorithms, social sciences, Software, preference network, community detection rules]
Homophily, Structure, and Content Augmented Network Representation Learning
2016 IEEE 16th International Conference on Data Mining
None
2016
Advances in social networking and communication technologies have witnessed an increasing number of applications where data is not only characterized by rich content information, but also connected with complex relationships representing social roles and dependencies between individuals. To enable knowledge discovery from such networked data, network representation learning (NRL) aims to learn vector representations for network nodes, such that off-the-shelf machine learning algorithms can be directly applied. To date, existing NRL methods either primarily focus on network structure or simply combine node content and topology for learning. We argue that in information networks, information is mainly originated from three sources: (1) homophily, (2) topology structure, and (3) node content. Homophily states social phenomenon where individuals sharing similar attributes (content) tend to be directly connected through local relational ties, while topology structure emphasizes more on global connections. To ensure effective network representation learning, we propose to augment three information sources into one learning objective function, so that the interplay roles between three parties are enforced by requiring the learned network representations (1) being consistent with node content and topology structure, and also (2) following the social homophily constraints in the learned space. Experiments on multi-class node classification demonstrate that the representations learned by the proposed method consistently outperform state-of-the-art NRL methods, especially for very sparsely labeled networks.
[Context, pattern classification, Social network services, data mining, NRL, knowledge discovery, Topology, Electronic mail, augmented network representation learning, node content, multiclass node classification, Network topology, topology structure, social homophily constraints, Australia, learning (artificial intelligence), machine learning algorithms, Context modeling]
Fixing the Convergence Problems in Parallel Asynchronous Dual Coordinate Descent
2016 IEEE 16th International Conference on Data Mining
None
2016
Solving L2-regularized empirical risk minimization (e.g., linear SVMs and logistic regression) using multiple cores has become an important research topic. Among all the existing algorithms, Parallel ASynchronous Stochastic dual Co-Ordinate DEscent (PASSCoDe) demonstrates superior performance compared with other methods. Although PASSCoDe is fast when it converges, the algorithm has been observed to diverge on several cases especially when a relatively large number of threads are used. This is mainly due to the delayed parameter access problem - the parameters used for the current update may be delayed and are not the latest ones. In theory, the algorithm converges only when the delay is small enough, but in practice the delay depends on the underlying parallel computing environment and cannot be guaranteed. In this work, we propose a simple and computational efficient way to fix the convergence problem of PASSCoDe. Instead of allowing all worker threads to conduct asynchronous updates wildly, we add periodic check points to the procedure, where all workers need to stop and refine the current solution at each check point. The resulting "semi-asynchronous" algorithm is guaranteed to converge for any problem even when PASSCoDe diverges, and for the cases where PASSCoDe converges they have almost identical speed.
[risk management, multi-threading, Instruction sets, convergence, parallel asynchronous stochastic dual coordinate descent, Coordinate descent, convergence problem, Convergence, Support vector machines, asynchronous updates, L2-regularized empirical risk minimization, semiasynchronous algorithm, PASSCoDe, Delays, Risk management, minimisation, Asynchronous algorithm, parallel computing environment, periodic check points, Logistics, Message systems]
HogWild++: A New Mechanism for Decentralized Asynchronous Stochastic Gradient Descent
2016 IEEE 16th International Conference on Data Mining
None
2016
Stochastic Gradient Descent (SGD) is a popular technique for solving large-scale machine learning problems. In order to parallelize SGD on multi-core machines, asynchronous SGD (Hogwild!) has been proposed, where each core updates a global model vector stored in a shared memory simultaneously, without using explicit locks. We show that the scalability of Hogwild! on modern multi-socket CPUs is severely limited, especially on NUMA (Non-Uniform Memory Access) system, due to the excessive cache invalidation requests and false sharing. In this paper we propose a novel decentralized asynchronous SGD algorithm called HogWild++ that overcomes these drawbacks and shows almost linear speedup on multi-socket NUMA systems. The main idea in HogWild++ is to replace the global model vector with a set of local model vectors that are shared by a cluster (a set of cores), keep them synchronized through a decentralized token-based protocol that minimizes remote memory access conflicts and ensures convergence. We present the design and experimental evaluation of HogWild++ on a variety of datasets and show that it outperforms state-of-the-art parallel SGD implementations in terms of efficiency and scalability.
[Algorithm design and analysis, Machine learning algorithms, HOGWILD!, decentralized asynchronous SGD, Scalability, Instruction sets, decentralized token-based protocol, cache invalidation requests, asynchronous SGD, local model vectors, parallel processing, shared memory systems, HogWild++, large-scale machine learning, nonuniform memory access system, learning (artificial intelligence), protocols, stochastic processes, gradient methods, multicore machines, multisocket NUMA systems, NUMA system, Multicore processing, Computational modeling, Non-uniform memory access (NUMA) architecture, Stochastic gradient descent, Sockets, pattern clustering, multisocket CPU, remote memory access conflict minimisation, decentralized asynchronous stochastic gradient descent, Decentralized algorithm, shared memory, global model vector]
Predicting COPD Failure by Modeling Hazard in Longitudinal Clinical Data
2016 IEEE 16th International Conference on Data Mining
None
2016
Chronic obstructive pulmonary disease (COPD) accounts for the highest rate of hospital readmissions and is the third leading cause of death in Canada, the United States and worldwide. Predicting COPD failure provides a prognostic warning of death or readmission, and is crucial to early intervention and decision-making. The aim of this study is to perform COPD failure prediction on longitudinal data. To address the inappropriate estimation of Cox hazard in current approaches, we propose a new representation of hazard to capture the relationship between survival probability and time-varying risk factors in a concise but effective way. To optimize model parameters, we design and maximize a new joint likelihood that comprises two components used to estimate survival status separately for failure and censored patients. A regularized optimization is performed on the joint likelihood to prevent overfitting arising from model learning. Our approach is applied to a real-life COPD data set and outperforms the current state-of-the-art prediction models in terms of the survival AUC, concordance index and Birer score metrics, this reveals that the great promise of our approach for clinical prediction.
[Birer score metrics, Cox hazard, Predictive models, clinical data, Learning, Analytical models, Regression Model, concordance index, medical administrative data processing, Mathematical model, hospital readmissions, time-varying risk factors, data analysis, probability, diseases, Hazards, Indexes, risk analysis, Hospitals, COPD failure prediction, Failure Prediction, survival probability, Survival Analysis, survival AUC, Data models, hazard modeling, Clinical Data, chronic obstructive pulmonary disease]
Reliable Gender Prediction Based on Users&#x2019; Video Viewing Behavior
2016 IEEE 16th International Conference on Data Mining
None
2016
With the growth of the digital advertising market, it has become more important than ever to target the desired audiences. Among various demographic traits, gender information plays a key role in precisely targeting the potential consumers in online advertising and ecommerce. However, such personal information is generally unavailable to digital media sellers. In this paper, we propose a novel task-specific multi-task learning algorithm to predict users' gender information from their video viewing behaviors. To detect as many desired users as possible, while controlling the Type I error rate at a user-specified level, we further propose Bayes testing and decision procedures to efficiently identify male and female users, respectively. Comprehensive experiments have justified the effectiveness and reliability of our framework.
[TV, type I error rate, advertising, digital media sellers, ecommerce, Motion pictures, demographic traits, learning (artificial intelligence), Bayes testing, Advertising, statistical testing, electronic commerce, personal information, task-specific multitask learning algorithm, gender prediction, Media, digital advertising market, gender information, user video viewing behavior, Streaming media, decision procedures, Data models, Bayes methods, Reliability, gender issues, online advertising]
Probabilistic-Mismatch Anomaly Detection: Do One&#x2019;s Medications Match with the Diagnoses
2016 IEEE 16th International Conference on Data Mining
None
2016
Anomaly detection in healthcare data like patient records is no trivial task. The anomalies in these datasets are often caused by mismatches between different types of feature, e.g., medications that do not match with the diagnoses. Existing anomaly detection methods do not perform well when detecting "mismatches" between multiple types of feature, especially when the feature space is high-dimensional and sparse. This paper introduces a novel anomaly detection paradigm: Probabilistic-Mismatch Anomaly Detection (PMAD), which detects mismatches between features by modeling a normal instance with a common latent probability distribution that governs the generation of all types of feature. Under this paradigm, the target of anomaly detection is to find instances with dissimilar latent distributions. We further propose Topical PMAD based on an extended Latent Dirichlet Allocation (LDA) model, which is able to capture the latent relationship between features in a high-dimensional space. Experiments on both synthetic data and real-world patient records show that Topical PMAD can effectively detect anomalies with mismatched features, and is highly robust against high-dimensional data as well as inaccurate model selection. The real-world anomalies detected on a patient record dataset show a promising application prospect.
[Solid modeling, topical PMAD, Medical services, anomaly detection, statistical distributions, dissimilar latent distributions, feature extraction, medications, feature mismatch, high-dimensional sparse feature space, medical administrative data processing, patient records, health care, Context, topic model, diagnosis, probabilistic-mismatch anomaly detection, healthcare data, feature mismatches, feature generation, extended latent Dirichlet allocation, LDA model, Feature extraction, Data models, Bayes methods, Medical diagnostic imaging, patient diagnosis, Context modeling]
Inferring Latent Network from Cascade Data for Dynamic Social Recommendation
2016 IEEE 16th International Conference on Data Mining
None
2016
Social recommendation explores social information to improve the quality of a recommender system. It can be further divided into explicit and implicit social network recommendation. The former assumes the existence of explicit social connections between users in addition to the rating data. The latter one assumes the availability of only the ratings but not the social connections between users since the explicit social information data may not necessarily be available and usually are binary decision values (e.g., whether two people are friends), while the strength of their relationships is missing. Most of the works in this field use only rating data to infer the latent social networks. They ignore the dynamic nature of users that the preferences of users drift over time distinctly. To this end, we propose a new Implicit Dynamic Social Recommendation (IDSR) model, which infers latent social network from cascade data. It can sufficiently mine the information contained in time by mining the cascade data and identify the dynamic changes in the users in time by using the latest updated social network to make recommendations. Experiments and comparisons on three real-world datasets show that the proposed model outperforms the state-of-the-art solutions in both explicit and implicit scenarios.
[Social network services, latent social network, data mining, information mining, Predictive models, Probabilistic logic, Latent dynamic social network, social information, Data mining, IDSR model, Social recommendation, Optimization, implicit dynamic social recommendation model, Cascade data, recommender systems, recommender system, cascade data mining, explicit social connections, social networking (online), Data models, explicit social network recommendation, implicit social network recommendation, Recommender systems, binary decision values]
Group Preference Aggregation: A Nash Equilibrium Approach
2016 IEEE 16th International Conference on Data Mining
None
2016
Group-oriented services such as group recommendations aim to provide services for a group of users. For these applications, how to aggregate the preferences of different group members is the toughest yet most important problem. Inspired by game theory, in this paper, we propose to explore the idea of Nash equilibrium to simulate the selections of members in a group by a game process. Along this line, we first compute the preferences (group-dependent optimal selections) of each individual member in a given group scene, i.e., an equilibrium solution of this group, with the help of two pruning approaches. Then, to get the aggregated unitary preference of each group from all group members, we design a matrix factorization-based method which aggregates the preferences in latent space and estimates the final group preference in rating space. After obtaining the group preference, group-oriented services (e.g., group recommendation) can be directly provided. Finally, we construct extensive experiments on two real-world data sets from multiple aspects. The results clearly demonstrate the effectiveness of our method.
[Group Recommendation, group-oriented services, Nash Equilibrium, Computational modeling, game theory, group preference aggregation, Nash equilibrium, matrix decomposition, Computer science, group theory, pruning approaches, Aggregates, Preference Aggregation, Prototypes, Games, aggregated unitary preference, matrix factorization-based method, group-dependent optimal selections, Nash equilibrium approach]
Multi-resolution Spatial Event Forecasting in Social Media
2016 IEEE 16th International Conference on Data Mining
None
2016
Social media has become a significant surrogate forspatial event forecasting. The accuracy and discernibility of aspatial event forecasting model are two key concerns, whichrespectively determine how accurate and how detailed themodel's predictions could be. Existing work pays most attentionon the accuracy alone, seldom considering the accuracyand discernibility simultaneously, because this would requiresa considerably more sophisticated model while still sufferingfrom several challenges: 1) the precise formulation of thetrade-off between accuracy and discernibility, 2) the scarcityof social media data with a high spatial resolution, and 3)the characterization of spatial correlation and heterogeneity. This paper proposes a novel feature learning model thatconcurrently addresses all the above challenges by formulatingprediction tasks for different locations with different spatialresolutions, allowing the heterogeneous relationships amongthe tasks to be characterized. This characterization is thenintegrated into our new model based on multitask learning, whose parameters are optimized by our proposed algorithmbased on the Alternative Direction Method of Multipliers(ADMM). Extensive experimental evaluations on 11 datasetsfrom different domains demonstrated the effectiveness of ourproposed approach.
[Urban areas, Predictive models, Twitter, spatial correlation, multitask learning, Forecasting, alternative direction method of multipliers, optimisation, forecasting theory, social media data, social networking (online), spatial resolution, multiresolution spatial event forecasting, Sensors, parameter optimization, learning (artificial intelligence), Spatial resolution, ADMM, correlation methods, feature learning model]
To be or Not to be Friends: Exploiting Social Ties for Venture Investments
2016 IEEE 16th International Conference on Data Mining
None
2016
Recent years have witnessed the boom of venture capital industry. Venture capitalists can attain great financial rewards if their invested companies exit successfully, via being acquired or going IPO (Initial Public Offering). The literature has revealed that, from both financial and managerial perspectives, decision-making process and successful rates of venture capital (VC) investments can be greatly improved if the investors well know the team members of target startups. However, much less efforts have been made on understanding the impact of prominent social ties between the members of VC firms and start-up companies on investment decisions. To this end, we propose to study such social relationship and see how this information can contribute to foreseeing investment deals. We aim at providing analytical guidance for the venture capitalists in choosing right investment targets. Specifically, we develop a Social-Adjusted Probabilistic Matrix Factorization (PMF) model to exploit members social connections information from VC firms and startups for investment recommendations. Unlike previous studies, we make use of the directed relationship between any pair of connected members from the two institutions respectively and quantify the variety of social network groups. As a result, it brings in much more flexibility, and the modeling results inherently provide meaningful managerial implications for the operators of VC firms and startups. Finally, we evaluate our model on both synthetic and real-world data. The results demonstrate that our approach outperforms the baseline algorithms with a significant margin.
[social relationship, venture capital industry, Companies, social connections information, Predictive models, Twitter, IPO, matrix decomposition, social ties, initial public offering, decision-making process, venture investments, social sciences computing, Facebook, financial rewards, venture capital, Decision making, probability, investment recommendations, social-adjusted probabilistic matrix factorization model, managerial perspective, recommender systems, Venture capital, financial perspective, Investment, social-adjusted PMF model]
Graph-Structured Sparse Optimization for Connected Subgraph Detection
2016 IEEE 16th International Conference on Data Mining
None
2016
Structured sparse optimization is an important and challenging problem for analyzing high-dimensional data in a variety of applications such as bioinformatics, medical imaging, social networks, and astronomy. Although a number of structured sparsity models have been explored, such as trees, groups, clusters, and paths, connected subgraphs have been rarely explored in the current literature. One of the main technical challenges is that there is no structured sparsity-inducing norm that can directly model the space of connected subgraphs, and there is no exact implementation of a projection oracle for connected subgraphs due to its NP-hardness. In this paper, we explore efficient approximate projection oracles for connected subgraphs, and propose two new efficient algorithms, namely, Graph-IHT and Graph-GHTP, to optimize a generic nonlinear objective function subject to connectivity constraint on the support of the variables. Our proposed algorithms enjoy strong guarantees analogous to several current methods for sparsity-constrained optimization, such as Projected Gradient Descent (PGD), Approximate Model Iterative Hard Thresholding (AM-IHT), and Gradient Hard Thresholding Pursuit (GHTP) with respect to convergence rate and approximation accuracy. We apply our proposed algorithms to optimize several well-known graph scan statistics in several applications of connected subgraph detection as a case study, and the experimental results demonstrate that our proposed algorithms outperform state-of-the-art methods.
[Algorithm design and analysis, gradient hard thresholding pursuit, projected gradient descent, approximate model iterative hard thresholding, sparsity-constrained optimization, nonlinear programming, graph theory, generic nonlinear objective function, AM-IHT, anomaly detection, Optimization, high-dimensional data analysis, approximate projection oracles, graph-GHTP, Silicon, subgraph detection, Mathematical model, Bioinformatics, NP-hardness problem, graph-structured sparse optimization, structure sparsity, data analysis, connected subgraph detection, connected subgraph, Social network services, structured sparsity models, graph scan statistics, PGD, graph-IHT, event detection, Approximation algorithms, GHTP, convergence rate, computational complexity]
Bi-Level Rare Temporal Pattern Detection
2016 IEEE 16th International Conference on Data Mining
None
2016
Nowadays, temporal data is generated at an unprecedented speed from a variety of applications, such as wearable devices, sensor networks, wireless networks and etc. In contrast to such large amount of temporal data, it is usually the case that only a small portion of them contains information of interest. For example, for the ECG signals collected by wearable devices, most of them collected from healthy people are normal, and only a small number of them collected from people with certain heart diseases are abnormal. Furthermore, even for the abnormal temporal sequences, the abnormal patterns may only be present in a few time segments and are similar among themselves, forming a rare category of temporal patterns. For example, the ECG signal collected from an individual with a certain heart disease may be normal in most time segments, and abnormal in only a few time segments, exhibiting similar patterns. What is even more challenging is that such rare temporal patterns are often non-separable from the normal ones. Existing works on outlier detection for temporal data focus on detecting either the abnormal sequences as a whole, or the abnormal time segments directly, ignoring the relationship between abnormal sequences and abnormal time segments. Moreover, the abnormal patterns are typically treated as isolated outliers instead of a rare category with self-similarity. In this paper, for the first time, we propose a bi-level (sequence-level/ segment-level) model for rare temporal pattern detection. It is based on an optimization framework that fully exploits the bi-level structure in the data, i.e., the relationship between abnormal sequences and abnormal time segments. Furthermore, it uses sequence-specific simple hidden Markov models to obtain segment-level labels, and leverages the similarity among abnormal time segments to estimate the model parameters. To solve the optimization framework, we propose the unsupervised algorithm BIRAD, and also the semi-supervised version BIRAD-K which learns from a single labeled example. Experimental results on both synthetic and real data sets demonstrate the performance of the proposed algorithms from multiple aspects, outperforming state-of-the-art techniques on both temporal outlier detection and rare category analysis.
[Heart, time segments, BIRAD-K, real data sets, sensor networks, temporal data, self-similarity, Optimization, electrocardiography, abnormal temporal sequences, Electrocardiography, synthetic data sets, temporal outlier detection, ECG signals, wearable devices, bilevel rare temporal pattern detection, Time series analysis, wireless networks, time series, abnormal time segments, rare category detection, Diseases, medical signal processing, rare category analysis, Hidden Markov models, heart diseases, Data models, segment-level model, temporal data mining, sequence-level model]
A Bayesian Nonparametric Approach to Dynamic Dyadic Data Prediction
2016 IEEE 16th International Conference on Data Mining
None
2016
An important issue of using matrix factorization for recommender systems is to capture the dynamics of user preference over time for more accurate prediction. We find that considering the existence of clusters among users with respect to evolution behavior of their preference can improve performance effectively. This is especially important to commercial recommender systems, where the evolution of preference for different users is heterogeneous, and historical ratings are not enough to estimate the preference of each user individually. Based on this, we propose a novel Bayesian nonparametric method based on the Dirichlet process, to detect users sharing the same evolution behavior of their preference. For each community, we use vector autoregressive model (VAR) to capture the evolution to explore higher-order dependency on historical user preference, and incorporate this feature with a novel adaptive prior strategy. We also derive variational inference approach to infer our method. Finally, we conduct extensive empirical experiments to show the advantage of our method over state-of-the-art algorithms.
[Adaptation models, Gold, data analysis, historical user preference, Heuristic algorithms, VAR, adaptive prior strategy, autoregressive processes, Dirichlet process, inference mechanisms, Matrix Factorization, vector autoregressive model, Reactive power, Motion pictures, Bayes methods, Recommender System, Bayesian Nonparametric, Kalman filters, variational inference, Bayesian nonparametric method, data prediction]
Matrix Profile II: Exploiting a Novel Algorithm and GPUs to Break the One Hundred Million Barrier for Time Series Motifs and Joins
2016 IEEE 16th International Conference on Data Mining
None
2016
Time series motifs have been in the literature for about fifteen years, but have only recently begun to receive significant attention in the research community. This is perhaps due to the growing realization that they implicitly offer solutions to a host of time series problems, including rule discovery, anomaly detection, density estimation, semantic segmentation, etc. Recent work has improved the scalability to the point where exact motifs can be computed on datasets with up to a million data points in tenable time. However, in some domains, for example seismology, there is an insatiable need to address even larger datasets. In this work we show that a combination of a novel algorithm and a high-performance GPU allows us to significantly improve the scalability of motif discovery. We demonstrate the scalability of our ideas by finding the full set of exact motifs on a dataset with one hundred million subsequences, by far the largest dataset ever mined for time series motifs. Furthermore, we demonstrate that our algorithm can produce actionable insights in seismology and other domains.
[joins, Scalability, Time series analysis, Graphics processing units, GPUs, seismology, Time series, time series, Data mining, Indexes, time series motifs, graphics processing units, Earthquakes, Seismology, motifs, matrix profile, high-performance GPU, motif discovery]
Measuring Patient Similarities via a Deep Architecture with Medical Concept Embedding
2016 IEEE 16th International Conference on Data Mining
None
2016
Evaluating the clinical similarities between pairwise patients is a fundamental problem in healthcare informatics. Aproper patient similarity measure enables various downstream applications, such as cohort study and treatment comparative effectiveness research. One major carrier for conducting patient similarity research is the Electronic Health Records(EHRs), which are usually heterogeneous, longitudinal, and sparse. Though existing studies on learning patient similarity from EHRs have shown being useful in solving real clinical problems, their applicability is limited due to the lack of medical interpretations. Moreover, most previous methods assume a vector based representation for patients, which typically requires aggregation of medical events over a certain time period. As aconsequence, the temporal information will be lost. In this paper, we propose a patient similarity evaluation framework based on temporal matching of longitudinal patient EHRs. Two efficient methods are presented, unsupervised and supervised, both of which preserve the temporal properties in EHRs. The supervised scheme takes a convolutional neural network architecture, and learns an optimal representation of patient clinical records with medical concept embedding. The empirical results on real-world clinical data demonstrate substantial improvement over the baselines.
[pattern matching, patient similarity measurement, healthcare informatics, real-world clinical data, EHR, medical concept embedding, medical interpretations, Natural language processing, Medical Concept Embedding, health care, Context, patient similarity evaluation, temporal longitudinal patient EHR matching, patient similarity research, deep architecture, Patient Similarity, electronic health records, Deep Matching, patient care, Diseases, unsupervised learning, vector based representation, Neural networks, clinical similarities, convolutional neural network architecture, Medical diagnostic imaging]
ConTrack: A Scalable Method for Tracking Multiple Concepts in Large Scale Multidimensional Data
2016 IEEE 16th International Conference on Data Mining
None
2016
In industrial domains such as finance, telecommunications, the internet, and sensor monitoring, large volumes of unlabeled temporal data are continuously generated, such as financial transactions, sensor measurements and user activities. From a data analysis standpoint, there is significant utility to be gained by detecting and understanding changes in the data, such as physical activity recognition and content consumption behavior, or anomalies and faults in robots and sensors. However, because the data is unlabeled, it is challenging to visualize and understand in a way that produces interpretable insights, furthermore, the large volume of data imposes a scalability requirement. In the concept drift and stream mining literature, existing methods may focus on one or two, but rarely all three, of the aforementioned aspects: unlabeled data, interpretable output, scalability. Addressing this need, we propose ConTrack, an unsupervised method that tracks multiple evolving concepts in temporal data, and which is parallelized over a cluster of machines. To enhance interpretability, our method structures its output at a per-user (or actor) level, where users subscribe to one or more evolving concepts. Our method applies to problem settings (multiple concepts, unsupervised data, temporal data, user-oriented data) that cannot be handled by existing concept drift and stream mining methods, and outperforms popular unsupervised baselines from the wider Data Mining and Machine Learning literature.
[Algorithm design and analysis, Scalability, ConTrack, Scalable, data mining, multiple concept tracking, temporal data, Unsupervised, Data mining, stream mining, unsupervised data, Concept Drift, industrial domains, Activity recognition, Robot sensing systems, interpretability, unsupervised baselines, Legged locomotion, data analysis, user-oriented data, unlabeled temporal data, machine learning, unsupervised learning, large scale multidimensional data, concept drift, Distributed, Data models]
Fractality of Massive Graphs: Scalable Analysis with Sketch-Based Box-Covering Algorithm
2016 IEEE 16th International Conference on Data Mining
None
2016
Analysis and modeling of networked objects are fundamental pieces of modern data mining. Most real-world networks, from biological to social ones, are known to have common structural properties. These properties allow us to model the growth processes of networks and to develop useful algorithms. One remarkable example is the fractality of networks, which suggests the self-similar organization of global network structure. To determine the fractality of a network, we need to solve the so-called box-covering problem, where preceding algorithms are not feasible for large-scale networks. The lack of an efficient algorithm prevents us from investigating the fractal nature of large-scale networks. To overcome this issue, we propose a new box-covering algorithm based on recently emerging sketching techniques. We theoretically show that it works in near-linear time with a guarantee of solution accuracy. In experiments, we have confirmed that the algorithm enables us to study the fractality of million-scale networks for the first time. We have observed that its outputs are sufficiently accurate and that its time and space requirements are orders of magnitude smaller than those of previous algorithms.
[Algorithm design and analysis, Greedy algorithms, large-scale networks, graph theory, networked object modeling, data mining, network theory (graphs), Fractals, complex networks, Data mining, sketch-based box-covering, fractality, time requirements, networked object analysis, box cover, massive graph fractality, million-scale network fractality, Biological system modeling, space requirements, global network structure, graph, structural properties, box-covering problem, self-similar organization, min-hash sketch, Approximation algorithms, network analysis, computational complexity]
Cut Tree Construction from Massive Graphs
2016 IEEE 16th International Conference on Data Mining
None
2016
The construction of cut trees (also known as Gomory-Hu trees) for a given graph enables the minimum-cut size of the original graph to be obtained for any pair of vertices. Cut trees are a powerful back-end for graph management and mining, as they support various procedures related to the minimum cut, maximum flow, and connectivity. However, the crucial drawback with cut trees is the computational cost of their construction. In theory, a cut tree is built by applying a maximum flow algorithm for n times, where n is the number of vertices. Therefore, naive implementations of this approach result in cubic time complexity, which is obviously too slow for today's large-scale graphs. To address this issue, in the present study, we propose a new cut-tree construction algorithm tailored to real-world networks. Using a series of experiments, we demonstrate that the proposed algorithm is several orders of magnitude faster than previous algorithms and it can construct cut trees for billion-scale graphs.
[cut tree construction, Heuristic algorithms, Conferences, cut tree, data mining, trees (mathematics), cubic time complexity, Graph theory, Data mining, minimum-cut size, graph management, graph, graph mining, Gomory-Hu trees, large-scale graphs, network analysis, Computational efficiency, maximum flow algorithm, max-flow, Time complexity, Informatics, computational complexity, min-cut]
Learning from Your Network of Friends: A Trajectory Representation Learning Model Based on Online Social Ties
2016 IEEE 16th International Conference on Data Mining
None
2016
Location-Based Social Networks (LBSNs) capture individuals whereabouts for a large portion of the population. To utilize this data for user (location)-similarity based tasks, one must map the raw data into a low-dimensional uniform feature space. However, due to the nature of LBSNs, many users have sparse and incomplete check-ins. In this work, we propose to overcome this issue by leveraging the network of friends, when learning the new feature space. We first analyze the impact of friends on individuals's mobility, and show that individuals trajectories are correlated with thoseof their friends and friends of friends (2-hop friends) in an online setting. Based on our observation, we propose a mixed-membership model that infers global mobility patterns from users' check-ins and their network of friends, without impairing the model's complexity. Our proposed model infers global patterns and learns new representations for both usersand locations simultaneously. We evaluate the inferred patterns and compare the quality of the new user representation against baseline methods on a social link prediction problem.
[social link prediction problem, Social network services, global mobility patterns, mixed membership models, online social ties, low-dimensional uniform feature space, user-similarity based tasks, mixed-membership model, Complexity theory, Sparse matrices, LBSNs, location-based social networks, mobile computing, trajectory representation learning model, location based social networks, Semantics, Trajectory representation learning, social networking (online), Data models, location-similarity based tasks, Trajectory, Labeling, learning (artificial intelligence)]
A Combinatorial Approach to Role Discovery
2016 IEEE 16th International Conference on Data Mining
None
2016
We provide a new formulation for the problem of role discovery in graphs. Our definition is structural: two vertices should be assigned to the same role if the roles of their neighbors, when viewed as multi-sets, are similar enough. An attractive characteristic of our approach is that it is based on optimizing a well-defined objective function, and thus, contrary to previous approaches, the role-discovery task can be studied with the tools of combinatorial optimization. We demonstrate that, when fixing the number of roles to be used, the proposed role-discovery problem is np-hard, while another (seemingly easier) version of the problem is np-hard to approximate. On the positive side, despite the recursive nature of our objective function, we can show that finding a perfect (zero-cost) role assignment with the minimum number of roles can be solved in polynomial time. We do this by connecting the zero-cost role assignment with the notion of equitable partition. For the more practical version of the problem with fixed number of roles we present two natural heuristic methods, and discuss how to make them scalable in large graphs.
[approximation theory, Heuristic algorithms, graph theory, Linear programming, set theory, Vehicle dynamics, Standards, graph mining, combinatorial optimization, optimisation, role mining, NP-hard problem, Clustering algorithms, combinatorial approach, role-discovery task, Cost function, polynomial time, computational complexity, objective function]
DESQ: Frequent Sequence Mining with Subsequence Constraints
2016 IEEE 16th International Conference on Data Mining
None
2016
Frequent sequence mining methods often make use of constraints to control which subsequences should be mined, e.g., length, gap, span, regular-expression, and hierarchy constraints. We show that many subsequence constraints-including and beyond those considered in the literature-can be unified in a single framework. In more detail, we propose a set of simple and intuitive "pattern expressions" to describe subsequence constraints and explore algorithms for efficiently mining frequent subsequences under such general constraints. A unified treatment allows researchers to study jointly many types of subsequence constraints (instead of each one individually) and helps to improve usability of pattern mining systems for practitioners.
[Context, DESQ, Computational modeling, pattern expressions, finite state transducers, data mining, pattern language, Electronic mail, Data mining, pattern mining systems, Analytical models, Databases, hierarchy constraints, pattern clustering, subsequence constraints, learning (artificial intelligence), Usability, frequent sequence mining, sequence mining]
EXTRACT: Strong Examples from Weakly-Labeled Sensor Data
2016 IEEE 16th International Conference on Data Mining
None
2016
Thanks to the rise of wearable and connected devices, sensor-generated time series comprise a large and growing fraction of the world's data. Unfortunately, extracting value from this data can be challenging, since sensors report low-level signals (e.g., acceleration), not the high-level events that are typically of interest (e.g., gestures). We introduce a technique to bridge this gap by automatically extracting examples of real-world events in low-level data, given only a rough estimate of when these events have taken place. By identifying sets of features that repeat in the same temporal arrangement, we isolate examples of such diverse events as human actions, power consumption patterns, and spoken words with up to 96% precision and recall. Our method is fast enough to run in real time and assumes only minimal knowledge of which variables are relevant or the lengths of events. Our evaluation uses numerous publicly available datasets and over 1 million samples of manually labeled sensor data.
[Shape, data analysis, Time series analysis, Semi-supervised learning, Transforms, Feature extraction, weakly-labeled sensor data, Data mining, Sparse matrices, Acceleration, Sensor data, EXTRACT]
A Theoretical Analysis of the Fuzzy K-Means Problem
2016 IEEE 16th International Conference on Data Mining
None
2016
One of the most popular fuzzy clustering techniques is the fuzzy K-means algorithm (also known as fuzzy-c-means or FCM algorithm). In contrast to the K-means and K-median problem, the underlying fuzzy K-means problem has not been studied from a theoretical point of view. In particular, there are no algorithms with approximation guarantees similar to the famous K-means++ algorithm known for the fuzzy K-means problem. This work initiates the study of the fuzzy K-means problem from an algorithmic and complexity theoretic perspective. We show that optimal solutions for the fuzzy K-means problem cannot, in general, be expressed by radicals over the input points. Surprisingly, this already holds for simple inputs in one-dimensional space. Hence, one cannot expect to compute optimal solutions exactly. We give the first (1+eps)-approximation algorithms for the fuzzy K-means problem. First, we present a deterministic approximation algorithm whose runtime is polynomial in N and linear in the dimension D of the input set, given that K is constant, i.e. a polynomial time approximation scheme (PTAS) for fixed K. We achieve this result by showing that for each soft clustering there exists a hard clustering with similar properties. Second, by using techniques known from coreset constructions for the K-means problem, we develop a deterministic approximation algorithm that runs in time almost linear in N but exponential in the dimension D. We complement these results with a randomized algorithm which imposes some natural restrictions on the sought solution and whose runtime is comparable to some of the most efficient approximation algorithms for K-means, i.e. linear in the number of points and the dimension, but exponential in the number of clusters.
[Algorithm design and analysis, Frequency modulation, fuzzy k-means, one-dimensional space, FCM algorithm, fuzzy set theory, K-median problem, soft clustering, Complexity theory, approximation algorithms, Data mining, Runtime, PTAS, Clustering algorithms, K-means++ algorithm, unsolvability by radicals, fuzzy K-means clustering algorithm, first (1+eps)-approximation algorithms, randomized algorithm, deterministic algorithms, run-time polynomial, hard clustering, pattern clustering, polynomial time approximation scheme, randomized algorithms, deterministic approximation algorithm, Approximation algorithms, clustering, probabilistic method, complexity theoretic perspective, coreset constructions, computational complexity]
Efficient Sampling-Based Kernel Mean Matching
2016 IEEE 16th International Conference on Data Mining
None
2016
Many real-world applications exhibit scenarios where distributions represented by training and test data are not similar, but related by a covariate shift, i.e., having equal class conditional distribution with unequal covariate distribution. Traditional data mining techniques suffer to learn a good predictive model in the presence of covariate shift. Recent studies have proposed approaches to address this challenge by weighing training instances based on density ratio between test and training data distributions. Kernel Mean Matching (KMM) is a well known method for estimating density ratio, but has time complexity cubic in the size of training data. Therefore, KMM is not suitable in real-world applications, especially in cases where the predictive model needs to be updated periodically with large training data. We address this challenge by taking fixed-size samples from training and test data, performing independent computations on these samples, and combining the results to obtain overall density ratio estimates. Our empirical evaluation demonstrates a large gain in execution time, while also achieving competitive accuracy on numerous benchmark datasets.
[sampling methods, estimation theory, Scalability, Bootstrap Aggregation, data mining, time complexity cubic, Covariate Shift, Data mining, statistical distributions, Density Ratio, covariate distribution, Training, sampling-based KMM, density ratio estimation, Memory management, Training data, data distributions, class conditional distribution, sampling-based kernel mean matching, covariate shift, Kernel, Time complexity, computational complexity]
DeBot: Twitter Bot Detection via Warped Correlation
2016 IEEE 16th International Conference on Data Mining
None
2016
We develop a warped correlation finder to identify correlated user accounts in social media websites such as Twitter. The key observation is that humans cannot be highly synchronous for a long duration, thus, highly synchronous user accounts are most likely bots. Existing bot detection methods are mostly supervised, which requires a large amount of labeled data to train, and do not consider cross-user features. In contrast, our bot detection system works on activity correlation without requiring labeled data. We develop a novel lag-sensitive hashing technique to cluster user accounts into correlated sets in near real-time. Our method, named DeBot, detects thousands of bots per day with a 94% precision and generates reports online everyday. In September 2016, DeBot has accumulated about 544,868 unique bots in the previous one year. We compare our detection technique with per-user techniques and with Twitter's suspension system. We observe that some bots can avoid Twitter's suspension mechanism and remain active for months, and, more alarmingly, we show that DeBot detects bots at a rate higher than the rate Twitter is suspending them.
[social media websites, activity correlation, Correlation, Conferences, synchronous user accounts, Automated accounts, correlated user accounts, Warping, Social Media, Twitter suspension system, Twitter, DeBot, Data mining, security of data, warped correlation, lag-sensitive hashing technique, file organisation, social networking (online), Bot Detection, Twitter bot detection]
Interpretable Clustering via Discriminative Rectangle Mixture Model
2016 IEEE 16th International Conference on Data Mining
None
2016
Clustering is a technique that is usually applied as a tool for exploratory data analysis. Because of the exploratory nature of this task, it would be beneficial if a clustering method generates interpretable results, and allows incorporating domain knowledge. This motivates us to develop a probabilistic discriminative model that learns a rectangular decision rule for each cluster, we call Discriminative Rectangle Mixture (DReaM) model. DReaM gives interpretable clustering results, because the rectangular decision rules discovered explicitly illustrate how one cluster is defined and differs from other clusters. It also facilitates us to take advantage of existing rules because we can choose informative prior distributions for the rectangular rules. Moreover, DReaM allows that the features for generating rules do not have to be the same as the features for discovering cluster structure. We approximate the distribution for the rules discovered via variational inference. Experimental results demonstrate that DReaM gives more interpretable clustering results, and yet its performance is comparable to existing clustering methods when solving traditional clustering. Furthermore, in real applications, DReaM is able to effectively take advantage of domain knowledge, and to generate reasonable clustering results.
[discriminative rectangle mixture model, data analysis, Clustering methods, interpretable clustering, Genomics, DReaM, Probabilistic logic, inference mechanisms, rectangular decision rule, exploratory data analysis, pattern clustering, Data models, Mathematical model, Decision trees, mixture models, probabilistic discriminative model, variational inference, Bioinformatics, semi-supervised clustering]
Asymptotic Analysis of Equivalences and Core-Structures in Kronecker-Style Graph Models
2016 IEEE 16th International Conference on Data Mining
None
2016
Growing interest in modeling large, complexnetworks has spurred significant research into generative graphmodels. Kronecker-style models (e.g. SKG and R-MAT) are oftenused due to their scalability and ability to mimic key propertiesof real-world networks. Although a few papers theoreticallyestablish these models' behavior for specific parameters, manyclaims used to justify their use are supported only empirically. In this work, we prove several results using asymptotic analysiswhich illustrate that empirical studies may not fully capture thetrue behavior of the models. Paramount to the widespread adoption of Kronecker-stylemodels was the introduction of a linear-time edge-samplingvariant (R-MAT), which existing literature typically treats asinterchangeable with SKG. We prove that although several R-MAT formulations are asymptotically equivalent, their behaviordiverges from that of SKG. Further, we show these resultsare observable even at relatively small graph sizes. Second, weconsider a case where asymptotic analysis reveals unexpectedbehavior within a given model.
[Limiting, sampling methods, degeneracy, random graph, Computational modeling, graph theory, Stochastic processes, equivalence asymptotic analysis, linear-time edge-sampling variant, Generators, Sparse matrices, Kronecker-style graph models, Kronecker, SKG, generative graph models, Analytical models, Data models, R-MAT]
Event Grounding from Multimodal Social Network Fusion
2016 IEEE 16th International Conference on Data Mining
None
2016
This paper studies the problem of extracting real world event information from social media streams. Although existing work focuses on event signals of bursty mentions extracted from a single-source of textual streams, these signals are likely to be noisy due to ambiguous occurrences of individual mentions. To extract accurate event signals, we propose a framework capable of "grounding" mentions to unique event using multiple social networks with complementary strength. We show that our framework jointly using multiple sources outperforms state-of-the-arts using publicly available datasets.
[Grounding, Event detection, Lattices, information retrieval, Metadata, Twitter, complementary strength, multimodality, multimodal social network fusion, social network, Flickr, information extraction, event detection, social networking (online), event grounding]
Can Active Learning Experience Be Transferred?
2016 IEEE 16th International Conference on Data Mining
None
2016
Active learning is an important machine learning problem in reducing the human labeling effort. Current active learning strategies are designed from human knowledge, and are applied on each dataset in an immutable manner. In other words, experience about the usefulness of strategies cannot be updated and transferred to improve active learning on other datasets. This paper initiates a pioneering study on whether active learning experience can be transferred. We first propose a novel active learning model that linearly aggregates existing strategies. The linear weights can then be used to represent the active learning experience. We equip the model with the popular linear upper-confidence-bound (LinUCB) algorithm for contextual bandit to update the weights. Finally, we extend our model to transfer the experience across datasets with the technique of biased regularization. Empirical studies demonstrate that the learned experience not only is competitive with existing strategies on most single datasets, but also can be transferred across datasets to improve the performance on future learning tasks.
[Algorithm design and analysis, Context, Uncertainty, human labeling effort, human knowledge, LinUCB, Probabilistic logic, datasets, linear upper-confidence-bound algorithm, machine learning problem, biased regularization, data handling, Labeling, learning (artificial intelligence), active learning experience, Joining processes, Context modeling]
Outlier Detection from Network Data with Subnetwork Interpretation
2016 IEEE 16th International Conference on Data Mining
None
2016
Detecting a small number of outliers from a set of data observations is always challenging. This problem is more difficult in the setting of multiple network samples, where computing the anomalous degree of a network sample is generally not sufficient. In fact, explaining why a given network is exceptional, expressed in the form of subnetwork, is also equally important. We develop a novel algorithm to address these two key problems. We treat each network sample as a potential outlier and identify subnetworks that help discriminate it from nearby samples. The algorithm is developed in the framework of network regression combined with the constraints on both network topology and L1-norm shrinkage to perform subnetwork discovery. Our method thus goes beyond subspace/subgraph discovery. We also show that the developed method converges to a global optimum. Empirical evaluation on various real-world network datasets demonstrates the advantages of our algorithm over various baseline methods.
[Algorithm design and analysis, multiple network samples, graph theory, data mining, L1-norm shrinkage, regression analysis, Network mining, Linear programming, Topology, network data, network topology, subspace-subgraph discovery, outlier detection, Outlier interpretation, subnetwork discovery, Diseases, Support vector machines, Network topology, Databases, network regression framework, data observations, Outlier Detection, subnetwork interpretation]
Incorporating Expert Feedback into Active Anomaly Discovery
2016 IEEE 16th International Conference on Data Mining
None
2016
Unsupervised anomaly detection algorithms search for outliers and then predict that these outliers are the anomalies. When deployed, however, these algorithms are often criticized for high false positive and high false negative rates. One cause of poor performance is that not all outliers are anomalies and not all anomalies are outliers. In this paper, we describe an Active Anomaly Discovery (AAD) method for incorporating expert feedback to adjust the anomaly detector so that the outliers it discovers are more in tune with the expert user's semantic understanding of the anomalies. The AAD approach is designed to operate in an interactive data exploration loop. In each iteration of this loop, our algorithm first selects a data instance to present to the expert as a potential anomaly and then the expert labels the instance as an anomaly or as a nominal data point. Our algorithm updates its internal model with the instance label and the loop continues until a budget of B queries is spent. The goal of our approach is to maximize the total number of true anomalies in the B instances presented to the expert. We show that when compared to other state-of-the-art algorithms, AAD is consistently one of the best performers.
[Algorithm design and analysis, expert systems, AAD, expert feedback, anomaly detector, Linear programming, expert labels, Anomaly detection, active anomaly discovery, interactive data exploration loop, active learning, security of data, Detectors, Feature extraction, expert user semantic, Data models, data handling, user feedback, Mathematical model, Detection algorithms, unsupervised anomaly detection]
Spell: Streaming Parsing of System Event Logs
2016 IEEE 16th International Conference on Data Mining
None
2016
System event logs have been frequently used as a valuable resource in data-driven approaches to enhance system health and stability. A typical procedure in system log analytics is to first parse unstructured logs, and then apply data analysis on the resulting structured data. Previous work on parsing system event logs focused on offline, batch processing of raw log files. But increasingly, applications demand online monitoring and processing. We propose an online streaming method Spell, which utilizes a longest common subsequence based approach, to parse system event logs. We show how to dynamically extract log patterns from incoming logs and how to maintain a set of discovered message types in streaming fashion. Evaluation results on large real system logs demonstrate that even compared with the offline alternatives, Spell shows its superiority in terms of both efficiency and effectiveness.
[Printing, Data analysis, unstructured log parsing, data analysis, longest common subsequence based approach, Conferences, dynamic log pattern extraction, system event log parsing, online streaming method, Stability analysis, Data mining, Spell, grammars, Batch production systems, data-driven approach, tree data structures, system log analytics, Monitoring, streaming parsing]
Modeling Time Lags in Citation Networks
2016 IEEE 16th International Conference on Data Mining
None
2016
The extant work on network analyses has thus far paid little attention to the heterogeneity in time lags and speed of information propagation along edges. In this paper, we study this novel problem, modeling the time dimension and lags on network edges, in the context of paper and patent citation networks where the variation in the speed of knowledge flows between connected nodes is apparent. We propose to model time lags in knowledge diffusions in citation networks in one of the two ways: deterministic lags and probabilistic lags. Then, we discuss two approaches of computationally working with time lags in edges of citation networks. Experimentally, we study two different applications to demonstrate the importance of the time dimension and lags in citations: (1) HITS algorithm and (2) patent citation recommendation. We conduct experiments on millions of U. S. patent data and Web of Science (WOS) paper data. Our experiments show that incorporating time dimension and lags in edges significantly improve network modeling and analyses.
[Knowledge engineering, US patent data, Art, HITS algorithm, patent citation recommendation, network modeling improvement, network edges, Network modeling, information filtering, knowledge flows, patent citation networks, probabilistic lags, Analytical models, time dimension modeling, Aging, citation analysis, patents, information propagation, Patents, WOS paper data, Computational modeling, Web of Science paper data, Probabilistic logic, recommender systems, time lag modeling, Time dimension, scientific information systems, network analysis, Knowledge diffusion, deterministic lags]
Efficient and Distributed Algorithms for Large-Scale Generalized Canonical Correlations Analysis
2016 IEEE 16th International Conference on Data Mining
None
2016
Generalized canonical correlation analysis (GCCA) aims at extracting common structure from multiple 'views', i.e., high-dimensional matrices representing the same objects in different feature domains - an extension of classical two-view CCA. Existing (G)CCA algorithms have serious scalability issues, since they involve square root factorization of the correlation matrices of the views. The memory and computational complexity associated with this step grow as a quadratic and cubic function of the problem dimension (the number of samples / features), respectively. To circumvent such difficulties, we propose a GCCA algorithm whose memory and computational costs scale linearly in the problem dimension and the number of nonzero data elements, respectively. Consequently, the proposed algorithm can easily handle very large sparse views whose sample and feature dimensions both exceed 100,000 - while the current approaches can only handle thousands of features / samples. Our second contribution is a distributed algorithm for GCCA, which computes the canonical components of different views in parallel and thus can further reduce the runtime significantly (by &#x2265; 30% in experiments) if multiple cores are available. Judiciously designed synthetic and real-data experiments using a multilingual dataset are employed to showcase the effectiveness of the proposed algorithms.
[Algorithm design and analysis, multilingual word embeddings, Correlation, Complexity theory, Sparse matrices, computational costs, distributed algorithm, generalized canonical correlations analysis, distributed algorithms, GCCA algorithm, Feature extraction, Approximation algorithms, distributed GCCA, Lagre-scale generalized canonical correlation analysis, Distributed algorithms, computational complexity, correlation methods]
Service Usage Analysis in Mobile Messaging Apps: A Multi-label Multi-view Perspective
2016 IEEE 16th International Conference on Data Mining
None
2016
The service usage analysis, aiming at identifying customers' messaging behaviors based on encrypted App traffic flows, has become a challenging and emergent task for service providers. Prior literature usually starts from segmenting a traffic sequence into single-usage subsequences, and then classify the subsequences into different usage types. However, they could suffer from inaccurate traffic segmentations and mixed-usage subsequences. To address this challenge, we exploit a multi-label multi-view learning strategy and develop an enhanced frame-work for in-App usage analytics. Specifically, we first devise an enhanced traffic segmentation method to reduce mixed-usage sub-sequences. Besides, we develop a multi-label multi-view logistic classification method, which comprises two alignments. The first alignment is to make use of the classification consistency between packet-length view and time-delay view of traffic subsequences and improve classification accuracy. The second alignment is to combine the classification of single-usage subsequence and the post-classification of mixed-usage subsequences into a unified multi-label logistic classification problem. Finally, we present extensive experiments with real-world datasets to demonstrate the effectiveness of our approach.
[electronic messaging, traffic sequence segmentation, Mobile communication, classification accuracy, Data mining, service usage analysis, mobile messaging apps, mixed-usage subsequence post-classification, mobile computing, mixed-usage subsequence reduction, single-usage subsequence classification, Clustering algorithms, multilabel multiview logistic classification method, learning (artificial intelligence), pattern classification, encrypted App traffic flows, enhanced traffic segmentation method, customer messaging behaviors, multilabel multiview learning strategy, unified multilabel logistic classification problem, in-App usage analytics, time-delay view, Data collection, packet-length view, Feature extraction, Internet, Logistics]
A Semi-Supervised AUC Optimization Method with Generative Models
2016 IEEE 16th International Conference on Data Mining
None
2016
This paper presents a semi-supervised learning method for improving the performance of AUC-optimized classifiers by using both labeled and unlabeled samples. In actual binary classification tasks, there is often an imbalance between the numbers of positive and negative samples. For such imbalanced tasks, the area under the ROC curve (AUC) is an effective measure with which to evaluate binary classifiers. The proposed method utilizes generative models to assist the incorporation of unlabeled samples in AUC-optimized classifiers. The generative models provide prior knowledge that helps learn the distribution of unlabeled samples. To evaluate the proposed method in text classification, we employed naive Bayes models as the generative models. Our experimental results using three test collections confirmed that the proposed method provided better classifiers for imbalanced tasks than supervised AUC-optimized classifiers and semi-supervised classifiers trained to maximize the classification accuracy of labeled samples. Moreover, the proposed method improved the effect of using unlabeled samples for AUC optimization especially when we used appropriate generative models.
[text analysis, semi-supervised learning method, Parameter estimation, Computational modeling, semi-supervised AUC optimization method, Optimization methods, Probability distribution, text classification, optimisation, naive Bayes models, Semisupervised learning, binary classification tasks, Data models, Bayes methods, learning (artificial intelligence)]
MeGS: Partitioning Meaningful Subgraph Structures Using Minimum Description Length
2016 IEEE 16th International Conference on Data Mining
None
2016
How can we fully structure a graph into pieces of meaningful information? Into structures that provide us with insights and carry a meaning beyond simple clustering. How can we also exploit these patterns to compress the graph for fast transmission and easier storage? In many applications of graph analysis like network analysis or medical information extraction we are searching for special patterns. Here, it is not sufficient to extract only parts of the relevant information in a graph, but to understand the complete underlying structure. Therefore, we propose our algorithm MeGS (Partitioning Meaningful Subgraph Structures using Minimum Description Length) to fully understand how a graph is constructed. The most common primitives (clique, hub, tree, bipartite, and sparse) serve as models to split a graph into meaningful structures. Using the principle of Minimum Description Length (MDL) structure types and counts are determined by the best fitting model. These structures achieve the best compression of the adjacency matrix. As result, every node is part of exactly one structure and has an interpretable context. No unknown areas remain in the graph. The higher a model compresses its section of the graph, the stronger its match with the corresponding structural assumption. MeGS, a fast and parameter-free split-and-merge algorithm, automatically finds the optimal structures achieving the best compression. We compare to state-of-the-art algorithms to prove MeGS' ability for interpretation and compression.
[MeGS, graph theory, data mining, medical information extraction, Entropy, best fitting model, adjacency matrix compression, simple clustering, Data mining, MDL structure types, meaningful subgraph structure partitioning algorithm, minimum description length, Graph Mining, data compression, parameter-free split-and-merge algorithm, graph analysis, Information retrieval, Partitioning algorithms, Channel coding, matrix algebra, information extraction, pattern clustering, Vegetation, Minimum Description Length, network analysis]
Probabilistic Formulations of Regression with Mixed Guidance
2016 IEEE 16th International Conference on Data Mining
None
2016
Regression problems assume every instance is annotated(labeled) with a real value, a form of annotation we call strong guidance. In order for these annotations to be accurate, they must be the result of a precise experiment or measurement. However, in some cases additional weak guidance might be given by imprecise measurements, a domain expert or even crowd sourcing. Current formulations of regression are unable to use both types of guidance. We propose a regression framework that can also incorporate weak guidance based on relative orderings, bounds, neighboring and similarity relations. Consider learning to predict ages from portrait images, these new types of guidance allow weaker forms of guidance such as stating a person is in their 20s or two people are similar in age. These types of annotations can be easier to generate than strong guidance. We introduce a probabilistic formulation for these forms of weak guidance and show that the resulting optimization problems are convex. Our experimental results show the benefits of these formulations on several data sets.
[Maximum likelihood estimation, crowd sourcing, mixed guidance, data mining, regression analysis, Regression, Probabilistic logic, Weak Guidance, Standards, Optimization, optimisation, weak guidance, regression probabilistic formulations, Random variables, Mathematical model, optimization problems, Logistics]
HLGPS: A Home Location Global Positioning System in Location-Based Social Networks
2016 IEEE 16th International Conference on Data Mining
None
2016
The rapid spread of mobile internet and location-acquisition technologies have led to the increasing popularity of Location-Based Social Networks(LBSNs). Users in LBSNs can share their life by checking in at various venues at any time. In LBSNs, identifying home locations of users is significant for effective location-based services like personalized search, targeted advertisement, local recommendation and so on. In this paper, we propose a Home Location Global Positioning System called HLGPS to tackle with the home location identification problem in LBSNs. Firstly, HLGPS uses an influence model named as IME to model edges in LBSNs. Then HLGPS uses a global iteration algorithm based on IME model to position home location of users so that the joint probability of generating all the edges in LBSNs is maximum. Extensive experiments on a large real-world LBSN dataset demonstrate that HLGPS significantly outperforms state-of-the-art methods by 14.7%.
[iterative methods, Social network services, Atmospheric modeling, Urban areas, global iteration algorithm, Location-Based Social Networks, Data mining, location-based social networks, home location global positioning system, Global Positioning System, Home Location Identification, LBSN, home location identification problem, Social Networks, social networking (online), Data models, location-based services, Mathematical model, HLGPS, Influence Model]
Large-Scale Embedding Learning in Heterogeneous Event Data
2016 IEEE 16th International Conference on Data Mining
None
2016
Heterogeneous events, which are defined as events connecting strongly-typed objects, are ubiquitous in the real world. We propose a HyperEdge-Based Embedding (Hebe) framework for heterogeneous event data, where a hyperedge represents the interaction among a set of involving objects in an event. The Hebe framework models the proximity among objects in an event by predicting a target object given the other participating objects in the event (hyperedge). Since each hyperedge encapsulates more information on a given event, Hebe is robust to data sparseness. In addition, Hebe is scalable when the data size spirals. Extensive experiments on large-scale real-world datasets demonstrate the efficacy and robustness of Hebe.
[Context, Hebe, Noise Pairwise Ranking, data sparseness, data size spirals, Predictive models, Embedding, heterogeneous event data, Optimization, hyperedge-based embedding framework, large-scale real-world datasets, Heterogeneous Event Data, Robustness, Data models, data handling, learning (artificial intelligence), strongly-typed objects, large-scale embedding learning, Large Scale, Business, Context modeling, Hyperedge]
Direct Mining of Subjectively Interesting Relational Patterns
2016 IEEE 16th International Conference on Data Mining
None
2016
Data is typically complex and relational. Therefore, the development of relational data mining methods is an increasingly active topic of research. Recent work has resulted in new formalisations of patterns in relational data and in a way to quantify their interestingness in a subjective manner, taking into account the data analyst's prior beliefs about the data. Yet, a scalable algorithm to find such most interesting patterns is lacking. We introduce a new algorithm based on two notions: (1) the use of Constraint Programming, which results in a notably shorter development time, faster runtimes, and more flexibility for extensions such as branch-and-bound search, and (2), the direct search for the most interesting patterns only, instead of exhaustive enumeration of patterns before ranking them. Through empirical evaluation, we find that our novel bounds yield speedups up to several orders of magnitude, especially on dense data with a simple schema. This makes it possible to mine the most subjectively-interesting relational patterns present in databases where this was previously impractical or impossible.
[Algorithm design and analysis, direct search, data analysis, data mining, Relational databases, Programming, Data mining, tree searching, relational data patterns, Itemsets, pattern clustering, branch-and-bound search, Motion pictures, direct mining, relational data mining methods, constraint handling, constraint programming, subjectively interesting relational patterns]
Semi-Supervised Multi-label Dimensionality Reduction
2016 IEEE 16th International Conference on Data Mining
None
2016
Multi-label data with high dimensionality arise frequently in data mining and machine learning. It is not only time consuming but also computationally unreliable when we use high-dimensional data directly. Supervised dimensionality reduction approaches are based on the assumption that there are large amounts of labeled data. It is infeasible to label a large number of training samples in practice especially in multi-label learning. To address these challenges, we propose a novel algorithm, namely Semi-Supervised Multi-Label Dimensionality Reduction (SSMLDR), which can utilize the information from both labeled data and unlabeled data in an effective way. First, the proposed algorithm enlarges the multi-label information from the labeled data to the unlabeled data through a special designed label propagation method. It then learns a transformation matrix to perform dimensionality reduction by incorporating the enlarged multi-label information. Extensive experiments on a broad range of datasets validate the effectiveness of our approach against other well-established algorithms.
[Algorithm design and analysis, Correlation, semi-supervised, multi-label linear discriminant analysis, data mining, Data mining, Convergence, multilabel information, multi-label, Training, label propagation method, dimensionality reduction, SSMLDR, multi-label label propagation, transformation matrix, Prediction algorithms, Eigenvalues and eigenfunctions, unlabeled data, learning (artificial intelligence), semisupervised multilabel dimensionality reduction]
A Novel Uncertainty Sampling Algorithm for Cost-Sensitive Multiclass Active Learning
2016 IEEE 16th International Conference on Data Mining
None
2016
Active learning is a setup that allows the learning algorithm to iteratively and strategically query the labels of some instances for reducing human labeling efforts. One fundamental strategy, called uncertainty sampling, measures the uncertainty of each instance when making querying decisions. Traditional active learning algorithms focus on binary or multiclass classification, but few works have studied active learning for cost-sensitive multiclass classification (CSMCC), which allows charging different costs for different types of misclassification errors. The few works are generally based on calculating the uncertainty of each instance by probability estimation, and can suffer from the inaccuracy of the estimation. In this paper, we propose a novel active learning algorithm that relies on a different way of calculating the uncertainty. The algorithm is based on our newly-proposed cost embedding approach (CE) for CSMCC. CE embeds the cost information in the distance measure of a special hidden space with non-metric multidimensional scaling, and deals with both symmetric and asymmetric cost information by our carefully designed mirroring trick. The embedding allows the proposed algorithm, active learning with cost embedding (ALCE), to define a cost-sensitive uncertainty measure from the distance in the hidden space. Extensive experimental results demonstrate that ALCE selects more useful instances by taking the cost information into account through the embedding and is superior to existing cost-sensitive active learning algorithms.
[Algorithm design and analysis, probability estimation, Uncertainty, instance label query, nonmetric multidimensional scaling, query processing, cost-sensitive uncertainty measure, ALCE, multiclass classification, uncertainty sampling algorithm, Prediction algorithms, learning (artificial intelligence), binary classification, symmetric cost information, querying decision making, pattern classification, Symmetric matrices, sampling methods, CE, Estimation, probability, active learning with cost embedding, cost-sensitive multiclass classification, Extraterrestrial measurements, CSMCC, asymmetric cost information, Measurement uncertainty, cost embedding approach, cost-sensitive multiclass active learning algorithm]
SOAL: Second-Order Online Active Learning
2016 IEEE 16th International Conference on Data Mining
None
2016
This paper investigates the problem of online active learning for training classification models from sequentially arriving data. This is more challenging than conventional online learning tasks since the learner not only needs to figure out how to effectively update the classifier but also needs to decide when is the best time to query the label of an incoming instance given limited label budget. The existing online active learning approaches are often based on first-order online learning methods which generally fall short in slow convergence rate and sub-optimal exploitation of available information when querying the labeled data. To overcome the limitations, in this paper, we present a new framework of Second-order Online Active Learning (SOAL), which fully exploits both first-order and second-order information to achieve high learning accuracy with low labeling cost. We conduct both theoretical analysis and empirical studies for evaluating the proposed SOAL algorithm extensively. The encouraging results show clear advantages of the proposed algorithm over a family of state-of-the-art online active learning algorithms.
[Algorithm design and analysis, SOAL algorithm, pattern classification, Machine learning algorithms, classification models, Computational modeling, sub-optimal exploitation, Training, Learning systems, slow convergence rate, active learning, first-order online learning methods, second-order online active learning, Labeling, Mathematical model, learning (artificial intelligence), online learning]
Learning Compatibility Across Categories for Heterogeneous Item Recommendation
2016 IEEE 16th International Conference on Data Mining
None
2016
Identifying relationships between items is a key task of an online recommender system, in order to help users discover items that are functionally complementary or visually compatible. In domains like clothing recommendation, this task is particularly challenging since a successful system should be capable of handling a large corpus of items, a huge amount of relationships among them, as well as the high-dimensional and semantically complicated features involved. Furthermore, the human notion of "compatibility" to capture goes beyond mere similarity: For two items to be compatible-whether jeans and a t-shirt, or a laptop and a charger-they should be similar in some ways, but systematically different in others. In this paper we propose a novel method, Monomer, to learn complicated and heterogeneous relationships between items in product recommendation settings. Recently, scalable methods have been developed that address this task by learning similarity metrics on top of the content of the products involved. Here our method relaxes the metricity assumption inherent in previous work and models multiple localized notions of 'relatedness,' so as to uncover ways in which related items should be systematically similar, and systematically different. Quantitatively, we show that our system achieves state-of-the-art performance on large-scale compatibility prediction tasks, especially in cases where there is substantial heterogeneity between related items.
[large-scale compatibility prediction tasks, Visualization, Recommender Systems, learning compatibility, Computational modeling, product recommendation settings, Metric Learning, online recommender system, metricity assumption, Extraterrestrial measurements, Training, heterogeneous item recommendation, recommender systems, Visual Compatibility, Data models, learning (artificial intelligence), Monomer, Recommender systems]
HNP3: A Hierarchical Nonparametric Point Process for Modeling Content Diffusion over Social Media
2016 IEEE 16th International Conference on Data Mining
None
2016
This paper introduces a novel framework for modeling temporal events with complex longitudinal dependency that are generated by dependent sources. This framework takes advantage of multidimensional point processes for modeling time of events. The intensity function of the proposed process is a mixture of intensities, and its complexity grows with the complexity of temporal patterns of data. Moreover, it utilizes a hierarchical dependent nonparametric approach to model marks of events. These capabilities allow the proposed model to adapt its temporal and topical complexity according to the complexity of data, which makes it a suitable candidate for real world scenarios. An online inference algorithm is also proposed that makes the framework applicable to a vast range of applications. The framework is applied to a real world application, modeling the diffusion of contents over networks. Extensive experiments reveal the effectiveness of the proposed framework in comparison with state-of-the-art methods.
[Adaptation models, modeling content diffusion, Social network services, Computational modeling, Social Media, HNP3, Complexity theory, History, inference mechanisms, multidimensional point processes, Dependent Nonparametric Models, Content Diffusion, social networking (online), Hawkes Process, Data models, Inference algorithms, online inference algorithm, Document Modeling, social media, Point Process, hierarchical nonparametric point process, computational complexity]
Improved and Scalable Bradley-Terry Model for Collaborative Ranking
2016 IEEE 16th International Conference on Data Mining
None
2016
In collaborative ranking, the Bradley-Terry (BT) model is widely used for modeling pairwise user preferences. However, when this model is combined with matrix factorization on sparsely observed ratings, a challenging identifiability issue arises since the optimization will involve non-convex constraints. Besides, in some situations, fitting the Bradley-Terry model yields a numerical challenge as it may include an objective function that is unbounded from below. In this paper, we will discuss and develop a simple strategy to resolve these issues. Specifically, we propose an Improved-BT model by adding a penalty term, and we develop two parallel algorithms to make Improved-BT model scalable. Through extensive experiments on benchmark datasets, we show that our proposed method outperforms many considered state-of-the-art collaborative ranking approaches in terms of both ranking performance and time efficiency.
[collaborative filtering, parallel algorithms, matrix factorization, Computational modeling, identifiability issue, Predictive models, Linear programming, improved-BT model, matrix decomposition, Sparse matrices, Optimization, pairwise user preference modeling, collaborative ranking, nonconvex constraints, Collaboration, improved scalable Bradley-Terry model, Numerical models, benchmark testing, concave programming, benchmark datasets]
Sparse Gaussian Markov Random Field Mixtures for Anomaly Detection
2016 IEEE 16th International Conference on Data Mining
None
2016
We propose a new approach to anomaly detection from multivariate noisy sensor data. We address two major challenges: To provide variable-wise diagnostic information and to automatically handle multiple operational modes. Our task is a practical extension of traditional outlier detection, which is to compute a single scalar for each sample. To consistently define the variable-wise anomaly score, we leverage a predictive conditional distribution. We then introduce a mixture of Gaussian Markov random field and its Bayesian inference, resulting in a sparse mixture of sparse graphical models. Our anomaly detection method is capable of automatically handling multiple operational modes while removing unwanted nuisance variables. We demonstrate the utility of our approach using real equipment data from the oil industry.
[Gaussian Markov random field, Gaussian distribution, sensor fusion, anomaly detection, variational Bayes, Data mining, predictive conditional distribution, outlier detection, variable-wise diagnostic information, sparse Gaussian Markov random field mixtures, sparse graphical models, unwanted nuisance variables, Mathematical model, Bayesian inference, oil industry, random processes, multivariate noisy sensor data, variable-wise anomaly score, Noise measurement, Markov random fields, automatic multiple operational mode handling, Gaussian processes, Markov processes, Bayes methods, data handling, mixture models, real equipment data, Gaussian mixtures]
A Robust Framework for Classifying Evolving Document Streams in an Expert-Machine-Crowd Setting
2016 IEEE 16th International Conference on Data Mining
None
2016
An emerging challenge in the online classification of social media data streams is to keep the categories used for classification up-to-date. In this paper, we propose an innovative framework based on an Expert-Machine-Crowd (EMC) triad to help categorize items by continuously identifying novel concepts in heterogeneous data streams often riddled with outliers. We unify constrained clustering and outlier detection by formulating a novel optimization problem: COD-Means. We design an algorithm to solve the COD-Means problem and show that COD-Means will not only help detect novel categories but also seamlessly discover human annotation errors and improve the overall quality of the categorization process. Experiments on diverse real data sets demonstrate that our approach is both effective and efficient.
[Algorithm design and analysis, text analysis, item categorization, Taxonomy, online classification, novel concept detection, Electromagnetic compatibility, text classification, outlier detection, Optimization, optimisation, Clustering algorithms, COD-means problem, Labeling, social media, constrained clustering, pattern classification, Social network services, expert-machine-crowd setting, stream classification, pattern clustering, social networking (online), human annotation error discovery, social media data streams, heterogeneous data streams]
Learning Deep Networks from Noisy Labels with Dropout Regularization
2016 IEEE 16th International Conference on Data Mining
None
2016
Large datasets often have unreliable labels-such as those obtained from Amazon's Mechanical Turk or social media platforms-and classifiers trained on mislabeled datasets often exhibit poor performance. We present a simple, effective technique for accounting for label noise when training deep neural networks. We augment a standard deep network with a softmax layer that models the label noise statistics. Then, we train the deep network and noise model jointly via end-to-end stochastic gradient descent on the (perhaps mislabeled) dataset. The augmented model is underdetermined, so in order to encourage the learning of a non-trivial noise model, we apply dropout regularization to the weights of the noise model during training. Numerical experiments on noisy versions of the CIFAR-10 and MNIST datasets show that the proposed dropout technique outperforms state-of-the-art methods.
[dropout regularization, Computational modeling, image classification, Stochastic processes, end-to-end stochastic gradient descent, Convolutional Neural Networks, Noise measurement, Standards, softmax layer, Deep Learning, Training, Supervised Learning, Dropout Regularization, deep neural network training, Label Noise, Neural networks, label noise statistics, MNIST datasets, CIFAR-10 datasets, Numerical models, learning (artificial intelligence), gradient methods, deep network learning, neural nets]
Personalized Ranking in Signed Networks Using Signed Random Walk with Restart
2016 IEEE 16th International Conference on Data Mining
None
2016
How can we rank users in signed social networks? Relationships between nodes in a signed network are represented as positive (trust) or negative (distrust) edges. Many social networks have adopted signed networks to express trust between users. Consequently, ranking friends or enemies in signed networks has received much attention from the data mining community. The ranking problem, however, is challenging because it is difficult to interpret negative edges. Traditional random walk based methods such as PageRank and Random Walk with Restart cannot provide effective rankings in signed networks since they assume only positive edges. Although several methods have been proposed by modifying traditional ranking models, they also fail to account for proper rankings due to the lack of ability to consider complex edge relations. In this paper, we propose Signed Random Walk with Restart, a novel model for personalized ranking in signed networks. We introduce a signed random surfer so that she considers negative edges by changing her sign for walking. Our model provides proper rankings reflecting signed edges based on the signed surfer. Through extensive experiments, we demonstrate that SRWR achieves the best accuracy (up to 87%) for sign prediction, and predicts trolls 4&#x00D7; more accurately than other ranking models.
[personalized ranking, Computational modeling, Social network services, Signed random walk with restart, signed random walk, Data mining, Computer science, data mining community, social networking (online), Personalized ranking in signed networks, Nickel, Iterative methods, Mathematical model, SRWR, signed networks]
ExploreKit: Automatic Feature Generation and Selection
2016 IEEE 16th International Conference on Data Mining
None
2016
Feature generation is one of the challenging aspects of machine learning. We present ExploreKit, a framework for automated feature generation. ExploreKit generates a large set of candidate features by combining information in the original features, with the aim of maximizing predictive performance according to user-selected criteria. To overcome the exponential growth of the feature space, ExploreKit uses a novel machine learning-based feature selection approach to predict the usefulness of new candidate features. This approach enables efficient identification of the new features and produces superior results compared to existing feature selection solutions. We demonstrate the effectiveness and robustness of our approach by conducting an extensive evaluation on 25 datasets and 3 different classification algorithms. We show that ExploreKit can achieve classification-error reduction of 20% overall. Our codeis available at https://github.com/giladkatz/ExploreKit.
[ExploreKit, classification algorithms, pattern classification, Machine learning algorithms, predictive performance maximization, Pregnancy, Analytical models, Machine learning, machine learning-based feature selection, Prediction algorithms, user-selected criteria, Diabetes, Space exploration, classification-error reduction, learning (artificial intelligence), automatic feature selection, feature selection, automatic feature generation]
Steering Social Media Promotions with Effective Strategies
2016 IEEE 16th International Conference on Data Mining
None
2016
On social media platforms, companies, organizations and individuals are using the function of sharing or retweeting information to promote their products, policies, and ideas. While a growing body of research has focused on identifying the promoters from millions of users, the promoters themselves are seeking to know what strategies can improve promotional effectiveness, which is rarely studied in literature. In this work, we study a new problem of promotional strategy effect estimation which is challenging in identifying and quantifying promotional strategies, as well as estimating effectiveness of promotional strategies with selection bias in observational data. Here we study a series of strategies on both context and content levels. To alleviate the selection bias issue, we propose a method based on Propensity Score Matching (PSM) to evaluate the effect of each promotional strategy. Our data study provides three interpretable and insightful ideas on steering social media promotions, including (1) three significant and stable strategies, (2) a critical trade-off, and (3) different concerns for promoters of different popularity. These results provided comprehensive suggestions to the practitioners to steer social media promotions with effective strategies.
[Context, Computer science, propensity score matching, promotional strategy effect estimation, Estimation, Companies, social media promotions, Twitter, social networking (online), marketing data processing, Data mining]
Mining Statistically Significant Attribute Associations in Attributed Graphs
2016 IEEE 16th International Conference on Data Mining
None
2016
Graphs are widely used to represent many different kinds of real world data such as social networks, protein-protein interactions, and road networks. In many cases, each node in a graph is associated with a set of its attributes and it is critical to not only consider the link structure of a graph but also use the attribute information to achieve more meaningful results in various graph mining tasks. Most previous works dealing with attributed graphs take into account attribute relationships only between individually connected nodes. However, it should be greatly valuable to find out which sets of attributes are associated with each other and whether or not they are statistically significant over an entire graph. Mining such significant associations, we can uncover novel relationships among the sets of attributes in the graph. We propose an algorithm that can find those attribute associations efficiently and effectively, and show experimental results that confirm the high efficacy of the proposed algorithm.
[Social network services, Conferences, graph mining tasks, graph theory, data mining, Probability, link structure, Attribute associations, Social network, statistically significant attribute association mining, Electronic mail, Data mining, Computer science, Proteins, Attributed Graphs, attributed graphs, statistical analysis, Graph mining]
Time-Aware User Identification with Topic Models
2016 IEEE 16th International Conference on Data Mining
None
2016
Accounts are often shared by multiple users, each of them having different item consumption and temporal habits. Identifying of the active user can lead to improvements in a variety of services by switching from account personalized services to user personalized services. To do so, we develop a topic model extending the Latent Dirichlet Allocation using a hidden variable representing the active user and assuming consumption times to be generated by latent time topics. We create a new dataset of composite accounts from real users to test the identification capabilities of our model. We show that our model is able to learn temporal patterns from the whole set of accounts and infer the active user using both the consumption time and the consumed item.
[latent Dirichlet allocation, Parameter estimation, TV, temporal habits, Biological system modeling, Twitter, composite accounts, History, time-aware user identification, Shared accounts, IP-TV recommendation, Motion pictures, topic models, User profile decomposition, data handling, Resource management, active user identification, User identification, item consumption]
Toward Time-Evolving Feature Selection on Dynamic Networks
2016 IEEE 16th International Conference on Data Mining
None
2016
Recent years have witnessed the prevalence of networked data in various domains. Among them, a large number of networks are not only topologically structured but also have a rich set of features on nodes. These node features are usually of high dimensionality with noisy, irrelevant and redundant information, which may impede the performance of other learning tasks. Feature selection is useful to alleviate these critical issues. Nonetheless, a vast majority of existing feature selection algorithms are predominantly designed in a static setting. In reality, real-world networks are naturally dynamic, characterized by both topology and content changes. It is desirable to capture these changes to find relevant features tightly hinged with network structure continuously, which is of fundamental importance for many applications such as disaster relief and viral marketing. In this paper, we study a novel problem of time-evolving feature selection for dynamic networks in an unsupervised scenario. Specifically, we propose a TeFS framework by leveraging the temporal evolution property of dynamic networks to update the feature selection results incrementally. Experimental results show the superiority of TeFS over the state-of-the-art batch-mode unsupervised feature selection algorithms.
[Heuristic algorithms, time-evolving feature selection, Linear programming, Data mining, Noise measurement, unsupervised scenario, information networks, Optimization, unsupervised learning, noisy information, TeFS framework, content changes, Network topology, irrelevant information, topology changes, dynamic networks, Feature extraction, redundant information, learning performance, high dimensionality node features, feature selection, temporal evolution property]
Concept Based Short Text Stream Classification with Topic Drifting Detection
2016 IEEE 16th International Conference on Data Mining
None
2016
Short text stream classification is a challenging and significant task due to the characteristics of short length, weak signal, high velocity and especially topic drifting in short text stream. However, this challenge has received little attention from the research community. Motivated by this, we propose a new feature extension approach for short text stream classification using a large scale, general purpose semantic network obtained from a web corpus. Our approach is built on an incremental ensemble classification model. First, in terms of the open semantic network, we introduce more semantic contexts in short texts to make up of the data sparsity. Meanwhile, we disambiguate terms by their semantics to reduce the noise impact. Second, to effectively track hidden topic drifts, we propose a concept cluster based topic drifting detection method. Finally, extensive experiments demonstrate that our approach can detect topic drifts effectively compared to several well-known concept drifting detection methods in data streams. Meanwhile, our approach can perform best in the classification of text data streams compared to several state-of-the-art short text classification approaches.
[Context, general purpose semantic network, research community, text analysis, concept cluster, Taxonomy, Web corpus, Europe, topic drift, open semantic network, text data streams, Electronic mail, topic drifting detection, feature extension approach, classification, short text stream, short text stream classification, pattern clustering, Semantics, Clustering algorithms, data sparsity, Mathematical model]
Regularized Large Margin Distance Metric Learning
2016 IEEE 16th International Conference on Data Mining
None
2016
Distance metric learning plays an important role in many applications, such as classification and clustering. In this paper, we propose a novel distance metric learning using two hinge losses in the objective function. One is the constraint of the pairs which makes the similar pairs (the same label) closer and the dissimilar (different labels) pairs separated as far as possible. The other one is the constraint of the triplets which makes the largest distance between pairs intra the class larger than the smallest distance between pairs inter the classes. Previous works only consider one of the two kinds of constraints. Additionally, different from the triplets used in previous works, we just need a small amount of such special triplets. This improves the efficiency of our proposed method. Consider the situation in which we might not have enough labeled samples, we extend the proposed distance metric learning into a semi-supervised learning framework. Experiments are conducted on several landmark datasets and the results demonstrate the effectiveness of our proposed method.
[semisupervised learning, triplet constraints, Fasteners, Linear programming, classification, hinge losses, pairwise constraints, metric learning, Euclidean distance, Semisupervised learning, regularized large margin distance metric learning, clustering, semi-supervised learning, learning (artificial intelligence), Principal component analysis, objective function]
Mutual Reinforcement of Academic Performance Prediction and Library Book Recommendation
2016 IEEE 16th International Conference on Data Mining
None
2016
The prediction of academic performance is one of the most important tasks in educational data mining, and has been widely studied in MOOCs and intelligent tutoring systems. Academic performance could be affected with factors like personality, skills, social environment, the use of library books and so on. However, it is still less investigated that how could the use of library books affect academic performance of college students and even leverage book-loan history for predicting academic performance. To this end, we propose a supervised content-aware matrix factorization for mutual reinforcement of academic performance prediction and library book recommendation. This model not only addresses the sparsity challenge by explainable dimension reduction techniques, but also promotes library book recommendation by recommending "right" books for students based on their performance levels and book meta information. Finally, we evaluate the proposed model on three years of the book-loan history and cumulative grade point average of 13,047 undergraduate students in one university. The results show that the proposed model outperforms the competing baselines on both tasks, and that academic performance is not only predictable from the book-loan history but also improves the recommendation of library books for students.
[cumulative grade point average, further education, student personality, dimension reduction technique, Predictive models, undergraduate students, Linear programming, book-loan history, academic libraries, History, Data mining, student skills, academic performance prediction, library book recommendation, recommender systems, supervised content-aware matrix factorization, Hidden Markov models, college students, educational data mining, mutual reinforcement, Prediction algorithms, Libraries, educational institutions, social environment]
Regularized Content-Aware Tensor Factorization Meets Temporal-Aware Location Recommendation
2016 IEEE 16th International Conference on Data Mining
None
2016
Although weighted tensor factorization tailored to implicit feedback has shown its superior performance in temporal-aware location recommendation, it suffers from three critical challenges. First, it doesn't distinguish the confidence of negative preference for time-dependent unvisited locations from that for fully unvisited ones. Second, discontinuity arises from time discretization, and thus an infinitely large margin may exist between different bins of time. Third, geographical constraints of neighbor locations are not taken into account. To address these challenges, we propose a regularized content-aware tensor factorization (RCTF) algorithm, which exploits three strategies to address the corresponding challenges. First, it introduces a novel interaction regularization, second, it represents each bin of time by a derived feature vector from eigen decomposition of a time-bin similarity matrix, to capture the proximity of neighbor bins of time, third, it encodes geographical information of locations by discrete spatial distributions, so that spatial proximity constraints can be satisfied by simply feeding them into location content. The proposed algorithm is then evaluated for time-aware location recommendation on two large scale location-based social network datasets. The experimental results show the superiority of the proposed algorithm to several competing time-aware recommendation baselines, and verify the significant benefit of three strategies in the proposed algorithm.
[Symmetric matrices, eigen decomposition, Social network services, time-bin similarity matrix, RCTF, temporal-aware location recommendation, weighted tensor factorization, geographic information systems, tensors, interaction regularization, time-aware location recommendation, Matrix decomposition, spatial proximity constraints, eigenvalues and eigenfunctions, feature vector, Tensile stress, Graphical models, mobile computing, recommender systems, Collaboration, regularized content-aware tensor factorization, social networking (online), large scale location-based social network datasets, Distribution functions]
Whether This Participant will Attract You to This Event? Exploiting Participant Influence for Event Recommendation
2016 IEEE 16th International Conference on Data Mining
None
2016
When a user is making a decision on whether to participate an event in Event-based Social Networks (EBSN), one of the common considerations is who have agreed to join this event. The reason is that existing participants of the event affect the decision of the user, to which we refer as participant influence. However, participant influence is not well studied by previous works. In this paper, we propose an event recommendation model which considers participant influence, exploiting the influence of existing participants, on the decisions of new participants. Specifically, we investigate participant influence in relation to several commonly used contextual aspects of the event based on Poisson factorization. We have conducted extensive experiments on some datasets extracted from a real-world EBSN. The results demonstrate that the consideration of participant influence can improve event recommendation.
[Context, Vocabulary, Social network services, Probabilistic logic, real-world EBSN, contextual aspects, Sparse matrices, Tensile stress, recommender systems, event recommendation model, event-based social networks, Poisson factorization, social networking (online), social sciences computing, Mathematical model]
HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based Ensembles for Time Series Classification
2016 IEEE 16th International Conference on Data Mining
None
2016
There have been many new algorithms proposed over the last five years for solving time series classification (TSC) problems. A recent experimental comparison of the leading TSC algorithms has demonstrated that one approach is significantly more accurate than all others over 85 datasets. That approach, the Flat Collective of Transformation-based Ensembles (Flat-COTE), achieves superior accuracy through combining predictions of 35 individual classifiers built on four representations of the data into a flat hierarchy. Outside of TSC, deep learning approaches such as convolutional neural networks (CNN) have seen a recent surge in popularity and are now state of the art in many fields. An obvious question is whether CNNs could be equally transformative in the field of TSC. To test this, we implement a common CNN structure and compare performance to Flat-COTE and a recently proposed time series-specific CNN implementation. We find that Flat-COTE is significantly more accurate than both deep learning approaches on 85 datasets. These results are impressive, but Flat-COTE is not without deficiencies. We improve the collective by adding new components and proposing a modular hierarchical structure with a probabilistic voting scheme that allows us to encapsulate the classifiers built on each transformation. We add two new modules representing dictionary and interval-based classifiers, and significantly improve upon the existing frequency domain classifiers with a novel spectral ensemble. The resulting classifier, the Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE) is significantly more accurate than Flat-COTE and represents a new state of the art for TSC. HIVE-COTE captures more sources of possible discriminatory features in time series and has a more modular, intuitive structure.
[pattern classification, Machine learning algorithms, Dictionaries, ensemble classifiers, Time series analysis, time series, modular hierarchical structure, Classification algorithms, probabilistic voting, Training, hierarchical vote collective of transformation-based ensembles, dictionary classifier, deep learning, CNN structure, interval-based classifier, data representations, Machine learning, Prediction algorithms, TSC algorithms, convolutional neural networks, frequency domain classifier, neural nets, HIVE-COTE, time series classification]
House Price Modeling over Heterogeneous Regions with Hierarchical Spatial Functional Analysis
2016 IEEE 16th International Conference on Data Mining
None
2016
Online real-estate information systems such as Zillow and Trulia have gained increasing popularity in recent years. One important feature offered by these systems is the online home price estimate through automated data-intensive computation based on housing information and comparative market value analysis. State-of-the-art approaches model house prices as a combination of a latent land desirability surface and a regression from house features. However, by using uniformly damping kernels, they are unable to handle irregularly shaped regions or capture land value discontinuities within the same region due to the existence of implicit sub-communities, which are common in real-world scenarios. In this paper, we explore the novel application of recent advances in spatial functional analysis to house price modeling and propose the Hierarchical Spatial Functional Model (HSFM), which decomposes house values into land desirability at both the global scale and hidden local scales as well as the feature regression component. We propose statistical learning algorithms based on finite-element spatial functional analysis and spatial constrained clustering to train our model. Extensive evaluations based on housing data in a major Canadian city show that our proposed approach can reduce the mean relative house price estimation error down to 6.60%.
[Solid modeling, spatial constrained clustering, Computational modeling, Biological system modeling, land desirability, functional analysis, hierarchical spatial functional model, regression analysis, Rivers, finite element analysis, real-estate information systems, Analytical models, real estate data processing, statistical learning algorithms, HSFM, pattern clustering, house price modeling, Data models, feature regression component, finite-element spatial functional analysis, Kernel, pricing]
Context-Aware Sequential Recommendation
2016 IEEE 16th International Conference on Data Mining
None
2016
Since sequential information plays an important role in modeling user behaviors, various sequential recommendation methods have been proposed. Methods based on Markov assumption are widely-used, but independently combine several most recent components. Recently, Recurrent Neural Networks (RNN) based methods have been successfully applied in several sequential modeling tasks. However, for real-world applications, these methods have difficulty in modeling the contextual information, which has been proved to be very important for behavior modeling. In this paper, we propose a novel model, named Context-Aware Recurrent Neural Networks (CA-RNN). Instead of using the constant input matrix and transition matrix in conventional RNN models, CA-RNN employs adaptive context-specific input matrices and adaptive context-specific transition matrices. The adaptive context-specific input matrices capture external situations where user behaviors happen, such as time, location, weather and so on. And the adaptive context-specific transition matrices capture how lengths of time intervals between adjacent behaviors in historical sequences affect the transition of global sequential features. Experimental results show that the proposed CA-RNN model yields significant improvements over state-of-the-art sequential recommendation methods and context-aware recommendation methods on two public datasets, i.e., the Taobao dataset and the Movielens-1M dataset.
[Context, adaptive context-specific input matrices, Adaptation models, Recurrent neural networks, context-aware recurrent neural networks, recurrent neural nets, Business process re-engineering, History, ubiquitous computing, constant input matrix, context-aware sequential recommendation, matrix algebra, recommender systems, adaptive context-specific transition matrices, Mathematical model, global sequential features, CA-RNN, Context modeling]
Structure-Preserved Multi-source Domain Adaptation
2016 IEEE 16th International Conference on Data Mining
None
2016
Domain adaptation has achieved promising results in many areas, such as image classification and object recognition. Although a lot of algorithms have been proposed to solve the task with different domain distributions, it remains a challenge for multi-source unsupervised domain adaptation. In addition, most of the existing algorithms learn a classifier on the source domain and predict the labels for the target data, which indicates that only the knowledge derived from the hyperplane is transferred to the target domain and the structure information is ignored. In light of this, we propose a novel algorithm for multi-source unsupervised domain adaptation. Generally speaking, we aim to preserve the whole structure from source domains and transfer it to serve the task on the target domain. The source and target data are put together for clustering, which simultaneously explores the structures of the source and target domains. The structure-preserved information from source domain further guides the clustering process on the target domain. Extensive experiments on two widely used databases on object recognition and face identification show the substantial improvement of our proposed approach over several state-of-the-art methods. Especially, our algorithm can take use of multi-source domains and achieve robust and better performance compared with the single source domain adaptation methods.
[Computers, object recognition, face identification, single-source domain adaptation, structure-preserved information, multisource unsupervised domain adaptation, Transfer Learning, Conferences, image classification, target domain, Linear programming, Constraint Clustering, database management systems, Multi-Source Domain Adaptation, structure-preserved multisource domain adaptation, Databases, Clustering algorithms, face recognition, Prediction algorithms, target data, Robustness]
Sublinear Dual Coordinate Ascent for Regularized Loss Minimization
2016 IEEE 16th International Conference on Data Mining
None
2016
We present a sublinear version of the dual coordinate ascent method for solving a group of regularized loss minimization problems in machine learning. The proposed method seamlessly integrates sampling techniques, the dual coordinate ascent method, and a multiplicative update algorithm. The sampling techniques choose the "expected" examples, and estimate the corresponding inner products. The dual coordinate ascent method generates an updated iterative step, which outperforms the time-learning step used in the previous sublinear perceptron algorithm. The multiplicative update algorithm updates the example weighting. The proposed method is implemented with an iterative step of order O(log(n)), where n is the size of examples, and achieves a better result than other methods, with high probability. We present a theoretical analysis of the sublinear iterative in order to justify its benefits. We then apply the proposed optimization method to support vector machine and conduct experiments on three large-scale datasets. Our experimental results validate our theoretical findings.
[Algorithm design and analysis, iterative methods, Machine learning algorithms, Runtime, sampling techniques, sublinear perceptron algorithm, Cost function, time-learning step, learning (artificial intelligence), sublinear dual coordinate ascent, sampling methods, support vector machines, probability, sampling, Minimization, machine learning, multiplicative update, sublinear iterative, support vector machine, iterative step, Australia, minimisation, regularized loss minimization problems, Dual coordinate ascent, computational complexity]
Efficient and Scalable Detection of Overlapping Communities in Big Networks
2016 IEEE 16th International Conference on Data Mining
None
2016
Community detection is a hot topic for researchers in the fields including graph theory, social networks and biological networks. Generally speaking, a community refers to a group of densely linked nodes in the network. Nodes usually have more than one community label, indicating their multiple roles or functions in the network. Unfortunately, existing solutions aiming at overlapping-community-detection are not capable of scaling to large-scale networks with millions of nodes and edges. In this paper, we propose a fast overlapping-communitydetection algorithm - FOX. In the experiment on a network with 3.9 millions nodes and 20 millions edges, the detection finishes in 14 minutes and provides the most qualified results. The second fastest algorithm, however, takes ten times longer to run. As for another network with 22 millions nodes and 127 millions edges, our algorithm is the only one that can provide an overlapping community detection result and it only takes 238 minutes. Our algorithm draws lessons from potential games, a concept in game theory. We measure the closeness of a node to a community by counting the number of triangles formed by the node and two other nodes form the community. Potential games ensure that the algorithm can reach convergence. We also extend the exploitation of triangle to open-triangle, which enlarges the scale of the detected communities.
[Image edge detection, Heuristic algorithms, Social network services, graph theory, convergence, game theory, Big Data, network theory (graphs), Community detection, network edges, Game theory, Potential Games, FOX, network nodes, open-triangle, Clustering algorithms, Games, Approximation algorithms, overlapping-community-detection algorithm, Big Networks, Heuristic]
Factorizing Complex Discrete Data &#x201c;with Finesse&#x201d;
2016 IEEE 16th International Conference on Data Mining
None
2016
Can we mine latent patterns from discrete, non-numeric heterogeneous data? Many modern data sets contain heterogeneous non-numerical information measured over Boolean, ordinal and ternary scales. Values for features like these are "mixable" in the sense that they have intuitive non-linear analogs to classical "addition" (e.g. logical OR for Boolean data). We present a novel, general and extensible matrix factorization framework for any such "mixable" features. The framework lets us support heterogeneous data and encourages us to deduce other interesting "mixable" features, like those which encapsulate sub-trees over an ontology. We present Finesse, an algorithm with linear run-time complexity in the size of the data. Finesse outperforms state-of-the-art techniques in the special cases in terms of effectiveness and efficiency, and yields insightful patterns from its novel application to large real-world heterogeneous data.
[mixable features, heterogeneous nonnumerical information, Frequency modulation, Conferences, finite sets, data mining, complex data, Ontologies, Matrix factorization, Complexity theory, matrix decomposition, heterogeneous data, extensible matrix factorization framework, dimensionality reduction, ontology, Context, logical data, Linear programming, discrete data, discrete nonnumeric heterogeneous data, latent pattern mining, linear run-time complexity, tree structures, Vegetation, ontologies (artificial intelligence), complex discrete data with finesse factorization, efficient algorithms, sub-trees encapsulation, computational complexity]
Towards Scalable Network Delay Minimization
2016 IEEE 16th International Conference on Data Mining
None
2016
Reduction of end-to-end network delays is an optimization task with applications in multiple domains. Low delays enable improved information flow in social networks, quick spread of ideas in collaboration networks, low travel times for vehicles on road networks and increased rate of packets in communication networks. Delay reduction can be achieved by both improving the propagation capabilities of individual nodes and adding additional edges in the network. One of the main challenges in such design problems is that the effects of local changes are not independent, and as a consequence, there is a combinatorial search space of possible improvements. Thus, minimizing the cumulative propagation delay requires novel scalable and data-driven approaches. In this paper, we consider the problem of network delay minimization via node upgrades. Although the problem is NP-hard, we show that probabilistic approximation for a restricted version can be obtained. We design scalable and high-quality techniques for the general setting based on sampling that are targeted to different models of delay distribution. Our methods scale almost linearly with the graph size and consistently outperform competitors in quality.
[NP-hard, graph theory, network theory (graphs), collaboration networks, vehicles, road networks, Complexity theory, Shortest paths, propagation capabilities, information flow, graph size, optimisation, Sampling, search problems, scalable network delay minimization, Graph mining, end-to-end network delay reduction, Social network services, Computational modeling, communication networks, probabilistic approximation, social networks, Network optimization, Node delay, Airports, Minimization, scalable approach, delay distribution, optimization task, data-driven approach, combinatorial search space, Approximation algorithms, Delays, node upgrades]
Foundations of Perturbation Robust Clustering
2016 IEEE 16th International Conference on Data Mining
None
2016
Clustering is a fundamental data mining tool that aims to divide data into groups of similar items. Intuition about clustering reflects the ideal case - exact data sets endowed with flawless dissimilarity between individual instances. In practice however, these cases are in the minority, and clustering applications are typically characterized by noisy data sets with approximate pairwise dissimilarities. As such, the efficacy of clustering methods necessitates robustness to perturbations. In this paper, we address foundational questions on perturbation robustness, studying to what extent can clustering techniques exhibit this desirable characteristic. Our results also demonstrate the type of cluster structures required for robustness of popular clustering paradigms.
[Algorithm design and analysis, Additives, Hamming distance, Theory, data mining, Perturbation Robustness, Classification algorithms, Data mining, Clustering, Machine Learning, perturbation robust clustering, data mining tool, pattern clustering, flawless dissimilarity, Clustering algorithms, Robustness]
Faster Kernels for Graphs with Continuous Attributes via Hashing
2016 IEEE 16th International Conference on Data Mining
None
2016
While state-of-the-art kernels for graphs with discrete labels scale well to graphs with thousands of nodes, the few existing kernels for graphs with continuous attributes, unfortunately, do not scale well. To overcome this limitation, we present hash graph kernels, a general framework to derive kernels for graphs with continuous attributes from discrete ones. The idea is to iteratively turn continuous attributes into discrete labels using randomized hash functions. We illustrate hash graph kernels for the Weisfeiler-Lehman subtree kernel and for the shortest-path kernel. The resulting novel graph kernels are shown to be, both, able to handle graphs with continuous attributes and scalable to large graphs and data sets. This is supported by our theoretical analysis and demonstrated by an extensive experimental evaluation.
[randomized hash functions, ICDM, shortest-path kernel, Conferences, Social network services, graph theory, hashing, Big Data, continuous attributes, Data Mining, Data mining, Machine Learning, Graph Kernel, Image analysis, graphs, hash graph kernels, Classification, discrete labels, Approximation algorithms, Weisfeiler-Lehman subtree kernel, Kernel, Bioinformatics]
Optimizing the Multiclass F-Measure via Biconcave Programming
2016 IEEE 16th International Conference on Data Mining
None
2016
The F-measure and its variants are performance measures of choice for evaluating classification and retrieval tasks in the presence of severe class imbalance. It is thus highly desirable to be able to directly optimize these performance measures on large-scale data. Recent advances have shown that this is possible in the simple binary classification setting. However, scant progress exists in multiclass settings with a large number of classes where, in addition, class-imbalance is much more severe. The lack of progress is especially conspicuous for the macro-averaged F-measure, which is the widely preferred F-measure variant in multiclass settings due to its equal emphasis on rare classes. Known methods of optimization scale poorly for macro F-measure, often requiring run times that are exponential in the number of classes. We develop BEAM-F, the first efficient method for directly optimizing the macro F-measure in multiclass settings. The challenge here is the intractability of optimizing a sum of fractional-linear functions over the space of confusion matrices. We overcome this difficulty by formulating the problem as a biconcave maximization program and solve it using an efficient alternating maximization approach that involves a Frank-Wolfe based iterative solver. Our approach offers guaranteed convergence to a stationary point and experiments show that, for a range synthetic data sets and real-world applications, our method offers superior performance on problems exhibiting large class imbalance.
[iterative methods, retrieval tasks, Macro F-measure, Stochastic processes, classification tasks, Harmonic analysis, Optimization, biconcave maximization program, Training, Alternating maximization, Frank-Wolfe method, multiclass settings, fractional-linear functions, concave programming, pattern classification, BEAM-F, Frank-Wolfe based iterative solver, Class imbalance, information retrieval, Multiclass classification, Tuning, Standards, macro-averaged F-measure, matrix algebra, Support vector machines, Biconcave program, multiclass F-measure, confusion matrices]
Budgeted Batch Bayesian Optimization
2016 IEEE 16th International Conference on Data Mining
None
2016
Parameter settings profoundly impact the performance of machine learning algorithms and laboratory experiments. The classical trial-error methods are exponentially expensive in large parameter spaces, and Bayesian optimization (BO) offers an elegant alternative for global optimization of black box functions. In situations where the functions can be evaluated at multiple points simultaneously, batch Bayesian optimization is used. Current batch BO approaches are restrictive in fixing the number of evaluations per batch, and this can be wasteful when the number of specified evaluations is larger than the number of real maxima in the underlying acquisition function. We present the budgeted batch Bayesian optimization (B3O) for hyper-parameter tuning and experimental design - we identify the appropriate batch size for each iteration in an elegant way. In particular, we use the infinite Gaussian mixture model (IGMM) for automatically identifying the number of peaks in the underlying acquisition functions. We solve the intractability of estimating the IGMM directly from the acquisition function by formulating the batch generalized slice sampling to efficiently draw samples from the acquisition function. We perform extensive experiments for benchmark functions and two real world applications - machine learning hyper-parameter tuning and experimental design for alloy hardening. We show empirically that the proposed B3O outperforms the existing fixed batch BO approaches in finding the optimum whilst requiring a fewer number of evaluations, thus saving cost and time.
[Machine learning algorithms, Metals, batch Bayesian optimization, black box functions, budgeted batch Bayesian optimization, parallel global optimization, Optimization, Gaussian mixture model, optimisation, laboratory experiments, hyper-parameter tuning, experimental design, learning (artificial intelligence), IGMM, machine learning algorithms, acquisition function, infinite Gaussian mixture model, alloy hardening, Tuning, global optimization, batch generalized slice sampling, Bayes methods, mixture models, hyperparameter tuning, B3O]
One-Pass Logistic Regression for Label-Drift and Large-Scale Classification on Distributed Systems
2016 IEEE 16th International Conference on Data Mining
None
2016
Logistic regression (LR) for classification is the workhorse in industry, where a set of predefined classes is required. The model, however, fails to work in the case where the class labels are not known in advance, a problem we term label-drift classification. Label-drift classification problem naturally occurs in many applications, especially in the context of streaming settings where the incoming data may contain samples categorized with new classes that have not been previously seen. Additionally, in the wave of big data, traditional LR methods may fail due to their expense of running time. In this paper, we introduce a novel variant of LR, namely one-pass logistic regression (OLR) to offer a principled treatment for label-drift and large-scale classifications. To handle largescale classification for big data, we further extend our OLR to a distributed setting for parallelization, termed sparkling OLR (Spark-OLR). We demonstrate the scalability of our proposed methods on large-scale datasets with more than one hundred million data points. The experimental results show that the predictive performances of our methods are comparable orbetter than those of state-of-the-art baselines whilst the executiontime is much faster at an order of magnitude. In addition, the OLR and Spark-OLR are invariant to data shuffling and have no hyperparameter to tune that significantly benefits data practitioners and overcomes the curse of big data cross-validationto select optimal hyperparameters.
[Industries, label-drift classification problem, Big Data cross-validation, regression analysis, distributed processing, distributed system, optimal hyperparameter selection, class labels, distributed systems, Big data, large-scale classification, Context, sparkling OLR, pattern classification, Estimation, Big Data, label-drift, data shuffling, Apache Spark, large-scale datasets, Logistic regression, one-pass logistic regression, execution time, Spark-OLR, Data models, Bayes methods, Logistics]
Self-Grouping Multi-network Clustering
2016 IEEE 16th International Conference on Data Mining
None
2016
Joint clustering of multiple networks has been shown to be more accurate than performing clustering on individual networks separately. Many multi-view and multi-domain network clustering methods have been developed for joint multi-network clustering. These methods typically assume there is a common clustering structure shared by all networks, and different networks can provide complementary information on this underlying clustering structure. However, this assumption is too strict to hold in many emerging real-life applications, where multiple networks have diverse data distributions. More popularly, the networks in consideration belong to different underlying groups. Only networks in the same underlying group share similar clustering structures. Better clustering performance can be achieved by considering such groups differently. As a result, an ideal method should be able to automatically detect network groups so that networks in the same group share a common clustering structure. To address this problem, we propose a novel method, ComClus, to simultaneously group and cluster multiple networks. ComClus treats node clusters as features of networks and uses them to differentiate different network groups. Network grouping and clustering are coupled and mutually enhanced during the learning process. Extensive experimental evaluation on a variety of synthetic and real datasets demonstrates the effectiveness of our method.
[Measurement, Electrical engineering, self-grouping multinetwork clustering, Symmetric matrices, Clustering methods, Conferences, learning process, multiple networks, network groups, network theory (graphs), Data mining, Tensile stress, pattern clustering, data distributions, individual networks, multidomain network clustering methods, data handling, similar clustering structures, learning (artificial intelligence)]
A Scalable Framework for Stylometric Analysis Query Processing
2016 IEEE 16th International Conference on Data Mining
None
2016
Stylometry is the statistical analyses of variationsin the author's literary style. The technique has been used inmany linguistic analysis applications, such as, author profiling, authorship identification, and authorship verification. Over thepast two decades, authorship identification has been extensivelystudied by researchers in the area of natural language processing. However, these studies are generally limited to (i) a small number of candidate authors, and (ii) documents with similar lengths. In this paper, we propose a novel solution by modeling authorship attribution as a set similarity problem to overcome the two stated limitations. We conducted extensive experimental studies on a real dataset collected from an online book archive, Project Gutenberg. Experimental results show that in comparison to existing stylometry studies, our proposed solution can handlea larger number of documents of different lengths written by alarger pool of candidate authors with a high accuracy.
[Project Gutenberg, natural language processing, linguistic analysis applications, Probabilistic logic, Data mining, stylometric analysis, authorship identification, Authorship Attribution, query processing, Plagiarism Detection, Stylometry, Plagiarism, Query processing, Locality Sensitive Hashing, Syntactics, Writing, Feature extraction, online book archive, Large Scale Database, statistical analysis, author literary style]
Compressing Random Forests
2016 IEEE 16th International Conference on Data Mining
None
2016
Ensemble methods are considered among the state-of-the-art predictive modeling approaches. Applied to modern big data, these methods often require a large number of sub-learners, where the complexity of each learner typically grows with the size of the dataset. This phenomenon results in an increasing demand for storage space, which may be very costly. This problem mostly manifests in a subscriber based environment, where a user-specific ensemble needs to be stored on a personal device with strict storage limitations (such as a cellular device). In this work we introduce a novel method for lossless compression of tree-based ensemble methods, focusing on Random Forests. Our suggested method is based on probabilistic modeling of the ensemble's trees, followed by model clustering via Bregman divergence. This allows us to find a minimal set of models that provides an accurate description of the trees, and at the same time is small enough to store and maintain. Our compression scheme demonstrates high compression rates on a variety of modern datasets. Importantly, our scheme enables predictions from the compressed format and a perfect reconstruction of the original ensemble.
[Dictionaries, Compression, user-specific ensemble, Probability distribution, probabilistic modeling, Random Forest, predictive modeling approaches, model clustering, Bregman divergence, random forests compression, storage space, Mathematical model, learning (artificial intelligence), big data, data compression, probability, lossless compression, Big Data, Probabilistic logic, Encoding, tree-based ensemble methods, Entropy Coding, Vegetation, cellular device, storage limitations, Periodic structures]
A Fast Factorization-Based Approach to Robust PCA
2016 IEEE 16th International Conference on Data Mining
None
2016
Robust principal component analysis (RPCA) has been widely used for recovering low-rank matrices in many data mining and machine learning problems. It separates a data matrix into a low-rank part and a sparse part. The convex approach has been well studied in the literature. However, state-of-the-art algorithms for the convex approach usually have relatively high complexity due to the need of solving (partial) singular value decompositions of large matrices. A non-convex approach, AltProj, has also been proposed with lighter complexity and better scalability. Given the true rank r of the underlying low rank matrix, AltProj has a complexity of O(r2dn), where d &#x00D7; n is the size of data matrix. In this paper, we propose a novel factorization-based model of RPCA, which has a complexity of O(kdn), where k is an upper bound of the true rank. Our method does not need the precise value of the true rank. From extensive experiments, we observe that AltProj can work only when r is precisely known in advance, however, when the needed rank parameter r is specified to a value different from the true rank, AltProj cannot fully separate the two parts while our method succeeds. Even when both work, our method is about 4 times faster than AltProj. Our method can be used as a light-weight, scalable tool for RPCA in the absence of the precise value of the true rank.
[scalable, fast factorization-based approach, low-rank matrices, robust PCA, Conferences, factorization-based model, data matrix, nonconvex approach, data mining, robust principal component analysis, Data mining, machine learning, factorization, AltProj, non-convex, RPCA, learning (artificial intelligence), principal component analysis, singular value decomposition, computational complexity, singular value decompositions]
Background Check: A General Technique to Build More Reliable and Versatile Classifiers
2016 IEEE 16th International Conference on Data Mining
None
2016
We introduce a powerful technique to make classifiers more reliable and versatile. Background Check equips classifiers with the ability to assess the difference of unlabelled test data from the training data. In particular, Background Check gives classifiers the capability to (i) perform cautious classification with a reject option, (ii) identify outliers, and (iii) better assess the confidence in their predictions. We derive the method from first principles and consider four particular relationships between background and foreground distributions. One of these assumes an affine relationship with two parameters, and we show how this bivariate parameter space naturally interpolates between the above capabilities. We demonstrate the versatility of the approach by comparing it experimentally with published special-purpose solutions for outlier detection and confident classification on 41 benchmark datasets. Results show that Background Check can match and in many cases surpass the performances of specialised approaches.
[pattern classification, outlier identification, Conferences, Estimation, bivariate parameter space, Probability, versatile classifiers, outlier detection, Standards, background check, confident classification, multiclass classification, Training data, foreground distributions, Data models, background distributions, Reliability, cautious classification]
Product-Based Neural Networks for User Response Prediction
2016 IEEE 16th International Conference on Data Mining
None
2016
Predicting user responses, such as clicks and conversions, is of great importance and has found its usage inmany Web applications including recommender systems, webs earch and online advertising. The data in those applications is mostly categorical and contains multiple fields, a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between interfieldcategories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two-large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.
[distributed data representation, distributed representation learning, Product-based Neural Networks, Artificial neural networks, Predictive models, Encoding, Complexity theory, user response prediction, Deep Learning, User Response Prediction, PNN, product-based neural networks, Data models, human computer interaction, categorical data representation, Advertising, neural nets, Multi-field Categorical Data]
Efficient Algorithms for the Three Locus Problem in Genome-Wide Association Study
2016 IEEE 16th International Conference on Data Mining
None
2016
Using the recent advances in sequencing technology thousands of genomes have been sequenced. This sequence data can be fruitfully employed in diagnosis, drug design, etc. Genome-wide Association Study (GWAS) focuses on this important problem of extracting useful information from genomic data. As an example, a comparison of different genomes could throw light on causes for different diseases. Human variabilities happen due to single nucleotide polymorphisms (SNPs). Thus it might suffice to focus on these SNPs while comparing different genomes. One of the important problems in GWAS is that of identifying the correlation between genotypes (SNPs for example) and phenotypes (i.e., different characteristics such as addiction, the presence of cancer, etc.) Different approaches exist for addressing this problem. One important approach is via modeling this problem as the k-locus problem (k being any integer). The case of k = 1 has been studied widely. Some algorithms also exist for solving the case of k = 2. The real cause for a disease could be more than two SNPs. The case of k &gt; 2 has not been studied in the literature. For the first time, in this paper we present an efficient algorithm for solving the 3-locus problem that is several orders of magnitude faster than the brute force algorithm. All the software can be obtained from: engr.uconn.edu/~rajasek/ThreeLocus.
[Correlation, GWAS, Force, Three Locus Problem, Genomics, k-locus problem, Data mining, sequencing technology, SNPs, genotype-phenotype correlations, genomics, Genome-wide Association Study, single nucleotide polymorphisms, Bioinformatics, sequence data, Hamming distance, genotypes, three locus problem, diseases, Diseases, 3-locus problem, information extraction, phenotypes, bioinformatics, genome-wide association study, efficient algorithms, human variability]
Scalable Block Scheduling for Efficient Multi-database Record Linkage
2016 IEEE 16th International Conference on Data Mining
None
2016
Record linkage (RL) is a task in data integration that aims to identify matching records that refer to the same entity from different databases. When records from more than two databases are to be linked RL is significantly challenged by the intrinsic exponential growth in the number of potential record comparisons to be conducted. We propose a scalable meta blocking protocol to be used for Multi-Database RL (MDRL) to significantly reduce the complexity of the matching (comparison and classification) phase. Our approach uses a graph structure to schedule the comparison of pairs of blocks with the aim of minimizing the number of repeated and superfluous comparisons between records. We provide an analysis of our approach and conduct an empirical study on large real-world databases.
[scalable meta blocking protocol, Schedules, Protocols, pattern matching, Record Linkage, scalable block scheduling, matching record identification, Encoding, Data mining, database management systems, multidatabase record linkage, Meta Blocking, Couplings, Block Scheduling, Multi-databases Linkage, MDRL, Distributed databases, graph structure, data integration]
Sequential Ensemble Learning for Outlier Detection: A Bias-Variance Perspective
2016 IEEE 16th International Conference on Data Mining
None
2016
Ensemble methods for classification have been effectively used for decades, while for outlier detection it has only been studied recently. In this work, we design a new ensemble approach for outlier detection in multi-dimensional point data, which provides improved accuracy by reducing error through both bias and variance by considering outlier detection as a binary classification task with unobserved labels. In this paper, we propose a sequential ensemble approach called CARE that employs a two-phase aggregation of the intermediate results in each iteration to reach the final outcome. Unlike existing outlier ensembles, our ensemble incorporates both the parallel and sequential building blocks to reduce bias as well as variance by (i) successively eliminating outliers from the original dataset to build a better data model on which outlierness is estimated (sequentially), and (ii) combining the results from individual base detectors and across iterations (parallelly). Through extensive experiments on 16 real-world datasets mainly from the UCI machine learning repository [1], we show that CARE performs significantly better than or at least similar to the individual baselines as well as the existing state-of-the-art outlier ensembles.
[pattern classification, data model, Error analysis, Bias, ensemble classification methods, bias-variance perspective, Outlier detection, multidimensional point data, Complexity theory, Ensemble methods, outlier detection, sequential ensemble learning, CARE, Variance, Aggregates, unobserved labels, Detectors, UCI machine learning repository, Feature extraction, Data models, learning (artificial intelligence), binary classification task, Bagging, outlier ensembles]
Learning Independent, Diverse Binary Hash Functions: Pruning and Locality
2016 IEEE 16th International Conference on Data Mining
None
2016
Information retrieval in large databases of complex objects, such as images, audio or documents, requires approximate search algorithms in practice, in order to return semantically similar objects to a given query in a reasonable time. One practical approach is supervised binary hashing, where each object is mapped onto a small binary vector so that Hamming distances approximate semantic similarities, and the search is done in the binary space more efficiently. Much work has focused on designing objective functions and optimization algorithms for learning b-bit hash functions from a dataset. Recent work has shown that comparable or better results can be obtained by training b hash functions independently from each other and making them cooperate by introducing diversity with ensemble learning techniques. We show that this can be further improved by two techniques: pruning an ensemble of hash functions, and learning local hash functions. We show how it is possible to train our improved algorithms in datasets orders of magnitude larger than those used by most works on supervised binary hashing.
[pruning, diverse binary hash functions, Search problems, b-bit hash function learning, Optimization, Training, approximate search algorithms, optimisation, binary hashing, Databases, semantic similarities, very large databases, optimization, Binary codes, small binary vector, learning (artificial intelligence), ensemble learning techniques, local hash function learning, search problems, binary space, Laplace equations, supervised binary hashing, ensemble diversity, optimization algorithms, information retrieval, locality, objective functions, Linear programming, Hamming distances, complex objects, vectors, large databases]
A Rotation Invariant Latent Factor Model for Moveme Discovery from Static Poses
2016 IEEE 16th International Conference on Data Mining
None
2016
We tackle the problem of learning a rotation invariant latent factor model when the training data is comprised of lower-dimensional projections of the original feature space. The main goal is the discovery of a set of 3-D bases poses that can characterize the manifold of primitive human motions, or movemes, from a training set of 2-D projected poses obtained from still images taken at various camera angles. The proposed technique for basis discovery is data-driven rather than hand-designed. The learned representation is rotation invariant, and can reconstruct any training instance from multiple viewing angles. We apply our method to modeling human poses in sports (via the Leeds Sports Dataset), and demonstrate the effectiveness of the learned bases in a range of applications such as activity classification, inference of dynamics from a single frame, and synthetic representation of movements.
[Solid modeling, image classification, moveme discovery, Machine Learning, Image reconstruction, Human Motion Analysis, Training, cameras, lower-dimensional projections, 2D projected poses, primitive human motions, feature extraction, rotation invariant latent factor model, Training data, pose estimation, Activity recognition, Latent Factor Modeling, human pose modeling, activity classification, Representation Learning, image reconstruction, Matrix decomposition, image motion analysis, static poses, camera angles, Rotation Invariance, 3D bases poses, Data models, synthetic movement representation, statistical analysis, sport, still images, Leeds Sports Dataset]
Patterns in Cognitive Rehabilitation of Traumatic Brain Injury Patients: A Text Mining Approach
2016 IEEE 16th International Conference on Data Mining
None
2016
Traumatic Brain Injury (TBI) is a leading cause of disability worldwide, there is one TBI case every 15 seconds and in every 5 minutes someone becomes permanently disabled due to it. Brain injuries lack of surgical or pharmacological therapies, therefore Cognitive Rehabilitation (CR) is the generally adopted treatment. Computerized CR tasks are increasingly replacing traditional "paper and pencil" approaches. Nevertheless, CR plans are manually designed by clinicians from scratch based on their own experience. There is very little research on the amount and type of practice that occurs during computerized CR treatments and its relationship to patients' outcomes. While task repetition is not the only important feature, it is becoming clear that neuroplastic change and functional improvement occur after specific tasks are performed, but do not occur with others. In this work we focus on the preprocessing, patterns and knowledge extraction phases of a Knowledge Discovery in Databases (KDD) framework. We propose considering CR programs as sequences of sessions and pattern searching (association rules, classification models, clustering and shallow neural models) to support clinicians in the selection of specific interventions (e.g. tasks assignations). The proposed framework is applied to 40000 tasks executions from real clinical setting. Results show different execution patterns on patients with positive and negative responses to treatment, predictive models outperform previous recent research, therapists are provided with new insights and tools for tasks selection criteria and design of CR programs.
[Visualization, text analysis, data mining, association rules, computerized CR tasks, preprocessing phase, Brain injuries, patient rehabilitation, cognitive rehabilitation, healthcare, functional improvement, shallow neural models, patient treatment, computerized CR treatments, pattern searching, classifiers, text mining, TBI, surgical therapy, knowledge discovery in databases framework, pattern phase, Text mining, traumatic brain injury patients, pattern classification, clinical application, classification models, Computational modeling, Medical treatment, knowledge extraction phase, KDD framework, traumatic brain injury, brain, neuroplastic change, pharmacological therapy, pattern clustering, Tag clouds, clustering]
Detecting Change Processes in Dynamic Networks by Frequent Graph Evolution Rule Mining
2016 IEEE 16th International Conference on Data Mining
None
2016
The analysis of the temporal evolution of dynamic networks is a key challenge for understanding complex processes hidden in graph structured data. Graph evolution rules capture such processes on the level of small subgraphs by describing frequently occurring structural changes within a network. Existing rule discovery methods make restrictive assumptions on the change processes present in networks. We propose EvoMine, a frequent graph evolution rule mining method that, for the first time, supports networks with edge insertions and deletions as well as node and edge relabelings. EvoMine defines embedding-based and event-based support as two novel measures to assess the frequency of rules. These measures are based on novel mappings from dynamic networks to databases of union graphs that retain all evolution information relevant for rule mining. Using these mappings the rule mining problem can be solved by frequent subgraph mining. We evaluate our approach and two baseline algorithms on several real datasets. To the best of our knowledge, this is the first empirical comparison of rule mining algorithmsfor dynamic networks.
[graph structured data, Heuristic algorithms, Conferences, graph theory, frequent graph evolution rule mining method, data mining, frequent subgraph mining, edge relabelings, Frequency measurement, Topology, Data mining, graph evolution rules, change process detection, Databases, Network topology, edge deletions, rule discovery methods, dynamic networks, EvoMine, edge insertions]
Reliable Semi-supervised Learning
2016 IEEE 16th International Conference on Data Mining
None
2016
In this paper, we propose a Reliable Semi-Supervised Learning framework, called ReSSL, for both static and streaming data. Instead of relaxing different assumptions, we do model the reliability of cluster assumption, quantify the distinct importance of clusters (or evolving micro-clusters on data streams), and integrate the cluster-level information and labeled data for prediction with a lazy learning framework. Extensive experiments demonstrate that our method has good performance compared to state-of-the-art algorithms on data sets in both static and real streaming environments.
[data analysis, streaming data, Particle separators, reliability, ReSSL, data stream, cluster-level information, Supervised learning, static data, Clustering algorithms, Training data, cluster assumption reliability, labeled data, Semisupervised learning, reliable semisupervised learning framework, lazy learning framework, Data models, semi-supervised learning, Reliability, learning (artificial intelligence)]
Online Unsupervised Multi-view Feature Selection
2016 IEEE 16th International Conference on Data Mining
None
2016
In this paper, we propose an Online unsupervised Multi-View Feature Selection method, OMVFS, which deals with large-scale/streaming multi-view data in an online fashion. OMVFS embeds unsupervised feature selection into a clustering algorithm via nonnegative matrix factorization with sparse learning. It further incorporates the graph regularization to preserve the local structure information and help select discriminative features. Instead of storing all the historical data, OMVFS processes the multi-view data chunk by chunk and aggregates all the necessary information into several small matrices. By using the buffering technique, the proposed OMVFS can reduce the computational and storage cost while taking advantage of the structure information. Furthermore, OMVFS can capture the concept drifts in the data streams. Extensive experiments on four real-world datasets show the effectiveness and efficiency of the proposed OMVFS method. More importantly, OMVFS is about 100 times faster than the off-line methods.
[Laplace equations, multi-view, online unsupervised multiview feature selection method, clustering algorithm, Buffer storage, Linear programming, graph regularization, online feature selection, Electronic mail, matrix decomposition, Sparse matrices, nonnegative matrix factorization, Optimization, unsupervised learning, OMVFS method, Aggregates, pattern clustering, unsupervised, buffering technique, feature selection]
Prefix and Suffix Invariant Dynamic Time Warping
2016 IEEE 16th International Conference on Data Mining
None
2016
While there exist a plethora of classification algorithms for most data types, there is an increasing acceptance that the unique properties of time series mean that the combination of nearest neighbor classifiers and Dynamic Time Warping (DTW) is very competitive across a host of domains, from medicine to astronomy to environmental sensors. While there has been significant progress in improving the efficiency and effectiveness of DTW in recent years, in this work we demonstrate that an underappreciated issue can significantly degrade the accuracy of DTW in real-world deployments. This issue has probably escaped the attention of the very active time series research community because of its reliance on static highly contrived benchmark datasets, rather than real world dynamic datasets where the problem tends to manifest itself. In essence, the issue is that DTW's eponymous invariance to warping is only true for the main "body" of the two time series being compared. However, for the "head" and "tail" of the time series, the DTW algorithm affords no warping invariance. The effect of this is that tiny differences at the beginning or end of the time series (which may be either consequential or simply the result of poor "cropping") will tend to contribute disproportionally to the estimated similarity, producing incorrect classifications. In this work, we show that this effect is real, and reduces the performance of the algorithm. We further show that we can fix the issue with a subtle redesign of the DTW algorithm, and that we can learn an appropriate setting for the extra parameter we introduced. We further demonstrate that our generalization is amiable to all the optimizations that make DTW tractable for large datasets.
[classification algorithms, pattern classification, Heuristic algorithms, dynamic datasets, Similarity, Time series analysis, prefix, time series, time warp simulation, Time measurement, Classification algorithms, Data mining, suffix, Training, nearest neighbor classifiers, Weapons, Time Series, DTW algorithm, Dynamic Time Warping, invariant dynamic time warping, benchmark datasets]
Mining Summaries for Knowledge Graph Search
2016 IEEE 16th International Conference on Data Mining
None
2016
Mining and searching heterogeneous and large knowledge graphs is challenging under real-world resource constraints such as response time. This paper studies a framework that discover to facilitate knowledge graph search. 1) We introduce a class of summaries characterized by graph patterns. In contrast to conventional summaries defined by frequent subgraphs, the summaries are capable of adaptively summarize entities with similar neighbors up to a bounded hop. 2) We formulate the computation of graph summarization as a bi-criteria pattern mining problem. Given a knowledge graph G, the problem is to discover k diversified summaries that maximizes the informativeness measure. Although this problem is NP-hard, we show that it is 2-approximable. We also introduce an online mining algorithm that trade-off speed and accuracy, under given resource constraints. 3) We develop query evaluation algorithms that make use of the summaries as views. These algorithms efficiently compute (approximate) answers with high accuracy, and only refer to a small number of summaries. Our experimental study verifies that online mining over large knowledge graphs is feasible, and can suggest bounded search in knowledge graphs.
[graph summarization, Redundancy, Knowledge based systems, data mining, Inspection, 2-approximable problem, Data mining, query evaluation algorithms, graph patterns, query processing, online mining algorithm, bi-criteria pattern mining problem, NP-hard problem, Query processing, informativeness measure, Approximation algorithms, Time factors, computational complexity, knowledge graph search]
Structure Selection for Convolutive Non-negative Matrix Factorization Using Normalized Maximum Likelihood Coding
2016 IEEE 16th International Conference on Data Mining
None
2016
Convolutive non-negative matrix factorization (CNMF) is a promising method for extracting features from sequential multivariate data. Conventional algorithms for CNMF require that the structure, or the number of bases for expressing the data, be specified in advance. We are concerned with the issue of how we can select the best structure of CNMF from given data. We first introduce a framework of probabilistic modeling of CNMF and reduce this issue to statistical model selection. The problem is here that conventional model selection criteria such as AIC, BIC, MDL cannot straightforwardly be applied since the probabilistic model for CNMF is irregular in the sense that parameters are not uniquely identifiable. We overcome this problem to propose a novel criterion for best structure selection for CNMF. The key idea is to apply the technique of latent variable completion in combination with normalized maximum likelihood coding criterion under the minimum description length principle. We empirically demonstrate the effectiveness of our method using artificial and real data sets.
[probability, Probabilistic logic, Encoding, latent variable completion, matrix decomposition, probabilistic modeling, Data mining, normalized maximum likelihood coding criterion, Information science, convolutive nonnegative matrix factorization, best structure selection, sequential multivariate data, Convolution, feature extraction, minimum description length principle, Feature extraction, Data models, data handling, CNMF, statistical model selection]
Modeling Real Estate for School District Identification
2016 IEEE 16th International Conference on Data Mining
None
2016
The affiliated school district of a real estate property is often a crucial concern. How to automate the identification of residential homes located in a favorable educational environment, however, is largely unexplored until now. The availability of heterogeneous estate-related data offers a great opportunity for this task. Nevertheless, it is such heterogeneity that poses significant challenges to their amalgamation in a unified fashion. To this end, we develop G-LRMM model to integrate digital price, textual comments, and geographical location information together. The proposed approach is able to capture the in-depth interaction among multi-type data greatly. The evaluation on the dataset of Beijing property market justifies the benefits of our approach over baselines. The further comparison among different components is also conducted and demonstrates their important roles. Moreover, the proposed model can offer useful insights into modeling heterogeneous data sources.
[Parameter estimation, Computational modeling, automated residential home identification, real estate property, Appraisal, heterogeneous data source modeling, G-LRMM model, real estate modeling, real estate data processing, geographical location information, Education, Mixture models, school district identification, textual comments, Data models, Mathematical model, heterogeneous estate-related data, digital price]
Feature Grouping Using Weighted l1 Norm for High-Dimensional Data
2016 IEEE 16th International Conference on Data Mining
None
2016
Building effective predictive models from high-dimensional data is an important problem in several domains such as in bioinformatics, healthcare analytics and general regression analysis. Extracting feature groups automatically from such data with several correlated features is necessary, in order to use regularizers such as the group lasso which can exploit this deciphered grouping structure to build effective prediction models. Elastic net, fused-lasso and Octagonal Shrinkage Clustering Algorithm for Regression (oscar) are some of the popular feature grouping methods proposed in the literature which recover both sparsity and feature groups from the data. However, their predictive ability is affected adversely when the regression coefficients of adjacent feature groups are similar, but not exactly equal. This happens as these methods merge such adjacent feature groups erroneously, which is also called the misfusion problem. In order to solve this problem, in this paper, we propose a weighted l1 norm-based approach which is effective at recovering feature groups, despite the proximity of the coefficients of adjacent feature groups, building extremely accurate predictive models. This convex optimization problem is solved using the fast iterative soft-thresholding algorithm (FISTA). We depict how our approach is more effective at resolving the misfusion problem on synthetic datasets compared to existing feature grouping methods such as the elastic net, fused-lasso and oscar. We also evaluate the goodness of the model on real-world breast cancer gene expression and the 20-Newsgroups datasets.
[sparsity groups, FISTA, feature group recovery, regression analysis, high-dimensional data, regularization, convex programming, fast iterative soft-thresholding algorithm, breast cancer gene expression, regression coefficients, 20-Newsgroups datasets, group theory, predictive models, regularizers, feature extraction, regression, feature grouping, data handling, group lasso, deciphered grouping structure, weighted &#x2113;<sub>1</sub> norm-based approach, misfusion problem, convex optimization problem]
Learning Task Relational Structure for Multi-task Feature Learning
2016 IEEE 16th International Conference on Data Mining
None
2016
In multi-task learning, it is paramount to discover the relational structure of tasks and utilize the learned task structure. Previous works have been using the low-rank latent feature subspace to capture the task relations, and some of them aim to learn the group based relational structure of tasks. However, in many cases, the low-rank subspace may not exist for the specific group of tasks, thus using this paradigm would not work. To discover the task relational structures, we propose a novel multi-task learning method using the structured sparsity-inducing norms to automatically uncover the relations of tasks. Instead of imposing the low-rank constraint, our new model uses a more meaningful assumption, in which the tasks from the same relational group should share the common feature subspace. We can discover the group relational structure of tasks and learn the shared feature subspace for each task group, which help to improve the predictive performance. Our proposed algorithm avoids the high computational complexity of integer programming, thus it converges very fast. Empirical studies conducted on both synthetic and real-world data show that our method consistently outperforms related multi-task learning methods.
[Algorithm design and analysis, task relational structures, sparsity-inducing norms, Computational modeling, integer programming, convergence, predictive performance, Multi-Task Learning, Linear programming, group based task relational structure, Knowledge transfer, Optimization, Learning systems, multitask feature learning, Structured Sparsity-Inducing Norm, task relational structure learning, Computational efficiency, learning (artificial intelligence), latent feature subspace, computational complexity, Task Relational Structure]
Multi-view Clustering via Concept Factorization with Local Manifold Regularization
2016 IEEE 16th International Conference on Data Mining
None
2016
Real-world datasets often have representations in multiple views or come from multiple sources. Exploiting consistent or complementary information from multi-view data, multi-view clustering aims to get better clustering quality rather than relying on the individual view. In this paper, we propose a novel multi-view clustering method called multi-view concept clustering based on concept factorization with local manifold regularization, which drives a common consensus representation for multiple views. The local manifold regularization is incorporated into concept factorization to preserve the locally geometrical structure of the data space. Moreover, the weight of each view is learnt automatically and a co-normalized approach is designed to make fusion meaningful in terms of driving the common consensus representation. An iterative optimization algorithm based on the multiplicative rules is developed to minimize the objective function. Experimental results on nine reality datasets involving different fields demonstrate that the proposed method performs better than several state-of-the-art multi-view clustering methods.
[iterative methods, consensus representation, Symmetric matrices, data representation, feature extraction method, local manifold regularization, Linear programming, Loss measurement, matrix decomposition, Sparse matrices, real-world datasets, multiview data, nonnegative matrix factorization, Optimization, multiview concept clustering, Manifolds, optimisation, pattern clustering, feature extraction, multiplicative rules, geometrical data structure, concept factorization, iterative optimization algorithm, Kernel]
A Fast Distributed Classification Algorithm for Large-Scale Imbalanced Data
2016 IEEE 16th International Conference on Data Mining
None
2016
The Alternating Direction Method of Multipliers (ADMM) has been developed recently for distributed classification. Nevertheless, the widely-existing class imbalance problem has not been well investigated. Furthermore, previous imbalanced classification methods lack of efforts in studying the complex imbalance problem in a distributed environment. In this paper, we consider the imbalance problem as distributed data imbalance which includes three imbalance issues: (i) within-node class imbalance, (ii)between-node class imbalance, and (iii) between-node structure imbalance. In order to adequately deal with imbalanced data as well as improve time efficiency, a novel distributed Cost-Sensitive classification algorithm via Group-based ADMM (CS-GADMM) is proposed. Briefly, CS-GADMM derives the classification problem as a series of sub-problems with within-node class imbalance. To alleviate the time delay caused by between-node class imbalance, we propose a extension of dual coordinate descent method for the sub-problem optimization. Meanwhile, for between-node structure imbalance, we discreetly study the relationship between local functions, and combine the resulting local variables intra-group to update the global variables for prediction. The experimental results on various imbalanced datasets validate that CS-GADMM could be a efficient algorithm for imbalanced classification.
[pattern classification, Delay effects, distributed data imbalance, fast distributed classification algorithm, within-node class imbalance, distributed processing, between-node class imbalance, CS-GADMM, Data mining, Optimization, Convergence, Support vector machines, optimisation, large-scale imbalanced data, alternating direction method of multipliers, Distributed databases, distributed cost-sensitive classification algorithm via group-based ADMM, global variables, Software, between-node structure imbalance, local variable intragroup]
Differential Location Privacy for Sparse Mobile Crowdsensing
2016 IEEE 16th International Conference on Data Mining
None
2016
Sparse Mobile Crowdsensing (MCS) has become a compelling approach to acquire and make inference on urban-scale sensing data. However, participants risk their location privacy when reporting data with their actual sensing positions. To address this issue, we adopt e-differential-privacy in Sparse MCS to provide a theoretical guarantee for participants' location privacy regardless of an adversary's prior knowledge. Furthermore, to reduce the data quality loss caused by differential location obfuscation, we propose a privacypreserving framework with three components. First, we learn a data adjustment function to fit the original sensing data to the obfuscated location. Second, we apply a linear program to select an optimal location obfuscation function, which aims to minimize the uncertainty in data adjustment. We also propose a fast approximated variant. Third, we propose an uncertaintyaware inference algorithm to improve the inference accuracy of obfuscated data. Evaluations with real environment and traffic datasets show that our optimal method reduces the data quality loss by up to 42% compared to existing differential privacy methods.
[Data privacy, Uncertainty, urban-scale sensing data, differential location obfuscation, optimal location obfuscation function, data adjustment function learning, Mobile communication, linear program, linear programming, Servers, inference mechanisms, e-differential-privacy, differential location privacy, Temperature sensors, Privacy, mobile computing, uncertainty-aware inference, traffic datasets, participant location privacy, data privacy, sparse mobile crowdsensing, sparse MCS]
Robust Convex Clustering Analysis
2016 IEEE 16th International Conference on Data Mining
None
2016
Clustering is an unsupervised learning approach that explores data and seeks groups of similar objects. Many classical clustering models such as k-means and DBSCAN are based on heuristics algorithms and suffer from local optimal solutions and numerical instability. Recently convex clustering has received increasing attentions, which leverages the sparsity inducing norms and enjoys many attractive theoretical properties. However, convex clustering is based on Euclidean distance and is thus not robust against outlier features. Since the outlier features are very common especially when dimensionality is high, the vulnerability has greatly limited the applicability of convex clustering to analyze many real-world datasets. In this paper, we address the challenge by proposing a novel robust convex clustering method that simultaneously performs convex clustering and identifies outlier features. Specifically, the proposed method learns to decompose the data matrix into a clustering structure component and a group sparse component that captures feature outliers. We develop a block coordinate descent algorithm which iteratively performs convex clustering after outliers features are identified and eliminated. We also propose an efficient algorithm for solving the convex clustering by exploiting the structures on its dual problem. Moreover, to further illustrate the statistical stability, we present the theoretical performance bound of the proposed clustering method. Empirical studies on synthetic data and real-world data demonstrate that the proposed robust convex clustering can detect feature outliers as well as improve cluster quality.
[iterative methods, theoretical performance bound, Clustering methods, numerical instability, matrix decomposition, Sparse matrices, Optimization, group sparse component, k-means clustering models, Clustering algorithms, Robustness, Convex functions, local optimal solutions, heuristics algorithms, clustering structure component, unsupervised learning approach, convex programming, DBSCAN, robust convex clustering analysis, statistical stability, sparsity inducing norms, Unsupervised learning, unsupervised learning, pattern clustering, outlier features, Euclidean distance, data matrix decomposition, statistical analysis, feature outliers, block coordinate descent algorithm]
Bayesian Rule Sets for Interpretable Classification
2016 IEEE 16th International Conference on Data Mining
None
2016
A Rule Set model consists of a small number of short rules for interpretable classification, where an instance is classified as positive if it satisfies at least one of the rules. The rule set provides reasons for predictions, and also descriptions of a particular class. We present a Bayesian framework for learning Rule Set models, with prior parameters that the user can set to encourage the model to have a desired size and shape in order to conform with a domain-specific definition of interpretability. We use an efficient inference approach for searching for the MAP solution and provide theoretical bounds to reduce computation. We apply Rule Set models to ten UCI data sets and compare the performance with other interpretable and non-interpretable models.
[pattern classification, UCI data sets, Computational modeling, inference approach, Predictive models, association rules, Search problems, domain-specific interpretability definition, Data mining, inference mechanisms, interpretable classification, Bayesian modeling, Simulated annealing, Bayesian rule sets, classifier, Data models, learning rule set models, Bayes methods, learning (artificial intelligence), MAP, interpretable machine learning]
The Development of a Smart Taxicab Scheduling System: A Multi-source Data Fusion Perspective
2016 IEEE 16th International Conference on Data Mining
None
2016
Recent advances in vehicular networks, GPS and smartphone technologies have changed the paradigm of intelligent taxicab systems. Indeed, taxicab trajectories and online calling information have enabled us to provide more efficient and personalized services. However, existing approaches are not sufficient in exploiting cooperative scheduling techniques and utilizing real time calling information. To this end, in this paper, we model the time-varying regularities of traffic flows, activity ratios of passengers, and unoccupied taxicabs of road segments by mining statistical data on taxicab trajectories. Along this line, we propose a novel approach to calculate the expected revenue of possible routes for individual taxicabs while considering the influence of others, and at the same time, advance a dynamic taxicab scheduling mechanism with online taxicab calling information. Finally, we evaluate our algorithm on real-world taxicab data. Experimental results demonstrate that our approach outperforms existing alternative solutions in terms of average revenue of taxi drivers.
[taxicab trajectory, Roads, data mining, online taxicab calling information, sensor fusion, GPS, Smart taxicab scheduling, Vehicles, road vehicles, traffic information systems, road segments, scheduling, Multi-source, Real-time systems, vehicular networks, cooperative scheduling techniques, Trajectory, smartphone technology, traffic flows, multisource data fusion, real-time calling information, statistical data mining, intelligent transportation systems, dynamic taxicab scheduling mechanism, Scheduling, Public transportation, smart taxicab scheduling system, Data fusion, intelligent taxicab systems, time-varying regularity, statistical analysis]
Selecting Valuable Customers for Merchants in E-Commerce Platforms
2016 IEEE 16th International Conference on Data Mining
None
2016
An e-commerce website provides a platform for merchants to sell products to customers. While most existing research focuses on providing customers with personalized product suggestions by recommender systems, in this paper, we consider the role of merchants and introduce a parallel problem, i.e., how to select the most valuable customers for a merchant? Accurately answering this question can not only help merchants to gain more profits, but also benefit the ecosystem of e-commence platforms. To deal with this problem, we propose a general approach by taking into consideration the interest and profit of each customer to the merchant, i.e., select the customers who are not only interested in the merchant to ensure the visit of the merchant, but also capable of making good profits. Specifically, we first generate candidate customers for a given merchant by using traditional recommendation techniques. Then we select a set of the valuable customers from candidate customers, which has the balanced maximization between the interest and the profit metrics. Given the NP-hardness of the balanced maximization formulation, we further introduce efficient techniques to solve this maximization problem by exploiting the inherent submodularity property. Finally, extensive experimental results on a real-world dataset demonstrate the effectiveness of our proposed approach.
[Measurement, profit metrics, balanced maximization problem, TV, valuable customer selection, e-commerce platforms, Data mining, optimisation, recommender systems, parallel problem, submodularity property, Collaboration, Mathematical model, Web sites, merchants, NP-hardness problem, Recommender systems, Business, computational complexity, electronic commerce]
Canonical Consistent Weighted Sampling for Real-Value Weighted Min-Hash
2016 IEEE 16th International Conference on Data Mining
None
2016
Min-Hash, as a member of the Locality Sensitive Hashing (LSH) family for sketching sets, plays an important role in the big data era. It is widely used for efficiently estimating similarities of bag-of-words represented data and has been extended to dealing with multi-sets and real-value weighted sets. Improved Consistent Weighted Sampling (ICWS) has been recognized as the state-of-the-art for real-value weighted Min-Hash. However, the algorithmic implementation of ICWS is flawed because it violates the uniformity of the Min-Hash scheme. In this paper, we propose a Canonical Consistent Weighted Sampling (CCWS) algorithm, which not only retains the same theoretical complexity as ICWS but also strictly complies with the definition of Min-Hash. The experimental results demonstrate that the proposed CCWS algorithm runs faster than the state-of-the-arts while achieving similar classification performance on a number of real-world text data sets.
[text analysis, Complexity theory, Electronic mail, Quantization (signal), Min-Hash, similarity estimation, ICWS, Consistent weighted sampling, improved consistent weighted sampling, Big data, big data, canonical consistent weighted sampling algorithm, real-value weighted min-hash, pattern classification, Big Data, Indexes, theoretical complexity, sketching sets, text data sets, locality sensitive hashing family, CCWS algorithm, LSH family, classification performance, Approximation algorithms, bag-of-words represented data, Australia]
Spectrum-Revealing Cholesky Factorization for Kernel Methods
2016 IEEE 16th International Conference on Data Mining
None
2016
Kernel methods represent some of the most popular machine learning tools for data analysis. Since exact kernel methods can be prohibitively expensive for large problems, reliable low-rank matrix approximations and high-performance implementations have become indispensable for practical applications of kernel methods. In this work, we introduce spectrum-revealing Cholesky factorization, a reliable low-rank matrix factorization, for kernel matrix approximation. We also develop an efficient and effective randomized algorithm for computing this factorization. Our numerical experiments demonstrate that this algorithm is as effective as other Cholesky factorization based kernel methods on machine learning problems, but significantly faster.
[Algorithm design and analysis, kernel matrix approximation, approximation theory, matrix factorization, Machine learning algorithms, data analysis, Complexity theory, matrix decomposition, spectrum-revealing Cholesky factorization, machine learning, low-rank approximation, matrix approximations, kernel methods, Approximation algorithms, Prediction algorithms, Reliability, learning (artificial intelligence), Kernel, Spectrum-Revealing Cholesky factorization]
Topic Discovery for Short Texts Using Word Embeddings
2016 IEEE 16th International Conference on Data Mining
None
2016
Discovering topics in short texts, such as news titles and tweets, has become an important task for many content analysis applications. However, due to the lack of rich context information in short texts, the performance of conventional topic models on short texts is usually unsatisfying. In this paper, we propose a novel topic model for short text corpus using word embeddings. Continuous space word embeddings, which is proven effective at capturing regularities in language, is incorporated into our model to provide additional semantics. Thus we model each short document as a Gaussian topic over word embeddings in the vector space. In addition, considering that background words in a short text are usually not semantically related, we introduce a discrete background mode over word types to complement the continuous Gaussian topics. We evaluate our model on news titles from data sources like abcnews, showing that our model is able to extract more coherent topics from short texts compared with the baseline methods and learn better topic representation for each short document.
[Context, Electronic publishing, text analysis, data sources, baseline methods, news titles, topic model, short texts, Encyclopedias, Gaussian distribution, content analysis applications, abcnews, word embeddings, tweets, vectors, discrete background mode, topic discovery, Semantics, Gaussian processes, continuous space word embeddings, Internet, vector space, short text corpus, Gaussian topic]
Dynamic Contextual Multi Arm Bandits in Display Advertisement
2016 IEEE 16th International Conference on Data Mining
None
2016
We model the ad selection task as a multi-armed bandit problem. Standard assumptions in the multi-armed bandit (MAB) setting are that samples drawn from each arm are independent and identically distributed, rewards (or conversion rates in our scenario) are stationary and rewards feedback are immediate. Although the payoff function of an arm is allowed to evolve over time, the evolution is assumed to be slow. Display ads, on the other hand, are regularly created while others are removed from circulation. This can occur when budgets run out, campaign goal changes, holiday season ends and many other latent factors that go beyond the control of the ad selection system. Another big challenge is that the set of available ads is often extremely huge but standard multi-armed bandit strategies converge with linear time complexity that cannot accommodate the usually dynamic changes. Due to the above challenges and the restrictions of the original MAB, we propose a novel dynamic contextual MAB which tightly integrates components of dynamic conversion rates prediction, contextual learning and arm overlapping modeling in a principled framework. Besides we propose an accompanied meta analyses framework that allows us to conclude experiments in a more statistically robust manner. We demonstrate on a world leading demand side platform (DSP) that our framework can effectively discriminate premium arms and significantly outperform some standard variations of MAB to these settings.
[Algorithm design and analysis, Dynamic, Heuristic algorithms, dynamic contextual multiarm bandits, ad selection system, Stochastic processes, Predictive models, feedback, Contextual, rewards feedback, Meta Analyses, contextual learning, learning (artificial intelligence), demand side platform, Display Advertisement, display advertisement, Context, stationary feedback, DSP, advertising data processing, overlapping modeling, Standards, MAB, dynamic conversion rates prediction, Digital signal processing, Multi Arm Bandits]
Functional Regression with Mode-Sparsity Constraint
2016 IEEE 16th International Conference on Data Mining
None
2016
Functional data is ubiquitous in many domains such as healthcare, social media, manufacturing process, sensor networks, etc. Functional data analysis involves the analysis of data which is treated as infinite-dimensional continuous functions rather than discrete, finite-dimensional vectors. In this paper, we propose a novel function-on-function regression model based on mode-sparsity regularization. The main idea is to represent the regression coefficient function between predictor and response as the double expansion of basis functions, and then use mode-sparsity constraint to automatically filter out the irrelevant basis functions for both predictors and responses. The mode-sparsity regularization covers a wide spectrum of sparse models for function-on-function regression. The resulting optimization problem is challenging due to the non-smooth property of the mode-sparsity. We develop an efficient and convergence-guaranteed algorithm to solve the problem. The effectiveness of the proposed approach is verified on benchmark functional data sets in various domains.
[mode-sparsity regularization, optimization problem, Additives, data analysis, regression analysis, Predictive models, Linear programming, functional data analysis, infinite-dimensional continuous functions, Data mining, Optimization, Convergence, optimisation, functional regression, mode-sparsity constraint, Functional regression, basis function double-expansion, nonsmooth property, benchmark functional data sets, mode-sparsity, Data models, function-on-function regression model, convergence-guaranteed algorithm, regression coefficient function]
Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View That Includes Motifs, Discords and Shapelets
2016 IEEE 16th International Conference on Data Mining
None
2016
The all-pairs-similarity-search (or similarity join) problem has been extensively studied for text and a handful of other datatypes. However, surprisingly little progress has been made on similarity joins for time series subsequences. The lack of progress probably stems from the daunting nature of the problem. For even modest sized datasets the obvious nested-loop algorithm can take months, and the typical speed-up techniques in this domain (i.e., indexing, lower-bounding, triangular-inequality pruning and early abandoning) at best produce one or two orders of magnitude speedup. In this work we introduce a novel scalable algorithm for time series subsequence all-pairs-similarity-search. For exceptionally large datasets, the algorithm can be trivially cast as an anytime algorithm and produce high-quality approximate solutions in reasonable time. The exact similarity join algorithm computes the answer to the time series motif and time series discord problem as a side-effect, and our algorithm incidentally provides the fastest known algorithm for both these extensively-studied problems. We demonstrate the utility of our ideas for two time series data mining problems, including motif discovery and novelty discovery.
[time series data mining problems, time series motif, data mining, time series discord problem, time series subsequences, Data mining, shapelets, datatypes, nested-loop algorithm, Motif Discovery, Clustering algorithms, exact similarity join algorithm, matrix profile, motif discovery, Similarity Joins, search problems, novelty discovery, Time series analysis, scalable algorithm, time series, Indexes, all-pairs-similarity-search, Text processing, matrix algebra, Euclidean distance, Time Series, Approximation algorithms, modest sized datasets]
Deep Convolutional Factor Analyser for Multivariate Time Series Modeling
2016 IEEE 16th International Conference on Data Mining
None
2016
Deep generative models can perform dramatically better than traditional graphical models in a number of machine learning tasks. However, training such models remains challenging because their latent variables typically do not have an analytical posterior distribution, largely due to the nonlinear activation nodes. We present a deep convolutional factor analyser (DCFA) for multivariate time series modeling. Our network is constructed in a way that bottom layer nodes are independent. Through a process of up-sampling and convolution, higher layer nodes gain more temporal dependency. Our model can thus give a time series different representations at different depths. DCFA only consists of linear Gaussian nodes. Therefore, the posterior distributions of latent variables are also Gaussian and can be estimated easily using standard variational Bayes algorithm. We show that even without nonlinearity the proposed deep model can achieve state-of-the-art results in anomaly detection, classification and clustering using both synthetic and real-world datasets.
[Gaussian distribution, anomaly detection, multivariate time series modeling, Training, Analytical models, Convolution, latent variables, deep convolutional factor analyser, standard variational Bayes algorithm, convolution, learning (artificial intelligence), machine learning tasks, time series classification, pattern classification, temporal dependency, Time series analysis, DCFA, time series, linear Gaussian nodes, classification, factor analyser, deep generative models, pattern clustering, Hidden Markov models, Gaussian processes, graphical models, Data models, clustering, Bayes methods, solid modelling]
Incorporating Pre-Training in Long Short-Term Memory Networks for Tweets Classification
2016 IEEE 16th International Conference on Data Mining
None
2016
The paper presents deep learning models for tweets binary classification. Our approach is based on the Long Short-Term Memory (LSTM) recurrent neural network and hence expects to be able to capture long-term dependencies among words. We develop two models for tweets classification. The basic model, called LSTM-TC, takes word embeddings as input, uses the LSTM layer to derive semantic tweet representation, and applies logistic regression to predict tweet label. The basic LSTM-TC model, like other deep learning models, requires a large amount of well-labeled training data to achieve good performance. To address this challenge, we further develop an improved model, called LSTM-TC*, that incorporates a large amount of weakly-labeled data for classifying tweets. We present two approaches of constructing the weakly-labeled data. One is based on hashtag information and the other is based on the prediction output of some traditional classifier that does not need a large amount of well-labeled training data. Our LSTM-TC* model first learns tweet representation based on the weakly-labeled data, and then trains the logistic regression classifier based on the small amount of well-labeled data. Experimental results show that: (1) the proposed method can be successfully used for tweets classification and outperform existing state-of-the-art methods, (2) pre-training tweet representation, which utilizes weakly-labeled tweets, can significantly improve the accuracy of tweets classification.
[logistic regression classifier, long-term dependencies, regression analysis, tweets binary classification, Pre-training, Twitter, semantic tweet representation, Training, Deep learning, Semantics, weakly-labeled tweets, learning (artificial intelligence), logistic regression, hashtag information, pattern classification, recurrent neural network, LSTM, recurrent neural nets, Tweets classification, tweet label, deep learning models, Tagging, Logic gates, social networking (online), Data models, weakly-labeled data, well-labeled training data, long short-term memory networks, tweet representation, LSTM-TC model, Logistics]
Low-Rank Sparse Feature Selection for Patient Similarity Learning
2016 IEEE 16th International Conference on Data Mining
None
2016
Comparing and identifying similar patients is a fundamental task in medical domains - an efficient technique can, for example, help doctors to track patient cohorts, compare the effectiveness of treatments, or predict medical outcomes. The goal of patient similarity learning is to derive a clinically meaningful measure to evaluate the similarity amongst patients represented by their key clinical indicators. However, it is challenging to learn such similarity, as medical data are usually high dimensional, heterogeneous, and complex. In addition, a desirable patient similarity is dependent on particular clinical settings, which implies supervised learning scheme is more useful in medical domains. To address these, in this paper we present a novel similarity learning approach formulated as the generalized Mahalanobis similarity function with pairwise constraints. Considering there always exists some features non-discriminative and contains redundant information, we encode a low-rank structure to our similarity function to perform feature selection. We evaluate the proposed model on both UCI benchmarks and a real clinical dataset for several medical tasks, including patient retrieval, classification, and cohort discovery. The results show that our similarity model significantly outperforms many state-of-the-art baselines, and is effective at removing noisy or redundant features.
[Measurement, Algorithm design and analysis, clinical dataset, supervised learning, Medical services, Electronic mail, Sparse matrices, clinical indicators, low-rank sparse feature selection, patient treatment, generalized Mahalanobis similarity function, complex high dimensional heterogeneous medical data, similarity model, patient similarity learning, learning (artificial intelligence), feature selection, pattern classification, UCI benchmarks, cohort discovery, information retrieval, medical domains, Linear programming, low-rank structure, patient retrieval, classification, pairwise constraints, clinical settings, medical computing, Medical diagnostic imaging]
ROM: A Robust Online Multi-task Learning Approach
2016 IEEE 16th International Conference on Data Mining
None
2016
A series of online multi-task learning (OMTL) algorithms have been proposed to avoid the expensive training cost and poor adaptability of traditional batch multi-task learning (MTL) algorithms in recent years. However, these OMTL algorithms usually assume that all tasks are closely related, which may not hold in practical scenarios. More importantly, their theoretical reliability is weakened due to the lack of proof on the cumulative regrets. To overcome these limitations, we present a robust online multi-task classification framework (ROM) and its two optimization algorithms (ROM-PGD, ROM-RDA). The proposed algorithms can not only automatically capture the common features among all tasks and individual features for each task, but also identify the potential existence of outlier task. Theoretically, we prove that the regret bounds of these two algorithms are sub-linear compared with the best separating algorithm in hindsight. Empirical studies on both synthetic and real-world datasets also demonstrate the effectiveness of our proposed algorithms when compared with the state-of-the-art OMTL algorithms.
[Algorithm design and analysis, robust online multitask classification framework, pattern classification, robust, batch multitask learning algorithms, optimization algorithms, separating algorithm, outlier task, Electronic mail, cumulative regrets, Matrix decomposition, Read only memory, Optimization, ROM-PGD, ROM, multi-task learning, optimisation, ROM-RDA, robust online multitask learning approach, OMTL, Prediction algorithms, Robustness, learning (artificial intelligence), online learning]
Scalable Discrete Supervised Hash Learning with Asymmetric Matrix Factorization
2016 IEEE 16th International Conference on Data Mining
None
2016
Hashing methods map similar data to binary hashcodes with smaller hamming distance, and it has received a broad attention due to its low storage cost and fast retrieval speed. However, the existing limitations make the present algorithms difficult to deal with large-scale datasets: (1) discrete constraints are involved in the learning of the hash function, (2) pairwise or triplet similarity is adopted to generate efficient hashcodes, resulting both time and space complexity are greater than O(n2). To address these issues, we propose a novel discrete supervised hash learning framework which can be scalable to large-scale datasets. First, the learning procedure is decomposed into a binary classifier learning scheme and hashcodes learning scheme. Second, we adopt the Asymmetric Low-rank Matrix Factorization and propose the Fast Clustering-based Batch Coordinate Descent method, such that the time and space complexity is reduced to O(n). The proposed framework also provides a flexible paradigm to incorporate with arbitrary hash function, including deep neural networks. Experiments on large-scale datasets demonstrate that the proposed method is superior or comparable with state-of-the-art hashing algorithms.
[pattern classification, asymmetric matrix factorization, Discrete Method, time complexity, Hash Learning, matrix decomposition, Matrix decomposition, fast clustering-based batch coordinate descent method, Asymmetric Matrix Factorization, Training, Quantization (signal), Neural networks, Semantics, discrete constraints, Training data, Binary codes, binary classifier learning scheme, Information Retrieval, learning (artificial intelligence), neural nets, space complexity, deep neural networks, computational complexity, scalable discrete supervised hash learning framework]
Multi-type Co-clustering of General Heterogeneous Information Networks via Nonnegative Matrix Tri-Factorization
2016 IEEE 16th International Conference on Data Mining
None
2016
Many kinds of real world data can be modeled by a heterogeneous information network (HIN) which consists of multiple types of objects. Clustering plays an important role in mining knowledge from HIN. Several HIN clustering algorithms have been proposed in recent years. However, these algorithms suffer from one or moreof the following problems: (1) inability to model general HINs, (2) inability to simultaneously generate clusters for all types of objects, (3) inability to use similarity information of the objects with the same type. In this paper, we propose a powerful HIN clustering algorithm which can handle general HINs, simultaneously generate clusters for all types of objects, and use the similarity information of the same type of objects. First, we transform a general HIN into a meta-path-encoded relationship set. Second, we propose a nonnegative matrix tri-factorization multi-type co-clustering method, HMFClus, to cluster all types of objects in HIN simultaneously. Third, we integrate the information between the objects with the same type into HMFClus by using a similarity regularization. Extensive experiments on real world datasets show that the proposed algorithm outperforms the state-of-the-art methods.
[knowledge mining, multitype coclustering, Conferences, data mining, data modelling, similarity regularization, Transforms, heterogeneous information networks, HMFClus, Electronic mail, matrix decomposition, nonnegative matrix tri-factorization, Data mining, information networks, HIN clustering algorithm, pattern clustering, Semantics, Clustering algorithms, Software, meta-path-encoded relationship set]
Dynamic Poisson Factor Analysis
2016 IEEE 16th International Conference on Data Mining
None
2016
We introduce a novel dynamic model for discrete time-series data, in which the temporal sampling may be nonuniform. The model is specified by constructing a hierarchy of Poisson factor analysis blocks, one for the transitions between latent states and the other for the emissions between latent states and observations. Latent variables are binary and linked to Poisson factor analysis via Bernoulli-Poisson specifications. The model is derived for count data but can be readily modified for binary observations. We derive efficient inference via Markov chain Monte Carlo, that scales with the number of non-zeros in the data and latent binary states, yielding significant acceleration compared to related models. Experimental results on benchmark data show the proposed model achieves state-of-the-art predictive performance. Additional experiments on microbiome data demonstrate applicability of the proposed model to interesting problems in computational biology where interpretability is of utmost importance.
[latent states, computational biology, Correlation, inference, binary latent variables, binary observations, Analytical models, dynamic discrete time-series data model, Monte Carlo methods, biology computing, count data, microbiome data, Bernoulli-Poisson specifications, Load modeling, Biological system modeling, Computational modeling, time series, inference mechanisms, latent binary states, Markov chain Monte Carlo method, dynamic Poisson factor analysis, Hidden Markov models, nonuniform temporal sampling, Markov processes, Data models]
Gaussian Component Based Index for GMMs
2016 IEEE 16th International Conference on Data Mining
None
2016
Efficient similarity search for uncertain data is a challenging task in many modern data mining applications like image retrieval, speaker recognition and stock market analysis. A common way to model the uncertainty of data objects is using probability density functions in the form of Gaussian Mixture Models (GMMs), which have an ability to approximate arbitrary distribution. However, due to the possible unequal length of mixture models, the use of existing index techniques has serious problems for the objects modeled by GMMs. Either the techniques cannot handle GMMs or they have too many limitations. Hence, we propose a dynamic index structure, Gaussian Component based Index (GCI), for GMMs. GCI decomposes GMMs into the single, pairs, or n-lets of Gaussian components, stores these components into well studied index trees such as U-tree and Gauss-Tree, and refines the corresponding GMMs in a conservative but tight way. GCI supports both k-most-likely queries and probability threshold queries by means of Matching Probability. Extensive experimental evaluations of GCI demonstrate a considerable speed-up of similarity search on both synthetic and real-world data sets.
[Gauss-tree, similarity search, data mining, Gaussian distribution, index trees, statistical distributions, Covariance matrices, GMMs, Gaussian mixture model, query processing, Mixture models, Probability density function, Gaussian component based index techniques, matching probability, probability threshold query, probability, trees (mathematics), k-most-likely query, probability density functions, U-tree, Indexes, GCI, data object uncertainty, uncertain data, Gaussian processes, Gaussian mixture models, dynamic index structure, mixture models]
Multi-label Learning with Emerging New Labels
2016 IEEE 16th International Conference on Data Mining
None
2016
Multi-label learning is widely applied in many tasks, where an object possesses multiple concepts with each represented by a class label. Previous studies on multi-label learning have focused on a fixed set of class labels, i.e., the class label set of test data is the same as that in the training set. In many applications, however, the environment is open and new concepts may emerge with previously unseen instances. In order to maintain good predictive performance in this environment, a multi-label learning method must have the ability to detect and classify those instances with emerging new labels. To this end, we propose a new approach called Multi-label learning with Emerging New Labels (MuENL). It builds models with three functions: classify instances on currently known labels, detect the emergence of a new label in new instances, and construct a new classifier for each new label that works collaboratively with the classifier for known labels. Our empirical evaluation shows the effectiveness of MuENL.
[pattern classification, learnware, Correlation, class label, Predictive models, MuENL, new classifier construction, Training, Learning systems, new label emergence detection, emerging new labels, incremental learning, Detectors, Robustness, Data models, learning (artificial intelligence), multilabel learning method, instance classification, multi-label learning]
[Publisher's information]
2016 IEEE 16th International Conference on Data Mining
None
2016
Provides a listing of current committee members and society officers.
[]
Message from the Conference Chairs
2017 IEEE International Conference on Data Mining
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from Program Co-Chairs
2017 IEEE International Conference on Data Mining
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2017 IEEE International Conference on Data Mining
None
2017
Provides a listing of current committee members and society officers.
[]
Program Committee
2017 IEEE International Conference on Data Mining
None
2017
Provides a listing of current committee members and society officers.
[]
Split Miner: Discovering Accurate and Simple Business Process Models from Event Logs
2017 IEEE International Conference on Data Mining
None
2017
The problem of automated discovery of process models from event logs has been intensively researched in the past two decades. Despite a rich field of proposals, state-of-the-art automated process discovery methods suffer from two recurrent deficiencies when applied to real-life logs: (i) they produce large and spaghetti-like models; and (ii) they produce models that either poorly fit the event log (low fitness) or highly generalize it (low precision). Striking a tradeoff between these quality dimensions in a robust and scalable manner has proved elusive. This paper presents an automated process discovery method that produces simple process models with low branching complexity and consistently high and balanced fitness, precision and generalization, while achieving execution times 2-6 times faster than state-of-the-art methods on a set of 12 real-life logs. Further, our approach guarantees deadlock-freedom for cyclic process models and soundness for acyclic. Our proposal combines a novel approach to filter the directly-follows graph induced by an event log, with an approach to identify combinations of split gateways that accurately capture the concurrency, conflict and causal relations between neighbors in the directly-follows graph.
[balanced fitness, low branching complexity, graph theory, data mining, Process Mining, Event Log, real-life logs, Complexity theory, Concurrent computing, automated process discovery method, split gateways, cyclic process models, Directly-follows Graph, Computational modeling, Automated Process Discovery, Split Miner, spaghetti-like model, simple business process models, directly-follows graph, concurrency, BPMN, causal relations, Logic gates, System recovery, event log, business data processing, conflict]
A Deep Transfer Learning Approach for Improved Post-Traumatic Stress Disorder Diagnosis
2017 IEEE International Conference on Data Mining
None
2017
Post-traumatic stress disorder (PTSD) is a traumatic-stressor related disorder developed by exposure to a traumatic or adverse environmental event that caused serious harm or injury. Structured interview is the only widely accepted clinical practice for PTSD diagnosis but suffers from several limitations including the stigma associated with the disease. Diagnosis of PTSD patients by analyzing speech signals has been investigated as an alternative since recent years, where speech signals are processed to extract frequency features and these features are then fed into a classification model for PTSD diagnosis. In this paper, we developed a deep belief network (DBN) model combined with a transfer learning (TL) strategy for PTSD diagnosis. We computed three categories of speech features and utilized the DBN model to fuse these features. The TL strategy was utilized to transfer knowledge learned from a large speech recognition database, TIMIT, for PTSD detection where PTSD patient data is difficult to collect. We evaluated the proposed methods on two PTSD speech databases, each of which consists of audio recordings from 26 patients. We compared the proposed methods with other popular methods and showed that the state-of-the-art support vector machine (SVM) classifier only achieved an accuracy of 57.68%, and TL strategy boosted the performance of the DBN from 61.53% to 74.99%. Altogether, our method provides a pragmatic and promising tool for PTSD diagnosis.
[TIMIT database, Solid modeling, PTSD detection, speech recognition database, injuries, Transfer learning, deep belief network model, Mel frequency cepstral coefficient, Deep learning, PTSD patient diagnosis, speech recognition, patient treatment, feature extraction, improved posttraumatic stress disorder diagnosis, Post-traumatic stress disorder, speech processing, deep-transfer learning strategy, speech features, speech signals, belief networks, learning (artificial intelligence), Interviews, PTSD speech databases, support vector machines, traumatic-stressor related disorder, diseases, DBN model, Deep belief network, Stress, TL strategy, signal classification, medical disorders, medical signal processing, frequency feature extraction, Speech recognition, Speech, Feature extraction, patient diagnosis]
Knowledge Guided Short-Text Classification for Healthcare Applications
2017 IEEE International Conference on Data Mining
None
2017
The need for short-text classification arises in many text mining applications particularly health care applications. In such applications shorter texts mean linguistic ambiguity limits the semantic expression, which in turns would make typical methods fail to capture the exact semantics of the scarce words. This is particularly true in health care domains when the text contains domain-specific or infrequently appearing words, whose embedding can not be easily learned due to the lack of training data. Deep neural network has shown great potentials in boost the performance of such problems according to its strength on representation capacity. In this paper, we propose a bidirectional long short-term memory (BI-LSTM) recurrent network to address the short-text classification problem that can be used in two settings. Firstly when a knowledge dictionary is available we adopt the well-known attention mechanism to guide the training of network using the domain knowledge in the dictionary. Secondly, to address the cases when domain knowledge dictionary is not available, we present a multi-task model to jointly learn the domain knowledge dictionary and do the text classification task simultaneously. We apply our method to a real-world interactive healthcare system and an extensively public available ATIS dataset. The results show that our model can positively grasp the key point of the text and significantly outperforms many state-of-the-art baselines.
[pattern classification, text analysis, Dictionaries, interactive healthcare system, data mining, recurrent neural nets, domain knowledge dictionary, semantic expression, Valves, bidirectional long short-term memory recurrent network, knowledge guided short-text classification, Diseases, BI-LSTM recurrent network, Neural networks, Semantics, text mining, learning (artificial intelligence), scarce words, health care, deep neural network]
A Generic Framework for Interesting Subspace Cluster Detection in Multi-attributed Networks
2017 IEEE International Conference on Data Mining
None
2017
Detection of interesting (e.g., coherent or anomalous) clusters has been studied extensively on plain or univariate networks, with various applications. Recently, algorithms have been extended to networks with multiple attributes for each node in the real-world. In a multi-attributed network, often, a cluster of nodes is only interesting for a subset (subspace) of attributes, andthis type of clusters is called subspace clusters. However, in the current literature, few methods are capable of detecting subspace clusters, which involves concurrent feature selection and network cluster detection. These relevant methods are mostly heuristic-driven and customized for specific application scenarios. In this work, we present a generic and theoretical framework for detection of interesting subspace clusters in large multi-attributed networks. Specifically, we propose a subspace graph-structured matching pursuit algorithm, namely, SG-Pursuit, to address a broad class of such problems for different scorefunctions (e.g., coherence or anomalous functions) and topology constraints (e.g., connected subgraphs and dense subgraphs). We prove that our algorithm 1) runs in nearly-linear time on the network size and the total number of attributes and 2) enjoys rigorous guarantees (geometrical convergence rate and tight error bound) analogous to those of the state-of-the-art algorithms for sparse feature selection problems and subgraph detection problems. As a case study, we specialize SG-Pursuit to optimizea number of well-known score functions for two typical tasks, including detection of coherent dense and anomalous connected subspace clusters in real-world networks. Empirical evidence demonstrates that our proposed generic algorithm SG-Pursuit is superior over state-of-the-art methods that are designed specifically for these two tasks.
[Algorithm design and analysis, iterative methods, subspace graph, graph theory, data mining, multiple attributes, concurrent feature selection, scorefunctions, Anomalous pattern detection, subgraph detection problems, Subspace cluster detection, Clustering algorithms, feature selection, sparse feature selection problems, subspace cluster detection, anomalous connected subspace clusters, Dense subgraph mining, Social network services, Matching pursuit algorithms, Attributed networks, subspace graph-structured matching pursuit, network cluster detection, topology constraints, Surveillance, pattern clustering, SG-Pursuit, generic algorithm SG-Pursuit, pursuit algorithm, Feature extraction, social networking (online), univariate networks, generic framework, coherent dense subspace clusters, interesting subspace cluster detection, computational complexity]
Revisiting Spectral Graph Clustering with Generative Community Models
2017 IEEE International Conference on Data Mining
None
2017
The methodology of community detection can be divided into two principles: imposing a network model on a given graph, or optimizing a designed objective function. The former provides guarantees on theoretical detectability but falls short when the graph is inconsistent with the underlying model. The latter is model-free but fails to provide quality assurance for the detected communities. In this paper, we propose a novel unified framework to combine the advantages of these two principles. The presented method, SGC-GEN, not only considers the detection error caused by the corresponding model mismatch to a given graph, but also yields a theoretical guarantee on community detectability by analyzing Spectral Graph Clustering (SGC) under GENerative community models (GCMs). SGC-GEN incorporates the predictability on correct community detection with a measure of community fitness to GCMs. It resembles the formulation of supervised learning problems by enabling various community detection loss functions and model mismatch metrics. We further establish a theoretical condition for correct community detection using the normalized graph Laplacian matrix under a GCM, which provides a novel data-driven loss function for SGC-GEN. In addition, we present an effective algorithm to implement SGC-GEN, and show that the computational complexity of SGC-GEN is comparable to the baseline methods. Our experiments on 18 real-world datasets demonstrate that SGC-GEN possesses superior and robust performance compared to 6 baseline methods under 7 representative clustering metrics.
[Measurement, SGC-GEN, graph theory, designed objective function, normalized graph Laplacian matrix, model mismatch, Stochastic processes, supervised learning, network theory (graphs), model mismatch metrics, error detection, network model, Spectral Graph Clustering, data-driven loss function, learning (artificial intelligence), community detection loss functions, community detection, Laplace equations, generative community models, community fitness, Computational modeling, generative models, Linear programming, theoretical detectability, spectral algorithms, Standards, matrix algebra, pattern clustering, community detectability, Supervised learning, computational complexity]
Improving I/O Complexity of Triangle Enumeration
2017 IEEE International Conference on Data Mining
None
2017
In the age of big data, many graph algorithms are now required to operate in external memory and deliver performance that does not significantly degrade with the scale of the problem. One particular area that frequently deals with graphs larger than RAM is triangle listing, where the algorithms must carefully piece together edges from multiple partitions to detect cycles. In recent literature, two competing proposals (i.e., Pagh and PCF) have emerged; however, neither one is universally better than the other. Since little is known about the I/O cost of PCF or how these methods compare to each other, we undertake an investigation into the properties of these algorithms, model their I/O cost, understand their shortcomings, and shed light on the conditions under which each method defeats the other. This insight leads us to develop a novel framework we call Trigon that surpasses the I/O performance of both previous techniques in all graphs and under all RAM conditions.
[Algorithm design and analysis, triangle enumeration, I/O complexity, algorithms, RAM conditions, multiple partitions, graph theory, Random access memory, Complexity theory, storage management, triangle listing, graphs, Pagh, data structures, big data, graph algorithms, random-access storage, Image edge detection, Color, Big Data, Partitioning algorithms, external memory, graph mining, PCF, Memory management, computational complexity]
TensorCast: Forecasting with Context Using Coupled Tensors (Best Paper Award)
2017 IEEE International Conference on Data Mining
None
2017
Given an heterogeneous social network, can we forecast its future? Can we predict who will start using a given hashtag on twitter? Can we leverage side information, such as who retweets or follows whom, to improve our membership forecasts? We present TensorCast, a novel method that forecasts time-evolving networks more accurately than current state of the art methods by incorporating multiple data sources in coupled tensors. TensorCast is (a) scalable, being linearithmic on the number of connections; (b) effective, achieving over 20% improved precision on top-1000 forecasts of community members; (c) general, being applicable to data sources with different structure. We run our method on multiple real-world networks, including DBLP and a Twitter temporal network with over 310 million non-zeros, where we predict the evolution of the activity of the use of political hashtags.
[Twitter temporal network, heterogeneous social network, politics, Tensor Forecasting, Predictive models, Twitter, side information leveraging, Forecasting, political hashtags, membership forecasts, TensorCast, real-world networks, Computer science, time-evolving networks, Tensile stress, Link Prediction, Contextual Recommendation, multiple data sources, Coupled Tensor Factorization, Tagging, social networking (online), coupled tensors, Twitter hashtag]
Situation Aware Multi-task Learning for Traffic Prediction
2017 IEEE International Conference on Data Mining
None
2017
Due to the recent vast availability of transportation traffic data, major research efforts have been devoted to traffic prediction, which is useful in many applications such as urban planning, traffic management and navigations systems. Current prediction methods that independently train a model per traffic sensor cannot accurately predict traffic in every situation (e.g., rush hours, constructions and accidents) because there may not exist sufficient training samples per sensor for all situations. To address this shortcoming, our core idea is to explore the commonalities of prediction tasks across multiple sensors who behave similarly in a specific traffic situation. Instead of building a model independently per sensor, we propose a Multi-Task Learning (MTL) framework that aims to first automatically identify the traffic situations and then simultaneously build one forecasting model for similar-behaving sensors per traffic situation. The key innovation here is that instead of the straightforward application of MTL where each "task" corresponds to a sensor, we relate each MTL's "task" to a traffic situation. Specifically, we first identify these traffic situations by running clustering algorithms on all sensors' data. Subsequently, to enforce the commonalities under each identified situation, we use the group Lasso regularization in MTL to select a common set of features for the prediction tasks, and we adapt efficient FISTA algorithm with guaranteed convergence rate. We evaluated our methods with a large volume of real-world traffic sensor data; our results show that by incorporating traffic situations, our proposed MTL framework performs consistently better than naively applying MTL per sensor. Moreover, our holistic approach, under different traffic situations, outperforms all the best traffic prediction approaches for a given situation by up to 18% and 30% in short and long term predictions, respectively.
[Roads, MTL, convergence, Predictive models, situation-aware multi-task learning, traffic management, Data mining, Training, situation aware multitask learning, navigations systems, clustering algorithms, transportation traffic data, group Lasso regularization, long term predictions, short term predictions, learning (artificial intelligence), similar-behaving sensors, FISTA algorithm, traffic prediction, holistic approach, data analysis, traffic prediction approaches, multitask learning framework, prediction tasks, traffic engineering computing, real-world traffic sensor data, transportation, traffic situation clustering, pattern clustering, long term traffic forecasting, specific traffic situation, traffic situations, convergence rate, Accidents]
Large Scale Kernel Methods for Online AUC Maximization
2017 IEEE International Conference on Data Mining
None
2017
Learning to optimize AUC performance for classifying label imbalanced data in online scenarios has been extensively studied in recent years. Most of the existing work has attempted to address the problem directly in the original feature space, which may not suitable for non-linearly separable datasets. To solve this issue, some kernel-based learning methods are proposed for non-linearly separable datasets. However, such kernel approaches have been shown to be inefficient and failed to scale well on large scale datasets in practice. Taking this cue, in this work, we explore the use of scalable kernel-based learning techniques as surrogates to existing approaches: random Fourier features and Nystro&#x0308;m method, for tackling the problem and bring insights to the differences between the two methods based on their online performance. In contrast to the conventional kernel-based learning methods which suffer from high computational complexity of the kernel matrix, our proposed approaches elevate this issue with linear features that approximate the kernel function/matrix. Specifically, two different surrogate kernel-based learning models are presented for addressing the online AUC maximization task: (i) the Fourier Online AUC Maximization (FOAM) algorithm that samples the basis functions from a data-independent distribution to approximate the kernel functions; and (ii) the Nystro&#x0308;m Online AUC Maximization (NOAM) algorithm that samples a subset of instances from the training data to approximate the kernel matrix by a low rank matrix. Another novelty of the present work is the proposed mini-batch Online Gradient Descent method for model updating to control the noise and reduce the variance of gradients. We provide theoretical analyses for the two proposed algorithms. Empirical studies on commonly used large scale datasets show that the proposed algorithms outperformed existing state-of-the-art methods in terms of both AUC performance and computational efficiency.
[scale datasets, Machine learning algorithms, surrogate kernel-based learning models, AUC performance, Learning systems, Nystro&#x0308;m Online AUC Maximization algorithm, label imbalanced data, FOAM algorithm, training data, kernel matrix, learning techniques, linear features, Fourier Online AUC Maximization algorithm, learning (artificial intelligence), kernel approaches, Kernel, gradient methods, online performance, learning methods, approximation theory, pattern classification, scalable kernel, Computational modeling, random Fourier features, online scenarios, mini-batch Online Gradient Descent method, Support vector machines, basis functions, low rank matrix, nonlinearly separable datasets, Approximation algorithms, NOAM algorithm, online AUC maximization task, scale Kernel methods, data-independent distribution, conventional kernel, computational complexity]
A Hyperplane-Based Algorithm for Semi-Supervised Dimension Reduction
2017 IEEE International Conference on Data Mining
None
2017
We consider the semi-supervised dimension reduction problem: given a high dimensional dataset with a small number of labeled data and huge number of unlabeled data, the goal is to find the low-dimensional embedding that yields good classification results. Most of the previous algorithms for this task are linkage-based algorithms. They try to enforce the must-link and cannot-link constraints in dimension reduction, leading to a nearest neighbor classifier in low dimensional space. In this paper, we propose a new hyperplane-based semi-supervised dimension reduction method-the main objective is to learn the low-dimensional features that can both approximate the original data and form a good separating hyperplane. We formulate this as a non-convex optimization problem and propose an efficient algorithm to solve it. The algorithm can scale to problems with millions of features and can easily incorporate non-negative constraints in order to learn interpretable non-negative features. Experiments on real world datasets demonstrate that our hyperplane-based dimension reduction method outperforms state-of-art linkage-based methods when very few labels are available.
[separating hyperplane, Electronic mail, Optimization, data reduction, high dimensional dataset, nonnegative constraints, nonnegative features, Semi-supervised learning, optimization, low dimensional space, low-dimensional embedding, nonconvex optimization problem, learning (artificial intelligence), concave programming, pattern classification, cannot-link constraints, nearest neighbor classifier, Linear programming, semisupervised dimension reduction method, low-dimensional features, Support vector machines, Dimension reduction, semisupervised dimension reduction problem, must-link constraints, Approximation algorithms, unlabeled data, Principal component analysis]
IterativE Grammar-Based Framework for Discovering Variable-Length Time Series Motifs
2017 IEEE International Conference on Data Mining
None
2017
In recent years, finding repetitive similar patterns in time series has become a popular problem. These patterns are called time series motifs. Recent studies show that using grammar compression algorithms to find repeating patterns from the symbolized time series holds promise in discovering approximate motifs with variable length. However, grammar compression algorithms are traditionally designed for string compression. Therefore, existing work on grammar induction has not fully utilized much available information that can be used to enhance the performance of the algorithms. In this work, an iterative framework based on grammar induction is proposed. In each iteration, a revision operator called Noise Reduction Operator is applied to revise the symbolized time series string based on the rules returned from a base grammar induction algorithm. In our experiments, we show that the proposed work can find motifs of the same quality, with much faster running time compared to the state-of-the-art variable-length exact motif discovery algorithm in real world time series data.
[Algorithm design and analysis, variable-length exact motif discovery algorithm, IterativE grammar, Time series analysis, Noise reduction, variable-length time series motifs, approximate motifs, time series, repetitive similar patterns, Noise Reduction Operator, Grammar, Data mining, world time series data, grammar compression algorithms, string compression, grammars, symbolized time series string, grammar induction, Approximation algorithms, iteration, repeating patterns, string matching, revision operator]
Matrix Profile VIII: Domain Agnostic Online Semantic Segmentation at Superhuman Performance Levels
2017 IEEE International Conference on Data Mining
None
2017
Unsupervised semantic segmentation in the time series domain is a much-studied problem due to its potential to detect unexpected regularities and regimes in poorly understood data. However, the current techniques have several shortcomings, which have limited the adoption of time series semantic segmentation beyond academic settings for three primary reasons. First, most methods require setting/learning many parameters and thus may have problems generalizing to novel situations. Second, most methods implicitly assume that all the data is segmentable, and have difficulty when that assumption is unwarranted. Finally, most research efforts have been confined to the batch case, but online segmentation is clearly more useful and actionable. To address these issues, we present an algorithm which is domain agnostic, has only one easily determined parameter, and can handle data streaming at a high rate. In this context, we test our algorithm on the largest and most diverse collection of time series datasets ever considered, and demonstrate our algorithm's superiority over current solutions. Furthermore, we are the first to show that semantic segmentation may be possible at superhuman performance levels.
[Heart, time series datasets, Online Algorithms, Time series analysis, superhuman performance levels, time series, domain agnostic online semantic segmentation, time series domain, unsupervised learning, Motion segmentation, Semantics, image segmentation, unsupervised semantic segmentation, Hidden Markov models, Clustering algorithms, time series semantic segmentation, matrix profile, Time Series, Semantic Segmentation, academic settings, Sensors, online segmentation]
Overlapping Community Detection via Constrained PARAFAC: A Divide and Conquer Approach
2017 IEEE International Conference on Data Mining
None
2017
The task of community detection over complex networks is of paramount importance in a multitude of applications. The present work puts forward a top-to-bottom community identification approach, termed DC-EgoTen, in which an egonet-tensor (EgoTen) based algorithm is developed in a divide-and-conquer (DC) fashion for breaking the network into smaller subgraphs, out of which the underlying communities progressively emerge. In particular, each step of DC-EgoTen forms a multi-dimensional egonet-based representation of the graph, whose induced structure enables casting the task of overlapping community identification as a constrained PARAFAC decomposition. Thanks to the higher representational capacity of tensors, the novel egonet-based representation improves the quality of detected communities by capturing multi-hop connectivity patterns of the network. In addition, the top-to-bottom approach ensures successive refinement of identified communities, so that the desired resolution is achieved. Synthetic as well as real-world tests corroborate the effectiveness of DC-EgoTen.
[egonet-tensor based algorithm, multidimensional egonet-based representation, divide and conquer methods, divide-and-conquer approach, constrained PARAFAC, graph theory, overlapping community identification, egonet subgraphs, tensors, complex networks, Sparse matrices, DC-EgoTen, Optimization, constrained PARAFAC decomposition, Slabs, top-to-bottom community identification approach, subgraphs, Community detection, Minimization, detected communities, identified communities, overlapping communities, Matrix decomposition, Tensile stress, pattern clustering, multihop connectivity patterns, tensor decomposition]
Scalable Algorithms for Locally Low-Rank Matrix Modeling
2017 IEEE International Conference on Data Mining
None
2017
We consider the problem of modeling data matrices with locally low rank (LLR) structure, a generalization of the popular low rank structure widely used in a variety of real world application domains ranging from medical imaging to recommendation systems. While LLR modeling has been found to be promising in real world application domains, limited progress has been made on the design of scalable algorithms for such structures. In this paper, we consider a convex relaxation of LLR structure, and propose an efficient algorithm based on dual projected gradient descent (D-PGD) for computing the proximal operator. While the original problem is non-smooth, so that primal (sub)gradient algorithms will be slow, we show that the proposed D-PGD algorithm has geometrical convergence rate. We present several practical ways to further speed up the computations, including acceleration and approximate SVD computations. With experiments on both synthetic and real data from MRI (magnetic resonance imaging) denoising, we illustrate the superior performance of the proposed D-PGD algorithm compared to several baselines.
[Algorithm design and analysis, MRI denoising, Noise reduction, LLR structure, Convergence, algorithm theory, data matrices, primalgradient algorithms, gradient methods, singular value decomposition, locally low rank structure, Computational modeling, convex programming, low-rank matrix modeling, D-PGD algorithm, world application domains, magnetic resonance imaging denoising, matrix algebra, recommendation systems, dual projected gradient descent, Approximation algorithms, original problem, approximate SVD computations, Acceleration, medical imaging]
A Self-Adaptive Sliding Window Based Topic Model for Non-uniform Texts
2017 IEEE International Conference on Data Mining
None
2017
The contents generated from different data sources are usually non-uniform, such as long texts produced by news websites and short texts produced by social media. Uncovering topics over large-scale non-uniform texts becomes an important task for analyzing network data. However, the existing methods may fail to recognize the difference between long texts and short texts. To address this problem, we propose a novel topic modeling method for non-uniform text topic modeling referred to as self-adaptive sliding window based topic model (SSWTM). Specifically, in all kinds of texts, relevant words have a closer distance to each other than irrelevant words. Based on this assumption, SSWTM extracts relevant words by using a selfadaptive sliding window and models on the whole corpus. The self-adaptive sliding window can filter noisy information and change the size of a window according to different text contents. Experimental results on short texts from Twitter and long texts from Chinese news articles demonstrate that our method can discover more coherent topics for non-uniform texts compared with state-of-the-art methods.
[text contents, Adaptation models, Solid modeling, text analysis, large-scale nonuniform texts, data sources, coherent topics, self-adaptive sliding window based topic model, Computational modeling, natural language processing, short texts, SSWTM, Probabilistic logic, Twitter, Data mining, nonuniform text topic, Analytical models, selfadaptive sliding window, social networking (online), long texts, Chinese news articles, relevant words]
Kernel Conditional Clustering
2017 IEEE International Conference on Data Mining
None
2017
Clustering results are often affected by covariates that are independent of the clusters one would like to discover. Traditionally, Alternative Clustering algorithms can be used to solve such a problem. However, these suffer from at least one of the following problems: i) continuous covariates or non-linearly separable clusters cannot be handled; ii) assumptions are made about the distribution of the data; iii) one or more hyper-parameters need to be set. Here we propose a novel algorithm, named Kernel Conditional Clustering (KCC), whose objective is derived from a kernel based conditional dependence measure. KCC is parameter-light and makes no assumptions about the cluster structure, the covariates, or the distribution of the data. On both simulated and real-world datasets, the proposed KCC algorithm detects the ground truth cluster structures more accurately than state-of-the-art alternative clustering methods.
[KCC algorithm, Conditional Dependence Measure, Alternative Clustering, Linear programming, alternative clustering algorithms, continuous covariates, Conditional Clustering, Covariance matrices, nonlinearly separable clusters, Optimization, conditional dependence measure, ground truth cluster structures, kernel conditional clustering, pattern clustering, Clustering algorithms, cluster structure, Random variables, Kernel, Bioinformatics]
Data-Driven Utilization-Aware Trip Advisor for Bike-Sharing Systems
2017 IEEE International Conference on Data Mining
None
2017
Rapid development of bike-sharing systems has brought people enormous convenience during the past decade. On the other hand, high transport flexibility comes with dynamic distribution of shared bikes, leading to an unbalanced bike usage and growing maintenance cost. In this paper, we consider to rebalance bicycle utilization by means of directing users to different stations. For the first time, we devise a trip advisor that recommends bike check-in and check-out stations with joint consideration of service quality and bicycle utilization. From historical data, we firstly identify that biased bike usage is rooted from circumscribed bicycle circulation among few active stations. Therefore, with defined station activeness, we optimize the bike circulation by leading users to shift bikes between highly active stations and inactive ones. We extensively evaluate the performance of our design through real-world datasets. Evaluation results show that the percentage of frequent used bikes decreases by 33.6% on usage number and 28.6% on usage time.
[Meters, Legged locomotion, Urban areas, biased bike usage, Maintenance engineering, Probabilistic logic, bicycle utilization, traffic engineering computing, shared bikes, transportation, data-driven utilization-aware trip advisor, Pollution, historical data, bike-sharing systems, bike check-in, bicycle circulation, Bicycles, bike circulation, bicycles, unbalanced bike usage]
Multi-task Multi-modal Models for Collective Anomaly Detection
2017 IEEE International Conference on Data Mining
None
2017
This paper proposes a new framework for anomaly detection when collectively monitoring many complex systems. The prerequisite for condition-based monitoring in industrial applications is the capability of (1) capturing multiple operational states, (2) managing many similar but different assets, and (3) providing insights into the internal relationship of the variables. To meet these criteria, we propose a multi-task learning approach based on a sparse mixture of sparse Gaussian graphical models (GGMs). Unlike existing fused- and group-lasso-based approaches, each task is represented by a sparse mixture of sparse GGMs, and can handle multi-modalities. We develop a variational inference algorithm combined with a novel sparse mixture weight selection algorithm. To handle issues in the conventional automatic relevance determination (ARD) approach, we propose a new &#x2113;<sub>0</sub>-regularized formulation that has guaranteed sparsity in mixture weights. We show that our framework eliminates well-known issues of numerical instability in the iterative procedure of mixture model learning. We also show better performance in anomaly detection tasks on real-world data sets. To the best of our knowledge, this is the first proposal of multi-task GGM learning allowing multi-modal distributions.
[sparse Gaussian graphical models, mixture model learning, regression analysis, variational inference algorithm, Electronic mail, multitask learning approach, group-lasso-based approaches, sparse GGM, multitask multimodal models, anomaly detection tasks, Mathematical model, learning (artificial intelligence), Monitoring, data analysis, multitask GGM learning, collective anomaly detection, inference mechanisms, sparse mixture weight selection algorithm, Anomaly detection, condition-based monitoring, conventional automatic relevance determination approach, ARD, Gaussian processes, industrial applications, Data models, Bayes methods]
Exploratory Analysis of Graph Data by Leveraging Domain Knowledge
2017 IEEE International Conference on Data Mining
None
2017
Given the soaring amount of data being generated daily, graph mining tasks are becoming increasingly challenging, leading to tremendous demand for summarization techniques. Feature selection is a representative approach that simplifies a dataset by choosing features that are relevant to a specific task, such as classification, prediction, and anomaly detection. Although it can be viewed as a way to summarize a graph in terms of a few features, it is not well-defined for exploratory analysis, and it operates on a set of observations jointly rather than conditionally (i.e., feature selection from many graphs vs. selection for an input graph conditioned on other graphs). In this work, we introduce EAGLE (Exploratory Analysis of Graphs with domain knowLEdge), a novel method that creates interpretable, feature-based, and domain-specific graph summaries in a fully automatic way. That is, the same graph in different domains-e.g., social science and neuroscience-will be described via different EAGLE summaries, which automatically leverage the domain knowledge and expectations. We propose an optimization formulation that seeks to find an interpretable summary with the most representative features for the input graph so that it is: diverse, concise, domain-specific, and efficient. Extensive experiments on synthetic and real-world datasets with up to ~1M edges and ~400 features demonstrate the effectiveness and efficiency of EAGLE and its benefits over existing methods. We also show how our method can be applied to various graph mining tasks, such as classification and exploratory analysis.
[Measurement, domain knowledge, Feature selection, data analysis, graph mining tasks, graph theory, Domain knowledge, data mining, graph data, exploratory analysis, Data mining, Optimization, Analytical models, summarization techniques, Neuroscience, knowledge based systems, domain-specific graph summaries, representative features, Feature extraction, representative approach, EAGLE summaries, feature selection, Summarization]
Efficiently Discovering Locally Exceptional Yet Globally Representative Subgroups
2017 IEEE International Conference on Data Mining
None
2017
Subgroup discovery is a local pattern mining technique to find interpretable descriptions of sub-populations that stand out on a given target variable. That is, these sub-populations are exceptional with regard to the global distribution. In this paper we argue that in many applications, such as scientific discovery, subgroups are only useful if they are additionally representative of the global distribution with regard to a control variable. That is, when the distribution of this control variable is the same, or almost the same, as over the whole data. We formalise this objective function and give an efficient algorithm to compute its tight optimistic estimator for the case of a numeric target and a binary control variable. This enables us to use the branch-and-bound framework to efficiently discover the top-k subgroups that are both exceptional as well as representative. Experimental evaluation on a wide range of datasets shows that with this algorithm we discover meaningful representative patterns and are up to orders of magnitude faster in terms of node evaluations as well as time.
[scientific discovery, tight optimistic estimator, data mining, Aerospace electronics, Linear programming, local pattern mining technique, Data mining, Statistics, Standards, Fairness, Branch-and-bound, sub-populations, Sociology, locally exceptional yet globally representative subgroups, binary control, top-k subgroups, interpretable descriptions, Subgroup discovery, Informatics, subgroup discovery]
Visually-Aware Fashion Recommendation and Design with Generative Image Models
2017 IEEE International Conference on Data Mining
None
2017
Building effective recommender systems for domains like fashion is challenging due to the high level of subjectivity and the semantic complexity of the features involved (i.e., fashion styles). Recent work has shown that approaches to 'visual' recommendation (e.g. clothing, art, etc.) can be made more accurate by incorporating visual signals directly into the recommendation objective, using 'off-the-shelf' feature representations derived from deep networks. Here, we seek to extend this contribution by showing that recommendation performance can be significantly improved by learning 'fashion aware' image representations directly, i.e., by training the image representation (from the pixel level) and the recommender system jointly; this contribution is related to recent work using Siamese CNNs, though we are able to show improvements over state-of-the-art recommendation techniques such as BPR and variants that make use of pretrained visual features. Furthermore, we show that our model can be used generatively, i.e., given a user and a product category, we can generate new images (i.e., clothing items) that are most consistent with their personal taste. This represents a first step towards building systems that go beyond recommending existing items from a product corpus, but which can be used to suggest styles and aid the design of new products.
[Visualization, deep networks, generative image models, visually-aware fashion recommendation, visual recommendation, recommendation objective, Gallium nitride, image representation learning, Training, feature extraction, visual features, learning (artificial intelligence), pixel level, Recommender systems, off-the-shelf feature representations, Clothing, semantic complexity, Business process re-engineering, fashion styles, recommender systems, recommender system, computer vision, image representation, visual signals, clothing, fashion aware image representations, clothing items]
AutoLearn &#x2014; Automated Feature Generation and Selection
2017 IEEE International Conference on Data Mining
None
2017
In recent years, the importance of feature engineering has been confirmed by the exceptional performance of deep learning techniques, that automate this task for some applications. For others, feature engineering requires substantial manual effort in designing and selecting features and is often tedious and non-scalable. We present AutoLearn, a regression-based feature learning algorithm. Being data-driven, it requires no domain knowledge and is hence generic. Such a representation is learnt by mining pairwise feature associations, identifying the linear or non-linear relationship between each pair, applying regression and selecting those relationships that are stable and improve the prediction performance. Our experimental evaluation on 18 UC Irvine and 7 Gene expression datasets, across different domains, provides evidence that the features learnt through our model can improve the overall prediction accuracy by 13.28%, compared to original feature space and 5.87% over other top performing models, across 8 different classifiers without using any domain knowledge.
[Algorithm design and analysis, Correlation, Feature Generation, data mining, Transforms, regression analysis, Predictive models, pairwise feature associations, Automated Feature Generation, mining, Feature Selection, AutoLearn, regression-based feature learning algorithm, Classification, feature engineering, Prediction algorithms, deep learning techniques, Data models, learning (artificial intelligence), feature selection]
Collective Entity Resolution in Familial Networks
2017 IEEE International Conference on Data Mining
None
2017
Entity resolution in settings with rich relational structure often introduces complex dependencies between co-references. Exploiting these dependencies is challenging - it requires seamlessly combining statistical, relational, and logical dependencies. One task of particular interest is entity resolution in familial networks. In this setting, multiple partial representations of a family tree are provided, from the perspective of different family members, and the challenge is to reconstruct a family tree from these multiple, noisy, partial views. This reconstruction is crucial for applications such as understanding genetic inheritance, tracking disease contagion, and performing census surveys. Here, we design a model that incorporates statistical signals, such as name similarity, relational information, such as sibling overlap, and logical constraints, such as transitivity and bijective matching, in a collective model. We show how to integrate these features using probabilistic soft logic, a scalable probabilistic programming framework. In experiments on real-world data, our model significantly outperforms state-of-the-art classifiers that use relational features but are incapable of collective reasoning.
[complex dependencies, probabilistic logic, collective model, data mining, probabilistic soft logic, Programming, collective entity resolution, Electronic mail, History, multiple partial representations, statistical signals, Image color analysis, relational features, relational information, learning (artificial intelligence), family tree, collective reasoning, family members, probability, trees (mathematics), Probabilistic logic, logical dependencies, inference mechanisms, information networks, relational dependencies, familial networks, statistical dependencies, logical constraints, Diabetes, statistical analysis]
Scalable and Adaptive Algorithms for the Triangle Interdiction Problem on Billion-Scale Networks
2017 IEEE International Conference on Data Mining
None
2017
Motivated by the relevance of clustering or transitivity to a variety of network applications, we study the Triangle Interdiction Problem (TIP), which is to find a minimum-size set of edges that intersects all triangles of a network. As existing approximation algorithms for this NP-hard problem either do not scale well to massive networks or have poor solution quality, we formulate two algorithms, TARL and DART, with worst-case guarantees 5/2 and 3 with respect to optimal, respectively. Furthermore, DART is able to efficiently maintain its worst-case guarantee under dynamic edge insertion and removal to the network. In our comprehensive experimental evaluation, we demonstrate that DART is able to run on networks with billions of triangles within 2 hours and is able to dynamically update its solution in microseconds.
[TARL, Heuristic algorithms, Triangle Interdiction, graph theory, worst-case guarantee, network theory (graphs), approximation algorithms, massive networks, Clustering algorithms, IP networks, minimum-size set, Facebook, triangle interdiction problem, DART, Scalable approximation, approximation theory, dynamic edge insertion, network applications, NP-hard problem, transitivity, Approximation algorithms, clustering, Reliability, billion-scale networks, Cycle Transversal, computational complexity]
Online Learning of Acyclic Conditional Preference Networks from Noisy Data
2017 IEEE International Conference on Data Mining
None
2017
We deal with online learning of acyclic Conditional Preference networks (CP-nets) from data streams, possibly corrupted with noise. We introduce a new, efficient algorithm relying on (i) information-theoretic measures defined over the induced preference rules, which allow us to deal with corrupted data in a principled way, and on (ii) the Hoeffding bound to define an asymptotically optimal decision criterion for selecting the best conditioned variable to update the learned network. This is the first algorithm dealing with online learning of CP-nets in the presence of noise. We provide a thorough theoretical analysis of the algorithm, and demonstrate its effectiveness through an empirical evaluation on synthetic and on real datasets.
[Algorithm design and analysis, conditional preferences networks, decision theory, data streams, information-theoretic measures, Electronic mail, Data mining, preference learning, optimisation, Semantics, information theory, learning (artificial intelligence), noisy preferences, Recommender systems, graphical learning, learned network, corrupted data, CP-nets, Color, Hoeffding bound, Noise measurement, inference mechanisms, induced preference rules, noisy data, asymptotically optimal decision criterion, acyclic Conditional Preference networks, online learning]
GoGP: Fast Online Regression with Gaussian Processes
2017 IEEE International Conference on Data Mining
None
2017
One of the most current challenging problems in Gaussian process regression (GPR) is to handle large-scale datasets and to accommodate an online learning setting where data arrive irregularly on the fly. In this paper, we introduce a novel online Gaussian process model that could scale with massive datasets. Our approach is formulated based on alternative representation of the Gaussian process under geometric and optimization views, hence termed geometric-based online GP (GoGP). We developed theory to guarantee that with a good convergence rate our proposed algorithm always produces a (sparse) solution which is close to the true optima to any arbitrary level of approximation accuracy specified a priori. Furthermore, our method is proven to scale seamlessly not only with large-scale datasets, but also to adapt accurately with streaming data. We extensively evaluated our proposed model against state-of-the-art baselines using several large-scale datasets for online regression task. The experimental results show that our GoGP delivered comparable, or slightly better, predictive performance while achieving a magnitude of computational speedup compared with its rivals under online setting. More importantly, its convergence behavior is guaranteed through our theoretical analysis, which is rapid and stable while achieving lower errors.
[Ground penetrating radar, Gaussian Process regression, fast online regression, regression analysis, Predictive models, online setting, Optimization, Training, GoGP, optimisation, Gaussian process model, learning (artificial intelligence), online regression task, data streaming, optimization views, geometric views, Online learning, Standards, Support vector machines, geometric-based online GP, Gaussian processes, Gaussian process regression, convergence rate, computational complexity, online learning]
HiMuV: Hierarchical Framework for Modeling Multi-modality Multi-resolution Data
2017 IEEE International Conference on Data Mining
None
2017
Many real-world applications are characterized by temporal data collected from multiple modalities, each sampled with a different resolution. Examples include manufacturing processes and financial market prediction. In these applications, an interesting observation is that within the same modality, we often have data from multiple views, thus naturally forming a 2-level hierarchy: with the multiple modalities on the top, and the multiple views at the bottom. For example, in aluminum smelting processes, the multiple modalities include power, noise, alumina feed, etc; and within the same modality such as power, the different views correspond to various voltage, current and resistance control signals and measured responses. For such applications, we aim to address the following challenge, i.e., how can we integrate such multi-modality multi-resolution data to effectively predict the targets of interest, such as bath temperature in aluminum smelting cell and the volatility in financial market. In this paper, for the first time, we simultaneously model the hierarchical data structure and the multi-resolution property via a novel framework named HiMuV. Different from existing work based on multiple views on a single level or a single resolution, the proposed framework is based on the key assumption that the information from different modalities is complementary, whereas the information within the same modality (across different views) is redundant in terms of predicting the targets of interest. Therefore, we introduce an optimization framework where the objective function contains both the prediction loss and a novel regularizer enforcing the consistency among different views within the same modality. To solve this optimization framework, we propose an iterative algorithm based on randomized block coordinate descent. Experimental results on synthetic data, benchmark data, and various real data sets from aluminum smelting processes, and stock market prediction demonstrate the effectiveness and efficiency of the proposed algorithm.
[iterative methods, Voltage measurement, data analysis, multi-resolution, Aluminum, Smelting, Time series analysis, Process control, time series, temporal data, hierarchical framework, iterative algorithm, Temperature measurement, data models, multimodality multiresolution data modeling, optimisation, regression, aluminum smelting, optimization framework, multi-modality, Data models, HiMuV framework, hierarchical data structure, objective function]
Linear Time Complexity Time Series Classification with Bag-of-Pattern-Features
2017 IEEE International Conference on Data Mining
None
2017
Time series classification has attracted much attention due to the ubiquity of time series. With the advance of technologies, the volume of available time series data becomes huge and the content is changing rapidly. This requires time series data mining methods to have low computational complexities. In this paper, we propose a parameter-free time series classification method that has a linear time complexity. The approach is evaluated on all the 85 datasets in the well-known UCR time series classification archive. The results show that the new method achieves better overall classification accuracy performance than the widely used benchmark, i.e. 1-nearest neighbor with dynamic time warping, while consuming orders of magnitude less running time. The proposed method is also applied on a large real-world bird sounds dataset to verify its effectiveness.
[pattern classification, dynamic time warping, Time series analysis, mathematics computing, data mining, Transforms, magnitude less running time, time series classification archive, Birds, time series, low computational complexities, Time measurement, Windows, Data mining, classification accuracy performance, bag-of-pattern-features, linear time complexity time series classification, time series data mining methods, parameter-free time series classification method, Time complexity, computational complexity]
An Analysis of Boosted Linear Classifiers on Noisy Data with Applications to Multiple-Instance Learning
2017 IEEE International Conference on Data Mining
None
2017
An interesting observation about the well-known AdaBoost algorithm is that, though theory suggests it should overfit when applied to noisy data, experiments indicate it often does not do so in practice. In this paper, we study the behavior of AdaBoost on datasets with one-sided uniform class noise using linear classifiers as the base learner. We show analytically that, under some ideal conditions, this approach will not overfit, and can in fact recover a zero-error concept with respect to the true, uncorrupted instance labels. We also analytically show that AdaBoost increases the margins of predictions over boosting iterations, as has been previously suggested in the literature. We then compare the empirical behavior of AdaBoost using real world datasets with one-sided noise derived from multiple-instance data. Although our assumptions may not hold in a practical setting, our experiments show that standard AdaBoost still performs well, as suggested by our analysis, and often outperforms baseline variations in the literature that explicitly try to account for noise.
[uncorrupted instance labels, pattern classification, zero-error concept, Machine learning algorithms, Noise, Multiple-Instance Learning, Boosting, Noise measurement, one-sided uniform class noise, Standards, Training, Computer science, boosted linear classifiers, AdaBoost, noisy data, multiple-instance data, one-sided noise, AdaBoost algorithm, boosting iterations, multiple-instance learning, learning (artificial intelligence)]
BiCycle: Item Recommendation with Life Cycles
2017 IEEE International Conference on Data Mining
None
2017
Recommender systems have attracted much attention in last decades, which can help the users explore new items in many applications. As a popular technique in recommender systems, item recommendation works by recommending items to users based on their historical interactions. Conventional item recommendation methods usually assume that users and items are stationary, which is not always the case in real-world applications. Many time-aware item recommendation models have been proposed to take the temporal effects into the considerations based on the absolute time stamps associated with observed interactions. We show that using absolute time to model temporal effects can be limited in some circumstances. In this work, we propose to model the temporal dynamics of both users and items in item recommendation based on their life cycles. This problem is very challenging to solve since the users and items can co-evolve in their life cycles and the sparseness of the data become more severe when we consider the life cycles of both users and items. A novel time-aware item recommendation model called BiCycle is proposed to address these challenges. BiCycle is designed based on two important observations: 1) correlated users or items usually share similar patterns in the similar stages of their life cycles. 2) user preferences and item characters can evolve gradually over different stages of their life cycles. Extensive experiments conducted on three real-world datasets demonstrate the proposed approach can significantly improve the performance of recommendation tasks by considering the inner life cycles of both users and items.
[Collaborative Filtering, Filtering, data analysis, Data Mining, History, Indexes, item recommendation works, BiCycle, recommender systems, Item Recommendation, Life Cycle, item characters, life cycles, Collaboration, temporal effects, recommendation tasks, time-aware item recommendation models, Bicycles, Motion pictures, Evolution Inference]
Telling Cause from Effect Using MDL-Based Local and Global Regression
2017 IEEE International Conference on Data Mining
None
2017
We consider the fundamental problem of inferring the causal direction between two univariate numeric random variables X and Y from observational data. The two-variable case is especially difficult to solve since it is not possible to use standard conditional independence tests between the variables. To tackle this problem, we follow an information theoretic approach based on Kolmogorov complexity and use the Minimum Description Length (MDL) principle to provide a practical solution. In particular, we propose a compression scheme to encode local and global functional relations using MDL-based regression. We infer X causes Y in case it is shorter to describe Y as a function of X than the inverse direction. In addition, we introduce Slope, an efficient linear-time algorithm that through thorough empirical evaluation on both synthetic and real world data we show outperforms the state of the art by a wide margin.
[local regression, regression analysis, real world data, Complexity theory, two-variable case, global functional relations, global regression, Minimum Description Length principle, Turing machines, MDL, information theoretic approach, Kolmogorov complexity, Benchmark testing, standard conditional independence tests, Y, information theory, belief networks, efficient linear-time algorithm, fundamental problem, Computational modeling, inverse direction, Kolmogorov Complexity, Regression, observational data, Hypercompression, Causal Inference, Markov processes, Data models, synthetic world data, Random variables, data handling, computational complexity]
Distributing Frank-Wolfe via Map-Reduce
2017 IEEE International Conference on Data Mining
None
2017
Large-scale optimization problems abound in data mining and machine learning applications, and the computational challenges they pose are often addressed through parallelization. We identify structural properties under which a convex optimization problem can be massively parallelized via map-reduce operations using the Frank-Wolfe (FW) algorithm. The class of problems that can be tackled this way is quite broad and includes experimental design, AdaBoost, and projection to a convex hull. Implementing FW via map-reduce eases parallelization and deployment via commercial distributed computing frameworks. We demonstrate this by implementing FW over Spark, an engine for parallel data processing, and establish that parallelization through map-reduce yields significant performance improvements: we solve problems with 10 million variables using 350 cores in 44 minutes; the same operation takes 133 hours when executed serially.
[Algorithm design and analysis, iterative methods, large-scale optimization problems, data mining, parallel processing, Frank-Wolfe algorithm, Optimization, Convergence, Distributed Algorithms, Program processors, optimisation, Frank-Wolfe distribution, Convex functions, learning (artificial intelligence), convex optimization problem, commercial distributed computing, Data structures, convex programming, Sparks, parallelization, machine learning, structural properties, parallel data processing, Frank-Wolfe, Convex Optimization, data handling, Spark]
Glocalized Weisfeiler-Lehman Graph Kernels: Global-Local Feature Maps of Graphs
2017 IEEE International Conference on Data Mining
None
2017
Most state-of-the-art graph kernels only take local graph properties into account, i.e., the kernel is computed with regard to properties of the neighborhood of vertices or other small substructures. On the other hand, kernels that do take global graph properties into account may not scale well to large graph databases. Here we propose to start exploring the space between local and global graph kernels, so called glocalized graph kernels, striking the balance between both worlds. Specifically, we introduce a novel graph kernel based on the k-dimensional Weisfeiler-Lehman algorithm. Unfortunately, the k-dimensional Weisfeiler-Lehman algorithm scales exponentially in k. Consequently, we devise a stochastic version of the kernel with provable approximation guarantees using conditional Rademacher averages. On bounded-degree graphs, it can even be computed in constant time. We support our theoretical results with experiments on several graph classification benchmarks, showing that our kernels often outperform the state-of-the-art in terms of classification accuracies.
[Algorithm design and analysis, local graph properties, global-local feature maps, Graph Classification, Heuristic algorithms, graph theory, k-dimensional Weisfeiler-Lehman algorithm scales, Graph Kernel, global graph properties, glocalized Weisfeiler-Lehman graph kernels, local graph kernels, Databases, Image color analysis, global graph kernels, Approximation algorithms, bounded-degree graphs, graph classification benchmarks, conditional Rademacher averages, Labeling, Sampling, Kernel, glocalized graph kernels, computational complexity, graph databases]
Importance Sketching of Influence Dynamics in Billion-Scale Networks
2017 IEEE International Conference on Data Mining
None
2017
The blooming availability of traces for social, biological, and communication networks opens up unprecedented opportunities in analyzing diffusion processes in networks. However, the sheer sizes of the nowadays networks raise serious challenges in computational efficiency and scalability. In this paper, we propose a new hyper-graph sketching framework for influence dynamics in networks. The core of our sketching framework, called SKIS, is an efficient importance sampling algorithm that returns only non-singular reverse cascades in the network. Comparing to previously developed sketches like RIS and SKIM, our sketch significantly enhances estimation quality while substantially reducing processing time and memory-footprint. Further, we present general strategies of using SKIS to enhance existing algorithms for influence estimation and influence maximization which are motivated by practical applications like viral marketing. Using SKIS, wedesign high-quality influence oracles for seed sets with average estimation error up to 10x times smaller than those using RIS and 6x times smaller than SKIMs. In addition, our influence maximization using SKIS substantially improves the quality of solutions for greedy algorithms. It achieves up to 10x times speed-up and 4x memory reduction for the fastest RIS-based DSSA algorithm, while maintaining the same theoretical guarantees.
[Algorithm design and analysis, Greedy algorithms, DSSA algorithm, high-quality influence oracles, nonsingular reverse cascades, graph theory, computational efficiency, importance sketching, Electronic mail, scalability, biological networks, optimisation, diffusion processes, Importance Sampling, Billion-scale Networks, SKIM, Influence Dynamics, sheer sizes, estimation quality, importance sampling algorithm, Estimation error, memory-footprint reduction, greedy algorithms, SKIS, communication networks, processing time reduction, social networks, seed sets, Probabilistic logic, influence estimation, importance sampling, social networking (online), influence maximization, influence dynamics, hyper-graph sketching framework, billion-scale networks, Integrated circuit modeling, RIS]
Bayesian Optimization in Weakly Specified Search Space
2017 IEEE International Conference on Data Mining
None
2017
Bayesian optimization (BO) has recently emerged as a powerful and flexible tool for hyper-parameter tuning and more generally for the efficient global optimization of expensive black-box functions. Systems implementing BO has successfully solved difficult problems in automatic design choices and machine learning hyper-parameters tunings. Many recent advances in the methodologies and theories underlying Bayesian optimization have extended the framework to new applications and provided greater insights into the behavior of these algorithms. Still, these established techniques always require a user-defined space to perform optimization. This pre-defined space specifies the ranges of hyper-parameter values. In many situations, however, it can be difficult to prescribe such spaces, as a prior knowledge is often unavailable. Setting these regions arbitrarily can lead to inefficient optimization - if a space is too large, we can miss the optimum with a limited budget, on the other hand, if a space is too small, it may not contain the optimum point that we want to get. The unknown search space problem is intractable to solve in practice. Therefore, in this paper, we narrow down to consider specifically the setting of "weakly specified" search space for Bayesian optimization. By weakly specified space, we mean that the pre-defined space is placed at a sufficiently good region so that the optimization can expand and reach to the optimum. However, this pre-defined space need not include the global optimum. We tackle this problem by proposing the filtering expansion strategy for Bayesian optimization. Our approach starts from the initial region and gradually expands the search space. Wedevelop an efficient algorithm for this strategy and derive its regret bound. These theoretical results are complemented by an extensive set of experiments on benchmark functions and tworeal-world applications which demonstrate the benefits of our proposed approach.
[Algorithm design and analysis, Search problems, BO, Optimization, efficient global optimization, weakly specified search space, optimisation, regret bound, hyper-parameter tuning, hyper-parameter values, black-box functions, unknown search space problem, Benchmark testing, experimental design, user-defined spaces, learning (artificial intelligence), Bayesian optimization, search problems, weakly specified space, filtering expansion strategy, Tuning, evolutionary computation, machine learning hyper-parameters tunings, Gaussian processes, inefficient optimization, Bayes methods, automatic design choices]
Relational Mixture of Experts: Explainable Demographics Prediction with Behavioral Data
2017 IEEE International Conference on Data Mining
None
2017
Given a collection of basic customer demographics (e.g., age and gender) andtheir behavioral data (e.g., item purchase histories), how can we predictsensitive demographics (e.g., income and occupation) that not every customermakes available?This demographics prediction problem is modeled as a classification task inwhich a customer's sensitive demographic y is predicted from his featurevector x. So far, two lines of work have tried to produce a"good" feature vector x from the customer's behavioraldata: (1) application-specific feature engineering using behavioral data and (2) representation learning (such as singular value decomposition or neuralembedding) on behavioral data. Although these approaches successfullyimprove the predictive performance, (1) designing a good feature requiresdomain experts to make a great effort and (2) features obtained fromrepresentation learning are hard to interpret. To overcome these problems, we present a Relational Infinite SupportVector Machine (R-iSVM), a mixture-of-experts model that can leveragebehavioral data. Instead of augmenting the feature vectors of customers, R-iSVM uses behavioral data to find out behaviorally similar customerclusters and constructs a local prediction model at each customer cluster. In doing so, R-iSVM successfully improves the predictive performance withoutrequiring application-specific feature designing and hard-to-interpretrepresentations. Experimental results on three real-world datasets demonstrate the predictiveperformance and interpretability of R-iSVM. Furthermore, R-iSVM can co-existwith previous demographics prediction methods to further improve theirpredictive performance.
[demographic prediction problem, demographics prediction, R-iSVM, Predictive models, History, Training, kernel machine, behavioral data, entity-relationship data, demography, behaviorally similar customer-clusters, Mathematical model, learning (artificial intelligence), mixture-of-experts model, singular value decomposition, pattern classification, Bayesian nonparametrics, support vector machines, basic-customer demographics, Computational modeling, predictive performance, application-specific feature engineering, demographic prediction methods, pattern clustering, Data models, explainable demographic prediction]
Unsupervised Feature Learning with Discriminative Encoder
2017 IEEE International Conference on Data Mining
None
2017
In recent years, deep discriminative models have achieved extraordinary performance on supervised learning tasks, significantly outperforming their generative counterparts. However, their success relies on the presence of a large amount of labeled data. How can one use the same discriminative models for learning useful features in the absence of labels? We address this question in this paper, by jointly modeling the distribution of data and latent features in a manner that explicitly assigns zero probability to unobserved data. Rather than maximizing the marginal probability of observed data, we maximize the joint probability of the data and the latent features using a two step EM-like procedure. To prevent the model from overfitting to our initial selection of latent features, we use adversarial regularization. Depending on the task, we allow the latent features to be one-hot or real-valued vectors, and define a suitable prior on the features. For instance, one-hot features correspond to class labels, and are directly used for unsupervised and semi-supervised classification task, whereas real-valued feature vectors are fed as input to simple classifiers for auxiliary supervised discrimination tasks. The proposed model, which we dub dicriminative encoder (or DisCoder), is flexible in the type of latent features that it can capture. The proposed model achieves state-of-the-art performance on several challenging tasks. Qualitative visualization of the latent features shows that the features learnt by the DisCoder are indeed meaningful.
[pattern classification, latent features, auxiliary supervised discrimination tasks, DisCoder, deep discriminative models, Encoding, Decoding, supervised learning tasks, unsupervised feature learning, Optimization, unsupervised learning, one-hot features, real-valued feature vectors, feature extraction, Hidden Markov models, Training data, Data models, semi-supervised learning, learning (artificial intelligence), discriminative model]
Learning Doubly Stochastic Affinity Matrix via Davis-Kahan Theorem
2017 IEEE International Conference on Data Mining
None
2017
Building an ideal graph which reveals the exact intrinsic structure of the data is critical in graph-based clustering. There have been a lot of efforts to construct an affinity matrix satisfying such a need in terms of a similarity measure. A recent approach attracting attention is on using doubly stochastic normalization of the affinity matrix to improve the clustering performance. In this paper, we propose a novel method to build a high-quality affinity matrix via incorporating Davis-Kahan theorem of matrix perturbation theory in the doubly stochastic normalization problem. We interpret the goal of the doubly stochastic normalization problem as minimizing the relative distance between the eigenspaces of the corresponding matrices. Also, for the doubly stochastic normalization problem we include an additional constraint that each eigenvalue be on the unit interval to fully conform to the spectral graph theory. Experiments on our framework present superior performance over various datasets.
[Laplace equations, Symmetric matrices, graph theory, matrix perturbation theory, eigenvalues and eigenfunctions, matrix algebra, spectral graph theory, stochastic affinity matrix, ideal graph, Davis-Kahan theorem, exact intrinsic data structure, high-quality affinity matrix, Perturbation methods, pattern clustering, perturbation theory, Clustering algorithms, Cost function, Eigenvalues and eigenfunctions, data structures, learning (artificial intelligence), stochastic processes, Kernel, doubly stochastic normalization problem]
Adaptive Laplace Mechanism: Differential Privacy Preservation in Deep Learning
2017 IEEE International Conference on Data Mining
None
2017
In this paper, we focus on developing a novel mechanism to preserve differential privacy in deep neural networks, such that: (1) The privacy budget consumption is totally independent of the number of training steps; (2) It has the ability to adaptively inject noise into features based on the contribution of each to the output; and (3) It could be applied in a variety of different deep neural networks. To achieve this, we figure out a way to perturb affine transformations of neurons, and loss functions used in deep neural networks. In addition, our mechanism intentionally adds "more noise" into features which are "less relevant" to the model output, and vice-versa. Our theoretical analysis further derives the sensitivities and error bounds of our mechanism. Rigorous experiments conducted on MNIST and CIFAR-10 datasets show that our mechanism is highly effective and outperforms existing solutions.
[Data privacy, Computational modeling, Neurons, differential privacy preservation, loss functions, Laplace transforms, adaptive Laplace mechanism, Biological neural networks, affine transformations, Deep Learning, affine transforms, privacy budget consumption, Training, Laplace Mechanism, Privacy, deep learning, Machine learning, Differential Privacy, data privacy, learning (artificial intelligence), neural nets, deep neural networks, training steps]
A Short-Term Rainfall Prediction Model Using Multi-task Convolutional Neural Networks
2017 IEEE International Conference on Data Mining
None
2017
Precipitation prediction, such as short-term rainfall prediction, is a very important problem in the field of meteorological service. In practice, most of recent studies focus on leveraging radar data or satellite images to make predictions. However, there is another scenario where a set of weather features are collected by various sensors at multiple observation sites. The observations of a site are sometimes incomplete but provide important clues for weather prediction at nearby sites, which are not fully exploited in existing work yet. To solve this problem, we propose a multi-task convolutional neural network model to automatically extract features from the time series measured at observation sites and leverage the correlation between the multiple sites for weather prediction via multi-tasking. To the best of our knowledge, this is the first attempt to use multi-task learning and deep learning techniques to predict short-term rainfall amount based on multi-site features. Specifically, we formulate the learning task as an end-to-end multi-site neural network model which allows to leverage the learned knowledge from one site to other correlated sites, and model the correlations between different sites. Extensive experiments show that the learned site correlations are insightful and the proposed model significantly outperforms a broad set of baseline models including the European Centre for Medium-range Weather Forecasts system (ECMWF).
[Rainfall Prediction, weather prediction, Correlation, European centre-for-medium-range weather forecast system, end-to-end multisite neural network model, Weather forecasting, Predictive models, precipitation prediction, weather forecasting, feature extraction, ECMWF, learning (artificial intelligence), satellite images, multitask convolutional neural networks, rain, meteorological service, multitask convolutional neural network model, geophysics computing, time series, multitask learning, short-term rainfall prediction model, short-term rainfall amount, Multi-site Features, Neural networks, weather features, atmospheric techniques, Feature extraction, deep learning techniques, Multi-Task Convolutional Neural Networks, neural nets, leveraging radar data]
Scalable Hashing-Based Network Discovery
2017 IEEE International Conference on Data Mining
None
2017
Discovering and analyzing networks from non-network data is a task with applications in fields as diverse as neuroscience, genomics, energy, economics, and more. In these domains, networks are often constructed out of multiple time series by computing measures of association or similarity between pairs of series. The nodes in a discovered graph correspond to time series, which are linked via edges weighted by the association scores of their endpoints. After graph construction, the network may be thresholded such that only the edges with stronger weights remain and the desired sparsity level is achieved. While this approach is feasible for small datasets, its quadratic time complexity does not scale as the individual time series length and the number of compared series increase. Thus, to avoid the costly step of building a fully-connected graph before sparsification, we propose a fast network discovery approach based on probabilistic hashing of randomly selected time series subsequences. Evaluation on real data shows that our methods construct graphs nearly 15 times as fast as baseline methods, while achieving both network structure and accuracy comparable to baselines in task-based evaluation.
[Correlation, multiple time series, data analysis, Time series analysis, graph theory, data mining, network theory (graphs), time series, Time measurement, probabilistic hashing, association scores, sparsity level, Neuroscience, randomly selected time series subsequences, fast network discovery approach, Market research, quadratic time complexity, network structure, graph construction, scalable hashing-based network discovery, computational complexity, nonnetwork data]
Benchmark Generator for Dynamic Overlapping Communities in Networks
2017 IEEE International Conference on Data Mining
None
2017
We describe a dynamic graph generator with overlapping communities that is capable of simulating community scale events while at the same time maintaining crucial graph properties. Such a benchmark generator is useful to measure and compare the responsiveness and efficiency of dynamic community detection algorithms. Since the generator allows the user to tune multiple parameters, it can also be used to test the robustness of a community detection algorithm across a spectrum of inputs. In an experimental evaluation, we demonstrate the generator's performance and show that graph properties are indeed maintained over time. Further, we show that standard community detection algorithms are able to find the generated community structure. To the best of our knowledge, this is the first time that all of the above have been combined into one benchmark generator, and this work constitutes an important building block for the development of efficient and reliable dynamic, overlapping community detection algorithms.
[Heuristic algorithms, Image edge detection, graph theory, network theory (graphs), dynamic graph generator, Social networks, Generators, Electronic mail, complex networks, networks, Clustering, dynamic community detection algorithms, benchmark generator, community structure, community scale events, graph properties, Benchmark testing, dynamic overlapping communities, Reliability, Detection algorithms]
Accurate Detection of Automatically Spun Content via Stylometric Analysis
2017 IEEE International Conference on Data Mining
None
2017
Spammers use automated content spinning techniques to evade plagiarism detection by search engines. Text spinners help spammers in evading plagiarism detectors by automatically restructuring sentences and replacing words or phrases with their synonyms. Prior work on spun content detection relies on the knowledge about the dictionary used by the text spinning software. In this work, we propose an approach to detect spun content and its seed without needing the text spinner's dictionary. Our key idea is that text spinners introduce stylometric artifacts that can be leveraged for detecting spun documents. We implement and evaluate our proposed approach on a corpus of spun documents that are generated using a popular text spinning software. The results show that our approach can not only accurately detect whether a document is spun but also identify its source (or seed) document - all without needing the dictionary used by the text spinner.
[stylometry, text analysis, Dictionaries, search engines, text spinning, content spinning techniques, spun content detection, unsolicited e-mail, stylometric artifacts, Frequency measurement, text spinner dictionary, text spinning software, stylometric analysis, Plagiarism, Search engines, spammers, Feature extraction, Software, plagiarism detection, spun documents, Spinning, spam, plagiarism detector evasion]
STExNMF: Spatio-Temporally Exclusive Topic Discovery for Anomalous Event Detection
2017 IEEE International Conference on Data Mining
None
2017
Understanding newly emerging events or topics associated with a particular region of a given day can provide deep insight on the critical events occurring in highly evolving metropolitan cities. We propose herein a novel topic modeling approach on text documents with spatio-temporal information (e.g., when and where a document was published) such as location-based social media data to discover prevalent topics or newly emerging events with respect to an area and a time point. We consider a map view composed of regular grids or tiles with each showing topic keywords from documents of the corresponding region. To this end, we present a tilebased spatio-temporally exclusive topic modeling approach called STExNMF, based on a novel nonnegative matrix factorization (NMF) technique. STExNMF mainly works based on the two following stages: (1) first running a standard NMF of each tile to obtain general topics of the tile and (2) running a spatiotemporally exclusive NMF on a weighted residual matrix. These topics likely reveal information on newly emerging events or topics of interest within a region. We demonstrate the advantages of our approach using the geo-tagged Twitter data of New York City. We also provide quantitative comparisons in terms of the topic quality, spatio-temporal exclusiveness, topic variation, and qualitative evaluations of our method using several usage scenarios. In addition, we present a fast topic modeling technique of our model by leveraging parallel computing.
[text analysis, topic keywords, data mining, text documents, Twitter, anomaly detection, matrix decomposition, Data mining, topic modeling approach, parallel processing, topic quality, metropolitan cities, Analytical models, tiles, social media data, spatiotemporal exclusiveness, parallel computing, regular grids, Topic modeling, topic variation, matrix factorization, data analysis, Computational modeling, Urban areas, weighted residual matrix, spatiotemporal information, anomalous event detection, critical events, nonnegative matrix factorization technique, STExNMF, spatiotemporally exclusive NMF, spatiotemporally exclusive topic discovery, event detection, social network analysis, social networking (online), Data models]
A Probabilistic Approach for Learning with Label Proportions Applied to the US Presidential Election
2017 IEEE International Conference on Data Mining
None
2017
Ecological inference (EI) is a classical problem from political science to model voting behavior of individuals given only aggregate election results. Flaxman et al. recently formulated EI as machine learning problem using distribution regression, and applied it to analyze US presidential elections. However, distribution regression unnecessarily aggregates individual-level covariates available from census microdata, and ignores known structure of the aggregation mechanism. We instead formulate the problem as learning with label proportions (LLP), and develop a new, probabilistic, LLP method to solve it. Our model is the straightforward one where individual votes are latent variables. We use cardinality potentials to efficiently perform exact inference over latent variables during learning, and introduce a novel message-passing algorithm to extend cardinality potentials to multivariate probability models for use within multiclass LLP problems. We show experimentally that LLP outperforms distribution regression for predicting individual-level attributes, and that our method is as good as or better than existing state-of-the-art LLP methods.
[politics, regression analysis, Predictive models, census microdata, probabilistic approach, machine learning problem, Voting, latent variables, probability models, US presidential election, Mathematical model, learning (artificial intelligence), distribution regression, individual votes, message passing, Biological system modeling, individual-level attributes, EI, probability, label proportions, Probabilistic logic, cardinality potentials, inference mechanisms, Standards, political science, aggregation mechanism, Ecological Inference, multiclass LLP problems, ecological inference, Learning with Label Proportions, Bayes methods, government data processing]
Edge-Based Wedge Sampling to Estimate Triangle Counts in Very Large Graphs
2017 IEEE International Conference on Data Mining
None
2017
The number of triangles in a graph is useful to deduce a plethora of important features of the network that the graph is modeling. However, finding the exact value of this number is computationally expensive. Hence, a number of approximation algorithms based on random sampling of edges, or wedges (adjacent edge pairs) have been proposed for estimating this value. We argue that for large sparse graphs with power-law degree distribution, random edge sampling requires sampling large number of edges before providing enough information for accurate estimation, and existing wedge sampling methods lead to biased samplings, which in turn lead to less accurate estimations. In this paper, we propose a hybrid algorithm between edge and wedge sampling that addresses the deficiencies of both approaches. We start with uniform edge sampling and then extend each selected edge to form a wedge that is more informative for estimating the overall triangle count. The core estimate we make is the number of triangles each sampled edge in the first phase participates in. This approach provides accurate approximations with very small sampling ratios, outperforming the state-of-the-art up to 8 times in sample size while providing estimations with 95% confidence.
[Algorithm design and analysis, Measurement, hybrid algorithm, Conferences, graph theory, large graphs, computational geometry, wedge sampling methods, biased samplings, approximation algorithms, power-law degree distribution, Random edge sampling, sample size, Random wedge sampling, approximation theory, sampling methods, Triangle count estimation, Estimation, sparse graphs, uniform edge sampling, random processes, triangle count estimations, adjacent edge pairs, accurate estimation, sampling ratios, Approximation algorithms, Sampling methods, Feature extraction, random edge sampling, triangle count]
GANG: Detecting Fraudulent Users in Online Social Networks via Guilt-by-Association on Directed Graphs
2017 IEEE International Conference on Data Mining
None
2017
Detecting fraudulent users in online social networks is a fundamental and urgent research problem as adversaries can use them to perform various malicious activities. Global social structure based methods, which are known as guilt-by-association, have been shown to be promising at detecting fraudulent users. However, existing guilt-by-association methods either assume symmetric (i.e., undirected) social links, which oversimplifies the asymmetric (i.e., directed) social structure of real-world online social networks, or only leverage labeled fraudulent users or labeled normal users (but not both) in the training dataset, which limits detection accuracies. In this work, we propose GANG, a guilt-by-association method on directed graphs, to detect fraudulent users in OSNs. GANG is based on a novel pairwise Markov Random Field that we design to capture the unique characteristics of the fraudulent-user-detection problem in directed OSNs. In the basic version of GANG, given a training dataset, we leverage Loopy Belief Propagation (LBP) to estimate the posterior probability distribution for each user and uses it to predict a user's label. However, the basic version is not scalable enough and not guaranteed to converge because it relies on LBP. Therefore, we further optimize GANG and our optimized version can be represented as a concise matrix form, with which we are able to derive conditions for convergence. We compare GANG with various existing guilt-by-association methods on a large-scale Twitter dataset and a large-scale Sina Weibo dataset with labeled fraudulent and normal users. Our results demonstrate that GANG substantially outperforms existing methods, and that the optimized version of GANG is significantly more efficient than the basic version.
[fraudulent user detection, guilt-by-association method, fraudulent-user-detection problem, Image edge detection, graph theory, data mining, probability, online social networks, Twitter, pairwise Markov random field, Markov random fields, Training, loopy belief propagation, GANG, directed graphs, fraud, global social structure based methods, Markov processes, probability distribution, social networking (online), Robustness, Random variables, LBP]
Topological Recurrent Neural Network for Diffusion Prediction
2017 IEEE International Conference on Data Mining
None
2017
In this paper, we study the problem of using representation learning to assist information diffusion prediction on graphs. In particular, we aim at estimating the probability of an inactive node to be activated next in a cascade. Despite the success of recent deep learning methods for diffusion, we find that they often underexplore the cascade structure. We consider a cascade as not merely a sequence of nodes ordered by their activation time stamps; instead, it has a richer structure indicating the diffusion process over the data graph. As a result, we introduce a new data model, namely diffusion topologies, to fully describe the cascade structure. We find it challenging to model diffusion topologies, which are dynamic directed acyclic graphs (DAGs), with the existing neural networks. Therefore, we propose a novel topological recurrent neural network, namely Topo-LSTM, for modeling dynamic DAGs. We customize Topo-LSTM for the diffusion prediction task, and show it improves the state-of-the-art baselines, by 20.1%-56.6% (MAP) relatively, across multiple real-world data sets.
[probability estimation, data model, estimation theory, representation learning, data graph, neural networks, Predictive models, Topo-LSTM, activation time stamps, dynamic directed acyclic graphs, dynamic DAGs, Network topology, deep learning methods, learning (artificial intelligence), cascade structure, information diffusion prediction, recurrent neural nets, Receivers, Topology, diffusion prediction task, data models, diffusion topologies, directed graphs, Machine learning, social networking (online), inactive node, Data models, topological recurrent neural network, diffusion process, Integrated circuit modeling]
Multi-task Survival Analysis
2017 IEEE International Conference on Data Mining
None
2017
Collecting labeling information of time-to-event analysis is naturally very time consuming, i.e., one has to wait for the occurrence of the event of interest, which may not always be observed for every instance. By taking advantage of censored instances, survival analysis methods internally consider more samples than standard regression methods, which partially alleviates this data insufficiency problem. Whereas most existing survival analysis models merely focus on a single survival prediction task, when there are multiple related survival prediction tasks, we may benefit from the tasks relatedness. Simultaneously learning multiple related tasks, multi-task learning (MTL) provides a paradigm to alleviate data insufficiency by bridging data from all tasks and improves generalization performance of all tasks involved. Even though MTL has been extensively studied, there is no existing work investigating MTL for survival analysis. In this paper, we propose a novel multi-task survival analysis framework that takes advantage of both censored instances and task relatedness. Specifically, based on two common used task relatedness assumptions, i.e., low-rank assumption and cluster structure assumption, we formulate two concrete models, COX-TRACE and COX-cCMTL, under the proposed framework, respectively. We develop efficient algorithms and demonstrate the performance of the proposed multi-task survival analysis models on the The Cancer Genome Atlas (TCGA) dataset. Our results show that the proposed approaches can significantly improve the prediction performance in survival analysis and can also discover some inherent relationships among different cancer types.
[censored instances, COX-cCMTL, tasks relatedness, regression analysis, regularization, Predictive models, multitask survival analysis models, prediction performance, Multi-task learning, Analytical models, single survival prediction task, learning (artificial intelligence), Bioinformatics, time-to-event analysis, COX-TRACE, Hazards, multitask learning, Cancer Genome Atlas, standard regression methods, Survival analysis, Cox model, pattern clustering, cluster structure assumption, low-rank assumption, task relatedness assumptions, Concrete, Cancer]
Tracking Hit-and-Run Vehicle with Sparse Video Surveillance Cameras and Mobile Taxicabs
2017 IEEE International Conference on Data Mining
None
2017
Due to the sparse distribution of road video surveillance cameras, precise trajectory tracking for hit-and-run vehicles remains a challenging task. Previous research on vehicle trajectory recovery mostly focuses on recovering trajectory with low-sampling-rate GPS coordinates by retrieving road traffic flow patterns from collected GPS information. However, to the best of our knowledge, none of them considered using on-road taxicabs as mobile video surveillance cameras as well as the time-varying characteristics of vehicle traveling and road traffic flow patterns, therefore not suitable for recovering trajectories of hit-and-run vehicles. With this insight, we model the travel time-cost of a road segment during various time periods precisely with LNDs (Logarithmic Normal Distributions), then use LSNDs (Log Skew Normal Distributions) to approximate the time-cost of an urban trip during various time periods. We propose a novel approach to calculate possible location and time distribution of the hit-and-run vehicle in parallel, select the optimal taxicab to verify the distribution by uploading and checking video clips of this taxicab, finally refine the restoring trajectory in a parallel and recursive manner. We evaluate our solution on real-world taxicab and road surveillance system datasets. Experimental results demonstrate that our approach outperforms alternative solutions in terms of accuracy ratio of vehicle tracking.
[restoring trajectory, Roads, hit-and-run vehicle tracking, vehicle traveling, Mobile communication, normal distribution, precise trajectory tracking, possible location, road vehicles, low-sampling-rate GPS, mobile taxicabs, time distribution, on-road taxicabs, sparse video surveillance cameras, Trajectory, video surveillance, road video surveillance cameras, parallel taxicab, road traffic flow patterns, logarithmic normal distributions, travel time-cost, log skew normal distributions, traffic engineering computing, road segment, sparse distribution, Global Positioning System, time-varying characteristics, road surveillance system datasets, mobile video surveillance cameras, vehicle trajectory recovery, Streaming media, Cameras, Video surveillance, GPS information, time periods, vehicle tracking]
Discovering Truths from Distributed Data
2017 IEEE International Conference on Data Mining
None
2017
In the big data era, the information about the same object collected from multiple sources is inevitably conflicting. The task of identifying true information (i.e., the truths) among conflicting data is referred to as truth discovery, which incorporates the estimation of source reliability degrees into the aggregation of multi-source data. However, in many real-world applications, large-scale data are distributed across multiple servers. Traditional truth discovery approaches cannot handle this scenario due to the constraints of communication overhead and privacy concern. Another limitation of most existing work is that they ignore the differences among objects, i.e., they treat all the objects equally. This limitation would be exacerbated in distributed environments where significant differences exist among the objects. To tackle the aforementioned issues, in this paper, we propose a novel distributed truth discovery framework (DTD), which can effectively and efficiently aggregate conflicting data stored across distributed servers, with the differences among the objects as well as the importance level of each server being considered. The proposed framework consists of two steps: the local truth computation step conducted by each local server and the central truth estimation step taking place in the central server. Specifically, we introduce the uncertainty values to model the differences among objects, and propose a new uncertainty-based truth discovery method (UbTD) for calculating the true information of objects in each local server. The outputs of the local truth computation step include the estimated local truths and the variances of objects, which are the input information of the central truth estimation step. To infer the final true information in the central server, we propose a new algorithm to aggregate the outputs of all the local servers with the quality of different local servers taken into account. The proposed distributed truth discovery framework can infer object truths without delivering any raw data to the central server, and thus can reduce communication overhead as well as preserve data privacy. Experimental results on three real world datasets show that the proposed DTD framework can efficiently estimate object truths with accuracy guarantee, and the proposed UbTD algorithm significantly outperforms the state-of-the-art batch truth discovery approaches.
[Data privacy, Uncertainty, preserve data privacy, distributed servers, data mining, distributed processing, distributed system, big data era, Servers, data aggregation, database management systems, communication overhead, distributed truth discovery, storage management, multisource data, local truth computation step, truth discovery, Distributed databases, privacy concern, central server, Truth discovery, local server, multiple servers, central truth estimation step, Estimation, Big Data, distributed environments, uncertainty estimation, multiple sources, UbTD, source reliability degrees, uncertainty-based truth discovery method, large-scale data, data privacy, Reliability, distributed data, batch truth discovery]
Local Bayes Risk Minimization Based Stopping Strategy for Hierarchical Classification
2017 IEEE International Conference on Data Mining
None
2017
In large-scale data classification tasks, it is becoming more and more challenging in finding a true class from a huge amount of candidate categories. Fortunately, a hierarchical structure usually exists in these massive categories. The task of utilizing this structure for effective classification is called hierarchical classification. It usually follows a top-down fashion which predicts a sample from the root node with a coarse-grained category to a leaf node with a fine-grained category. However, misclassification is inevitable if the information is insufficient or large uncertainty exists in the prediction process. In this scenario, we can design a stopping strategy to stop the sample at an internal node with a coarser category, instead of predicting a wrong leaf node. Several studies address the problem by improving performance in terms of hierarchical accuracy and informative prediction. However, all of these researches ignore an important issue: when predicting a sample at the current node, the error is inclined to occur if large uncertainty exists in the next lower level children nodes. In this paper, we integrate this uncertainty into a risk problem: when predicting a sample at a decision node, it will take precipitance risk in predicting the sample to a children node in the next lower level on one hand, and take conservative risk in stopping at the current node on the other. We address the risk problem by designing a Local Bayes Risk Minimization (LBRM) framework, which divides the prediction process into recursively deciding to stop or to go down at each decision node by balancing these two risks in a top-down fashion. Rather than setting a global loss function in the traditional Bayes risk framework, we replace it with different uncertainty in the two risks for each decision node. The uncertainty on the precipitance risk and the conservative risk are measured by information entropy on children nodes and information gain from the current node to children nodes, respectively. We propose a Weighted Tree Induced Error (WTIE) to obtain the predictions of minimum risk with different emphasis on the two risks. Experimental results on various datasets show the effectiveness of the proposed LBRM algorithm.
[Uncertainty, Hierarchical Classification, coarse-grained category, minimum risk, prediction process, fine-grained category, Information entropy, Stopping Strategy, conservative risk, LBRM, large-scale data classification tasks, entropy, lower level children nodes, informative prediction, Local Bayes Risk Minimization, learning (artificial intelligence), pattern classification, trees (mathematics), hierarchical structure, hierarchical classification, Sun, stopping strategy, local bayes risk minimization framework, Current measurement, traditional Bayes risk framework, Measurement uncertainty, Bayes methods, Risk management, hierarchical accuracy, risk problem, precipitance risk]
AWDA: An Adaptive Wishart Discriminant Analysis
2017 IEEE International Conference on Data Mining
None
2017
Linear Discriminant Analysis (LDA) is widely-used for supervised dimension reduction and linear classification. Classical LDA, however, suffers from the ill-posed estimation problem on data with high dimension and low sample size (HDLSS). To cope with this problem, in this paper, we propose an Adaptive Wishart Discriminant Analysis (AWDA) for classification, that makes predictions in an ensemble way. Comparing to existing approaches, AWDA has two advantages: 1) leveraging theWishart distribution, AWDA ensembles multiple LDA classifiers parameterized by the sampled covariance matrices via a Bayesian Voting Scheme, which theoretically improves the robustness of classification, compared to LDA classifiers using a single (probably ill-posed) covariance matrix estimator; 2) AWDA updates the weights for voting optimally to adapt the local information of each new input data, so as to enable the nonlinear classification. Theoretical analysis indicates that AWDA guarantees a close approximation to the optimal Bayesian inference and thus achieves robust performance on high dimensional data. Extensive experiments on real-world datasets show that our approach outperforms state-of-the-art algorithms by a large margin.
[Algorithm design and analysis, Data Mining, Complexity theory, statistical distributions, estimation problem, Covariance matrices, nonlinear classification, Training, high dimension and low sample size, data reduction, high dimensional data, feature extraction, pose estimation, Linear Discriminant Analysis, supervised dimension reduction, Classification, optimal Bayesian inference, Bayesian Inference and Wishart Distribution, sampled covariance matrices, covariance matrices, pattern classification, Estimation, Wishart distribution, inference mechanisms, AWDA, matrix algebra, covariance matrix estimator, Bayesian Voting Scheme, linear classification, Approximation algorithms, Bayes methods, Adaptive Wishart Discriminant Analysis, multiple LDA classifiers]
Generating Medical Hypotheses Based on Evolutionary Medical Concepts
2017 IEEE International Conference on Data Mining
None
2017
Literature based discovery (LBD) is a task that aims to uncover hidden associations between non-interacting scientific concepts by rationally connecting independent nuggets of information. Broadly, prior approaches to LBD include use of: a) distributional statistics and explicit representation, b) graph-theoretic measures, and c) supervised machine learning methods to find associations. However, purely distributional approaches may not necessarily entail semantically meaningful association and graph-theoretic approaches suffer from scalability issues. While supervised machine learning based approaches have the potential to elucidate associations, the training data required is too expensive to generate. In this paper we propose a novel dynamic Medical Subject Heading (MeSH) embedding model which is able to model the evolutionary behavior of medical concepts to uncover latent associations between them. The proposed model allows us to learn the evolutionary trajectories of MeSH embeddings and detect informative terms. Hence, based on the dynamic MeSH embeddings, meaningful medical hypotheses can be efficiently generated. To evaluate the efficacy of the proposed model, we perform both qualitative and quantitative evaluation. The results demonstrate that leveraging the evolutionary features of MeSH concepts is an effective way for predicting novel associations.
[supervised machine learning methods, literature based discovery, Unified modeling language, graph theory, data mining, Manuals, explicit representation, statistical distributions, evolutionary behavior, dynamic medical subject heading embedding model, LBD, medical hypotheses generation, Semantics, evolutionary features, graph-theoretic approaches, Fish, learning (artificial intelligence), latent associations, semantically meaningful association, Oils, MeSH concepts, medical information systems, Diseases, informative terms detection, evolutionary medical concepts, distributional statistics, evolutionary trajectories, dynamic MeSH embeddings]
HistoSketch: Fast Similarity-Preserving Sketching of Streaming Histograms with Concept Drift
2017 IEEE International Conference on Data Mining
None
2017
Histogram-based similarity has been widely adopted in many machine learning tasks. However, measuring histogram similarity is a challenging task for streaming data, where the elements of a histogram are observed in a streaming manner. First, the ever-growing cardinality of histogram elements makes any similarity computation inefficient. Second, the concept-drift issue in the data streams also impairs the accurate assessment of the similarity. In this paper, we propose to overcome the above challenges with HistoSketch, a fast similarity-preserving sketching method for streaming histograms with concept drift. Specifically, HistoSketch is designed to incrementally maintain a set of compact and fixed-size sketches of streaming histograms to approximate similarity between the histograms, with the special consideration of gradually forgetting the outdated histogram elements. We evaluate HistoSketch on multiple classification tasks using both synthetic and real-world datasets. The results show that our method is able to efficiently approximate similarity for streaming histograms and quickly adapt to concept drift. Compared to full streaming histograms gradually forgetting the outdated histogram elements, HistoSketch is able to dramatically reduce the classification time (with a 7500x speedup) with only a modest loss in accuracy (about 3.5%).
[Weight measurement, pattern classification, fixed-size sketches, Buildings, data streams, multiple classification tasks, fast similarity-preserving sketching method, Electronic mail, Streaming Data, machine learning, Similarity-Preserving Sketching, Concept Drift, Histograms, Image color analysis, concept drift, Consistent Weighted Sampling, Streaming media, HistoSketch, streaming histograms, learning (artificial intelligence), histogram similarity]
Mining Customer Valuations to Optimize Product Bundling Strategy
2017 IEEE International Conference on Data Mining
None
2017
Product bundling is widely adopted for information goods and online services because it can increase profit for companies. For example, cable companies often bundle Internet access and video streaming services together. However, it is challenging to obtain an optimal bundling strategy, not only because it is computationally expensive, but also that customers' private information (e.g., valuations for products) is needed for the decision, and we need to infer it from accessible datasets. As customers' purchasing data are getting richer due to the popularity of online shopping, doors are open for us to infer this information. This paper aims to address: (1) How to infer customers' valuations from the purchasing data? (2) How to determine the optimal product bundle to maximize the profit? We first formulate a profit maximization framework to select the optimal bundle set. We show that finding the optimal bundle set is NPhard. We then identify key factors that impact the profitability of product bundling. These findings give us insights to develop a computationally efficient algorithm to approximate the optimal product bundle with a provable performance guarantee. To obtain the input of the bundling algorithm, we infer the distribution of customers' valuations from their purchasing data, based on which we run our bundling algorithm and conduct experiments on an Amazon co-purchasing dataset. We extensively evaluate the accuracy of our inference and the bundling algorithm. Our results reveal conditions under which bundling is highly profitable and provide insights to guide the deployment of product bundling.
[profitability, profit maximization framework, purchasing, data mining, Companies, consumer behaviour, Data Mining, Cost accounting, purchasing data, Amazon co-purchasing dataset, Sociology, information goods, Approximation Algorithm, Mathematical model, Consumer Valuations, online services, Economics, optimal bundling strategy, optimal bundle set, bundling algorithm, customer valuation mining, product bundling strategy, Bundling, optimal product bundle, Approximation algorithms, Inference algorithms, Internet]
Matrix Profile VI: Meaningful Multidimensional Motif Discovery
2017 IEEE International Conference on Data Mining
None
2017
Time series motifs are approximately repeating patterns in real-valued time series data. They are useful for exploratory data mining and are often used as inputs for various time series clustering, classification, segmentation, rule discovery, and visualization algorithms. Since the introduction of the first motif discovery algorithm for univariate time series in 2002, multiple efforts have been made to generalize motifs to the multidimensional case. In this work, we show that these efforts, which typically attempt to find motifs on all dimensions, will not produce meaningful motifs except in the most contrived situations. We explain this finding and introduce mSTAMP, an algorithm that allows meaningful discovery of multidimensional motifs. Beyond producing objectively and subjectively meaningful results, our algorithm has a host of additional advantages, including being much faster, requiring fewer parameters and supporting streaming data. We demonstrate the utility of our mSTAMP-based motif discovery framework on domains as diverse as audio processing, industry, and sports analytics.
[Industries, mathematics computing, motif discovery framework, data mining, univariate time series, rule discovery, Classification algorithms, Data mining, time series motifs, Multidimensional Data, Motif Discovery, Clustering algorithms, data visualisation, multidimensional motifs, Robustness, real-valued time series data, pattern classification, matrix profile VI, meaningful discovery, data streaming, Time series analysis, generalize motifs, time series, meaningful motifs, mSTAMP, meaningful multidimensional motif discovery, visualization algorithms, pattern clustering, motif discovery algorithm, Time Series, Approximation algorithms, time series clustering, exploratory data mining]
Deep Similarity-Based Batch Mode Active Learning with Exploration-Exploitation
2017 IEEE International Conference on Data Mining
None
2017
Active learning aims to reduce manual labeling efforts by proactively selecting the most informative unlabeled instances to query. In real-world scenarios, it's often more practical to query a batch of instances rather than a single one at each iteration. To achieve this we need to keep not only the informativeness of the instances but also their diversity. Many heuristic methods have been proposed to tackle batch mode active learning problems, however, they suffer from two limitations which if addressed would significantly improve the query strategy. Firstly, the similarity amongst instances is simply calculated using the feature vectors rather than being jointly learned with the classification model. This weakens the accuracy of the diversity measurement. Secondly, these methods usually exploit the decision boundary by querying the data points close to it. However, this can be inefficient when the labeled set is too small to reveal the true boundary. In this paper, we address both limitations by proposing a deep neural network based algorithm. In the training phase, a pairwise deep network is not only trained to perform classification, but also to project data points into another space, where the similarity can be more precisely measured. In the query selection phase, the learner selects a set of instances that are maximally uncertain and minimally redundant (exploitation), as well as are most diverse from the labeled instances (exploration). We evaluate the effectiveness of the proposed method on a variety of classification tasks: MNIST classification, opinion polarity detection, and heart failure prediction. Our method outperforms the baselines with both higher classification accuracy and faster convergence rate.
[Uncertainty, query strategy, diversity measurement, deep neural network based algorithm, Electronic mail, decision boundary, query processing, Clustering algorithms, batch mode active learning problems, Labeling, learning (artificial intelligence), feature selection, pattern classification, query selection phase, exploration-exploitation, Redundancy, heuristic methods, classification model, Support vector machines, vectors, data points projection, Neural networks, feature vectors, informative unlabeled instances, deep similarity, neural nets, pairwise deep network]
SPTF: A Scalable Probabilistic Tensor Factorization Model for Semantic-Aware Behavior Prediction
2017 IEEE International Conference on Data Mining
None
2017
With the rapid rise of various e-commerce and social network platforms, users are generating large amounts of heterogeneous behavior data, such as purchasehistory, adding-to-favorite, adding-to-cart and click activities, and this kind of user behavior data is usually binary, only reflecting a user's action or inaction (i.e., implicit feedback data). Tensor factorization is a promising means of modeling heterogeneous user behaviors by distinguishing different behavior types. However, ambiguity arises in the interpretation of the unobserved user behavior records that mix both real negative examples and potential positive examples. Existing tensor factorization models either ignore unobserved examples or treat all of them as negative examples, leading to either poor prediction performance or huge computation cost. In addition, the distribution of positive examples w.r.t. behavior types is heavily skewed. Existing tensor factorization models would bias towards the type of behaviors with a large number of positive examples. In this paper, we propose a scalable probabilistic tensor factorization model (SPTF) for heterogeneous behavior data and develop a novel negative sampling technique to optimize SPTF by leveraging both observed and unobserved examples with much lower computational costs and higher modeling accuracy. To overcome the issue of the heavy skewness of the behavior data distribution, we propose a novel adaptive ranking-based positive sampling approach to speed up the model convergence and improve the prediction accuracy for sparse behavior types. Our proposed model optimization techniques enable SPTF to be scalable to large-scale behavior datasets. Extensive experiments have been conducted on a large-scale e-commerce dataset, and the experimental results show the superiority of our proposed SPTF model in terms of prediction accuracy and scalability.
[SPTF model, heterogeneous user behaviors, Adaptation models, model convergence, scalable probabilistic tensor factorization model, data mining, user behavior data, Predictive models, tensors, matrix decomposition, negative examples, optimisation, positive examples, heterogeneous behavior data, Mathematical model, learning (artificial intelligence), implicit feedback data, electronic commerce, sampling methods, adaptive ranking, sparse behavior types, Computational modeling, Probabilistic logic, positive sampling, tensor factorization models, unobserved user behavior records, semantic-aware behavior prediction, Tensile stress, recommender systems, large-scale behavior datasets, behavior data distribution, social networking (online), Data models, negative sampling, social network platforms, model optimization]
Supervised Belief Propagation: Scalable Supervised Inference on Attributed Networks
2017 IEEE International Conference on Data Mining
None
2017
Given an undirected network where some of the nodes are labeled, how can we classify the unlabeled nodes with high accuracy? Loopy Belief Propagation (LBP) is an inference algorithm widely used for this purpose with various applications including fraud detection, malware detection, web classification, and recommendation. However, previous methods based on LBP have problems in modeling complex structures of attributed networks because they manually and heuristically select the most important parameter, the propagation strength. In this paper, we propose Supervised Belief Propagation (SBP), a scalable and novel inference algorithm which automatically learns the optimal propagation strength by supervised learning. SBP is generally applicable to attributed networks including weighted and signed networks. Through extensive experiments, we demonstrate that SBP generalizes previous LBP-based methods and outperforms previous LBP and RWR based methods in real-world networks.
[Scalability, graph theory, supervised learning, undirected network, network theory (graphs), scalable Supervised inference, optimal propagation strength, real-world networks, Training, optimisation, RWR based methods, weighted networks, unlabeled nodes, Mathematical model, belief networks, learning (artificial intelligence), signed networks, SBP, pattern classification, Computational modeling, malware detection, Social network, Encoding, inference mechanisms, Node classification, inference algorithm, fraud, fraud detection, Inference algorithms, attributed networks, Loopy Belief Propagation, Belief propagation]
BL-MNE: Emerging Heterogeneous Social Network Embedding Through Broad Learning with Aligned Autoencoder
2017 IEEE International Conference on Data Mining
None
2017
Network embedding aims at projecting the network data into a low-dimensional feature space, where the nodes are represented as a unique feature vector and network structure can be effectively preserved. In recent years, more and more online application service sites can be represented as massive and complex networks, which are extremely challenging for traditional machine learning algorithms to deal with. Effective embedding of the complex network data into low-dimension feature representation can both save data storage space and enable traditional machine learning algorithms applicable to handle the network data. Network embedding performance will degrade greatly if the networks are of a sparse structure, like the emerging networks with few connections. In this paper, we propose to learn the embedding representation for a target emerging network based on the broad learning setting, where the emerging network is aligned with other external mature networks at the same time. To solve the problem, a new embedding framework, namely "Deep alIgned autoencoder based eMbEdding" (DIME), is introduced in this paper. DIME handles the diverse link and attribute in a unified analytic based on broad learning, and introduces the multiple aligned attributed heterogeneous social network concept to model the network structure. A set of meta paths are introduced in the paper, which define various kinds of connections among users via the heterogeneous link and attribute information. The closeness among users in the networks are defined as the meta proximity scores, which will be fed into DIME to learn the embedding vectors of users in the emerging network. Extensive experiments have been done on real-world aligned social networks, which have demonstrated the effectiveness of DIME in learning the emerging network embedding vectors.
[emerging network embedding vectors, Machine learning algorithms, Terminology, online application service sites, deep aligned autoencoder based embedding, target emerging network, Twitter, embedding representation, complex networks, network embedding performance, broad learning setting, storage management, feature extraction, Complex networks, Heterogeneous networks, learning (artificial intelligence), low-dimension feature representation, machine learning algorithms, Heterogeneous Information Network, Broad Learning, embedding framework, complex network data, low-dimensional feature space, Network Embedding, BL-MNE, social networks, data storage space, attributed heterogeneous social network concept, Multiple Aligned Social Networks, external mature networks, social networking (online), DIME]
Data-Driven Immunization
2017 IEEE International Conference on Data Mining
None
2017
Given a contact network and coarse-grained diagnostic information like electronic Healthcare Reimbursement Claims (eHRC) data, can we develop efficient intervention policies to control an epidemic? Immunization is an important problem in multiple areas especially epidemiology and public health. However, most existing studies focus on developing pre-emptive strategies assuming prior epidemiological models. In practice, disease spread is usually complicated, hence assuming an underlying model may deviate from true spreading patterns, leading to possibly inaccurate interventions. Additionally, the abundance of health care surveillance data (like eHRC) makes it possible to study data-driven strategies without too many restrictive assumptions. Hence, such an approach can help public-health experts take more practical decisions. In this paper, we take into account propagation log and contact networks for controlling propagation. We formulate the novel and challenging Data-Driven Immunization problem without assuming classical epidemiological models. To solve it, we first propose an efficient sampling approach to align surveillance data with contact networks, then develop an efficient algorithm with the provably approximate guarantee for immunization. Finally, we show the effectiveness and scalability of our methods via extensive experiments on multiple datasets, and conduct case studies on nation-wide real medical surveillance data.
[prior epidemiological models, contact network, electronic Healthcare Reimbursement Claims data, Vaccines, eHRC data, epidemiology, contact networks, coarse-grained diagnostic information, Sociology, medical administrative data processing, surveillance, intervention policies, health care, public health, Immune system, sampling methods, disease spread, pre-emptive strategies, diseases, electronic health records, Diseases, Data-Driven Immunization, propagation controlling, sampling approach, epidemiological models, epidemics, Surveillance, health care surveillance data, Approximation algorithms, public-health experts, Resource management, account propagation log, data-driven strategies]
Online and Distributed Robust Regressions Under Adversarial Data Corruption
2017 IEEE International Conference on Data Mining
None
2017
In today's era of big data, robust least-squares regression becomes a more challenging problem when considering the adversarial corruption along with explosive growth of datasets. Traditional robust methods can handle the noise but suffer from several challenges when applied in huge dataset including 1) computational infeasibility of handling an entire dataset at once, 2) existence of heterogeneously distributed corruption, and 3) difficulty in corruption estimation when data cannot be entirely loaded. This paper proposes online and distributed robust regression approaches, both of which can concurrently address all the above challenges. Specifically, the distributed algorithm optimizes the regression coefficients of each data block via heuristic hard thresholding and combines all the estimates in a distributed robust consolidation. Furthermore, an online version of the distributed algorithm is proposed to incrementally update the existing estimates with new incoming data. We also prove that our algorithms benefit from strong robustness guarantees in terms of regression coefficient recovery with a constant upper bound on the error of state-of-the-art batch methods. Extensive experiments on synthetic and real datasets demonstrate that our approaches are superior to those of existing methods in effectiveness, with competitive efficiency.
[data block, estimation theory, Stochastic processes, regression analysis, corruption estimation, online robust regression, least-squares regression, regression coefficients, optimisation, online version, heuristic hard thresholding, heterogeneously distributed corruption, Distributed databases, Online Algorithm, incoming data, Robustness, big data, Distributed algorithms, synthetic datasets, distributed robust regression, least squares approximations, adversarial data corruption, Distributed Algorithm, Big Data, batch methods, real datasets, Robust Regression, distributed algorithm, Upper bound, regression coefficient recovery, distributed algorithms, distributed robust consolidation, Explosives, Data models]
MetaLDA: A Topic Model that Efficiently Incorporates Meta Information
2017 IEEE International Conference on Data Mining
None
2017
Besides the text content, documents and their associated words usually come with rich sets of meta information, such as categories of documents and semantic/syntactic features of words, like those encoded in word embeddings. Incorporating such meta information directly into the generative process of topic models can improve modelling accuracy and topic quality, especially in the case where the word-occurrence information in the training data is insufficient. In this paper, we present a topic model, called MetaLDA, which is able to leverage either document or word meta information, or both of them jointly. With two data argumentation techniques, we can derive an efficient Gibbs sampling algorithm, which benefits from the fully local conjugacy of the model. Moreover, the algorithm is favoured by the sparsity of the meta information. Extensive experiments on several real world datasets demonstrate that our model achieves comparable or improved performance in terms of both perplexity and topic quality, particularly in handling sparse texts. In addition, compared with other models using meta information, our model runs significantly faster.
[word Meta Information, text analysis, Computational modeling, Social network services, topic model, documents, short texts, word-occurrence information, data argumentation, Electronic mail, word embeddings, MetaLDA, meta information, topic quality, Gibbs sampling algorithm, associated words, Analytical models, Monte Carlo methods, Aggregates, Semantics, Markov processes, topic models, Data models]
Collaborative Filtering with Social Local Models
2017 IEEE International Conference on Data Mining
None
2017
Matrix Factorization (MF) is a very popular method for recommendation systems. It assumes that the underneath rating matrix is low-rank. However, this assumption can be too restrictive to capture complex relationships and interactions among users and items. Recently, Local LOw-Rank Matrix Approximation (LLORMA) has been shown to be very successful in addressing this issue. It just assumes the rating matrix is composed of a number of low-rank submatrices constructed from subsets of similar users and items. Although LLORMA outperforms MF, how to construct such submatrices remains a big problem. Motivated by the availability of rich social connections in today's recommendation systems, we propose a novel framework, i.e., Social LOcal low-rank Matrix Approximation (SLOMA), to address this problem. To the best of our knowledge, SLOMA is the first work to incorporate social connections into the local low-rank framework. Furthermore, we enhance SLOMA by applying social regularization to submatrices factorization, denoted as SLOMA++. Therefore, the proposed model can benefit from both social recommendation and the local low-rank assumption. Experimental results from two real-world datasets, Yelp and Douban, demonstrate the superiority of the proposed models over LLORMA and MF.
[low-rank framework, collaborative filtering, Collaborative Filtering, Social groups, social connections, low-rank submatrices, Matrix factorization, matrix decomposition, Matrix Factorization, Optimization, Recommendation system, submatrices factorization, Mathematical model, SLOMA, Social network services, Local low-rank, social regularization, Social network, Social LOcal low-rank Matrix Approximation, Indexes, Matrix decomposition, social recommendation, recommendation systems, recommender systems, LLORMA, social local models, Collaboration, social networking (online)]
Exploiting Hierarchical Structures for POI Recommendation
2017 IEEE International Conference on Data Mining
None
2017
With the rapid development of location-based social networks, Point-of-Interest (POI) recommendation has played an important role in helping people discover attractive locations. However, existing POI recommendation methods assume a flat structure of POIs, which are better described in a hierarchical structure in reality. Furthermore, we discover that both users' content and spatial preferences exhibit hierarchical structures. To this end, in this paper, we propose a hierarchical geographical matrix factorization model (HGMF) to utilize the hierarchical structures of both users and POIs for POI recommendation. Specifically, we first describe the POI influence degrees over regions with two-dimensional normal distribution, and learn the influence areas of different layers of POIs as the input of HGMF. Then, we perform matrix factorization on user content preference matrix, user spatial preference matrix, and POIs characteristic matrix jointly with the modeling of implicit hierarchical structures. Moreover, a two-step optimization method is proposed to learn the implicit hierarchical structure and find the solution of HGMF efficiently. Finally, we evaluate HGMF on two large-scale real-world location-based social networks datasets. Our experimental results demonstrate that it outperforms the state-of-the-art methods in terms of precision and recall.
[Point-of-Interest recommendation, Social network services, hierarchical geographical matrix factorization model, user content preference matrix, Optimization methods, POI recommendation methods, Gaussian distribution, implicit hierarchical structure, matrix decomposition, Matrix decomposition, Data mining, Computer science, recommender systems, social networking (online), HGMF, POIs characteristic matrix, user spatial preference matrix, Recommender systems, POI influence degrees]
AnySCAN: An Efficient Anytime Framework with Active Learning for Large-Scale Network Clustering
2017 IEEE International Conference on Data Mining
None
2017
Network clustering is an essential approach to finding latent clusters in real-world networks. As the scale of real-world networks becomes increasingly larger, the existing network clustering algorithms fail to discover meaningful clusters efficiently. In this paper, we propose a framework called AnySCAN, which applies anytime theory to the structural clustering algorithm for networks (SCAN). Moreover, an active learning strategy is proposed to advance the refining procedure in AnySCAN framework. AnySCAN with the active learning strategy is able to find the exactly same clustering result on large-scale networks as the original SCAN in a significantly more efficient manner. Extensive experiments on real-world and synthetic networks demonstrate that our proposed method outperforms existing network clustering approaches.
[anytime framework, Machine learning algorithms, Conferences, graph theory, anytime theory, active learning strategy, network theory (graphs), Data structures, Electronic mail, Data mining, real-world networks, pattern clustering, Clustering algorithms, structural clustering algorithm, Rocks, social networking (online), AnySCAN framework, learning (artificial intelligence), latent clusters, large-scale network clustering]
SCED: A General Framework for Sparse Tensor Decomposition with Constraints and Elementwise Dynamic Learning
2017 IEEE International Conference on Data Mining
None
2017
CANDECOMP/PARAFAC Decomposition (CPD) is one of the most popular tensor decomposition methods that has been extensively studied and widely applied. In recent years, sparse tensors that contain a huge portion of zeros but a limited number of non-zeros have attracted increasing interest. Existing techniques are not directly applicable to sparse tensors, since they mainly target dense ones and usually have poor efficiency. Additionally, specific issues also arise for sparse tensors, depending on different data sources and applications: the role of zero entries can be different; incorporating constraints like non-negativity and sparseness might be necessary; the ability to learn on-the-fly is a must for dynamic scenarios that new data keeps arriving at high velocity. However, state-of-art algorithms only partially address the above issues. To fill this gap, we propose a general framework for finding the CPD of sparse tensors. Modeling the sparse tensor decomposition problem by a generalized weighted CPD formulation and solving it efficiently, our proposed method is also flexible to handle constraints and dynamic data streams. Through experiments on both synthetic and real-world datasets, for the static case, our method demonstrates significant improvements in terms of effectiveness, efficiency and scalability. Moreover, under the dynamic setting, our method speeds up current technology by hundreds to thousands times, without sacrificing decomposition quality.
[data sources, generalized weighted CPD formulation, Heuristic algorithms, tensors, matrix decomposition, Matrix decomposition, Sparse matrices, Data mining, Optimization, Tensile stress, CANDECOMP/PARAFAC decomposition, sparse tensor decomposition problem, Loading, elementwise dynamic learning, tensor decomposition methods, data handling, learning (artificial intelligence)]
A Randomized Approach for Crowdsourcing in the Presence of Multiple Views
2017 IEEE International Conference on Data Mining
None
2017
Driven by the dramatic growth of data both in terms of the size and sources, learning from heterogeneous data is emerging as an important research direction for many real applications. One of the biggest challenges of this type of problem is how to meaningfully integrate heterogeneous data to considerably improve the generality and quality of the learning model. In this paper, we first present a unified learning framework that aims to leverage the structural information from two types of data heterogeneity: view heterogeneity (as in multi-view learning) and worker heterogeneity (as in crowdsourcing). The objective follows the principles of view consistency and worker consensus by minimizing the loss term with a regularized prediction tensor. We then propose to relax and solve the optimization framework with an iterative updating method. We also prove that the gradient of the most time-consuming updating block is separable with respect to the workers, which leads to a randomized algorithm with faster speed and better convergence. Finally, we compare the proposed method with several state-of-the-arts and demonstrate its effectiveness on various data sets.
[worker heterogeneity, iterative methods, Correlation, tensors, Data mining, multiple views, Optimization, worker consensus, heterogeneous data, multiview learning, unified learning framework, regularized prediction tensor, data sets, optimization framework, structural information, learning (artificial intelligence), Kernel, view heterogeneity, Crowdsourcing, Multi-view Learning, randomized approach, iterative updating method, crowdsourcing, data analysis, randomized algorithm, prediction theory, randomised algorithms, view consistency, Tensile stress, learning model quality, data heterogeneity, Heterogeneous Learning, loss term minimization, Data models, minimisation]
Matrix Profile VII: Time Series Chains: A New Primitive for Time Series Data Mining (Best Student Paper Award)
2017 IEEE International Conference on Data Mining
None
2017
Since their introduction over a decade ago, time series motifs have become a fundamental tool for time series analytics, finding diverse uses in dozens of domains. In this work we introduce Time Series Chains, which are related to, but distinct from, time series motifs. Informally, time series chains are a temporally ordered set of subsequence patterns, such that each pattern is similar to the pattern that preceded it, but the first and last patterns are arbitrarily dissimilar. In the discrete space, this is similar to extracting the text chain "hit, hot, dot, dog" from a paragraph. The first and last words have nothing in common, yet they are connected by a chain of words with a small mutual difference. Time series chains can capture the evolution of systems, and help predict the future. As such, they potentially have implications for prognostics. In this work, we introduce a robust definition of time series chains, and a scalable algorithm that allows us to discover them in massive datasets.
[Heart, Hamming distance, data analysis, Time series analysis, data mining, time series analytics, time series, Data mining, time series motifs, Prognostics, Link Analysis, time series chains, Motifs, Dogs, Time Series, Robustness, time series data mining]
Spatio-Temporal Neural Networks for Space-Time Series Forecasting and Relations Discovery
2017 IEEE International Conference on Data Mining
None
2017
We introduce a dynamical spatio-temporal model formalized as a recurrent neural network for forecasting time series of spatial processes, i.e. series of observations sharing temporal and spatial dependencies. The model learns these dependencies through a structured latent dynamical component, while a decoder predicts the observations from the latent representations. We consider several variants of this model, corresponding to different prior hypothesis about the spatial relations between the series. The model is evaluated and compared to state-of-the-art baselines, on a variety of forecasting problems representative of different application areas: epidemiology, geo-spatial statistics and car-traffic prediction. Besides these evaluations, we also describe experiments showing the ability of this approach to extract relevant spatial relations.
[data mining, Predictive models, latent representations, relations discovery, structured latent dynamical component, spatio-temporal neural networks, car-traffic prediction, temporal dependencies, learning (artificial intelligence), recurrent neural network, Biological system modeling, Computational modeling, Time series analysis, recurrent neural nets, dynamical spatio-temporal model, spatial dependencies, time series, geo-spatial statistics, Forecasting, space-time series forecasting, Hidden Markov models, forecasting theory, spatial processes, Data models, statistical analysis, relevant spatial relations]
Differentially Private Mixture of Generative Neural Networks
2017 IEEE International Conference on Data Mining
None
2017
Generative models are used in an increasing number of applications that rely on large amounts of contextually rich information about individuals. Owing to possible privacy violations, however, publishing or sharing generative models is not always viable. In this paper, we introduce a novel solution for privately releasing generative models and entire high-dimensional datasets produced by these models. We model the generator distribution of the training data by a mixture of k generative neural networks. These are trained together and collectively learn the generator distribution of a dataset. Data is divided into k clusters, using a novel differentially private kernel k-means, then each cluster is given to separate generative neural networks, such as Restricted Boltzmann Machines or Variational Autoencoders, which are trained only on their own cluster using differentially private gradient descent. We evaluate our approach using the MNIST dataset and a large Call Detail Records dataset, showing that it produces realistic synthetic samples, which can also be used to accurately compute arbitrary number of counting queries.
[data training, Data privacy, separate generative neural networks, differentially private mixture, Restricted Boltzmann Machines, Training, query processing, Privacy, high-dimensional datasets, generator distribution, learning (artificial intelligence), Variational Autoencoders, Kernel, gradient methods, publishing, differentially private kernel k-means, generative models, Standards, differentially private gradient descent, pattern clustering, Neural networks, Data models, data privacy, generative neural networks, clustering, privacy violations, neural nets, differential privacy, kernel k-means]
A Probabilistic Geographical Aspect-Opinion Model for Geo-Tagged Microblogs
2017 IEEE International Conference on Data Mining
None
2017
Due to the rapid increase in the number of users owning location-based devices, there is a considerable amount of geo-tagged data available on social media websites, such as Twitter and Facebook. This geo-tagged data can be useful in a variety of ways to extract location-specific information, as well as to comprehend the variation of information across different geographical regions. A lot of techniques have been proposed for extracting location-based information from social media, but none of these techniques aim to utilize an important characteristic of this data, which is the presence of aspects and their opinions, expressed by the users on these platforms. In this paper, we propose Geographic Aspect Opinion model (GASPOP), a probabilistic model that jointly discovers the variation of aspect and opinion, that correspond to different topics across various geographical regions from geo-tagged social media data. It incorporates the syntactic features of text in the generative process to differentiate aspect and opinion words from general background words. The user-based modeling of topics, also enables it to determine the interest distribution of various users. Furthermore, our model can be used to predict the location of different tweets based on their text. We evaluated our model on Twitter data, and our experimental results show that GASPOP can jointly discover latent aspect and opinion words for different topics across latent geographical regions. Moreover, a quantitative analysis of GASPOP using widely used evaluation metrics shows that it outperforms the state-of-the-art methods.
[opinion mining, location based services, location-specific information, Twitter data, data mining, general background words, Twitter, topic modeling, latent geographical regions, Data mining, probabilistic model, geo-tagged microblogs, mobile computing, Voting, probabilistic geographical Aspect-Opinion model, social media data, Facebook, location extractions, social media websites, aspect mining, probability, Geographic Aspect Opinion model, Probabilistic logic, Microblogs, opinion words, location-based devices, geographical regions, probabilistic models, geo-tagged data, social networking (online), Data models, GASPOP, latent aspect, Web sites]
Aspect Sentiment Model for Micro Reviews
2017 IEEE International Conference on Data Mining
None
2017
This paper aims at an aspect sentiment model for aspect-based sentiment analysis (ABSA) focused on micro reviews. This task is important in order to understand short reviews majority of the users write, while existing topic models are targeted for expert-level long reviews with sufficient co-occurrence patterns to observe. Current methods on aggregating micro reviews using metadata information may not be effective as well due to metadata absence, topical heterogeneity, and cold start problems. To this end, we propose a model called Micro Aspect Sentiment Model (MicroASM). MicroASM is based on the observation that short reviews 1) are viewed with sentiment-aspect word pairs as building blocks of information, and 2) can be clustered into larger reviews. When compared to the current state-of-the-art aspect sentiment models, experiments show that our model provides better performance on aspect-level tasks such as aspect term extraction and document-level tasks such as sentiment classification.
[text analysis, aspect term extraction, aspect-level tasks, data mining, short reviews majority, expert-level long reviews, sentiment-aspect word pairs, Analytical models, aspect-based sentiment analysis, microreviews, Motion pictures, document-level tasks, aspect sentiment models, Mathematical model, meta data, pattern classification, Atmospheric modeling, natural language processing, sentiment analysis, sentiment classification, MicroASM, information retrieval, aspect sentiment model, MicroAspect Sentiment Model, ABSA, recommender systems, Tagging, topic models, Data models]
Mining the Demographics of Political Sentiment from Twitter Using Learning from Label Proportions
2017 IEEE International Conference on Data Mining
None
2017
Opinion mining and demographic attribute inference have many applications in social science. In this paper, we propose models to infer daily joint probabilities of multiple latent attributes from Twitter data, such as political sentiment and demographic attributes. Since it is costly and time-consuming to annotate data for traditional supervised classification, we instead propose scalable Learning from Label Proportions (LLP) models for demographic and opinion inference using U.S. Census, national and state political polls, and Cook partisan voting index as population level data. In LLP classification settings, the training data is divided into a set of unlabeled bags, where only the label distribution of each bag is known, removing the requirement of instance-level annotations. Our proposed LLP model, Weighted Label Regularization (WLR), provides a scalable generalization of prior work on label regularization to support weights for samples inside bags, which is applicable in this setting where bags are arranged hierarchically (e.g., county-level bags are nested inside of state-level bags). We apply our model to Twitter data collected in the year leading up to the 2016 U.S. presidential election, producing estimates of the relationships among political sentiment and demographics over time and place. We find that our approach closely tracks traditional polling data stratified by demographic category, resulting in error reductions of 28-44% over baseline approaches. We also provide descriptive evaluations showing how the model may be used to estimate interactions among many variables and to identify linguistic temporal variation, capabilities which are typically not feasible using traditional polling methods.
[demographic attribute inference, politics, Twitter data, data mining, state political polls, Twitter, Data Mining, national political polls, population level data, Machine Learning, county-level bags, LLP, multiple latent attributes, NLP, supervised classification, Sociology, demography, political sentiment mining, demographic opinion inference, Robustness, learning (artificial intelligence), pattern classification, sentiment analysis, probability, joint probabilities, LLP classification settings, Weighted Label Regularization, Indexes, social science, Statistics, Learning from Label Proportions, Label Regularization, LLP model, demographic attributes, social networking (online), Data models, label distribution, Opinion mining, instance-level annotations, unlabeled bags, Cook partisan voting index, US Census, demographics, demographic category, state-level bags]
Hierarchical Multinomial-Dirichlet Model for the Estimation of Conditional Probability Tables
2017 IEEE International Conference on Data Mining
None
2017
We present a novel approach for estimating conditional probability tables, based on a joint, rather than independent, estimate of the conditional distributions belonging to the same table. We derive exact analytical expressions for the estimators and we analyse their properties both analytically and via simulation. We then apply this method to the estimation of parameters in a Bayesian network. Given the structure of the network, the proposed approach better estimates the joint distribution and significantly improves the classification performance with respect to traditional approaches.
[exact analytical expressions, Maximum likelihood estimation, Computational modeling, probability, hierarchical multinomial-dirichlet model, statistical distributions, Analytical models, conditional probability estimation, conditional probability tables, parameter estimation, Data models, Bayes methods, Numerical models, Bayesian networks, belief networks, Bayesian network, hierarchical Bayesian model, conditional distributions]
Multi-party Sparse Discriminant Learning
2017 IEEE International Conference on Data Mining
None
2017
Sparse Discriminant Analysis (SDA) has been widely used to improve the performance of classical Fisher's Linear Discriminant Analysis in supervised metric learning, feature selection and classification. With the increasing needs of distributed data collection, storage and processing, enabling the Sparse Discriminant Learning to embrace the Multi-Party distributed computing environments becomes an emerging research topic. This paper proposes a novel Multi-Party SDA algorithm, which can learn SDA models effectively without sharing any raw dataand basic statistics among machines. The proposed algorithm 1) leverages the direct estimation of SDA [1] to derive a distributed loss function for the discriminant learning, 2) parameterizes the distributed loss function with local/global estimates through bootstrapping, and 3) approximates a global estimation of linear discriminant projection vector by optimizing the "distributed bootstrapping loss function" with gossip-based stochastic gradient descent. Experimental results on both synthetic and real-world benchmark datasets show that our algorithm can compete with the centralized SDA with similar performance, and significantly outperforms the most recent distributed SDA [2] in terms of accuracy and F1-score.
[Algorithm design and analysis, direct estimation, SDA models, MultiParty SDA algorithm, distributed processing, Covariance matrices, SDA, Bayesian Asymptotic Efficiency, linear discriminant projection, Multiparty Sparse Discriminant Learning, gossip-based stochastic gradient descent, Linear discriminant analysis, centralized SDA, learning (artificial intelligence), stochastic processes, gradient methods, supervised metric learning, feature selection, pattern classification, Fisher's Linear Discriminant Analysis, Computational modeling, Estimation, distributed loss function, raw data, MultiParty distributed computing environments, distributed data storage, basic statistics, distributed data collection, classification, vectors, Multi-Party Statistical Learning, Approximation algorithms, Data models, local estimation, distributed bootstrapping loss function, statistical analysis, Sparse Discriminant Analysis, distributed data processing, global estimation]
MDL for Causal Inference on Discrete Data
2017 IEEE International Conference on Data Mining
None
2017
The algorithmic Markov condition states that the most likely causal direction between two random variables X and Y can be identified as the direction with the lowest Kolmogorov complexity. This notion is very powerful as it can detect any causal dependency that can be explained by a physical process. However, due to the halting problem, it is also not computable. In this paper we propose an computable instantiation that provably maintains the key aspects of the ideal. We propose to approximate Kolmogorov complexity via the Minimum Description Length (MDL) principle, using a score that is mini-max optimal with regard to the model class under consideration. This means that even in an adversarial setting, the score degrades gracefully, and we are still maximally able to detect dependencies between the marginal and the conditional distribution. As a proof of concept, we propose CISC, a linear-time algorithm for causal inference by stochastic complexity, for pairs of univariate discrete variables. Experiments show that CISC is highly accurate on synthetic, benchmark, as well as real-world data, outperforming the state of the art by a margin, and scales extremely well with regard to sample and domain sizes.
[Additives, approximate Kolmogorov complexity, univariate discrete variables, halting problem, Complexity theory, minimax techniques, linear-time algorithm, Minimum Description Length principle, MDL, algorithmic Markov condition, causal direction, information theory, Discrete Data, causal dependency, stochastic complexity, Computational modeling, mini-max optimal score, discrete data, conditional distribution, Causal Inference, Markov processes, Data models, Inference algorithms, causal inference, computational complexity, CISC algorithm]
Efficient Mining of Subsample-Stable Graph Patterns
2017 IEEE International Conference on Data Mining
None
2017
A scalable method for mining graph patterns stable under subsampling is proposed. The existing subsample stability and robustness measures are not antimonotonic according to definitions known so far. We study a broader notion of antimonotonicity for graph patterns, so that measures of subsample stability become antimonotonic. Then we propose gSOFIA for mining the most subsample-stable graph patterns. The experiments on numerous graph datasets show that gSOFIA is very efficient for discovering subsample-stable graph patterns.
[Economics, sampling methods, gSOFIA, Conferences, graph theory, data mining, subsampling, subsample stability, Time measurement, Data mining, Thermal stability, graph mining, Formal Concept Analysis, Itemsets, subsample-stable graph patterns mining, graph patterns antimonotonicity, Robustness]
Multi-level Feedback Web Links Selection Problem: Learning and Optimization
2017 IEEE International Conference on Data Mining
None
2017
Selecting the right web links for a website is important because appropriate links not only can provide high attractiveness but can also increase the website's revenue. In this work, we first show that web links have an intrinsic multi-level feedback structure. For example, consider a 2-level feedback web link: the 1st level feedback provides the Click-Through Rate (CTR) and the 2nd level feedback provides the potential revenue, which collectively produce the compound 2-level revenue. We consider the context-free links selection problem of selecting links for a homepage so as to maximize the total compound 2-level revenue while keeping the total 1st level feedback above a preset threshold. We further generalize the problem to links with n (n &#x2265; 2)-level feedback structure. The key challenge is that the links' multi-level feedback structures are unobservable unless the links are selected on the homepage. To our best knowledge, we are the first to model the links selection problem as a constrained multi-armed bandit problem and design an effective links selection algorithm by learning the links' multi-level structure with provable sub-linear regret and violation bounds. We uncover the multi-level feedback structures of web links in two real-world datasets. We also conduct extensive experiments on the datasets to compare our proposed LExp algorithm with two state-of-the-art context-free bandit algorithms and demonstrate that LExp algorithm is the most effective in links selection while satisfying the constraint.
[Algorithm design and analysis, Knowledge engineering, CTR, click-through rate, compound 2-level revenue, constrained multi-armed bandit, Stochastic processes, link selection, learning, Random processes, Compounds, multilevel structure, links selection algorithm, feedback, optimisation, LExp, optimization, multi-level feedback, learning (artificial intelligence), total 1st level feedback, advertising data processing, context-free links selection problem, Probabilistic logic, 2-level feedback web link, 2-level feedback structure, Web pages, multilevel feedback structure, total compound 2-level revenue, Website revenue, multilevel feedback web links selection problem, Internet, 2nd level feedback, Web sites, constrained multiarmed bandit problem]
HitFraud: A Broad Learning Approach for Collective Fraud Detection in Heterogeneous Information Networks
2017 IEEE International Conference on Data Mining
None
2017
On electronic game platforms, different payment transactions have different levels of risk. Risk is generally higher for digital goods in e-commerce. However, it differs based on product and its popularity, the offer type (packaged game, virtual currency to a game or subscription service), storefront and geography. Existing fraud policies and models make decisions independently for each transaction based on transaction attributes, payment velocities, user characteristics, and other relevant information. However, suspicious transactions may still evade detection and hence we propose a broad learning approach leveraging a graph based perspective to uncover relationships among suspicious transactions, i.e., inter-transaction dependency. Our focus is to detect suspicious transactions by capturing common fraudulent behaviors that would not be considered suspicious when being considered in isolation. In this paper, we present HitFraud that leverages heterogeneous information networks for collective fraud detection by exploring correlated and fast evolving fraudulent behaviors. First, a heterogeneous information network is designed to link entities of interest in the transaction database via different semantics. Then, graph based features are efficiently discovered from the network exploiting the concept of meta-paths, and decisions on frauds are made collectively on test instances. Experiments on real-world payment transaction data from Electronic Arts demonstrate that the prediction performance is effectively boosted by HitFraud with fast convergence.
[transaction processing, transaction attributes, heterogeneous information network, inter-transaction dependency, graph theory, data mining, broad learning approach, heterogeneous information networks, HitFraud approach, packaged game, Training, Semantics, computer games, electronic game platforms, financial data processing, IP networks, learning (artificial intelligence), real-world payment transaction data, payment velocities, graph based features, information networks, Currencies, security of data, collective fraud detection, fraud, Games, existing fraud policies, Feature extraction]
Domain Adaptation for Online ECG Monitoring
2017 IEEE International Conference on Data Mining
None
2017
Successful ECG monitoring algorithms often rely on learned models to describe the heartbeats morphology. Unfortunately, when the heart rate increases the heartbeats get transformed, and a model that can properly describe the heartbeats of a specific user in resting conditions might not be appropriate for monitoring the same user during everyday activities. We model heartbeats by dictionaries yielding sparse representations and propose a novel domain-adaptation solution which transforms user-specific dictionaries according to the heart rate. In particular, we learn suitable linear transformations from a large dataset containing ECG tracings, and we show that these transformations can successfully adapt dictionaries when the heart rate changes. Remarkably, the same transformations can be used for multiple users and different sensing apparatus. We investigate the implications of our findings in ECG monitoring by wearable devices, and present an efficient implementation of an anomaly-detection algorithm leveraging such transformations.
[Adaptation models, model heartbeats, Dictionaries, heart rate changes, sparse representations, novel domain-adaptation solution, learned models, ECG tracings, suitable linear transformations, patient monitoring, signal representation, heartbeat morphology, electrocardiography, signal classification, Training, online ECG monitoring, medical signal processing, Heart beat, Electrocardiography, learning (artificial intelligence), user-specific dictionaries, anomaly-detection algorithm leveraging, Monitoring, dictionaries]
EC3: Combining Clustering and Classification for Ensemble Learning
2017 IEEE International Conference on Data Mining
None
2017
We propose EC3, a novel algorithm that merges classification and clustering together in order to support both binary and multi-class classification. EC3 is based on a principled combination of multiple classification and multiple clustering methods using a convex optimization function. We additionally propose iEC3, a variant of EC3 that handles imbalanced training data. We perform an extensive experimental analysis comparing EC3 and iEC3 with 12 baseline methods on 13 standard benchmark datasets. We show that our methods outperform other baselines for every single dataset, achieving at most 10% higher AUC. Moreover our methods are faster, more resilient to noise and class imbalance than the best baseline method.
[convex optimization function, imbalanced training data handling, imbalanced data, Clustering methods, multiple clustering methods, Optimization, Training, multiclass classification, EC3, Clustering algorithms, learning (artificial intelligence), Testing, pattern classification, data analysis, baseline methods, Linear programming, convex programming, classification, Standards, ensemble learning, Ensemble algorithm, pattern clustering, principled combination, clustering, multiple classification methods]
Boosting Deep Learning Risk Prediction with Generative Adversarial Networks for Electronic Health Records
2017 IEEE International Conference on Data Mining
None
2017
The rapid growth of Electronic Health Records (EHRs), as well as the accompanied opportunities in Data-Driven Healthcare (DDH), has been attracting widespread interests and attentions. Recent progress in the design and applications of deep learning methods has shown promising results and is forcing massive changes in healthcare academia and industry, but most of these methods rely on massive labeled data. In this work, we propose a general deep learning framework which is able to boost risk prediction performance with limited EHR data. Our model takes a modified generative adversarial network namely ehrGAN, which can provide plausible labeled EHR data by mimicking real patient records, to augment the training dataset in a semi-supervised learning manner. We use this generative model together with a convolutional neural network (CNN) based prediction model to improve the onset prediction performance. Experiments on two real healthcare datasets demonstrate that our proposed framework produces realistic data samples and achieves significant improvements on classification tasks with the generated data over several stat-of-the-art baselines.
[generative adversarial network, semisupervised learning, Data-Driven Healthcare, healthcare academia, Medical services, Predictive models, risk prediction performance, Gallium nitride, Training, deep learning, convolutional neural network based prediction model, learning (artificial intelligence), health care, generative adversarial networks, risk management, ehrGAN, EHRs, Generators, electronic health records, healthcare datasets, Machine learning, Data models, Electronic Health Records, deep learning risk prediction, electronic health record, neural nets]
Clustering by Shift
2017 IEEE International Conference on Data Mining
None
2017
In order to yield a more balanced partitioning, we investigate the use of additive regularizations for the Min Cut cost function, instead of normalization. In particular, we study the case where the regularization term is the sum of the squared size of the clusters, which then leads to shifting (adaptively) the pairwise similarities. We study the connection of such a model with Correlation Clustering and then propose an efficient local search optimization algorithm to solve the new clustering problem. Finally, we demonstrate the superior performance of our method by extensive experiments on different datasets.
[Shift, Adaptation models, Local search, Correlation, Laplace equations, Correlation Clustering, Europe, Search problems, efficient local search optimization algorithm, Clustering, additive regularizations, Min Cut cost function, pattern clustering, balanced partitioning, clustering problem, Efficiency, Cost function, Regularization, search problems]
Efficient Computation of Pairwise Minimax Distance Measures
2017 IEEE International Conference on Data Mining
None
2017
We study efficient computation of Minimax distances measures, which enable to capture the correct structures via taking the transitive relations into account. We analyze in detail two settings, the dense graphs and the sparse graphs. In particular, we show that an adapted variant of the Kruskal's algorithm is the most efficient approach for computing pairwise Minimax distances. However, for dense graphs we require a preprocessing step based on the Prim's algorithm, in order to reduce the set of candidate edges to be investigated. For each case, we study the correctness, efficiency and computational optimality of our approach. We perform numerical experiments on several datasets to validate the superior performance of our methods.
[Measurement, Heuristic algorithms, graph theory, computational optimality, dense graphs, set theory, minimax techniques, candidate edges, Runtime, correct structures, Efficiency, adapted variant, Distance measure, efficiency, Sparse Graphs, efficient computation, Europe, sparse graphs, Data structures, Dense graphs, Indexes, pairwise Minimax distance measures, transitive relations, Minimax distances, matrix algebra, Prim's algorithm, Vegetation, Kruskal's algorithm, pairwise Minimax distances]
Warehouse Site Selection for Online Retailers in Inter-Connected Warehouse Networks
2017 IEEE International Conference on Data Mining
None
2017
Supply chain management aims at delivering goods in the shortest time at the lowest possible price while ensuring the best possible quality and is now vital to the success of the online retail business. Executing effective warehouse site selection has been one of the key challenges in the development of a successful supply chain system. While some effective strategies for warehouse site selection have been identified by the domain experts based on their experiences, the emergence of new ways of collecting fine-grained supply chain data has enabled a new paradigm for warehouse site selection. Indeed, in this paper, we provide a data-smart approach for addressing the connected capacitated warehouse location problem (CCWL), which searches for the minimum total transportation cost of the warehouse network including supplier-warehouses shipping cost, warehouse-customer delivering cost and the cost of warehouse-warehouse inter-transportation. Specifically, we first design a sales distribution prediction model and evaluate the importance of customer logistic service utilities on online market sales demand for online retailers. Then, we propose the E&amp;M algorithm to optimize warehouse locations continuously with much less computation cost. Moreover, the computation cost is further reduced through delivery demand based Hierarchical Clustering which reduces the problem size by grouping delivering cities with close locations. Finally, we validate the proposed method on real-world e-Commerce supply chain data and the selection effect of new warehouses is evaluated in terms of sales improvement with faster delivery and more effective inventory management.
[warehouse location optimization, supply chains, Supply chains, Transportation, warehouse automation, logistics, E-commerce, warehousing, Site Selection, warehouse site selection, warehouse-warehouse inter-transportation, Training, optimisation, inter-connected warehouse network, Clustering algorithms, goods distribution, e-Commerce supply chain data, supplier-warehouses shipping cost, online market sales demand, Mathematical model, retailing, online retail business, electronic commerce, cost reduction, minimum total transportation cost, customer logistic service utility, warehouse-customer delivering cost, Clustering, transportation, Demand Prediction, supply and demand, supply chain management, goods delivery, E and M algorithm, goods quality, demand based hierarchical clustering, inventory management, Manganese, connected capacitated warehouse location problem]
Learning Multiple Similarities of Users and Items in Recommender Systems
2017 IEEE International Conference on Data Mining
None
2017
In addition to the sparse user-item (U-I) matrix, an increasing number of current recommender systems seek to improve performance by exploiting extra heterogeneous data sources (e.g., online social networks). Such rich side sources can provide very useful information about users' personal behaviors and items' properties, therefore can significantly benefit recommender systems. Most existing work can only incorporate a single side source of users or items. In many real-life applications, however, there exist multiple side sources for both users and items, which may be constructed from multiple domains. To incorporate multiple sources, we propose a flexible and robust algorithm called MSUI (Learning Multiple Similarities of Users and Items). The key idea of MSUI is to simultaneously capture the associations between latent factors in the U-I matrix and multiple side sources of users and items through joint nonnegative matrix factorization. MSUI has several advantages over existing methods. First, it seamlessly integrates information from the U-I matrix and information from multiple side sources. Second, by incorporating multiple sources that are usually in complementary, it's more robust to noise contained in each source. Finally, it provides a unified framework that can be easily extended to incorporate additional data sources.
[Drugs, sparse user-item, Recommender Systems, Multi-View Learning, Social network services, U-I matrix, extra heterogeneous data sources, online social networks, Linear programming, joint nonnegative matrix factorization, matrix decomposition, Sparse matrices, Matrix Factorization, Diseases, multiple side sources, recommender systems, Collaboration, item properties, social networking (online), multiple domains, Recommender systems, MSUI, user personal behavior]
Learning to Fuse Music Genres with Generative Adversarial Dual Learning
2017 IEEE International Conference on Data Mining
None
2017
FusionGAN is a novel genre fusion framework for music generation that integrates the strengths of generative adversarial networks and dual learning. In particular, the proposed method offers a dual learning extension that can effectively integrate the styles of the given domains. To efficiently quantify the difference among diverse domains and avoid the vanishing gradient issue, FusionGAN provides a Wasserstein based metric to approximate the distance between the target domain and the existing domains. Adopting the Wasserstein distance, a new domain is created by combining the patterns of the existing domains using adversarial learning. Experimental results on public music datasets demonstrated that our approach could effectively merge two genres.
[generative adversarial networks, adversarial learning, target domain, Linear programming, Generators, Gallium nitride, Creativity, Convergence, music genres, music, Wasserstein based metric, Music, Machine learning, music generation, public music datasets, genre fusion framework, FusionGAN, Wasserstein distance, vanishing gradient issue, learning (artificial intelligence), generative adversarial dual learning, dual learning extension]
Data Prefetching for Large Tiered Storage Systems
2017 IEEE International Conference on Data Mining
None
2017
In multi-tier storage systems with large amounts of data, most of the data is stored on inexpensive slower tiers such as cloud or tape to achieve cost savings. This also implies that retrieving the data from the slower storage tiers incurs high latency. Therefore, it would be beneficial to proactively prefetch data from slower tiers to faster tiers by predicting future data accesses. State-of-the-art access prediction methods typically record access history of individual files, data objects, or data segments. However, in systems with large amounts of infrequently accessed (or cold) data, file-level access history is often unavailable for much of the data due to the low frequency of access. In this paper, we extract information from file metadata to predict file accesses in a storage system. The proposed method relies on the hypothesis that users and applications access data stored in the system in a given context and that the context and, therefore, the set of files that are likely to be accessed can be identified by detecting access patterns in file metadata. As an application, we consider the LOFAR radio telescope's long term archive, where the access patterns are learned based on a rich set of metadata, and these patterns are then used to make predictions as to likely future accesses by the astronomers.
[storage system, LOFAR, Metadata, Predictive models, data prefetching, History, Data mining, multitier storage systems, access prediction methods, caching, storage management, file metadata, file-level access history, data accesses, access prediction, meta data, Prefetching, data objects, data segments, information retrieval, storage tiers, access patterns, archive, machine learning, information extraction, data handling, Large Tiered Storage Systems]
Audio-Visual Sentiment Analysis for Learning Emotional Arcs in Movies
2017 IEEE International Conference on Data Mining
None
2017
Stories can have tremendous power - not only useful for entertainment, they can activate our interests and mobilize our actions. The degree to which a story resonates with its audience may be in part reflected in the emotional journey it takes the audience upon. In this paper, we use machine learning methods to construct emotional arcs in movies, calculate families of arcs, and demonstrate the ability for certain arcs to predict audience engagement. The system is applied to Hollywood films and high quality shorts found on the web. We begin by using deep convolutional neural networks for audio and visual sentiment analysis. These models are trained on both new and existing large-scale datasets, after which they can be used to compute separate audio and visual emotional arcs. We then crowdsource annotations for 30-second video clips extracted from highs and lows in the arcs in order to assess the micro-level precision of the system, with precision measured in terms of agreement in polarity between the system's predictions and annotators' ratings. These annotations are also used to combine the audio and visual predictions. Next, we look at macro-level characterizations of movies by investigating whether there exist 'universal shapes' of emotional arcs. In particular, we develop a clustering approach to discover distinct classes of emotional arcs. Finally, we show on a sample corpus of short web videos that certain emotional arcs are statistically significant predictors of the number of comments a video receives. These results suggest that the emotional arcs learned by our approach successfully represent macroscopic aspects of a video story that drive audience engagement. Such machine understanding could be used to predict audience reactions to video stories, ultimately improving our ability as storytellers to communicate with each other.
[Visualization, Sentiment analysis, Shape, visual predictions, Heuristic algorithms, entertainment, video story, audience engagement, audio sentiment analysis, video, emotional arcs, emotion recognition, audio-visual sentiment analysis, movies, visual sentiment analysis, Motion pictures, audio predictions, learning (artificial intelligence), video signal processing, emotions, stories, Media, multimodal, audio arcs, visual emotional arcs, Feature extraction, neural nets]
Robust Estimation of Gaussian Copula Causal Structure from Mixed Data with Missing Values
2017 IEEE International Conference on Data Mining
None
2017
We consider the problem of causal structure learning from data with missing values, assumed to be drawn from a Gaussian copula model. First, we extend the 'Rank PC' algorithm, designed for Gaussian copula models with purely continuous data (so-called nonparanormal models), to incomplete data by applying rank correlation to pairwise complete observations and replacing the sample size with an effective sample size in the conditional independence tests to account for the information loss from missing values. The resulting approach works when the data are missing completely at random (MCAR). Then, we propose a Gibbs sampling procedure to draw correlation matrix samples from mixed data under missingness at random (MAR). These samples are translated into an average correlation matrix, and an effective sample size, resulting in the 'Copula PC' algorithm for incomplete data. Simulation study shows that: 1) the usage of the effective sample size significantly improves the performance of 'Rank PC' and 'Copula PC'; 2) 'Copula PC' estimates a more accurate correlation matrix and causal structure than 'Rank PC' under MCAR and, even more so, under MAR. Also, we illustrate our methods on a real-world data set about gene expression.
[Algorithm design and analysis, Correlation, estimation theory, average correlation matrix, data mining, Copula PC algorithm, Rank PC algorithm, genetics, missingness at random, Mathematical model, belief networks, learning (artificial intelligence), missing completely at random, rank correlation, correlation matrix samples, missing values, mixed data, sampling methods, Gaussian Copula causal structure, MCAR, Computational modeling, Copula PC estimation, nonparanormal models, Gibbs sampling, Standards, structure learning, matrix algebra, robust estimation, Markov processes, Data models, Bayes methods, Gaussian copula model, incomplete data, correlation methods, MAR]
Network Clocks: Detecting the Temporal Scale of Information Diffusion
2017 IEEE International Conference on Data Mining
None
2017
Information diffusion models typically assume a discrete timeline in which an information token spreads in the network. Since users in real-world networks vary significantly in their intensity and periods of activity, our objective in this work is to answer: How to determine a temporal scale that best agrees with the observed information propagation within a network? A key limitation of existing approaches is that they aggregate the timeline into fixed-size windows, which may not fit all network nodes' activity periods. We propose the notion of a heterogeneous network clock: a mapping of events to discrete timestamps that best explains their occurrence according to a given cascade propagation model. We focus on the widely-adopted independent cascade (IC) model and formalize the optimal clock as the one that maximizes the likelihood of all observed cascades. The single optimal clock (OC) problem can be solved exactly in polynomial time. However, we prove that learning multiple optimal clocks (kOC), corresponding to temporal patterns of groups of network nodes, is NP-hard. We propose scalable solutions that run in almost linear time in the total number of cascade activations and discuss approximation guarantees for each variant. Our algorithms and their detected clocks enable improved cascade size classification (up to 8% F1 lift) and improved missing cascade data inference (0.15 better recall). We also demonstrate that the network clocks exhibit consistency within the type of content diffusing in the network and are robust with respect to the propagation probability parameters of the IC model.
[cascade activations, NP-hard, independent cascade model, data mining, cascade size classification, fixed-size windows, learning, Complexity theory, network nodes, real-world networks, network clocks, heterogeneous network clock, cascades, cascade data inference, learning (artificial intelligence), Information diffusion, information propagation, pattern classification, Social network services, IC model, information token, information diffusion models, Aggregates, detected clocks, social network analysis, single optimal clock problem, Approximation algorithms, cascade propagation model, Integrated circuit modeling, Clocks, computational complexity]
Local Community Detection in Dynamic Networks
2017 IEEE International Conference on Data Mining
None
2017
Given a time-evolving network, how can we detect communities over periods of high internal and low external interactions? To address this question we generalize traditional local community detection in graphs to the setting of dynamic networks. Adopting existing static-network approaches in an &#x201C;aggregated&#x201D; graph of all temporal interactions is not appropriate for the problem as dynamic communities may be short-lived and thus lost when mixing interactions over long periods. Hence, dynamic community mining requires the detection of both the community nodes and an optimal time interval in which they are actively interacting. We propose a filter-and-verify framework for dynamic community detection. To scale to long intervals of graph evolution, we employ novel spectral bounds for dynamic community conductance and employ them to filter suboptimal periods in near-linear time. We also design a time-and-graph-aware locality sensitive hashing family to effectively spot promising community cores. Our method PHASR discovers communities of consistently higher quality (2 to 67 times better) than those of baselines. At the same time, our bounds allow for pruning between 55% and 95% of the search space, resulting in significant savings in running time compared to exhaustive alternatives for even modest time intervals of graph evolution.
[graph theory, data mining, network theory (graphs), community cores, dynamic community detection, conductance, high internal interactions, dynamic graphs, graphs, modest time intervals, dynamic networks, suboptimal periods, Eigenvalues and eigenfunctions, time-evolving network, dynamic community mining, community detection, aggregated graph, filter framework, Laplace equations, spectral methods, Image edge detection, local community detection, dynamic communities, optimal time interval, method PHASR discovers communities, low external interactions, dynamic community conductance, Computer science, near-linear time, Sensitivity, static-network, Aggregates, community nodes, time-aware locality sensitive hashing, temporal interactions, file organisation, social networking (online), graph evolution, Nickel, graph-aware locality sensitive hashing, verify framework]
Online Nearest Neighbor Search in Binary Space
2017 IEEE International Conference on Data Mining
None
2017
We revisit the K Nearest Neighbors (KNN) problem in large binary datasets which is of major importance in several applied areas. The goal is to find the K nearest items in a dataset to a query point where both the query and the items lie in the Hamming cube. The problem is addressed in its online setting, that is, data items are inserted sequentially into the dataset. To accommodate efficient similarity search and fast insertion of new items, we propose a data structure that partitions the feature space based on the Hamming weights of the binary codes and their substrings. Empirical evaluations on a large-scale dataset show significant speedup over the linear scan baseline.
[Heuristic algorithms, data structure, Search problems, KNN, online setting, online nearest neighbor search, query processing, feature extraction, Binary codes, data structures, Tree search, search problems, binary space, Nearest neighbor search, pattern classification, binary codes, Hamming distance, partitions the feature space, K-nearest neighbor problem, binary datasets, Hamming cube, Data structures, Hamming weights, query point, Hamming weight, Nearest neighbor searches, Similarity search, data items, large-scale dataset, K nearest items]
Finding Streams in Knowledge Graphs to Support Fact Checking
2017 IEEE International Conference on Data Mining
None
2017
The volume of information generated online makes it impossible to manually fact-check all claims. Computational approaches for fact checking may be the key to help mitigate the risks of massive misinformation spread. Such approaches can be designed to not only be scalable and effective at assessing veracity of dubious claims, but also to boost a human fact checker's productivity by surfacing relevant facts and patterns to aid their analysis. We present a novel, unsupervised network-flow based approach to determine the truthfulness of a statement of fact expressed in the form of a triple. We view a knowledge graph of background information about real-world entities as a flow network, and show that computational fact checking then amounts to finding a "knowledge stream" connecting the subject and object of the triple. Evaluation on a range of real-world and hand-crafted datasets of facts reveals that this network-flow model can be very effective in discerning true statements from false ones, outperforming existing algorithms on many test cases. Moreover, the model is expressive in its ability to automatically discover several useful patterns and surface relevant facts that may help a human fact checker.
[Knowledge engineering, Scalability, public domain software, graph theory, computational fact, data mining, Companies, knowledge stream, flow network, Fact Checking, surface relevant facts, Semantics, human fact checker, useful patterns, massive misinformation spread, Computational modeling, Minimum Cost Maximum Flow, information retrieval, fact checking, knowledge graph, network-flow model, background information, unsupervised network-flow, stream finding, Knowledge Stream, Network Flow, Dogs, Writing, Internet, Knowledge Graph]
Generating Synthetic Time Series to Augment Sparse Datasets
2017 IEEE International Conference on Data Mining
None
2017
In machine learning, data augmentation is the process of creating synthetic examples in order to augment a dataset used to learn a model. One motivation for data augmentation is to reduce the variance of a classifier, thereby reducing error. In this paper, we propose new data augmentation techniques specifically designed for time series classification, where the space in which they are embedded is induced by Dynamic Time Warping (DTW). The main idea of our approach is to average a set of time series and use the average time series as a new synthetic example. The proposed methods rely on an extension of DTW Barycentric Averaging (DBA), the averaging technique that is specifically developed for DTW. In this paper, we extend DBA to be able to calculate a weighted average of time series under DTW. In this case, instead of each time series contributing equally to the final average, some can contribute more than others. This extension allows us to generate an infinite number of new examples from any set of given time series. To this end, we propose three methods that choose the weights associated to the time series of the dataset. We carry out experiments on the 85 datasets of the UCR archive and demonstrate that our method is particularly useful when the number of available examples is limited (e.g. 2 to 6 examples per class) using a 1-NN DTW classifier. Furthermore, we show that augmenting full datasets is beneficial in most cases, as we observed an increase of accuracy on 56 datasets, no effect on 7 and a slight decrease on only 22.
[pattern classification, dynamic time warping, DBA, Heuristic algorithms, Conferences, data augmentation, Time series analysis, DTW, time series, augment sparse datasets, Data mining, machine learning, synthetic Time series generation, Training, Manifolds, DTW Barycentric Averaging, average time series, Data models, Dynamic Time Warping, learning (artificial intelligence), statistical analysis, time series classification]
Efficient and Invariant Convolutional Neural Networks for Dense Prediction
2017 IEEE International Conference on Data Mining
None
2017
Convolutional neural networks have shown great success on feature extraction from raw input data such as images. Although convolutional neural networks are invariant to translations on the inputs, they are not invariant to other transformations, including rotation and flip. Recent attempts have been made to incorporate more invariance in image recognition applications, but they are not applicable to dense prediction tasks, such as image segmentation. In this paper, we propose a set of methods based on kernel rotation and flip to enable rotation and flip invariance in convolutional neural networks. The kernel rotation can be achieved on kernels of 3 &#x00D7; 3, while kernel flip can be applied on kernels of any size. By rotating in eight or four angles, the convolutional layers could produce the corresponding number of feature maps based on eight or four different kernels. By using flip, the convolution layer can produce three feature maps. By combining produced feature maps using maxout, the resource requirement could be significantly reduced while still retain the invariance properties. Experimental results demonstrate that the proposed methods can achieve various invariance at reasonable resource requirements in terms of both memory and time.
[kernel rotation, raw input data, image classification, inference mechanisms, image recognition applications, Training, Image segmentation, Convolution, feature extraction, image segmentation, dense prediction tasks, feature maps, invariance properties, convolutional neural networks, learning (artificial intelligence), flip invariance, resource requirement, Kernel, convolutional layers, feedforward neural nets]
Spectral Lens: Explainable Diagnostics, Tools and Discoveries in Directed, Weighted Graphs
2017 IEEE International Conference on Data Mining
None
2017
How can we quickly explain large-scale directed and weighted graphs? We present Spectral Lens (SL) to analyze a variety of real-world networks with both negative and positive edge weights, like DBLP relationships, email communications and Bitcoin trust votings. SL offers value on three levels: (a) Diagnostics: Spectral Lens combines spectral properties from Singular Value Decomposition to create an SL-Dictionary (SLD) to enhance understanding of any directed, weighted graph. (b) Tools: the SL-Algorithm (SLA) automatically extracts the top groups of nodes with similar connectivity, finds groups of shared connectivity and detects suspicious behaviour in a graph. (c) Discoveries: Experiments on several real-world networks illustrate the effectiveness of SLA. Observations from synthetic and real-world networks reveal relations between spectral and graph properties. We show that SLA is highly scalable and linear on the size of a graph. Analyzing a graph with over 2 million edges takes less than 5 minutes. Overall, SL provides an easy-to-use tool for practitioners to explain a weighted and directed graph quickly, to understand its connectivity and identify regular and anomalous behaviors.
[weighted graph, time 5.0 min, Superluminescent diodes, SLA, SLD, negative edge weights, real-world networks, weighted graphs, directed graph, spectral properties, SL-dictionary, singular value decomposition, Singular value decomposition, Graph Mining, positive edge weights, Image edge detection, Tools, Matrix decomposition, Bitcoin trust votings, SL-algorithm, Bridges, Singular Value Decomposition, Spectral Lens, directed graphs, large-scale directed, Lenses, dictionaries]
High-Dimensional Dependency Structure Learning for Physical Processes
2017 IEEE International Conference on Data Mining
None
2017
In this paper, we consider the use of structure learning methods for probabilistic graphical models to identify statistical dependencies in high-dimensional physical processes. Such processes are often synthetically characterized using PDEs (partial differential equations) and are observed in a variety of natural phenomena. In this paper, we present ACLIME-ADMM, an efficient two-step algorithm for adaptive structure learning, which decides a suitable edge specific threshold in a data-driven statistically rigorous manner. Both steps of our algorithm use (inexact) ADMM to solve suitable linear programs, and all iterations can be done in closed form in an efficient block parallel manner. We compare ACLIME-ADMM with baselines on both synthetic data simulated by PDEs that model advection-diffusion processes, and real data of daily global geopotential heights to study information flow in the atmosphere. ACLIME-ADMM is shown to be efficient, stable, and competitive, usually better than the baselines especially on difficult problems. On real data, ACLIME-ADMM recovers the underlying structure of global atmospheric circulation, including switches in wind directions at the equator and tropics entirely from the data.
[iterative methods, adaptive structure learning, Predictive models, edge specific threshold, linear programming, Electronic mail, high-dimensional physical processes, Optimization, probabilistic graphical models, two-step algorithm, learning (artificial intelligence), natural phenomena, advection-diffusion processes, linear programs, data analysis, Atmospheric modeling, Computational modeling, probability, PDEs, structure learning methods, high-dimensional dependency structure learning, structure learning, PC stable, geoscience, statistical dependencies, block parallel manner, data-driven statistically rigorous manner, Data models, partial differential equations, statistical analysis, high-dimensional physical process, ACLIME-ADMM]
Learning with Inadequate and Incorrect Supervision
2017 IEEE International Conference on Data Mining
None
2017
Practically, we are often in the dilemma that the labeled data at hand are inadequate to train a reliable classifier, and more seriously, some of these labeled data may be mistakenly labeled due to the various human factors. Therefore, this paper proposes a novel semi-supervised learning paradigm that can handle both label insufficiency and label inaccuracy. To address label insufficiency, we use a graph to bridge the data points so that the label information can be propagated from the scarce labeled examples to unlabeled examples along the graph edges. To address label inaccuracy, Graph Trend Filtering (GTF) and Smooth Eigenbase Pursuit (SEP) are adopted to filter out the initial noisy labels. GTF penalizes the l_0 norm of label difference between connected examples in the graph and exhibits better local adaptivity than the traditional l_2 norm-based Laplacian smoother. SEP reconstructs the correct labels by emphasizing the leading eigenvectors of Laplacian matrix associated with small eigenvalues, as these eigenvectors reflect real label smoothness and carry rich class separation cues. We term our algorithm as "Semi-supervised learning under Inadequate and Incorrect Supervision" (SIIS). Thorough experimental results on image classification, text categorization, and speech recognition demonstrate that our SIIS is effective in label error correction, leading to superior performance to the state-of-the-art methods in the presence of label noise and label scarcity.
[image classification, semisupervised learning, graph theory, Incorrect Supervision, smooth eigenbase pursuit, text categorization, eigenvalues and eigenfunctions, label inaccuracy, graph edges, speech recognition, Market research, unlabeled examples, Eigenvalues and eigenfunctions, Laplacian matrix, learning (artificial intelligence), label error correction, Graph Trend Filtering, Laplace equations, semisupervised learning paradigm, data points, Noise measurement, label insufficiency, correct labels, matrix algebra, scarce labeled examples, label smoothness, eigenvectors, Semisupervised learning, label noise, Nickel, label difference, semi-supervised learning, graph trend filtering, Reliability, initial noisy labels, label information, label scarcity]
Market Basket Prediction Using User-Centric Temporal Annotated Recurring Sequences
2017 IEEE International Conference on Data Mining
None
2017
Nowadays, a hot challenge for supermarket chains is to offer personalized services to their customers. Market basket prediction, i.e., supplying the customer a shopping list for the next purchase according to her current needs, is one of these services. Current approaches are not capable of capturing at the same time the different factors influencing the customer's decision process: co-occurrence, sequentuality, periodicity and recurrency of the purchased items. To this aim, we define a pattern named Temporal Annotated Recurring Sequence (TARS). We define the method to extract TARS and develop a predictor for next basket named TBP (TARS Based Predictor) that, on top of TARS, is able to understand the level of the customer's stocks and recommend the set of most necessary items. A deep experimentation shows that TARS can explain the customers' purchase behavior, and that TBP outperforms the state-of-the-art competitors.
[Adaptation models, supermarket chains, Conferences, purchasing, data mining, Predictive models, consumer behaviour, History, Data mining, market basket prediction, TARS Based Predictor, shopping list, Itemsets, Next Basket Prediction, user-centric Temporal Annotated Recurring, Temporal Patterns, Temporal Sequences, customer purchase behavior, purchased items, personalized services, marketing data processing, Temporal Annotated Recurring Sequence, customer services, Markov processes, periodicity, Market Basket Analysis, customer stocks]
Tensor Based Relations Ranking for Multi-relational Collective Classification
2017 IEEE International Conference on Data Mining
None
2017
In this paper, we study relations ranking and object classification for multi-relational data where objects are interconnected by multiple relations. The relations among objects should be exploited for achieving a good classification. While most existing approaches exploit either by directly counting the number of connections among objects or by learning the weight of each relation from labeled data only. In this paper, we propose an algorithm, TensorRRCC, which is able to determine the ranking of relations and the labels of objects simultaneously. Our basic idea is that highly ranked relations within a class should play more important roles in object classification, and class membership information is important for determining a ranking quality over the relations w.r.t. a specific learning task. TensorRRCC implements the idea by modeling a Markov chain on transition probability graphs from connection and feature information with both labeled and unlabeled objects and propagates the ranking scores of relations and relevant classes of objects. An iterative progress is proposed to solve a set of tensor equations to obtain the stationary distribution of relations and objects. We compared our algorithm with current collective classification algorithms on two real-world data sets and the experimental results show the superiority of our method.
[Correlation, multiple relations, TensorRRCC, graph theory, object classification, data mining, Probability distribution, tensors, collective classification algorithms, Data mining, highly ranked relations, Mathematical model, learning (artificial intelligence), unlabeled objects, class membership information, pattern classification, relations ranking, probability, multirelational collective classification, classification, tensor, Tensile stress, ranking quality, Markov processes, multirelational data, multi-relational data]
Sub-Gibbs Sampling: A New Strategy for Inferring LDA
2017 IEEE International Conference on Data Mining
None
2017
Latent Dirichlet Allocation (LDA) has been widely used in text mining to discover topics from documents. One major approach to learn LDA is Gibbs sampling. The basic Collapsed Gibbs Sampling (CGS) algorithm requires O(NZ) computations to learn an LDA model with Z topics from a corpus containing N tokens. Existing approaches that improve the complexity of CGS focus on reducing the factor Z. In this work, we propose a novel and general Sub-Gibbs Sampling (SGS) strategy to improve the Gibbs-Sampling computation by reducing the sample space. This new strategy targets at reducing the factor N by sampling only a subset of the whole corpus. The design of the SGS strategy is based on two properties that we observe: (i) topic distributions of tokens are skewed and (ii) a subset of documents can approximately represent the semantics of the whole corpus. We prove that the SGS strategy can achieve comparable effectiveness (with bounded errors) and significantly reduce the complexity of existing Gibbs sampling algorithms. Extensive experiments on large real-world data sets show that the proposed SGS strategy is much faster than several state-of-the-art fast Gibbs sampling algorithms and the proposed SGS strategy can learn comparable LDA models as other Gibbs sampling algorithms.
[Algorithm design and analysis, text analysis, sampling methods, Gibbs Sampling, Computational modeling, data mining, SGS strategy, CGS focus, Topic Models, Complexity theory, Partitioning algorithms, inference mechanisms, Latent Dirichlet Allocation, Gibbs-sampling computation, Semantics, LDA model, Sub-sampling, Approximation algorithms, Bayes methods, Resource management, statistical analysis]
Behind Distribution Shift: Mining Driving Forces of Changes and Causal Arrows
2017 IEEE International Conference on Data Mining
None
2017
We address two important issues in causal discovery from nonstationary or heterogeneous data, where parameters associated with a causal structure may change over time or across data sets. First, we investigate how to efficiently estimate the "driving force" of the nonstationarity of a causal mechanism. That is, given a causal mechanism that varies over time or across data sets and whose qualitative structure is known, we aim to extract from data a low-dimensional and interpretable representation of the main components of the changes. For this purpose we develop a novel kernel embedding of nonstationary conditional distributions that does not rely on sliding windows. Second, the embedding also leads to a measure of dependence between the changes of causal modules that can be used to determine the directions of many causal arrows. We demonstrate the power of our methods with experiments on both synthetic and real data.
[driving force, Force, data mining, Nonstationary driving force, distribution shift, Data mining, Data distribution shift, heterogeneous data, low-dimensional representation, causal arrows, data sets, Skeleton, nonstationary conditional distributions, Kernel, Causal discovery, nonstationary data, causal structure, interpretable representation, Estimation, Indexes, Matrix decomposition, causality, causal discovery, driving force mining, qualitative structure, Kernel mean embedding]
A Self-Paced Category-Aware Approach for Unsupervised Adaptation Networks
2017 IEEE International Conference on Data Mining
None
2017
The success of deep neural networks usually relies on a large number of labeled training samples, which unfortunately are not easy to obtain in practice. Unsupervised domain adaptation focuses on the problem where there is no labeled data in the target domain. In this paper, we propose a novel deep unsupervised domain adaptation method that learns transferable features. Different from most existing methods, it attempts to learn a better domain-invariant feature representation by performing a category-wise adaptation to match the conditional distributions of samples with respect to each category. A self-paced learning strategy is used to bring the awareness of label information gradually, which makes the category-wise adaptation feasible even if the labels are unavailable in target domain. Then, we give detailed theoretical analysis to explain how the better performance is obtained. The experimental results show that our method outperforms the current state of the arts on standard domain adaptation datasets.
[Adaptation models, Transfer Learning, Unsupervised Domain Adaptation, unsupervised adaptation networks, domain-invariant feature representation, deep unsupervised domain adaptation method, self-paced category-aware approach, Optimization, transferable features, Training, category-wise adaptation, feature extraction, deep neural networks, labeled training samples, Computer vision, standard domain adaptation datasets, category-aware approach, Standards, unsupervised learning, self-paced learning strategy, Upper bound, Feature extraction, neural nets, label information, conditional distributions]
CRAD: Clustering with Robust Autocuts and Depth
2017 IEEE International Conference on Data Mining
None
2017
We develop a new density-based clustering algorithm named CRAD which is based on a new neighbor searching function with a robust data depth as the dissimilarity measure. Our experiments prove that the new CRAD is highly competitive at detecting clusters with varying densities, compared with the existing algorithms such as DBSCAN, OPTICS and DBCA. Furthermore, a new effective parameter selection procedure is developed to select the optimal underlying parameter in the real-world clustering, when the ground truth is unknown. Lastly, we suggest a new clustering framework that extends CRAD from spatial data clustering to time series clustering without a-priori knowledge of the true number of clusters. The performance of CRAD is evaluated through extensive experimental studies.
[Density measurement, robust data depth, data depth, dissimilarity measure, Time series analysis, Optics, time series, Spatial databases, Clustering with Robust Autocuts and Depth, CRAD, space-time processes, parameter selection, density-based clustering algorithm, neighbor searching function, real-world clustering, pattern clustering, Toy manufacturing industry, Clustering algorithms, Robustness, spatial data clustering, clustering, time series clustering, feature selection, search problems]
SLANT+: A Nonlinear Model for Opinion Dynamics in Social Networks
2017 IEEE International Conference on Data Mining
None
2017
Online Social Networks (OSNs) have emerged as a global media for forming and shaping opinions on a broad spectrum of topics like politics, e-commerce, sports, etc. So, research on understanding and predicting opinion dynamics in OSNs, especially using a tractable linear model, has abound in literature. However, these linear models are too simple to uncover the actual complex dynamics of opinion flow in social networks. In this paper, we propose SLANT+, a novel nonlinear generative model for opinion dynamics, by extending our earlier linear opinion model SLANT [7]. To design this model, we rely on a network-guided recurrent neural network architecture which learns a proper temporal representation of the messages as well as the underlying network. Furthermore, we probe various signals from the real life datasets and offer a conceptually interpretable nonlinear function that not only provides concrete clues of the opinion exchange process, but also captures the coupled dynamics of message timings and opinion flow. As a result, with five real-life datasets crawled from Twitter, our proposal gives significant accuracy boost over six state-of-the-art baselines.
[Recurrent neural networks, Social network services, sentiment analysis, data mining, recurrent neural nets, Predictive models, recurrent neural network architecture, opinion exchange process, Social networks, Online Social Networks, History, Proposals, OSNs, nonlinear function, SLANT+, linear opinion model, Computer architecture, social networking (online), opinion dynamics, Temporal representations, Timing, nonlinear generative model, Opinion dynamics, opinion flow]
Effective Large-Scale Online Influence Maximization
2017 IEEE International Conference on Data Mining
None
2017
In this paper, we study a highly generic version of influence maximization (IM), one of optimizing influence campaigns by sequentially selecting "spread seeds" from a set of candidates, a small subset of the node population, under the hypothesis that, in a given campaign, previously activated nodes remain "persistently" active throughout and thus do not yield further rewards. We call this problem online influence maximization with persistence. We introduce an estimator on the candidates' missing mass - the expected number of nodes that can still be reached from a given seed candidate - and justify its strength to rapidly estimate the desired value. We then describe a novel algorithm, GT-UCB, relying on upper confidence bounds on the missing mass. We show that our approach leads to high-quality spreads on classic IM datasets, even though it makes almost no assumptions on the diffusion medium. Importantly, it is orders of magnitude faster than state-of-the-art IM methods.
[missing mass, online social networks, influence campaigns, Twitter, GT-UCB, set theory, multi-armed bandits, high-quality spreads, optimisation, Sociology, spread seeds, node population, Advertising, information diffusion, classic IM datasets, Influence maximization, information dissemination, activated nodes, seed candidate, Topology, Statistics, social networking (online), large-scale online influence maximization, Integrated circuit modeling, online learning]
Identifying Media Bias by Analyzing Reported Speech
2017 IEEE International Conference on Data Mining
None
2017
Media analysis can reveal interesting patterns in the way newspapers report the news and how these patterns evolve over time. One example pattern is the quoting choices that media make, which could be used as bias indicators. Media slant can be expressed both with the choice of reporting an event, e.g. a person's statement, but also with the words used to describe the event. Thus, automatic discovery of systematic quoting patterns in the news could illustrate to the readers the media' beliefs, such as political preferences. In this paper, we aim to discover political media bias by demonstrating systematic patterns of reporting speech in two major British newspapers. To this end, we analyze news articles from 2000 to 2015. By taking into account different kinds of bias, such as selection, coverage and framing bias, we show that the quoting patterns of newspapers are predictable.
[politics, text analysis, political preferences, political bias, data mining, identifying media bias, systematic patterns, Data mining, reported speech, British newspapers, Systematics, news articles, quoting choices, person statement, systematic quoting patterns, publishing, coverage, Blogs, news analysis, Media, reported speech analysis, media bias, bias indicators, classification, automatic discovery, political media bias, Speech, Feature extraction, media analysis, framing bias, media slant]
Fast Compressive Spectral Clustering
2017 IEEE International Conference on Data Mining
None
2017
Compressive spectral clustering (CSC) efficiently leverages graph filter and random sampling techniques to speed up clustering process. However, we find that CSC algorithm suffers from two main problems: i) The direct use of the dichotomy and eigencount techniques for estimating laplacian matrix's k-th eigenvalue is expensive. ii) The computation of polynomial approximation repeats in each iteration for every cluster in the interpolation process, which occupies most of the computation time of CSC. To address these problems, we propose a new approach called FCSC for fast compressive spectral clustering. FCSC addresses the first problem by assuming that the eigenvalues approximately satisfy local uniform distribution, and addresses the second problem by recalculating the pairwise similarity between nodes with low-dimensional representation to reconstruct denoised laplacian matrix. The time complexity of reconstruction is linear with the number of non-zeros in laplacian matrix. As experimentally demonstrated on artificial and real-world datasets, our approach significantly reduces the computation time while preserving high clustering accuracy comparable to previous designs, verifying the effectiveness of FCSC.
[eigenvalues, high clustering accuracy, graph theory, CSC algorithm suffers, local uniform distribution, eigenvalues and eigenfunctions, computation time, polynomial approximation, graph filter, Laplacian matrix k-th eigenvalue estimation, Eigenvalues and eigenfunctions, denoised laplacian matrix, approximation theory, Laplace equations, interpolation process, Estimation, Matrix decomposition, FCSC, matrix algebra, Interpolation, interpolation, pattern clustering, fast compressive spectral clustering, random sampling techniques, polynomial approximation repeats, Acceleration, Time complexity, computational complexity]
A Broad Learning Approach for Context-Aware Mobile Application Recommendation
2017 IEEE International Conference on Data Mining
None
2017
With the rapid development of mobile apps, the availability of a large number of mobile apps in application stores brings challenges to locate appropriate apps for users. Providing accurate mobile app recommendation for users becomes an imperative task. Conventional approaches mainly focus on learning users' preferences and app features to predict the user-app ratings. However, most of them did not consider the interactions among the context information of apps. To address this issue, we propose a broad learning approach for Context-Aware app recommendation with Tensor Analysis (CATA). Specifically, we utilize a tensor-based framework to effectively integrate app category information and multi-view features on users and apps, respectively, to facilitate the performance of rating prediction. The multidimensional structure is employed to capture the hidden relationships among the app categories and the multiview features. We develop an efficient factorization method which applies Tucker decomposition to learn the full-order interactions among the app categories and features. Furthermore, we employ a group &#x2113;<sub>1</sub>-norm regularization to learn the group-wise feature importance of each view with respect to each app category. Experiments on a real-world mobile app dataset demonstrate the effectiveness of the proposed method.
[mobile apps, broad learning approach, Mobile communication, tensors, mobile computing, CATA, app category information, learning (artificial intelligence), context-aware mobile application recommendation, real-world mobile app dataset, Navigation, Buildings, learning users, multidimensional structure, Mobile applications, Computer science, Tensile stress, recommender systems, application stores, accurate mobile app recommendation, factorization method, user-app ratings, Context-Aware app recommendation with Tensor Analysis, Tucker decomposition, group &#x2113;1-norm regularization]
Recover Fine-Grained Spatial Data from Coarse Aggregation
2017 IEEE International Conference on Data Mining
None
2017
In this paper, we study a new type of spatial sparse recovery problem, that is to infer the fine-grained spatial distribution of certain density data in a region only based on the aggregate observations recorded for each of its subregions. One typical example of this spatial sparse recovery problem is to infer spatial distribution of cellphone activities based on aggregate mobile traffic volumes observed at sparsely scattered base stations. We propose a novel Constrained Spatial Smoothing (CSS) approach, which exploits the local continuity that exists in many types of spatial data to perform sparse recovery via finite-element methods, while enforcing the aggregated observation constraints through an innovative use of the ADMM algorithm. We also improve the approach to further utilize additional geographical attributes. Extensive evaluations based on a large dataset of phone call records and a demographical dataset from the city of Milan show that our approach significantly outperforms various state-of-the-art approaches, including Spatial Spline Regression (SSR).
[aggregated observation constraints, Spatial Sparse Recovery, fine-grained spatial distribution, data aggregation, coarse aggregation, mobile computing, optimisation, spatial data structures, aggregate mobile traffic volumes, Constrained Spatial Smoothing, finite-element methods, sparsely scattered base stations, Base stations, Smoothing methods, Spatial Spline Regression, data analysis, CSS approach, density data, Estimation, spatial sparse recovery problem, ADMM algorithm, Spatial databases, finite element analysis, inference mechanisms, Alternating Direction Method of Multipliers, Aggregates, Cellular phones, aggregate observations, fine-grained spatial data recovery, cellphone activities, statistical analysis, Splines (mathematics), constrained spatial smoothing approach]
Multi-view Graph Embedding with Hub Detection for Brain Network Analysis
2017 IEEE International Conference on Data Mining
None
2017
Multi-view graph embedding and hub detection have both become widely studied problems in the area of graph learning. Both graph embedding and hub detection relate to the node clustering structure of graphs. The multi-view graph embedding usually implies the node clustering structure of the graph based on the multiple views, while hubs are the boundary-spanning nodes across different node clusters in the graph and thus may potentially influence the clustering structure of the graph. However, none of the existing works considered joint learning the multi-view embeddings and the hubs from multi-view graph data. In this paper, we propose to incorporate the hub detection task into the multi-view graph embedding framework so that the two tasks could benefit from each other. Specifically, we propose an auto-weighted framework of Multi-view Graph Embedding with Hub Detection (MVGE-HD) for brain network analysis. The MVGE-HD framework learns a unified graph embedding across all the views while reducing the potential influence of the hubs on blurring the boundaries between node clusters in the graph, thus leading to a clear and discriminative node clustering structure for the graph. We apply MVGE-HD on two real multi-view brain network datasets (i.e., HIV and Bipolar). The experimental results demonstrate the superior performance of the proposed framework in brain network analysis for clinical investigation and application.
[Correlation, clinical application, boundary-spanning nodes, joint learning, graph theory, Linear programming, node clustering structure, MVGE-HD, brain, multiview brain network datasets, Diffusion tensor imaging, multiview graph data, Neuroscience, clinical investigation, multiview graph embedding, pattern clustering, hub detection, brain network analysis, graph learning, learning (artificial intelligence), medical computing, autoweighted framework]
BEEP: A Bayesian Perspective Early Stage Event Prediction Model for Online Social Networks
2017 IEEE International Conference on Data Mining
None
2017
In recent years, predicting future hot events in online social networks is becoming increasingly meaningful in marketing, advertisement, and recommendation systems to support companies' strategy making. Currently, most prediction models require long-term observations over the event or depend a lot on other features which are expensive to extract. However, at the early stage of an event, the temporal features of hot events and non-hot events are not distinctive yet. Besides, given the small amount of available data, high noise and complex network structure, those state-of-art models are unable to give an accurate prediction at the very early stage of an event. Hence, we propose two Bayesian perspective models to handle this dilemma. We first mathematically define the hot event prediction problem and introduce the general early stage event prediction framework, then model the five selected features into several continuous distributions, and present two Semi-Naive Bayes Classifier based prediction models, BEEP and SimBEEP, which is the simplified version of BEEP. Extensive experiments on real dataset have demonstrated that our model significantly outperforms the baseline methods.
[hot event prediction problem, pattern classification, Conferences, online social networks, seminaive Bayes classifier based prediction models, Data mining, BEEP, Bayesian perspective early stage event prediction model, temporal features, social networking (online), general early stage event prediction framework, Silicon, Bayes methods, Acceleration, belief networks, feature selection]
Automatic Classification of Music Genre Using Masked Conditional Neural Networks
2017 IEEE International Conference on Data Mining
None
2017
Neural network based architectures used for sound recognition are usually adapted from other application domains such as image recognition, which may not harness the time-frequency representation of a signal. The ConditionaL Neural Networks (CLNN) and its extension the Masked ConditionaL Neural Networks (MCLNN) are designed for multidimensional temporal signal recognition. The CLNN is trained over a window of frames to preserve the inter-frame relation, and the MCLNN enforces a systematic sparseness over the network's links that mimics a filterbank-like behavior. The masking operation induces the network to learn in frequency bands, which decreases the network susceptibility to frequency-shifts in time-frequency representations. Additionally, the mask allows an exploration of a range of feature combinations concurrently analogous to the manual handcrafting of the optimum collection of features for a recognition task. MCLNN have achieved competitive performance on the Ballroom music dataset compared to several hand-crafted attempts and outperformed models based on state-of-the-art Convolutional Neural Networks.
[Time-frequency analysis, masking operation, Image recognition, Music Information Retrieval, multidimensional temporal signal recognition, Conditional Neural Network, Spectrogram, DBN, music, Restricted Boltzmann Machine, recognition task, Deep Belief Net, convolutional neural networks, learning (artificial intelligence), RBM, Masked Conditional Neural Network, sound recognition, Conditional RBM, CRBM, MCLNN, signal representation, Biological neural networks, neural network based architectures, signal classification, audio signal processing, Hidden Markov models, CLNN, network susceptibility, masked conditional neural networks, MIR, Feature extraction, neural nets]
Theoretically and Empirically High Quality Estimation of Closeness Centrality
2017 IEEE International Conference on Data Mining
None
2017
In the field of network analysis, centrality values of graph nodes, which represents the importance of nodes, have been widely studied. In this paper, we focus on one of the most basic centrality measures: closeness centrality. Since the exact computation of closeness centrality for all nodes of a network is prohibitively costly for massive networks, algorithms for estimating closeness centrality have been studied. In previous works, theoretical bounds on relative error have been improved. However, for complex networks such as social networks, empirical estimation qualities have hardly been improved since the sampling-based algorithm was proposed. In this paper, we propose simple and highly scalable algorithms for estimating closeness centrality of undirected networks. Our algorithms have theoretically and empirically better estimation quality than previous ones. As a result, our algorithms achieve strong quality guarantees and experimentally small relative errors at the same time. Also, our algorithms can be extended to strongly connected directed networks. Moreover, we can apply our algorithms to weighted centrality, in which nodes may have different weight, with slight modification.
[Algorithm design and analysis, estimation quality, sampling-based algorithm, Social network services, basic centrality measures, graph theory, Estimation, network theory (graphs), Harmonic analysis, complex networks, centrality values, graph nodes, closeness centrality estimation, empirical estimation qualities, directed graphs, massive networks, Complex networks, high quality estimation, Approximation algorithms, network analysis, Random variables, weighted centrality, computational complexity]
Efficient Computation of Multiple Density-Based Clustering Hierarchies
2017 IEEE International Conference on Data Mining
None
2017
HDBSCAN*, a state-of-the-art density-based hierarchical clustering method, produces a hierarchical organization of clusters in a dataset w.r.t. a parameter mpts. While the performance of HDBSCAN* is robust w.r.t. mpts, choosing a "good" value for it can be challenging: depending on the data distribution, a high or low value for mpts may be more appropriate, and certain data clusters may reveal themselves at different values of mpts. To explore results for a range of mpts, one has to run HDBSCAN* for each value in the range independently, which is computationally inefficient. In this paper we propose an efficient approach to compute all HDBSCAN* hierarchies for a range of mpts by replacing the graph used by HDBSCAN* with a much smaller graph that is guaranteed to contain the required information. Our experiments show that our approach can obtain, for example, over one hundred hierarchies for a cost equivalent to running HDBSCAN* about 2 times. In fact, this speedup tends to increase with the number of hierarchies to be computed.
[Conferences, Clustering methods, data distribution, graph theory, data clusters, HDBSCAN*, Data mining, HDBSCAN running, Clustering algorithms, Relative Neighborhood Graph, clustering hierarchies, Hierarchical Clustering, good value, high value, hierarchical clustering method, parameter mpts, low value, HDBSCAN* hierarchies, Clustering, multiple density, pattern clustering, Euclidean distance, Organizations, Australia, computational complexity]
Reductions for Frequency-Based Data Mining Problems
2017 IEEE International Conference on Data Mining
None
2017
Computational complexity of various maximal pattern mining problems, including maximal frequent items, maximal frequent subgraphs in labelled graphs, and maximal subsequences with no repetitions, is studied, and the complexities are found to be equivalent under novel constrained reductions. The results extend those of Kimelfeld and Kolaitis [ACM TODS, 2014].
[Algorithm design and analysis, maximal frequent subgraphs, graph theory, data mining, maximal frequent items, labelled graphs, constrained reductions, maximal pattern mining problems, Data mining, Computational complexity, Itemsets, maximal subsequences, data mining problems, computational complexity]
Ranking Causal Anomalies by Modeling Local Propagations on Networked Systems
2017 IEEE International Conference on Data Mining
None
2017
Complex systems are prevalent in many fields such as finance, security and industry. A fundamental problem in system management is to perform diagnosis in case of system failure such that the causal anomalies, i.e., root causes, can be identified for system debugging and repair. Recently, invariant network has proven a powerful tool in characterizing complex system behaviors. In an invariant network, a node represents a system component, and an edge indicates a stable interaction between two components. Recent approaches have shown that by modeling fault propagation in the invariant network, causal anomalies can be effectively discovered. Despite their success, the existing methods have a major limitation: they typically assume there is only a single and global fault propagation in the entire network. However, in real-world large-scale complex systems, it's more common for multiple fault propagations to grow simultaneously and locally within different node clusters and jointly define the system failure status. Inspired by this key observation, we propose a two-phase framework to identify and rank causal anomalies. In the first phase, a probabilistic clustering is performed to uncover impaired node clusters in the invariant network. Then, in the second phase, a low-rank network diffusion model is designed to backtrack causal anomalies in different impaired clusters. Extensive experimental results on real-life datasets demonstrate the effectiveness of our method.
[system failure status, matrix factorization, fault diagnosis, large-scale complex systems, causal anomaly detection, Time series analysis, Debugging, Probabilistic logic, probabilistic clustering, Indexes, networked systems, Complex systems, single fault propagation, Invariant network, security of data, pattern clustering, global fault propagation, low-rank network diffusion model, invariant network, Clustering algorithms, complex system behaviors, Monitoring]
Automated Medical Diagnosis by Ranking Clusters Across the Symptom-Disease Network
2017 IEEE International Conference on Data Mining
None
2017
The rapid growth of medical recording data has increased the demand for automated analysis. An important problem in recent medical research is automated medical diagnosis, which is to infer likely diseases for the observed symptoms. Existing approaches typically perform the inference on a sparse bipartite graph with two sets of nodes representing diseases and symptoms, respectively. By using this graph, existing methods basically assume no direct dependency exists between diseases (or symptoms), which may not be true in practice. To address this limitation, we propose to integrate two domain networks encoding similarities between diseases and those between symptoms to avoid information loss as well as to alleviate the sparsity problem of the bipartite graph. Another limitation of the existing methods is that they usually output a ranked list of diseases mixed from very different etiologies which greatly limits their practical usefulness. An ideal method should allow a clustered structure in the disease ranking list so that both similar and different diseases can be easily identified. Therefore, we formulate automated diagnosis as a novel cross-domain cluster ranking problem, which identifies and ranks the disease clusters simultaneously in the symptom-disease network. Our formulation employs a joint learning scheme in which the dual procedures of cluster finding and cluster ranking are coupled and mutually reinforced. Experimental results on real-world datasets demonstrate the effectiveness of our method.
[Network clustering, domain networks encoding, graph theory, symptom-disease network, Stochastic processes, data mining, cross-domain cluster, Medical diagnosis, Respiratory system, automated medical diagnosis, sparse bipartite graph, Clustering algorithms, cluster finding, disease clusters, Bipartite graph, medical administrative data processing, learning (artificial intelligence), medical data recording, medical diagnostic computing, disease ranking list, matrix factorization, ranking clusters, medical diagnosis, diseases, electronic health records, Diseases, cluster ranking, ranked list, pattern clustering, automated diagnosis, Medical diagnostic imaging, patient diagnosis]
LCD: A Fast Contrastive Divergence Based Algorithm for Restricted Boltzmann Machine
2017 IEEE International Conference on Data Mining
None
2017
Restricted Boltzmann Machine (RBM) is the building block of Deep Belief Nets and other deep learning tools. Fast learning and prediction are both essential for practical usage of RBM-based machine learning techniques. This paper proposes Lean Contrastive Divergence (LCD), a modified Contrastive Divergence (CD) algorithm, to accelerate RBM learning and prediction without changing the results. LCD avoids most of the required computations with two optimization techniques. The first is called bounds-based filtering, which, through triangle inequality, replaces expensive calculations of many vector dot products with fast bounds calculations. The second is delta product, which effectively detects and avoids many repeated calculations in the core operation of RBM, Gibbs Sampling. The optimizations are applicable to both the standard contrastive divergence learning algorithm and its variations. Results show that the optimizations can produce several-fold (up to 3X for training and 5.3X for prediction) speedups.
[lean contrastive divergence, modified contrastive divergence algorithm, Probability, machine learning techniques, Optimization, Boltzmann machines, deep belief nets, Training, Computer science, standard contrastive divergence learning algorithm, deep learning tools, fast contrastive divergence based algorithm, Machine learning, restricted Boltzmann machine, building block, Prediction algorithms, Liquid crystal displays, learning (artificial intelligence), RBM, LCD algorithm]
Dynamic Propagation Rates: New Dimension to Viral Marketing in Online Social Networks
2017 IEEE International Conference on Data Mining
None
2017
Online Social Networks (OSNs) are effective platforms for viral marketing. Due to their importance, viral marketing related problems in OSNs have been extensively studied in the past decade. However, none of the existing works can cope with the situation that the propagation rate dynamically increases for popular topics, as they all assume known propagation rates. In this paper, to better describe realistic information propagation in OSNs, we propose a novel model, Dynamic Influence Propagation (DIP), that allows propagation rate to change during the diffusion. We then define a new research problem: Threshold Activation Problem under DIP (TAP-DIP) to study the impact of DIP. TAP-DIP adds extra complexity on the already #P-hard TAP problem. Despite it hardness, we are able to approximate TAP-DIP with O(log|V|) ratio. Sitting in the core of our algorithm are the Lipschitz optimization technique and a novel solution to the general version of TAP, the Multi-TAP problem. Using various real OSN datasets, we experimentally demonstrate the impact of DIP and that our solution not only generates high-quality seed sets when being aware of the rate increase, but also is scalable.
[Algorithm design and analysis, #P-hard TAP problem, data analysis, threshold activation problem, Heuristic algorithms, realistic information propagation, OSN datasets, online social networks, Twitter, marketing, viral marketing related problems, Probability density function, Approximation algorithms, social networking (online), dynamic propagation rates, research problem, multiTAP problem, Integrated circuit modeling, Electronics packaging, approximate TAP-DIP, dynamic influence propagation]
Domain Specific Feature Transfer for Hybrid Domain Adaptation
2017 IEEE International Conference on Data Mining
None
2017
Heterogeneous domain adaptation needs supplementary information to link up domains. However, this supplementary information is unavailable in many real cases. In this paper, a new problem setting called hybrid domain adaptation is investigated. It is a special case of heterogeneous domain adaptation in which different domains share some common features, but also have their own domain specific features. In this case, it can be efficiently solved without any supplementary information by using the common features to link up the domains in adaptation. We propose a domain specific feature transfer (DSFT) method, which can link up different domains using the common features and simultaneously reduce domain divergences. Specifically, we first learn the translations between the common features and the domain specific features. Then we cross-use the learned translations to transfer the domain specific features of one domain to another domain. Finally, we compose a homogeneous space in which the domain divergences are minimized. Extensive experiments verify the effectiveness of our proposed method.
[Adaptation models, Sentiment analysis, text analysis, domain specific feature, data mining, Linear programming, Electronic mail, hybrid domain adaptation, domain specific feature transfer method, heterogeneous domain adaptation, Computer science, knowledge transfer, supplementary information, minimisation, Kernel, domain divergences, homogeneous space]
Multi-level Multiple Attentions for Contextual Multimodal Sentiment Analysis
2017 IEEE International Conference on Data Mining
None
2017
Multimodal sentiment analysis involves identifying sentiment in videos and is a developing field of research. Unlike current works, which model utterances individually, we propose a recurrent model that is able to capture contextual information among utterances. In this paper, we also introduce attentionbased networks for improving both context learning and dynamic feature fusion. Our model shows 6-8% improvement over the state of the art on a benchmark dataset.
[Sentiment analysis, Visualization, utterances, Fuses, Social network services, image classification, context learning, image fusion, sentiment analysis, data mining, videos, contextual information, Videos, recurrent model, dynamic feature fusion, contextual multimodal sentiment, feature extraction, multilevel multiple attentions, multimodal sentiment analysis, Feature extraction, attention-based networks, learning (artificial intelligence), Context modeling]
Statistical Link Label Modeling for Sign Prediction: Smoothing Sparsity by Joining Local and Global Information
2017 IEEE International Conference on Data Mining
None
2017
One of the major issues in signed networks is to use network structure to predict the missing sign of an edge. In this paper, we introduce a novel probabilistic approach for the sign prediction problem. The main characteristic of the proposed models is their ability to adapt to the sparsity level of an input network. Building a model that has an ability to adapt to the sparsity of the data has not yet been considered in the previous related works. We suggest that there exists a dilemma between local and global structures and attempt to build sparsity adaptive models by resolving this dilemma. To this end, we propose probabilistic prediction models based on local and global structures and integrate them based on the concept of smoothing. The model relies more on the global structures when the sparsity increases, whereas it gives more weights to the information obtained from local structures for low levels of the sparsity. The proposed model is assessed on three real-world signed networks, and the experiments reveal its consistent superiority over the state of the art methods. As compared to the previous methods, the proposed model not only better handles the sparsity problem, but also has lower computational complexity and can be updated using real-time data streams.
[Adaptation models, smoothing sparsity, Signed networks, missing sign, Predictive models, network theory (graphs), statistical link label, sign prediction problem, probabilistic prediction models, Local structure, local structures, global information, novel probabilistic approach, network structure, global structures, sparsity adaptive models, sparsity increases, Smoothing methods, input network, Computational modeling, probability, Probabilistic logic, local information, Smoothing, Link label prediction, Global structure, Feature extraction, Data models, sparsity problem, dilemma]
Novel Exact and Approximate Algorithms for the Closest Pair Problem
2017 IEEE International Conference on Data Mining
None
2017
The closest pair problem (CPP) is an important problem that has numerous applications in clustering, graph partitioning, image processing, patterns identification, intrusion detection, etc. Numerous algorithms have been presented for solving the CPP. For instance, on n points there exists an O(n log n) time algorithm for CPP (when the dimension is a constant). There also exist randomized algorithms with an expected linear run time. However these algorithms do not perform well in practice. The algorithms that are employed in practice have a worst case quadratic run time. One of the best performing algorithms for the CPP is MK (originally designed for solving the time series motif finding problem). In this paper we present an elegant exact algorithm called MPR for the CPP that performs better than MK. Also, we present approximation algorithms for the CPP that are faster than MK by up to a factor of more than 40, while maintaining a very good accuracy.
[Algorithm design and analysis, Measurement, approximation theory, CPP, time series motif mining, Heuristic algorithms, Time series analysis, time series motif finding problem, Partitioning algorithms, approximation algorithms, closest pair problem, approximate algorithms, exact algorithms, Clustering algorithms, randomized algorithms, Approximation algorithms, computational complexity, linear run time]
Informing the Use of Hyperparameter Optimization Through Metalearning
2017 IEEE International Conference on Data Mining
None
2017
One of the challenges of data mining is finding hyperparameters for a learning algorithm that will produce the best model for a given dataset. Hyperparameter optimization automates this process, but it can still take significant time. It has been found that hyperparameter optimization does not always result in induced models with significant improvement over default values, yet no systematic analysis of the role of hyperparameter optimization in machine learning has been conducted. We use metalearning to inform the decision of whether to optimize hyperparameters based on expected performance improvement and computational cost.
[Algorithm design and analysis, learning algorithm, metalearning, mathematics computing, data mining, hyperparameter optimization, Statistics, Optimization, Genetic algorithms, Support vector machines, Metalearning, optimisation, Sociology, Prediction algorithms, hyperparameters, learning (artificial intelligence)]
Kernel-Based Feature Extraction for Collaborative Filtering
2017 IEEE International Conference on Data Mining
None
2017
Singular value decomposition (SVD) has been used widely in the literature to recover the missing entries of a matrix. The basic principle in such methods is to assume that the correlated data is distributed with a low-rank structure. The knowledge of the low-rank structure is then used to predict the missing entries. SVD is based on the assumption that the data (user ratings) are distributed on a linear hyperplane. This is not always the case, and the data could often be distributed on a nonlinear hyperplane. Therefore, in this paper, we explore the methodology of kernel feature extraction to complement off-the-shelf methods for improving their accuracy. The extracted features can be used to enhance a variety of existing methods such as biased matrix factorization and SVD++. We present experimental results illustrating the effectiveness of using this approach.
[SVD, collaborative filtering, linear hyperplane, Computational modeling, user ratings, low-rank structure, biased matrix factorization, missing entries, Manifolds, correlated data, feature extraction, Distributed databases, SVD++, Feature extraction, Robustness, data handling, off-the-shelf methods, Kernel, nonlinear hyperplane, kernel feature extraction, singular value decomposition, Principal component analysis]
Reputation-Based Ranking Systems and Their Resistance to Bribery
2017 IEEE International Conference on Data Mining
None
2017
We study bribery resistance properties in two classes of reputation-based ranking systems, where the rankings are computed by weighting the rates given by users with their reputations. In the first class, the rankings are the result of the aggregation of all the ratings, and all users are provided with the same ranking for each item. In the second class, there is a first step that clusters users by their rating pattern similarities, and then the rankings are computed cluster-wise. Hence, for each item, there is a different ranking for distinct clusters. We study the setting where the seller of each item can bribe users to rate the item, if they did not rate it before, or to increase their previous rating on the item. We model bribing strategies under these ranking scenarios and explore under which conditions it is profitable to bribe a user, presenting, in several cases, the optimal bribing strategies. By computing dedicated rankings to each cluster, we show that bribing, in general, is not as profitable as in the simpler without clustering. Finally, we illustrate our results with experiments using real data.
[bribing strategies, item, distinct clusters, Social network services, bribery resistance properties, dedicated rankings, Companies, Data Mining, ranking scenarios, Reputation-based Ranking Systems, Convergence, Resistance, rating pattern similarities, pattern clustering, reputation-based ranking systems, Robustness, Silicon, social sciences computing, Bribery, behavioural sciences computing, Manganese, computed cluster-wise]
The Many Faces of Link Fraud
2017 IEEE International Conference on Data Mining
None
2017
Most past work on social network link fraud detection tries to separate genuine users from fraudsters, implicitly assuming that there is only one type of fraudulent behavior. But is this assumption true? And, in either case, what are the characteristics of such fraudulent behaviors? In this work, we set up honeypots ("dummy" social network accounts), and buy fake followers (after careful IRB approval). We report the signs of such behaviors including oddities in local network connectivity, account attributes, and similarities and differences across fraud providers. Most valuably, we discover and characterize several types of fraud behaviors. We discuss how to leverage our insights in practice by engineering strongly performing entropy-based features and demonstrating high classification accuracy. Our contributions are (a) observations: we analyze our honeypot fraudster ecosystem and give surprising insights into the multifaceted behaviors of these fraudster types, and (b) features: we propose novel features that give strong (&gt;0.95 precision/recall) discriminative power on ground-truth Twitter data.
[fraud providers, honeypot fraudster ecosystem, social network link fraud detection, Twitter data, Ecosystems, multifaceted behaviors, Twitter, link fraud, anomaly detection, Electronic mail, genuine users, high classification accuracy, social network, entropy, feature extraction, fake followers, account attributes, fraudulent behavior, social media, Google, fraudster types, dummy social network accounts, honeypots, local network connectivity, fraud behaviors, fraud, fraudsters, Data collection, Feature extraction, social networking (online)]
Synchronization-Inspired Co-Clustering and Its Application to Gene Expression Data
2017 IEEE International Conference on Data Mining
None
2017
In this paper, we propose a new synchronization-inspired co-clustering algorithm by dynamic simulation, called CoSync, which aims to discover biologically relevant subgroups embedding in a given gene expression data matrix. The basic idea is to view a gene expression data matrix as a dynamical system, and the weighted two-sided interactions are imposed on each element of the matrix from both aspects of genes and conditions, resulting in the values of all element in a co-cluster synchronizing together. Experiments show that our algorithm allows uncovering high-quality co-clusterings embedded in gene expression data sets and has its superiority over many state-of-the-art algorithms.
[Algorithm design and analysis, gene expression data, Heuristic algorithms, dynamic simulation, gene expression data sets, gene expression data matrix, Synchronization, Gene expression, called CoSync, biologically relevant subgroups, matrix algebra, synchronisation, dynamical system, two-sided interactions, co-clustering, genetics, biology computing, pattern clustering, Clustering algorithms, synchronization, Data models, co-clustering algorithm, Artificial intelligence]
Exploring Common and Distinct Structural Connectivity Patterns Between Schizophrenia and Major Depression via Cluster-Driven Nonnegative Matrix Factorization
2017 IEEE International Conference on Data Mining
None
2017
In this paper, we introduce a novel method to discover common and distinct structural connectivity patterns between SZP and MDD via a Cluster-Driven Nonnegative Matrix Factorization (called CD-NMF). Specifically, CD-NMF is applied to decompose the joint structural connectivity map into common and distinct parts, and each part is further factorized into two sub-matrices (i.e. common/distinct basis matrix and common/distinct encoding matrix) correspondingly. By imposing the clustering constraints on common and distinct encoding matrices, the discriminative patterns as well as the common patterns between the two disorders are extracted simultaneously. Experimental results demonstrate that CD-NMF allows finding the common and distinct structural patterns effectively. More importantly, the derived distinct patterns, show powerful ability to discriminate the patients of schizophrenia and major depressive disorder.
[Brain, discriminative patterns, Transmission line matrix methods, distinct encoding matrices, distinct structural connectivity patterns, matrix decomposition, schizophrenia, common/distinct basis matrix, CD-NMF, structural connectivity, common/distinct encoding matrix, joint structural connectivity map, major depression, Buildings, Encoding, Matrix decomposition, biomarker, nonnegative matrix factorization, medical disorders, pattern clustering, clustering constraints, Feature extraction, medical computing, distinct structural patterns, cluster-driven nonnegative matrix factorization]
WRS: Waiting Room Sampling for Accurate Triangle Counting in Real Graph Streams
2017 IEEE International Conference on Data Mining
None
2017
If we cannot store all edges in a graph stream, which edges should we store to estimate the triangle count accurately?Counting triangles (i.e., cycles of length three) is a fundamental graph problem with many applications in social network analysis, web mining, anomaly detection, etc. Recently, much effort has been made to accurately estimate global and local triangle counts in streaming settings with limited space. Although existing methods use sampling techniques without considering temporal dependencies in edges, we observe temporal locality in real dynamic graphs. That is, future edges are more likely to form triangles with recent edges than with older edges. In this work, we propose a single-pass streaming algorithm called Waiting-Room Sampling (WRS) for global and local triangle counting. WRS exploits the temporal locality by always storing the most recent edges, which future edges are more likely to form triangles with, in the waiting room, while it uses reservoir sampling for the remaining edges. Our theoretical and empirical analyses show that WRS is: (a) Fast and 'any time': runs in linear time, always maintaining and updating estimates while new edges arrive, (b) Effective: yields up to 47% smaller estimation error than its best competitors, and (c) Theoretically sound: gives unbiased estimates with small variances under the temporal locality.
[Algorithm design and analysis, estimation theory, temporal locality, Heuristic algorithms, graph theory, local triangle counting, edge sampling, Electronic mail, dynamic graphs, Waiting-Room Sampling, global triangle counting, fundamental graph problem, WRS, triangle counting, single-pass streaming algorithm, global triangle counts estimation, Estimation error, sampling methods, Social network services, Image edge detection, reservoir sampling, real graph streams, local triangle counts estimation, Reservoirs, graph stream, computational complexity]
Collaborative Inference of Coexisting Information Diffusions
2017 IEEE International Conference on Data Mining
None
2017
The purpose of diffusion history inference is to reconstruct the missing traces of information diffusion according to incomplete observations. Existing methods, however, often focus only on single diffusion trace, while in a real-world social network, there often coexist multiple information diffusions. In this paper, we propose a novel approach called Collaborative Inference Model (CIM) for the problem of the inference of coexisting information diffusions. CIM can holistically model multiple information diffusions without any prior assumption of diffusion models, and collaboratively infer the histories of the coexisting information diffusions via low-rank approximation with a fusion of heterogeneous constraints generated from additional data sources. We also propose an optimized algorithm called Time Window based Parallel Decomposition Algorithm (TWPDA) to speed up the inference without compromise on the accuracy. Extensive experiments are conducted on real-world datasets to verify the effectiveness and efficiency of CIM and TWPDA.
[data sources, TWPDA, Sparse tensor approximation, Social network services, information dissemination, Time Window based Parallel Decomposition Algorithm, single diffusion trace, diffusion history inference, Social network, History, Matrix decomposition, Sparse matrices, inference mechanisms, heterogeneous constraints, Tensile stress, Collaborative Inference Model, Collaboration, Approximation algorithms, Information diffusion, diffusion models, information diffusion]
Epidemic Forecasting Framework Combining Agent-Based Models and Smart Beam Particle Filtering
2017 IEEE International Conference on Data Mining
None
2017
Over the past decades, numerous techniques have been developed to forecast the temporal evolution of epidemic outbreaks. This paper proposes an approach that combines high resolution agent-based models using realistic social contact networks for simulating epidemic evolution with a particle filter based method for assimilation based forecasting. Agent-based modeling using realistic social contact networks provides two key advantages: (i) they capture the causal processes underlying the epidemic and hence are useful to understand the role of interventions on the course of the epidemics - typically time series models cannot capture this and as a result often do not perform well in such situations; (ii) they provide detailed forecast information - this allows us to produce forecast at high levels of temporal, spatial and social granularity. We also propose a new variation of particle filter technique called beam search particle filtering. The modification allows us to more efficiently search the parameter space which is necessitated by the fact that agent-based techniques are computationally expensive. We illustrate our methodology on the synthetic dataset of Ebola provided as a part of the NSF/NIH Ebola forecasting challenge. Our results show the efficacy of the proposed approach and suggest that agent-based causal models can be combined with filtering techniques to yield a new class of assimilation models for infectious disease forecasting.
[multi-agent systems, epidemic outbreaks, Particle Filter, Epidemic forecasting, infectious disease forecasting, Predictive models, particle filtering (numerical methods), assimilation models, social granularity, high resolution agent-based models, Ebola disease modeling, causal processes, Mathematical model, epidemic evolution, Particle beams, agent-based techniques, Computational modeling, Biological system modeling, particle filter technique, diseases, time series, forecast information, Forecasting, Diseases, time series models, assimilation based forecasting, agent-based causal models, epidemics, numerous techniques, smart beam particle, forecasting theory, beam search particle filtering, realistic social contact networks, NSF-NIH Ebola forecasting challenge, temporal evolution, epidemic forecasting framework, spatial granularity, temporal granularity, Data assimilation]
Autoregressive Tensor Factorization for Spatio-Temporal Predictions
2017 IEEE International Conference on Data Mining
None
2017
Analysis of spatio-temporal data is a common research topic that requires the interpolations of unknown locations and the predictions of feature observations by utilizing information about where and when the data were observed. One of the most difficult problems is to make predictions of unknown locations. Tensor factorization methods are popular in this field because of their capability of handling multiple types of spatio-temporal data, dealing with missing values, and providing computationally efficient parameter estimation procedures. However, unlike traditional approaches such as spatial autoregressive models, the existing tensor factorization methods have not tried to learn spatial autocorrelations. These methods employ previously inferred spatial dependencies, often resulting in poor performances on the problem of making interpolations and predictions of unknown locations. In this paper, we propose a new tensor factorization method that estimates low-rank latent factors by simultaneously learning the spatial and temporal autocorrelations. We introduce new spatial autoregressive regularizers based on existing spatial autoregressive models and provide an efficient estimation procedure. With experiments on publicly available traffic transporting data, we demonstrate that our proposed method significantly improves the predictive performances in our problems in comparison to the existing state-of-the-art spatio-temporal analysis methods.
[Correlation, low-rank latent factors, publicly available traffic transporting data, tensors, matrix decomposition, temporal autocorrelations, interpolations, Reactive power, autoregressive tensor factorization, parameter estimation, data analysis, tensor factorization method, Biological system modeling, Computational modeling, autoregressive processes, predictive performances, spatio-temporal predictions, Matrix decomposition, inferred spatial dependencies, spatial autocorrelations, spatiotemporal phenomena, computationally efficient parameter estimation procedures, Interpolation, Tensile stress, interpolation, feature observations, spatial autoregressive regularizers]
Time-Aware Latent Hierarchical Model for Predicting House Prices
2017 IEEE International Conference on Data Mining
None
2017
It is widely acknowledged that the value of a house is the mixture of a large number of characteristics. House price prediction thus presents a unique set of challenges in practice. While a large body of works are dedicated to this task, their performance and applications have been limited by the shortage of long time span of transaction data, the absence of real-world settings and the insufficiency of housing features. To this end, a time-aware latent hierarchical model is introduced to capture underlying spatiotemporal interactions behind the evolution of house prices. The hierarchical perspective obviates the need for historical transaction data of exactly same houses when temporal effects are considered. The proposed framework is examined on a large-scale dataset of the property transaction in Beijing. The whole experimental procedure strictly complies with the real-world scenario. The empirical evaluation results demonstrate the outperformance of our approach over alternative competitive methods.
[Economics, housing features, Predictive models, historical transaction data, Indexes, Optimization, Beijing, Education, hierarchical perspective, property transaction, house price prediction, Data models, property market, pricing, time-aware latent hierarchical model]
Crowdsourced Correlation Clustering with Relative Distance Comparisons
2017 IEEE International Conference on Data Mining
None
2017
Crowdsourced, or human computation based clustering algorithms usually rely on relative distance comparisons, as these are easier to elicit from human workers than absolute distance information. We build upon existing work on correlation clustering, a well-known non-parametric approach to clustering, and present a novel clustering algorithm for human computation. We first define a novel variant of correlation clustering that is based on relative distance comparisons, and briefly outline an approximation algorithm for this problem. As a second contribution, we propose a more practical algorithm, which we empirically compare against existing methods from literature. Experiments with synthetic data suggest that our approach can outperform more complex methods. Also, our method efficiently finds good and intuitive clusterings from real relative distance comparison data.
[Algorithm design and analysis, approximation theory, Correlation, crowdsourcing, clustering algorithm, relative distance comparison data, Optimized production technology, Search problems, crowdsourced correlation clustering, pattern clustering, Clustering algorithms, human workers, approximation algorithm, Approximation algorithms, non-parametric approach, human computation based clustering algorithms]
Multimodal Content Analysis for Effective Advertisements on YouTube
2017 IEEE International Conference on Data Mining
None
2017
The recent advancement of web-scale digital advertising saw a paradigm shift from the conventional focus of digital advertisement distribution towards integrating digital processes and methodologies and forming a seamless workflow of advertisement design, production, distribution, and effectiveness monitoring. In this work, we implemented a computational framework for the predictive analysis of the content-based features extracted from advertisement video files and various effectiveness metrics to aid the design and production processes of commercial advertisements. Our proposed predictive analysis framework extracts multi-dimensional temporal patterns from the content of advertisement videos using multimedia signal processing and natural language processing tools. The pattern analysis part employs an architecture of cross modality feature learning where data streams from different feature dimensions are employed to train separate neural network models and then these models are fused together to learn a shared representation. Subsequently, a neural network model trained on this joint representation is utilized as a classifier for predicting advertisement effectiveness. Based on the predictive patterns identified between the content features and the effectiveness metrics of advertisements, we have elicited a useful set of auditory, visual and textual patterns that is strongly correlated with the proposed effectiveness metrics while can be readily implemented in the design and production processes of commercial advertisements. We validate our approach using subjective ratings from a dedicated user study, the text sentiment strength of online viewer comments, and a viewer opinion metric of the likes/views ratio of each advertisement from YouTube video-sharing website.
[Measurement, Visualization, text analysis, effectiveness metrics, advertisement videos, multimodal content analysis, Multimedia communication, multimedia computing, effectiveness monitoring, digital processes, neural network model, content features, advertisement design, web-scale digital advertising, feature extraction, Production, classifier, commercial advertisements, learning (artificial intelligence), video signal processing, feature dimensions, cross modality feature learning, visual patterns, natural language processing tools, pattern analysis part, pattern classification, advertising data processing, effective advertisements, natural language processing, multimedia signal processing, predictive patterns, Neural networks, textual patterns, predictive analysis framework, Streaming media, advertisement video files, Feature extraction, social networking (online), YouTube video-sharing website, separate neural network models, neural nets, multidimensional temporal patterns]
Balanced Distribution Adaptation for Transfer Learning
2017 IEEE International Conference on Data Mining
None
2017
Transfer learning has achieved promising results by leveraging knowledge from the source domain to annotate the target domain which has few or none labels. Existing methods often seek to minimize the distribution divergence between domains, such as the marginal distribution, the conditional distribution or both. However, these two distances are often treated equally in existing algorithms, which will result in poor performance in real applications. Moreover, existing methods usually assume that the dataset is balanced, which also limits their performances on imbalanced tasks that are quite common in real problems. To tackle the distribution adaptation problem, in this paper, we propose a novel transfer learning approach, named as Balanced Distribution Adaptation (BDA), which can adaptively leverage the importance of the marginal and conditional distribution discrepancies, and several existing methods can be treated as special cases of BDA. Based on BDA, we also propose a novel Weighted Balanced Distribution Adaptation (W-BDA) algorithm to tackle the class imbalance issue in transfer learning. W-BDA not only considers the distribution adaptation between domains but also adaptively changes the weight of each class. To evaluate the proposed methods, we conduct extensive experiments on several transfer learning tasks, which demonstrate the effectiveness of our proposed algorithms over several state-of-the-art methods.
[distribution adaptation problem, Machine learning algorithms, Conferences, target domain, Transfer learning, Estimation, distribution adaptation, Data mining, leveraging knowledge, source domain, transfer learning tasks, Learning systems, Training, marginal distribution, conditional distribution, Weighted Balanced Distribution Adaptation algorithm, class imbalance, algorithm theory, learning (artificial intelligence), domain adaptation, transfer learning approach, BDA, distribution divergence]
DPiSAX: Massively Distributed Partitioned iSAX
2017 IEEE International Conference on Data Mining
None
2017
Indexing is crucial for many data mining tasks that rely on efficient and effective similarity query processing. Consequently, indexing large volumes of time series, along with high performance similarity query processing, have became topics of high interest. For many applications across diverse domains though, the amount of data to be processed might be intractable for a single machine, making existing centralized indexing solutions inefficient. We propose a parallel indexing solution that gracefully scales to billions of time series, and a parallel query processing strategy that, given a batch of queries, efficiently exploits the index. Our experiments, on both synthetic and real world data, illustrate that our index creation algorithm works on 1 billion time series in less than 2 hours, while the state of the art centralized algorithms need more than 5 days. Also, our distributed querying algorithm is able to efficiently process millions of queries over collections of billions of time series, thanks to an effective load balancing mechanism.
[parallel query processing strategy, parallel algorithms, data mining tasks, index creation algorithm, Time series analysis, data mining, time series, effective similarity query processing, Partitioning algorithms, Sparks, Data mining, DPiSAX, load balancing mechanism, query processing, efficient similarity query processing, high performance similarity query processing, database indexing, resource allocation, Query processing, temporal databases, distributed querying algorithm, parallel indexing solution, Indexing]
An Influence-Receptivity Model for Topic Based Information Cascades
2017 IEEE International Conference on Data Mining
None
2017
We consider the problem of estimating the latent structure of a social network based on observational data on information diffusion processes, or cascades. Here for a given cascade, we only observe the time a node/agent is infected but not the source of infection. Existing literature has focused on estimating network diffusion matrix without any underlying assumptions on the structure of the network. We propose a novel model for inferring network diffusion matrix based on the intuition that an information datum is more likely to propagate among two nodes if they are interested in similar topics, which are common with the information. In particular, our model endows each node with an influence vector (how authoritative they are on each topic) and a receptivity vector (how susceptible they are on each topic). We show how this node-topic structure can be estimated from observed cascades. The estimated model can be used to build recommendation system based on the receptivity vectors, as well as for marketing based on the influence vectors.
[multi-agent systems, estimation theory, Heuristic algorithms, node-topic structure, influence vector, Media, network diffusion matrix estimation, observational data, Information technology, recommendation system, Diseases, receptivity vector, social network, information diffusion processes, recommender systems, social networking (online), Inference algorithms, influence-receptivity model, information datum, topic based information cascades, Immune system, Business]
Risk Control of Best Arm Identification in Multi-armed Bandits via Successive Rejects
2017 IEEE International Conference on Data Mining
None
2017
Best arm identification in stochastic Multi-Armed Bandits (MAB) has become an essential variant in the research line of bandits for decision-making problems. In previous work, the best arm usually refers to an arm with the highest expected payoff in a given decision-arm set. However, in many practical scenarios, it would be more important and desirable to incorporate the risk of an arm into the best decision. In this paper, motivated by practical applications with risk via bandits, we investigate the problem of Risk Control of Best Arm Identification (RCBAI) in stochastic MAB. Based on the technique of Successive Rejects (SR), we show that the error resulting from the mean-variance estimation is sub-Gamma by setting mild assumptions on stochastic payoffs of arms. Besides, we develop an algorithm named as RCMAB. SR, and derive an upper bound for the probability of error for RCBAI in stochastic MAB. We demonstrate the superiority of the RCMAB. SR algorithm in synthetic datasets, and then apply the RCMAB. SR algorithm in financial data for yearly investments to show its superiority for practical applications.
[Algorithm design and analysis, highest expected payoff, successive rejects, decision-making problems, decision theory, Successive rejects, Decision making, Risk control, Stochastic processes, Estimation, best arm identification, game theory, Multi-armed bandits, stochastic MAB, Upper bound, risk control, Sub-Gamma noise, SR algorithm, decision making, Random variables, stochastic processes, stochastic multiarmed bandits, decision-arm set]
Multi-level Multi-task Learning for Modeling Cross-Scale Interactions in Nested Geospatial Data
2017 IEEE International Conference on Data Mining
None
2017
Predictive modeling of nested geospatial data is a challenging problem as the models must take into account potential interactions among variables defined at different spatial scales. These cross-scale interactions, as they are commonly known, are particularly important to understand relationships among ecological properties at macroscales. In this paper, we present a novel, multi-level multi-task learning framework for modeling nested geospatial data in the lake ecology domain. Specifically, we consider region-specific models to predict lake water quality from multi-scaled factors. Our framework enables distinct models to be developed for each region using both its local and regional information. The framework also allows information to be shared among the region-specific models through their common set of latent factors. Such information sharing helps to create more robust models especially for regions with limited or no training data. In addition, the framework can automatically determine cross-scale interactions between the regional variables and the local variables that are nested within them. Our experimental results show that the proposed framework outperforms all the baseline methods in at least 64% of the regions for 3 out of 4 lake water quality datasets evaluated in this study. Furthermore, the latent factors can be clustered to obtain a new set of regions that is more aligned with the response variables than the original regions that were defined a priori from the ecology domain.
[latent factors, regional variables, Predictive models, Geospatial analysis, cross-scale interactions, ecology, nested geospatial data, lake ecology domain, training data, lakes, Mathematical model, learning (artificial intelligence), 4 lake water quality datasets, regional information, Biological system modeling, geophysics computing, Linear programming, local information, multilevel multitask learning, robust models, region-specific models, water quality, Lakes, distinct models, Data models, hydrological techniques]
Wave2Vec: Learning Deep Representations for Biosignals
2017 IEEE International Conference on Data Mining
None
2017
Time series data mining has gained increasing attention in health domain. Recently, researchers attempt to employ Natural Language Processing (NLP) to health data mining, in order to learn proper representations of discrete medical concepts from Electronic Health Records (EHRs). However, existing models do not take continuous physiological records into account, which are naturally existed in EHRs. The major challenges for this task are to model non-obvious representations from observed high dimensional biosignals, and to interpret the learned features. To address these issues, we propose Wave2Vec, an end-to-end deep learning model, to bridge the gap between biosignal processing and language modeling. Wave2Vec jointly learns both inherent and embedding representations of biosignals at the same time. To evaluate the performance of our model in clinical task, we carry out experiments on two real world benchmark biosignal datasets. Experimental results show that the proposed Wave2Vec model outperforms the six feature leaning baselines in biosignal processing.
[Wave2Vec model, context learning, representation learning, data mining, world benchmark biosignal datasets, Electroencephalography, biosignals, Data mining, health data mining, deep learning, deep representation learning, learning (artificial intelligence), end-to-end deep learning model, embedding representations, Biological system modeling, natural language processing, health domain, EHRs, Time series analysis, biosignal processing, discrete medical concepts, language modeling, time series, medical information systems, electronic health records, continuous physiological records, observed high dimensional biosignals, Machine learning, Brain modeling, time series data mining]
Recurrent Encoder-Decoder Networks for Time-Varying Dense Prediction
2017 IEEE International Conference on Data Mining
None
2017
Dense prediction is concerned with predicting a label for each of the input units, such as pixels of an image. Accurate dense prediction for time-varying inputs finds applications in a variety of domains, such as video analysis and medical imaging. Such tasks need to preserve both spatial and temporal structures that are consistent with the inputs. Despite the success of deep learning methods in a wide range of artificial intelligence tasks, time-varying dense prediction is still a less explored domain. Here, we proposed a general encoder-decoder network architecture that aims to addressing time-varying dense prediction problems. Given that there are both intra-image spatial structure information and temporal context information to be processed simultaneously in such tasks, we integrated fully convolutional networks (FCNs) with recurrent neural networks (RNNs) to build a recurrent encoder-decoder network. The proposed network is capable of jointly processing two types of information. Specifically, we developed convolutional RNN (CRNN) to allow dense sequence processing. More importantly, we designed CRNN bottleneck modules for alleviating the excessive computational cost incurred by carrying out multiple convolutions in the CRNN layer. This novel design is shown to be a critical innovation in building very flexible and efficient deep models for time-varying dense prediction. Altogether, the proposed model handles time-varying information with the CRNN layers and spatial structure information with the FCN architectures. The multiple heterogeneous modules can be integrated into the same network, which can be trained end-to-end to perform time-varying dense prediction. Experimental results showed that our model is able to capture both high-resolution spatial information and relatively low-resolution temporal information as compared to other existing models.
[general encoder-decoder network architecture, time-varying information, Recurrent neural networks, Computational modeling, image classification, recurrent neural nets, time-varying dense prediction, video analysis, Dense prediction, time-varying data, Image segmentation, bottleneck module, recurrent neural networks, intra-image spatial structure information, Computer architecture, Logic gates, recurrent encoder-decoder network, Computational efficiency, convolutional neural networks, learning (artificial intelligence), image coding, time-varying dense prediction problems, time-varying inputs]
A Practically Competitive and Provably Consistent Algorithm for Uplift Modeling
2017 IEEE International Conference on Data Mining
None
2017
Randomized experiments have been critical tools of decision making for decades. However, subjects can show significant heterogeneity in response to treatments in many important applications. Therefore it is not enough to simply know which treatment is optimal for the entire population. What we need is a model that correctly customize treatment assignment base on subject characteristics. The problem of constructing such models from randomized experiments data is known as Uplift Modeling in the literature. Many algorithms have been proposed for uplift modeling and some have generated promising results on various data sets. Yet little is known about the theoretical properties of these algorithms. In this paper, we propose a new tree-based ensemble algorithm for uplift modeling. Experiments show that our algorithm can achieve competitive results on both synthetic and industry-provided data. In addition, by properly tuning the "node size" parameter, our algorithm is proved to be consistent under mild regularity conditions. This is the first consistent algorithm for uplift modeling that we are aware of.
[Algorithm design and analysis, data analysis, Estimation, trees (mathematics), Uplift modeling, Predictive models, provably consistent algorithm, Partitioning algorithms, tree-based ensemble algorithm, personalized treatment learning, Training, uplift modeling, practically competitive algorithm, correctly customize treatment assignment base, Training data, decision making, Data models, learning (artificial intelligence), randomized experiments data]
Incorporating Spatio-Temporal Smoothness for Air Quality Inference
2017 IEEE International Conference on Data Mining
None
2017
It is well recognized that air quality inference is of great importance for environmental protection. However, due to the limited monitoring stations and various impact factors, e.g., meteorology, traffic volume and human mobility, inference of air quality index (AQI) could be a difficult task. Recently, with the development of new ways for collecting and integrating urban, mobile, and public service data, there is a potential to leverage spatial relatedness and temporal dependencies for better AQI estimation. To that end, in this paper, we exploit a novel spatio-temporal multi-task learning strategy and develop an enhanced framework for AQI inference. Specifically, both time dependence within a single monitoring station, and spatial relatedness across all the stations will be captured, and then well trained with effective optimization to support AQI inference tasks. As air-quality related features from cross-domain data have been extracted and quantified, comprehensive experiments based on real-world datasets validate the effectiveness of our proposed framework with significant margin compared with several state-of-the-art baselines, which support the hypothesis that our spatio-temporal multi-task learning framework could better predict and interpret AQI fluctuation.
[single monitoring station, Urban Computing, Multi-task Learning, AQI fluctuation, monitoring stations, Data mining, AQI estimation, Optimization, air quality inference, spatio-temporal smoothness, air quality index, Real-time systems, AQI Prediction, learning (artificial intelligence), Monitoring, air quality, spatio-temporal multitask learning framework, Air quality, inference mechanisms, urban service data, environmental protection, public service data, human mobility, environmental science computing, Feature extraction, air pollution, AQI inference tasks]
An Automatic Approach for Transit Advertising in Public Transportation Systems
2017 IEEE International Conference on Data Mining
None
2017
Transit advertising provides frequent exposure to a large number of residents in different urban regions. However, traditional methods are generally manual and qualitatively based on a rough estimation, such as the number of passengers taken by a bus or functional regions covered. How to accurately put an advertisement on appropriate transportations becomes an important task for potential business value. In this paper, our goal is to recommend top-k bus routes for an ad, which can maximize advertising effectiveness, by mining multiple data sources, such as Smart Card Transaction (SCT) data, geographic data and point of interests (POIs) data. We propose a framework that captures the mobility patterns of passengers and the characteristics of bus stations so as to evaluate the influence of an ad for the passengers quantitatively. Specifically, we first extract the passenger trajectories to model passenger mobility from SCT data and then model each bus station over three characteristics (i.e., hub degree, topic distribution, district region) by geographic data and nearby POIs. According to three characteristics of bus stations, we formulate three application scenarios and design the corresponding methods to make top-k bus routes recommendation. Extensive experiments on real-world public transportation data validate the effectiveness of our proposed approach.
[Transit advertising, geographic data, potential business value, passenger mobility, data mining, frequent exposure, smart cards, real-world public transportation data, SCT data, advertising, Data mining, point of interests data, transit advertising, Trajectory, advertising effectiveness, Advertising, passenger trajectories, passengers, public transportation systems, traffic engineering computing, Public transportation, smart card transaction data, mobility patterns, Optimal route selection, public transport, top-k bus routes recommendation, bus station, Data models]
iNEAT: Incomplete Network Alignment
2017 IEEE International Conference on Data Mining
None
2017
Network alignment and network completion are two fundamental cornerstones behind many high-impact graph mining applications. The state-of-the-arts have been addressing these tasks in parallel. In this paper, we argue that network alignment and completion are inherently complementary with each other, and hence propose to jointly address them so that the two tasks can benefit from each other. We formulate it from the optimization perspective, and propose an effective algorithm iNEAT to solve it. The proposed method offers two distinctive advantages. First (Alignment accuracy), our method benefits from higher-quality input networks while mitigates the effect of incorrectly inferred links introduced by the completion task itself. Second (Alignment efficiency), thanks to the low-rank structure of the complete networks and alignment matrix, the alignment can be significantly accelerated. The extensive experiments demonstrate the performance of our algorithm.
[network Alignment, graph theory, data mining, high-impact graph mining applications, Topology, Alignment efficiency, Sparse matrices, Data mining, network completion, Optimization, Alignment accuracy, optimisation, Network topology, optimization, low-rank, higher-quality input networks, Incomplete network alignment, Eigenvalues and eigenfunctions, complete networks, iNEAT]
GaDei: On Scale-Up Training as a Service for Deep Learning
2017 IEEE International Conference on Data Mining
None
2017
Deep learning (DL) training-as-a-service (TaaS) is an important emerging industrial workload. TaaS must satisfy a wide range of customers who have no experience and/or resources to tune DL hyper-parameters (e.g., mini-batch size and learning rate), and meticulous tuning for each user's dataset is prohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with values that are applicable to all users. Unfortunately, few research papers have studied how to design a system for TaaS workloads. By evaluating the IBM Watson Natural Language Classfier (NLC) workloads, the most popular IBM cognitive service used by thousands of enterprise-level clients globally, we provide empirical evidence that only the conservative hyper-parameter setup (e.g., small mini-batch size) can guarantee acceptable model accuracy for a wide range of customers. Unfortunately, smaller mini-batch size requires higher communication bandwidth in a parameter-server based DL training system. In this paper, we characterize the exceedingly high communication bandwidth requirement of TaaS using representative industrial deep learning workloads. We then present GaDei, a highly optimized shared-memory based scale-up parameter server design. We evaluate GaDei using both commercial benchmarks and public benchmarks and demonstrate that GaDei significantly outperforms the state-of-the-art parameter-server based implementation while maintaining the required accuracy. GaDei achieves near-best-possible runtime performance, constrained only by the hardware limitation. Furthermore, to the best of our knowledge, GaDei is the only scale-up DL system that provides fault-tolerance.
[enterprise-level clients, Graphics processing units, highly optimized shared-memory based scale-up parameter server design, IBM cognitive service, conservative hyper-parameter setup, important emerging industrial workload, meticulous tuning, Servers, Open source software, System, Training, Bandwidth, state-of-the-art parameter-server, shared memory systems, cloud computing, learning (artificial intelligence), representative industrial deep learning workloads, TaaS workloads, scale-up training, smaller mini-batch size, deep learning training-as-a-service, DL hyper-parameters, Training as a Service, Tuning, Deep Learning, acceptable model accuracy, IBM Watson natural language classfier workloads, Machine learning, GaDei, TaaS hyper-parameters, parameter-server based DL training system]
Scalable Constrained Spectral Clustering via the Randomized Projected Power Method
2017 IEEE International Conference on Data Mining
None
2017
Constrained spectral clustering is an important area with many applications. However, most previous work has only been applied to relatively small data sets: graphs with thousands of points. This prevents this work from being applied to the large data sets found in application domains such as medical imaging and document data. Recent work on constrained and unconstrained spectral clustering has explored scalability of these methods via data approximations such as the Nystrom method which requires the selection of landmarks. However, compressing a graph may lead to undesirable results and poses the additional problem of how to chose landmarks. Instead in this paper, we propose a fast and scalable numerical algorithmic solution for the constrained clustering problem. We show the convergence and stability of our approach by proving its rate of convergence and demonstrate the effectiveness of our algorithm with empirical results on several real data sets. Our approach achieved comparable accuracy as popular constrained spectral clustering algorithms but taking several hundred times less time.
[Algorithm design and analysis, Nystrom method, approximation theory, application domains, Laplace equations, graph theory, convergence, data approximations, constrained clustering problem, convergence of numerical methods, Matrix decomposition, unconstrained spectral clustering, Clustering, Convergence, Computer science, randomized projected power method, Constraints, pattern clustering, Constrained Clustering, Clustering algorithms, data sets, Eigenvalues and eigenfunctions, Spectral]
New Class Adaptation Via Instance Generation in One-Pass Class Incremental Learning
2017 IEEE International Conference on Data Mining
None
2017
One pass learning updates a model with only a single scan of the dataset, without storing historical data. Previous studies focus on classification tasks with a fixed class set, and will perform poorly in an open dynamic environment when new classes emerge in a data stream. The performance degrades because the classifier needs to receive a sufficient number of instances from new classes to establish a good model. This can take a long period of time. In order to reduce this period to deal with any-time prediction task, we introduce a framework to handle emerging new classes called One-Pass Class Incremental Learning (OPCIL). The central issue in OPCIL is: how to effectively adapt a classifier of existing classes to incorporate emerging new classes. We call it the new class adaptation issue, and propose a new approach to address it, which requires only one new class instance. The key is to generate pseudo instances which are optimized to satisfy properties that produce a good discriminative classifier. We provide the necessary propertiesand optimization procedures required to address this issue. Experiments validate the effectiveness of this approach.
[Adaptation models, pattern classification, class incremental learning, one-pass class incremental learning, any-time prediction task, class instance, class adaptation, one-pass learning, classification tasks, New class adaptation, Optimization, Dispersion, Learning systems, Training, optimisation, pseudoinstances optimization, fixed class set, Data models, instance generation, OPCIL, learning (artificial intelligence)]
[Publisher's information]
2017 IEEE International Conference on Data Mining
None
2017
Provides a listing of current committee members and society officers.
[]
Efficient discovery of time series motifs with large length range in million scale time series
2017 IEEE International Conference on Data Mining
None
2017
Detecting repeated variable-length patterns, also called variable-length motifs, has received a great amount of attention in recent years. Current state-of-the-art algorithm utilizes fixed-length motif discovery algorithm as a subroutine to enumerate variable-length motifs. As a result, it may take hours or days to execute when enumeration range is large. In this work, we introduce an approximate algorithm called HierarchIcal based Motif Enumeration (HIME) to detect variable-length motifs with a large enumeration range in million-scale time series. We show in the experiments that the scalability of the proposed algorithm is significantly better than that of the state-of-the-art algorithm. Moreover, the motif length range detected by HIME is considerably larger than previous sequence-matching based approximate variable-length motif discovery approach. We demonstrate that HIME can efficiently detect meaningful variable-length motifs in long, real world time series.
[approximation theory, variable-length patterns, fixed-length motif discovery algorithm, Scalability, Time series analysis, approximate variable-length motif discovery approach, hierarchical based motif enumeration, time series, motif length range, Grammar, Data mining, time series motifs, Task analysis, enumeration range, sequence-matching, approximate algorithm, algorithm theory, HIME, Approximation algorithms, million scale time series]
