2981
PILOT: a prescription for program performance measurement
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
Application performance measurement and evaluation has developed much more slowly than its sister fields of system performance evaluation and software developments. Progress has been impeded by a shortage of measurement tools designed specifically for programmers, and inadequate detail in the output of existing tools for diagnosing performance problems. The authors propose features that would correct these deficiencies and offer a tool called PILOT to support the claim that a tool embodying those features can be built and would be well received by the software community.<<ETX>>
[program performance measurement, Instruments, Software performance, performance evaluation, Time measurement, Application software, Programming profession, measurement tools, PILOT, Runtime, software community, System performance, User interfaces, system performance evaluation, Hardware, software engineering, software developments, programmers, Impedance]
A Model For The Reuse Of Software Design Information
11th International Conference on Software Engineering
None
1989
This paper presents a general model for the representation and manipulation of module level software design information, leading to the effective reuse of software design information across different programming languages. Language independent design documents are represented as ASCII files containing tagged design information sufficient for the construction of a compilable program architecture. The resulting architecture is composed of documented module stubs which describe calling relationships, parameters, functional descriptions, and algorithms characteristic of the architecture. No executable code is included in the compilable modules. Frameworks of tagged locations in language dependent standard module templates are matched against corresponding tags in the ASCII design files, effectively blending language dependent and independent information into a compilable stub architecture. The process is reversible in that a language independent design file can be generated from am architecture in the standard tagged format, thus supporting the movement of design information across different programming languages.
[Algorithm design and analysis, Computer languages, Software design, Software libraries, Costs, Handicapped aids, Computer architecture, Permission, Programming, Marine vehicles]
Implementing faceted classification for software reuse
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
Experience with the development, implementation, and deployment of reuse library technology is reported. The focus is on organizing software collections for reuse using faceted classifications. Briefly described are the successfully GTE Data Services' Asset Management Program and the steps taken at Contel for furthering reuse technology. The technology developed for reuse libraries is presented, followed by a description of how it was transferred. The experience described indicates that reuse library technology is available and transferable, and that it definitely has a positive financial impact on the organization implementing it.<<ETX>>
[Vocabulary, Text analysis, software reuse, reuse library technology, Asset Management Program, Information retrieval, Asset management, classification, faceted classification, Organizing, Software libraries, Automatic control, software reusability, Software systems, subroutines, Standards development, Software reusability, Contel]
Extending the Potts and Bruns model for recording design rationale
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
An extension of the model proposed by C. Potts and G. Bruns (1988) for recording design rationale is presented. The extension consists of enriching the internal structure of justification in the Potts and Bruns model by making explicit the goals presupposed by arguments, the relations among arguments, and the first-class nature of these relations. The author describes the Potts and Bruns model briefly. A language whose underlying model extends the Potts and Bruns model and a system that supports the use of this language are presented. Related studies on recording design rationale are briefly discussed. The limitations of the language presented are discussed as further research problems.<<ETX>>
[Process design, Design methodology, Laboratories, language, Potts model, History, Hypertext systems, Software design, internal structure, software engineering, arguments, justification, Problem-solving, Joining processes, design rationale recording, Bruns model, Disk recording, Software engineering]
Productivity Analysis Of Software Development With An Integrated CASE Tool
International Conference on Software Engineering
None
1992
It has been expected that CASE tools reduce programming efforts and increase development productivity. However, little has been reported on the analysis of the effect on productivity using quantitative data. This paper discusses productivity improvement through the use of an integrated CASE tool system named EAGLE (Effective Approach to Achieving High Leve Software Productivity), as shown by various data collected in Hitachi from 1980 through 1990. We have evaluated productivity by using three metrics, 1)program generation rate using reusable program skeletons and components, 2)fault density at two test phase, and 3)learning curve for the education of inexperienced programmers. We will show that productivity have been improved by the various facilities of EAGLE.
[Productivity, Educational programs, Computer aided software engineering, Computer aided manufacturing, Educational products, Systems engineering and theory, Software tools, Programming profession, Information systems, Business]
Reliable software and communication: software quality, reliability, and safety
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Examines the software development process and suggests opportunities for improving the process by using a combination of statistical and other process control techniques. Each phase of the software process affects the ultimate quality, reliability, and safety of the software. Control of the process, supported by appropriate tools to collect and analyze data, is essential to improvement of the software product. Since the ability to observe, control, and improve software depends on the ability to measure and analyze data drawn from the software process, data collection is central to the approach. Detailed data about each of the subprocesses are needed, along with tools to measure and analyze the data. Statistical process control techniques, besides improving system reliability, can produce a substantial economic gain in the software development process. The views are based upon experiences with large telecommunications systems.<<ETX>>
[software development process, Data analysis, telecommunications systems, software reliability, Process control, Communication system control, software safety, software product improvement, Programming, system reliability, Software safety, software quality, data collection, statistical process control, Centralized control, economic gain, telecommunications computing, safety, Software quality, Software measurement, Reliability, Software tools]
An instrumented approach to improving software quality through formal technical review
Proceedings of 16th International Conference on Software Engineering
None
1994
Formal technical review (FTR) is an essential component of all software quality assessment, assurance and improvement techniques. However, current FTR practice leads to significant expense, clerical overhead, group process obstacles, and research methodology problems. CSRS is an instrumented, computer-supported cooperative work environment for formal technical review. CSRS addresses problems in the practice of FTR by providing computer support for both the process and products of FTR. CSRS also addresses problems in research on FTR through instrumentation supporting fine-grained, high quality data collection and analysis. This paper describes CSRS, a computer-mediated review method called FTArm, and selected findings from their use to explore issues in formal technical review.<<ETX>>
[research methodology problem, computer-mediated review method, Humans, Programming, software quality, fine-grained high quality data collection, groupware, software quality assurance, clerical overhead, software tools, formal technical review, Capability maturity model, Probes, Testing, group process obstacles, Data analysis, data analysis, Instruments, Collaborative software, expense, CSRS, FTArm, computer-supported cooperative work, systems analysis, software quality assessment, Software quality, FTR practice, Software engineering]
A Compact Petri Net Representation for Concurrent Programs
1995 17th International Conference on Software Engineering
None
1995
This paper presents a compact Petri net representation that is efficient to construct for concurrent programs that use explicit tasking and rendezvous style communication. These Petri nets are based on task interaction graphs and are called TIG-based Petri nets (TPN)s. They form a compact representation by summarizing the effects of large regions of a program and making useful information about those regions available for program analysis. We present a flexible framework for checking a variety of properties of concurrent programs using the reachability graph generated from a TPN. We present experimental results that demonstrate the benefit of TPNs over alternate Petri net representations and discuss the applicability of Petri net reduction techniques to TPNs.
[Software engineering]
Assertional reasoning about pairwise transient interactions in mobile computing
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Mobile computing represents a major point of departure from the traditional distributed computing paradigm. The potentially very large number of independent computing units, a decoupled computing style, frequent disconnections, continuous position changes, and the location-dependent nature of the behavior and communication patterns of the individual components present designers with unprecedented challenges in the areas of modularity and dependability. The paper describes two ideas regarding a modular approach to specifying and reasoning about mobile computing. The novelty of our approach rests with the notion of allowing transient interactions among programs which move in space. We restrict our concern to pairwise interactions involving variable sharing and action synchronization. The motivation behind the transient nature of the interactions comes from the fact that components can communicate with each other only when they are within a certain range. The notation we propose is meant to simplify the writing of mobile applications and is a direct extension of that used in UNITY. Reasoning about mobile computations relies on the UNITY proof logic.
[Military computing, distributed processing, UNITY proof logic, Mobile communication, Distributed computing, distributed computing, location-dependent communication patterns, pairwise transient interactions, mobile computing, modularity, network operating systems, location-dependent behavior, Personal digital assistants, Logic devices, transient program interactions, mobile radio, continuous position changes, Peer to peer computing, dependability, computer networks, decoupled computing style, action synchronization, Routing, assertional reasoning, synchronisation, Computer science, variable sharing, Writing, independent computing units, Mobile computing, disconnections]
Assessing software review meetings: A controlled experimental study using CSRS
Proceedings of the
None
1997
false
[Process design, Formal technical review, Software reviews, Costs, Collaborative software, Time to market, Software quality, Inspection, Permission, experimental study, CSRS, inspection]
Software requirements negotiation: some lessons learned
Proceedings of the 20th International Conference on Software Engineering
None
1998
Negotiating requirements is one of the first steps in any software system life cycle, but its results have probably the most significant impact on the system's value. However, the processes of requirements negotiation are not well understood. We have had the opportunity to capture and analyze requirements negotiation behavior for groups of projects developing library multimedia archive systems, using an instrumented version of the USC WinWin groupware system for requirements negotiation. Some of the more illuminating results were: most stakeholder Win Conditions were noncontroversial (were not involved in issues); negotiation activity varied by stakeholder role; LCO package quality (measured by grading criteria) could be predicted by negotiation attributes; and WinWin increased cooperativeness, reduced friction, and helped focus on key issues.
[Collaborative software, Multimedia systems, Instruments, Taxonomy, library multimedia archive systems, library automation, system value, multimedia computing, formal specification, system life cycle, Computer science, software requirements negotiation, WinWin, Software libraries, groupware system, LCO package quality, systems analysis, groupware, Packaging, grading criteria, Collaborative work, Software systems, software projects, Software engineering]
Splitting the organization and integrating the code: Conway's law revisited
Proceedings of the 1999 International Conference on Software Engineering
None
1999
It is widely acknowledged that coordination of large scale software development is an extremely difficult and persistent problem. Since the structure of the code mirrors the structure of the organization, one might expect that splitting the organization across time zones, cultures, and (natural) languages would make it difficult to assemble the components. This paper presents a case study of what indeed turned out to be the most difficult part of a geographically distributed software project, i.e., integration. Coordination problems were greatly exaggerated across sites, largely because of the breakdown of informal communication channels. The results imply that multi-site development can benefit to some extent from stable plans, processes, and specifications. The inherently unpredictable aspects of projects, however, require communication channels that can be invoked spontaneously, by developers, as needed. These results shed light on the problems and mechanisms underlying the coordination needs of development projects generally, be they co-located or distributed.
[organizational splitting, geographically distributed software project, Programming, specifications, informal communication channels, Permission, Large-scale systems, Mirrors, Assembly, time zones, code integration, Conway's law, project management, processes, Electric breakdown, Collaborative software, large scale software development coordination, software development management, International collaboration, Pattern recognition, plans, Communication channels, cultures, component assembly, natural languages, multi-site development]
Supporting diversity with component frameworks as architectural elements
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The author describes personal experience with component frameworks within a family architecture for a medical imaging product family. The component frameworks are handled as an integral part of the architectural approach and are an important means to support diversity in the functionality provided by the individual family members. The paper focuses on a particular kind of component framework that has been applied throughout the medical imaging product family. This kind of framework is useful when the various family members are based on the same concepts and the diversity is formed by the differences in the specific instances of these concepts that are present in the family members. These component frameworks have a number of similarities, allowing a standardised approach to their development. They support the division of the system into a generic architectural skeleton, which can be extended with plug-ins to realise specific family members, each with their own set of features.
[architectural elements, Costs, Laboratories, product family architecture, Time to market, component frameworks, Software safety, Security, software architecture, component based development, Embedded system, Permission, Skeleton, medical image processing, Biomedical imaging, Testing, family members, object-oriented programming, plug-ins, standardised approach, family architecture, medical imaging product family, architectural approach, software reusability, embedded system markets, generic architectural skeleton]
CodeWeb: data mining library reuse patterns
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Developers learn to use a software library not just from its documentation but also from toy examples and existing real-life application code (e.g. by using grep). The CodeWeb tool takes this simple idea further by a deeper analysis of a large collection of applications to see what characteristic usage of the library is like. We demonstrate the tool by showing how the KDE core libraries are used in real-life KDE applications (KDE is a graphical desktop environment for UNIX). Moreover, we look at a recently-developed feature that helps software developers port an application from an old version of a library to a new one.
[Unix, characteristic usage, graphical desktop environment, data mining, application porting, KDE core libraries, software library versions, Data mining, Open source software, software libraries, software tools, toy examples, grep, Documentation, software library reuse patterns, Application software, Association rules, software application collection analysis, UNIX, Computer science, configuration management, software portability, application code, Software libraries, CodeWeb tool, software reusability, Australia, programming environments]
Agent-based tactics for goal-oriented requirements elaboration
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Goal orientation is an increasingly recognized paradigm for eliciting, structuring, analyzing and documenting system requirements. Goals are statements of intent ranging from high-level, strategic concerns to low-level, technical requirements on the software-to-be and assumptions on its environment. Achieving goals require the cooperation of agents such as software components, input/output devices and human agents. The assignment of responsibilities for goals to agents is a critical decision in the requirements engineering process as alternative agent assignments define alternative system proposals. The paper describes a systematic technique to support the process of refining goals, identifying agents, and exploring alternative responsibility assignments. The underlying principles are to refine goals until they are assignable to single agents, and to assign a goal to an agent only if the agent can realize the goal. There are various reasons why a goal may not be realizable by an agent, e.g., the goal may refer to variables that are not monitorable or controllable by the agent. The notion of goal realizability is first defined on formal grounds; it provides a basis for identifying a complete taxonomy of realizability problems. From this taxonomy we systematically derive a catalog of tactics for refining goals and identifying agents so as to resolve realizability problems. Each tactics corresponds to the application of a formal refinement pattern that relieves the specifier from verifying the correctness of refinements in temporal logic. Our techniques have been used in two case studies of significant size; excerpts are shown to illustrate the main ideas.
[Taxonomy, Humans, Documentation, temporal logic, Software safety, Proposals, formal specification, software agents, Aerospace control, requirements engineering, goals to agents, formal verification, alternative agent assignments, Permission, Air transportation, Logic, software components, Monitoring]
A framework for component deployment testing
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Component-based development is the emerging paradigm in software production, though several challenges still slow down its full taking up. In particular, the "component trust problem" refers to how adequate guarantees and documentation about a component's behaviour can be transferred from the component developer to its potential users. The capability to test a component when deployed within the target application environment can help establish the compliance of a candidate component to the customer's expectations and certainly contributes to "increase trust". To this purpose, we propose the CDT framework for Component Deployment Testing. CDT provides the customer with both a technique to early specify a deployment test suite and an environment for running and reusing the specified tests on any component implementation. The framework can also be used to deliver the component developer's test suite and to later re-execute it. The central feature of CDT is the complete decoupling between the specification of the tests and the component implementation.
[test specification, Java, object-oriented programming, program testing, formal specification, software production, Testing, component-based development, component deployment testing]
Tutorial: an overview of UML 2.0
Proceedings. 26th International Conference on Software Engineering
None
2004
This paper covers the salient aspects of the first major revision of the Unified Modeling Language - UML 2.0. In this brief summary, we briefly review some of the main points covered in the paper.
[Tutorial, Automation, Unified Modeling Language, Unified modeling language, Programming, reviews, Computer architecture, specification languages, Computer industry, Software standards, Performance analysis, Standards development, UML 2.0, Context modeling]
Dependability assessment of software-based systems: state of the art
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
This paper presents a personal and rather selective view of the state of the art of some aspects of dependability assessment for software-based systems. This short note gives a brief outline of the issues that the author addresses.
[Statistical analysis, program verification, software reliability, Stochastic processes, software safety, probability, Probability, software dependability assessment, Software reliability, Security, security of data, Bayesian methods, software-based systems, Battery powered vehicles, Measurement techniques, Safety, Bayes methods, software security, Software engineering]
Sequential Circuits for Relational Analysis
29th International Conference on Software Engineering
None
2007
The alloy tool-set has been gaining popularity as an alternative to traditional manual testing and checking for design correctness. Alloy uses a first-order relational logic for modeling designs. The alloy analyzer translates alloy formulas for a given scope, i.e., a bound on the universe of discourse, to Boolean formulas in conjunctive normal form (CNF), which are subsequently checked using prepositional satisfiability solvers. We present SERA, a novel algorithm that compiles a relational logic formula for a given scope to a sequential circuit. There are two key advantages of sequential circuits: they form a more succinct representation than CNF formulas, sometimes by several orders of magnitude. Also sequential circuits are amenable to a range of powerful automatic analysis techniques that have no counterparts for CNF formulas. Our experiments show that SERA, used in conjunction with a sequential circuit analyzer, can check formulas for scopes that are an order of magnitude higher than those feasible with the alloy analyzer.
[System testing, program verification, Alloy tool-set, Manuals, Logic design, Encoding, Circuit analysis, Circuit testing, conjunctive normal form, Sequential analysis, Boolean formulas, Boolean functions, relational analysis, first-order relational logic, sequential circuits, Software systems, Sequential circuits, Performance analysis, software tools, relational logic formula]
Using components for architecture-based management
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Components are widely used for managing distributed applications because they not only capture the software architecture of managed applications as an assembly of components but also permit to dynamically adapt these applications to changing environments. Following this approach, our practical experience in the JADE environment about developing an autonomic repair management service with a self-healing behavior shows novel requirements on reflective component models for architecture-based management systems. First, we have identified five essential runtime abstractions that a component model must include in order to efficiently support an autonomic repair service. Second, our experience suggests that traditional reflective component models should be extended to allow specializing meta-operations. Third, our experience also shows that a meta-data checkpointing capability is best-suited for meta-data recovery after failures. We demonstrate the soundness of these findings in several ways. We applied the difficult problem of autonomic repair to both J2EE and JMS middleware. We further stressed our algorithms and mechanisms by applying them recursively towards gaining a self-healing property for the repair service itself. Although our experience was done in the JADE context, using the FRACTAL component model, we believe our findings to be general to architecture-based management systems using reflective component models.
[Checkpointing, checkpointing, JMS middleware, self-healing property, Fractals, Environmental management, runtime abstractions, software architecture, Runtime, Software architecture, JADE environment, Assembly, middleware, architecture-based management systems, meta-data checkpointing capability, architecture, meta data, FRACTAL component model, autonomic repair management service, Mechanical factors, Application software, Middleware, reflective component models, component, meta-data recovery, J2EE middleware, self-healing, Context modeling]
Predicting faults using the complexity of code changes
2009 IEEE 31st International Conference on Software Engineering
None
2009
Predicting the incidence of faults in code has been commonly associated with measuring complexity. In this paper, we propose complexity metrics that are based on the code change process instead of on the code. We conjecture that a complex code change process negatively affects its product, i.e., the software system. We validate our hypothesis empirically through a case study using data derived from the change history for six large open source projects. Our case study shows that our change complexity metrics are better predictors of fault potential in comparison to other well-known historical predictors of faults, i.e., prior modifications and prior faults.
[program verification, program diagnostics, Project management, Predictive models, Entropy, History, Delay, software fault tolerance, code changes complexity, fault incidence, Software systems, Lab-on-a-chip, complexity measurement, Software measurement, fault prediction, complexity metrics, code change process, Information theory, software metrics]
Transparent combination of expert and measurement data for defect prediction: an industrial case study
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Defining strategies on how to perform quality assurance (QA) and how to control such activities is a challenging task for organizations developing or maintaining software and software-intensive systems. Planning and adjusting QA activities could benefit from accurate estimations of the expected defect content of relevant artifacts and the effectiveness of important quality assurance activities. Combining expert opinion with commonly available measurement data in a hybrid way promises to overcome the weaknesses of purely data-driven or purely expert-based estimation methods. This article presents a case study of the hybrid estimation method HyDEEP for estimating defect content and QA effectiveness in the telecommunication domain. The specific focus of this case study is the use of the method for gaining quantitative predictions. This aspect has not been empirically analyzed in previous work. Among other things, the results show that for defect content estimation, the method performs significantly better statistically than purely data-based methods, with a relative error of 0.3 on average (MMRE).
[expert systems, QA activity, hybrid estimation, telecommunication domain, Predictive models, software quality, measurement data, defect prediction, defect content, Context, effectiveness, data analysis, hybrid estimation method, quality control, Estimation, Equations, expert-based estimation method, defect content estimation, quality assurance, data-based method, HyDEEP, expert data, Data models, Planning, Context modeling, software intensive system]
Static extraction of program configuration options
2011 33rd International Conference on Software Engineering
None
2011
Many programs use a key-value model for configuration options. We examined how this model is used in seven open source Java projects totaling over a million lines of code. We present a static analysis that extracts a list of configuration options for a program. Our analysis finds 95% of the options read by the programs in our sample, making it more complete than existing documentation. Most configuration options we saw fall into a small number of types. A dozen types cover 90% of options. We present a second analysis that exploits this fact, inferring a type for most options. Together, these analyses enable more visibility into program configuration, helping reduce the burden of configuration documentation and configuration debugging.
[Context, Java, program debugging, program diagnostics, public domain software, system documentation, static extraction, program configuration options, key-value model, configuration, Optimized production technology, Documentation, documentation, static analysis, experiences, Open source software, configuration management, configuration documentation, System software, open source Java projects, Facebook, configuration debugging]
Recovering traceability links between an API and its learning resources
2012 34th International Conference on Software Engineering
None
2012
Large frameworks and libraries require extensive developer learning resources, such as documentation and mailing lists, to be useful. Maintaining these learning resources is challenging partly because they are not explicitly linked to the frameworks' API, and changes in the API are not reflected in the learning resources. Automatically recovering traceability links between an API and learning resources is notoriously difficult due to the inherent ambiguity of unstructured natural language. Code elements mentioned in documents are rarely fully qualified, so readers need to understand the context in which a code element is mentioned. We propose a technique that identifies code-like terms in documents and links these terms to specific code elements in an API, such as methods. In an evaluation study with four open source systems, we found that our technique had an average recall and precision of 96%.
[Context, Java, application program interfaces, system documentation, Documentation, documentation, HTML, traceability link, learning resources, software libraries, XML, unstructured natural language, mailing list, Libraries, API, code element, Joining processes]
GuideArch: Guiding the exploration of architectural solution space under uncertainty
2013 35th International Conference on Software Engineering
None
2013
A system's early architectural decisions impact its properties (e.g., scalability, dependability) as well as stakeholder concerns (e.g., cost, time to delivery). Choices made early on are both difficult and costly to change, and thus it is paramount that the engineer gets them &#x201C;right&#x201D;. This leads to a paradox, as in early design, the engineer is often forced to make these decisions under uncertainty, i.e., not knowing the precise impact of those decisions on the various concerns. How could the engineer make the &#x201C;right&#x201D; choices in such circumstances? This is precisely the question we have tackled in this paper. We present GuideArch, a framework aimed at quantitative exploration of the architectural solution space under uncertainty. It provides techniques founded on fuzzy math that help the engineer with making informed decisions.
[Decision Making, Uncertainty, Software Architecture, Batteries, Synthetic aperture sonar, architectural solution space, software architecture, Computer architecture, decision making, GuideArch, Hardware, Time factors, quantitative exploration, Software engineering]
Automatic and Continuous Software Architecture Validation
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software systems tend to suffer from architectural problems as they are being developed. While modern software development methodologies such as Agile and Dev-Ops suggest different ways of assuring code quality, very little attention is paid to maintaining high quality of the architecture of the evolving systems. By detecting and alerting about violations of the intended software architecture, one can often avoid code-level bad smells such as spaghetti code. Typically, if one wants to reason about the software architecture, the burden of first defining the intended architecture falls on the developer's shoulders. This includes definition of valid and invalid dependencies between software components. However, the developers are seldom familiar with the entire software system, which makes this task difficult, time consuming and error-prone. We propose and implement a solution for automatic detection of architectural violations in software artifacts. The solution, which utilizes a number of predefined and user-defined patterns, does not require prior knowledge of the system or its intended architecture. We propose to leverage this solution as part of the nightly build process used by development teams, thus achieving continuous automatic validation of the system's software architecture. As we show in multiple open-source and proprietary cases, a small set of predefined patterns can detect architectural violations as they are introduced over the course of development, and also capture deterioration in existing architectural problems. By evaluating the tool on relatively large open-source projects, we also validate its scalability and practical applicability to large software systems.
[source code (software), predefined patterns, program verification, software systems, Production facilities, software quality, systems architecture, code quality, software architecture, Software architecture, spaghetti code, software artifacts, Semantics, Computer architecture, continuous software architecture validation, software components, object-oriented programming, quality maintenance, agile software development methodologies, code-level, Dev-Ops, architectural problems, architectural violations automatic detection, Software systems, user-defined patterns, development teams, automatic software architecture validation]
Toward a Framework for Detecting Privacy Policy Violations in Android Application Code
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Mobile applications frequently access sensitive personal information to meet user or business requirements. Because such information is sensitive in general, regulators increasingly require mobile-app developers to publish privacy policies that describe what information is collected. Furthermore, regulators have fined companies when these policies are inconsistent with the actual data practices of mobile apps. To help mobile-app developers check their privacy policies against their apps' code for consistency, we propose a semi-automated framework that consists of a policy terminology- API method map that links policy phrases to API methods that produce sensitive information, and information flow analysis to detect misalignments. We present an implementation of our framework based on a privacy-policy-phrase ontology and a collection of map- pings from API methods to policy phrases. Our empirical evaluation on 477 top Android apps discovered 341 potential privacy policy violations.
[Google, application program interfaces, Android Applications, Natural languages, Humanoid robots, Ontologies, Mobile communication, Android application code, privacy policy violation detection, Privacy, Android (operating system), mobile computing, API method map, map-ping collection, mobile applications, Privacy Policies, data privacy, semiautomated framework, Androids, information flow analysis, Violation Detection, privacy-policy-phrase ontology]
Machine Learning-Based Detection of Open Source License Exceptions
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
From a legal perspective, software licenses govern the redistribution, reuse, and modification of software as both source and binary code. Free and Open Source Software (FOSS) licenses vary in the degree to which they are permissive or restrictive in allowing redistribution or modification under licenses different from the original one(s). In certain cases, developers may modify the license by appending to it an exception to specifically allow reuse or modification under a particular condition. These exceptions are an important factor to consider for license compliance analysis since they modify the standard (and widely understood) terms of the original license. In this work, we first perform a large-scale empirical study on the change history of over 51K FOSS systems aimed at quantitatively investigating the prevalence of known license exceptions and identifying new ones. Subsequently, we performed a study on the detection of license exceptions by relying on machine learning. We evaluated the license exception classification with four different supervised learners and sensitivity analysis. Finally, we present a categorization of license exceptions and explain their implications.
[source code (software), FOSS licenses, pattern classification, Classifiers, supervised learners, license exception classification, software reuse, public domain software, Software Licenses, sensitivity analysis, source code, open source license exceptions, Empirical Studies, license compliance analysis, free and open source software, software redistribution, machine learning-based detection, software modification, software reusability, binary code, software licenses, learning (artificial intelligence), Software engineering]
Modeling software tools with ICON
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The author describes a software test automation tool, a powerful programming language, and the software development process that resulted when these tools were combined. The first model of the Buster automated testing system was conceived, designed, and implemented ahead of schedule in less than six months, complete with many features and components. The Buster system provides a test-information subsystem with facilities for multiproject test sharing, per-project test storage and planning, and test downloading for lab use. A separate test execution facility is also included that features test-result logging, a results database, and per-session I/O recording. The software modeling technique that was used to create the Buster test system is an idea that can be used to produce reliable low-cost software in many applications. The model can be used by customers as it is slowly evolved into a finished product.<<ETX>>
[Software testing, System testing, software development process, program testing, Programming, Buster automated testing system, multiproject test sharing, software engineering, programming language, software tools, ICON, Automation, test-information subsystem, test downloading, lab use, Spatial databases, Application software, Power system modeling, I/O recording, Computer languages, Automatic testing, results database, test automation tool, reliable software, Software tools]
Validating the TAME resource data model
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The authors present a conceptual model of software development resource data and validates the model by references to the published literatures on necessary resource data for development support environments. The conceptual model was developed using a top-down strategy. A resource data model is a prerequisite to the development of integrated project support environments which aim to assist in the processes of resource estimation, evaluation, and control. The model proposed is a four-dimensional view of resources which can be used for resource estimation, utilization, and review. The model is validated by reference to three publications on resource databases, and the implications of the model arising out of these comparisons is discussed.<<ETX>>
[Costs, software development resource data, NASA, Programming, Educational institutions, development support environments, TAME resource data model, Databases, conceptual model, Data models, software engineering, top-down strategy, Software measurement, Software tools, programming environments, integrated project support environments, Software engineering, Testing]
An operational requirement description model for open systems
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The author presents a conceptual model, REMOS, for incomplete requirement descriptions. The model is especially designed for supporting the requirement specification and the analysis of open systems. An analysis of existing models and languages shows the main problem in requirement engineering to be the harmony between a well-defined basic model and a convenient language. REMOS combines the complex requirements of open systems with the basic characteristics of transaction-oriented systems, which are fault-tolerant and offer security and privacy mechanisms. REMOS and the applicative language RELOS take advantage of transaction properties. REMOS aids users with making their requirements more clear, and RELOS offers a medium for requirement definition.<<ETX>>
[requirement engineering, open systems, requirement definition, Data security, Communities, Data engineering, applicative language, privacy mechanisms, Application software, transaction-oriented systems, requirement specification, Computer science, Privacy, security, Open systems, specification languages, Broadcasting, Software systems, RELOS, software engineering, REMOS, Contracts, requirement description model, fault-tolerant]
Software process modeling: a behavioral approach
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
An approach is presented to software process modeling which is based on behavior descriptions of software development activities. The use of behavioral descriptions makes it possible to describe the software process at any desired level of abstraction and, therefore, assists in accommodating aspects of the process which are poorly understood. This approach also provides the ability to reason about the software process and is sufficiently rigorous to provide a basis for structuring automated software environments. An overview of the model is presented, followed by a formal definition. Examples are given to illustrate the application of the model to existing software processes and software methods. Finally, the implications of the model for automated software environments are discussed.<<ETX>>
[software development activities, software process modeling, abstraction, Programming, Application software, formal definition, behavior descriptions, Vehicles, Embedded software, software environments, Software design, Computer architecture, Software systems, software engineering, Software tools, Artificial intelligence, programming environments, Software engineering]
Extension and software development
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The design and implementation of an extension mechanism is described that allows users to augment a software system without modifying the underlying source code. The authors demonstrate how the availability of this flexible mechanism alters not only the enhancement phase of the life cycle, but the design and implementation phases as well.<<ETX>>
[Costs, software reuse, software development, life cycle, Buildings, source code, Collision mitigation, Application software, Computer science, Computer languages, Runtime, flexible mechanism, Software systems, software engineering, Dynamic programming, Virtual manufacturing, extension mechanism]
Recording the reasons for design decisions
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The authors outline a generic model for representing design deliberation and the relation between deliberation and the generation of method-specific artifacts. A design history is regarded as a network consisting of artifacts and deliberation nodes. Artifacts represent specifications or design documents. Deliberation nodes represent issues, alternatives or justifications. Existing artifacts give rise to issue about the evolving design, an alternative is one of several positions that respond to the issue (perhaps calling for the creation or modification of an artifact), and a justification is a statement giving the reasons for and against the related alternative. The model is applied to the development of a text formatter. The example development is represented in hypertext and as a Prolog database, the two representations being shown to complement each other. The authors conclude with a discussion of the relation between this model and other work and of the implications for tool support and methods.<<ETX>>
[Process design, design documents, Design methodology, Prolog database, Programming, History, database management systems, specifications, design decisions, tool support, hypertext, Databases, Production, specification languages, text formatter, generic model, software engineering, design history, DBMS, method-specific artifacts]
A review of automated debugging systems: knowledge, strategies and techniques
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The authors propose a classification of debugging knowledge, and a description of the corresponding knowledge representation in the systems. Then they propose a classification of global debugging strategies used in the systems, and a description of the corresponding techniques. They assess the identified strategies from a real-world program development point of view. The knowledge types identified are: (1) knowledge of the intended program; (2) knowledge of the actual program; (3) understanding of the programming language; (4) general programming expertise; (5) knowledge of the application domain; (6) knowledge of bugs; and (7) knowledge of debugging methods. The strategies identified are: (1) filtering; (2) checking computational equivalence of intended program and actual one; (3) checking the well-formedness of actual program; and (4) recognizing stereotyped errors.<<ETX>>
[knowledge engineering, program debugging, application domain, automated debugging systems, stereotyped errors, actual program, Prototypes, computational equivalence, software engineering, programming language, filtering, knowledge, Filtering, Debugging, Knowledge representation, well-formedness, Programming profession, intended program, Computer languages, real-world program development, debugging methods, Computer bugs, knowledge representation, Error correction, programming expertise]
A programming environment supporting reuse of object-oriented software
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The authors have developed a programming environment for object-oriented programming. This environment supports reuse of classes, especially retrieval of them with an expert system. The user can find classes and methods by describing the features of objects and operations according to an object model proposed. The target programming language is MOMO, which has been developed to implement the object model. A description is given of the retrieval part of the environment.<<ETX>>
[MOMO, expert system, expert systems, reuse, object-oriented programming, Terminology, Object oriented modeling, object-oriented software, methods, classes, target programming language, Programming profession, Programming environments, Information science, Computer languages, programming environment, Software libraries, retrieval, User interfaces, software engineering, object model, Object oriented programming, Expert systems, programming environments]
High level specification of concurrency control in distributed database systems
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The authors propose a high-level specification that is based on an object-oriented model of concurrency control algorithms. Concurrency control algorithms are specified in a high-level fashion without losing their formality. Using the object-oriented model, objects are individually specified. Therefore, the specification of a concurrency control algorithm consists of the specifications of objects and their interactions.<<ETX>>
[Object oriented modeling, Scattering, object-oriented model, Control systems, formality, Concurrency control, Transaction databases, specifications, parallel programming, distributed database systems, Zirconium, Distributed databases, concurrency control, distributed databases, specification languages, Distributed control, Database systems, Contracts]
KDA-a tool for automatic design evaluation and refinement using the blackboard model of control
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The author discusses the use of the blackboard model of control for design evaluation and refinement, and describes KDA (knowledge-based design assistant), which has been built to demonstrate that applicability of this idea. The tool's architecture and example sessions are included and explained. The examples indicate the applicability of this architecture to the evaluation and refinement problem.<<ETX>>
[Automatic programming, expert systems, blackboard model, Humans, Data structures, Educational institutions, KDA, Computer science, Software design, Refining, Computer architecture, Automatic control, knowledge-based design assistant, automatic design evaluation, refinement, software tools, Software engineering]
Software engineering for distributed applications: the DESIGN project
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The DESIGN project combines a set of several approaches to software engineering for distributed applications. Distributed applications may thereby consist of a large, varying number of interacting processes. Specific problems encountered with the development of such distributed applications are not suitably reflected by known programming languages and software engineering environments. The DESIGN system in its current version integrates consistent approaches specifically suited for distributed-application development. These approaches pertain to the areas of language support, performance prediction/rapid prototyping, and project support environment. Most parts of the DESIGN system have been implemented and successfully applied to first-sample distributed applications.<<ETX>>
[DESIGN project, project support environment, Software prototyping, Project management, rapid prototyping, Standardization, distributed processing, language support, Topology, Application software, distributed applications, performance prediction, Computer languages, Design engineering, Runtime, Prototypes, software engineering, programming environments, Software engineering]
Modeling mutation and a vector processor
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
Mutation analysis is a software testing methodology designed to substantiate the correctness of a program Phi . The mutation approach is to induce syntactically correct changes in Phi , thereby creating a set of mutant programs. The goal of a tester is to construct a set of test data T that distinguishes the output of Phi (T) from that of all mutant programs. Test data sensitive enough to distinguish all mutant programs is deemed adequate to infer the probable correctness of Phi . An algorithm is proposed which was designed to exploit the architecture of a vector processor like the Cyber 205 or Cray X/MP. The algorithm manages the simultaneous execution of multiple mutant Fortran 77 programs. This is accomplished by viewing the execution of these mutants as a sequence of vector instructions. The algorithm promises potential to greatly increase the performance of a mutation-based testing system, as well as points towards a general method of simultaneous program execution against multiple data sets.<<ETX>>
[Software testing, Algorithm design and analysis, correctness, Cray X/MP, Fortran 77 programs, program testing, Genetic mutations, mutation approach, vector processor, software quality, parallel programming, multiple data sets, software testing methodology, syntactically correct changes, Cyber 205, software engineering, Error correction, Vector processors]
Safety verification in MURPHY using fault tree analysis
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
MURPHY is a language-dependent, experimental methodology for building safety-critical, real-time software, which will include an integrated tool set. Using Ada as an example, the authors present a technique for verifying the safety of complex, real-time software using software fault tree analysis. The templates for Ada are presented along with an example of applying the technique to an Ada program. The tools in the MURPHY tool set to aid in this type of analysis are described.<<ETX>>
[integrated tool set, Military computing, program verification, experimental methodology, fault tree analysis, Control systems, Software safety, Air traffic control, MURPHY, Aerospace control, Power engineering computing, real-time systems, safety, Military aircraft, software engineering, safety critical software, Injuries, Fault trees, Software engineering, real-time software, Ada]
Rigi: a system for programming-in-the-large
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The authors describe Rigi, a model and tool that uses a graph model and abstraction mechanisms to structure and represent the information accumulated during the development process. The objects and relationships of the graph model represent system components and their dependencies. The objects can be arranged in aggregation and generalization hierarchies. Rigi was designed to address three of the most difficult problems in the area of programming-in-the-large: the mastery of the structural complexity of large software systems, the effective presentation of development information, and the definition of procedures for checking and maintaining the completeness, consistency, and traceability of system descriptions. Thus, the major objective of Rigi is to effectively represent and manipulate the building blocks of a software system and their myriad dependencies, thereby aiding the development phases of the project.<<ETX>>
[Productivity, Costs, Navigation, Control systems, programming-in-the-large, completeness, traceability, Yarn, consistency, Programming profession, development process, dependencies, Computer science, Software design, project support, graph model, abstraction mechanisms, Packaging, structural complexity, Software systems, software engineering, building blocks, programming environments, Rigi]
Tools for real-time system design
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
A description is given of the development of a toolset that supports the specification and design of large real-time systems. The starting point is the evolution of a method suitable for use in practical environments: for example, in the specification of telecommunication systems. The characteristics that are required of a good specification method for real-time systems are discussed and a method based on the CCITT SDL notation is described which aims to satisfy these characteristics. The component parts in the toolset to support this method are described.<<ETX>>
[Real time systems, Heart, Availability, toolset, Natural languages, Laboratories, Automated highways, specification, Telecommunications, real-time system design, Design engineering, Heat engines, practical environments, real-time systems, specification languages, telecommunication systems, CCITT SDL notation, software tools, Software engineering]
Demeter: a case study of software growth through parameterized classes
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
Demeter is a system designed for the development of large software projects using a software design methodology which focuses on growing rather than building software. A description is given of the software development process as one of growth and evolution as opposed to building and rebuilding because most complex objects in the real world are grown and not built. Since software design is obviously a complex process this new paradigm may be helpful in unraveling some of the problems associated with current software design practices. Demeter begins by providing an ideal environment for the sprouting and nurturing of a seed (data dictionary) into a plant (large scale software project). In addition, through the combined use of object-oriented programming technology, and parameterized classes, Demeter provides a facility for the reuse of software which was developed in previous software projects.<<ETX>>
[parameterized classes, Computer aided software engineering, Dictionaries, reuse, Buildings, data dictionary, software growth, Educational institutions, large scale software project, software design methodology, complex objects, Computer science, Software design, Animals, object-oriented programming technology, Demeter, Software systems, software engineering, Large-scale systems, Object oriented programming, programming environments]
Enhancing program readability and comprehensibility with tools for program visualization
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
To make computer programs for comprehensible, the presentation of program source text, program documentation, and program execution needs to be enhanced. A number of techniques and tools developed to achieve these ends are described. One of these is a design for the effective presentation of source text in the C programming language using high-quality digital typography; a processor which implements the design is also given. Some experimental evidence is summarized to demonstrate that the resulting source text presentation is significantly more readable and comprehensible than the presentation conventionally used today. Brief descriptions are given of two other techniques, the development of a system of structured program documentation incorporating both text and graphics and the portrayal of program execution with colored computer animation.<<ETX>>
[Computer interfaces, Visualization, Computer aided software engineering, computer programs, program documentation, colored computer animation, program readability, program source text, system documentation, comprehensibility, program execution, structured program documentation, graphics, program visualization, Programming profession, Computer science, Computer languages, Computer graphics, Writing, C programming language, Animation, software tools, high-quality digital typography, Software engineering]
Models of software development environments
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The authors present a general model of software development environments that consist of three components: policies, mechanisms, and structures. The advantage of this formalization is that it distinguishes precisely those aspects of an environment that are useful in comparing and contrasting software development environments. They introduce four classes of models by a sociological metaphor that emphasizes scale: the individual, the family, the city, and the state models. The authors argue that there is a qualitative difference between the interactions among a small, family project and a large, city project and that this qualitative difference requires a fundamentally different model of software development environments. They illustrate the city model with Inscape/Infuse and ISTAR.<<ETX>>
[family project, sociological metaphor, Taxonomy, software development environments, policies, structures, Inscape/Infuse, Programming profession, Research and development, ISTAR, city project, Pressing, Cities and towns, Software systems, Hardware, software engineering, Software tools, Kernel, programming environments, mechanisms]
Static analysis-based program evolution support in the common Lisp framework
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The common Lisp framework (CLF) is an object-oriented environment to support the development and maintenance of programs written in the language common Lisp. A static analysis tool, which is part of CLF, supports program evolution in CLF. Since the effectiveness of the approach stems from the basic design and architecture of CLF, the author provides an overview of the relevant features of CLF. The static analysis tool asserts the static properties of program objects definitions in the CLF object base. CLF's general mechanism to add rules to the object base provides a conceptual basis respond to changes in the static properties of program objects in several interesting ways, from programming routine responses to program changes to viewing program alterations idiomatically.<<ETX>>
[object-oriented environment, Maintenance engineering, program objects definitions, Mechanical factors, program alterations, History, object base, Programming profession, Research and development, Information analysis, Programming environments, common Lisp, Computer architecture, LISP, static analysis tool, program evolution support, programming environments]
Language and visualization support for large-scale concurrency
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
SDL (shared dataspace language) is a language for writing and visualizing programs consisting of thousands of processes executing on a highly parallel multiprocessor. SDL is based on a model in which processes use powerful transactions to manipulate abstract views of a virtual content-addressable data structure called the dataspace. The process society is dynamic and supports varying degrees of process anonymity. The transactions are executed over abstract views of the dataspace. This facilitates elegant conceptualization of dataspace transformations and compact program representation. Processes and transactions enable SDL to combine elements of both large- and fine-grained concurrency. The view is a novel abstraction mechanism whose significance is derived from the fact that it allows processes to interrogate the dataspace at a level of abstraction convenient for the task they are pursuing.<<ETX>>
[Visualization, compact program representation, visualization support, parallel multiprocessor, Cultural differences, transactions, Programming profession, parallel programming, fine-grained concurrency, Concurrent computing, Computer science, virtual content-addressable data structure, Computer languages, abstraction mechanism, large-scale concurrency, specification languages, Writing, Hypercubes, shared dataspace language, data structures, software engineering, SDL, Large-scale systems, Power system reliability]
Deriving specification from requirements
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
A description is given of an approach to specification development in which the specification arises naturally through the requirements analysis process. The emerging specification is developed into a complete system description using formal transformations called high-level editing commands. Automated support for this development process within the knowledge-based specification assistant is described. This support involves applying high-level editing commands, assisting in the choice of editing commands, and tracking the effects of these commands.<<ETX>>
[Software maintenance, Automatic programming, expert systems, requirements, text editing, complete system description, knowledge-based specification assistant, Specification languages, Formal specifications, development process, Feathers, Computer bugs, Intersymbol interference, specification development, high-level editing commands, US Government, specification languages, Writing, formal transformations, software engineering, Contracts]
A method for asynchronous parallelization
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
Asynchronous parallelization assists the utilization of parallel hardware by sequential software. A sequential program is decomposed into a program network according to a given partition. Each generated program may run autonomously, and dependencies are realized by communication statements. An algorithm is presented for decomposing Pascal programs and producing a distributed-Pascal program network. A description is given of an existing tool using the presented method, and an example illustrates the method.<<ETX>>
[sequential software, Communication system control, parallel hardware, Partitioning algorithms, Pascal programs, parallel programming, distributed-Pascal program network, asynchronous parallelization, program network, partition, Hardware, software engineering, software tools, communication statements]
STATEMATE: a working environment for the development of complex reactive systems
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
A brief overview is given of the STATEMATE system, a graphical working environment, intended for the specification, analysis, design, and documentation of large and complex reactive systems, such as real-time embedded systems, control and communication systems, and interactive software. It enable a user to prepare, analyze, and debug diagrammatic, yet precise, descriptions of the system under development from three interrelated points of view, capturing, respectively, structure, functionality, and behavior. These views are represented by three graphical languages, the most intricate of which is the language of state charts used to depict reactive behavior over time. In addition to use of state charts, the main novelty of STATEMATE is in the fact that it understands the entire descriptions perfectly, to the point of being able to analyze them for crucial dynamic properties, to carry out rigorous animated executions and simulations of the described system, and to create running code automatically.<<ETX>>
[Real time systems, automatic programming, complex reactive systems, interactive software, communication systems, Communication system control, Documentation, documentation, Control systems, specification, STATEMATE system, real-time embedded systems, working environment, Embedded software, Analytical models, Communication system software, Embedded system, Software systems, Animation, programming environments, graphical languages]
m-EVES: a tool for verifying software
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
A description is given of the development of a tool for formally verifying software. The tool is called m-EVES and consists of a language, called m-Verdi, for implementing and specifying software; a logic that has been proved sound; and theorem prover, called m-NEVER, which integrates many state-of-the-art techniques drawn from the theorem proving literature. Two simple examples are used to present the fundamental ideas embodied within the system.<<ETX>>
[Software prototyping, program verification, m-EVES, Programming, Application software, Formal specifications, m-NEVER, specifying software, m-Verdi, theorem prover, Prototypes, Software quality, specification languages, Telephony, software engineering, software tools, theorem proving, Logic, verifying software, Software tools, Formal verification]
Using Raddle to design distributed systems
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
A description is given of a linguistic tool, Raddle, and its use in designing distributed systems. Raddle coordinates concurrent processes with the n-party interaction, a high-level communication primitive. Another important feature of Raddle is the team, which encapsulates communicating processes. The authors discuss their efforts to develop a complete methodology for using Raddle and briefly describe their software toolset.<<ETX>>
[linguistic tool, Computational modeling, Communication system control, distributed processing, n-party interaction, Proposals, program compilers, compilers, parallel programming, communicating processes, high-level communication primitive, Message passing, System recovery, distributed systems, software toolset, Hardware, Robustness, concurrent processes, software tools, Raddle, Software tools]
Design and test of distributed applications
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The author presents a paradigm and system that will support a programmer who is designing, programming, and testing a distributed application. The main point is that the structure introduced by the paradigm must be kept and used during design, programming, and testing. One is not helped by a programming method which introduces a structure just to break it down again. A structural model has been developed which is based on common structures in distributed systems. This model is used to describe the logical relationships between components in a system. It is also integrated into the programming environment.<<ETX>>
[Encapsulation, System testing, program testing, Debugging, Documentation, testing, distributed processing, Application software, Distributed computing, distributed applications, Programming profession, Programming environments, Information science, programming environment, design, Timing, programming environments, programming, structural model]
Assessing the quality of abstract data types written in Ada
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
A method is presented for assessing the quality of ADTs (abstract data types) in terms of cohesion and coupling. It is argued that an ADT that contains and exports only one domain and exports only operations that pertain to that domain has the best cohesive properties, and that ADTs that make neither explicit nor implicit assumptions about other ADTs in the system have the best coupling properties. Formal definitions are presented for each of the cohesion and coupling characteristics discussed. Their application to Ada packages is also investigated, and it is shown how a tool can be developed to assess the quality of an Ada package that represents an ADT.<<ETX>>
[Software maintenance, software reliability, abstract data types, software quality, Guidelines, quality, coupling, ADT, Computer architecture, Software quality, Packaging, Software systems, data structures, Quality assessment, software tools, cohesion, Software measurement, Books, Energy management, Ada]
Requirements analysis for real-time automation projects
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
A formal language for specifying and analyzing requirements is introduced. An integrated set of tools support the transition from ideas to a requirements specification. The system is implemented on an IBM PC. The special characteristics of the language and the tools are their suitability for the description of embedded real-time systems. The most useful feature of the system is the possibility of graphical input and output as an alternative of the mere textual specification. Transformation between textual and graphical representation is done by the PROREC system.<<ETX>>
[Real time systems, Robot control, microcomputer applications, Control systems, textual specification, graphical input, specification languages, requirements specification, graphical representation, Automatic control, PROREC system, software tools, Informatics, formal languages, Process control, IBM PC, Specification languages, IBM computers, Formal specifications, Robotics and automation, requirements analysis, real-time systems, embedded real-time systems, real-time automation projects, Software engineering, formal language]
Design principles behind Chrion: a UIMS for software environments
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
User interface facilities are a crucial part of the infrastructure of a software environment. The authors discuss the particular demands and constraints on a user interface management system (UIMS) for a software environment, and the relation between the architecture of the environment and the UIMS. A model for designing user interface management systems for large, extensible environments is presented. This model synthesizes several recent advances in user interfaces and specializes them to the domain of software environments. The model can be applied to a wide variety of environment contexts. A prototype implementation is described.<<ETX>>
[Software prototyping, Software performance, Human factors, extensible environments, user interface management system, user interfaces, Environmental management, software environments, Computer science, Computer architecture, User interfaces, Chrion, Aircraft, programming environments, UIMS, Assembly, Context modeling]
SDA: a novel approach to software environment design and construction
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The Software Designer's Associate (SDA) is a workstation-based collection of tools which support the description, evaluation, and comparison of software system architectural designs and cooperation among, and management of, a team of software designers. Each SDA is a specific instance of a generic facility which supports a team member's design activities, cooperation among team members, and overall team management. It provides a framework for the integration of tools supporting the use of various notations within the context of a particular set of technical and managerial methods. These tools, notations, and methods may be adapted to support the needs of a particular project or the habits of an individual developer by selecting the particular tools to be added to the generic facility. The authors describe the concept of SDAs and the cooperative, international project which is intended to lend to its realization.<<ETX>>
[Software maintenance, Software prototyping, comparison, software environment design, Software Designer's Associate, architectural designs, team management, Programming, software designers, DP management, description, Knowledge management, Computer science, evaluation, SDA, Software design, management, Disaster management, Computer industry, Workstations, Software tools, programming environments]
Modeling of data-processing software for generating and reusing their programs
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
A modeling scheme named S-model (semantic model) is proposed, which is based on a uniform object-relationship formalism. S-model combines notions from logic, set theory, and abstract syntax and covers a wide range of information on file-processing and data-structure-manipulating software. With the procedural algorithms and problem-solving capability of the S-model system, it is possible to generate and reuse programs in various fields.<<ETX>>
[Automatic programming, Logic programming, software reuse, Laboratories, Data structures, data-structure-manipulating software, set theory, Application software, S-model, semantic model, abstract syntax, Software design, data-processing software, Set theory, Inference algorithms, data structures, software engineering, file-processing, logic, Problem-solving, Artificial intelligence, object-relationship formalism]
Automatic programming for streams. II. Transformation implementation
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
For pt.I see ISCAI p.232-237, Los Angeles, USA, (August 1985). Phi NIX is an automatic programming system for writing programs which interact with external devices through temporally-ordered streams of values. Abstract specifications are stated in terms of constrains on the values of input and output streams. The target language is the Stream Machine, a language which includes concurrently executing processes communicating and synchronizing through streams. Phi NIX produces programs by repeatedly transforming abstract specifications through successively more concrete forms until concrete Stream Machine programs are produced. An example which Phi NIX has successfully implemented involves three major steps: transforming the specification into an applicative expression, transforming the applicative expression into three imperative processes, and merging the processes into a single process. Each major step involves several other transformation steps that reformulate and simplify intermediate expressions.<<ETX>>
[automatic programming, Automatic programming, Stream Machine, Merging, Communication system control, streams, values, Control systems, imperative processes, automatic programming system, abstract specifications, Well logging, target language, Petroleum, Physics, Phi NIX, specification languages, Writing, Automatic control, Concrete, applicative expression]
On the influence of scale in a distributed system
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
Scale is proposed as a primary factor influencing the architecture and implementation of distributed systems. The author uses Andrew, a distributed environment at Carnegie-Mellon University, to validate this proposition. The design of Andrew is dominated by considerations of performance, operability, and security. Caching of information and placing trust in as few machines as possible emerge as two general principles that enhance scalability. The separation of concerns made possible by specialized mechanisms is also valuable. Heterogeneity is a natural consequence of growth and anticipating it in the initial stages of system design is important. A location-transparent shared-file system considerably enhances the usability of a distributed environment.<<ETX>>
[Scalability, distributed processing, distributed system, scale, distributed environment, Network servers, security, File systems, Carnegie-Mellon University, Computer architecture, software engineering, Workstations, Contracts, operability, architecture, location-transparent shared-file system, Andrew, implementation, system design, Computer science, performance, Information security, Usability, programming environments, Software engineering]
Algebraic specification of Macintosh's QuickDraw using OBJ2
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The authors describe QuickDraw, a typical graphics package, using OBJ2, a powerful algebraic language now in the phase of experimental use as a specification language. The results show the applicability of OBJ2 to practical problem domains and the advantages of the use of formal specification techniques brings. The authors take a critical look at the design of QuickDraw; they detect incomplete procedure definitions and find imprecise the classification of procedures.<<ETX>>
[QuickDraw, Apple computers, formal specification techniques, Laboratories, Macintosh, microcomputer applications, software packages, specification languages, Cities and towns, graphics package, software engineering, specification language, Testing, OBJ2, algebraic language, Specification languages, Formal specifications, classification, Equations, Graphics, Computer languages, computer graphics, incomplete procedure definitions, Software packages, Packaging]
Heuristic software partitioning algorithms for distributed real-time applications
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The authors deal with the problems of software partition for distributed real-time applications. The software-partitioning models analyzed are those that take maximizing the efficiency in resource utilization for their objective, while observing the constraints on CPU throughput, memory space available, maximally allowed task execution time, and the order of module execution. The problem of software partition based on the models is proved to be NP-complete. There is no efficient partitioning algorithm for the models. The authors present two heuristic software-partitioning algorithms.<<ETX>>
[Real time systems, Costs, task execution time, CPU throughput, Heuristic algorithms, software partition, partitioning algorithm, Software algorithms, Software performance, distributed processing, Throughput, Partitioning algorithms, Application software, program compilers, Delay, heuristic software-partitioning algorithms, module execution, real-time systems, distributed real-time applications, NP-completeness, memory space available, Resource management, resource utilization]
Interpretation in a tool-fragment environment
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The philosophy of composing new software tools from previously created tool fragments can facilitate the development software systems. An examination is made of the extension of this philosophy to the design of program interpreters, demonstrating how the separation of interpretation into a core algorithm, value-kind definitions, and computation model allows the capture of conventional execution models, symbolic execution models, dynamic dataflow tracking, and other useful forms of program interpretation. An interpretation system based on this separation, called ARIES, is currently under development.<<ETX>>
[Algorithm design and analysis, Computational modeling, Laboratories, core algorithm, Programming, Yarn, Condition monitoring, program interpreters, Information science, execution models, computation model, dynamic dataflow tracking, Software systems, software engineering, Data flow computing, software tools, value-kind definitions, ARIES, Software tools, tool-fragment environment]
The Space Station Information System and software support environment
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
A large part of the value of the US Space Station will reside in the information exchanged between it and the related ground facilities. To insure that this exchange is effective, the Space Station Information Systems (SSIS) has been identified as an explicit element of the program and a management structure has been put in place to oversee the development of the end-to-end system, including those elements not provided by NASA. The SSIS is an extensive collection of hardware (computers, networks, facilities) and software, whose primary purpose is to carry data between a space-based source and a ground-based user.<<ETX>>
[end-to-end system, NASA, Space power stations, ground-based user, Space Station Information System, Data systems, Space stations, Information systems, US Space Station, Space technology, software support environment, aerospace computing, Software systems, Hardware, space-based source, programming environments, Assembly, Payloads]
Programming at the processor-memory-switch level
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
Users of networks of heterogeneous processors are concerned with allocating specialized resources to tasks of medium to large size. They need to create processes, which are instances of tasks, allocate these processes to processors, and specify the communication patterns between processes. These activities constitute processor-memory-switch (PMS)-level programming. The authors describe the use of PMS-level programming in computation-intensive, real-time applications, e.g. vision, robotics, and vehicular control, that require efficient concurrent execution of multiple tasks, e.g. sensor data collection, obstacle recognition, and global path planning, devoted to specific pieces of the application. They discuss the programming of heterogeneous machines and present the Durra language and tools, which they are developing to support PMS-level programming.<<ETX>>
[Robot control, Communication system control, high level languages, Sensor phenomena and characterization, communication patterns, robotics, processor-memory-switch level, vehicular control, multiple tasks, parallel programming, Concurrent computing, real-time applications, processor-memory-switch, Robot sensing systems, software engineering, obstacle recognition, software tools, Durra language, PMS-level programming, Computer vision, Robot vision systems, path planning, Robot programming, heterogeneous processors, vision, sensor data collection, Computer applications, Resource management]
An early report on ENCOMPASS
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
ENCOMPASS is an environment designed to support the incremental construction of Ada programs using executable specifications and formal techniques similar to the Vienna Development Method. ENCOMPASS supports the rigorous development of software: parts of a project may use completely formal methods, while other, less critical parts use less expensive techniques. ENCOMPASS provides automated support for all aspects of the development process including specification, prototyping, testing, formal verification, documentation, configuration control, and project management. In ENCOMPASS, software can be specified using PLEASE, an Ada-based executable specification language which can be automatically translated into Prolog. A prototype implementation of ENCOMPASS has been constructed. The authors give an overview of ENCOMPASS, describe the decisions made in the design of the prototype, and discuss the lessons learned in the process.<<ETX>>
[Costs, program testing, program verification, system documentation, prototyping, formal verification, specification languages, Trademarks, executable specifications, Testing, formal techniques, project management, NASA, Natural languages, testing, documentation, ENCOMPASS, Prolog, Specification languages, Formal specifications, configuration control, Computer science, environment, Refining, Ada programs, US Government, formal methods, incremental construction, PLEASE, programming environments, Ada]
Theory-W software project management: a case study
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
The authors present a candidate unifying principle to guide software project management. Reflecting various alphabetical management theories (X, Y, Z), it is called the Theory W approach to software project management. They explain the Theory W principle and its two subsidiary principles: plan the flight and fly the plan; and, identify and manage your risks. To test the practicability of Theory W, a case study is presented and analyzed: the attempt to introduce new information systems to a large industrial corporation in an emerging nation. The analysis shows that Theory W and its subsidiary principles do an effective job both in explaining why the project encountered problems, and in prescribing ways in which the problems could have been avoided.<<ETX>>
[Software maintenance, System testing, Computer aided software engineering, Art, Project management, DP management, Theory W, fly the plan, emerging nation, Software development management, Information analysis, Computer science, plan the flight, alphabetical management theories, risks, industrial corporation, Management information systems, software engineering, information systems, Risk management, project engineering, software project management]
Software development productivity tools and metrics
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
A description is given of the software development process used within one software engineering group at Digital Equipment Corporation, and the methods and tools used within the group to support the development of software products. Experience in the application of software metrics to parts of the process as a means of quantifying productivity and quality is described. The approach used to measure productivity is described.<<ETX>>
[Productivity, software development process, Project management, Digital Equipment Corporation, Programming, DP management, quality, Engineering management, productivity tools, productivity, Software quality, Hardware, software engineering group, software tools, metrics, Software measurement, Software tools, Software engineering, Testing]
Integrated support for project management
Proceedings. [1989] 11th International Conference on Software Engineering
None
1988
Areas of project management that need support are defined. A description is given of ISTAR, an integrated project support environment designed to provide integrated project management using the contractual approach. Management activities supported by ISTAR are discussed, namely, planning, work breakdown, personnel resource requirements, estimation, and scheduling. Resource management in ISTAR is examined. Facilities for cost management are discussed.<<ETX>>
[personnel resource requirements, project management, Project management, Strategic planning, DP management, Subcontracting, Scheduling, ISTAR, estimation, planning, Quality control, integrated project support environment, scheduling, Cost function, cost management, contractual approach, Resource management, programming environments, work breakdown, Contracts, Monitoring, Quality management]
Does Imperfect Debugging Affect Software Reliability Growth?
11th International Conference on Software Engineering
None
1989
This paper discusses the improvement of conventional software reliability growth models by elimination of the unreasonable assumption that errors or faults in a program can be perfectly removed when they are detected. The results show that exponential-type soft- ware reliability growth models that deal with error- counting data could be used even if the perfect debugging assumption were not held, in which case the interpretation of the model parameters should be changed. An analysis of real project data is presented.
[Fault detection, Laboratories, Debugging, Permission, Computer errors, Software systems, Time measurement, Software reliability, Software measurement, Testing]
The Scientific Engineering Of Software
11th International Conference on Software Engineering
None
1989
false
[Process design, Knowledge engineering, Software algorithms, Debugging, Permission, Computer industry, Machinery, Marine vehicles, Software engineering, Testing]
Configuration Management In BiiN/sup TM/ Sms
11th International Conference on Software Engineering
None
1989
false
[Circuits, Programming in the large, Programming, Educational institutions, configuration. management, Software development management, Operating systems, Permission, Trademarks, Software systems, Computer industry, Software tools, programming environments]
Test Adequacy And Program Mutation
11th International Conference on Software Engineering
None
1989
false
[Software testing, System testing, Materials testing, Costs, Genetic mutations, Permission, Software systems, Distributed computing, Machinery, Programming profession]
Timing Analysis Of Cyclic Concurrent Programs
11th International Conference on Software Engineering
None
1989
An approach to timing analysis of cyclic concurrent programs is presented. GR/sub 0/ path-expressions are used to describe synchronization and concurrency of atomic operations in cyclic concurrent programs. The behavior of a cyclic concurrent program is represented as a partial order of atomic operations, and a technique to derive this partial order from a GR/sub 0/ program is developed. Given the execution times of the individual atomic operations of a GR/sub 0/ program and a set of timing constraints, our timing analysis technique uses the partial order to determine whether the concurrent program, when executed, will satisfy the set of timing constraints. The timing analysis technique can be completely automated.
[Real time systems, Computer science, Concurrent computing, Permission, Automatic control, Timing, Safety, Electronic mail, Logic, Clocks]
Validation Through Testing
11th International Conference on Software Engineering
None
1989
false
[Software testing, Computer science, System testing, Transducers, Automatic testing, Computer bugs, Permission, Distributed computing, Machinery, Software engineering]
Completeness, Robustness, And Safety In Real-time Software Requirements Specification
11th International Conference on Software Engineering
None
1989
This paper presents an approach to providing a rigorous basis for ascertaining whether or not a given set of software requirements is internally complete, i.e., closed with respect to questions and inferences that can be made on the basis of information included in the specification. Emphasis is placed on aspects of software requirements specifications that previously have not been adequately handled, including timing abstractions, safety, and robustness.
[Computer science, Software testing, Production, Permission, Computer errors, Robustness, Software safety, Timing, Aircraft, Software engineering]
Twenty Years Of Software Engineering: Looking Forward, Looking Back
11th International Conference on Software Engineering
None
1989
false
[Military computing, Investments, Legislation, Permission, Computer industry, Public policy, Distributed computing, Machinery, Aerospace industry, Software engineering]
In Defense Of Coverage Criteria
11th International Conference on Software Engineering
None
1989
false
[Software testing, Performance evaluation, Computer science, Data analysis, Computer bugs, Software quality, Software performance, Information processing, Permission, Distributed computing]
A Hierarchical And Functional Software Process Description And Its Enaction
11th International Conference on Software Engineering
None
1989
A hierarchical and functional software process model HFSP is introduced which satisfies the basic requirements for process description and enaction. In HFSP, software process is described as a collection of activities which are characterized by their input and output relationship and defined as mathematical functions. When the relationship is not simple enough, activities are decomposed into subactivities together with the definitions of their input and output. HFSP allows clear and highly understandable process descriptions due to its static and declarative description style. HFSP realizes flexibility and dynamism needed for software process model by introducing reflection, that is, by treating its execution status as the basic data it can handle and process enaction mechanism. Principle of HFSP and its features such as objectbase access, tool invocation, concurrency and backtracking control, and process enaction mechanism are discussed.
[Computer science, Concurrent computing, Software maintenance, Software design, Project management, Humans, Process control, Permission, Reflection, Software engineering]
The Relationship Between Slices And Module Cohesion
11th International Conference on Software Engineering
None
1989
High module cohesion is often suggested as a desirable property of program modularity. The subjective nature of the definition of cohesion, however, can make it difficult to use in practice. In this paper, we examine the relationship between the data flow in a module and its level of cohesion using aprocessing element flow graph (PFG). Based on these PFG's, we regroup the original seven levels of cohesion into four classifications, Slice profiles are then defined by generating slices for all output variables of a module. A relationship is then shown between these slice profiles and the, PFG used to indicate levels of cohesion. It is suggested that these slice profiles can be used to determine more easily the cohesiveness of a module.
[Computer science, Software testing, Software maintenance, Psychology, Software quality, Permission, Software measurement, Flow graphs, Decision trees, Programming profession]
Tools To Support Formal Methods
11th International Conference on Software Engineering
None
1989
A key factor in the acceptance of high level programming languages has been the development of a comprehensive set of tools to support the user. If formal languages for specification are to achieve the same level of acceptance, they too will require extensive automated support. This paper describes a set of prototype tools which are designed to assist the developer in the use of fortnal specilication techniques.
[Software design, Fault detection, Prototypes, Permission, Formal specifications, Distributed computing, Machinery]
Completely Validated Software
11th International Conference on Software Engineering
None
1989
false
[Computer science, Real time systems, Production systems, Weapons, Permission, Control systems, Formal specifications, Distributed computing, Machinery, Formal verification]
Technology Dependence In Function Point Analysis: A Case Study And Critical Review
11th International Conference on Software Engineering
None
1989
Because Function Point Analysis (FPA) has now been in use for a decade, and in spite of its increasing popularity has met with some recent criticisms, it is time to review how appropriate it still is for today's technologies. A critical review of the FPA approach examines in particular the pioneering and continuing work of Albrecht and more recent work by Symons. Technological dependencies in FPA-type metrics are identified and a general model for deriving a new FPA-type metric for a new software technology is given. A model for the calibration of FPA-type metrics for new technologies in terms of a reference technology is also presented. Such calibration is essential for comparative productivity studies. The role of module estimation in exposing parts of the 'anatomy' of the FPA approach is investigated. The derivation and calibration models are applied to a significant case study in which a new FPA-type metric suited to a particular software development technology is derived, calibrated and compared with other published versions of FPA metrics.
[Productivity, Computer aided software engineering, Programming, Permission, Size measurement, Calibration, Appropriate technology, Anatomy, Application software, Distributed computing]
Software Development: State Of The Art Vs. State Of The Practice
11th International Conference on Software Engineering
None
1989
The state of the art of software development has changed considerably from the folkloric approaches of the 1950s and 60s. But has the state of the practice kept up? A commonly held (rather cynical) view is that the great revolutions associated with the names of Dijkstra, Wirth, Mills, Hoare, Parnas, Myers and others might as well not have happened for all the effect they had on the practice of the average developer. During the years 1984 through 1987, the authors conducted a series of performance benchmarking exercises to allow individuals and organizations to evaluate their relative productivity. The emphasis of the benchmarks was on speed of program construction and low defect rate. A side-effect of the exercise was that nearly 400 programmers wrote the same program (they all wrote to the same specification) and sent in listings of these programs along with their questionnaires, time logs, and test results. This afforded an opportunity to assess design and coding practice of a wide sample of developers.
[Productivity, Software design, Design methodology, Permission, Benchmark testing, Software measurement, Distributed computing, Machinery, Programming profession, Milling machines]
Three Problems Overcome With Behavioral Models Of The Software Development Process
11th International Conference on Software Engineering
None
1989
false
[Productivity, Software prototyping, Discrete transforms, Prototypes, Software quality, Programming, Permission, Software systems, Distributed computing, Machinery]
Performance Engineering As A Part Of The Development Life Cycle For Large-scale Software Systems
11th International Conference on Software Engineering
None
1989
The development of a large-scale software system occurs in a structured manner, proceeding through a number of distinct development phases. An effort is needed throughout the three to five year development of these systems to avoid last minute discovery that the system fails to meet specified levels of performance. The role of performance engineering during the early (modeling) portions of development is well known. This paper addresses the role of performance engineering throughout the entire software development life cycle. The paper draws upon experiences from successful performance engineering efforts applied to large transaction-based software projects in which multiple hundreds of thousands of lines of code were developed . The nature of the performance engineering support required by such projects changes dramatically during the middle of the development period; failure to recognize the need for this change can result in discovery of performance shortfalls too late to correct them before the system must become operational.
[Algorithm design and analysis, Software testing, System testing, Design engineering, Life testing, Prototypes, Programming, Permission, Software systems, Large-scale systems]
Structuring Criteria For Real Time System Design
11th International Conference on Software Engineering
None
1989
This paper discusses and compares the criteria used by different design methods for decomposing a real time system into tasks and modules. The criteria considered are coupling, cohesion and information hiding for module structuring and concurrency for tasks. The Structured Design method uses the module coupling and cohesion criteria. The NRL method and Object Oriented Design use information hiding as the primary criterion for identifying modules and objects respectively. The Darts design method uses a set of task structuring criteria for identifying the concurrent tasks in the system. A new design method for real time systems is introduced that uses both task structuring and information hiding module structuring criteria. The method is described and illustrated by means of an example of an automoble cruise control system.
[Real time systems, Concurrent computing, Productivity, Design engineering, Software design, Design methodology, Permission, Control systems, Information technology, Yarn]
Validating System Requirements By Functional Decomposition And Dynamic Analysis
11th International Conference on Software Engineering
None
1989
This paper describes an analysis method and a pro- gram, DAO, for validating requirements of embedded systems. In the real-world systems that contain embedded systems, service is provided by some "server" objects to some "client" objects. The "client" objects have needs to be satisfied. Usually, these objects are unpredictable and ephemeral. Oil the other hand, the "server" objects are there to help. They are capable, predictable and stable. These observations suggest that if we categorize objects by the functions they perform, constraints will be associated with each category. We can ask the user to specify the category of an object in addition to normal requirements specification. Then the analysis method can detect errors by checking if the behaviors of the object satisfy the constraints associated with the category of the object.
[Embedded system, Object detection, Permission, Writing, Concrete, Power system modeling, Pattern analysis, Distributed computing, Machinery, Information analysis]
Software Development In The Year 2000
11th International Conference on Software Engineering
None
1989
false
[Computer languages, Software libraries, Abstracts, Permission, Manufacturing, Software safety, Distributed computing, Machinery, Programming profession, Software engineering]
SRE: A Knowledge-based Environment For Large-scale Software Re-engineering Activities
11th International Conference on Software Engineering
None
1989
In this paper, we address issues related to the reengineering of large-scale software systems. The key to the software re-engineering activity is the ability to recover (re-engineer) "lost" or otherwise unavailable information concerning specification and system design decisions from the information available in the existing system source code. Subsequently, a forward engineering step may reimplement and possibly upgrade the existing systems. This paper describes the underlying princi- ples of a knowlkige-based Software Re-engineering Environment (SRE) which is intended to provide high-level support to various software maintenance and re-engineering activities.
[Knowledge engineering, Software maintenance, Permission, Continuous production, Software systems, Large-scale systems, Business process re-engineering, Error correction, Software tools, Information systems]
A Formal Adaptation Method For Process Descriptions
11th International Conference on Software Engineering
None
1989
Requirement to describing software development processes in formal manners has been increased, and demand for altering and tailoring the process descriptions has been emerged. In this paper, we propose a functional language PDL (Process Description Language), designed to describe various development processes under a certain environment. To create and modify the PDL scripts easily and correctly, we propose a method of stepwise refinement from abstract scripts into concrete scripts. By this method, the abstract definitions of software process flow and product flow initially given as function definitions irk PDL, are transformed into the concrete definitions of the tool activations, message displays, and so on. We also discuss an architecture design of a software development environment (Adaptable Software Development Environment), which can be adapted in many ways for designer's requirements.
[Process design, Computer languages, Computer architecture, Programming, Permission, Displays, Page description languages, Concrete, Hardware, Software tools]
Mathematics-based Software Engineering For Completely Validated Software
11th International Conference on Software Engineering
None
1989
false
[Software testing, System testing, Error analysis, Statistical analysis, Programming, Computer errors, Probability, Software debugging, Software engineering, Milling machines]
NLH/E: A Natural Language Help System
11th International Conference on Software Engineering
None
1989
A natural language help (NLH) system answers questions that are entered as typed, natural language sentences. Compared to traditional keyword lookup, natural language input allows greater flexibility and precision. This report describes such a help system, NLH/E, which answers questions formulated in English. Currently, it can process questions about a substantial domain: over 130 functions described in five chapters of the Common Lisp manual. NLH/E is built with a novel caseframe parser that operates with a thesaurus, case inheritance, and noun/verb phrase unification. These features reduce the size of the grammr considerably, making it feasible to build realistic NLH systems. Statistical data about the grammar are included. NLH is a first step towards a reusability expert, i.e., a program that helps a client find reusable software components by engaging in a natural language dialogue.
[natural language processing, software reuse., Natural languages, Thesauri, caseframe parsing, help systems, Distributed computing, artificial intelligence, Computer science, Tree graphs, Permission, Natural language processing, software engineering, Software reusability, Artificial intelligence, Software engineering]
What We Really Need Are Process Model Generators
11th International Conference on Software Engineering
None
1989
false
[Design engineering, Software prototyping, Spirals, Uncertainty, Scalability, Computer architecture, Permission, Software systems, Software standards, Distributed computing]
An Algebraic Data Type Specification Language And Its Rapid Prototyping Environment*
11th International Conference on Software Engineering
None
1989
This paper describes an algebraic data type specification language, called ADTS, and its associated rapid prototyping environment. ADTS is a specification and design language that supports constructivity and includes software engineering principles as modularity, parameterization and error and exception handling. Rapid prototyping provides a means for the early execution of specifications. In particular a translator that compiles constructive specifications into a high level language program and its associated environment is discussed.
[Software testing, Software prototyping, System testing, Prototypes, Permission, Writing, Telephony, Specification languages, High level languages, Software engineering]
Software Process Modeling: Principles Of Entity Process Models
11th International Conference on Software Engineering
None
1989
A defined software process is needed to provide organizations with a consistent framework for performing their work and im- proving the way they do it. An overall framework for modeling simplifies the task of producing process models, permits them to be tailored to individual needs, and facilitates process evolution. This paper outlines the principles of entity process, models and suggests ways in which they can help to address some of the problems with more conventional approaches to modeling software processes.
[Software testing, Software maintenance, Feedback, Software performance, Permission, Electronic switching systems, Scheduling, Software tools, Distributed computing, Software engineering]
An Error Complexity Model For Software Reliability Measurement
11th International Conference on Software Engineering
None
1989
Many simple software errors are found in earlier software test phases. The ratio of complex errors to simple errors gradually increases with continual testing. This paper describes a software reliability model called the Error Complexity Model. In this model, errors are classified by error complexity which is a measure of error detectability. The number of remaining software errors is estimated from the ratio of complex to simple errors and the number of discovered errors. New criteria for error complexity classification are proposed. The model is evaluated and compared with existing models using actual error data.
[Software testing, Costs, Error analysis, Laboratories, software reliability, error analysis, reliability model, Electrochemical machining, Time measurement, Software reliability, Permission, software engineering, Software measurement, error classification, Software engineering]
My Thoughts On Software Engineering In The Late 1960s
11th International Conference on Software Engineering
None
1989
false
[Computer hacking, Education, Production, Permission, Speech, Software reusability, High level languages, Distributed computing, Programming profession, Software engineering]
Modular Specification Of Incremental Program Transformation Systems*
11th International Conference on Software Engineering
None
1989
Advanced programming environments being developed to support ambitious program optimization and parallelization will perform extensive program analysis to gather facts used in performing complex program transformations. The need for timely response to the programmer's incremental modifications suggests that the program analysis database and transformed program be updated incrementally. Hand-coding these systems in conventional programming languages is both tedious and error prone. This paper presents a phase-oriented approach to incremental transformation system development in which transformations and analyses are separated into phases that interact only through explicit transmission of data. We will demonstrate how this approach can be applied within an attribute grammar setting in which a transformation system is non- procedurally specified and then automatically generated from its specification. Through the description of an incremental optimizer and an incremental parallelizing tool we demonstrate how this approach significantly simplifies the modification and extension of incremental transformation systems.
[Computer science, Data analysis, Databases, Permission, Parallel processing, Software systems, Performance analysis, Safety, Software tools, Programming environments]
"Not Waving But Drowning": Representation Schemes For Modelling Software Development
11th International Conference on Software Engineering
None
1989
false
[Programming, Permission, Application software, Kernel, Distributed computing, Machinery, Certification, Vehicles, Software engineering, Software development management]
Software Engineering In The Year 2001
11th International Conference on Software Engineering
None
1989
false
[Computer science, Computer languages, Silver, Humans, Documentation, Permission, Specification languages, History, Acceleration, Software engineering]
A Knowledge-based Environment For The Development Of Software Parts Composition Systems
11th International Conference on Software Engineering
None
1989
A number of systems have been developed that demonstrate the utility of knowledge-based techniques in the construction of software reuse systems. The amount of manual effort expended in the development of such systems can be reduced through the use of a knowledge acquisition environment tailored for a reuse-oriented model of the software development process. We present a model of the software development process and then describe the Bauhaus, a knowledge-based user interface to Ada software libraries.
[Software prototyping, Software libraries, Knowledge acquisition, Knowledge representation, Programming, Permission, Software systems, Application software, History, Software reusability]
Rethinking the Taxonomy of Fault Detection Techniques
11th International Conference on Software Engineering
None
1989
The conventional classification of software fault detection techniques by their operational characteristics (static vs. dy- namic analysis) is inadequate as a basis for identifying useful relationships between techniques. A more useful distinction is between techniques which sample the space of possible ex- ecutions, and techniques which fold the space. The new dis- tinction provides better insight into the ways different tech- niques can interact, and is a necessary basis for considering hybrid fault detection techniques.
[Computer science, System testing, Fault detection, Taxonomy, Permission, Sampling methods, Aerospace materials, State-space methods, Aircraft manufacture, Distributed computing]
A Process-oriented Approach To Configuration Management
11th International Conference on Software Engineering
None
1989
We present a framework integrating concepts from con- figuration management and process management. A language, interpretable by the environment, is proposed in order to specify the development process of versions and configurations. Two structuring mechanisms are provided: a class-subclass hierarchy and a task-subtask hierarchy. The resulting expressive power is illustrated on some examples. We describe then how the environment supports the processes specified in this language as well as how it supports the dynamic refinement of process specifications.
[Software maintenance, Laboratories, Knowledge management, development process modeling, Environmental management, Distributed computing, Modeling, Software development management, configuration management, generic environments., Permission, Software systems, Contracts]
Error-based Validation Completeness
11th International Conference on Software Engineering
None
1989
false
[Software testing, Computer science, System testing, Fault detection, Genetic mutations, Permission, Computer errors, Predictive models, Programming profession, Context modeling]
A Generic Model For Representing Design Methods
11th International Conference on Software Engineering
None
1989
A paradigm for representing process information is described. It consists of a simple generic model and a specialization mechanism for customizing the model for any design method. The generic model supports the representation of design arti- facts, steps and heuristics. A specialization of the generic model for JSD is illustrated in detail. The JSD model is then applied to model episodes from a pedagogical design process: the design of a lift system controller.
[Process design, Design automation, Design methodology, Programming, Encoding, Distributed computing, Design knowledge., Design methods, Process modeling, Tail, Permission, Control system synthesis, Performance analysis]
Software Engineering For Business DP Looking Back And Looking Forward
11th International Conference on Software Engineering
None
1989
false
[Encapsulation, Design engineering, Software prototyping, Prototypes, Wounds, Data models, Database systems, Software tools, Programming profession, Software engineering]
Work Structures And Shifts: An Empirical Analysis Of Software Specification Teamwork
11th International Conference on Software Engineering
None
1989
The study and support of teamwork in upstream software development activities (e.g., specification, design) have been approached from a variety of perspectives. Those which address aspects of the division of labor typically focus on authority or communication structures. In this paper, we examine how teams of engineers develop software specifications, from a perspective emphasizing the division of labor in terms of the work structures themselves. We present a new typology of work structures and report on an empirical investigation of these work structures. We examine the teamwork process followed by each of five comparable teams of specification developers. The teams worked over a ten-day period with state-of-the- art specification resources to deliver functional specification documents meeting prescribed quality standards. Our data and analysis show the recurrence of various kinds of shifts in the teams' work structures. We discuss the resulting patterns of work structures and shifts and their implications. In particular, separative work structures were associated with improved specification teamwork efficiency, whereas integrative work structures were associated with improved specification product quality.
[Computer science, Art, Data analysis, Costs, Project management, Programming, Permission, Software systems, Product design, Teamwork]
Twenty-year Retrospective: The Nato Software Engineering Conferences
11th International Conference on Software Engineering
None
1989
false
[Computer science, Permission, Software systems, Hardware, Mathematics, History, Distributed computing, Machinery, Software engineering]
Object Management In A Case Environment
11th International Conference on Software Engineering
None
1989
The Sun Network Software Environment (NSE) is a net- work-based object manager for software development. The NSE supports parallel development through an optimistic concurrency control mechanism, in which developers do not acquire locks before modifying objects, Instead, developers copy objects, modify the copies, and merge the modified objects with the originals. Objects managed by the NSE are typed, and the set of types can be extended by tool builders. The NSE is designed to work with heterogeneous implementations and poor communication.
[Neutron spin echo, Intelligent networks, Computer aided software engineering, Costs, Permission, Concurrency control, Environmental management, Sun, Testing, Software development management]
Modeling The Software Engineering Process
11th International Conference on Software Engineering
None
1989
false
[Spirals, Permission, Distributed computing, Machinery, Software engineering]
Remembrances Of A Graduate Student
11th International Conference on Software Engineering
None
1989
false
[Software testing, Software maintenance, Costs, Feedback, Permission, Software systems, Software measurement, Distributed computing, Conference proceedings, Machinery]
Software Development Process From Natural Language Specification
11th International Conference on Software Engineering
None
1989
A process to derive incrementally a formal specification from an informal specification written in natural-language is presented. This process consists of two major activities, "design" and "elaborate". Through the design activity, the structure of software modules based on an object oriented model is interactively extracted from the informal English description. Each word such as nouns and verbs in the natural-language sentences is associated with a software concept, e.g. class, attribute, and method. Especially we concentrate on types of verb patterns occurring in the sentences to increase applicability to dynamic systems. The elaborate activity is one to refine and rewrite the informal specification more precisely based on the derived mod- ule design document. We can complete the formal specification through the elaborate - design activity cycle. Each document created in our process is called product. The integration man- agement of the various kinds of the products are supported by a hypertext system. Furthermore we discuss the whole of soft- ware development process in which the design process and the elaborate one are embedded.
[Process design, Software prototyping, Object oriented modeling, Natural languages, Prototypes, Humans, Programming, Permission, Formal specifications, Hypertext systems]
Software Engineering Research Agendas A View From The Trenches
11th International Conference on Software Engineering
None
1989
false
[Computer science, Sociology, Permission, Mathematics, Dentistry, History, Application software, Software engineering, Accidents, Lifting equipment]
Software Survivor
11th International Conference on Software Engineering
None
1989
false
[Software maintenance, Publishing, Life testing, Psychology, Permission, Educational institutions, Environmental factors, Distributed computing, Machinery, Programming profession]
The State Of Software Engineering Practice: A Preliminary Report
11th International Conference on Software Engineering
None
1989
This is the first in a series of SEI reports to provide periodic updates on the state of software engineering practice in the DoD software community. The SEI has developed, and is refining, a process framework and assessment methodology for characterizing the processes used by software organizations to develop and evolve software products. This report provides a brief overview of the process framework and assessment ap- proach, describes assessment results obtained to date, and dis- cusses implications of the current state of the practice for both customers and suppliers of DoD software.
[Software maintenance, Costs, Project management, Software quality, Permission, Control systems, Scheduling, Management training, Software engineering, Quality management]
Thoughts On Software Engineering
11th International Conference on Software Engineering
None
1989
false
[Software testing, System testing, Software maintenance, Production systems, Art, Computer bugs, Permission, Hardware, Distributed computing, Software engineering]
Stepwise Refinement Process With Modularity: An Algebraic Approach
11th International Conference on Software Engineering
None
1989
Stepwise refinement method can be formalised algebraically, using specification morphism. Algebraic schemes also offer a natural way, like that we have adopted, to modularise speci- fications in the shape of distinct sort sets. Our definition of stepwise refinement with modularity is a faithful rendition of system development processes, as is shown by the example of designing an editor. We propose a concrete environment implementing our approach, based on OBJ2.
[Algebra, Shape, Terminology, Laboratories, Permission, Cities and towns, Concrete, Specification languages, Distributed computing]
The Dimensionality Of Program Complexity
11th International Conference on Software Engineering
None
1989
Software complexity metrics attempt to define the unique chaxacteristics of computer programs in an analytical way. Many such metrics have been developed to explain various perceived differences among programs. Many studies have been conducted to show the similarity among classes of these metrics. What is lacking in this body of literature is a technique which will aid in the establishment of the true dimensionality of the complexity problem space. The objective of this paper is to examine some recent investigations in the area of software complexity using factor analysis to begin an exploration of the actual di- mensionality of the complexity metrics. This technique can expose the relationships of these many metrics, one to another. Some correlation coefficients from recent empirical studies on software metrics were factor analyzed, showing the probable existence of five complexity dimensions within thirty five different complexity measures.
[Software metrics, Taxonomy, Permission, Covariance matrix, Software measurement, Distributed computing, Machinery]
The Nato Conferences From The Perspective Of An Active Software Engineer
11th International Conference on Software Engineering
None
1989
false
[Production, Permission, Books, Cultural differences, Plastics, Springs, Distributed computing, Machinery, Software engineering]
Obnet: An Objoct-oriented Approach For Supporting large, Long-lived, Highly Configurable Systems
11th International Conference on Software Engineering
None
1989
The aim of the ObNet model is to provide a framework for the design, development, maintenance and run-time support of large, long-lived, highly configurable systems for which the integration of different programming paradigms and environments plays a major role. The model defines some basic mechanisms which can be used in developing specialized software engineering environments for specific application domains. A system is a network of (multi-representation) objects; different specialized environments are different views over the network. Objects could be typed and are manipulated via methods; both types and methods are in turn objects. The concept of dependency between objects is supported and specialized to represent versions, instances, conversions and collections.
[Runtime environment, Object oriented modeling, Switches, Software performance, Permission, Application software, Environmental management, Software engineering, Embedded software, Testing]
The Inscape Environment
11th International Conference on Software Engineering
None
1989
The lnscape Environment is an integrated software development enviroment for building large software systems by large groups of developers. It provides tools that are knowledgeable about the process of system construction and evolution and that work in symbiosis with the system builders and evolvers. These tools are integrated around the constructive use of formal module interface specifications. We first discuss the problems that Inscapo addresses, outline our research strategies and approaches to solving these problems, and summarize the contributions of the Inscape Environment. We then discuss the major aspects of the Inscape Environment: the specification language, system construction, system evolution, use and reuse, and validation, We illustrate these various components with examples and discussions.
[Symbiosis, Buildings, Writing, Permission, Programming, User interfaces, Software systems, Specification languages, Formal specifications, Distributed computing]
Tool Integration In The Pact Environment.
11th International Conference on Software Engineering
None
1989
The Part environment is a software engineering environment (SEE) being built in the Pact project. The environment is being constructed on the PCTE interfaces. The project has defined an environment architecture which identifies Common Services. These are reusable tool components that factor out code that would otherwise have to be developed in several tools, and which significantly increase the integration of the environment. The main activity of the Part project is to develop "method-independent" tools, independent of a particular engineering technique for the production of software.
[Computer aided instruction, Laboratories, Project management, Computer architecture, Programming, Trademarks, Personnel, Software tools, Kernel, Software engineering]
The Idea Design Environment
11th International Conference on Software Engineering
None
1989
The design of software is a complex process requiring the software designer to simultaneously perform a variety of activities. These include the exploration and analysis of design alternatives, the consideration and reuse of previous solutions, the management of design goals, dependencies, and partial solutions, and the recording of design decisions. Very few software development environments provide all these aspects of design support. The IDeA system is a prototypical design environment that was developed to demonstrate that a single environment could integrate many different support aspects. The objective of such an environment is to relieve the designer from the more mundane and mechanical aspects of design, and thus permit him to devote his energies to the more difficult and intel- lectually challenging aspects of design. This paper describes the IDeA environment and the aspects of design support that it provides.
[Process design, Software prototyping, Software design, Prototypes, Software performance, Drives, Programming, Aging, Microelectronics, Testing]
Declarative Visualization In The Shared Dataspace Paradigm
11th International Conference on Software Engineering
None
1989
This paper is concerned with the use of program visualization as a means for the understanding, debugging, and monitoring of large-scale concurrent programs. Following an overview of the shared dataspace paradigm and the declarative approach to visualization, the paper discusses: (1) mechanisms for specifying declarative visualization in the shared dataspace paradigm and ways of relating the specifications to program verification; (2) a computational model which provides a unified framework for comparing both visual and nonvisual algorithms; and (3) strategies for implementing declarative visualization on parallel machines.
[Concurrent computing, Computer science, Computational modeling, Humans, Data visualization, Debugging, Permission, Displays, Rendering (computer graphics), Large-scale systems]
Task Interaction Graphs For Concurrency Analysis
11th International Conference on Software Engineering
None
1989
A representation for concurrent programs, called task inter- action graphs, is presented. Task interaction graphs divide a program into maximal sequential regions connected by edges representing task interactions. This representation is illustrated and it is shown how it can be used to create concurrency graph representations that are much smaller than those created from control flow graph representations. Both task interaction graphs and their corresponding concurrency graphs facilitate analysis of concurrent programs. Some analyses and optimizations on these representations are also described.
[Concurrent computing, Computer science, Analytical models, Permission, Programming, Educational institutions, Performance analysis, Flow graphs, Distributed computing, Machinery]
Recent advances in software measurement
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
Summary form only given. There has been continued progress in the area of software measurement. The main inroads have been made in the scope of measurement, a deeper understanding of the perspectives from which measurement can be applied, the development of frameworks for the definition and interpretation of measurement, the refinement of the measures and the models on which the metrics are based, the automation of the models and measures, and the increased application of measurement in many organizations.<<ETX>>
[Performance evaluation, Automation, Process control, Programming, Educational institutions, Size measurement, Time measurement, Application software, Computer science, software measurement, software engineering, metrics, Software measurement]
An object-oriented software application architecture
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The authors present a novel application architecture and supporting environment. The application architecture is based upon the concept of separating the application domain knowledge (including semantics, constraints, and functionality) from the application code. This application domain knowledge is embedded in a domain model that can be shared by multiple applications. This is made possible by the use of object-oriented technology. The other advantages of this separation of the domain model from the application code include: (i) the provision of a consistent view of the application domain across multiple applications; and (ii) its use as a communication and learning aid. The application architecture also supports the separation of the user-interface issues from those of application functionality. The development of a network editor application conforming to this architecture is also presented.<<ETX>>
[Productivity, application domain, object-oriented programming, learning aid, Object oriented modeling, Programming, semantics, functionality, Application software, Personnel, supporting environment, constraints, network editor application, application domain knowledge, application code, user-interface, Computer architecture, Software quality, User interfaces, application generators, object-oriented software application architecture, Large-scale systems, Expert systems]
Mixing abstract and concrete modules: specification, development and prototyping
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The authors present a methodology for developing large modular software, in which the modules may exist and interact while at different development stages. Each module may be either fully abstract (i.e., an algebraic specification), fully concrete (in which case computations are run directly in the object code) or anywhere in the middle. In the proposed approach, the authors consider executable specifications in which axioms are Horn clauses built over equations or predicates, mixed evaluation with Horn clauses, and computing goals using logic computation rules. For the 'concrete' parts, examples are developed in Ada. An essential aspect of this approach is that prototyping may be performed at every moment, whatever the state of the different modules. This introduces the usual advantages of running a prototype (debug, test, conformity to the requirements, etc.), without the usual drawbacks: having to program stubs and drivers, having to care about the evolution of the module interfaces, etc. The methodology allows for both locally top-down and bottom-up development strategies and stimulates systematic stepwise refinement schemes.<<ETX>>
[Software testing, top-down development, prototyping, systematic stepwise refinement schemes, Programming, large modular software, bottom-up development strategies, formal specification, Prototypes, software engineering, Logic, concrete modules, executable specifications, Software prototyping, development, Horn clauses, Life testing, specification, Formal specifications, Equations, Computer science, logic computation, abstract modules, Concrete, Ada]
Assessing failure probabilities in safety-critical systems containing software
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
SRCS (safety-related computer systems) need to be kept under firm intellectual control throughout their development so that the range of possible behavior can always be specified precisely. This is equivalent to a requirement that the behavior be modeled mathematically and is a powerful argument for the use of mathematically formal notations (formal methods) throughout the specification, design, and implementation of software and hardware for SRCS.<<ETX>>
[safety-related computer systems, System testing, mathematically formal notations, software reliability, Probability, Data engineering, specification, SRCS, Software safety, intellectual control, Programmable control, safety-critical systems, Operating systems, Failure analysis, safety, formal methods, Software systems, Systems engineering and theory, Hardware, failure probabilities]
TRW's Ada Process Model for incremental development of large software systems
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
TRW's Ada Process Model has proved to be the key to the Command Center Processing and Display System-Replacement (CCPDS-R) project's success in developing over 300000 lines of Ada source code executing in a distributed VAX VMS environment. The Ada Process Model is a uniform application of incremental development coupled with a demonstration-based approach to design review for continuous and insightful thread testing and risk management. The use of Ada as the life-cycle language for design evolution provides the vehicle for uniformity and a basis for consistent software progress metrics. The author provides an overview of the techniques and benefits of the Ada Process Model and describes some of the experience and lessons learned.<<ETX>>
[Ada Process Model, software progress metrics, Missiles, risk management, insightful thread testing, large software systems, Displays, State-space methods, Personnel, Yarn, incremental development, Voice mail, Vehicles, Ada source code, Software systems, distributed VAX VMS environment, software engineering, design review, Risk management, life-cycle language, Command Center Processing and Display System-Replacement, design evolution, Testing, Ada]
A rule-based approach to modular system design
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The author proposes an approach to the development of software systems based on ideas and techniques from the algebraic theory of graph grammars. Given an initial specification, SPEC productions (SPEC is an algebraic specification) are applied sequentially or in parallel to generate another specification: if the productions are the interfaces of module specifications from a library, then the derivation sequence can be translated into a modular system. The results obtained are intended as a formal support for a rule-based (expert) system to aid the systematic development of large software systems from a library of reusable components. The author discusses the notions of derivability and translation of a derivation sequence into an interconnection of modules, and he indicates how to anticipate the application of a production using the parallelism theorem. This equivalence of derivations translates into provably equivalent modular systems.<<ETX>>
[Production systems, expert systems, reusable components, software systems, large software systems, formal specification, derivability, initial specification, graph grammars, Software design, Algebra, Libraries, software tools, systematic development, Natural languages, Mechanical factors, rule-based approach, Equations, SPEC productions, modular system design, Packaging, software reusability, translation, Software systems, algebraic theory]
An environment for specifying and executing hierarchical Petri nets
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
An environment for the construction and execution of requirement specifications for embedded systems is described. High-order Petri nets are used as a notation for the specifications. The environment consists of a net browser, a net editor, an animated net simulator, and a simulation engine. A hierarchically structured specification can be constructed using the graphics-based net editor. The animated simulator helps to detect errors and inconsistencies. The net browser enables management of several net hierarchies and the retrieval and reuse of nets. The simulation engine allows specifications to be executed in real time in the real environment. The authors concentrate on the net browser and the concepts, methods, and graphical notations used for improving the usability of high-order Petri nets.<<ETX>>
[net browser, reuse, Petri nets, graphical notations, formal specification, Engines, Research and development, Embedded system, embedded systems, specification languages, Libraries, software tools, real time, animated net simulator, Object oriented modeling, animated simulator, Flow graphs, hierarchical Petri nets, Graphics, simulation engine, real-time systems, requirement specifications, Animation, net editor, Usability]
CEFRIEL: an innovative approach to University-industry cooperation in information technologies
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The author describes the organization of research in the CEFRIEL Center and discusses the management of intellectual property (industry, center, faculty, students). The CEFRIEL Consortium operates through a center devoted to both research and education in the field of information technologies (computer and communication). Both advanced and permanent educational activities are targeted, together with cooperative research activities oriented toward medium-/long-term applications. An innovative form of implementation was conceived for the center's operation. The basic idea consists of the creation of a research center with both research resident employees of the sponsoring firms and research employees of the center itself. Faculty members visit the center for research and teaching purposes. Lessons learned at CEFRIEL are described.<<ETX>>
[University-industry cooperation, Job shop scheduling, CEFRIEL, Laboratories, Very large scale integration, technology transfer, Information technology, intellectual property, Education, Digital signal processing, information technologies, Cities and towns, research employees, cooperative research activities, sponsoring firms, Flexible manufacturing systems, research resident employees, Asynchronous transfer mode, Software engineering]
Using object-oriented development to support prototyping
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The authors describe a project in which semiformal software development methods were used in parallel with a prototyping effort to develop a real-time monitor and control computer system. The prototype was used to define and clarify the requirements for the system. The semiformal approach was used to capture and document those requirements. The resulting description of the system was then used to migrate the prototype to a first-version production system. The combined approach helped to overcome many of the problems typically associated with prototyping alone. This project was also used as a vehicle for technology transfer. Owing to constraints in the local environment, the approach to technology transfer used differed substantially from those typically described in the literature. The development and technology efforts and the lessons learned from each are described.<<ETX>>
[Real time systems, Software prototyping, Production systems, object-oriented programming, real-time monitor, Computerized monitoring, prototyping, Programming, Control systems, technology transfer, control computer system, Vehicles, Concurrent computing, Prototypes, Technology transfer, real-time systems, object-oriented development, software engineering, semiformal software development methods]
Human-oriented conceptual abstractions in the reengineering of software
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
Semiformal, human-oriented, and domain-specific abstractions play a critical role in both reverse and forward engineering, and therefore, in reengineering. Such conceptual abstractions are fundamental to the reengineering process whether it is a totally manual or partially automated process.<<ETX>>
[Humans, Concrete, software engineering, conceptual abstractions, Pattern recognition, Large-scale systems, Pattern analysis, Pattern matching, software reengineering, Testing]
Some experiences of critical software development
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
Experience with the development of nuclear power plant protection systems is discussed. The state of the art in this field is examined, and the problems entailed in demonstrating the safety of these systems are considered.<<ETX>>
[Redundancy, software reliability, Humans, Programming, nuclear power plant protection systems, Authorization, critical software development, Power system protection, safety, Hardware, Safety, Power generation, fission reactor safety]
Improving software quality: the use of formal inspections at the Jet Propulsion Laboratory
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
After surveying detection practices in the best of industry, JPL (Jet Propulsion Laboratory) Software Product Assurance decided that the most cost-effective early defect detection technique was the Fagan inspection procedure. The author describes this technique, how it was introduced to JPL, some of the difficulties involved in transferring technology, and the first provisional set of results.<<ETX>>
[Software testing, Jet Propulsion Laboratory, quality control, Laboratories, software reliability, Inspection, technology transfer, software quality, transferring technology, Management training, formal inspections, Phase detection, Fagan inspection procedure, Software design, Software quality, Propulsion, Software systems, Meeting planning]
Technology transfer aspects of environment construction
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The author describes his experience with technology transfer aspects of environment construction. Emphasis is placed on the concepts of 'technology push and pull' and the 'playground'.<<ETX>>
[environment construction, Project management, Programming, technology transfer, Technology planning, Environmental management, Organizing, Construction industry, Technology management, Technology transfer, playground, software engineering, technology push and pull, Assembly, Software engineering]
System support for modular order-sorted Horn clause specifications
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The authors present an overview of CEC, a rewrite rule laboratory for order-sorted specifications with conditional equations. CEC differs from related systems, such as OBJ-3, in that it can check or achieve by completion the semantic prerequisites for correct operational execution of specifications by conditional term rewriting. CEC supports the modular structure of specifications in two ways. Its speculation browser allows accessing, editing, and administrating the modules of a specification through a highly interactive user interface. The completion procedure, apart from transforming a specification into executable rewrite code, will find the inconsistencies, if any, between the formal and actual parameters of generic specifications. CEC can handle a large class of conditional equations with extra variables in the condition without having to resort to inefficient goal solving methods at rewrite time.<<ETX>>
[CEC, Visualization, Modular construction, rewriting systems, Art, conditional term rewriting, Laboratories, OBJ-3, Formal specifications, executable rewrite code, formal specification, goal solving methods, Equations, rewrite rule laboratory, highly interactive user interface, speculation browser, User interfaces, Logic functions, generic specifications, modular order-sorted Horn clause specifications, correct operational execution, software tools, conditional equations]
A decision-based configuration process model
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
Configuration management is linked to a data model of software processes in order to clarify the relationship between programming-in-the-large, in-the-small, and in-the-many. The data model also allows for the separation of conceptual from document-based version and configuration management by focusing on careful modeling of design decisions as a unifying concept to describe versioning, configuration, and mapping tasks. The close relationship to in-the-small ideas suggest applicability in process reusability. The authors also report on a prototype implementation on the knowledge base management system ConceptBase and experiences in a real-world case study.<<ETX>>
[Software prototyping, Software maintenance, data model, Content management, project support environments, versioning, decision-based configuration process model, Programming, programming-in-the-large, Knowledge management, knowledge base management system, History, in-the-small, process reusability, mapping tasks, software processes, Software systems, in-the-many, Data models, software engineering, Large-scale systems, ConceptBase, Artificial intelligence]
Reengineering: can a program put intelligence in stupid programs?
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The reengineering of old programs is approached as a purely technical problem. The relevance of organizational aspects is pointed out, and the needed tools are indicated. The ideal environment should allow the description of the structure of the organization; the description of the software development cycle; the description of the procedural rules for configuration, version, and change management and control; a collection of tools for measuring properties of the software; tools for helping in decisions on the basis of the measures obtained on a product; and a collection of tools for supporting technical activities (specifications, design, coding, debugging, etc.).<<ETX>>
[Reverse engineering, organizational aspects, configuration, Documentation, reengineering, Technical activities, procedural rules, Intelligent structures, Organizing, Stress, change management, Design engineering, Computer architecture, software engineering, Software measurement, Software tools, software development cycle]
Prism=methodology+process-oriented environment
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
A description is given of Prism, an experimental process-oriented environment supporting methodical development, instantiation, and execution of software process models. Also described is an architecture that captures this model in its various components. The architecture has been designed to hold a product software process description, the life cycle of which is supported by an explicit representation of a higher level (or meta) process description. Also described is the nine-step Prism methodology for building and tailoring process models, and several scenarios to support this description are given. In Prism, process models are built using a hybrid process modeling language, which is based on a high-level Petri net formalism and rules. The Prism prototype has been implemented on UNIX, GRAS database for attributed graph structures, and the Sunview user interface.<<ETX>>
[Software maintenance, process-oriented environment, Prism methodology, attributed graph structures, Sunview user interface, Buildings, Project management, Financial management, GRAS database, Environmental management, formal specification, UNIX, Computer science, high-level Petri net, hybrid process modeling language, Prototypes, Computer architecture, software process models, Human resource management, programming environments, Prism, Software engineering, instantiation, product software process description]
Multi-level specification and verification of real-time software
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The authors present a state-based approach for specification of real-time software at multiple levels of abstraction. In this approach, specification at each level is performed in terms of a hierarchical multistate (HMS) machine, with the higher levels defining requirements and the lower levels indicating methods of achieving the requirements. This approach leads to a considerable simplification of the specification process; it can lead to reusability of specifications and is fundamentally different from the refinement approach. A restricted method of verifying the consistency of multilevel specifications is also presented. By the use of this method, necessary scheduling of concurrent tasks to satisfy a complex set of logical and temporal constraints can often be derived.<<ETX>>
[Real time systems, program verification, Petri nets, concurrent tasks, hierarchical multistate, Software performance, abstraction, reusability, temporal constraints, HMS machines, specification process, Formal specifications, formal specification, Computer science, Software design, USA Councils, Automata, real-time systems, logical constraints, automata, Computer security, Contracts, verification, real-time software]
Application of software reliability modeling to product quality and test process
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
Software reliability modeling of data collected during the testing of a large-scale industrial system (System T) was used to measure software quality from the customer perspective. Specifically, software quality was measured in terms of the system operation. The testing phase analyzed, stability test, was an operational profile-driven test in that a controlled load was imposed on the system reflective of the system's busy-hour usage pattern. The usage profile was determined from requirements specifying both the frequency of invocation of each command and the alarm arrival rate for the largest expected user site. For this controlled test environment, a Poisson-type reliability growth model, the exponential nonhomogeneous Poisson process model, exhibited a good fit to the observed failure data. Furthermore, the model demonstrated predictive power for future failure rates. The use of an operational profile to drive system test is an effective test strategy and that the operational profile must be taken into account when predicting field reliability from reliability measured during test.<<ETX>>
[Software testing, System testing, program testing, failure rates, software reliability, Poisson-type reliability growth model, large-scale industrial system, test process, product quality, stability test, largest expected user site, exponential nonhomogeneous Poisson process model, Large-scale systems, Software measurement, frequency of invocation, quality control, alarm arrival rate, operational profile, observed failure data, Software reliability, Application software, busy-hour usage, Power system modeling, customer perspective, operational profile-driven test, predictive power, software reliability modeling, Software quality, Computer industry, Power system reliability]
On the assessment of safety-critical software systems
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
An introduction to the assessment of safety-critical software systems is presented. Particular attention is given to the role of process models.<<ETX>>
[Performance evaluation, software reliability, safety-critical software systems, Probability, Software safety, Application software, Statistics, Fault tolerance, assessment, process models, safety, Production, Software systems, Hardware, Risk management]
SACEM software validation
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The authors report on the software validation process for SACEM, a partly embedded system (hard and soft) which continuously controls the speed of all trains on the RER Line A in Paris. Modern techniques have been used for validation, including formal specification, assertions, and formal proofs. About 100 man-years have been spent in validating the software. The authors conclude that a safe system has been realized and that all the formal work was useful, essentially to make the specifications more precise.<<ETX>>
[Safety devices, program verification, formal proofs, Railway safety, Switches, Control systems, Software safety, Formal specifications, Application software, formal specification, railways, partly embedded system, train speed control, Communication system signaling, Embedded system, SACEM software validation, Software systems, assertions]
Building an evolution transformation library
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The authors have been developing knowledge-based tools to support the evolutionary development of specifications. Evolution is accomplished by means of evolution transformations, which are meaning-changing transformations applied to formal specifications. A sizable library of evolution transformations has been developed for the specification language Gist. The authors assess the results of their previous work on evolution transformations. They then describe their current efforts to build a versatile, usable evolution transformation library. They have identified important dimensions along which to describe transformation functionality, so that it is possible to assess the coverage of a library along each dimension. The potential applicability of this formal evolution paradigm to other environments is assessed.<<ETX>>
[Gist, Knowledge based systems, Programming, Specification languages, Formal specifications, formal specification, knowledge-based tools, formal specifications, Information analysis, meaning-changing transformations, Feathers, Computer languages, Software libraries, Intersymbol interference, knowledge based systems, software tools, Contracts, evolution transformation library, specification language]
Software process a la Algebra: OBJ for OBJ
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
A process paradigm centered on products is presented. This approach allows the separation of purely technical aspects from other aspects, such as managerial ones; makes it easy to define the effects of activities; and makes it possible to analyze and control the ripple effects of modifications. An example of this approach as applied to an algebraic methodology based on OBJ, an algebraic specification language, is given. The software process model presented can be viewed as and environment for software development with OBJ. St. OBJ (stairway to OBJ), the environment, is described in OBJ itself.<<ETX>>
[Costs, software development, Laboratories, algebraic specification language, ripple effects, St. OBJ, Environmental management, formal specification, algebraic methodology, Software development management, Programming environments, products, Computer languages, Algebra, process paradigm, OBJ, specification languages, Cities and towns, software tools, Human resource management, Research and development management, software process model]
Technology transfer as a collaboration: the receptor group
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The author explains the concept of the technology receptor group. This technology receptor group tracks, screens, installs, and evaluates new methods and technology for improved software engineering practice within an organization. It also works to orchestrate process improvement activities such as process maturity assessment, process databases, and process education and training. The SEI technology receptor group strategy is considered as an example, and successes and failures at SEI are described.<<ETX>>
[Gold, process maturity assessment, technology receptor group, Collaborative software, Programming, software engineering practice, technology transfer, training, receptor group, Manufacturing processes, process improvement activities, process education, Technology transfer, Collaboration, SEI, process databases, Collaborative work, Software systems, software engineering, Expert systems, Software engineering]
Getting started on metrics-Jet Propulsion Laboratory productivity and quality
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
A description is given of the SPA (Software Product Assurance) Metrics Study, part of an effort to improve software quality and the productivity and predictability of software development. The first objective was to collect whatever data could be found from as many projects for which they were still available. Data were assembled on the basis of four basic parameters: source lines of code, dollars, work years, and defects. By using these four basic parameters, it was possible to construct quality and productivity baselines. Quality was defined as the number of defects per thousand lines of source code. Productivity was defined in two ways: dollars per source line of code and source lines of code per work month. Preliminary results of this study are presented.<<ETX>>
[Productivity, Jet Propulsion Laboratory, source lines of code, Costs, software development, work years, quality control, Laboratories, software reliability, SPA, Project management, Programming, Software development management, quality, productivity, Software quality, defects, Propulsion, Software systems, Systems engineering and theory]
Towards systems engineering-a personal view of progress
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The article is written from the standpoint of an industrial consumer of tools, methods, and theories as they concern the building of software-intensive IT (information technology) systems. A personal view is given of the contribution that advances in software engineering and related disciplines have made or might make to the job of improving the way in which systems might be constructed in a predictable, cost-effective manner that meets the customer's requirements.<<ETX>>
[Technological innovation, Costs, industrial consumer, Application software, Communication industry, Systems engineering and theory, Computer industry, Software systems, systems engineering, Hardware, software engineering, Software tools, software-intensive IT, Software engineering]
Software reengineering position statement
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
Software reengineering work can be divided into three classes of activity: (1) choosing a calculus (it is suggested that the predicate calculus is a more promising medium than a data/control flow graph calculus because it is easier to prove equivalence between two expressions in the former); (2) building an industry standard library of primitive expressions in this calculus that will cover the domain of interest at its most abstract level; (3) building a system to recognize and prove equivalences between these high-level primitives and lower level expressions in the calculus that directly express the primitive operators of the original implementation.<<ETX>>
[Automatic programming, Industrial control, Reverse engineering, predicate calculus, Control systems, Calculus, Data mining, Code standards, high-level primitives, Computer industry, Electrical equipment industry, software engineering, Data flow computing, data/control flow graph calculus, industry standard library]
Position paper on technology transfer
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
In order for the transference of the tools provided by a research center to be successful, two things must be accomplished, one by the research centers and the other by the product groups. First, the research centers must provide their results in terms of products in time for the projects of the product groups (i.e., they must be in line with project plans of product divisions). Second, this objective cannot be reached by the research centers alone. The transfer of the results has to happen between the research centers and the product divisions. Close cooperation is required; overall plans and schedules must be established between research centers and product groups.<<ETX>>
[Software prototyping, technology transfer, Personnel, Seminars, Refining, Technology transfer, Prototypes, product groups, research centers, software engineering, Functional programming, Software tools, Testing]
Use of video for program documentation
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The use of VHS video technology as a medium for program documentation is shown to offer some compelling advantages. These include reduced drudgery, ease of motivation of the documenters, low cost, and appeal of the product to its target audience. A case study concerning the use of video for program documentation is presented. It is believed that many other areas of software engineering could also benefit from the application of video technology to the documentation process.<<ETX>>
[Software maintenance, Costs, program documentation, video recording, system documentation, Project management, Documentation, Programming, Product design, Personnel, case study, Software development management, VHS video technology, Software quality, software engineering, US Department of Defense]
The evolution of technology transfer at MCC's software technology program: from didactic to dialectic
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
MCC's software technology program (STP) has a dual mission: to create tools and methodologies to assist development teams in the design of large, complex, distributed software systems, and to ensure the widespread diffusion of these technologies within STP shareholder organizations. STP has developed a comprehensive set of traditional technology transfer methods; although these methods make it possible to communicate research results successfully to shareholder receptors, they have spurred little actual use of released technology. STP has recently reevaluated its approach to technology transfer and discovered how it can be better accomplished-by fostering collaboration as its chief agent, rather than by depending on traditional transfer-and-feedback mechanisms. The authors describe the evolution of this new approach and present some of the results of collaborative experiences to date. It is found that collaborations revitalize research and that they empower STP's shareholders to affect the directions of that research.<<ETX>>
[Technological innovation, Video sharing, Educational technology, technology transfer, distributed software systems, Microelectronics, technology transfer methods, shareholder organizations, Technology transfer, Collaboration, collaboration, Production, Software systems, software engineering, software technology program, Software tools, development teams, Software engineering]
Object-oriented specification of reactive systems
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
A novel approach to the operational specification of concurrent systems that leads to an object-oriented specification language is presented. In contrast to object-oriented programming languages, objects are structured as hierarchical state-transition systems, methods of individual objects are replaced by roles in cooperative multiobject actions whereby explicit mechanisms for process communication are avoided, and a simple nondeterministic execution model that requires no explicit invocation of actions is introduced. The approach has a formal basis, and it emphasizes structured derivation of specifications. Top-down and bottom-up methodologies are reflected in two variants of inheritance. The former captures the methodology of designing distributed systems by superimposition; the latter is suited to the specification of reusable modules.<<ETX>>
[process communication, Design methodology, Laboratories, reactive systems, inheritance, superimposition, formal specification, object-oriented specification language, parallel programming, Concurrent computing, Software design, top-down methods, modularity, explicit mechanisms, specification languages, operational specification, Object oriented programming, object-oriented programming, bottom-up methodologies, Object oriented modeling, nondeterministic execution model, Specification languages, joint actions, Computer science, Computer languages, cooperative multiobject actions, hierarchical state-transition systems, concurrent systems, reusable modules, software reusability, Software systems]
The 'Catch 22' of reengineering
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
In the software reengineering discussion it is assumed that the system can be understood on the following four levels: the programming language level, the control structure level, the generic algorithm level, and the problem domain level. It is noted that it is now possible to build tools which understand systems on the first three levels. There have been considerable advances in system data analysis that will lead directly to identification of abstract data types and objects. It is suggested that future progress will critically depend on the ability to represent and reason about the problem domain. The reengineering systems of tomorrow will require knowledge not only of software engineering, but more important, of particular problem domains.<<ETX>>
[Knowledge engineering, Assembly systems, Spirals, abstract data types, System analysis and design, Programming profession, software reengineering, problem domain, problem domain level, Design engineering, Insurance, Inventory management, Lab-on-a-chip, Systems engineering and theory, software engineering, programming language level, control structure level, generic algorithm level, system data analysis]
Evaluation of software safety
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The features of both probabilistic and nonprobabilistic approaches are considered. Particular emphasis is placed on an approach in which safety verification and analysis are backed up by using software safety design techniques that protect against hazardous states that might result from undetected software faults, including those stemming from flaws in the software requirements specification.<<ETX>>
[software reliability, software safety, software requirements specification, Hazards, Software safety, Software reliability, nonprobabilistic approaches, design techniques, hazardous states, software faults, Computer science, Physics computing, safety, Hardware, safety verification, Software measurement, Injuries, Accidents, Power engineering and energy]
Experience using the Graphite meta-tool
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
Graphite is a metatool for generating Ada source code for graph structure manipulation from a concise specification of a graph structure. The author reports an experiment in which Graphite was used to generate part of the code for a simple hierarchical data management tool. The experiment showed that the use of Graphite improved productivity, code quality, and flexibility and allowed a number of important concerns to be separated. The results prompt speculations about generalizing meta-tools into metaparts that can be used in a wide spectrum of applications, and about ways to improve the utility and applicability of such metaparts.<<ETX>>
[Productivity, Software prototyping, graph structure manipulation, concise specification, Code standards, Graphite meta-tool, formal specification, code quality, Software design, flexibility, Databases, USA Councils, productivity, hierarchical data management tool, Production, Ada source code, Packaging, User interfaces, Lakes, software tools, graph structure]
LaSSIE: a knowledge-based software information system
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The authors discuss the important problem of invisibility that is inherent in the task of developing large software systems. It is pointed out that there are no direct solutions to this problem; however, there are several categories of systems-relational code analyzers, reuse librarians, and project management databases-that can be seen as addressing aspects of the invisibility problem. It is argued that these systems do not adequately deal with certain important aspects of the problem of invisibility-semantic proliferation, multiple views, and the need for intelligent indexing. A system called LaSSIE, which uses knowledge representation and reasoning technology to address each of these three issues directly and thereby help with the invisibility problem, has been built. The authors conclude with an evaluation of the system and a discussion of open problems and ongoing work.<<ETX>>
[Costs, project support environments, Project management, large software systems, Switches, Relational databases, Programming, knowledge-based software information system, database management systems, multiple views, Information systems, knowledge based systems, software tools, Buildings, reuse librarians, semantic proliferation, inference mechanisms, reasoning technology, invisibility, project management databases, relational code analyzers, knowledge representation, software reusability, Software systems, intelligent indexing, Software engineering, Indexing, LaSSIE]
Evaluation criteria for functional specifications
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
Functional correctness is a technique for deriving a program and proving that this program meets its specifications. Both a program and its specifications are viewed as functions. Through the use of techniques based upon symbolic execution and denotational semantics, a proof methodology has been developed. The authors extend this theory of functional specifications, making it possible to model various life-cycle methods in a consistent manner. Given several possible implementations for a given specification, they develop techniques for evaluating one implementation over another.<<ETX>>
[functional specifications, program verification, Memory, Educational institutions, evaluation criteria, life-cycle methods, formal specification, Milling machines, Computer science, functional correctness, Computer languages, symbolic execution, Concrete, proof methodology, denotational semantics]
Semiformal process model for technology transfer
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The author describes a semiformal process model for technology transfer at Ferranti Computer Systems. Experience has shown that satisfactory transition takes place only when staff members move from one group to the next either with the technology or with the problem.<<ETX>>
[Real time systems, Technological innovation, Terminology, Object oriented modeling, Project management, staff members, DP management, technology transfer, Management training, Feedback loop, Job production systems, Technology transfer, software engineering, Mirrors, semiformal process model, Ferranti Computer Systems]
Logical animation
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
When a formal specification of requirements is presented as a logical theory, symbolic animation of the theory can assist with validation. An animator displays a logical model of the formal specification. The authors show how minimal models can be derived from a tableau proof system. Examples illustrate the use of animation with a simple database and its use for validating specifications in the Modal Action Logic of the UK Alvey Forest Project.<<ETX>>
[requirements, logical theory, Alvey Forest Project, Computational modeling, Programming, Educational institutions, Displays, Calculus, Formal specifications, formal specification, tableau proof system, Modal Action Logic, computer animation, Databases, symbolic animation, Animation, software tools, logical model, Logic, Virtual prototyping]
Automatic maintenance of routine programming tasks based on a declarative description
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The authors describe the writing of a small, very specific, software tool, Igor, that automates the creation and maintenance of many routine and repetitive code fragments used in a large software system. Igor is viewed as an application-specific application generator; it generates C source code from a higher level specification as an application generator does, and it is designed to be used only for this one purpose in this one project. The data structure is described in a concise, declarative notation, and a special-purpose translator was written to process the description. The translator generates files of source code that implement the many simple declarations, manipulations, and interrogations of this data structure. The authors discuss experience using this paradigm to implement the intermediate format and how it contributed to solving the larger task of building the CAE (computer-aided engineering) system of which it is a part. The authors compare their solution with other approaches and examine what aspects of the paradigm may be applicable to other software development efforts.<<ETX>>
[C source code, Igor, declarative description, data structure, application-specific application generator, formal specification, large software system, software tools, repetitive code fragments, higher level specification, Automatic programming, software development, Buildings, CAE, Data structures, routine programming tasks, Maintenance, Application software, software tool, Writing, Software systems, Systems engineering and theory, application generators, Computer aided engineering, Software tools, maintenance, special-purpose translator]
CASE and reengineering: from archeology to software perestroika
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
CASE (computer-aided software engineering), reverse engineering, and reengineering together form a coherent set of strategies for software organizations to get a handle on and reclaim their existing software assets. Each of the three supports and depends upon the others for long-term success. It is argued that they will be a permanent part of the process of evolutionary development. Even in future CASE systems with reliable automatic generation of executable code, the resulting system will be (perhaps automatically) reread by a reverse engineering process back into the CASE dictionary. This will allow the discovery of ramifications and side effects which are not foreseeable in the forward engineering process.<<ETX>>
[Heart, Software maintenance, Computer aided software engineering, Dictionaries, Reverse engineering, Redundancy, software assets, reengineering, Reliability engineering, reverse engineering, Application software, computer-aided software engineering, Information analysis, CASE, Investments, software engineering, software tools]
Specification level interoperability
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
Whereas most previous approaches to interoperability have provided support at the representation level, the authors are pursuing an approach that provides support at the specification level. They have developed a model of such support that consists of four components: (1) a unified type model, which is a notation for describing the entities to be shared by interoperating programs; (2) language bindings, which connect the type models of the languages to the unified type model; (3) underlying implementations, which realize the types used by the different interoperating programs; and (4) automated assistance, which eases the task of combining components into an interoperable whole. The authors discuss the representation-level and specification-level approaches to interoperability, describe their current prototype realization of the specification-level approach and their experience with its use, and outline their plans for extending both the approach and its realization. Experiences with the initial prototype are found to be extremely encouraging.<<ETX>>
[Context, Availability, specification level, representation-level, unified type model, interoperability, Distributed computing, formal specification, Programming profession, language bindings, Information science, Numerical analysis, Physics computing, Prototypes, data structures, automated assistance, Vector processors, Lifting equipment]
Position statement: ICSE-12 workshop on industrial experience using formal methods
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The authors define a system of coordinate systems within which one might profitably discuss the proper role of and experience with formal methods in industrial software development. So far, contributions of formal methods have primarily been within narrow confines of techniques for carrying out minute subtasks. Formal studies of the method aspects, those of connecting various notations, techniques, and tools so as to form a coherent, formally justifiable method, have been lacking.<<ETX>>
[industrial software development, formal methods, software engineering, coordinate systems, Software engineering]
An experiment in formal software development: using the B theorem prover on a VDM case study
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The authors give a short overview of the B theorem prover and report on its experimental evaluation. They describe the application of B to control VDM developments and illustrate this by means of excerpts from a case study. This work results in a formal description of the logical framework of VDM and opens perspectives for the reuse of software developments.<<ETX>>
[Computer aided software engineering, reuse, program verification, Programming, formal description, Application software, B theorem prover, experimental evaluation, Design engineering, VDM case study, software reusability, Hardware, software engineering, formal software development, Large-scale systems, theorem proving, Logic, Software tools, Artificial intelligence, logical framework, Software engineering]
Design decisions for the incremental Adage framework
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
Adage, an incremental software development support environment, is discussed. Underlying the Adage architecture is the concept of wide-spectrum services around a common data model, the GDL (Graph Description Language). This model extends the entity-relationship model and provides useful concepts, such as inheritance and self-reflexion, to support genericity and incrementability. Services respond to both end-user and administrator requirements. The last point is achieved through active help in tool integration and tool building and thus in making the environment evolve to cover needs.<<ETX>>
[incremental Adage framework, Software maintenance, tool integration, Power system management, entity-relationship model, Programming, inheritance, Production facilities, wide-spectrum services, Environmental management, GDL, tool building, Software development management, common data model, Graph Description Language, self-reflexion, Computer architecture, Data models, incremental software development support environment, Software tools, programming environments, Monitoring]
Negotiation behavior during requirement specification
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
Negotiation is part of specification; during specification acquisition, users negotiate among themselves and with analysts. During specification design, designers negotiate among themselves and with a project leader. The author reports on work concerned with multiagent specification design. He describes how various agents, often with conflicting goals, can resolve their differences, integrate their results, and produce a unified specification. Such bargaining behavior is both ubiquitous in complex specification and unrepresented by current methods. Automated means to promote integrative behavior during specification are presented. Formal models of users' desires and resolution methods are necessary for integrative reasoning.<<ETX>>
[multiagent specification design, Protocols, Eyes, bargaining behavior, formal specification, requirement specification, integrative behavior, Standby generators, Information science, project leader, specification design, specification acquisition, negotiation, Writing, Contracts, Software engineering]
Practice of quality modeling and measurement on software life-cycle
[1990] Proceedings. 12th International Conference on Software Engineering
None
1990
The authors introduce quality metrics into the quantitative software quality estimation technique, embracing the quality estimate of design, as well as of the source code, in studying a quality quantification support system. They outline a quality quantification technique for this system, describe examples of both its application to actual projects and its evaluation, and consider its relationship conventional techniques for estimate indexing of T.J. McCabe (IEEE Trans. Softw. Eng., vol.SE-2, no.4, 1976) and M.H. Halstead (Elements of Software Science, North Holland, NY, 1977).<<ETX>>
[Software testing, Visualization, software life-cycle, Phase measurement, quality control, software reliability, quality estimate, source code, Programming, Application software, quality modeling, quality quantification support system, Software design, quantitative software quality estimation technique, Phase estimation, Software quality, Quality control, software tools, quality metrics, Software measurement]
Nontechnological issues in software engineering
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
A panel position statement is presented. The author comments that if sociology is more important to success than technology, then the manager has to be principally involved in managing the project's sociology. She/he needs to spend the most time and energy on tasks like team formation, hiring, motivation, bureaucracy reduction, workplace redesign and tuning, and environmental improvement. The author discusses one particular aspect of project sociology requiring special attention: the role of the team. Guidelines for team building are considered.<<ETX>>
[tuning, hiring, Laboratories, Project management, workplace redesign, team formation, motivation, bureaucracy reduction, Programming, Conference management, DP management, Scheduling, technology, project sociology, Technology management, Space technology, Sociology, Employment, success, social sciences computing, software engineering, team building, environmental improvement, Software engineering]
Directions in software engineering education
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors briefly review the history and literature of computing education. The goal is to point out resources for those who would like to gain a perspective on these issues. The authors consider broader issues of software engineering education. Four of these issues are: (a) the definition of software engineering and its relationship to other disciplines; (b) the content of software engineering programs; (c) the organization of computing education at the university; and (d) the relationship of computing education with the applications of computing. These issues are discussed.<<ETX>>
[Educational programs, Vocabulary, computer science education, university, history, History, Application software, Computer science, Computer aided instruction, literature, Computer applications, organization, Software systems, software engineering education, software engineering, Computer science education, computing education, Software engineering]
Techies as nontechnological factors in software engineering?
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The author discusses human issues in software engineering and considers two technological factors that offered dramatic productivity and quality growth in the last decade. The individual differences in the performance range among software engineers are reviewed. Steps to reduce the wide variation in performance among individuals are considered. The first technological factor is having bigger machines with more memory that allowed software engineers more time to work on the task rather than wrestling with machine limitations that inhibited the task. The second technological factor was Lisp machines and the powerful programming environments that accompanied them. The author observes that hardware will still play a dramatic role in productivity growth, but that nontechnological market factors will limit the extent to which the best software ideas will be translated into industry-wide advances.<<ETX>>
[Productivity, bigger machines, memory, Humans, human factors, Software performance, Programming, human issues, nontechnological market factors, Machinery production industries, Materials science and technology, Proportional control, Lisp machines, Automatic control, Computer industry, software engineering, software ideas, programming environments, productivity growth, Software engineering, hardware]
Integrating prior knowledge with a software reliability growth model
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors describe an application of Ohba's inflection S reliability growth model to an IBM software product consisting of 1200 KLOC. The central theme deals with the management requirement that forecasts be made early in the test cycle when there is little data. A simple brute force Monte Carlo simulation is implemented which utilizes prior knowledge about the total number of failures in the product prior to test. Attention is also given to the decisions made regarding the data to select as input to the model. Scaled forecasts and field results are presented which show reasonably good results.<<ETX>>
[scaled forecasts, Costs, software reliability growth model, Computer aided manufacturing, software reliability, prior knowledge, Data engineering, Software reliability, Application software, management requirement, Ohba's inflection S reliability growth model, Monte Carlo methods, Engineering management, Software quality, Computer errors, IBM software product, Software systems, 1200 KLOC, Monte Carlo simulation, Testing]
Hardware/software codesign: a perspective
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors propose that rather than maintain the traditional distinction between hardware and software engineering, a more fruitful approach to computer system design is to combine the hardware and software perspectives from the earliest stages of the design process and exploit the design flexibility and efficient allocation of function that such an approach offers. Since current hardware and software design methodologies have their differences, a unified codesign approach must be developed that will comprise both the hardware and software points of view. Increasingly, computer system design of the future will require this codesign approach. Custom chip design and ASIC design are discussed. Issues in codesign are considered.<<ETX>>
[Military computing, Time to market, hardware engineering, Application software, Distributed computing, Chip scale packaging, computer system design, Software design, Computer displays, unified codesign approach, custom chip design, ASIC design, Computer graphics, design flexibility, circuit CAD, Hardware, software engineering, application specific integrated circuits, Software engineering]
ASPECT: an economical bug-detector
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
A technique-ASPECT-for catching code-level bugs is presented. ASPECT is intended to be economical for everyday software development. It comprises a formal specification language and a code-checking method. Programmers write simple assertions about the information required to compute a result; an efficient mechanical checker can then find bugs in the annotated code. Careless slips that escape type-checking and standard compiler anomaly tests can be detected, without the cost of formal verification or the uncertainty of testing. The essence of ASPECT is reasoning about dependencies between the aspects of abstract objects.<<ETX>>
[program debugging, Costs, Uncertainty, reasoning, annotated code, formal specification, careless slips, specification languages, software engineering, Testing, software development, Power generation economics, bug-detector, Specification languages, formal specification language, Formal specifications, Programming profession, code-level bugs, dependencies, code-checking method, Computer bugs, ASPECT, Error correction codes, abstract objects, Formal verification]
Nontechnological issues in software engineering
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
A panel discussion is reported. Three nontechnological problems were selected which each serve to cluster several lower-level elements. First, the software engineering profession has not produced a cadre of capable/competent managers. Second, software development is largely practiced as an individual creative activity, rather than a team effort. Third, the software engineering community has not taken positive action to reduce the performance (e.g., productivity and quality) differences among individuals (or across teams). Much of the potential impact of technological advances may be blocked by these factors. The panelists offer a wealth of experience, insight, and international perspectives on these nontechnological issues. The panelists address both the problems and steps for their resolution.<<ETX>>
[Productivity, Technological innovation, Costs, software development, Psychology, Wheels, Programming, Engineering management, technological advances, Software systems, software engineering, Impedance, managers, Software engineering]
The Prism model of changes
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The author addresses the problem of managing changes to items of various types in a multitype software environment. Prism, a model of changes, has been designed with the following features: (1) a separation of concern between changes to the described items and changes to the environmental facilities housing these items; (2) a facility, called the dependency structure, for describing various items and their interdependencies, and for identifying the items affected by a given change; (3) a facility, called the change structure, for classifying, recording and analyzing change related data, and for making qualitative judgments of the consequences of a change; (4) identification of the many distinct properties of a change; and (5) a built-in mechanism for providing feedback. The rationale for the design of the model of changes as well as that of the dependency structure and the change structure is given.<<ETX>>
[Data analysis, change structure, Scanning probe microscopy, Project management, Programming, Mechanical factors, Reflection, Environmental management, Computer science, dependency structure, feedback, Councils, multitype software environment, Feedback, model of changes, software engineering, programming environments, Prism]
Parameter value computation by least square method and evaluation of software availability and reliability at service-operation by the hyper-geometric distribution software reliability growth model (HGDM)
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors explain precisely the idea of the capture-recapture process for software faults in the context of a proposed testing environment and introduce the least square method into the model to estimate the parameter values of the HDGM. For real observed data collected during the service-operational phase, the authors show the applicability of the HGDM in estimating the degree of unavailability of a software system in operation (service). Furthermore, the estimated probability of discovering zero faults as service-operation proceeds can be taken as a reliability measure.<<ETX>>
[Availability, Software testing, System testing, capture-recapture process, real observed data, least squares approximations, hyper-geometric distribution software reliability growth model, software reliability, reliability, HGDM, Software reliability, Least squares methods, software faults, Jacobian matrices, service-operational phase, Phase estimation, software availability, service-operation, Software quality, least square method, Software systems, Software measurement, parameter value computation, zero faults]
Domain modeling for software engineering
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors examine domain modeling approaches from the viewpoint of their operational goals. Domain models are representations of an application domain that can be used for a variety of operational goals in support of specific software engineering tasks or processes. Some of the operational goals of domain modeling for software engineering are illustrated. A workshop on domain modeling was organized around operational goals and their resultant domain models and modeling methodologies. The workshop verified the importance of generalized meta-models and their use in instantiating domain knowledge into application domain models. Representation, domain classification and analysis, model development, instantiation evolution and validation, and knowledge structuring and inference are discussed.<<ETX>>
[Educational programs, application domain, Art, inference, Reverse engineering, Metamodeling, Knowledge representation, domain modeling, inference mechanisms, domain classification, Information systems, knowledge structuring, knowledge representation, operational goals, model development, software engineering, generalized meta-models, validation, Software engineering, Context modeling, instantiation]
A methodology for prototyping-in-the-large
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors define prototyping as an experimental activity intended to reduce risk of failure in a software product. In this context, they explore the effect of scale in prototyping and then describe a methodology for prototyping a large application. The authors describe a system being developed to evaluate this methodology, featuring a pair of languages (Promo and Moblog) to serve both large-scale and component-level prototyping needs. The authors conclude with a presentation of how the proposed methodology would be applied to a sample problem, a fault-prediction subsystem within the Space Station Freedom project.<<ETX>>
[Software prototyping, Visualization, Costs, software prototyping, languages, component-level prototyping, high level languages, Educational institutions, scale, Moblog, Space Station Freedom project, Application software, Environmental economics, fault-prediction subsystem, Programming profession, Computer science, large scale prototyping, Promo, Prototypes, Large-scale systems, prototyping-in-the-large]
Software engineering management
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The author discusses the links between management and software engineering. The author tries to show why, in Europe and the USA, management has failed so often in this field. This seems to be due to a combination of narrow-minded attitudes from all the players: academia, work-force and management. The two main issues are software engineering does not produce competent managers, and software development is still an individual affair.<<ETX>>
[Educational programs, Technological innovation, USA, software development, Project management, Humans, Europe, Programming, DP management, Security, Engineering management, management, Innovation management, software engineering, Software engineering]
Tool support for formal methods
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The author discusses the state of the art of formal methods tool support. The focus is on a particular tool: the EVES verification system. EVES is a new formal methods tool that integrates techniques from, among others, language design, language semantics, and logic and automated deduction. EVES demonstrates a number of the features that can be found in the state-of-the-art formal methods tool. The author gives brief introductions to various other formal methods tools (e.g., HOL, EHDM, the Boyer-Moore theorem prover) and discusses how these tools are being applied.<<ETX>>
[EVES verification system, program verification, formal methods tool, Design methodology, HOL, automated deduction, Logic design, Mathematics, Toy industry, Sparks, language design, EHDM, formal specification, language semantics, Computer science, Databases, Hardware, Concrete, Libraries, software tools, logic, Boyer-Moore theorem prover]
Tolerating inconsistency (software development)
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The author presents a simple technique for creating a formalism that allows development environments and other software systems to tolerate and manage their inconsistencies. It softens constraints without introducing special cases by treating violations as temporary exceptions which will eventually be corrected. Until then the offending (i.e., inconsistent) data are automatically marked by guards to identify it to code segments that can help resolve the inconsistency-normally by notifying and involving human agents-and to screen such inconsistent data from other code segments that are sensitive to the violation. The formalism which performs the above operations is described.<<ETX>>
[Humans, Programming, Application software, Environmental management, code segments, Signal resolution, Organizing, Software development management, guards, inconsistencies management, development environments, Prototypes, Software systems, software engineering, Contracts, inconsistencies tolerance, inconsistent data]
PRESTIGE: a CASE workbench for the JSD implementor
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors provide an overview of PRESTIGE, a CASE toolkit which supports the implementation phase of Jackson System Development (JSD). The authors present an outline of JSD, a brief discussion on operational specifications, and a description of the two-stage model of JSD implementation which forms the basis of the toolkit's functionality. The toolkit's current capabilities are described and illustrated through simple examples. Finally, the authors indicate directions for further work.<<ETX>>
[Computer aided software engineering, Law, Inspection, Programming, Mathematics, History, CASE toolkit, Jackson System Development, Computer science, CASE workbench, systems analysis, PRESTIGE, JSD, software tools, two-stage model, Software tools, Legal factors, Bars]
Experiences with an environment generation system
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors report on research experience using the Gandalf environment generation system as a prototyping vehicle for the Inscape environment. A Gandalf-based environment consists of four parts: a structure editor kernel, which is simply linked into each executable, a set of grammar tables describing the language to the kernel in terms of its abstract syntax, one or more concrete syntax views, and a collection of action routines written in the extension language, ARL. Positive aspects of the research included experimentation, incremental evolution, multiple views, the coupling of semantic and editing actions, and the use of domain-specific facilities. Negative aspects consisted primarily of problems with presentation and object management.<<ETX>>
[Modular construction, grammar tables, Logic programming, prototyping, action routines, Data structures, Specification languages, multiple views, Vehicles, Gandalf environment generation system, Computer science, structure editor kernel, abstract syntax, Computer languages, incremental evolution, Prototypes, Concrete, Performance analysis, Inscape environment, ARL, programming environments, domain-specific facilities]
Software reuse in an industrial setting: a case study
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
A summary of an ongoing case study of software reuse being carried out by the authors in cooperation with Sperry Marine Incorporated is presented. The goals of the study are to analyze the problems that limit reuse and to seek solutions suitable for industrial application. To help determine its suitability for use within Sperry Marine, an experimental evaluation of object-oriented development was performed that focused on a specific subsystem domain found in several of Sperry Marine's commercial software products. A reuse library populated with classes written in C++ was prepared, and from this library simple versions of two representative subsystems were built. Measurement of the resulting software showed a very high level of reuse.<<ETX>>
[Performance evaluation, Computer aided software engineering, object-oriented programming, C++, software reuse, Computer aided manufacturing, Software performance, performance evaluation, Application software, experimental evaluation, commercial software products, reuse library, Computer science, Software libraries, Sperry Marine, object-oriented development, representative subsystems, software reusability, Computer industry, Software measurement, Marine vehicles, industrial setting]
Cost estimation of software intensive projects: a survey of current practices
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors describe a survey conducted at the Jet Propulsion Laboratory (JPL) to estimate software costs for software intensive projects in JPL's technical divisions. Respondents to the survey described what techniques they use in estimating software costs and, in an experiment, each respondent estimated the size and cost of a specific piece of software described in a design document provided by the authors. It was found that the majority of the technical staff estimating software costs use informal analogy and high-level partitioning of requirements, and that no formal procedure exists for incorporating risk and uncertainty. The technical staff is significantly better at estimating effort than size. However, in both cases the variances are so large that there is a 30% probability that any one estimate can be more than 50% off.<<ETX>>
[Costs, Uncertainty, Data analysis, Laboratories, NASA, Scheduling, cost estimation, design document, economics, high-level partitioning, Space technology, software intensive projects, Propulsion, survey, software engineering, Large-scale systems, Contracts]
The information technology security evaluation criteria
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The author presents the technical approach adopted for the information technology security evaluation criteria (ITSEC). The ITSEC are the result of harmonizing the security evaluation criteria of France, Germany, the Netherlands and the United Kingdom. Various background information which led to the development of the ITSEC in their current form with respect to structure, underlying evaluation philosophy, software engineering practice and external influence is presented. A wider field of applicability than previously developed security criteria was one of ITSEC's goals. By drawing from the best features of existing national criteria and harmonizing where possible and innovating only where necessary, it is expected that this goal has been achieved.<<ETX>>
[Military standards, social aspects of automation, underlying evaluation philosophy, security of data, Information security, software engineering practice, external influence, Information technology, office automation, information technology security evaluation criteria]
Designing software for use by humans, not machines
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors describe an architecture that supports a more user-centered way to design software applications architectures, and by using a specific example, they also describe benefits gained by the software applications that result. The architecture aims to resolve some management and maintenance problems which have arisen in the telecommunications support systems network specifically, but which the authors also envision arising in any large-scale, multivendor, database environment. This examination includes consideration of technological advances in user-centered design, recent discoveries about the ability to separate processing functionalityfrom user interface functionality, and the rapidly expanding wealth of available user interface software tools.<<ETX>>
[user interface functionality, User centered design, software design, multivendor, Humans, user interface software tools, Telecommunication network management, user interfaces, Application software, Environmental management, database environment, processing functionality, Software design, software applications, Databases, telecommunications support systems network, Computer architecture, User interfaces, software engineering, Large-scale systems, user-centered design]
Understanding natural programs using proper decomposition
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The author presents a practical method for automatic control concept recognition in large, unstructured imperative programs. Control concepts are abstract notions about interactions between control flow, data flow, and computation, e.g., read-process loops. They are recognized by comparing a language-independent abstract program representation against standard implementation plans. Recognition is efficient and scalable because the program representation is hierarchically decomposed by propers (single entry/exit control flow subgraphs). A recognition experiment using the UNPROG program understander shows the method's performance, the role of proper decomposition, and the ability to use standard implementations in a sample of programs. How recognized control concepts are used to perform Cobol restructuring with quality not possible with existing syntactic methods is described.<<ETX>>
[automatic control concept recognition, unstructured imperative programs, read-process loops, proper decomposition, Programming profession, computation, Cobol restructuring, propers, control flow, UNPROG program understander, natural programs, Production, Automatic control, single entry/exit control flow subgraphs, Robustness, software engineering, Data flow computing, Joining processes, data flow, programming, language-independent abstract program representation, Testing]
Position statement (software engineering)
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
A position statement is provided. To consider nontechnical aspects of the software development process, the author focuses on the differences in the goals of various process agents. There are several levels of concern in considering team efforts in the software process: organization, project, group and individual. There are three viewpoints in software development: the user's viewpoint is that of product-oriented project goals; the manager's viewpoint is that of the process model and the supporting environment: the developer's viewpoint is process oriented and centered around individual goals. The dichotomy between the authority and direction of management and the freedom and creativity of the individual is emphasized.<<ETX>>
[Process design, Chaos, software development, individual, Humans, Project management, process oriented, Programming, DP management, product-oriented project, supporting environment, Environmental management, Software development management, team efforts, organization, process model, software engineering, Software engineering, process agents, software process, group]
Defect type and its impact on the growth curve (software development)
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors present an empirical investigation on possible cause and effect relationships between defects and the software development process. The authors use defect data from an operating systems development project and find that initialization defects are strongly related to the inflection noticed in the reliability growth. The defect type distribution identified process problems that concurred with the developer's hindsight. Thus, it is shown that it is plausible that there exist other cause-effect relationships that could be identified. This finding could pave the way for a more systematic process control methodology to be applied to software development.<<ETX>>
[Availability, software development process, initialization defects, software development, Area measurement, software reliability, cause-effect relationships, Process control, reliability, Programming, Turning, operating systems development project, Manufacturing processes, Operating systems, systematic process control methodology, defect type, growth curve, Software measurement, Marine vehicles, Testing, inflection]
Using weaves for software construction and analysis
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors discuss the architectural features of weaves, their implementation, and their use in a variety of applications. Weaves are networks of concurrently executing tool fragments that communicate by passing objects. Weaves are distinguished from other dataflow styles by their emphasis on instrumentation, continuous observability, and dynamic rearrangement: basic low-overhead instrumentation is inserted automatically, executing weaves can be observed at any time by means of sophisticated analysis agents, without degrading the performance of the weave, and weaves can be dynamically snipped and spliced without interrupting the data flow.<<ETX>>
[Instruments, object passing, Software performance, Debugging, dynamic rearrangement, dataflow styles, Telemetry, networks, continuous observability, analysis agents, parallel programming, Sensor arrays, Aerospace engineering, concurrently executing tool fragments, Satellites, software construction, software analysis, Aerospace testing, Software systems, software engineering, Performance analysis, instrumentation, weaves]
Progress toward automated software testing
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
Mothra is an integrated environment for automated software validation. Using Mothra, a tester can create and execute test cases, measure test case adequacy, determine input-output correctness, locate and remove faults or bugs, and control and document the test. There are no size constraints built into the design of the environment. A goal of the research has been to exploit computational opportunities in each of the major subtasks of software validation. The author has attempted to reduce test case generation, measurement of the effectiveness of test cases, input-output correctness checking, and debugging to one or more computational metaphors and to design the appropriate algorithms. The result is a system where there is a high degree of control over the apparent cost of testing.<<ETX>>
[Software testing, System testing, Costs, program testing, program verification, test documentation, Genetic mutations, fault location, automated software testing, test cases, integrated environment, automated software validation, fault removal, bug location, Design engineering, input-output correctness, Mothra, Computer bugs, User interfaces, Automatic control, Motion pictures, programming environments, Clocks, bug removal]
A failure of management nerve and vision (software engineering)
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
A panel discussion on nontechnological issues in software engineering is presented. The position adopted includes these aspects: that nontechnological rather than technological problems are the important and difficult ones; that management failure is preeminent among nontechnological problems-a failure of management nerve and vision; and that seeking solutions in the management domain is therefore vital. The terms software engineering and management are broadly defined. It is assumed that management embraces the full range of managerial concerns (e.g., objectives, strategy, policy, planning, organization, control, and motivation) and all levels (from senior to supervisory). Typical manifestations of management failure are outlined, and approaches to this problem are considered. Technological and nontechnological issues are discussed.<<ETX>>
[Process design, management failure, Humans, motivation, DP management, control, planning, Technology management, Engineering management, Management information systems, organization, Software systems, Systems engineering and theory, Product development, software engineering, objectives, Resource management, strategy, Software engineering, policy]
Hiding distribution in distributed systems
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The author focuses on the evolution of transparency in distributed systems, that is, the provision of logically centralized system facilities on distributed system architectures. The author proposes a comparison between the virtues and faults of centralized and distributed architectures, and reviews the evolution of distributed systems. The notion of transparency is described in detail with some examples. The main features of new technologies used to build transparent subsystems on distributed architectures are discussed.<<ETX>>
[Laboratories, distributed processing, Command languages, Application software, Distributed computing, parallel programming, Programmable control, Computer languages, transparency, Software systems, distributed systems, Hardware, software engineering, Workstations, Books, logically centralized system facilities]
User interface development and software environments: the Chiron-1 system
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors discuss a list of requirements which should be met by the user interface development system (UIDS) to address the special demands of software environments. Chiron-1 uses an annotation-based, concurrent model which makes a clear separation between an application's functional and user interface parts, while still promoting effective communication between those parts. The UIDS model and language interface and the UIDS architecture are described. The authors describe how applications are built with Chiron-1, and illustrate the process with an example. Important Chiron-1 concepts are presented, and their significance in the software environment context is explained. The authors detail some design issues and report on the status of current work and the plans for future research.<<ETX>>
[annotation-based, concurrent model, Object oriented modeling, language interface, Programming, Maintenance engineering, user interface development system, Control systems, user interfaces, software environments, Chiron-1, Computer science, Runtime, User interfaces, Software systems, Libraries, Software tools, programming environments]
Metric-driven analysis and feedback systems for enabling empirically guided software development
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors summarize the goals of metric-driven analysis and feedback systems and describe a prototype system, Amadeus, which defines abstract interfaces and embodies architectural principles for these types of systems. Metric-driven analysis and feedback systems enable developers to define empirically guided processes for software development and maintenance. The authors provide an overview of the Amadeus system operation, including an example of the empirically guided process, a description of the system characteristics, an explanation of the system conceptual operation, and a summary of the users' view of the system. The centerpiece of the system is a pro-active server, which interprets scripts and coordinates event monitoring and agent activation. Amadeus provides an extensible framework for adding new empirically based analysis techniques.<<ETX>>
[Software maintenance, server, architectural principles, Programming, metric driven analysis, Information analysis, Amadeus, feedback, Feedback, Prototypes, Computer architecture, empirically guided software development, Software measurement, scripts, abstract interfaces, overview, Calendars, coordinates, feedback systems, Application software, software maintenance, Computer science, system characteristics, maintenance, software metrics]
Cognitive tools for locating and comprehending software objects for reuse
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The authors describe a conceptual framework to facilitate software reuse. It is shown that high functionality computer systems by themselves do not provide sufficient support for software reuse. Two systems that support this framework, CODEFINDER and EXPLAINER, are presented. CODEFINDER addresses issues on information access for software reuse. Support for comprehending software objects is demonstrated with EXPLAINER. A scenario describing how the two systems are used in a reuse situation is presented. The authors show how these systems fit into the bigger pictures of software development environments, address limitations of the systems, and discuss future directions.<<ETX>>
[Process design, Software prototyping, CODEFINDER, software objects for reuse, Design methodology, high functionality computer systems, software development environments, Humans, Programming, software objects, Computer science, EXPLAINER, information access, Computer architecture, software reusability, Cognitive science, Problem-solving, cognitive tools, Software tools, conceptual framework]
Toward new techniques to assess the software implementation process
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The author presents the concept of software boundaries and their automated detection. He describes an objective, high-level assessment technology to support process control of software development. The technique is largely independent of underlying design and development methods. An example illustrates an automated system partitioning application. The technique analyzes the end product and the delivered software code. By suitable adjustment of the analysis goals, measures, and criteria, the technique can help evaluate the effectiveness of many software implementation methods. The author discusses the technique's potential for increasing confidence in the fault-tolerating properties of a system.<<ETX>>
[Performance evaluation, high-level assessment technology, software development, Design methodology, software reliability, Process control, confidence, Programming, fault-tolerating properties, Maintenance, Application software, software boundaries, Manufacturing processes, software implementation, automated system partitioning, process control, Software systems, Software measurement, Total quality management, automated detection]
Ethical considerations in software engineering
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The author discusses ethical considerations that arise in the practice of software engineering and uses cases to help focus the discussion on ethical concerns in the practice of software engineering. The author comments on the relationship between software safety and ethics, informed consent and ethical decisions, the customer's responsibility for end-product quality, internal performance standards and customer consent to inferior work, and techniques for addressing ethical issues.<<ETX>>
[social aspects of automation, Military computing, performance standards, Laboratories, Software safety, Computer science, ethical considerations, Ethics, Engineering management, informed consent, safety, User interfaces, Cities and towns, Software standards, software engineering, end-product quality, Software engineering]
Formal methods: an international perspective
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The goal of formal methods is to base the software development process upon a workable set of mathematical techniques. The common names associated with various subclasses of formal methods express both the purpose and mode of the technique; formal specification, mathematical verification, proofs of correctness, formal description languages, rigorous development methods, stepwise refinement, etc. North American and European research groups took different technical directions. The author provides the perspective of one US researcher who has been particularly influenced by the international forces shaping the formal methods field as both a technical subject and a social enterprise.<<ETX>>
[Military standards, Regulators, rigorous development methods, program verification, software development, Government, correctness proofs, Programming, Mathematics, Formal specifications, History, Cultural differences, formal specification, mathematical techniques, proofs of correctness, stepwise refinement, specification languages, formal methods, Hardware, software engineering, System software, formal description languages, mathematical verification]
Process assessments in NASA
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
A software process assessment procedure, recently refined by the Software Engineering Institute (SEI), gives managers a useful tool to identify the strengths and weaknesses of software organizations. The author discusses how process assessment techniques were introduced to NASA and how the procedure was enlarged to evaluate not just software development but also software quality assurance operations. Initially, the techniques were extended to cover contract management and especially software quality assurance organizations. A preliminary assessment tool (questionnaire) consisting of 98 items was introduced. After some revision, it demonstrated its usefulness at a series of on-site surveys. The combined surveys demonstrated that a NASA questionnaire can point a team in the right direction.<<ETX>>
[contract management, software development, NASA, Space shuttles, Programming, DP management, software process assessment procedure, Software Engineering Institute, Space technology, Software quality, Propulsion, software engineering, software quality assurance operations, Software measurement, project engineering, Software tools, software organizations, Software engineering, Quality management]
An intelligent tool for re-engineering software modularity
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The author describes a software tool that provides heuristic modularization advice for improving existing code. A heuristic design similarity measure is defined, based on the Parna information hiding principle. The measure supports two services: clustering, which identifies groups of related procedures, and maverick analysis, which identifies individual procedures that appear to be in the wrong module. The tool has already provided useful advice in several real programming projects. The tool will soon incorporate an automatic tuning method, which allows the tool to learn from its mistakes, adapting its advice to the architect's preferences. A preliminary experiment demonstrates that the automatically tuned similarity function can assign procedures to modules very accurately.<<ETX>>
[Software testing, Software maintenance, heuristic modularization advice, maverick analysis, re-engineering, Tree graphs, software tools, Monitoring, heuristic design similarity measure, Educational institutions, Large scale integration, software modularity, Intelligent structures, software maintenance, code improvement, Programming profession, intelligent tool, Computer languages, software tool, Parna information hiding principle, software reusability, clustering, automatic tuning, mistake learning, Software tools]
A comparison of US and Japanese software process maturity
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
The report characterizes the software processes currently used by software managers and practitioners in the US and Japan. The US data for this comparative study are drawn from extensive Software Engineering Institute (SEI) assessments conducted from 1987 through 1989. The Japanese maturity data were gathered in connection with a three-week trip to Japan by the authors. This includes data on 168 US projects and 196 Japanese software projects. The data indicate that the Japanese software industry is, in some respects, both weaker and stronger than that of the US. There are also significant differences in the maturity findings between the US and Japanese organizations, and these have important implications for software organizations in both countries.<<ETX>>
[DP industry, Japan, Companies, Application software, Defense industry, Engineering management, software processes, Computer industry, Mice, software engineering, software projects, Japanese software industry, US, Software engineering, Business]
Augmenting SADT to develop computer support for cooperative work
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
Using the language-action perspective proposed by T. Winograd and F. Flores (1986), the author creates a general framework for both systems analysis and its practice. Structured analysis and design technique (SADT), a systems analysis methodology, is augmented using this framework. This work took place on the CONTRACT (commitment negotiation and tracking tool) project. The author includes the experiences of both users and systems analysts during the project, and emphasizes how to develop SADT descriptions with users to represent the richness and complexity of social interactions at work. The resulting software specification is also presented, including how it aided the work of the people who actually helped develop it.<<ETX>>
[Real time systems, structured analysis and design technique, computer support, software specification, CONTRACT project, Humans, Programming, Information analysis, commitment negotiation and tracking tool, Graphics, Software design, Feedback, Employment, systems analysis, groupware, SADT, Collaborative work, software engineering, cooperative work, language-action perspective, Contracts]
Software reuse: is it delivering?
[1991 Proceedings] 13th International Conference on Software Engineering
None
1991
A panel discussion is presented to examine the evidence, pro and con, that software reuse is delivering. Verifiable and repeatable increases in software quality and productivity in field settings are used as criteria for the success of reuse. Reusability is defined as a technology that can be of significant value to software development and maintenance. A scale to evaluate whether reuse is delivering is outlined. The scale is ordinal, with five being the best possible rating. The three respondents, from Germany, Japan, and the US, discuss how reuse, as defined, is delivering in their parts of the world.<<ETX>>
[Productivity, Software maintenance, software reuse, software development, software reliability, Japan, Capacitive sensors, software quality, software maintenance, Equations, Silver, software productivity, Software quality, software reusability, Software measurement, Software reusability, Object oriented programming, Germany, US, Qualifications]
High-pressure steam engines and computer software
International Conference on Software Engineering
None
1992
It is argued that we do not want to impede progress by writing unachievable standards or inadvertently increase risk by implementing the wrong standards. We have not scientifically established the benefits and effectiveness of most of our software engineering techniques. Depending on a particular software engineering methodology to assure safety by assuming it will produce error-free or ultra-high reliability software is dangerous. And as the technology progresses, standards that require the use of specific approaches often lag behind. Manufacturers may feel no ethical or legal duty to go beyond what is required in the standard. Moreover, manufacturers or those who will personally benefit financially from particular techniques being included or not included in the standards sometimes play a dominant role in the drafting process. The result may be watered down req1~irements or the recommendation of techniques with more commercial than technical value. The alternative is to construct flexible standards specifying general criteria for acceptability of a methodology instead of a specific methodology and ensuring that those building safety-critical software have the competency and personal responsibility to use the best approaches available at the time and for the particular project characteristics. As Edison argued with respect to electricity, increased government regulation of our technology may not be to anyone's benefit; but it is inevitable unless we, as the technology's developers and users, take the steps necessary to ensure safety in the devices that are constructed and technical competencies in those that construct them.
[Computer science, Technological innovation, Steam engines, Boilers, Software, History, Accidents]
Trusted Computing Systems: The ProCoS* Experience
International Conference on Software Engineering
None
1992
We delineate a fine-grained approach to the development of embedded computing systems, including such which exhibit real-time properties - that is: reactive systems. The "approach" is a postulate - it has not yet been widely tested. And the "dogmas" of the paper are rather idealistic - but indicative of current, we believe, leading research directions that is already being transferred into industry. The essence of the approach is twofold: (i) An increased emphasis on requirements development, a phase prior to software development, but not just the development of software requirements, as so restricted in [28], but requirements to and assumptions about the computing systems environment; and (ii) that both requirements and software development be so organised as to allow calculation of system properties. The approach arose out of the ESPRIT Basic Research Action project: ProCoS - and we also review the larger facets of this project. The paper provides an overview sketch: technical & scientific evidence is given in the referenced literature. The paper also outlines an initial research emphasis of the UNIIIST: The United Nations University's International Institute for Software Technology, a research and post- graduate training center for the developing world, which commenced operation in Macau 1. April, 1992.
[Real time systems, Embedded computing, Councils, Programming, Paper technology, Australia, Distributed computing, Embedded software, Testing]
Design of dependable real-time systems (extended abstract)
International Conference on Software Engineering
None
1992
Summary form only given. In this presentation, the current challenges to the power sector are illustrated. The main pillars of the strategy of power sector and their justifications are presented and discussed. The draft of the new electricity law is presented including how the law has handled the strategic targets. Also, the impact of the law on; the structure of the power sector, development of renewable energy and supporting energy efficiency, is discussed. Finally the challenges facing the reforms are presented.
[Real time systems, Process design, Computer science, Distributed information systems, Programming, Calculus, Safety, Timing, Machinery]
A measure for composite module cohesion
International Conference on Software Engineering
None
1992
An important software design activity is the decomposition of complex systems into conceptually independent modules that cooperate to achieve a desired result. This modularization represents a significant software engineering activity that continues to receive considerable research attention. This paper illustrates how software may be modularized by automatically determining the cohesiveness of modules in the system. Module cohesion is defined to be a quality attribute that seeks to measure the singleness of purpose of a module. We propose a metric that measures the cohesion of individual subprograms of a software system as related to each other. This metric is illustrated with detailed examples and is supported with empirical evidence supporting the viability of the measure.
[Process design, Software design, Laboratories, Packaging, Software systems, Maintenance, Software measurement, Software reusability, Guidelines]
Estimating software fault content before coding
International Conference on Software Engineering
None
1992
The standard software development process consists of multiple stages: requirements, design, coding, system test, first office, and finally delivery. An objective of this process is to minimize the number of faults in delivered code. Root cause analysis shows that many of the faults can be traced back to requirements or design faults. As part of the software development process, reviews are conducted to remove these faults before the requirements or design document is passed on to the next step. We have developed a method of instrumenting a review process to record document faults discovered by reviewers during their preparation. Then, using statistical techniques related to capture-recapture methods, we estimate the number of undiscovered faults remaining in the document. The key idea to our method is to look at how many common faults independent reviewers find and then extrapolate to die total number of faults. We do not seed the document with artificial faults - no additional faults are introduced. We have applied our methods to 13 review sessions (either feature requirements or feature design) and are in the process of a longitudinal study tracing these features. Our results to date estimate that about 20% of the faults are undetected by reviews. When the predicted number of undetected faults is greater than 20%, consideration should be given to reworking design and/or rereviewing the result. One surprising by-product of this study is a quantification of the number of faults found by group reviews. We find that only about 10% of the (discovered) document faults are found at the review (90% are found in preparation) and that the lead time to schedule a review is about ten working days.
[Software testing, Process design, System testing, Design engineering, Software design, Instruments, Programming, Systems engineering and theory, Software standards, Standards development]
Testing for linear errors in nonlinear computer programs
International Conference on Software Engineering
None
1992
This paper provides an approach to test nonlinear functions in computer programs, whether this function is used for control flow, such as a predicate inequality or equality constraint, or is given as an input-output relationship. This approach will obtain test data to detect linear errors in the given nonlinear function. An error-space criterion previously given by Zeil will be utilized, and a necessary and sufficient condition for the test data will be specified to guarantee the satisfaction of this criterion. This leads to a simple and efficient method to select test data which satisfies that condition; only (n+2) tests are required, where n is the number of input variables. An analysis will be given to show that this simple approach can be very effective in detecting nonlinear errors as well.
[Software testing, Computer science, Sufficient conditions, Input variables, Computer errors, Cost function, Machinery]
Towards A Method Of Programming With Assertions
International Conference on Software Engineering
None
1992
Embedded assertions have long been recognized as a Po- tentially powerful tool for automatic runtime detection of software faults during debugging, testing and maintenance. Yet despite the richness of the notations and the maturity of the techniques and tools that have been developed for programming with assertions, assertions are a development tool that has seen little widespread use in practice. The main reasons seem to be that (1) previous assertion processing tools did not integrate easily with existing programming environments, and (2) it is not well understood what kinds of assertions are most effective at detecting software faults. This paper describes experience using an assertion processing tool that was built to address the concerns of ease-of-use and effectiveness. The tool is called App, an Annotation Pre- Processor for C programs developed in UNIX-based de- velopment environments. APP has been used to develop a number of software systems over the past three years Based on this experience, the paper presents a classification of the assertions that were most effective at detecting faults. While the assertions that are described guard against many common kinds of faults and errors, the very commonness of such faults demonstrates the need for an explicit, high-level, automatically check-able specification of required behavior. It is hoped that the classification presented in this paper will prove to be a useful first step in developing a method of programming with assertions.
[Software testing, Runtime, Fault detection, Automatic testing, Permission, Software systems, Formal specifications, Software tools, Software debugging, Embedded software]
Specification-based test oracles for reactive systems
International Conference on Software Engineering
None
1992
The testing process is typically systematic in test data se- lection and test execution. For the most part, however, the effective use of test oracles has been neglected, even though they are a critical component of the testing process. Test or- acles prescribe acceptable behavior for test execution. In the absence of judging test results with formal oracles, testing does not achieve its goal of revealing failures or assuring cor- rect behavior in a practical manner; manual result checking is neither reliable nor cost-effective. We argue that test oracles should be derived from specifications in conjunction with testing criteria, represented in a common form, and their use made integral to the test- ing process. For complex, reactive systems, oracles must reflect the multiparadigm nature of the required behavior. Such systems are often specified using multiple languages, each selected for its utility specifying in a particular computational paradigm. Thus, we are developing an approach for deriving and using oracles based on multiparadigm and multilingual specifications to enable the verification of test results for reactive systems as well as less complex systems.
[Computer science, System testing, Materials testing, Government, Aerospace materials, Software systems, Aircraft manufacture, Distributed computing, Machinery]
Connecting software components with declarative glue
International Conference on Software Engineering
None
1992
We describe Bart, a software bus that addresses the problem of maintaining flexibility in software systems by supporting component independence. Software components can be built to be independent of the context in which they are used, allowing them to be reused in many different situations. Component independence also allows a software system to be extended by adding new components without modifying existing ones. The connections between software compo- nents are described using SCL, a declarative glue language that defines the relationships between data models in different components. This glue language is compiled into an efficient procedural form and, to reduce communication overhead, executed in the process where the data resides. Bart is a software bus that handles message transport, data sharing, and data translation. It operates in a distributed environment and can connect components written in different programming languages. We illustrate Bart by showing how it is used to support a hypertext system.
[Computer languages, Software maintenance, Databases, Laboratories, Software systems, Data models, Joining processes, Hypertext systems, Milling machines]
Visualizing and querying software structures
International Conference on Software Engineering
None
1992
Software engineering problems often involve large sets of objects and complex relationships among them. This report proposes that graphical visualization techniques can help engineers undersland and solve a class of these problems. To illustrate this, two problems are analyzed and recast using the graphical language GraphLog. The first problem is that of simplifying dependencies among components of a system, which translates into removing cycles from a graph. The second problem is that of designing an efficient code overlay structure, which is facilitated in several ways through graphical techniques.
[Visualization, Computer aided software engineering, Software design, Power engineering computing, Ducts, Displays, Visual databases, Software tools, Software engineering]
Software evolution through iterative prototyping
International Conference on Software Engineering
None
1992
The process of developing and evolving complex software systems is intrinsically exploratory in nature. Some prototyping activity is therefore inevitable in every stage of that process. Our program development and evolution methodology is predicated upon this observation. In this methodology, a prototype software system is developed as an approximation to an envisioned target system by compromising along one or more of the following dimensions: system performance, system functionality, or user interface. However, the prototype is not the end-product of the pro- cess. Instead, we support iterative evolution of the prototype towards the envisioned system by gradually dealing with the three general areas of compromise. This paper describes the methodology of using this al- ternative lifecycle; to wit, the programming language concepts and related implementation technology that support practice of the suggested methodology. We summarize the lessons we have learned in building and using this technology over the last several years.
[Software prototyping, Computer languages, System performance, Prototypes, User interfaces, Programming, Machinery, Software engineering, Information analysis]
Object-oriented analysis for evolving systems
International Conference on Software Engineering
None
1992
We are investigating the claim that object- oriented analysis (OOA) requirements models can be changed, reused, and integrated more easily than other kinds ofrequirements models. In thispaper, we describe one part of that investigation: an experiment involving an OOA method in which the requirementsfor an automated teller machine (ATM) system are changed and the effects on the model are assessed.
[Object oriented modeling, Design methodology, Robustness, Microelectronics, Distributed computing, Machinery, Testing]
Formal Specification Of Asynchronous Distributed Real-time Systems By APTL
International Conference on Software Engineering
None
1992
We propose a language, Asynchronous Propositional Temporal Logic (APTL), for the specification and verification of asynchronous hard-real-time systems. APTL extends the logic TPTL [1] by explicitly introducing local clocks whose numerical readings cannot be arithmetically compared to determine temporal precedence. To interprete timing inequalities involving different local clocks, we introduce an asynchronous distributed system model which interpretes timing inequalities in terms of the temporal precedence of local clock readings. With this model, we give the formal semantic definition of APTL formulas. APTL is especially useful for specifying and reasoning about properties inherent in asynchronous environments such as the bounded drift rates of local clocks. Two versions of a railroad crossing example are used to illustrate the expressiveness of APTL.
[Real time systems, Timing, Formal specifications, Logic, Distributed computing, Machinery, Contracts, Clocks, Arithmetic]
Validating Real-time Systems By History-checking TRIO Specifications
International Conference on Software Engineering
None
1992
We emphasize the importance of formal executable specifications in the development of real-time systems, as a means to assess the adequacy of the requirements before a costly development process takes place. TRIO is a first order temporal logic language for executable specification of real-time systems that deals with time in a quantitative way by providing a metric to indicate distance in time between events and length of time intervals. We surnmarise the language, its straightforward model-theoretic semantics, and a tableaux-based algorithm to decide satisfiability. Then we present an efficient algorithm to perform history-checking, i.e., to check tha a history of the system satisfies the specification. This algorithm can be used as a basis for an effective specification testing tool. The algorithm is described, a qualitative estimation of its complexity is provided, and the main functionalities of the tool are presented, together with sample test cases. Finally, we draw the conclusions and indicate directions of future research.
[Real time systems, Embedded computing, Computer applications, Logic, Formal specifications, History, Distributed computing, Machinery, Testing]
Graphical specifications for concurrent software systems
International Conference on Software Engineering
None
1992
We present a description of a graphical interval logic that is the foundation of a toolset we are developing to support formal specification and verification of concurrent software systems. Experience has shown that most software engineers find standard temporal logics difficult to under- stand and to use. Our objective is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. To illustrate the use of our graphical interval logic, we provide a specification for a readers/writers database system and prove several properties of the specification.
[Concurrent computing, System testing, Software systems, Software standards, Hardware, Mechanical factors, Logic, Formal specifications, Application software]
Seeking concurrency in rule-based programming
International Conference on Software Engineering
None
1992
This paper describes a formal approach for developing concurrent rule-based programs. Specification refinement is used to generate an initial version of the program. Program refinement is then applied to produce a highly concurrent and efficient version of the same program. Techniques for deriving concurrent programs through either specification or program refinement have been described in previous literature. The main contribution of this paper consists of extending the applicability of these techniques to a broad class of rule-based programs. To the best of our knowledge, this is the first time formal derivation is employed in the context of rule-based programming.
[Concurrent computing, Computer science, Refining, Formal specifications, Artificial intelligence, Intelligent systems, Distributed computing, Machinery, Parallel algorithms]
The property vector specification of a multiset iterator
International Conference on Software Engineering
None
1992
Trace assertion specification methods constrain behaviour at the interface of a module by identifying legal sequences of calls to the module's access programs. The legality of extending a trace by a call is specified by assertions that ex- press properties that the trace and call must satisfy. Trace specifications can often be simplified by formulating asser- tions about representative normal (or canonical) form traces, but this approach is only useful when convenient forms can be found. This paper: 1) introduces a new trace assertion method, called the Property Vector (PV) method, 2) demonstrates the method by specifying a multiset iterator module previously identified as difficult to specify [Lam90], 3) compares the PV method to other methods. The main new feature of the PV method is the use of a vector of property variables that explicitly represent relevant properties of a trace that influence a module's future behaviour. It is possible to obtain a simple trace assertion specification for a module even though convenient (legal) normal/canonical form traces are not apparent. Normal/canonical form trace assertion methods are special cases of the PV method where the only relevant property of a trace is its equivalent normal/canonical form.
[Law, Packaging, Distributed computing, Machinery, Legal factors]
A Toolbox For The Verification Of LOTOS Programs
International Conference on Software Engineering
None
1992
This paper presents the tools ALDEBARAN, CESAR, CESAR.ADT and CLEOPATRE which constitute a tool- box for compiling and verifying LOTOS programs. The principles of these tools are described, as well as their performances and limitations. Finally, the formal verification of the ret/REL atomic multicast protocol is given as an example to illustrate the practical use of the tool- box.
[Real time systems, Partial response channels, Automata, Control system synthesis, Distributed control, Multicast protocols, Electronic mail, Logic, Hardware design languages]
Concepts and implementation of a rule-based process engine/sup +/
International Conference on Software Engineering
None
1992
The key topic is the way how the Merlin process-centered environment provides execution of rule-based persistent software processes. Any software process in Merlin is de- scribed by rules which are executed either in a forward chaining or backward chaining mode and which are made persistent by storing them in an object-oriented data base After a brief introduction to the general Merlin concepts the paper focuses on the representation of rules in the un- derlying data base and their corresponding flexible and ef- ficient execution.
[Software prototyping, Object oriented modeling, Software performance, Collaborative work, Software tools, Environmental management, Software development management, Context modeling]
Process programming by hindsight
International Conference on Software Engineering
None
1992
Process programming refers to the activity of algorithmicly describing models of programming activities (processes). A serious limitation of process programming has been that . it is often hard to describe a programming process a priori. In this paper we present an approach to process programming which addresses this limitation. Our approach is based on the premise that process programs are easier to describe in hindsight rather than by foresight, and hence can be synthe- sized by observing and analyzing a recorded process history. In this paper we describe an adaptation of the well-known explanation-based learning algorithm to synthesize a process fragment from a process history. We demonstrate the useful- ness of the approach on a realistic example of coordinating file changes through a version control system.
[Humans, Control system synthesis, Software systems, Control systems, History, Distributed computing, Milling machines, Programming environments, Software engineering]
Call path profiling
International Conference on Software Engineering
None
1992
Practical performance improvement of a complex program must be guided by empirical measurements of its resource usage in order to avoid wasting programmer time and to avoid needlessly destroying the original, clear structure. Previous approaches to measuring programs, while very useful, have shortcomings in that they provide either too little or too much information. In this paper, I classify previous approaches, explain their strengths and weaknesses, and describe a new approach, call path profiling, that reports resource usage of subroutine calls in their full lexical contexts. This relates resource usage directly to design decisions, providing better guidance to the optimizer. I also discuss the implementation of cpp, a working prototype that operates on Common Lisp programs.
[Algorithms, Computer bugs, Prototypes, Debugging, Time measurement, Performance analysis, Distributed computing, Design optimization, Testing]
GENOA - A Customizable, Language- And Front-end Independent Code Analyzer
International Conference on Software Engineering
None
1992
Programmers working on large software systems spend a great deal of time examining code and trying to understand it. Code Analysis tools (e.g., cross referencing tools such as CIA and Cscope) can be very helpful in this process. In this paper we describe GENOA, an application generator that can produce a whole range of useful code analysis tools. GENOA is designed to be language- and front-end independent; it can be interfaced to any front-end for any language that produces an attributed parse tree, simply by writing an interface spec- ification. While GENOA programs can perform arbitrary analyses on the parse tree, the GENOA language has special, compact iteration operators that are tuned for expressing simple, polynomial time analysis programs; in fact, there is a useful sublanguage of GENOA that can express precisely all (and only) polynomial time (PTIME) analysis prograrns on parse-trees. Thus, we argue that GENOA is a convenient "little language" to implement simple, fast analysis tools. We describe the system, provide several practical examples, and present complexity and expressivity results for the abovementioned sublanguage of GENOA.
[Computer science, Computer languages, Costs, Writing, Software systems, Polynomials, Performance analysis, Data mining, Programming profession, Information systems]
Experience report on software reuse project: its structure, activities, and statistical results
International Conference on Software Engineering
None
1992
This paper describes afour-year experimental software reuse project conducted at Software Laboratories, Nippon Telegraph and Telephone. The targets of reuse are program code modules stored in a common library. The project is first described in derail; its framework, organizational structure, and activities are presented. The statistical result s of the project are then shown. Some important aspects of software reuse are discussed, including impediments to software reuse, incentives and domain selection. Finally, it concludes that software reuse is inherently a managerial issue and that key factors for successful software reuse are senior management commitment and good domain selection.
[Process design, Telegraphy, Software libraries, Project management, Programming, Telephony, Permission, Impedance, Software reusability]
A general economics model of software reuse
International Conference on Software Engineering
None
1992
A general model of software reuse economics is presented. The model provides a framework for making estimates and decisions about the economic desirability ofreusing software. It covers costs andproductivity, return on investment, and incremental funding schemes.
[Productivity, Costs, Software libraries, Investments, Economic forecasting, Finite impulse response filter, Application software, Software tools, Software reusability]
Software Reuse Economics: Cost-benefit Analysis On A Large-scale Ada Project
International Conference on Software Engineering
None
1992
The software industry is painfully realizing that a software reuse effort, if not carefully planned and properly carried out, oftentimes becomes an inhibitor rather than a catalyst to software productivity and quality. Despite numerous ar ticles in the areas of domain analysis, component classification, automated component storage/retrieval tools, reuse metrics, etc., only a handful have managed to address the economics of software reuse. In order to be successful, not only must a reuse program be technically sound, it must also be economically worthwhile. After all, reducing costs and increasing quality were the two main factors that drove software reuse into the software mainstream. This paper presents a number of reuse economics models used to perform an economic assessment of a reuse effort on a large-scale Ada project: the United States Federal Aviation Administration's Advanced Automation System (FAA/AAS). Reuse economics models, cost/benefit tracking methods, and reuse catalystslinhibitors are addressed.
[Process design, Automation, Inhibitors, Software quality, FAA, Automatic control, Control systems, Large-scale systems, Cost benefit analysis]
Behavior sampling: a technique for automated retrieval of reusable components
International Conference on Software Engineering
None
1992
A new method, called behavior sampling, is proposed for automated retrieval of reusable components from software libraries. Unlike other retrieval methods, behavior sampling exploits the property that distinguishes software from other forms of text - its executability. Basic behavior-sampling identifies relevant routines by executing library routines on a searcher-supplied sample of operational inputs and comparing the routines' output to output provided by the searcher. The probabilistic basis for behavior sampling is described, and experimental results are reported that suggest basic behavior-sampling exhibits high precision (percentage of retrieved components that are relevant). Extensions to basic behavior-sampling are proposed to improve its recall (percentage of relevant components retrieved) and to make behavior sampling applicable to the retrieval of abstract data types.
[Software testing, Materials testing, Software libraries, Costs, Materials reliability, Sampling methods, Information retrieval, Distributed computing, Machinery]
Programming-in-the-large: past, present, and future
International Conference on Software Engineering
None
1992
The historical development of the concepts of programming-in-the-large is surveyed, covering layered systems, inforniation hiding, languages for programming-in-the-large, object-oriented languages, and software configuration management. Large-scale reuse, stronger foundations and better tools, cataloguing of software architectures, and improved preparation of software engineers are identified as future directions.
[Computer languages, Software architecture, Operating systems, Hierarchical systems, Computer architecture, Data engineering, Large-scale systems, Software tools, Software development management]
The software engineering laboratory - an operational software experience factory
International Conference on Software Engineering
None
1992
For 15 years, the Software Engineering Laboratory (SEL) has been carrying out studies and experiments for the purpose of understand- ing, assessing, and improving software and software processes within a production software development environment at the National Aeronautics and Space Administration/Goddard Space Flight Center (NASA/GSFC). The SEL comprises three major organizations: NASA/GSFC, Flight Dynamics Division University of Maryland, Department of Computer Science Computer Sciences Corporation, Flight Dynamics Technology Group - These organizations have jointly carried out several hundred software studies, producing hundreds of reports, papers, and documents, all of which de scribe some aspect of the software engineering technology that has been analyzed in the flight dynamics environment at NASA. The studies range from small, controlled experiments (such as analyzing the effectiveness of code readingversus that of functional testing) tolarge, multiple- project studies (such as assessing the impacts of Ada on a production environment). The organization's driving goal is to improve the software process continually, so that sustained improvement may be observed in the resulting products. This paper discusses the SEL as a functioning example of an operational software experience factory and summarizes the characteristics of and major lessons learned from 15 years of SEL operations.
[Laboratories, Feedback, Process planning, Software quality, Packaging, Particle measurements, Production facilities, Software measurement, Character recognition, Software engineering]
Recent advances in software estimation techniques
International Conference on Software Engineering
None
1992
This paper describes some recent developments in the field of software estimation. Factors to be considered in software estimation are discussed, a framework for reconciling those factors is presented, software estimation techniques are categorized, and recent advances are described. The paper concludes with a forecast of likely future trends in software estimation.
[Software quality, Software performance, Scheduling, Hardware, Software safety, Calibration, Software tools, Software engineering, Software development management]
The use of program dependence graphs in software engineering
International Conference on Software Engineering
None
1992
This paper describes a language-independent program representation-the program dependence graph-and discusses how program dependence graphs, together with operations such as program slicing, can provide the basis for powerful programmmg tools that address important software-engineering problems, such as understanding what an existing program does and how it works, understanding the differences between several versions of a program, and creating new programs by combining pieces of old pro- grams. The paper primarily surveys work in this area that has been carried out at the University of Wisconsin during the past five years.
[Knowledge engineering, Computer languages, Computerized monitoring, Parallel processing, Data structures, Distributed computing, Programming profession, Contracts, Software engineering]
Program visualization: the art of mapping programs to pictures
International Conference on Software Engineering
None
1992
In this paper program visualization is defined as a mapping from programs to graphical representations. Simple forms of program visualization are frequently encountered in software engineering. For this reason current advances in program visualization are likely to influence future developments concerning software engineering tools and environments. This paper provides a new taxonomy of program visualization research. The proposed taxonomy becomes the vehicle through which we carry out a systernatic review of current systems, techniques, trends, and ideas in program visualization.
[Visualization, Subspace constraints, Taxonomy, Humans, Desktop publishing, Image storage, Software engineering]
Incremental Testing Of Object-Oriented Class Structures/spl dagger/
International Conference on Software Engineering
None
1992
Although there is much interest in creating libraries of well-designed, thoroughly-tested classes that can be confi- dently reused for many applications, few class testing techniques have been developed. In this paper, we present a class testing technique that exploits the hierarchical nature of the inheritance relation to test related groups of classes by reusing the testing information for a parent class to guide the testing of a subclass. We initially test base classes having no parents by designing a test suite that tests each memberfunction individually and also tests the interactions among member functions. To design a test suite for a subclass, our algorithm incrementally updates the history of its parent to reflect both the modified, inherited attributes and the subclass's newly defined attributes. Only those new attributes or affected, inherited attributes are tested and the parent class test suites are reused, if possible, for the testing. Inherited attributes are retested in their new context in a subclass by testing their interactions with the subclass's newly defined attributes. We have incorporated a data flow tester into Free Software Foundation, Inc's C++ compiler and are using it for our experimentation.
[Computer science, Algorithm design and analysis, Permission, Libraries, Performance analysis, Application software, History, Distributed computing, Machinery, Testing]
Prototyping a process monitoring experiment
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Features are often the basic unit of development for very large software systems and represent long-term efforts, spanning several years from inception to actual use. Thus, monitoring these lengthy processes is a long-term effort as well. The authors report on an initial step in the development of an experiment to monitor such processes relating to prototyping the experimental design using a representative process and reconstructed data from a large and rich feature development. This prototyping approach has yielded two interesting sets of results. First, it was found that it was necessary to modify the experimental design. The initial set of states did not represent the data as well as had been hoped. Second, some interesting conjectures were formulated. The conclusion is that the prototyping effort is a necessary part of developing and installing any large-scale process monitoring experiment.<<ETX>>
[Software prototyping, long-term effort, Instruments, software prototyping, program diagnostics, Interference, large-scale process monitoring experiment, Design for experiments, prototyping effort, representative process, prototyping approach, Prototypes, Software systems, Cost function, Sampling methods, system monitoring, experimental design, process monitoring experiment, Large-scale systems, Monitoring, very large software systems]
Requirements and design change in large-scale software development: analysis from the viewpoint of process backtracking
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Change of requirements and design specifications is one of the fatal causes of large scale software development project problems. The authors study this problem from the viewpoint of software process backtracking. They conducted case studies of real business application system development projects. It was found that there were several cases of process backtracking in large scale projects and the process model should be flexible to allow reversibility. The users learning processes are considered. Among many probable factors that cause the volatility of user requirements and thus result in process backtracking, specific attention is given to the fact that as users learn to use a system and accumulate experience of the system usage, they change their system requirements level. The importance of considering this evolution process and finding a way of predicting changes in user requirements are discussed.<<ETX>>
[learning processes, Software maintenance, Design automation, Humans, Project management, human factors, real business application system development projects, Programming, user interfaces, case studies, commerce, Analytical models, user requirements, large scale software development project problems, Aging, process model, Large-scale systems, project management, Navigation, fatal causes, Software development management, system usage, reversibility, software process backtracking, design specifications]
Cleanroom software engineering for zero-defect software
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Cleanroom software engineering is a theory-based, team-oriented process for developing very high quality software under statistical control. Cleanroom combines formal methods of object-based box structure specification and design, function-theoretic correctness verification, and statistical usage testing for quality certification to produce software that has zero defects with high probability. The process of cleanroom development and certification is carried out incrementally. Interface and design errors are rare because at each stage the harmonious operation of future increments at the next level of refinement is predefined by increments already in execution. The cleanroom process is being successfully applied in IBM and other applications. Quality results from several cleanroom projects are summarized.<<ETX>>
[Software testing, System testing, program verification, object-based box structure specification, software reliability, Humans, quality certification, Programming, quality software, software quality, formal specification, statistical control, team-oriented process, zero-defect software, IBM, cleanroom software engineering, object-oriented methods, function-theoretic correctness verification, statistical usage testing, Software debugging, Certification, certification, Software quality, Quality control, formal methods, Software systems, Software engineering]
Approach and case study of requirement analysis where end users take an active role
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
In the traditional approach to software analysis, systems analysts interview end users to capture requirements. The authors propose an approach called automated user requirements acquisition (AURA) where end users take an active role in analysis by identifying requirements. This approach takes advantage of end users domain knowledge. AURA uses a question-and-answer model to guide end users in describing their problem. Additionally, AURA provides problem domain knowledge to suggest answers for the questions. The business application domain was selected to explain and demonstrate the approach. While the AURA approach can benefit any analysis technique, an object-oriented analysis technique was selected as the underlying method. A prototype tool, AURA-BIZ, was developed to demonstrate AURA in the business domain. AURA-BIZ was evaluated through a case study where end users created the analyses of their business problem. The data collected showed that the end users successfully modeled their problem by following the questions and using the suggestions.<<ETX>>
[domain knowledge, Computer aided software engineering, Costs, Text analysis, systems analysts, business problem, Programming, question-and-answer model, problem domain knowledge, object-oriented analysis technique, user interfaces, commerce, Prototypes, Performance analysis, software tools, Testing, Software prototyping, object-oriented programming, Object oriented modeling, case study, Computer science, end users, software analysis, automated user requirements acquisition, business application domain, AURA, AURA-BIZ]
A high level language for specifying graph based languages and their programming environments
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The authors describe a high level language for specifying programming environments for programming languages that are based on directed attributed graphs. The high level language allows the specifier to describe views of portions of a program written in such a graph-based language, the editing operations used to create the program, animations of the execution of the program, and sufficient detail of the execution semantics to support the animations. The use of the specification language is demonstrated with a simple example of a graph-based language. The automatic generation of the programming environment is described for such graph based languages from descriptions made in the specification language. The specification language is based on using a grammar to describe the components of the graph based language and using a first-order logic based language to describe state changes in editing, execution, and animation.<<ETX>>
[Petri nets, graph theory, first-order logic, editing operations, execution semantics, programming languages, formal specification, grammar, specification languages, animations, Libraries, specification language, Graphical user interfaces, high level language, directed attributed graphs, graph based languages, program execution, Specification languages, High level languages, Programming environments, Computer science, Computer languages, Parallel programming, grammars, directed graphs, Animation, programming environments]
Simulating the behaviour of software modules by trace rewriting
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The trace assertion method is a module interface specification method based on the finite state machine model. To support this method, the authors plan to develop a specification simulation tool, a trace simulator, that symbolically interprets trace assertions of trace specifications and simulates the externally observable behavior of the modules specified. They first present the trace assertion method. Then trace rewriting systems are formally defined, and it is shown that trace rewriting, a technique similar to term rewriting, can be applied to implement trace simulation.<<ETX>>
[rewriting systems, trace rewriting, Computational modeling, Computer simulation, specification simulation tool, Discrete event simulation, software modules, Formal specifications, History, finite state machines, formal specification, Communication standards, trace simulator, module interface specification, Automata, virtual machines, Software systems, system monitoring, finite state machine, Software tools, Testing]
Generalized behavior-based retrieval [from a software reuse library]
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The user of a large reuse library faces the formidable discovery problem of searching for all and only those components useful in solving the current programming task. The author describes a retrieval technique that generalizes the simple idea of executing each component on test inputs, and reporting those that compute correct outputs. It exploits the precise extensional semantics of executable code to provide the canonicality in query formulation that is missing in information-retrieval approaches to reuse. One generalization improves recall by considering small programs that are constructible from library components, rather than just single components. Furthermore, functional modeling of components allows the technique to handle complex behaviors, such as side effects. The author analyzes the technique and presents a working prototype, which has been tested on two libraries: one containing general programming, and the other containing some Unix shell commands.<<ETX>>
[canonicality, Taxonomy, executable code, library components, behavior-based retrieval, Prototypes, software reuse library, recall, side effects, complex behaviors, Performance analysis, Books, query formulation, Testing, functional modeling, test inputs, Natural languages, extensional semantics, information retrieval, information-retrieval, Software libraries, programming task, generalization, software reusability, Unix shell commands, Sampling methods, subroutines, Indexing]
Developing initial OOA models
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The authors developed an initial object-oriented requirements model for an existing missile planning system. They adopted two approaches: a bottom-up approach in which object-oriented model fragments were constructed that corresponded to segments of the requirements documents, and a top-down approach that was driven by the construction and analysis of scenarios. The strengths and limitations of each approach were discussed in the context of the example. A bottom-up, fragment-driven approach is appropriate in cases where there is a large, rich body of documentation. A top-down, scenario-driven approach should dominate where there is no requirements specification, but some high-level, unintegrated information about how the system should operate.<<ETX>>
[Missiles, fragment-driven approach, scenarios, formal specification, Research and development, object-oriented requirements model, Weapons, top-down approach, requirements specification, object-oriented methods, missiles, Marine vehicles, Contracts, military computing, object-oriented programming, scenario-driven approach, Object oriented modeling, Natural languages, Documentation, bottom-up approach, documentation, Educational institutions, Microelectronics, requirements documents, missile planning system]
Expressing the relationships between multiple views in requirements specification
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The authors generalize and formalize the definition of a ViewPoint to facilitate its manipulation for composite system development. A ViewPoint is defined to be a loosely-coupled, locally managed object encapsulating representation knowledge, development process knowledge and partial specification knowledge about a system and its domain. In attempting to integrate multiple requirements specification ViewPoints, overlaps must be identified and expressed, complementary participants made to interact and cooperate, and contradictions resolved. The notion of inter-ViewPoint communication is addressed as a vehicle for ViewPoint integration. The communication model presented straddles both the method construction stage during which inter-ViewPoint relationships are expressed, and the method application stage during which these relationships are enacted.<<ETX>>
[object-oriented programming, composite system development, method construction stage, Interconnected systems, partial specification knowledge, inter-ViewPoint communication, Programming, Educational institutions, Knowledge management, complementary participants, History, formal specification, multiple views, development process knowledge, representation knowledge, Vehicles, configuration management, ViewPoint integration, knowledge representation, requirements specification, locally managed object, method application stage, Context modeling, Software engineering]
An analysis of SEI software process assessment results: 1987-1991
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The authors focus on the results of Software Engineering Institute (SEI) software process assessments conducted over a four-year period beginning in 1987. They contribute to the existing body of knowledge on the state of practice of software engineering in the United States by characterizing software sites from a process maturity perspective and profiling site software process weaknesses. The data analyzed are drawn from SEI software process assessments of 59 government and industry software sites. This work is an analysis of existing assessment data rather than a designed study.<<ETX>>
[Demography, Government, software reliability, Project management, Software performance, SEI software process assessment results, technology transfer, industry, software quality, Software development management, Software Engineering Institute, Defense industry, site software process weaknesses, government, Computer industry, software sites, Contracts, Software engineering, Testing]
Post-process feedback with and without attribute focusing: a comparative evaluation
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Historically, the identification and correction of inadequacies in the process of software production called process feedback has been a difficult, time-consuming, manual exercise. Recently, a methodology for process feedback, called attribute focusing, has been developed. The authors compare post-process feedback with and without attribute focusing to determine how the methodology fares against current practice in post-process correction. Five project teams analyzed post-process defect data and made recommendations to improve the quality of a large operating systems product. That data was based on a multiple-choice questionnaire that was completed for every defect in a sample of defects that was chosen by each team. Subsequently, the same data was reanalyzed using attribute focusing. The comparison suggests attribute focusing can do at least as well or better than current practice in postprocess analysis, while reducing cost of analysis substantially.<<ETX>>
[Costs, Data analysis, Laboratories, attribute focusing, DP management, post-process feedback, software production, Operating systems, Feedback, Customer satisfaction, Production, multiple-choice questionnaire, software engineering, project teams]
Exploring dataflow testing of arrays
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
A scheme is presented for constructing prototype testing tools to experiment with issues in data-flow testing of arrays. Data-flow testing is a variant of path testing that falls between the extremes of simple run-time instrumentation, and full path testing. In data-flow testing, the paths selected for exercising are variants of def/use (DU) paths for program variables. Tool support for data-flow testing takes the form of static program analysis followed by run-time instrumentation. The output from a test analyzer is a list of untested, possible DU associations. With the novel approach to constructing testing-supporting tools, it is easy to experiment with new analysis methods. The authors describe the implementation of a DU-path analyzer as an example of this approach. Using the prototype tool, experiments and issues are considered raised by an analyzer that treats arrays differently from existing tools. Each array element is considered to be a distinct data-flow object.<<ETX>>
[Software testing, def/use, test analyzer, program testing, data-flow testing, testing-supporting tools, static program analysis, parallel processing, Runtime, Program processors, Prototypes, simple run-time instrumentation, Instruments, program diagnostics, array testing, DU-path analyzer, full path testing, program variables, prototype testing tools, Flow graphs, possible DU associations, Computer science, Automatic testing, Software quality, array element, Iterative algorithms]
An examination of the current state of IPSE technology
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The concept of an integrated project support environment (IPSE) was developed in response to a recognition that communication and coordination between all tools used in software development and maintenance are essential to the efficient production of quality software systems. The author begins by examining some of the reasons for the relative implementation of IPSE in the commercial world, and then turns his attention to alternative solutions which have seen greater success. The main areas of IPSE research currently being pursued are highlighted. An analysis is given of key issues for the future.<<ETX>>
[Software maintenance, Production systems, project support environments, software development, Area measurement, Programming, software quality, Proposals, software maintenance, quality software systems, Current measurement, Computer architecture, integrated project support environment, Software systems, IPSE, Kernel, programming environments, Software engineering]
Observations on industrial practice using formal methods
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Formal methods refer to the use of mathematically based techniques in software and system engineering. The authors summarize observations on their use in a dozen applications in industrial settings. Application goals ranged from reengineering to system certification. The purpose is to extract some of the key observations about practice in software engineering terms with minimal reference to formal methods terminology and glossing over distinctions among methods. The methodology of the study is described. Applications include oscilloscopes, nuclear reactors, trains, planes, ships, satellites, smartcards, transaction processing, arithmetic units, networks, medical instruments, and language processors. The observations follow from a systematic survey of these applications using a structured interview process and analysis of results using a set of features covering various aspects of practice: process, methods, tools, and technology transfer.<<ETX>>
[nuclear reactors, transaction processing, arithmetic units, Terminology, industrial practice, Oscilloscopes, oscilloscopes, system certification, formal specification, trains, formal verification, software engineering, language processors, Marine vehicles, system engineering, satellites, reengineering, technology transfer, planes, Application software, Certification, Satellites, ships, smartcards, formal methods, Software systems, Systems engineering and theory, systems engineering, medical instruments, Software engineering, Arithmetic]
RECAST: Reverse engineering from COBOL to SSADM specification
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The reverse engineering into computer aided software engineering (CASE) technology method (RECAST) takes the source code for an existing COBOL system and derives a no-loss representation of the system documented in a structured systems analysis and design method (SSADM) format. This representation of the system is derived through the use of a series of transformations. The authors describe the environment within which RECAST has been developed, outline the stages and steps of the RECAST method and discuss the use of software support tools. An overview is given of a case study that has been carried out for a live system.<<ETX>>
[Software maintenance, SSADM specification, Computer aided software engineering, Costs, Design methodology, program diagnostics, Reverse engineering, Government, software support tools, COBOL, Documentation, source code, RECAST, software maintenance, formal specification, System analysis and design, Software systems, computer aided software engineering, software tools, structured systems analysis, Software tools]
Software process maturity: measuring its impact on productivity and quality
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
With the current worldwide focus on improvement in the software process, there is clearly a need for an understanding of its impact on software engineering productivity and quality. The author documents an attempt to provide an empirical metrics view of such initiatives based on data collected in a worldwide benchmarking effort conducted between March 1991 and December 1991. Of the more than 300 organizations that participated, fewer than one in five had any quantifiable performance data available prior to the start of this study. However, those that had embarked on significant process improvement efforts and were actively using metrics were able to demonstrate substantial gains in productivity and quality. In addition, insights derived from this large-scale data analysis provide a framework for determining which metrics should be included in a standard software engineering measurement dashboard.<<ETX>>
[Productivity, standard software engineering measurement dashboard, Electric breakdown, Educational institutions, quantifiable performance data, software quality, quality, Computer science, Current measurement, productivity, Software quality, empirical metrics, large-scale data analysis, software engineering, Large-scale systems, software process maturity, Software measurement, Portfolios, Software engineering, software metrics, worldwide benchmarking effort]
Software components in a data structure precompiler
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
This research aims to simplify component creation and reduce the necessity of implementing new components. It concentrates more on programmer productivity and code efficiency than on program evolution and maintenance. The project is called PREDATOR. PREDATOR is a data structure precompiler that generates efficient code for maintaining and querying complex data structures. It embodies a novel component reuse technology that transcends traditional generic data types. The authors explain the concepts of this work and the prototype system. It is shown that complex data structures can be specified as compositions of software building blocks. Performance results are presented that compare PREDATOR output to hand-optimized programs.<<ETX>>
[Software performance, program compilers, PREDATOR, software building blocks, Prototypes, code efficiency, data structure maintenance, data structures, software components, programmer productivity, data structure precompiler, Productivity, Software prototyping, hand-optimized programs, Data structures, Application software, Software debugging, Programming profession, performance, prototype system, generic data types, component creation, Binary trees, Writing, software reusability, component reuse technology, data structure querying]
Automating the detection of reusable parts in existing software
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Presents a model based on an expert-system approach for the scavenging of reusable components from existing software systems. The authors also describe a toolset called Code Miner that implements part of the model. The toolset uses Prolog as its inference engine. Code Miner is designed to assist the programmer in finding reusable components in existing software written in C. To investigate the feasibility of the approach, an empirical study was conducted of the effectiveness of the toolset. In the study, public-domain software was scanned by the toolset for reusable parts and its output was examined by a team of human experts. The results of this experiment are outlined.<<ETX>>
[toolset, Costs, expert systems, public domain software, Humans, reusable components detection, expert-system, Prolog, inference mechanisms, C language, Programming profession, Engines, public-domain software, Computer science, Software libraries, inference engine, Code Miner, existing software systems, software reusability, Software systems, software tools, Software reusability, Software tools, Expert systems]
Predicate-based test generation for computer programs
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The author first describes a number of existing testing strategies for simple predicates and then explains why intuitive extensions of such strategies are ineffective or impractical for testing compound predicates, which are predicates with one or more AND/OR operators. Two fault-based testing strategies for compound predicates are defined, BOR (Boolean operator) testing and BRO (Boolean and relational operator) testing. It is shown that for a predicate with n, n>0, AND/OR operators, at most n+2 (2*n+3) tests are needed to satisfy BOR (BRO) testing. Preliminary experimental results indicate that BOR and BRO testing are effective for the detection of various types of faults in a predicate and provide more specific guidance than branch testing for test generation.<<ETX>>
[Software testing, program testing, program verification, BOR, Boolean and relational operator, Boolean operator, fault-based testing strategies, compound predicates, Computer science, Boolean functions, simple predicates, Fault detection, testing strategies, AND/OR operators, BRO, Arithmetic]
Domain modeling-overview and ongoing research at EDS
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
An application domain model is defined as a formal representation of the knowledge necessary to support specific operational goals. EDS is a company which produces large business systems for a variety of industries. Domain modeling at EDS is discussed. First, domain-specific knowledge is acquired from various sources and it is synthesized into a domain model. At this stage, the focus is on digesting, understanding, and formalizing the gathered information. Second, to build specification models for various applications, relevant pieces of domain knowledge are selected from the domain model. In this phase, the focus is on tailoring general domain knowledge to the specific application. The approach to gathering domain knowledge is based on a combination of manual, semi-automated, and automated techniques. The knowledge acquisition synthesis system is described which was implemented using X Windows on a SPARC II client/server architecture. Domain experts interact with the system to store their knowledge into a domain model.<<ETX>>
[Software testing, domain-specific knowledge, System testing, large business systems, Reverse engineering, Formal languages, Finance, knowledge acquisition, Companies, application domain model, X Windows, Application software, Formal specifications, commerce, SPARC II client/server architecture, formal specification, formal representation, Insurance, knowledge based systems, knowledge acquisition synthesis system, specification models, Computer industry, operational goals]
Computational reflection in software process modeling: The SLANG approach
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
SLANG is a domain-specific language for software process modeling and enactment. The authors present the basic features provided by SLANG to support the enactment and, in particular, dynamic evolution of a process model. Software production processes are subject to changes during their lifetime. Therefore, software process formalism must include mechanisms to support the analysis and dynamic modification of process models, even while they are being enacted. It is thus necessary for a process model to have the ability to reason about its own structure. Petri net based process languages have been criticized because of the lack of these reflective features and their inability to effectively support process evolution. The reflective features offered by SLANG are outlined, which is a process formalism based on a high-level Petri net notation. In particular, the mechanisms are discussed to create and modify different net fragments while the modeled process is being enacted.<<ETX>>
[dynamic modification, Automation, Petri nets, Humans, software process modeling, Programming, Reflection, Petri net based process languages, Application software, process evolution, SLANG approach, dynamic evolution, high-level Petri net notation, domain-specific language, reflective features, Production, Software quality, Computer architecture, specification languages, software process formalism, Interleaved codes, process model, computational reflection, Software tools]
Documentation for safety critical software
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The authors review some of the fundamental difficulties presented by the design and the validation of software for safety critical applications. They suggest that software formal documentation techniques ameliorate the problems described. The principles behind a method of documenting both requirements and software design are presented. The methods have been used by the Atomic Energy Control Board of Canada in its safety assessment of the shutdown software of the Darlington generating station (D.L. Parnas et al., 1991). The method is illustrated by applying it to a small portion of the safety feature actuation system of a PWR nuclear reactor.<<ETX>>
[Software maintenance, program verification, Circuits, software reliability, system documentation, nuclear engineering computing, software design, Software performance, Control systems, Software safety, software requirements, Software design, software validation, safety, Hardware, formal documentation, safety critical software, safety feature actuation system, Documentation, Darlington generating station, Atomic Energy Control Board, safety critical applications, shutdown software, nuclear power stations, Application software, systems analysis, Software systems, PWR nuclear reactor]
An analytical comparison of the fault-detecting ability of data flow testing techniques
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Compares several data flow based software testing criteria to one another and to branch testing. The fact that criterion C/sub 1/ subsumes criterion C/sub 2/, does not guarantee that C/sub 1/ is better at detecting faults than C/sub 2/. However, if a certain stronger relation between the criteria holds, then for any program and any specification, C/sub 1/ is guaranteed to be better at detecting faults than C/sub 2/ in the following sense: a test suite selected by independent random selection of one test case from each C/sub 1/ subdomain is at least as likely to detect a fault as a suite similarly selected using C/sub 2/. It is shown that under those conditions, the expected number of failure-causing inputs in the C/sub 1/ test suite. These results are used to compare a number of data flow testing criteria to one another and to branch testing.<<ETX>>
[Data analysis, program testing, NASA, specification, error detection, parallel processing, formal specification, test suite, fault-detecting ability, Computer science, Fault detection, Space technology, Computer bugs, independent random selection, data flow testing techniques, branch testing, software testing criteria, failure-causing inputs, Testing]
Programming heterogeneous transactions for software development environments
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Describes an approach to supporting transaction model interoperability for software development environments. In attempting to integrate tools written in two languages with different transaction models, it was found that existing technology was insufficient to facilitate a centralized mechanism that is poorly integrated with features in most languages. The authors propose instead a decentralized approach to interoperability that gives programmers extended control over transactions to accommodate a variety of transaction models and to permit the programming of alternative forms of heterogeneous transactions. This approach is especially appropriate for supporting transaction model interoperability in software development environments.<<ETX>>
[transaction processing, heterogeneous transactions, languages, software development environments, tools integration, Concurrency control, Transaction databases, Application software, Centralized control, Computer science, Large-scale systems, Functional programming, Time factors, transaction model interoperability, programming environments, decentralized approach, programming, Testing, Software engineering]
The Graft-Host method for design change
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The authors report on the Graft-Host (G-H) method, a practical method for changing designs incrementally by means of graft-and-repair steps. The method is not automated. Computational support to designers applying G-H comes in the form of tools to help them reuse design information and manage databases of constraints. The method derives its strength from the reuse of analysis and design information. Some key features of G-H are: evaluation of grafts and host systems at the analysis and design level by reusing information from technology books (G. Arango et al., 1993); reuse and adaptation of software components driven by analysis and design considerations; evaluation of designs along multiple design dimensions; and composition of change rationales as formal explanations of change propagation and change tradeoff analyses. The G-H method was validated on the redesign of industrial products in the context of technology books.<<ETX>>
[Software maintenance, databases, Design methodology, Laboratories, change propagation, change tradeoff analyses, Knowledge management, Information analysis, graft-and-repair steps, Computer science, design information, Software design, design reuse, systems analysis, software reusability, technology books, Computer industry, Graft-Host method, Hardware, design change, software tools, Books, multiple design dimensions]
Modeling and managing risk early in software development
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The authors present an automated modeling technique which can be used as an alternative to regression techniques to improve the quality of the software development process. The modeling process will allow for the reliable detection of potential problem areas and for the interpretation of the cause of the problem so that the most appropriate remedial action can be taken. It is shown that it can be used to facilitate the identification and aid the interpretation of the significant trends which characterize high risk components in several Ada systems. The effectiveness of the technique is evaluated based on a comparison with logistic regression based models.<<ETX>>
[System testing, risk management, software development process, software development, software reliability, Ada systems, Programming, Predictive models, Educational institutions, DP management, software quality, automated modeling technique, Software development management, quality, Computer science, logistic regression based models, remedial action, Software quality, Risk management, Logistics, Software engineering, potential problem areas]
A comprehensive process model for studying software process papers
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Efficient and effective study of scientific papers is an important part of software engineering education. A comprehensive process model has been developed, enacted, improved and validated. The process model describes literature search, paper selection, reading, and group discussion of papers, as well as recording, validation, and retrieval of the data captured from the selected papers. The focus lies on the group discussion of the model. A model of this sub-process describes the systematic classification, analysis, and evaluation of the papers. It is used to guide the group discussion, and helps to ensure that pertinent information from the discussion is retained in an annotated database.<<ETX>>
[Educational programs, computer science education, literature search, Programming, Educational institutions, Information retrieval, group discussion, Research and development, Computer science, comprehensive process model, Databases, Bibliographies, scientific papers, paper selection, software engineering education, software engineering, Computer science education, Software engineering, software process]
Dynamic mutation testing in integrated regression analysis
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
A new method of integrated regression analysis is proposed. Its core is the clustering, a method for automatic identification of program modifications. Clustering is used to formulate a hypothesis about the existence of a fault in the modified program, and to guide the process of testing this hypothesis. It is postulated that static and dynamic program analysis be used for that purpose. Specifically, the authors introduce dynamic mutation testing (DMT), an experimental technique for testing the fault hypothesis. DMT estimates the sensitivity of the test-induced program state to reveal the postulated fault in the modified program. If all tests pass and are found to have a high sensitivity, the fault hypothesis can be rejected at a high level of confidence.<<ETX>>
[Software testing, dynamic program analysis, fault hypothesis, program testing, dynamic mutation testing, program diagnostics, Genetic mutations, software reliability, test-induced program state, Programming, Probability, postulated fault, DMT, Regression analysis, Flow graphs, Computer science, modified program, integrated regression analysis, OFDM modulation, clustering, State estimation, automatic identification, program modifications, Formal verification]
The concept assignment problem in program understanding
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Concept assignment is a process of recognizing concepts within a computer program and building up an understanding of the program by relating the recognized concepts to portions of the program, its operational context and to one another. The problem of discovering individual human oriented concepts and assigning them to their implementation oriented counterparts for a given program is the concept assignment problem. The authors argue that the solution to this problem requires methods that have a strong plausible reasoning component. They illustrate these ideas through example scenarios using an existing design recovery system called DESIRE. DESIRE is evaluated based on its usage on real-world problems over the years.<<ETX>>
[Context, Vocabulary, program diagnostics, Reverse engineering, Humans, Communication system control, Maintenance engineering, Pattern recognition, Microelectronics, implementation oriented counterparts, software maintenance, concept assignment problem, plausible reasoning, program understanding, design recovery system, DESIRE, Automatic control, software tools, human oriented concepts, Assembly]
Applying algorithm animation techniques for program tracing, debugging, and understanding
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Algorithm animation presents a dynamic visualization of an algorithm or program. This work seeks to bridge the two domains of data structure display and algorithm animation. The application-specific nature of algorithm animation views could be a valuable debugging aid for software developers. A system called Lens was developed that allows programmers to rapidly develop animations of their programs. Lens supports application-specific semantic program views as seen in many algorithm animation systems, but does not require graphics programming. Lens is integrated with a system debugger to support iterative testing and refinement. The authors describe the conceptual model on which Lens is based, illustrate how program animations are built with Lens, and outline some of the implementation challenges the system presents.<<ETX>>
[program debugging, program tracing, system debugger, Displays, computer animation, program understanding, data visualisation, refinement, iterative testing, dynamic visualization, program diagnostics, data structure display, Software algorithms, implementation, Data structures, Software debugging, Programming profession, Bridges, algorithm animation, Data visualization, virtual machines, Animation, conceptual model, Iterative algorithms, engineering graphics, Lens, Lenses, application-specific semantic program views]
A bi-level language for software process modeling
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The authors present a multi-user implementation of a bi-level process modeling language (PML). Most process modeling formalisms are well-suited to one of two levels of specification, but not both. Some concentrate on global control flow and synchronization. These languages make it easy to define the broad outline of a process, but harder to refine the process by expressing constraints and policies on individual tools and data. Other process formalisms are inherently local. It is easy to define constraints, but far from straightforward to express control flow. Combining global and local formalisms is proposed to produce bi-level formalisms suitable for expressing the enacting large scale processes. The new PML is called the activity structures language. The activity structures language integrates global constrained expressions with local rules. Its implementation on top of the Marvel rule-based environment is described.<<ETX>>
[Petri nets, software process modeling, process formalisms, specification, multi-user implementation, global constrained expressions, Engines, PML, activity structures language, Computer science, Databases, Turing machines, Marvel rule-based environment, bi-level process modeling language, knowledge based systems, Production, specification languages, local rules, Automatic control, Large-scale systems, Logic, Software engineering, global control flow]
An experimental evaluation of selective mutation
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Mutation testing is a technique for unit-testing software that, although powerful, is computationally expensive. The principal expense of mutation is that many variants of the test program, called mutants, must be repeatedly executed. Selective mutation is a way to approximate mutation testing that saves execution by reducing the number of mutants that must be executed. The authors report experimental results that compare selective mutation testing to standard, or nonselective, mutation testing. The results support the hypothesis that selective mutation is almost as strong as nonselective mutation. In experimental trials, selective mutations provide almost the same coverage as nonselective mutation, with significant reductions in cost.<<ETX>>
[Software testing, System testing, Costs, program testing, Genetic mutations, mutants, mutation testing, Application software, Approximation methods, Fault detection, Automatic testing, unit-testing software, Software quality, software engineering, Computational efficiency, selective mutation]
'. . . and nothing else changes': the frame problem in procedure specifications
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The first aim of this analysis is to outline a certain general problem which arises in all formal specifications using the pre/postcondition notation, and which is related to a longstanding problem in the field of AI, called the frame problem (J. McCarthy and P. Hayes, 1969). The authors then present examples illustrating this problem, which becomes particularly acute for large object-oriented specifications where inheritance plays a central role. The examples are intended to demonstrate that failure to deal with the frame problem compromises a formal specification language with respect to its notational suitability and its capacity to support a methodology for formally proving properties of specifications. How existing specification languages have endeavored to cope with the problem are reviewed. A novel approach is presented based on recent work intended to solve the frame problem in planning applications within AI.<<ETX>>
[object-oriented programming, frame problem, planning applications, procedure specifications, AI, inheritance, Specification languages, formal specification language, Formal specifications, Proposals, formal specification, formal specifications, Intelligent robots, artificial intelligence, Computer science, planning (artificial intelligence), large object-oriented specifications, Councils, specification languages, Logic, Standards development, Artificial intelligence, Intelligent systems, notational suitability]
Test templates: a specification-based testing framework
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Test templates and a test template framework are introduced as useful concepts in specification-based testing. The framework can be defined using any model-based specification notation and used to derive tests from model-based specifications. It is demonstrated using the Z notation. The framework formally defines test data sets and their relation to the operations in a specification and other test data sets, providing structure to the testing process. Flexibility is also preserved, so that many testing strategies can be used. Important application areas of the framework are discussed, including refinement of test data, regression testing, and test oracles.<<ETX>>
[Software testing, System testing, Stability, Terminology, program testing, Genetic mutations, regression testing, test oracles, Application software, Data mining, Formal specifications, formal specification, specification-based testing framework, Computer science, flexibility, test data refinement, test data sets, Computer errors, model-based specification notation, testing strategies, Z notation, test template framework]
Use of an environment classification model
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Various reference models have been proposed for the classification of features present in an integrated software engineering environment. Two such models are studied and a target system is mapped to the set of services present in these models. This system consisted of approximately 23000 lines of Pascal source code in 31 modules and provided tools for developing Pascal programs using either a syntax-directed editor or a line-oriented editor. In addition to describing the system, this mapping exercise provided additional feedback on the effectiveness of the two reference models used. The results of this mapping and comments on the effectiveness of the models are given.<<ETX>>
[environment classification model, Computer aided software engineering, program modules, text editing, software development, reference models, Standardization, Programming, integrated software engineering environment, Educational institutions, Computer science, Pascal source code, Software design, Open systems, NIST, Software systems, software tools, Pascal, Software tools, programming environments, syntax-directed editor, line-oriented editor]
A process for consolidating and reusing design knowledge
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Significant improvements in design quality and productivity are possible when designers operate in a domain-specific information workspace with low-cost access to relevant application, design, and technology information. The authors present a validated process for constructing such workspaces to support product family design and evolution. The process involves three related efforts: techniques to consolidate critical analysis and design information from different projects and different engineering sites called domain analysis; online representation of the information in structured form called technology books; and methods and tools to reuse information. Each area is described, with the focus being on technology books. Technology books are object-oriented databases whose objects capture typed information about analyses, designs, and code in a variety of notations. Relations with well-defined semantics link objects; these can be used to navigate through the technology book, and to reason about the information it contains.<<ETX>>
[Costs, design knowledge, design quality, object-oriented databases, low-cost access, product family design, Documentation, Product design, Knowledge management, Paper technology, Information analysis, design productivity, domain-specific information workspace, design information, Design engineering, online representation, Power engineering computing, typed information, systems analysis, software reusability, well-defined semantics link objects, Software systems, software tools, Books]
Rule-based approach to computing module cohesion
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Stevens, Myers, and Constantine introduced the notion of cohesion, an ordinal scale of seven levels that describes the degree to which the actions performed by a module contribute to a unified function (1974). They provided rules, termed as associative principles to examine the relationships between processing elements of a module and designate a cohesion level to it. Stevens et al., however, did not give a precise definition for the term processing element. The author interprets the output variables of a module as its processing elements. Stevens et al.'s associative principles are transformed to relate the output variables based on their data and control dependence relationships. What results is a rule-based approach to computing cohesion. Experimental results show that, but for temporal cohesion, the cohesion associated to a module under this reinterpretation and that due to the original definitions are identical for all examples.<<ETX>>
[Process design, Software testing, expert systems, Laboratories, unified function, Maintenance, control dependence relationships, rule-based approach, module cohesion, associative principles, term processing element, output variables, Software quality, Software systems, software tools, Marine vehicles, software metrics]
Program and interface slicing for reverse engineering
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
A case is presented for the use of conventional and interface slicing as enabling mechanisms for numerous reverse engineering and reengineering tasks. The authors first discuss the applicability of conventional slicing to algorithm extraction and design recovery at statement-level granularity. They then present interface slicing and show how it provides similar capabilities at module-level granularity. Module is a general term for a collection of subprograms, possibly with information hiding mechanisms: It includes but is not limited to Ada packages. Component refers to a module in a reuse repository. A component is thus a code asset of a repository, possibly also incorporated into a program. Ada is used for the example, as Ada's features facilitate the types of transformations which are invoked.<<ETX>>
[Algorithm design and analysis, Software maintenance, reuse repository, Reverse engineering, Lattices, algorithm extraction, Programming, module-level granularity, Software design, Hardware, design recovery, Ada packages, interface slicing, program diagnostics, statement-level granularity, information hiding, reengineering, reverse engineering, software maintenance, Statistics, Computer science, enabling mechanisms, software reusability, Software systems, Ada]
Inter-item correlations among function points
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Reports on an empirical investigation of Albrecht function points. They investigated the constituent elements of the function point counts to determine whether all the elements are required to provide a valid measure of size; whether the sum of all the elements is a better predictor of effort than the constituent elements; and whether the complexity adjustments are necessary. The study suggests that function points are not well-formed metrics because there is a correlation between their constituent elements. It also suggests that for the dataset under investigation, two of the constituent elements were as good at predicting effort as the raw function point count and that the unweighted counts can be reasonable predictors of effort.<<ETX>>
[Availability, Productivity, unweighted counts, Data analysis, Scattering, Predictive models, Size measurement, Management training, constituent elements, inter-item correlation, Albrecht function points, Insurance, complexity adjustments, Hardware, Marketing and sales, effort prediction, function point counts, software metrics]
Adding implicit invocation to traditional programming languages
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Implicit invocation based on event broadcasting is an increasingly important technique for integrating systems. The authors broaden the class of systems that can benefit from this approach by showing how to augment general-purpose programming languages with facilities for implicit invocation. They illustrate the approach in the context of the Ada language. Attempts to add implicit invocation to standard languages raise a number of design decisions that can have a significant impact on the properties of the mechanism and on its usability. These design considerations are highlighted so that any similar attempt to add implicit invocation to a strongly-typed, procedure-oriented programming language can benefit from this work.<<ETX>>
[IEEE news, procedure-oriented programming language, Relational databases, traditional programming languages, Application software, programming languages, Computer science, design decisions, Computer languages, strongly-typed, usability, Operating systems, Sockets, Broadcasting, implicit invocation, Database systems, event broadcasting, Ada]
Experimental evaluation of a fuzzy-set based measure of software correctness using program mutation
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Experimental evaluation of software reliability models that depend on the source code of the target program is expensive due to the need for a large sample of programs. The authors have used program mutation to generate many versions of one of the more complex components comprising a hypothetical but realistic nuclear reactor safety control program. Trivial mutants were filtered by using branch and path testing. These programs were used to assess a fuzzy set based measure of program correctness. The results confirmed that the model is conservative. In addition, the experiments provided new insights into the model, including reassessment of its assumptions and directions for refining it.<<ETX>>
[Software testing, fuzzy-set based measure, program verification, software correctness, nuclear reactor safety control program, Genetic mutations, software reliability, fuzzy set theory, Nuclear power generation, Software reliability, History, software reliability models, Computer science, Fuzzy sets, branch and path testing, program mutation, Failure analysis, Safety, Software measurement, software metrics, program correctness]
Coverage measurement experience during function test
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The authors discuss the issues of test coverage measurement in industry and justify the benefits of the measurement using a framework developed by them. Some results of experiences using test coverage measurement are outlined. The function test of large-scale system software is defined and analyzed. Based on the discussions of function test, a framework for analyzing the function test error removal process is developed. An experience-based error removal model and a cost model are proven to be useful tools for justifying test coverage measurement during function test. Data obtained from a real project are analyzed using the framework for validation.<<ETX>>
[Software testing, experience-based error removal model, System testing, program testing, program verification, large-scale system software, Time measurement, industry, function test, cost model, Packaging machines, function test error removal process, Operating systems, test coverage measurement, Software systems, Hardware, Large-scale systems, software cost estimation, Software measurement, Software tools, validation, software metrics]
Building, modifying and using component generators
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The tools used to generate implementations from component descriptions are called component generators. The protocol considered supports call processing by controlling the set-up and take-down of connections carrying telephone calls. The exercises discussed investigate the use of component generators in the construction of the protocol handler and related components. The component descriptions and generic architectures developed in these exercises are built by composing reusable components and applying generative tools. The component generators use information obtained from the component descriptions to instantiate the generic architecture. In addition to providing a perspective on the approach to component generation, related work in knowledge-based software engineering is presented. These exercises are generalized, outlining a process for developing software which takes advantage of component generator technology and supports several forms of component composition.<<ETX>>
[software development process, Protocols, generic architectures, reusable components, component generators, call processing, Data mining, generative tools, component descriptions, telecommunications computer control, telephone calls, knowledge based systems, Computer architecture, Telephony, software tools, automatic programming, Buildings, Process control, knowledge-based software engineering, Educational institutions, telephone systems, protocol handler, Programming profession, component composition, software reusability, subroutines, Software tools, Software engineering]
Prototyping in industrial software projects-bridging the gap between theory and practice
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
Prototyping has been adopted as a technique in software engineering. There has been a lack of documented experience with the use of prototyping in industrial software production. The present work tries to close this gap by presenting an analysis of results obtained from case studies of industrial software projects in which explicit use of prototypes was made with a differing understanding of the underlying concepts. The major concern was to analyze the experience gained in the projects and the major pitfalls in the use of prototyping and, in particular, to juxtapose these pros or cons and the claims made for prototyping in the literature. The analysis is not limited to success stories because understanding the limits and problems of prototyping will help to make full use of the obvious advantages.<<ETX>>
[Software prototyping, Costs, project management, Terminology, software prototyping, Programming, Design engineering, Software design, industrial software projects, Prototypes, Production, Tin, Computer industry, software engineering, industrial software production]
Modeling software for accurate data flow representation
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
A particular model used for data flow oriented structural testing is the def-use graph which captures intraprocedural data flow dependencies within the control flow exhibited by a program written in a procedural language. Since procedures in a program are closely interrelated, data flow oriented structural testing must also be performed at the program level utilizing the interprocedural data flow dependencies. The authors point out that the accuracy of the representation of data flow dependencies by the def-use graph is no longer acceptable at the program level where the accurate representation of interprocedural data flow dependencies is needed. This point is illustrated by an error that cannot be revealed by existing data flow oriented test path selection criteria when the procedures of a program are considered individually, but can be revealed by at least two of the criteria which require accurate representation of interprocedural data flow dependencies. A new model, called the extended def-use graph is proposed to represent both intraprocedural and interprocedural data flow dependencies in the control flow exhibited by a program. This model facilitates the application of the existing data flow oriented test path selection criteria at the program level.<<ETX>>
[Performance evaluation, program testing, def-use graph, test path selection criteria, data flow dependencies, Flow graphs, Data mining, formal specification, data flow oriented structural testing, Reactive power, control flow, accurate data flow representation, Lead, interprocedural data flow dependencies, procedural language, Testing]
Software improvements in an international company
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The Schlumberger Laboratory for Computer Science (SLCS) was formed in 1989 as a corporate-wide resource to enhance the quality and creativity of software products within Schlumberger and to improve the productivity of software development. Because of a wide range of needs, it was necessary to decide which techniques SLCS's small software improvement team could use most effectively. The authors describe the choices made and the effectiveness of those choices in work with Schlumberger's engineering organizations. Through the motivation efforts of a small group, productive changes have occurred across the company. Improvements were made in many development areas, including project planning and requirements management. The catalysts behind these advances included capability assessments, training, and collaboration.<<ETX>>
[Productivity, Software maintenance, software products, project management, capability assessments, software development, Laboratories, Project management, Companies, Programming, international company, corporate-wide resource, Computer science, Industrial training, requirements management, productivity, Software quality, Computer industry, small software improvement team, project planning]
Model based process assessments
Proceedings of 1993 15th International Conference on Software Engineering
None
1993
The authors present an approach that combines process modeling with process assessments. They use the structured analysis and design technique (SADT) modeling notation (D.A. Marca and C.L. McGowan, 1988). The DoD CIM Initiative has standardized on a subset of SADT, called IDEF0, to model business processes. A SADT (IDEF0) model was created of a large software maintenance process and the model led to process improvements that might have been missed otherwise. This model based process assessment approach is described as a process in its own right.<<ETX>>
[structured analysis and design technique, Petri nets, Computer integrated manufacturing, Predictive models, process assessments, business processes, structured programming, large software maintenance process, software maintenance, Delay, DoD CIM Initiative, process improvements, IDEF0, Standards organizations, Feedback, systems analysis, SADT, Software standards, Dynamic programming, Capability maturity model, military computing, process modeling, Business]
A process for hitting paydirt
Proceedings of 16th International Conference on Software Engineering
None
1994
The fundamentals of software engineering lie in establishing and controlling a process that gives engineers time to execute their skills and allows them to maintain project coordination. When these fundamentals are weak software development resembles thrashing, with the customer receiving the worst of it. Tools and design methods were not designed to solve these problems. Without a fundamentally sound process to protect their use, advanced technologies will have little chance to score impressively. People in crisis rarely use their tools well. The Software Engineering Institute's Capability Maturity Model for Software was designed to help establish sound fundamentals for building software. Its initial focus is on strengthening the project management and organizational deficiencies that so often confound good software developers.
[Design methodology, Statistical distributions, Process control, Computer architecture, Programming, Software reusability, Software tools, Software engineering]
TESTTUBE: a system for selective regression testing
Proceedings of 16th International Conference on Software Engineering
None
1994
The paper describes a system called TESTTUBE that combines static and dynamic analysis to perform selective retesting of software systems written in C. TESTTUBE first identifies which functions, types, variables and macros are covered by each test unit in a test suite. Each time the system under test is modified, TESTTUBE identifies which entities were changed to create the new version. Using the coverage and change information, TESTTUBE selects only those test units that cover the changed entities for testing the new version. We have applied TESTTUBE to selective retesting of two software systems, an I/O library and a source code analyzer. Additionally, we are adapting TESTTUBE for selective retesting of nondeterministic systems, where the main drawback is the unsuitability of dynamic analysis for identification of covered entities. Our experience with TESTTUBE has been quite encouraging, with an observed reduction of 50% or more in the number of test cases needed to test typical software changes.<<ETX>>
[Software testing, System testing, Software maintenance, macros, program testing, source code analyzer, Life testing, software maintenance, TESTTUBE, C language, test suite, nondeterministic systems, selective retesting, Information analysis, Software libraries, Tail, Software systems, I/O library, Performance analysis, software tools, statistical analysis, selective regression testing, Software engineering]
A programmer performance measure based on programmer state transitions in testing and debugging process
Proceedings of 16th International Conference on Software Engineering
None
1994
To organize and manage software development teams, it as important to evaluate the capability of each programmer based on reliable and easily collected data. We present a system which automatically monitors programmer activities, and propose a programmer debugging performance measure based on data monitored by the system. The system automatically categorizes programmer activity in real time into three types (compilation, program execution, and program modification) by monitoring and analyzing key strokes of a programmer. The resulting outputs are the time sequences of monitored activities. The measure we propose is the average length of debugging time per fault, D, estimated from the data sequences monitored by the system. To estimate the debugging time per fault, we introduce a testing and debugging process model. The process model has parameters associated with the average length of a program modification, d, and the probability of a fault being fixed completely by a program modification, r. By taking account of r as well as d, the debugging time per fault can be estimated with high accuracy. The model parameters, such as d and r, are computed from the monitored data sequences by using a maximum likelihood estimation method.<<ETX>>
[Real time systems, program debugging, program testing, Length measurement, debugging time, key stroke analysis, maximum likelihood estimation, Condition monitoring, software development team management, programmer performance measure, program modification, time sequences, debugging, process model, software engineering, real time, Testing, Maximum likelihood estimation, project management, Computerized monitoring, probability, Debugging, testing, data sequences, performance evaluation, programmer activity monitoring, program execution, maximum likelihood estimation method, Time measurement, Programming profession, Software development management, compilation, monitored data, programmer state transitions]
Storing and retrieving software components: a refinement based system
Proceedings of 16th International Conference on Software Engineering
None
1994
Software reuse poses a number of challenges, ranging from managerial to technical - not least of these is the problem of storing and retrieving software components in a time efficient manner. This paper presents the design and implementation of an automated software repository, where software components can be automatically stored and retrieved. This repository is based on a formal representation of programs and their specifications, as well as a refinement ordering of these specifications.<<ETX>>
[software reuse, time efficient, software component retrieval, software component storage, refinement based system, Information retrieval, Spatial databases, Personnel, Application software, Proposals, Formal specifications, formal program representation, formal specification, specifications, refinement ordering, Refining, Software quality, software reusability, automated software repository, subroutines]
A framework for evaluating regression test selection techniques
Proceedings of 16th International Conference on Software Engineering
None
1994
Regression testing is a necessary but expensive activity aimed at showing that code has not been adversely affected by changes. A selective approach to regression testing attempts to reuse tests from an existing test suite to test a modified program. This paper outlines issues relevant to selective retest approaches, and presents a framework within which such approaches can be evaluated. This framework is then used to evaluate and compare existing selective retest algorithms. The evaluation reveals strengths and weaknesses of existing methods, and highlights problems that future work in this area should address.<<ETX>>
[Performance evaluation, Software maintenance, Costs, program testing, regression test selection techniques, selective retest approaches, test suite, Computer science, modified program, Production, software engineering, Computational efficiency, statistical analysis, Testing]
The role of testing methodologies in open systems standards
Proceedings of 16th International Conference on Software Engineering
None
1994
This paper describes the lifecycle role of a conformance testing research facility in the open systems standards environment. This facility, the Clemson Automated Testing System (CATS), has demonstrated the value of integrating formalized test methods within all phases of standards development. IEEE's effort to develop a standard for operating systems interfaces (POSIX) has provided a working environment to investigate and evaluate the capabilities of CATS. In this arena, CATS has proven valuable in exposing critical issues in the emerging standard and in formulating feasible solutions on multiple occasions. The role of CATS in the areas of automated testing, profile development and real-time extensions is described. A discussion of future directions for CATS and testing in open system standards concludes the paper.<<ETX>>
[Software testing, System testing, CATS, open systems, program testing, Laboratories, real-time extensions, operating systems interfaces, conformance testing, profile development, formalized test methods, Software standards, software tools, Standards development, IEEE, Cats, Natural languages, Standards publication, open systems standards, POSIX, standards, Automatic testing, Open systems, Clemson Automated Testing System, testing methodologies, standards environment, lifecycle role]
Distributed software engineering
Proceedings of 16th International Conference on Software Engineering
None
1994
The term "distributed software engineering" is ambiguous. It includes both the engineering of distributed software and the process of distributed development of software, such as cooperative work. This paper concentrates on the former, giving an indication of the special needs and rewards in distributed computing. In essence, we argue that the structure of these systems as interacting components is a blessing which forces software engineers towards compositional techniques which offer the best hope for constructing scalable and evolvable systems in an incremental manner. We offer some guidance and recommendations as to the approaches which seem most appropriate, particularly in languages for distributed programming, specification and analysis techniques for modeling and distributed paradigms for guiding design.<<ETX>>
[Availability, distributed software engineering, scalable systems, distributed processing, Educational institutions, Control systems, specification, Application software, compositional techniques, Distributed computing, parallel programming, distributed computing, evolvable systems, Design engineering, Distributed processing, analysis techniques, Parallel processing, Collaborative work, software engineering, distributed programming, Software engineering]
Software engineering education: a place in the sun?
Proceedings of 16th International Conference on Software Engineering
None
1994
Virtually every research specialisation in software engineering would be prepared to claim that its particular concern is the most important in the field. The author wishes to claim such a status for software engineering education, a specialisation barely regarded as respectable among the majority of researchers. What are the grounds for this claim? Virtually all the technologies which we believe hold promise of improving software development are dependent on professionally skilled and educated staff. Software engineering education remains our most powerful means of technology transfer and hence of narrowing the gap between what is known in the research community and what is applied in industry and commerce. Despite economic hiccups the skills shortage is still a critical component of the omnipresent software crisis. Industry is spending a large proportion of its software development budget on training and on recruitment to offset the costs of fundamental education problems.<<ETX>>
[Programming, teaching, software development budget, training, software crisis, Recruitment, Industrial training, recruitment, Technology transfer, research specialisation, software engineering, Business, research community, computer science education, Power engineering education, Power generation economics, technology transfer, Sun, educated staff, professionally skilled staff, skills shortage, professional aspects, research and development management, Computer industry, software engineering education, Software engineering]
SAAM: a method for analyzing the properties of software architectures
Proceedings of 16th International Conference on Software Engineering
None
1994
While software architecture has become an increasingly important research topic in recent years, insufficient attention has been paid to methods for evaluation of these architectures. Evaluating architectures is difficult for two main reasons. First, there is no common language used to describe different architectures. Second, there is no clear way of understanding an architecture with respect to an organization's life cycle concerns -software quality concerns such as maintainability portability, modularity, reusability, and so forth. We address these shortcomings by describing three perspectives by which we can understand the description of a software architecture and then proposing a five-step method for analyzing software architectures called SAAM (Software Architecture Analysis Method). We illustrate the method by analyzing three separate user interface architectures with respect to the quality of modifiability.<<ETX>>
[software maintainability, user interfaces, software quality, formal specification, modifiability quality, Software architecture, Computer architecture, software engineering, SAAM, Instruments, software modularity, Application software, user interface, Computer science, software portability, systems analysis, Software quality, User interfaces, software reusability, Computer industry, Software systems, Software Architecture Analysis Method, software architecture analysis, Software engineering, organization life cycle]
The SMART approach for software process engineering
Proceedings of 16th International Conference on Software Engineering
None
1994
Describes a methodology for software process engineering and an environment, SMART, that supports it. SMART supports a process life-cycle that includes the modeling, analysis, and execution of software processes. SMART's process monitoring capabilities can be used to provide feedback from the process execution to the process model. SMART represents the integration of three separately developed process mechanisms, and it uses two modeling formalisms (object-oriented data representation and imperative-style programming language) to bridge the gap between process modeling, analysis, and execution. SMART demonstrates the meta-environment concept, using a process modeling formalism as input specification to a generator that produces process-centered software engineering environments (PSEEs). Furthermore, SMART supports a team-oriented approach for process modeling, analysis, and execution.<<ETX>>
[project support environments, process life-cycle, process analysis, object-oriented data representation, team-oriented approach, imperative-style programming language, Guidelines, feedback, USA Councils, Feedback, process-centered software engineering environments, software engineering, Large-scale systems, SMART environment, Object oriented programming, Monitoring, object-oriented programming, Object oriented modeling, meta-environment, process monitoring, Bridges, Computer languages, input specification, software process engineering methodology, process execution, process modeling, Software engineering]
Understanding "why" in software process modelling, analysis, and design
Proceedings of 16th International Conference on Software Engineering
None
1994
In trying to understand and redesign software processes, it is often necessary to have an understanding of the "whys" that underlie the "whats" - the motivations, intents, and rationales behind the activities and input-output flows. This paper presents a model which captures the intentional structure of a software process and its embedding organization, in terms of dependency relationships among actors. Actors depend on each other for goals to be achieved, tasks to be performed, and resources to be furnished. The model is embedded in the conceptual modeling language Telos. We outline some analytical tools to be developed for the model, and illustrate how the model can help in the systematic design of software processes. The examples used are adaptations of the ISPW-6/7 benchmark example.<<ETX>>
[Process design, Project management, Programming, systems design, dependency relationships, formal specification, Embedded software, Telos, Software design, Engineering management, input-output flows, actor dependency, software engineering, organisation modeling, software process modeling, performance evaluation, ISPW-6/7 benchmark, Computer science, actors, requirements engineering, systems analysis, Problem-solving, Software tools, conceptual modeling language, Software engineering]
Software architectures: critical success factors and cost drivers
Proceedings of 16th International Conference on Software Engineering
None
1994
Some useful perspectives on the potential and the pitfalls of software architecture investments can be gained via analysis of software architecture critical success factors and their associated cost and benefit drivers. Basically, the potential of software architecture investments comes from appropriately identifying and exploiting positive cost-benefit relationships. The pitfalls come from neglecting critical success factors or from making unrealistic assumptions about their associated cost drivers. Examples from practice of both potential and pitfalls are given in the context of a table which summarizes a framework of software architecture critical success factors and cost drivers being developed at USC.<<ETX>>
[cost drivers, Software maintenance, Costs, software reliability, unrealistic assumptions, Certification, Aerospace industry, software architecture investments, Software architecture, Investments, Computer architecture, Software quality, cost-benefit relationships, software cost estimation, critical success factors, Software reusability, benefit drivers, Software engineering]
Experiments on the effectiveness of dataflow- and control-flow-based test adequacy criteria
Proceedings of 16th International Conference on Software Engineering
None
1994
This paper reports an experimental study investigating the effectiveness of two code-based test adequacy criteria for identifying sets of test cases that detect faults. The all-edges and all-DUs (modified all-uses) coverage criteria were applied to 130 faulty program versions derived from seven moderate size base programs by seeding realistic faults. We generated several thousand test sets for each faulty program and examined the relationship between fault detection and coverage. Within the limited domain of our experiments, test sets achieving coverage levels over 90% usually showed significantly better fault detection than randomly chosen test sets of the same size. In addition, significant improvements in the effectiveness of coverage-based tests usually occurred as coverage increased from 90% to 100%. However the results also indicate that 100% code coverage alone is not a reliable indicator of the effectiveness of a test set. We also found that tests based respectively on control-flow and dataflow criteria are frequency complementary in their effectiveness.<<ETX>>
[Software testing, System testing, control-flow-based test adequacy criteria, Costs, Data analysis, program testing, Educational institutions, Control systems, dataflow-based test adequacy criteria, faulty program versions, test sets, Fault diagnosis, Fault detection, Investments, coverage criteria, software engineering, Monitoring]
A formal approach to determining parallel resource bindings
Proceedings of 16th International Conference on Software Engineering
None
1994
The paper investigates the nature of the design process for parallel operating systems. It proposes a temporal logic-based formal methodology addressing the high-level design of such systems. In operating systems design, much use is made of the informal notion of resource bindings. A way of improving the high-level design of parallel systems is proposed by providing a formal language for enumerating the design space and thus enabling all high-level design alternatives to be represented. A design process to be used with this language is given, the aim being to establish the most appropriate binding. The process is temporal logic-based and permits high level design of parallel systems to be analysed, tested and, in certain cases, formally verified before implementation is embarked upon.<<ETX>>
[Process design, System testing, Formal languages, temporal logic, Logic testing, formal specification, parallel programming, Concurrent computing, formal approach, formal verification, high-level design, operating systems design, Operating systems, Space technology, network operating systems, design process, parallel operating systems, formal languages, Computer science, parallel resource bindings, Collaboration, Feature extraction, temporal logic-based formal methodology, design space, formal language]
Visualizing software systems
Proceedings of 16th International Conference on Software Engineering
None
1994
There are many graphical techniques for visualizing software. Unfortunately, the current techniques do not scale to display large software systems and are largely unused. We present a method for visualizing statistics associated with code that is divided hierarchically into subsystems, directories, and files. Using this technique, we can display the relative sizes of the components in the system, which components are stable and which are changing, where the new functionality is being added, and identify error-prone code with many bug fixes. Using animation, we can display the historical evolution of the code.<<ETX>>
[Visualization, Software maintenance, program diagnostics, Project management, historical evolution, Displays, Software debugging, graphical techniques, animation, bug fixes, Flowcharts, computer graphics, Databases, error-prone code, Computer bugs, relative sizes, Software systems, Software tools, visual programming, software systems visualization]
Software process improvement experience in the DP/MIS function
Proceedings of 16th International Conference on Software Engineering
None
1994
This experience report outlines Corning Inc.'s experience in successfully using software process assessment as part of their Information Service Division's software process improvement program. Improvement actions executed as indicated and prioritized by our self-assessment findings resulted in largely eliminating the cost and schedule overruns on projects in ISD's Systems Engineering Group. This paper describes the ISD process improvement initiative; a summary of our observations and the key lessons we learned concludes the paper.<<ETX>>
[systems engineering projects, Costs, Corning Inc., Programming, DP management, capability maturity model, personnel management, schedule overruns, cost overruns, software process assessment, quality improvement, Management information systems, software process improvement, software engineering, management information systems development, Quality management, Cats, project management, self-assessment findings, data processing, Data processing, Scheduling, management information systems, information services, Software development management, prioritization, Software quality, customer satisfaction measurement, Systems engineering and theory, systems engineering, DP/MIS function]
Software aging
Proceedings of 16th International Conference on Software Engineering
None
1994
Programs, like people, get old. We can't prevent aging, but we can understand its causes, take steps to limits its effects, temporarily reverse some of the damage it has caused, and prepare for the day when the software is no longer viable. A sign that the software engineering profession has matured will be that we lose our preoccupation with the first release and focus on the long-term health of our products. Researchers and practitioners must change their perception of the problems of software development. Only then will software engineering deserve to be called "engineering".<<ETX>>
[long term product health, Engineering profession, Humans, Mathematics, Product design, software aging, Command languages, software viability, Machinery, Programming profession, software development problems, Embedded software, professional aspects, professional maturity, Aging, software engineering, product release, software engineering profession, Impedance]
Formal specification techniques
Proceedings of 16th International Conference on Software Engineering
None
1994
Formal approaches to software specification and development have been a topic of active research for a long time. There now exists an important corpus of knowledge and results in this domain. There is more and more interest in the industrial applications of these techniques, even if it is generally observed that transfer is difficult in this area. The article surveys formal specification techniques, but, as it is difficult (and probably meaningless) to speak of such techniques independently from the development process, some formal development methods are discussed, as well as the impact of formal specifications on the development activities.<<ETX>>
[software specification, Terminology, Petri nets, Prototypes, Life testing, Specification languages, Formal specifications, Application software, Mathematical model, formal specification, formal development methods]
On formal requirements modeling languages: RML revisited
Proceedings of 16th International Conference on Software Engineering
None
1994
Research issues related to requirements modeling are introduced and discussed through a review of the requirements modeling language RML, its peers and its successors from the time it was first proposed at the Sixth International Conference on Software Engineering (ICSE-6) to the present - ten ICSEs later. We note that the central theme of "Capturing More World Knowledge" in the original RML proposal is becoming increasingly important in requirements engineering. The paper highlights key ideas and research issues that have driven RML and its peers, evaluates them retrospectively in the context of experience and more recent developments, and points out significant remaining problems and directions for requirements modeling research.<<ETX>>
[Knowledge engineering, Shape, Laboratories, requirements modeling research, Educational institutions, Proposals, formal specification, Information analysis, requirements modeling, Computer science, RML, functional specification, specification languages, formal requirements modeling languages, software engineering, Error correction, Software engineering, Context modeling]
An experiment to assess different defect detection methods for software requirements inspections
Proceedings of 16th International Conference on Software Engineering
None
1994
Software requirements specifications (SRS) are usually validated by inspections, in which several reviewers read all or part of the specification and search for defects. We hypothesize that different methods for conducting these searches may have significantly different rates of success. Using a controlled experiment, we show that a scenario-based detection method, in which each reviewer executes a specific procedure to discover a particular class of defects has a higher defect detection rate than either ad hoc or checklist methods. We describe the design, execution and analysis of the experiment so others may reproduce it and test our results for different kinds of software developments and different populations of software engineers.<<ETX>>
[Software testing, program debugging, software engineers, program verification, software development, program diagnostics, software defect detection methods, Software performance, Inspection, Educational institutions, software requirements specifications, formal specification, Computer science, scenario-based detection method, Design engineering, software requirements inspections, Fault detection, Production, Hardware, Software standards, checklist method, defect detection rate]
Automated construction of testing and analysis tools
Proceedings of 16th International Conference on Software Engineering
None
1994
Many software testing and analyse's tools manipulate graph representations of programs, such as abstract syntax trees or abstract semantics graphs. Hand-crafting such tools in conventional programming languages can be difficult, error prone, and time consuming. Our approach is to use application generators targeted for the domain of graph-representation-based testing and analysis tools. Moreover, we generate the generators themselves, so that the development of tools based on different languages and/or representations can also be supported better. In this paper we report on our experiences in developing a system called Aria that generates testing and analysis tools based on an abstract semantics graph representation for C and C++ cabled Reprise. Aria itself was generated by the Genoa system. We demonstrate the utility of Aria and, thereby, the pourer of our approach, by showing Aria's use in the development of a tool that derives control dependence graphs directly from Reprise abstract semantics graphs.<<ETX>>
[Software testing, System testing, program testing, C, Laboratories, C language, abstract syntax trees, Reactive power, Tree graphs, testing tools, analysis tools, software tools, C++, Genoa system, abstract semantics graph representation, program diagnostics, graph-representation-based testing, Application software, Computer science, Computer languages, Automatic testing, Aria, Computer errors, object-oriented languages, abstract semantics graphs, application generators, Reprise]
Software reuse - facts and myths
Proceedings of 16th International Conference on Software Engineering
None
1994
The concept of systematic software reuse is simple: the idea of building and using "software preferred parts." By building systems out of carefully designed, pre-tested components, one will save the cost of designing, writing and testing new code. The practice of reuse has not proven to be this simple however, and there are many misconceptions about how to implement and gain benefit from software reuse.<<ETX>>
[System testing, Costs, software preferred parts, software reuse, Buildings, Programming, DP management, Application software, Software libraries, Software quality, Writing, software reusability, Software systems, Software reusability]
Workshop on software engineering and computer-human interaction: joint research issues
Proceedings of 16th International Conference on Software Engineering
None
1994
Software engineering and computer-human interaction have much to do with each other, but their respective research communities typically have little interaction. The purpose of the article is to explore the intersections of these areas, determining what each community has to offer the other as well as to identify and address open problems of mutual interest. Topics of discussion were drawn from the following: cost drivers, current products, prototyping, requirements, formal methods and specifications, testing and evaluation, design and development, architectures, user interfaces and software environments, CHI and CSCW concerns and toolkits.<<ETX>>
[open problems, cost drivers, architectures, requirements, prototyping, CHI, human factors, testing, Interactive computing, man-machine systems, user interfaces, research issues, specifications, CSCW concerns, software environments, formal methods, software engineering, Man-machine systems, computer-human interaction, Software engineering]
Software process description using LOTOS and its enaction
Proceedings of 16th International Conference on Software Engineering
None
1994
Software processes can be treated as cooperative works among several engineers. In order to enact a software process in a distributed environment, the engineers must communicate with each other for exchanging data values and synchronization messages. Such communications should be described in a process description for enacting the process automatically and clarifying the engineers' work. Since these communications are numerous, it is troublesome for the process designers to describe them minutely in the process description. They also make the description unreadable. We propose a formal software process model where we describe only a whole description of a process in which we describe only the contents and temporal orders of primitive activities, and do not specify the communications. From the whole description, we derive each engineer's individual description, automatically where the contents and orders of his activities and communications to others are described. A whole process is enacted by executing all individual descriptions in parallel. Both whole and individual descriptions are described in LOTOS/SPD, an extension of the formal specification language LOTOS. We have also developed a support system for deriving the individual descriptions and executing them on UNIX machines.<<ETX>>
[Process design, LOTOS, process description, Protocols, data exchange, Humans, distributed environment, formal specification, synchronization messages, specification languages, groupware, software development environment, Automatic control, Page description languages, cooperative work, Functional programming, Object oriented programming, LOTOS/SPD, Object oriented modeling, software process description, formal specification language, Formal specifications, formal software process model, UNIX machines, Collaborative work, programming environments]
Exoskeletal software
Proceedings of 16th International Conference on Software Engineering
None
1994
The author advocates the use of a separate and explicit structural language to describe software architectures. The structural nature makes it amenable to both textual and graphical description. Since it is a language, it can be used to support general descriptions and to provide the framework for checking interconnections. In addition, it can be used to generate and manage the system itself. This approach, initially under the guise of simple "module interconnection languages" (MIL) and subsequently as "configuration languages\
[general descriptions, dynamic constructs, software architectures, graphical description, Educational institutions, LAN interconnection, structured programming, unifying framework, skeleton architectures, explicit extension, configuration management, generalised support, exoskeletal software, configuration languages, interaction types, Computer architecture, module interconnection languages, Software systems, Concrete, Skeleton, system structure, explicit structural language]
The use of description logics in KBSE systems
Proceedings of 16th International Conference on Software Engineering
None
1994
The increasing size and complexity of many software systems demand a greater emphasis on capturing and maintaining knowledge at many different levels within the software development process. The knowledge-based software engineering (KBSE) research paradigm is concerned with systems that use formally represented knowledge, with associated inference procedures, to support the various subactivities of software development. As they grow in scale, KBSE systems must balance expressivity and inferential power with the real demands of knowledge base construction, maintenance, performance and comprehensibility. Description logics (DL's) possess several features - a terminological orientation, a formal semantics and efficient reasoning procedures - which offer an effective tradeoff of these factors. We discuss three KBSE systems in which DL's capture some of the requisite knowledge needed to support design, coding and testing activities. We close with a discussion of the benefits of DL's and ways to address some of their limitations.<<ETX>>
[System testing, knowledge engineering, complexity, software development process, Costs, knowledge-based software engineering research paradigm, inferential power, software systems, Project management, Programming, terminological orientation, Engines, formally represented knowledge, formal logic, reasoning procedures, description logics, formal semantics, knowledge based systems, software engineering, inference procedures, Logic, Buildings, knowledge acquisition, knowledge base construction, inference mechanisms, KBSE systems, Inference algorithms, testing activities, Artificial intelligence, Software engineering]
Formalizing architectural connection
Proceedings of 16th International Conference on Software Engineering
None
1994
As software systems become more complex the overall system structure - or software architecture - becomes a central design problem. An important step towards an engineering discipline of software is a formal basis for describing and analyzing these designs. We present a theory for one aspect of architectural description, the interactions between components. The key idea is to define architectural connectors as explicit semantic entities. These are specified as a collection of protocols that characterize each of the participant roles in an interaction and how these roles interact. We illustrate how this scheme can be used to define a variety of common architectural connectors. We provide a formal semantics and show how this lends to a sound deductive system in which architectural compatibility can be checked in a way analogous to type checking in programming languages.<<ETX>>
[Protocols, architectural compatibility, software systems, type checking, programming languages, formal specification, Research and development, explicit semantic entities, Design engineering, software architecture, architectural description, formal semantics, Software architecture, architectural connection, software engineering, engineering discipline, protocols, programming theory, Government, design problem, Connectors, Computer science, Computer languages, deductive system, systems analysis, Software systems, Computer industry, architectural connectors, system structure]
What small businesses and small organizations say about the CMM
Proceedings of 16th International Conference on Software Engineering
None
1994
The US Air Force sponsored research within the Department of Defense software development community to determine the applicability of the Software Engineering Institute's capability maturity model (CMM) for software to small businesses and small software organizations. The research found that small businesses are faced not only with a lack of resources and funds required to implement many of the practices stated in the CMM, but also with the task of basing their process improvement initiatives on practices that do not apply to a small business and small software organization. This paper discusses, from industry's perspective, why small businesses and organizations are experiencing difficulties implementing CMM-based process improvement programs and how they are tailoring their approach to the CMM to meet their quality goals.<<ETX>>
[Procurement, Costs, CMM, process improvement initiatives, Companies, Programming, US Air Force, DP management, capability maturity model, Software development management, Department of Defense software development community, quality goals, Coordinate measuring machines, Defense industry, small organizations, small businesses, software engineering, Capability maturity model, Contracts, military computing, Software engineering]
A periodic object model for real-time systems
Proceedings of 16th International Conference on Software Engineering
None
1994
Time-sensitive objects (TSO), a data-oriented model for real-time systems, impose timing constraints on object values using validity intervals and object histories. Periodic objects, a class of objects within the TSO model, are described in detail and compared with more traditional periodic processes. Advantages of periodic objects are identified, including greater scheduling independence of processing related through data dependencies, more opportunity for concurrency, and improved structure for timing fault tolerance.<<ETX>>
[Real time systems, time-varying systems, scheduling independence, Taxonomy, software reliability, periodic processes, Data engineering, object values, History, object histories, Concurrent computing, Fault diagnosis, data-oriented model, scheduling, object-oriented methods, periodic object model, programming theory, data dependencies, multiprocessing programs, validity intervals, TSO model, concurrency, Computer science, timing constraints, real-time systems, time-sensitive objects, timing fault tolerance, Frequency, fault tolerant computing, Timing, Periodic structures]
Nico Habermann's research: a brief retrospective
Proceedings of 16th International Conference on Software Engineering
None
1994
The last decade and a half of Nico Habermann's research career focused on software engineering, and in particular on software development environments. His earlier work was oriented more towards operating systems and programming language research. We take this opportunity to look back at his research, putting it in a larger perspective, identifying some general themes that characterize his contributions to software engineering in particular, and to computer science in general.<<ETX>>
[programming language research, Law, Engineering profession, software development environments, Documentation, operating systems, biographies, history, Programming profession, Computer science, Computer languages, Runtime, Operating systems, computer science, software engineering, research career, Nico Habermann research, Legal factors, Software engineering]
Facts and myths affecting software reuse
Proceedings of 16th International Conference on Software Engineering
None
1994
Discusses the three most important facts or myths affecting reuse. There is a great deal of misunderstanding about reuse in the software domain and it is difficult to pick out only three: there has been to much emphasis on the reuse of code; software reuse implies some form of modification of the artifact being reused; and software development processes do not explicitly support reuse, in fact they implicitly inhibit reuse.<<ETX>>
[Costs, software reuse, Object oriented modeling, Life testing, Switches, Manuals, Programming, Educational institutions, Computer science, Computer architecture, Packaging, software reusability, software development processes]
Software architecture: practice, potential, and pitfalls
Proceedings of 16th International Conference on Software Engineering
None
1994
Whatever the long-term impact of software architecture may turn out to be, an appropriate starting point is a concrete appraisal of the current state of the practice in the use of software architecture. It is the purpose of the article to take a step in this direction. It provides concrete examples of what is now possible when architectural principles are applied to industrial problems in systematic ways, considers the potential impact of software architecture over the next few years, and suggests steps that should be taken to bring this about.<<ETX>>
[industrial problems, high-level software design, Access protocols, architectural principles, long-term impact, Communication standards, Computer science, Business communication, software architecture, Software architecture, Computer architecture, Information processing, User interfaces, potential impact, Software systems, software engineering, Software engineering]
A paradigm for decentralized process modeling and its realization in the OZ environment
Proceedings of 16th International Conference on Software Engineering
None
1994
We present a model for decentralized Process Centered Environments (PCEs), which support concerted efforts among geographically-dispersed teams - each local team with its own autonomous process - and emphasize flexibility in the tradeoff between collaboration vs. autonomy. We consider both decentralized process modeling and decentralized process enaction. We describe a realization in the OZ decentralized PCE, which employs a rule-based formalism, and also investigate the application to PCEs based on Petri-nets.<<ETX>>
[Automation, project support environments, Computerized monitoring, Software performance, OZ environment, Application software, Personnel, geographically-dispersed teams, rule-based formalism, Computer science, Computer languages, Petri-nets, Process Centered Environments, Collaboration, Automatic control, Software systems, software engineering, software tools, decentralized process enaction, decentralized process modeling]
Comprehension processes during large scale maintenance
Proceedings of 16th International Conference on Software Engineering
None
1994
We present results of observing professional maintenance engineers working with industrial code at actual maintenance tasks. Protocol analysis is used to explore how code understanding might differ for small versus large scale code. The experiment confirms that cognition processes work at all levels of abstraction simultaneously as programmers build a mental model of the code. Cognition processes emerged at three levels of aggregation representing lower and higher level strategies of understanding. They show differences in what triggers them and how they achieve their goals. Results are useful for defining core competencies which maintenance engineers need for their work and for documentation and development standards.<<ETX>>
[Software maintenance, Protocols, core competencies, human factors, Switches, user modeling, industrial code, Cognition, abstraction levels, user interfaces, Code standards, professional maintenance engineers, Guidelines, large scale maintenance, cognition processes, Large-scale systems, Cognitive science, cognitive systems, code understanding, mental model, documentation, protocol analysis, software maintenance, Programming profession, maintenance tasks, Computer science, development standards, comprehension processes]
Experience with the development of hard real-time embedded Ada software
Proceedings of 16th International Conference on Software Engineering
None
1994
Though already intrinsically demanding, the development of real-time embedded on-board software is often made harsher by the constraining nature of the execution environment and the general lack of suitable support. One of the key needs in the design of these systems is to get guidance towards the definition of a system that is truly analysable against timing requirements; specialised methods and tools are needed to accommodate this particular demand. This paper reports on the use of a novel design method especially tailored towards the construction of hard real-time systems.<<ETX>>
[Real time systems, Design methodology, scheduling paradigms, Control systems, System analysis and design, Embedded software, HRT-HOOD method, Space technology, execution environment, worst-case execution time analysis, scheduling, Aerospace testing, software engineering, object-oriented methods, analysable system, on-board software development, Application software, timing requirements, object based design, hard real-time systems design, schedulability analysis, real-time systems, embedded Ada software, Software systems, Timing, hard real-time attributes, Ada]
Lessons from using Basic LOTOS
Proceedings of 16th International Conference on Software Engineering
None
1994
We describe three case studies in the use of Basic LOTOS for electronic switching systems software. The studies cover design recovery, requirements specification, and design activities. We also report lessons learned from the studies. Early lessons suggested changes to the syntax of the language used, and the need for some specific analysis tools. The last case study reports some of the results of these changes.<<ETX>>
[Software testing, System testing, software design, Electronic equipment testing, Programming, electronic switching systems, case studies, formal specification, software requirements, Software design, telecom computing, Production, specification languages, requirements specification, Basic LOTOS, analysis tools, design recovery, switching systems, design activities, Switching systems, ESS, Formal specifications, Automata, Software systems, syntax]
Software reuse experience at Hewlett-Packard
Proceedings of 16th International Conference on Software Engineering
None
1994
At Hewlett-Packard, we have had visible divisional software reuse efforts since the mid-1980s. In 1990, we initiated a multi-faceted corporate reuse program to gather information about reuse from within HP and from other companies. As we studied the existing reuse programs, we discovered that certain issues were poorly understood, and as a consequence, mistakes were made in starting and running certain programs at HP and elsewhere. Our corporate reuse program focused on packaging best-practice information and guidelines to avoid common pitfalls. We also developed technology transfer and educational processes to spread this information and enhance reuse practice within the company. In 1992, we launched a multi-disciplinary research program to investigate and develop better methods for domain-specific, reuse-based software engineering. We have learned that for large-scale reuse to work, the problems to be overcome are mostly non-technical.<<ETX>>
[Productivity, Costs, software reuse, nontechnical problems, Laboratories, Time to market, DP industry, Hewlett-Packard, technology transfer, multi-disciplinary research program, domain-specific software engineering, Production facilities, Asset management, best-practice information, educational processes, corporate reuse program, Milling machines, Guidelines, Software libraries, pitfalls, Packaging, software reusability, guidelines]
On the inference of configuration structures from source code
Proceedings of 16th International Conference on Software Engineering
None
1994
We apply mathematical concept analysis to the problem of inferring configuration structures from existing source code. Concept analysis has been developed by German mathematicians over the last years; it can be seen as a discrete analogon to Fourier analysis. Based on this theory, our tool will accept source code, where configuration-specific statements are controlled by the preprocessor. The algorithm will compute a so-called concept lattice, which - when visually displayed - allows remarkable insight into the structure and properties of possible configurations. The lattice not only displays fine-grained dependencies between configuration threads, but also visualizes the overall quality of configuration structures according to software engineering principles. The paper presents a short introduction to concept analysis, as well as experimental results on various programs.<<ETX>>
[Drugs, Visualization, mathematical concept analysis, inference, configuration structures, Lattices, source code, Fourier analysis, Mathematics, inference mechanisms, Yarn, Sun, Painting, configuration management, discrete analogon, software engineering principles, Computer displays, fine-grained dependencies, Algebra, concept lattice, configuration threads, Software engineering]
An integrated method for effective behaviour analysis of distributed systems
Proceedings of 16th International Conference on Software Engineering
None
1994
Behavioral analysis is a valuable aid for the design and maintenance of well-behaved distributed systems. Dataflow and reachability analyses are two orthogonal, but complementary, behavioral analysis techniques. Individually, each of these techniques may be inadequate for the analysis of large-scale distributed systems. On the one hand, dataflow analysis algorithms, while tractable, may not be sufficiently accurate to provide meaningful detection of errors. On the other hand, reachability analysis, while providing exhaustive analysis, may be computationally too expensive for complex systems. In this paper, we present a method which integrates dataflow and reachability analysis techniques to provide a flexible and effective means for analysing distributed systems at the preliminary and final design stages respectively. We also describe some effective measures taken to improve the adequacy of the individual analysis techniques using the concepts of action dependency and context constraints. A prototype supporting the method has been built, and its performance is described in this paper. A realistic example of a distributed track control system is used as a case study.<<ETX>>
[Algorithm design and analysis, integrated method, well-behaved distributed systems, behavioural analysis, distributed processing, Control systems, accuracy, error detection, Distributed computing, tractability, computational expense, complex systems, context constraints, Prototypes, distributed track control system, Large-scale systems, preliminary design stage, Data analysis, dataflow analysis, reachability analysis, program diagnostics, exhaustive analysis, action dependency, Educational institutions, Explosions, large-scale distributed systems, Reachability analysis, prototype, case study, large-scale systems, final design stage, performance, Distributed control, maintenance]
Software reuse myths revisited
Proceedings of 16th International Conference on Software Engineering
None
1994
In ACM Software Engineering Notices, vol. 13, no. 1, pp. 17-21 (1988), the author published the paper "Software reuse myths". This paper comments on these "myths" in the light of recent technology advances: (1) software reuse is a technical problem; (2) special tools are needed for software reuse; (3) reusing code results in huge increases in productivity; (4) artificial intelligence will solve the reuse problem; (5) the Japanese have solved the reuse problem; (6) Ada has solved the reuse problem; (7) designing software from reusable parts is like designing hardware using integrated circuits; (8) reused software is the same as reusable software; and (9) software reuse will just happen.<<ETX>>
[Costs, Computer aided software engineering, software reuse, Knowledge acquisition, nontechnical problems, software design, Documentation, Paper technology, Japanese, artificial intelligence, productivity, reused software, software reusability, software tools, reusable parts, reusable software, Impedance, Software tools, Artificial intelligence, Software reusability, Software engineering, Ada]
Reuse facts and myths
Proceedings of 16th International Conference on Software Engineering
None
1994
This paper on software reuse is the view of a practitioner rather than of a scientist. Myth 1: OO technology eats up reuse. Fact 1: OO does not automatically yield high reuse rates-both OO and reuse can complement each other. Myth 2: Incentives are key to reuse success. Fact 2: Incentives create awareness, are cheap but don't change much. Myth 3: Reuse is for free. Fact 3: Reuse is a mid-term investment impacting the entire software development process. It must be based on a product strategy which spans several releases or a family of products.<<ETX>>
[Software testing, System testing, software development process, product strategy, Costs, object-oriented programming, software reuse, Ducts, product releases, Programming, investment, Software libraries, OO technology, incentives, awareness, Investments, software reusability, product family, System software, mid-term investment, Software reusability, Software engineering]
Completeness and Consistency Analysis of State-Based Requirements
1995 17th International Conference on Software Engineering
None
1995
This paper describes methods for automatically analyzing formal, state-based requirements specifications for completeness and consistency. The approach uses a low-level functional formalism, simplifying the analysis process. State space exploslon problems are eliminated by applying the analysis at a high level of abstraction; i.e, instead of generating a reachability graph for analysis, the analysis is performed directly on the model. The method scales up to large systems by decomposing the specification into smaller, analyzable parts and then using functional composition rules to ensure that verified properties hold for the entire specification. The analysis algorithms and tools have been validated on TCAS II, a complex, airborne, collision-avoidance system reqmred on all commercial aircraft with more than 30 passengers that fly in U.S. airspace.
[Software engineering]
Deriving Specifications from Requirements: an Example
1995 17th International Conference on Software Engineering
None
1995
A requirement is a desired relationship among phenomena of the environment of a system, to be brought about by the hardware/software machine that will be constructed and installed in the environment. A specification describes machine behaviour sufficient to achieve the requirement. A specification is a restricted kind of requirement: all the environment phenomena mentioned in a specification are shared with the machine; the phenomena constrained by the specification are controlled by the machine; and the specified constraints can be determined without reference to the future. Specifications are derived from requirements by reasoning about the environment, using properties that hold independently of the behaviour of the machine. These ideas, and some associated techniques of description, are illustrated by a simple example.
[Software engineering]
Dealing with Non-Functional Requirements: Three Experimental Studies of a Process-Oriented Approach
1995 17th International Conference on Software Engineering
None
1995
Quality characteristics are vital for the success of software systems. To remedy the problems inherent in ad hoc development, a framework has been developed to deal with non-functional requirements (quality requirements or NFRs). Taking the premise that the quality of a product depends on the quality of the process that leads from high-Ievel NFRs to the product, the framework's objectives are to represent NFR-specific requirements, consider design tradeoffs, relate design decisions to IYFRs, justify the decisions, and assist defect detection. The purpose of this paper is to give an initial evaluation of the extent to which the framework's objectives are met. Three small portions of information systems were studied by the authors using the framework. The framework and empirical studies are evaluated herein, both from the viewpoint of domain experts who have reviewed the framework and studies, and ourselves as framework developers and users. The systems studied have a variety of characteristics, reflecting a variety of real application domains, and the studies deal with three important classes of NFRs for systems, namely, accuracy, security, and performance. The studies provide preliminary support for the usefulness of certain aspects of the framework, while raising some open issues.
[Software engineering]
Effect of Test Set Minimization on Fault Detection Effectiveness
1995 17th International Conference on Software Engineering
None
1995
Size and code coverage are important attributes of a set of tests. When a program P is executed on elements of the test set T, we can observe the fault detecting capability of T for P. We can also observe the degree to which T induces code coverage on P according to some coverage criterion. We would like to know whether it is the size of T or the coverage of T on P which determines the fault detection effectiveness of T for P. To address this issue we ask the following question: While keeping coverage constant, what is the effect on fault detection of reducing the size of a test set? We report results from an empirical study using the block and all-uses criteria as the coverage measures.
[Fault detection, Testing, Software engineering]
Testing Real-Time Constraints in a Process Algebraic Setting
1995 17th International Conference on Software Engineering
None
1995
Verifying timing properties of real-time systems by traditional approaches that depend on the exploration of the entire system state space is impractical for large systems. In contrast, testing allows the search for violations of a property to be narrowed to a relatively small portion of the overall state space, based on assumptions regarding the structure of an implementation. We present a domain testing based technique for verifying timing constraints of real-time systems, given timing constraints specified as minimum and maximum allowable delays between input/output events. The testing methodology is presented in a process algebraic setting where it is an efficient alternative to model checking for verifying system timing properties. An example illustrates the application of our testing technique to a timed interactive system.
[Testing, Software engineering]
Using Testability Measures for Dependability Assessment
1995 17th International Conference on Software Engineering
None
1995
Program "testability" is the probability that a fault in a program, if present, will cause the program to fail. Measures of testability can be used to draw inferences on program correctness from the observation of a series of failure-free test executions, a common need for software with "ultra-high reliability" requirements. For a program that has passed a certain number of tests without failing, a high value of testability implies a high probability that the program is correct. We give a general descriptive model of program execution and testing, and propose a more precise definition of program testability than that given by other authors. We then study the use of testability in: i) providing, through testing, confidence in the absence of faults and ii) bounding the probability of failures, from the results of operational testing. We derive the probability of absence of faults through a Bayesian inference procedure, criticise previously proposed derivations of this probability, and study the relationship between the testability of a program and its failure probability in operation. We derive the conditions under which a high testability improves one's expectations about program reliability. Last, we discuss the potential of these methods in practical applications.
[Testing, Software engineering]
Automating Process Discovery through Event-Data Analysis
1995 17th International Conference on Software Engineering
None
1995
Many software process methods and tools presuppose the existence of a formal model of a process. Unfortunately, developing a formal model for an on-going, complex process can be dificult, costly, and error prone. This presents a practical barrier to the adoption of process technologies. The barrier would be lowered by automatmg the creation of formal models. We are currently exploring techniques that can use basic event data captured from an on-going process to generate a formal model of process behavior. We term this kind of data analysis process discovery. Thts paper descrbes and illustrates three methods with whzch we have been experimenting: algorithmic grammar inference, Markov models, and neural networks.
[Software engineering]
Using Event Contexts and Matching Constraints to Monitor Software Processes
1995 17th International Conference on Software Engineering
None
1995
Automated monitoring of events that occur in a computer network is essential for non-intrusive software process enactment. Relevant events include modification of product artifacts like source files and documents, and execution and termination of tool invocations. Monitoring these events requires capturing the context in which they occur and constraining the monitoring to relevant events only. We introduce the concept of event context, and present constructs for using it to constrain event matching. The constructs have been implemented as an extension to an existing event monitoring system called Yeast, which serves as the event monitoring component in Provence. Provence is an open process-centered environment that monitors the actual execution of software processes, abstracted as sequences of low-level events, and maps these sequences of events into transitions in the process model enactment.
[Monitoring, Software engineering]
An Organizational Learning Approach to Domain Analysis
1995 17th International Conference on Software Engineering
None
1995
As the application of computer technology continues to proliferate and diversify, the identification and understanding of application domains is becoming increasingly important to software development methodologies. Domain analysis techniques have been developed to accumulate and formalize the knowledge necessary for successful software reuse. These techniques have been shown to be useful, but suffer from defining the domain too restrictively, burying important relationships deep in domain taxonomies, and prohibiting flexible identification of domains with common issues. Techniques are needed that dynamically detect recurring patterns of activities in development projects, This paper presents a method for developing and refining the knowledge and experience accumulated by a development organization so it can learn from previous efforts. A case-based repository of project experiences supports the re-use and refinement of domain knowledge to reduce duplicate effort, build on successful efforts, and avoid repeating mistakes in the process of building quality software systems.
[Software engineering]
Managing Software Reuse--An Experience Report
1995 17th International Conference on Software Engineering
None
1995
Schlumberger Oilfield Services started a software reuse effort at its Austin System Center (ASC) in 1988. ASC is responsible for computer systems integrated into products used within Schlumberger or sold to oil companies for oil data acquisition, transmission, and analysis. The group responsible for this reuse effort, known as the Common Software Library (CSL), is composed of 10 engineers who manage 17 common software component libraries used in several Oilfield Services products. The CSL has approximately 2 million lines of code; the 16 products, into which some or all of the CSL libraries are built, have 1 million to 10 million lines of code each. Since its inception, the group has found that a successful reuse strategy requires as much effort in dealing with people management issues as it does with technical issues. This article offers one model for how to introduce a software reuse program into an organization, how such a program can be efficiently managed, and what major challenges and achievements may be expected.
[Software engineering]
An experience in process assessment
1995 17th International Conference on Software Engineering
None
1995
This paper presents an experience in software process assessment that has been conducted in a mid-size Italian company. The assessment has been carried out using the CMM (Capability Maturity Model) and taking into account also the indications offered by QIP (Quality Improvement Paradigm). The paper discusses the results of the assessment and the lessons we have learned from running it. It particular, it argues that it is necessary to broaden the scope of assessment methods, and to evaluate the applicability to software processes of the experiences, methods, and techniques developed in other business domains.
[process, software process assessment, assessment methods, Software process, improvement, Software engineering]
Improvement of Software Process by Process Description and Benefit Estimation
1995 17th International Conference on Software Engineering
None
1995
This paper describes an actual experience of software process improvement at OMRON corporation. For effective technical transfer, a software engineering process group (SEPG) has set three principal goals as follows: (1) motivate developers to improve on their process, (2) describe and define current software process correctly and in detail, (3) present a feasible action plan for developers to follow. To attain these goals, the SEPG has proposed a process improvement procedure by describing the current process and estimating the benefits gained by the improvement. Firstly the SEPG, held a series of interviews with developers and depicted a flow map in the form of a Petri net which describes the current, software process. Secondly, the group constructed an action plan from the in-depth analysis of the current flow map, and estimated the benefits obtained if this plan were to be rigorously followed. As a result, both the action plan and the benefit estimation were agreed by the developers as a feasible action plan. Furthermore, by applying the action plan to a practical project, it was confirmed that, compared to a similar project, approximately 10&#x025; of the total effort/KLOC is reduced at test phases. Thus we came to a conclusion that the principal goals and the proposed procedure are proven to be effective for reducing the development effort at OMRON corporation.
[Software engineering]
Characterizing and Assessing a Large-Scale Software Maintenance Organization
1995 17th International Conference on Software Engineering
None
1995
One important component of a software process is the organizational context in which the process is enacted. This component is ofien missing or incomplete in current process modeling approaches. One technique for modeling this perspective is the Actor-Dependency (AD) Model. This paper reports on a case study which used this approach to analyze and assess a large software maintenance organization. Our goal was to identify the approach's strengths and weaknesses while providing practical recommendations for improvement and research directions. The AD model was found to be very useful in capturing the important properties of the organizational context of the maintenance process, and aided in the understanding of the flaws found in this process. However, a number of opportunities for extending and improving the AD model were identified. Among others, there is a need to incorporate quantitative information to complement the qualitative model.
[Software maintenance, Large-scale systems, Software engineering]
A Framework for Evaluating Specification Methods for Reactive Systems
1995 17th International Conference on Software Engineering
None
1995
Numerous formal specification methods for reactive systems have been proposed in the literature. Because the significant differences bet ween the methods are hard to determine, choosing the best method for a particular application can be difficult. We have applied several different methods, including Modechart, VFSM, ESTEREL, Basic LOTOS, Z, SDL and C, to an application problem encountered in the design of software for AT&#x00026;T's 5ESS&#x0AE; telephone switching system. We have developed a set of criteria for evaluating and comparing the different specification methods. We argue that the evaluation of a method must take into account not only academic concerns, but also the maturity of the method, its compatibility with the existing software development process and system execution environment, and its suitability for the chosen application domain.
[Software engineering]
PARTS: A Temporal Logic-Based Real-Time Software Specification and Verification Method
1995 17th International Conference on Software Engineering
None
1995
Areas of computer application are being broadened rapidly due to the rapid improvement of the performance of computer hardware. Applications that were not feasible before are now becoming feasible with high-performance computers. This results in increased demands for computer applications that are large and have complex temporal characteristics. Most analysis methods available, however, cannot handle large, complex real-time systems adequately; They do not scale-up, lack formalism to represent complex features and perform analyses with mathematical rigor, do not support analyses from different viewpoints, or are too hard to learn and apply. TVe need analysis methods that support formal specification and verification of real-time systems. Incremental performance of specification and analysis of systems from different viewpoints (e.g., user, analyst ) must also be supported with languages appropriate for each different viewpoint and for the users involved. This paper introduces a real-time systems analysis method, named PARTS, that aims at providing above features. PARTS supports analyses from two viewpoints: external viewpoint, a view of the sys-Permission tern from the user's perspective, and internal viewpoint, a view from the developer's perspective. These viewpoints are specified using formal languages, which are: Real-Time Events Trace (RTET) for the external viewpoint, and Time Enriched Statecharts (TES) and PARTS Data Flow Diagram (PDFD) for the internal viewpoint. All PARTS languages are based on the Real-Time Temporal Logic (RTTL), and consistency of the specifications made from two different viewpoints are analyzed based on the same RTTL formalism. PARTS converts RTET and TES specifications to RTTL specifications, which are then integrated and analyzed for consistency. All of the PARTS specificaticm languages support the top-down strategy to handle complexity.
[Software engineering]
Architectural Mismatch or Why it's hard to build systems out of existing parts
1995 17th International Conference on Software Engineering
None
1995
Many would argue that future breakthroughs in software productivity will depend on our ability to combine existing pieces of software to produce new applications. An important step towards this goal is the development of new techniques to detect and cope with mismatches in the assembled parts. Some problems of composition are due to low-level issues of interoperability, such as mismatches in programming languages or database schemas. However, in this paper we highlight a different, and in many ways more pervasive, class of problem: architectural mismatch. Specifically, we use our experience in building a family of software design environments from existing parts to illustrate a variety of types of mismatch that center around the assumptions a reusable part makes about the structure of the application in which is to appear. Based on this experience we show how an architectural view of the mismatch problem exposes some fundamental, thorny problems for software composition and suggests possible research avenues needed to solve them.
[Software engineering]
Reverse Engineering to the Architectural Level
1995 17th International Conference on Software Engineering
None
1995
Recovery of higher level "design" information and the ability to create dynamic, task adaptable software documentation is crucial to supporting a number of program understanding activities. This paper presents research that demonstrates that reverse engineering technology can be used to recover software architecture representations of source code. We have developed a framework that integrates reverse engineering technology and architectural style representations. Using the framework, analysts can recover custom, dynamic documentation to fit a variety of software analysis requirements. Our goal is to establish coherent abstractions appropriate for helping analysts to understand large software systems. We discuss a code coverage metric useful for assessing the degree of program understanding achieved.
[Reverse engineering, Software engineering]
Software Architecture in Industrial Applications
1995 17th International Conference on Software Engineering
None
1995
To help us identify and focus on pragmatic and concrete issues related to the role of software architecture in large systems, we conducted a survey of a variety of software systems used in industrial applications. Our premise, which guided the examination of these systems, was that software architecture is concerned with capturing the structures of a system and the relationships among the elements both within and between structures. The structures we found fell into several broad categories: conceptual architecture, module interconnection architecture, code architecture, and execution architecture. These categories address different engineering concerns. The separation of such concerns, combined with specialized implementation techniques, decreased the complexity of implementation, and improved reuse and reconfiguration.
[Software architecture, Computer industry, Application software, Software engineering]
Supporting evolution and maintenance by using a flexible automatic code generator
1995 17th International Conference on Software Engineering
None
1995
Generating code automatically from the design level increases product quality and productivity but also facilitates maintenance and evolution by limiting changes to the design level. Flexibility is a basic requirement that should be fulfilled by automatic code generators: the translation strategies should be easily adapted to different platforms or company standards and also to the evolution of the system which they produce. We present our approach to flexible code generation, in the frame of our SDL methodology, and the code generator tool ProgGen. The methodology focuses on the design phase and distinguishes between functional design (i.e. describing behavior) and implementation design (i.e. describing the concrete system). The implementation design description plays a central role in the code generation process. ProgGen is a generic tool which can be used to produce SDL translators; the output is controlled by a set of code skeletons. The skeletons can easily be tailored to support changes in the hand-coded software interfaces (e.g. driver interfaces), in the implementation platform or in the non functional requirements. The use of ProgGen at Alcatel Telecom Norway illustrates how the code generation process has been formalized. Finally, we show how ProgGen is used in the ESPRIT project PROTEUS where our goal is to complete automation of the entire system building.
[Software engineering]
A Framework for Selective Recompilation in the Presence of Complex Intermodule Dependencies
1995 17th International Conference on Software Engineering
None
1995
Compilers and other programming environment tools derive information from the source code of programs; derived information includes compiled code, interprocedurrd summary information, and call graph views. If the source program changes, the derived information needs to be updated. We present a simple framework for maintaining interrnodule dependencies, embodying different tradeoffs in terms of space usage, speed of processing, and selectivity of invalidation, that eases the implementation of incremental update of derived information. Our framework augments a directed acyclic graph representation of dependencies with factoring nodes (to save space) and filtering nodes (to increase selectivity), and it includes an algorithm for efficient invalidation processing. We show how several schemes for selective recompilation, such as smart recompilation, filter sets for interprocedural summary information, and dependencies for whole-program optimization of object-oriented languages, map naturally onto our framework. For this latter application, by exploiting the facilities of our framework, we are able to reduce the number of lines of source code recompiled by a factor of seven over a header file-based scheme, and by a factor of two over the previous state-of-the-art selective dependency mechanism without consuming additional space.
[Software engineering]
Towards a Formal Semantics of Parnas Tables
1995 17th International Conference on Software Engineering
None
1995
In Parnas at al. advocate the use of relational model for documenting the intended behaviour of programs. In this method, tabular expressions (or tables) are used to improve readability so that formal documentation can replace conventional documentation. Parnas describes several classes of tables and provides their formal syntax and semantics. In this paper, an alternative, more general and more homogeneous semantics is proposed.
[Software engineering]
Software Requirements Negotiation and Renegotiation Aids: A Theory-W Based Spiral Approach
1995 17th International Conference on Software Engineering
None
1995
A major problem in requirements engineering is obtaining requirements that address the concerns of multiple stakeholders. An approach to such a problem is the Theory-W based Spiral Model. This paper focuses on the problem of developing a support system for such a model. In particular it identifies needs and capabilities required to address the problem of negotiation and renegotiation that arises when the model is applied to incremental requirements engineering. The paper formulates elements of the support system, called WinWin, for providing such capabilities. The key elements of WinWin are described and their use in incremental requirements engineering are demonstrated, using an example renegotiation scenario from the domain of software engineering environments, for satellite ground stations.
[Spirals, Software engineering]
Decentralised Process Enactment in a Multi-Perspective Development Environment
1995 17th International Conference on Software Engineering
None
1995
The ViewPoints framework for distributed and concurrent software engineering provides an alternative approach to traditional centralised software development environments. We investigate the use of decentralised process models to drive consistency checking and conflict resolution in this framework. Our process models use pattern matching on local development histories to determine the particular situation (state) of the development process, and employ rules to trigger situation-dependent assistance to the user. We describe how communication between such process models facilitates the decentralised management of explicitly defined consistency constraints in the ViewPoints framework.
[Software engineering]
How To Deal With Deviations During Process Model Enactment
1995 17th International Conference on Software Engineering
None
1995
A fundamental problem in software processes is how the intrinsic rigidity of a predefined (formal) model can be reconciled with the need for flexibility, change, and evolution. We therefore distinguish between software processes, as specified in a process description, and their actual performance by humans. Further, we claim that the two inevitably diverge, and thus it is necessary to provide means to reconcile them. We present a preliminary exploration into the problem. In particular, we illustrate how a temporal logic-based approach can be used to capture and tolerate some deviations from the process description during execution. We present a simple process language (LATIN), and its prototype environment (SENTINEL), in which these ideas are currently experimented.
[Software engineering]
Experimental Software Engineering: A Report on the State of the Art
1995 17th International Conference on Software Engineering
None
1995
The goal of this session is to make the software engineering community aware of the opportunities that exist to pursue such an experimental approach. In the remainder of the essay, we describe an emerging model for empirical work and the language for discussing it. We then focus on the current state of experimental software engineering, the road blocks barring effective progress, and what developers and researchers can do to remove them.
[Software engineering]
The World and the Machine
1995 17th International Conference on Software Engineering
None
1995
As software developers we are engineers because we make useful machines. We are concerned both with the world, in which the machine serves a useful purpose, and with the machine itself. The competing demands and attractions of these two concerns must be appropriately balanced. Failure to balance them harms our work. Certainly it must take some of the blame for the gulf between researchers and practitioners in software development. To achieve proper balance we must sometimes fight against tendencies and inclinations that are deeply ingrained in our customary practices and attitudes in software development. In this paper some aspects of the relationship between the world and the machine are explored; some sources of distortion are identified; and some suggestions are put forward for maintaining a proper balance.
[Software engineering]
A Component- and Message-Based Architectural Style for GUI Software
1995 17th International Conference on Software Engineering
None
1995
While a large fraction of application system code is devoted to user interface (UI) functions, support for reuse in this domain has largely been confined to creation of UI toolkits ("widgets"). We present a novel architectural style directed at supporting larger grain reuse andjexible system composition. Moreover, the style supports design of distributed, concurrent, applications. A key aspect of the style is that components are not built with any dependencies on what typically would be considered lower-level components, such as user interface toolkits. Indeed, all components are oblivious to the existence of any components to which notification messages are sent. Asynchronous notification messages and asynchronous request messages are the sole basis for inter-component communication. While our focus has been on applications involving graphical user interfaces, the style has the potential for broader applicability. Several trial applications using the style are described.
[Graphical user interfaces, Software engineering]
An Architecture for Integrating Concurrency Control into Environment Frameworks
1995 17th International Conference on Software Engineering
None
1995
Layered and componentized systems promise substantial benefits from dividing responsibilities, but it is still unresolved how to construct a system from pre-esisting, independently developed pieces. Good solutions to this problem, in general or for specific classes of components, should reduce duplicate implementation efforts and promote reuse of large scale subsystems. We tackle the domain of software development environments and present an architecture for retrofitting external concurrency control components onto existing environment frameworks. We describe a sample ECC component, explain how we added concurrency control to a commercial product that had none, and briefly sketch how we replaced the concurrency control mechanism of a research system.
[Concurrency control, Software engineering]
Tool Integration: Experiences and Directions
1995 17th International Conference on Software Engineering
None
1995
As more software support tools become available, the construct ion of support environments by integration of existing tools becomes more desirable. The European project "Broadband Object-Oriented Service Technology" investigated the adaptation and integration of existing software tools to support telecommunications service creation. The investigation covered different approaches to adapting and integrating existing tools to form a service creation environment. The earliest environment encapsulated a wide range of these tools within an environment called EAST, supplied by the Soci&#x0E9;t&#x0E9; Frangaise de G&#x0E9;nie Logiciel. While EAST provided a number of benefits, the users of the environment expressed a desire for small special-purpose toolkits which were created by the tool-to-tool integration of specific tools. These toolkits were used on their own, and were also encapsulated within EAST. With the advent of Tcl, a language which facilitates tool programmability, it became possible to create programmable "capsules\
[Software engineering]
Reverse Engineering of Legacy Code Exposed
1995 17th International Conference on Software Engineering
None
1995
Reverse engineering of large legacy software systems generally cannot meet its objectives because it cannot be cost-effective. There are two main reasons for this. First, it is very costly to "understand" legacy code sufficiently well to permit changes to be made safely, because reverse engineering of legacy code is intractable in the usual computational complexity sense. Second, even if legacy code could be cost-effectively reverse engineered, the ultimate objective - re-engineering code to create a system that will not need to be reverse engineered again in the future - is presently unattainable. Not just crusty old systems, but even ones engineered today, from scratch, cannot escape the clutches of intractability until software engineers learn to design systems that support modular reasoning about their behavior. We hope these observations serve as a wake-up call to those who dream of developing high-quality software systems by transforming them from defective raw materials.
[Reverse engineering, Software engineering]
Reverse Engineering of Legacy Systems: A Path Toward Success
1995 17th International Conference on Software Engineering
None
1995
This paper addresses the question of whether the reverse engineering of legacy systems is doomed to failure. Our position is that the answer is highly dependent on the specific goals of the reverse engineering process. We argue that while most reverse engineering efforts may well fail to achieve the traditional goal of automatically extracted complete specifications suitable for forward engineering, they are likely to succeed on the more modest goal of automatically extracting partial specifications that can augmented by system-assisted human understanders.
[Reverse engineering, Software engineering]
Cooperating evolving components- A rigorous approach to evolving large software systems
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Large software systems have a large number of components and are developed over a long time period frequently by a large number of people. We describe a framework approach to evolving such systems based on an integration of product and process modelling. The evolving system is represented as a Product Tower, a hierarchy of components which provides views of the product at multiple levels of refinement. The evolution process is component based with the cooperation between components being mediated by the Product Tower. This ensures that the evolution process is scaleable and that it maintains, and evolves, the design model. We illustrate our approach with an example, outlining an evolution both of the product and of the process. The reflexive facilities of the process are shown to be key in ensuring the framework's ability to evolve.
[Process design, process modelling, Poles and towers, Process control, large software systems, product evolution, Environmental management, process evolution, evolving components, Computer science, Refining, Software systems, Marketing and sales, software engineering, design hierarchy, Logic, Product Tower, Software tools]
Linguistic support for the evolutionary design of software architectures
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
As a program's functionality evolves over time, its software architecture should evolve as well so that it continues to match the program's design. This paper introduces the architecture language of Clock, a language for the development of interactive, multiuser applications. This architecture language possesses three properties supporting the easy restructuring of software architectures: restricted scoping supported by a constraint-based communication system, automatic message routing, and easy hierarchical restructuring of architectures. Clock's architecture language has a visual syntax, supported by the Clock-Works programming environment.
[high level languages, software architectures, restricted scoping, constraint-based communication, interactive, Routing, Application software, architecture language, Clock, Programming profession, Programming environments, Computer science, automatic message routing, Software design, Software architecture, Interactive systems, Computer architecture, interactive systems, software engineering, multiuser applications, Clock-Works programming environment, programming environments, visual syntax, Clocks]
A generic, peer-to-peer repository for distributed configuration management
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Distributed configuration management is intended to support the activities of projects that span multiple sites. NUCM (Network-Unified Configuration Management) is a testbed that we are developing to help us explore the issues of distributed configuration management. NUCM separates configuration management repositories (i.e. the stores for versions of artifacts) from configuration management policies (i.e. the procedures by which the versions are manipulated) by providing a generic model of a distributed repository and an associated programmatic interface. This paper describes the model and the interface, presents an initial repository distribution mechanism, and sketches how NUCM can be used to implement two rather different configuration management policies, namely check-in/check-out and change sets.
[programmatic interface, application program interfaces, Laboratories, Project management, artifact versions, software management, multi-site projects, Network-Unified Configuration Management, configuration management repositories, Engineering management, NUCM, distributed databases, check-in/check-out, Contracts, Testing, Peer to peer computing, repository distribution mechanism, Government, generic peer-to-peer repository, change sets, Computer science, configuration management, Software systems, distributed configuration management, configuration management policies, Software engineering]
Beyond structured programming
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Structured programming principles are not strong enough to control complexity and guarantee high reliability of software at the module level. Stronger organizing principles and stronger properties of components are needed to make significant gains in the quality of software. Practical proposals, based on the definition of normal forms which have a mathematical/logical foundation, are suggested as a vehicle for constructing software that is both simpler and of higher quality with regard to clearly defined and justifiable criteria.
[Software maintenance, complexity, software reliability, Reliability theory, Graph theory, structured programming, software quality, Proposals, Organizing, Vehicles, software reliabilty, Databases, Software quality, Transformers, Software measurement, software metrics]
Experience assessing an architectural approach to large-scale systematic reuse
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Systematic reuse of large-scale software components promises rapid, low cost development of high-quality software through the straightforward integration of existing software assets. To date this promise remains largely unrealized, owing to technical, managerial, cultural, and legal barriers, One important technical barrier is architectural mismatch. Recently, several component integration architectures have been developed that purport to promote large-scale reuse. Microsoft's OLE technology and associated applications are representative of this trend. To understand the potential of these architectures to enable large-scale reuse, we evaluated OLE by using it to develop a novel fault-tree analysis tool. Although difficulties remain, the approach appears to overcome architectural impediments that have hindered some previous large-scale reuse attempts, to be practical for use in many domains, and to represent significant progress towards realizing the promise of barge-scale systematic reuse.
[high-quality software, Costs, architectural mismatch, Law, Large scale integration, software quality, Application software, Cultural differences, large-scale software components, Microsoft's OLE technology, Computer science, architectural approach, fault-tree analysis tool, Computer architecture, software reusability, Large-scale systems, Standards development, Legal factors, large-scale systematic reuse]
An exact array reference analysis for data flow testing
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Data-flow testing is a well-known technique, and it has proved to be better than the commercially-used branch testing. The problem with data-flow testing is that, apart from scalar variables, only approximate information is available. This paper presents an algorithm that precisely determines the definition-use pairs for arrays within a large domain. There are numerous methods addressing the array data-flow problem; however, these methods are only used in the optimization or parallelization of programs. Data-flow testing, however, requires at least one real solution of the problem for which the necessary program path is executed. Contrary to former precise methods, we avoid negation in formulae, which seems to be the biggest problem in all previous methods.
[formulae negation avoidance, Data analysis, exact array reference analysis, Optimizing compilers, program testing, Input variables, data flow analysis, program path execution, data flow testing, program optimization, precise method, approximate information, definition-use pairs, program parallelization, arrays, Testing]
Requirements for a layered software architecture supporting cooperative multi-user interaction
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Layered interactive systems lend themselves to be adapted for cooperation if inter-layer communication is charged to separated connectors. Point-to-point connectors can be replaced with cooperative connectors multiplexing and demultiplexing I/O between a particular layer and multiple instances of the next lower one. For this technique to be most effective, some general guidelines should be followed that support the design of good quality software where discrimination between heterogeneous functionality at the architectural level allows multiple interacting users to exploit different system features based on their role in the cooperation. This provides a sound basis for augmenting collaboration-transparent layered systems with powerful collaboration support (e.g. complex coordination policies) yet preserving separation of concerns between applicative and cooperative functionality. The paper discusses these issues both in general and with reference to their application within the CSDL framework for cooperative systems design.
[CSDL framework, Cooperative systems, inter-layer communication, cooperative connectors, Application software, cooperative multi-user interaction, Connectors, Demultiplexing, Software design, Software architecture, Interactive systems, Collaboration, Computer architecture, groupware, layered software architecture, User interfaces, interactive systems, software engineering, multi-access systems, cooperative systems, cooperative systems design]
Understanding and predicting the process of software maintenance releases
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
One of the major concerns of any maintenance organization is to understand and estimate the cost of maintenance releases of software systems. Planning the next release so as to maximize the increase in functionality and the improvement in quality are vital to successful maintenance management. The objective of the paper is to present the results of a case study in which an incremental approach was used to better understand the effort distribution of releases and build a predictive effort model for software maintenance releases. The study was conducted in the Flight Dynamics Division (FDD) of NASA Goddard Space Flight Center (GSFC). The paper presents three main results: (1) a predictive effort model developed for the FDD's software maintenance release process, (2) measurement-based lessons learned about the maintenance process in the FDD, (3) a set of lessons learned about the establishment of a measurement-based software maintenance improvement program. In addition, this study provides insights and guidelines for obtaining similar results in other maintenance organizations.
[Software maintenance, Costs, software systems, measurement-based software maintenance improvement program, Predictive models, software quality, functionality, Guidelines, cost estimation, Software measurement, NASA, software development management, Scheduling, software maintenance, predictive effort model, maintenance organization, quality, planning, software maintenance releases, maintenance management, Software systems, software cost estimation, Resource management, Software engineering, software metrics]
Configuration management with logical structures
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
When designing software, programmers usually think in terms of modules that are represented as functions and classes, but using existing configuration management systems, programmers have to deal with versions and configurations that are organized by files and directories. This is inconvenient and error-prone, since there is a gap between handling source code and managing configurations. We present a framework for programming environments that handles versions and configurations directly in terms of the functions and classes in source code. We show that with this framework, configuration management issues in software reuse and cooperative programming become easier. We also present a prototype environment that has been developed to verify our ideas.
[Software maintenance, functions, software design, classes, Control systems, logical structures, Environmental management, Software design, cooperative programming, directories, software reuse, Object oriented modeling, Buildings, source code, prototype environment, Programming profession, modules, Programming environments, Software development management, configuration management, Computer languages, versions, software reusability, files, computer aided software engineering, programming environments]
Forcing behavioral subtyping through specification inheritance
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
A common change to object-oriented software is to add a new type of data that is a subtype of some existing type in the program. However, due to message passing, unchanged pearls of the program may now call operations of the new type. To avoid reverification of unchanged code, such operations should have specifications that are related to the specifications of the appropriate operations in their supertypes. This paper presents a specification technique that uses inheritance of specifications to force the appropriate behavior on the subtype objects. This technique is simple, requires little effort by the specifier, and avoids reverification of unchanged code. We present two notions of such behavioral subtyping, one of which is new. We show how to use these techniques to specify examples in C++.
[Vocabulary, message passing, object-oriented programming, C++, Object oriented modeling, object-oriented software, inheritance, behavioral subtyping, Specification languages, History, formal specification, Computer science, Computer languages, Message passing, code reverification, specification inheritance]
A specification-based adaptive test case generation strategy for open operating system standards
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
The paper presents a specification based adaptive test case generation (SBATCG) method for integration testing in an open operating system standards environment. In the SBATCG method, templates describing abstract state transitions are derived from a model based specification, and the templates are refined to the internal structure of each implementation. We adopt the Z notation, one of the most widely used formal specification languages. We conducted mutation analysis to study the fault exposure abilities of the SBATCG method and that of a strategy based only on a specification. In our experiment, we used a Z version of the ITRON2 real time multi task operating system specification and two commercially available ITRON2 implementations. The results of this equipment show that the SBATCG method can achieve a higher fault detecting ability than can the strategy using only a specification.
[SBATCG method, System testing, Computer aided software engineering, program testing, open systems, ITRON2 real time multi task operating system specification, Genetic mutations, Standardization, model based specification, formal specification, formal specification languages, Information science, Operating systems, mutation analysis, specification languages, fault exposure abilities, abstract state transitions, specification based adaptive test case generation strategy, fault detecting ability, templates, Formal specifications, commercially available ITRON2 implementations, Phase detection, open operating system standards, internal structure, Fault detection, integration testing, real-time systems, Open systems, multiprogramming, operating systems (computers), Z notation, software standards]
System acquisition based on software product assessment
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
The procurement of complex software product involves many risks. To properly assess and manage those risks, Bell Canada has developed methods and tools that combine process capability assessment with a static analysis based software product assessment. This paper describes the software product assessment process that is part of our risk management approach. The process and the tools used to conduct a product assessment are described. The assessment is in part based on static source code metrics and inspections. A summary of the lessons learned since the initial implementation in 1993 is provided. Over 20 products totalling more than 100 million lines of code have gone through this process.
[Procurement, Real time systems, risk management, ISO standards, software selection, system acquisition, Inspection, inspections, software quality, Risk analysis, process capability assessment, static source code metrics, risk management approach, static analysis based software product assessment, Bell Canada, Risk management, complex software product procurement, Telecommunication services, Monitoring, Contracts, Board of Directors, software product assessment]
A flexible architecture for building data flow analyzers
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Data flow analysis is a versatile technique that can be used to address a variety of analysis problems. Typically, data flow analyzers are hand-crafted to solve a particular analysis problem. The cost of constructing analyzers can be high and is a barrier to evaluating alternative analyzer designs. We describe an architecture that facilitates the rapid prototyping of data flow analyzers. With this architecture, a developer chooses from a collection of pre-existing components or, using high-level component generators, constructs new components and combines them to produce a data flow analyzer. In addition to support for traditional data flow analysis problems, this architecture supports the development of analyzers for a class of combined data flow problems that offer increased precision. This architecture allows developers to investigate quickly and easily a wide variety of analyzer design alternatives and to understand the practical design tradeoffs better. We describe our experience using this architecture to construct a variety of different data flow analyzers.
[high-level component generators, Data analysis, program diagnostics, software prototyping, Buildings, Laboratories, design tradeoffs, rapid prototyping, Independent component analysis, data flow analysis, precision, Programming, Encoding, pre-existing components, combined data flow problems, Prototypes, Computer architecture, Cost function, data flow analyzer building, flexible architecture, Software reusability]
Checking subsystem safety properties in compositional reachability analysis
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
The software architecture of a distributed program can be represented by an hierarchical composition of subsystems, with interacting processes at the leaves of the hierarchy. Compositional reachability analysis has been proposed as a promising automated method to derive the overall behavior of a distributed program in stages, based on its architecture. The method is particularly suitable for the analysis of programs which are subject to evolutionary change. When a program evolves, only behavior of those subsystems affected by the change need be re-evaluated. The method however has a limitation. The properties available for analysis are constrained by the set of actions that remain globally observable. The properties of subsystems, may not be analyzed. We extend the method to check safety properties of subsystems which may contain actions that are not globally observable. These safety properties can still be checked in the framework of compositional reachability analysis. The extension is supported by augmenting finite-state machines with a special undefined state /spl pi/. The state is used to capture possible violation of the safety properties specified by software developers. The concepts are illustrated using a gas station system as a case study.
[distributed processing, software developers, Software safety, finite state machines, Distributed computing, Distributed processing, software architecture, Software architecture, compositional reachability analysis, safety, Computer architecture, program analysis, software engineering, distributed program, automated method, reachability analysis, program diagnostics, gas station system, undefined state, Application software, Reachability analysis, subsystem safety property checking, Diffusion tensor imaging, Computer science, evolutionary change, hierarchical subsystem composition, globally observable actions, Automata, safety property violation, interacting processes, finite-state machines]
Simplifying data integration: the design of the Desert software development environment
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
This paper describes the design and motivations behind the Desert environment. The Desert environment has been created to demonstrate that the facilities typically associated with expensive data integration can be provided inexpensively in an open framework. It uses three integration mechanisms: control integration, simple data integration based on fragments, and a common editor. It offers a variety of capabilities including hyperlinks and the ability to create virtual files containing only the portions of the software that are relevant to the task on hand. It does this in an open environment that is compatible with existing tools and programs. The environment currently consists of a set of support facilities including a context database, a fragment database, scanners, and a ToolTalk interface, as well as a preliminary set of programming tools including a context manager and extensions to FrameMaker to support program editing and insets for non-textual software artifacts.
[Costs, Programming, Environmental management, programming tools, Databases, control integration, context manager, software engineering, fragments, common editor, software tools, Gratings, FrameMaker, virtual files, Software development management, Computer science, ToolTalk interface, Desert software development environment, Software systems, Desert environment, Software tools, programming environments, data integration, hyperlinks, Software engineering]
OPSIS: a view mechanism for software processes which supports their evolution and reuse
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
The paper describes Opsis, a view mechanism applied to graph based process modelling languages of type Petri net. A view is a sub model which can be mechanistically constructed from another model by application of a perspective which: identifies all parts of the original model that are contained in the submodel; identifies and transforms all parts that constitute the interface to other sub models; adds new link relations to describe the behaviour of the sub model in interaction with the other sub models. Sub models are more easy to grasp and can be limited in scope to some well defined aspects of a global model, such as the view point ofa single role player. Composition of sub models is achieved through a merge operation on interface elements of sub models. The intended use of Opsis is: 1) process evolution-changes can be localised to certain views, which largely reduces the complexity of applying change; and 2) process reuse-libraries can contain reusable fragments of type view that can be combined using the composition operators.
[interface elements, complexity, Costs, OPSIS, Petri nets, Programming, Electronic switching systems, user interfaces, view mechanism, sub model, reusable fragments, software processes, Petri net, process reuse, Technological innovation, Automation, link relations, global model, process evolution, Software libraries, Inhibitors, type view, composition operators, Software quality, software reusability, Computer industry, graph based process modelling languages, simulation languages, merge operation, Software engineering]
Multilanguage interoperability in distributed systems. Experience report
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
The Q system provides interoperability support for multilingual, heterogeneous component-based software systems. Initial development of Q began in 1988, and was driven by the very pragmatic need for a communication mechanism between a client program written in Ada and a server written in C. The initial design was driven by language features present in C, but not in Ada, or vice-versa. In time our needs and aspirations grew and Q evolved to support other languages, such as C++, Lisp, Prolog, Java, and Tcl. As a result of pervasive usage by the Arcadia SDE research project, usage levels and modes of the Q system grew and so more emphasis was placed upon portability, reliability, and performance. In that context we identified specific ways in which programming language support systems can directly impede effective interoperability. This necessitated extensive changes to both our conceptual model and our implementation of the Q system. We also discovered the need to support modes of interoperability far more complex than the usual client-server. The continued evolution of Q has allowed the architecture of Arcadia software to become highly distributed and component-based, exploiting components written in a variety of languages. In addition to becoming an Arcadia project mainstay, and has also been made available to over 100 other sites, and it is currently in use in a variety of other projects. This paper summarizes key points that have been learned from this considerable base of experience.
[open systems, software reliability, multilanguage interoperability, reliability, C language, portability, Ada client program, programming language support systems, Distributed databases, Prototypes, Computer architecture, distributed systems, Hardware, Robustness, PROLOG, Lisp, Java, client-server systems, C++, multilingual heterogeneous component-based software systems, Prolog, Arcadia software architecture, Computer science, Computer languages, software portability, performance, object-oriented languages, Software systems, LISP, conceptual model, communication mechanism, Impedance, Tcl, Q system, C language server]
A new approach to consistency control in software engineering
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Quality assurance methods as suggested by standards like ISO 9000 focus on the principle of review and feedback loops, which may be implemented by computer-based software process management including life cycle models, version control, and change tracking. Provided that the software process is modelled independently of concrete design methods, development tools, and software representations, a general representation of quality assurance methods can be obtained. In our paper we introduce such a high-level formalism, heavily exploiting some remarkable analogy between the software development process and distributed computations. Our approach is based on labelling each software element and product version during development. By using these labels one can coordinate versions, variant designs, and reconstruct elements of old versions automatically. Though our model is independent of particular design methods or programming formalisms, it can be parameterized with tools and compilers in order to be tailored to specific projects. Some applications are demonstrated for important problems of software project management that cannot be solved or even detected with nowadays standard methods, but that can easily be dealt with by using our new model.
[Tracking loops, Design methodology, ISO standards, consistency control, software quality, high-level formalism, life cycle models, compilers, Feedback loop, computer-based software process management, Quality assurance, change tracking, Software standards, software engineering, software representations, software project management, Quality management, project management, software development management, version control, configuration management, standards, ISO 9000, quality assurance, Software quality, Software tools, Software engineering, software process]
Executable object modeling with statecharts
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
This paper reports on an effort to develop an integrated set of diagrammatic languages for modeling object-oriented systems, and to construct a supporting tool. The goal is for models to be intuitive and well-structured, yet fully executable and analyzable, enabling automatic synthesis of usable and efficient code in object-oriented languages such as C++. At the heart of the modeling method is the language of statecharts for specifying object behavior, and a hierarchical OMT-like language for describing the structure of classes and their inter-relationships, that we call O-charts. Objects can interact by event generation, or by direct invocation of operations. In the interest of keeping the exposition manageable, we leave out some technically involved topics, such as multiple-thread concurrency and active objects.
[Heart, statecharts, direct invocation, object-oriented programming, C++, Law, Object oriented modeling, integrated set, object-oriented systems, Yarn, active objects, Concurrent computing, executable object modeling, Computer languages, hierarchical OMT-like language, object-oriented languages, Concrete, diagrammatic languages, software tools, Time factors, Legal factors, Software engineering, O-charts, multiple-thread concurrency]
Slicing object-oriented software
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Describes the construction of system dependence graphs for object-oriented software on which efficient slicing algorithms can be applied. We construct these system dependence graphs for individual classes, groups of interacting classes and complete object-oriented programs. For an incomplete system consisting of a single class or a number of interacting classes, we construct a procedure dependence graph that simulates all possible calls to public methods in the class. For a complete system, we construct a procedure dependence graph from the main program in the system. Using these system dependence graphs, we show how to compute slices for individual classes, groups of interacting classes and complete programs. One advantage of our approach is that the system dependence graphs can be constructed incrementally because representations of classes can be reused. Another advantage of our approach is that slices can be computed for incomplete object-oriented programs such as classes or class libraries. We present our results for C++, but our techniques can be applied to other statically typed object-oriented languages such as Ada-95.
[Algorithm design and analysis, class representation reuse, diagrams, class libraries, software libraries, graphs, Ada-95, interacting classes, procedure call simulation, Libraries, Data flow computing, incomplete object-oriented programs, procedure dependence graph, Testing, efficient slicing algorithms, Data analysis, object-oriented programming, Computational modeling, Object oriented modeling, program diagnostics, Software algorithms, system dependence graphs, C++ language, statically typed object-oriented languages, Flow graphs, public methods, Computer science, software reusability, incremental construction, subroutines, object-oriented software slicing]
Assertion-oriented automated test data generation
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Assertions are recognized as a powerful tool for automatic run time detection of software errors. However, existing testing methods do not use assertions to generate test cases. We present a novel approach of automated test data generation in which assertions are used to generate test cases. In this approach the goal is to identify test cases on which an assertion is violated. If such a test is found then this test uncovers an error in the program. The problem of finding program input on which an assertion is violated may be reduced to the problem of finding program input on which a selected statement is executed. As a result, the existing methods of automated test data generation for white box testing may be used to generate tests to violate assertions. The experiments have shown that this approach may significantly improve the chances of finding software errors as compared to the existing methods of test generation.
[Software testing, automatic run time detection, automatic programming, Costs, Data analysis, program testing, program input, Debugging, Programming profession, Computer science, Runtime, Automatic testing, Computer bugs, white box testing, Computer errors, software engineering, software errors, assertion oriented automated test data generation, automated test data generation]
A case study in applying a systematic method for COTS selection
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
This paper describes a case study that used and evaluated key aspects of a method developed for systematic reusable off-the-shelf software selection. The paper presents a summary of the common problems in reusable off-the-shelf software selection, describes the method used and provides details about the case study carried out. The case study indicated that the evaluated aspects of the method are feasible, improve the quality and efficiency of reusable software selection and the decision makers have more confidence in the evaluation results, compared to traditional approaches. Furthermore, the case study also showed that the choice of evaluation data analysis method can influence the evaluation results.
[Computer aided software engineering, Data analysis, data analysis, Decision making, software selection, COTS selection, systematic reusable off-the-shelf software selection, Programming, Educational institutions, World Wide Web, decision makers, Information technology, Embedded software, quality, Computer science, software reusability, Software reusability]
A software engineering experiment in software component generation
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
The paper presents results of a software engineering experiment in which a new technology for constructing program generators from domain-specific specification languages has been compared with a reuse technology that employs sets of reusable Ada program templates. Both technologies were applied to a common problem domain, constructing message translation and validation modules for military command, control, communications and information systems (C/sup 3/I). The experiment employed four subjects to conduct trials of use of the two technologies on a common set of test examples. The experiment was conducted with personnel supplied and supervised by an independent contractor. Test cases consisted of message specifications taken from Air Force C/sup 3/I systems. The main results are that greater productivity was achieved and fewer error were introduced when subjects used the program generator than when they used Ada templates to implement software modules from sets of specifications. The differences in the average performance of the subjects are statistically significant at confidence levels exceeding 99 percent.
[System testing, program verification, software engineering experiment, Communication system control, Control systems, Air Force systems, Paper technology, human resource management, Personnel, formal specification, independent contractor, Information systems, message translation modules, productivity, command and control systems, specification languages, military command control communications and information systems, military computing, Military communication, automatic programming, software component generation, Automatic programming, average performance, reuse technology, Specification languages, domain-specific specification languages, software modules, message validation modules, confidence levels, program generator construction, program interpreters, reusable Ada program templates, message specifications, software reusability, personnel, Software engineering]
Independent on-line monitoring of evolving systems
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
We argue that the trustworthiness of evolving software systems can be significantly enhanced by a rigorous process of independent on-line monitoring. Such monitoring can prevent fraud, encourage careful maintenance, and serve as an early detector of irregularities in the state and behavior of a system. Unfortunately, there is a conflict between the concepts of on-line and independent monitoring. This conflict is due to the fact that on-line monitoring requires the embedding of some kinds of sensors in the base-system. But the introduction of such sensors requires a degree of cooperation with the developers of the base system, and may interfere with the operations of that system, contrary to the requirements of independent monitoring. We describe a way to resolve this conflict by applying the concept of law-governed architecture.
[Airplanes, System testing, systems software, Computerized monitoring, fraud prevention, Debugging, Sensor systems, software maintenance, Condition monitoring, Computer science, independent on-line monitoring, irregularity detection, sensors, Detectors, evolving software systems, Software systems, system monitoring, Protection, law-governed architecture]
Monitoring compliance of a software system with its high-level design models
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
As a complex software system evolves, its implementation tends to diverge from the intended or documented design models. Such undesirable deviation makes the system hard to understand, modify and maintain. This paper presents a hybrid computer-assisted approach for confirming that the implementation of a system maintains its expected design models and rules. Our approach closely integrates logic-based static analysis and dynamic visualization, providing multiple code views and perspectives. We show that the hybrid technique helps determine design-implementation congruence at various levels of abstraction: concrete rules like coding guidelines, architectural models like design patterns or connectors, and subjective design principles like low coupling and high cohesion. The utility of our approach has been demonstrated in the development of /spl mu/Choices, a new multimedia operating system which inherits many design decisions and guidelines learned from experience in the construction and maintenance of its predecessor, Choices.
[implementation divergence, Visualization, connectors, /spl mu/Choices, World Wide Web, abstraction levels, conformance testing, multimedia computing, Guidelines, subjective design principles, Operating systems, logic-based static analysis, design patterns, multimedia operating system, concrete rules, software engineering, documented design models, multiple code view, dynamic visualization, Computerized monitoring, Multimedia systems, high-level design models, complex software system evolution, monitoring, coding guidelines, architectural models, Computer science, low coupling, Utility programs, design-implementation congruence, software system compliance monitoring, hybrid computer-assisted approach, high cohesion, Software systems, operating systems (computers), Concrete]
How to identify binary relations for domain models
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Many approaches to requirements engineering include building a model of the domain. Those using entity relationship modeling or deriving from it employ the concept of relations between entities, but identifying the relations is still more of an art than science or engineering. We deal with this problem primarily in the context of object oriented analysis (OOA), where relations between object classes are to be identified. Our new approach uses natural language definitions of object classes and looks for names of other object classes in these definitions, since such a reference indicates a relation. Based on this idea, we identify most binary relations for domain models in a new way. We also provide tool support for this method, which shows that a high degree of automation is possible. Both a case study using the well known ATM (automated teller machine) example and real world experience with our approach suggest its usefulness.
[object oriented analysis, Art, domain models, Data engineering, natural language definitions, Electronic mail, formal specification, object classes, real world experience, Design engineering, Databases, object-oriented methods, OOA, Automation, automatic teller machines, Object oriented modeling, Natural languages, Buildings, abstract data types, automated teller machine, entity relationship modeling, binary relations, requirements engineering, natural languages, ATM, bank data processing, Context modeling]
The role of experimentation in software engineering: past, current, and future
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Software engineering needs to follow the model of other physical sciences and develop an experimental paradigm for the field. This paper proposes the approach towards developing an experimental component of such a paradigm. The approach is based upon a quality improvement paradigm that addresses the role of experimentation and process improvement in the content of industrial development. The paper outlines a classification scheme for characterizing such experiments.
[Encapsulation, Costs, classification scheme, Humans, Predictive models, process improvement, Computer science, Physics computing, Feedback, experimentation, quality improvement, Curing, software engineering, Problem-solving, experimental paradigm, Software engineering]
A systematic survey of CMM experience and results
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
The capability maturity model (CMM) for software has become very influential as a basis for software process improvement (SPI). Most of the evidence to date showing the results of these efforts has consisted of case studies. We present a systematic survey of organizations that have undertaken CMM-based SPI to get more representative results. We found evidence that process maturity is in fact associated with better organizational performance, and that software process appraisals are viewed, in retrospect, as extremely valuable and accurate guides for the improvement effort. The path was not always smooth, however, and efforts generally took longer and cost more than expected. A number of factors that distinguished highly successful from unsuccessful efforts are identified. Most of these factors are under management control, suggesting that a number of specific management decisions are likely to have a major impact on the success of the effort.
[software process appraisals, Costs, cost, management control, Project management, software development management, CMM experience, Scheduling, Appraisal, capability maturity model, case studies, process maturity, management decisions, organizational performance, Coordinate measuring machines, reviews, Standards organizations, software process improvement, survey, Software standards, software engineering, Capability maturity model, Monitoring, Software engineering]
Supporting the construction and evolution of component repositories
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Repositories must be designed to meet the evolving and dynamic needs of software development organizations. Current software repository methods rely heavily on classification, which exacerbates acquisition and evolution problems by requiring costly classification and domain analysis efforts before a repository can be used effectively. This paper outlines an approach in which minimal initial structure is used to effectively find relevant software components while methods are employed to incrementally improve repository structures. The approach is demonstrated through PEEL, a tool to semi-automatically identify reusable components, and CodeFinder, a retrieval system that compensates for the lack of explicit knowledge structures through spreading activation retrieval and allows component representations to be incrementally improved while users are searching for information. The combination of these techniques yields a flexible software repository that minimizes up-front costs and improves its retrieval effectiveness as developers use it to find reusable software artifacts.
[minimal initial structure, Costs, software development, software repository methods, Relational databases, component repositories, Programming, Information retrieval, domain analysis, classification, CodeFinder, Computer science, Design engineering, Software libraries, Investments, software reusability, PEEL, software engineering, retrieval system, Software reusability, reusable software artifacts, Indexing]
Using KIDS as a tool support for VDM
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
KIDS/VDM is an experimental environment that supports the synthesis of executable prototypes from VDM specifications. The development proceeds as a series of correctness preserving transformations under the strict control of the tool. A by-product of this development is the proof of consistency properties of the original specification. Experiments with the tool have shown its ability to handle independently written specifications. It also revealed useful to detect errors in specifications. The environment is based on, technologies of the Kestrel Institute Development System, including the REFINE and REGROUP languages, the design and optimization tactics, and the theorem prover.
[REGROUP, proof of consistency, program verification, Specification languages, Software safety, Formal specifications, Application software, formal specification, correctness preserving transformations, Certification, KIDS, Design optimization, tool support, executable prototypes synthesis, theorem prover, Refining, Prototypes, Production, specification languages, REFINE, Computer industry, VDM specifications, theorem proving, VDM]
A demand-driven analyzer for data flow testing at the integration level
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Data-flow testing relies on static analysis for computing the definition-use pairs that serve as the test case requirements for a program. When testing large programs, the individual procedures are first tested in isolation during unit testing. Integration testing is performed to specifically test the procedure interfaces. The procedures in a program are integrated and tested in several steps. Since each integration step requires data-flow analysis to determine the new test requirements, the accumulated cost of repeatedly analyzing a program can contribute considerably to the overhead of testing. Data-flow analysis is typically computed using an exhaustive approach or by using incremental data-flow updates. This paper presents a new and more efficient approach to data-flow integration testing that is based on demand-driven analysis. We developed and implemented a demand-driven analyzer and experimentally compared its performance during integration testing with the performance of (i) a traditional exhaustive analyzer, and (ii) an incremental analyzer. Our experiments show that demand-driven analysis is faster than exhaustive analysis by up to a factor of 25. The demand-driven analyzer also outperforms the incremental analyzer in 80% of the test programs by up to a factor of 5.
[Software testing, Performance evaluation, Optimizing compilers, program testing, unit testing, Cost benefit analysis, definition-use pairs, Data flow computing, Performance analysis, program procedure interfaces, test case requirements, incremental analyzer, Data analysis, overhead, data flow analysis, static analysis, data flow testing, Application software, large program testing, Computer science, exhaustive analyzer, performance, integration testing, demand-driven analyzer, Software engineering, incremental data-flow updates]
Effort estimation using analogy
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
The staff resources or effort required for a software project are notoriously difficult to estimate in advance. To date most work has focused upon algorithmic cost models such as COCOMO and Function Points. These can suffer from the disadvantage of the need to calibrate the model to each individual measurement environment coupled with very variable accuracy levels even after calibration. An alternative approach is to use analogy for estimation. We demonstrate that this method has considerable promise in that we show it to out perform traditional algorithmic methods for six different datasets. A disadvantage of estimation by analogy is that it requires a considerable amount of computation. The paper describes an automated environment known as ANGEL that supports the collection, storage and identification of the most analogous projects in order to estimate the effort for a new project. ANGEL is based upon the minimisation of Euclidean distance in n-dimensional space. The software is flexible and can deal with differing datasets both in terms of the number of observations (projects) and in the variables collected. Our analogy approach is evaluated with six distinct datasets drawn from a range of different environments and is found to outperform other methods. It is widely accepted that effective software effort estimation demands more than one technique. We have shown that estimating by analogy is a candidate technique and that with the aid of an automated environment is an eminently practical technique.
[Performance evaluation, analogous project identification, Project management, datasets, Environmental management, Storage automation, observations, Cost function, software tools, effort estimation, ANGEL automated environment, project management, variables, software development management, analogous project collection, Calibration, software project, n-dimensional space, Software development management, computation, analogy, Euclidean distance, Computer industry, software cost estimation, Resource management, Euclidean distance minimisation, analogous project storage]
A scaleable, automated process for year 2000 system correction
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
As the 21st century approaches, many computer programs will begin to fail. Applications that rely on dates of any kind may simply stop working or produce incorrect results. The year 2000 problem is a matter of business importance, not just software maintenance. Program failures arise from representing calendar dates (year, month, and day) in just 6 digits, a format that allows only 2 digits for the year. The Year 2000 problem is pervasive. It occurs in calculations, comparisons and other logic involving date-related processing including date-oriented sorting and date-indexed tables. The problem occurs in databases and files as well as in code. The paper discusses the design of an automated software tool for code and data Y2000 correction and shows how to use the tool in a large scale system correction process. The solution is discussed in the context of large, COBOL-based commercial systems. Nonetheless, our approach is fully generalizable to other languages and classes of applications.
[date-related processing, program testing, date-indexed tables, large scale system correction, Logic testing, large COBOL-based commercial systems, sorting, business importance, data structures, Large-scale systems, software tools, Business, calendar dates, databases, year 2000 system correction, computer program failure, Calendars, Maintenance, Application software, software maintenance, Sorting, automated software tool, code Y2000 correction, scaleable automated process, date-oriented sorting, data Y2000 correction, files, cod, logic, business data processing]
An empirical study of static call graph extractors
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Informally, a call graph represents calls between entities in a given program. The call graphs that compilers compute to determine the applicability of an optimization must typically be conservative: a call may be omitted only if it can never occur an any execution of the program. Numerous software engineering tools also extract call graphs, with the expectation that they will help software engineers increase their understanding of a program. The requirements placed on software engineering tools when computing call graphs are typically more related than for compilers. For example, some false negatives-calls that can in fact take place in some execution of the program, but which are omitted from the call graph-may be acceptable, depending on the understanding task at hand. In this paper we empirically show a consequence of this spectrum of requirements by comparing the C call graphs extracted from three software systems (mapmaker, mosaic, and gee) by five extraction tools (cflow, CIA, Field, mk-functmap, and rigiparse). A quantitative analysis of the call graphs extracted for each system shows considerable variation, a result that is counterintuitive to many experienced software engineers. A qualitative analysis of these results reveals a number of reasons for this variation: differing treatments of macros, function pointers, input formats, etc. We describe and discuss the study, sketch the design space, and discuss the impact of our study on practitioners, tool developers, and researchers.
[mapmaker, rigiparse, Optimizing compilers, graph theory, software systems, Humans, gee, understanding task, program compilers, compilers, false negatives, Design engineering, Field, Program processors, mk-functmap, program understanding, optimization, mosaic, CIA, software tools, quantitative analysis, software engineers, Scholarships, program diagnostics, static call graph extractors, C call graphs, software engineering tools, Computer science, cflow, qualitative analysis, Software systems, extraction tools, Software tools, Software engineering]
GRIDS-GRaph-based, integrated development of software: integrating different perspectives of software engineering
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
The paper presents a multi dimensional software engineering model, based on a formal graph specification. In contrast to other software engineering approaches, we concentrate on the integration of the "partial" models of software processes, system architectures and views onto the system into one consistent project framework, in order to enhance large scale software development. We first introduce the static part of the so called three dimensional model of software engineering (3DM), which meta models partial models and integrated project frameworks. We further describe the dynamic part of the 3DM, which defines the necessary actions to generate, manipulate and maintain the entities of the static part. Using the programmed graph rewriting system PROGRES gives us a powerful means to formally specify our conceptual model. We show how we apply PROGRES to formalize the 3DM, and present the prototype of a project supporting tool, generated from the formal specification of the static and dynamic parts of the 3DM.
[consistent project framework, GRIDS, graph theory, Metamodeling, Programming, World Wide Web, formal specification, project supporting tool, software processes, software engineering, software tools, large scale software development, partial models, system architectures, rewriting systems, project management, Object oriented modeling, programmed graph rewriting system, Formal specifications, three dimensional model of software engineering, Power system modeling, Computer science, graph based integrated development of software, multi dimensional software engineering model, Software systems, conceptual model, integrated project frameworks, 3DM, PROGRES, Manipulator dynamics, Software engineering, formal graph specification]
A reliability model combining representative and directed testing
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Directed testing methods, such as functional or structural testing, have been criticized for a lack of quantifiable results. Representative testing permits reliability modeling, which provides the desired quantification. Over time, however, representative testing becomes inherently less effective as a means of improving the actual quality of the software under test. A model is presented which permits representative and directed testing to be used in conjunction. Representative testing can be used early, when the rate of fault revelation is high. Later results from directed testing can be used to update the reliability estimates conventionally associated with representative methods. The key to this combination is shifting the observed random variable from interfailure time to a post-mortem analysis of the debugged faults, using order statistics to combine the observed failure rates of faults no matter how those faults were detected.
[Software testing, program debugging, program testing, failure rates, Genetic mutations, software reliability, fault detection, Predictive models, random variable, software quality, fault revelation, post-mortem debugged fault analysis, functional testing, Failure analysis, representative testing, order statistics, Statistical analysis, quantification, program diagnostics, random processes, reliability model, Time measurement, directed testing, Computer science, interfailure time, Fault detection, structural testing, Software quality, Random variables, reliability estimate updating, statistics]
DYNAMITE: DYNAMIc Task nEts for software process management
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Managing the software development and maintenance process has been identified as a great challenge for several years. Software processes are highly dynamic and can only rarely be planned completely in advance. Dynamic task nets take this into account. They are built and modified incrementally as a software process is executed. Dynamic task nets have been designed to solve important problems of process dynamics, deciding product-dependent structure evolution, feedback, and concurrent engineering. In order to describe editing and enactment (and their interaction) in a uniform way, task nets are formally defined by means of a programmed graph rewriting system.
[Software maintenance, Petri nets, Programming, software management, concurrent engineering, Concurrent computing, feedback, Feedback, dynamic task nets, software development, process dynamics, enactment, programmed graph rewriting system, Flow graphs, software maintenance, product-dependent structure evolution, Phase detection, Software development management, editing, software process management, planning, Councils, DYNAMITE, Concurrent engineering, incremental modification]
Experiences of software quality management using metrics through the life-cycle
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Many software quality metrics to objectively grasp software products and process have been proposed in the past decades. In actual projects, quality metrics has been widely applied to manage software quality. However, there are still several problems with providing effective feedback to intermediate software products and the software development process. We have proposed a software quality management using quality metrics which are easily and automatically measured. The purpose of this proposal is to establish a method for building in software quality by regularly measuring and reviewing. The paper outlines a model for building in software quality using quality metrics, and describes examples of its application to actual projects and its results. As the results, it was found that quality metrics can be used to detect and remove problems with process and products in each phase. Regular technical reviews using quality metrics and information on the change of the regularly measured results was also found to have a positive influence on the structure and module size of programs. Further, in the test phase, it was found that with the proposed model, the progress of corrective action could be quickly and accurately grasped.
[software products, software development process, program testing, life-cycle, Project management, Programming, software quality, Proposals, problem removal, corrective action, software quality metrics, software quality management, Feedback, test phase, Software measurement, Quality management, software development management, problem detection, Size measurement, Application software, Phase detection, regular technical reviews, Software quality, automatic measurement, programs, software metrics]
Reducing and estimating the cost of test coverage criteria
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Test coverage criteria define a set of entities of a program flowgraph and require that every entity is covered by some test. We first identify E/sub c/, the set of entities to be covered according to a criterion c, for a family of widely used test coverage criteria. We then present a method to derive a minimum set of entities, called a spanning set, such that a set of test paths covering the entities in this set covers every entity in E/sub c/. We provide a generalised algorithm, which is parametrized by the coverage criterion. We suggest several useful applications of spanning sets of entities to testing. In particular they help to reduce and to estimate the number of tests needed to satisfy test coverage criteria.
[Software testing, Performance evaluation, generalised algorithm, Costs, program testing, Application software, test coverage criteria cost reduction, Fault detection, minimum entity set, test paths, spanning set, flowcharting, test coverage criteria cost estimation, software cost estimation, program flowgraph entities, Contracts]
Industrial experience with design patterns
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
A design pattern is a particular prose form of recording design information such that designs which have worked well in the past can be applied again in similar situations in the future. The availability of a collection of design patterns can help both the experienced and the novice designer recognize situations in which design reuse could or should occur. We have found that design patterns: 1) provide an effective "shorthand" for communicating complex concepts effectively between designers, 2) can be used to record and encourage the reuse of "best practices\
[information sharing, system documentation, Documentation, software architecture documentation, Pattern recognition, History, design information recording, Pulp and paper industry, Software design, design reuse, design patterns, best practices, software reusability, industrial experience, Internet]
An analytic framework for specifying and analyzing imprecise requirements
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
There are at least three challenges with requirements analysis. First, it needs to bridge informal requirements, which are often vague and imprecise, to formal specification methods. Second, requirements often conflict with each other. Third, existing formal requirement specification methodologies are limited in supporting trade-off analysis between conflicting requirements and identifying the impact of a requirement change to the rest of the system. In this paper, an analytic framework is developed for the specification and analysis of imprecise requirements. In this framework, the elasticity of imprecise requirements is captured using fuzzy logic and the relationships between requirements are formally classified into four categories: conflicting, cooperative, mutually exclusive and irrelevant. This formal foundation facilitates the inference of relationships between requirements for detecting implicit conflicts, to assess the relative priorities of requirements for resolving conflicts, and to assess the effect of a requirement change.
[cooperative requirements, Elasticity, Programming, formal specification, imprecise requirements specification, conflict resolution, Cost function, requirement change, conflicting requirements, mutually exclusive requirements, irrelevant requirements, trade-off analysis, formal specification methods, Documentation, fuzzy logic, Formal specifications, Application software, informal requirements, Computer science, Bridges, Fuzzy logic, requirements analysis, implicit conflicts, relative priorities, systems analysis, Software systems]
Prototypes as assets, not toys. Why and how to extract knowledge from prototypes. (Experience report)
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Software prototypes are becoming more and more important, as computer applications invade new domains and as personal prototyping environments become more powerful. Although numerous approaches recommend their use, prototypes are sometimes treated like their developers' personal toys, and little effort is made to extract and share the experiences and knowledge that emerged as a by-product of building the prototype. In this paper, a strategy is proposed to extract crucial pieces of knowledge from a prototype and from its developer. The strategy is based on monitoring explanations that developers give, analyzing their structure, and feeding results back to support and to focus explanations. During this process, the prototype turns into the centerpiece of a hyperstructured information base, which can be used to convey concepts, implementation tricks and experiences. If organizations begin to view-and treat-prototypes as executable representations of knowledge, they can fully capitalize on the assets prototypes really are.
[Software prototyping, Spirals, software prototyping, Buildings, Programming, implementation tricks, Data mining, hyperstructured information base, experiences, Computer science, concepts, Prototypes, computer applications, Computer applications, system monitoring, Cognitive science, personal prototyping environments, executable knowledge representations, Monitoring, software prototypes, explanation monitoring, knowledge]
Designing and implementing COO: design process, architectural style, lessons learned
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
This paper reports on the design and implementation of a software development framework named COO (which stands for COOperation and COOrdination in the software process). Its design process is first detailed and justified. Then, the paper emphasizes its layered and subject-oriented architecture. Particularly, it is shown how this architectural style leads to a very flexible and powerful way of defining, integrating and combining services in a software development environment.
[Process design, Irrigation, project support environments, COO, layered subject-oriented architecture, Humans, implementation, Programming, Electronic mail, Guidelines, coordination, architectural style, software development framework, development systems, Computer architecture, software development environment, Writing, design process, computer aided software engineering, Software reusability, Testing, cooperation, software process]
Analytical and empirical evaluation of software reuse metrics
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
How much can be saved by using existing software components when developing new software systems? With the increasing adoption of reuse methods and technologies, this question becomes critical. However, directly tracking the actual cost savings due to reuse is difficult. A worthy goal would be to develop a method of measuring the savings indirectly by analyzing the code for reuse of components. The focus of the paper is to evaluate how well several published software reuse metrics measure the "time, money and quality" benefits of software reuse. We conduct this evaluation both analytically and empirically. On the analytic front, we introduce some properties that should arguably hold of any measure of "time, money and quality" benefit due to reuse. We assess several existing software reuse metrics using these properties. Empirically, we constructed a toolset (using GEN+S) to gather data on all published reuse metrics from CS+ code; then, using some productivity and quality data from "nearly replicated" student projects at the University of Maryland, we evaluate the relationship between the known metrics and the process data. Our empirical study sheds some light on the applicability of our different analytic properties, and has raised some practical issues to be addressed as we undertake broader study of reuse metrics in industrial projects.
[Software testing, Software maintenance, toolset, Costs, Industrial relations, cost savings, industrial projects, human resource management, software reuse metrics, software system development, empirical evaluation, USA Councils, quality data, student projects, software tools, Software measurement, software components, Productivity, savings measurement, Software quality, software reusability, Software systems, Computer industry, software cost estimation, analytical evaluation, productivity data, software metrics]
Engineering an 'Open' client/server-platform for a distributed Austrian Alpine road-pricing system in 240 days-case study and experience report
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
The paper describes engineering a system for a distributed Austrian Alpine road-pricing environment as well as the structure and organization of the software development. The client/server-based road-pricing system, handling on average 1.2 million vehicle transitions per month, had be to operational within a mere eight months after the start of the project. Current practical and industrial problems of client/server system strategies are discussed. Our main theses derived from the presented case study are: in current medium to large software engineering tasks there is a need for a) technical specialists for industrially identified project stress points (database, network, front-end) with experience in large projects, b) a project and process plan for a (very) short development time frame before production, and c) a dynamic production-oriented process model rather than a traditional linear process model.
[Programming, costing, open client/server-platform engineering, dynamic production-oriented process model, Automotive engineering, Network servers, planning (artificial intelligence), Databases, road vehicles, Road vehicles, technical specialists, Production, project plan, software engineering, client-server systems, software development, industrially identified project stress points, traffic engineering computing, process plan, Stress, short development time frame, systems analysis, distributed Austrian Alpine road-pricing environment, Systems engineering and theory, Computer industry, software engineering tasks, automated highways, vehicle transitions, Software engineering]
User interface prototyping-concepts, tools, and experience
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
In recent years the development of highly interactive software systems with graphical user interfaces has become increasingly common. The acceptance of such a system depends to a large degree on the quality of its user interface. Prototyping is an excellent means for generating ideas about how a user interface can be designed, and it helps to evaluate the quality of a solution at an early stage. We present the basic concepts behind user interface prototyping, a classification of tools supporting it and a case study of nine major industrial projects. Based on our analysis of these projects we present the following conclusions: prototyping is used more consciously than in recent years. No project applied a traditional life-cycle approach, which is one of the reasons why most of them were successful. Prototypes are increasingly used as a vehicle for developing and demonstrating visions of innovative systems.
[Software prototyping, user interface management systems, Terminology, software prototyping, graphical user interfaces, user interface prototyping, industrial projects, Programming, software quality, Application software, Vehicles, highly interactive software systems, tool classification, Machine vision, Prototypes, user interface quality, User interfaces, Software systems, software tools, Graphical user interfaces]
An evaluation of software test environment architectures
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Software test environments (STEs) provide a means of automating the test process and integrating testing tools to support required testing capabilities across the test process. Specifically, STEs may support test planning, test management, test measurement, test failure analysis, test development and test execution. The software architecture of an STE describes the allocation of the environment's functions to specific implementation structures. An STE's architecture can facilitate or impede modifications such as changes to processing algorithms, data representation or functionality. Performance and reusability are also subject to architecturally imposed constraints. Evaluation of an STE's architecture can provide insight into modifiability, extensibility, portability and reusability of the STE. This paper proposes a reference architecture for STEs. Its analytical value is demonstrated by using SAAM (Software Architectural Analysis Method) to compare three software test environments: PROTest II (PROLOG Test Environment, Version II), TAOS (Testing with Analysis and Oracle Support), and CITE (CONVEX Integrated Test Environment).
[Software testing, Performance evaluation, Costs, reference architecture, program testing, data representation, test management, PROLOG Test Environment, software test environment architectures, test failure analysis, Testing with Analysis and Oracle Support, CITE, reusability, functionality, test execution, processing algorithms, CONVEX Integrated Test Environment, Environmental management, portability, Version II, software architecture, Software Architectural Analysis Method, Software architecture, testing tools, test development, Computer architecture, modifiability, test planning, environment functions allocation, SAAM, software performance evaluation, test process automation, modifications, architecturally imposed constraints, PROTest II, implementation structures, extensibility, Computer science, software portability, performance, Automatic testing, TAOS, Software quality, software reusability, computer aided software engineering, Software tools, programming environments, test measurement]
An object-oriented implementation of B-ISDN signalling. 2. Extendability stands the test
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
The article discusses the extension to an existing object-oriented implementation for B-ISDN signalling. After a brief overview of the existing implementation it is shown where changes in the existing software were made to meet the new requirements (in particular, features for intelligent networks), what mechanisms were available or introduced to make this adaptation as easy as possible, and what experiences in software reuse resulted in this proceeding. The conclusions confirm the predictions concerning extendibility stated in an earlier article, giving reasons for emphasizing again the values and merits of the Call Model and an object-oriented approach.
[object-oriented programming, B-ISDN, software reuse, Object oriented modeling, software, Communication system control, telecommunication signalling, Predictive models, object-oriented implementation, asynchronous transfer mode, extendibility, telecommunication computing, Communication switching, Concurrent computing, Intelligent networks, B-ISDN signalling, Call Model, intelligent networks, software engineering, object-oriented methods, Communication networks, Testing, Asynchronous transfer mode]
Scene: using scenario diagrams and active text for illustrating object-oriented programs
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Scenario diagrams are a well-known notation for visualizing the message flow in object-oriented systems. Traditionally, they are used in the analysis and design phases of software development to prototype the expected behavior of a system. We show how they can be used in reverse for understanding and browsing existing software. We have implemented a tool called Scene (SCENario Environment) that automatically produces scenario diagrams for existing object-oriented systems. The tool makes extensive use of an active text framework providing the basis for various hypertext-like facilities. It allows the user to browse not only scenarios but also various kinds of associated documents, such as source code (method definitions and calls), class interfaces, class diagrams and call matrices.
[Visualization, scenario diagrams, Humans, class diagrams, hypermedia, hypertext-like facilities, Programming, systems design, method definitions, diagrams, Software design, Runtime, program understanding, data visualisation, software browsing, active text, object-oriented programs, call matrices, object-oriented methods, software tools, behavioural prototyping, Software prototyping, object-oriented programming, software development, Object oriented modeling, source code, reverse engineering, Application software, Scene, Computer science, Layout, systems analysis, class interfaces, illustration, subroutines, programming environments, message flow visualization]
System dynamics modeling of an inspection-based process
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
A dynamic simulation model of an inspection-based software lifecycle process has been developed to support quantitative process evaluation. The model serves to examine the effects of inspection practices on cost, scheduling and quality throughout the lifecycle. It uses system dynamics to model the interrelated flows of tasks, errors and personnel throughout different development phases and is calibrated to industrial data. If extends previous software project dynamics research by examining an inspection-based process with an original model, integrating it with a knowledge-based method for risk assessment and cost estimation, and using an alternative modeling platform. While specific enough to investigate inspection practices, it is sufficiently general to incorporate changes for other phenomena. It demonstrates the effects of performing inspections or not, the effectiveness of varied inspection policies, and the effects of other managerial policies such as manpower allocation. The results of testing indicate a valid model that can be used for process evaluation and project planning, and can serve as a framework for incorporating other dynamic process factors.
[risk assessment, Costs, inspection-based software lifecycle process, Programming, dynamic process factors, human resource management, industrial data, cost estimation, error flows, managerial policies, knowledge based systems, scheduling, calibration, inspection policies, Testing, inspection, knowledge-based method, risk management, Job shop scheduling, project management, personnel flows, development phases, quality control, software development management, quantitative process evaluation, Inspection, task flows, Dynamic scheduling, quality, process evaluation, software project dynamics, system dynamics modeling, manpower allocation, personnel, Error correction, software cost estimation, Risk management, Software tools, project planning, Software engineering]
The program understanding problem: analysis and a heuristic approach
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Program understanding is the process of making sense of a complex source code. This process has been considered as computationally difficult and conceptually complex. So far no formal complexity results have been presented, and conceptual models differ from one researcher to the next. We formally prove that program understanding is NP hard. Furthermore, we show that even a much simpler subproblem remains NP hard. However we do not despair by this result, but rather offer an attractive problem solving model for the program understanding problem. Our model is built on a framework for solving constraint satisfaction problems, or CSPs, which are known to have interesting heuristic solutions. Specifically, we can represent and heuristically address previous and new heuristic approaches to the program understanding problem with both existing and specially designed constraint propagation and search algorithms.
[Algorithm design and analysis, program understanding problem, reverse engineering, problem solving model, constraint propagation, search algorithms, conceptual models, CSPs, complex source code, formal complexity results, Computer science, heuristic solutions, Computer languages, constraint satisfaction problems, NP hard, Libraries, Problem-solving, constraint handling, Artificial intelligence, search problems, computational complexity, heuristic approach]
The design of whole-program analysis tools
Proceedings of IEEE 18th International Conference on Software Engineering
None
1996
Building efficient tools for understanding large software systems is difficult. Many existing program understanding tools build control flow and data flow representations of the program a priori, and therefore may require prohibitive space and time when analyzing large systems. Since much of these representations may be unused during an analysis, we construct representations on demand, not in advance. Furthermore, some representations, such as the abstract syntax tree, may be used infrequently during an analysis. We discard these representations and recompute them as needed, reducing the overall space required. Finally, we permit the user to selectively trade off time for precision and to customize the termination of these costly analyses in order to provide finer user control. We revised the traditional software architecture for compilers to provide these features without unnecessarily complicating the analyses themselves. These techniques have been successfully applied in the design of a program slicer for the Comprehensive Health Care System (CHCS), a million line hospital management system written in the MUMPS programming language.
[program understanding tools, Software maintenance, Optimizing compilers, Medical services, MUMPS programming language, Control systems, MUMPS, compilers, software architecture, large software systems understanding, control flow, data flow representations, hospital management system, Performance analysis, software tools, tree data structures, finer user control, Comprehensive Health Care System, program diagnostics, Buildings, abstract syntax tree, reverse engineering, medical information systems, program slicer, Programming profession, Computer science, Hospitals, whole program analysis tools, Software systems]
An Architecture for WWW-based Hypercode Environments
Proceedings of the
None
1997
false
[Computer science, Software libraries, Computer architecture, Permission, Programming, HTML, Web sites, Software tools, Software engineering, Lifting equipment]
Anywhere, Anytime Code Inspections: Using the Web to Remove Inspection Bottlenecks in Large-Scale Software Development
Proceedings of the
None
1997
false
[Costs, Inspection, Programming, Educational institutions, Delay, Computer science, Production, meetingless, Permission, Code inspections: web-based, Software, Large-scale systems, asynchronous; Natural occurring inspection experiment; Automated support for inspections]
Designing Distributed Applications with Mobile Code Paradigms
Proceedings of the
None
1997
false
[Broadband communication, Application software, distributed applications, Mobile code, Computer languages, Runtime, Web and internet services, Permission, Explosives, Computer networks, Large-scale systems, IP networks, design paradigms]
An Object-Oriented Modeling Method for Algebraic Specifications in CafeOBJ
Proceedings of the
None
1997
false
[Concurrent Rewriting, Object oriented modeling, Laboratories, Specification languages, CafeOBJ, Guidelines, Object-Oriented Modeling, Design engineering, Executable Specifications, Formal Methods, Collaboration, OBJ, Software quality, National electric code, Algebraic Specifications, Permission, Logic functions]
Formalizing and Integrating the Dynamic Model within OMT
Proceedings of the
None
1997
false
[Object oriented modeling, Formal specifications, formal specification, model integration, Computer science, Uniform resource locators, requirements analysis, Analytical models, Control charts, design, Permission, Functional programming, Erbium, Context modeling, Object-oriented modeling]
Introducing Formal Specification Methods in Industrial Practice
Proceedings of the
None
1997
false
[Computer languages, CASE tools, Computer aided software engineering, Data analysis, Costs, Failure analysis, Programming, Specification notations, Formal specifications, Application software, Formal methods, Kernel]
Choosing a Testing Method to Deliver Reliability
Proceedings of the
None
1997
false
[Software testing, System testing, Materials testing, statistical testing theory, Reliability theory, Probability, Software reliability, Computational Intelligence Society, Software debugging, Computer bugs, Permission, debugging, Reliability]
Re-estimation of Software Reliability After Maintenance
Proceedings of the
None
1997
false
[Software testing, Software maintenance, Materials testing, Costs, Statistical analysis, regression testing, software reliability, Permission, Predictive models, Software systems, Software reliability, Personnel]
A Study on the Failure Intensity of Different Software Faults
Proceedings of the
None
1997
false
[Software testing, software reliability growth model, Calendars, testing, hyperexponential SRGM, Predictive models, Programming, Software reliability, Littlewood model, Failure intensity, Delay, Information science, Fault detection, gamma distribution, Permission, Frequency]
An Empirical Study of Communication in Code Inspections
Proceedings of the
None
1997
false
[Productivity, process, Communication system control, Inspection, Educational institutions, empirical study, inspections, organizational structure, Computer science, Information processing, Permission, communication, Testing, Software engineering]
A Case Study of Distributed, Asynchronous Software Inspection
Proceedings of the
None
1997
false
[Groupware, Computer aided software engineering, Software Inspection, Collaborative software, Inspection, World Wide Web, Conducting materials, Videoconference, Collaboration, Permission, Collaborative work, Space exploration, CSCW, Time factors, Software tools, Concurrent Software Engineering]
Understanding the Effects of Developer Activities on Inspection Interval
Proceedings of the
None
1997
false
[Career development, queueing, Calendars, Inspection, Programming, Educational institutions, Regression analysis, empirical studies, Computer science, Software inspection, interval reduction, Production, Permission, Computer industry, statistical modeling]
A Meta-Model for Restructuring Stakeholder Requirements
Proceedings of the
None
1997
false
[Negotiation, Meta-modeling, Failure analysis, Metamodeling, Permission, Requirements/specification, Stakeholder conflict analysis, Information systems, Information analysis]
Early Specification of User-Interfaces: Toward a Formal Approach
Proceedings of the
None
1997
false
[Visualization, Buildings, Formal languages, Specification languages, Formal specifications, Design engineering, Requirement engineering, Prototypes, Formal specification, Permission, User-Interfaces, Mathematical model, Contracts]
Automated Analysis of Requirement Specifications
Proceedings of the
None
1997
false
[Space technology, Space missions, NASA, Natural languages, Project management, Software quality, Software performance, Risk management, Formal specifications, Software tools]
Integrating Support for Temporal Media into an Architecture for Graphical User Interfaces
Proceedings of the
None
1997
false
[Collaborative software, Multimedia programming, Application software, software architecture, Collaboration, Computer architecture, groupware, User interfaces, Streaming media, Collaborative work, Workstations, MVC, Graphical user interfaces, Clocks]
Manipulating Recovered Software Architecture Views
Proceedings of the
None
1997
false
[Software maintenance, Data analysis, Image analysis, Software architecture, Reverse engineering, Documentation, Computer architecture, Permission, Maintenance engineering, Software architectures, Software engineering]
Lessons on Converting Batch Systems to Support Interaction
Proceedings of the
None
1997
false
[Computer science, Concurrent computing, Interactive systems, Memory management, Humans, Computer architecture, Permission, Software systems, Systems engineering and theory, Delay]
Applying Design of Experiments to Software Testing
Proceedings of the
None
1997
false
[Software testing, US Department of Energy, Software maintenance, software testing, Laboratories, Pipelines, Design for experiments, History, Design of Experiments (DOE), Permission, Lab-on-a-chip, Sampling methods, partitioning]
A Theory of Probabilistic Functional Testing
Proceedings of the
None
1997
false
[Software testing, Performance evaluation, System testing, Software prototyping, probabilistic testing, software testing, reliability, Noise generators, partition testing, Formal specifications, formal specification, Noise level, functional testing, Prototypes, Permission, Hardware, random testing]
Analyzing Partially-Implemented Real-Time Systems
Proceedings of the
None
1997
false
[Real time systems, Graphical Interval Logic, System testing, Statistical analysis, Electronic equipment testing, Real-time, static analysis, temporal logic, concurrency, Computer science, Concurrent computing, hybrid systems, Permission, Timing, Error correction, Logic, Ada]
Constructing Multi-Formalism State-Space Analysis Tools: Using rules to specify dynamic semantics of models
Proceedings of the
None
1997
false
[Computational modeling, Computer simulation, Buildings, State-space analysis, multi-formalism analysis, Reachability analysis, heterogeneity, Information analysis, concurrency, Concurrent computing, Computer science, US Government, Permission]
Software Deviation Analysis
Proceedings of the
None
1997
false
[software safety, Automated highways, Programming, Control systems, Hazards, Software safety, Automobiles, Permission, hazard analysis, software deviation analysis, Software tools, Accidents, Software engineering]
A Predictive Metric Based on Discriminant Statistical Analysis
Proceedings of the
None
1997
false
[Software testing, Costs, Statistical analysis, Measurements, Metrics, Production, Software quality, Permission, Computer industry, Software systems, Electrical equipment industry, Experimentations, Software measurement, Statistical Analysis]
Communication Metrics for Software Development
Proceedings of the
None
1997
false
[Software testing, Context, Empirical software engineering, software development, Collaborative software, Communication system control, Programming, Equations, team-based projects, Design engineering, structural equations, Collaborative work, communication, Software tools, Software engineering]
Characterizing and Modeling the Cost of Rework in a Library of Reusable Software Components
Proceedings of the
None
1997
false
[Software maintenance, Costs, Software libraries, Software packages, Predictive models, Packaging, Programming, Educational institutions, Production facilities, Software reusability]
A New Software Project Simulator Based on Generalized Stochastic Petri-net
Proceedings of the
None
1997
false
[Costs, Stochastic processes, Project management, simulation, Human factors, Programming, Analytical models, Stochastic systems, Petri-net, Software quality, Permission, Testing, software process]
The Criticality of Modeling Formalisms in Software Design Method Comparison
Proceedings of the
None
1997
false
[Manufactured products, Laboratories, Humans, Base Framework, Process Formalism, Programming, Software Development Methodology, Computer science, Software Process, Software design, Comparison, Permission, Mechanical variables measurement, Contracts, Software engineering]
Specification of Software Controlling a Discrete-Continuous Environment
Proceedings of the
None
1997
false
[Analytical models, Embedded systems, Object oriented modeling, Embedded system, Differential equations, object-oriented specification, Automatic control, discrete-continuous systems, Boilers, Continuous time systems, Embedded software]
Automatic Checking of Instruction Specifications
Proceedings of the
None
1997
false
[Computer science, Application generators. Machine-code toolkit. Specification testing, Computer errors, Encoding, Decoding, Application software, Programming profession, Testing]
Lackwit: A Program Understanding Tool Based on Type Inference
Proceedings of the
None
1997
false
[Computer science, C, Prototypes, abstraction, Data structures, Inference algorithms, restructuring, representation]
Assessing Modular Structure of Legacy Code Based on Mathematical Concept Analysis
Proceedings of the
None
1997
false
[Modularization, Interference, Documentation, Reengineering, Automatic control, Concept Analysis, Aerodynamics, Entropy, Data mining, Software engineering]
Visualizing Interactions in Program Executions
Proceedings of the
None
1997
false
[Visualization, Object oriented modeling, Reverse engineering, Educational institutions, Data structures, reverse engineering, Information filtering, software visualization, program understanding, Production, object-oriented software engineering, Information filters, Software engineering]
Measuring Requirements Testing Experience Report
Proceedings of the
None
1997
false
[Software testing, System testing, Computer aided software engineering, Project management, Requirements, Quality assurance, Metrics, Space technology, Software quality, Software systems, Quality Assurance, CASE Tools, Software tools, Quality management, Testing]
Integrating Measurement with Improvement: An Action-Oriented Approach Experience Report
Proceedings of the
None
1997
false
[Productivity, cycle time, Costs, Medical services, Time measurement, Counting rules, software measurement definitions, software process improvement., Hospitals, productivity, Software quality, software development cost, Software measurement, software measurement goals, Software engineering]
Total Software Process Model Evolution in EPOS Experience Report
Proceedings of the
None
1997
false
[Productivity, Technology management, Process model evolution, Project management, Software quality, experience reuse and learning, Computer industry, categorization framework for process evolution, evolution pattern, empirical evolution data., Phase detection, Software engineering]
An Improved Process for the Development of PLC Software Experience Report
Proceedings of the
None
1997
false
[requirement engineering, Costs, reuse, Design methodology, Object oriented modeling, Process improvement, PLC, Software safety, Programmable control, Computer languages, IEC standards, design, Permission, Automatic control, Hardware]
An Investigation into Coupling Measures for C++
Proceedings of the
None
1997
false
[Computer languages, Phase measurement, Fault detection, Object oriented modeling, C++ programming language, Coupling on object-oriented design, Software quality, Predictive models, Software systems, Educational institutions, prediction model of fault-prone components, Software measurement]
Incremental Analysis of Side Effects for C Software Systems
Proceedings of the
None
1997
false
[Algorithm design and analysis, Computer science, Software testing, Data analysis, Debugging, Software systems, Educational institutions, Partitioning algorithms, Flow graphs, incremental analysis, Information analysis, Dataflow analysis]
Flow Insensitive C++ Pointers and Polymorphism Analysis and its application to slicing
Proceedings of the
None
1997
false
[slicing, C++, Reverse engineering, Debugging, polymorphism, Application software, reverse engineering., Programming profession, Computer languages, Points-to analysis, program understanding, Permission, User interfaces, Software systems, Software tools, Testing, flow analysis]
The Effect of Department Size on Developer Attitudes to Prototyping
Proceedings of the
None
1997
false
[Software prototyping, Information science, Prototypes, Process control, Programming, survey, Robustness, Prototyping, waterfall approach, Information systems]
Copyright in Shareware Software Distributed on the Internet - The Trumpet Winsock Case
Proceedings of the
None
1997
Since the 1980s most countries worldwide have introduced laws which extend copyright protection to computer software. In the first Australian case to consider copyright in a shareware computer program distributed on the Internet, the court held that the Internet service provider OzEmail had infringed Trumpet Software's copyright in Trumpet Winsock 2.OB by arranging for the program and a set of altered data files to be distributed with other software on diskette as a give-away inserted in copies of computer magazines. The implications of this case for software developers, distributors, and users are presented.
[Computer aided software engineering, Psychology, Internet Service Provider, Shareware, Intellectual Property, Distributed computing, Programming profession, Computer science, Copyright, Web and internet services, Employment, Intellectual property, Distribution, Copyright protection]
On the Economics of Mass-Marketed Software
Proceedings of the
None
1997
false
[Economic models, Costs, cost, Laboratories, Time to market, Predictive models, Programming, market size, quality, Computer science, price, Permission, Economic forecasting, Contracts, Software engineering]
Abstract Syntax from Concrete Syntax
Proceedings of the
None
1997
false
[domain-specific languages, object-oriented models, Object oriented modeling, Building materials, Reverse engineering, Abstract syntax, Formal languages, reverse engineering, Specification languages, concrete syntax, Domain specific languages, grammars, Permission, Concrete, program transformation, Contracts, Software engineering]
Open Implementation Design Guidelines
Proceedings of the
None
1997
false
[Computer science, open implementation, Software design, Costs, software reuse, software design, Software systems, Control systems, Software reusability]
Hooking into Object-Oriented Application Frameworks
Proceedings of the
None
1997
false
[Object-oriented application frameworks, software reuse, framework documentation, Software performance, Software quality, User interfaces, Collaborative work, Application software, Manufacturing systems]
Using Formal Methods to Reason about Architectural Standards
Proceedings of the
None
1997
false
[COM, Authoring systems, Multimedia systems, Documentation, empirical, OLE, ActiveX, Computer science, multimedia, Computer architecture, formal methods, integration, partial specification, Component Object Model, Software standards, Microsoft, Standards development, mediator, Testing, Software engineering, architecture]
Model-Checking of Real-Time Systems: A Telecommunications Application Experience Report
Proceedings of the
None
1997
false
[Real time systems, Concurrent computing, Switching systems, Telecommunications software, Software performance, Model checking, Real-time verification, Computer networks, Probabilistic verification., Application software, Formal methods, Distributed computing]
Specification-based Testing of Reactive Software: Tools and Experiments Experience Report
Proceedings of the
None
1997
false
[Software testing, System testing, Switching systems, Reactive Systems, Temporal Logic, Application software, Logic testing, Empirical Studies, Computer science, Automatic testing, Specification-based Testing, Telephony, Safety, Software tools]
Software Processes Are Software Too, Revisited: An Invited Talk on the Most Influential Paper of ICSE 9
Proceedings of the
None
1997
false
[Computer science, Productivity, Laboratories, Permission, Application software, Software tools, Contracts, Software engineering]
Process Modelling - Where Next
Proceedings of the
None
1997
false
[Process planning, feedback systems system dynamics, Educational institutions, process improvement, Application software, feedback, Coordinate measuring machines, process programming, Feedback, Permission, Computer industry, Process: modelling, Dynamic programming, Software engineering]
Leveraging a Large Banking Organization to Object Technology
Proceedings of the
None
1997
false
[Object oriented modeling, Roads, Banking, Relational databases, Business Objects, Knowledge transfer, Guidelines, Computer architecture, Frameworks, Permission, Concrete, Object Technology, Object oriented programming]
Tailoring OMT for an Industry Software Project
Proceedings of the
None
1997
false
[Process design, engineering concerns, Navigation, object-oriented analysis and design, Object oriented modeling, Design methodology, Process control, Educational institutions, Application software, Employee welfare, Computer industry, industrial applications, Software engineering]
Software Architecture Recovery of Embedded Software
Proceedings of the
None
1997
false
[Reverse engineering, Software Architecture, Documentation, Application software, Appropriate technology, Embedded software, Software architecture, Architecture Recovery, Re-engineering, Computer architecture, Signal processing, Software systems, Protection]
Integrating Forward and Reverse Object-Oriented Software Engineering
Proceedings of the
None
1997
false
[Railway engineering, Costs, Computer aided software engineering, forward engineering, Reverse engineering, Documentation, Companies, Switches, Control systems, reverse engineering, incremental process, iterative process, Design engineering, software process improvement, automated document generation, Software engineering]
The Windows 95 User Interface: Iterative Ddesign and Problem Tracking in Action
Proceedings of the
None
1997
false
[Process design, System testing, File systems, Scalability, Relational databases, User interfaces, Market research, Product design, Internet, Usability]
Prioritizing Software Requirements In An Industrial Setting
Proceedings of the
None
1997
false
[Software testing, System testing, Costs, Collaborative software, Requirements, Prototypes, Permission, Lead, Telephony, Computer industry, prioritizing, release planning, Software engineering]
Lessons Learned with the Systems Security Engineering Capability Maturity Model
Proceedings of the
None
1997
false
[Capability Maturity Model, process improvement, Appraisal, History, improvement, assurance, Coordinate measuring machines, security, Aggregates, Information security, Permission, Systems engineering and theory, SPICE, INFOWAR, Capability maturity model, National security, system engineering]
Bootstrap: Four Years of Assessment Experience
Proceedings of the
None
1997
false
[Coordinate measuring machines, Instruments, Feedback, Europe, Software quality, Permission, process improvement., Software standards, Software measurement, Software engineering, Quality management, Process maturity assessment]
Code Reviews Enhance Software Quality
Proceedings of the
None
1997
false
[Costs, Power system management, Code reviews, Project management, Control systems, software quality, Application software, Programming environments, Programmable control, Software quality, Automatic control, software engineering, Manufacturing automation]
Implementing Cleanroom Software Engineering into a Mature CMM-Based Software Organization
Proceedings of the
None
1997
false
[Employee welfare, Coordinate measuring machines, Technology management, Costs, CMM, Cleanroom Software Engineering, Instruments, Programming, Scheduling, Capability maturity model, Software engineering, Testing]
Redesigning the Systems Development Process
Proceedings of the
None
1997
false
[Object oriented modeling, benchmarking, object oriented, software reliability, Companies, Reuse, Programming, use case, Software development management, Manufacturing processes, Permission, Software systems, Concurrent engineering, Hardware, Bonding, process redesign]
Platforms for Software Execution: Databases vs. Operating Systems vs. Browsers
Proceedings of the
None
1997
false
[Java, databases, Programming, operating systems, market lead-time, World Wide Web, Application software, Sun, Computer science, software development strategy, Technology management, Databases, Operating systems, W WW browsers, Artificial intelligence]
Architecting Families of Software-Intensive Products
Proceedings of the
None
1997
false
[Cellular networks, Software architecture, Cellular phones, Computer architecture, Switches, Software performance, Permission, Telecommunication switching, Embedded software, Radio access networks]
Pragmatic Software Metrics for Iterative Development
Proceedings of the
None
1997
false
[Productivity, Software maintenance, Costs, process, Time measurement, Software development management, Software metrics, Software quality, Permission, Iterative development, metrics, Software measurement, Risk management]
An Overview of the State of the Art in Software Architecture
Proceedings of the
None
1997
false
[Software architecture, Buildings, Computer architecture, Permission, Software systems, Code standards, Standards development, Yarn, Pattern analysis, Software engineering]
Everything You NEED To Know About Collaboration and Collaboration Software
Proceedings of the
None
1997
false
[Computer science, Collaborative software, Collaborative tools, Bibliographies, Permission, Programming, Collaborative work, Software systems, Software engineering, Videos]
Verlticatlon of Concurrent Software with FLAVERS
Proceedings of the
None
1997
false
[Computer science, Data analysis, Automata, Permission, Hardware, Time factors, Information analysis]
Nitpick: A Tool For Interactive Design Analysis
Proceedings of the
None
1997
false
[Protocols, software design, Programming, Formal specifications, formal specification, System analysis and design, Computer science, Software design, Computer bugs, design checking, Permission, Writing, Safety]
Endeavors: A Process System Infrastructure
Proceedings of the
None
1997
false
[Visualization, Object oriented modeling, Communication system control, Programming, Flow graphs, Petroleum, Open, Computer science, Software architecture, Computer architecture, Permission, distributed process technology, architecture]
Argo: A Design Environment for Evolving Software Architectures
Proceedings of the
None
1997
false
[Process design, Decision making, Humans, Domain-onentecl design environments, software evolution, Computer science, software architecture, Software design, human cognitive needs, Software architecture, Feedback, Computer architecture, Permission, Contracts]
Automatic Monitoring of Software Requirements
Proceedings of the
None
1997
false
[Software maintenance, requirements, Computerized monitoring, Watches, robustness, assumptions, software evolution, Computer languages, Feathers, Runtime, Software design, Permission, Software systems, Robustness, expectations, maintenance, Monitoring]
Preventive Program Maintenance in Demeter/Java
Proceedings of the
None
1997
false
[Java, Unified modeling language, Educational institutions, Preventive maintenance, Programming profession, aspectoriented programming, software evolution, Computer science, Genetic programming, Permission, Writing, Robustness, separation of concerns]
Rigi: A Visualization Environment for Reverse Engineering
Proceedings of the
None
1997
false
[Visualization, Reverse engineering, reverse engineering, Ice, software visualization, Computer science, Graphics, Computer displays, Tree graphs, nested graphs, Fisheye views, Permission, Software systems, graph editor, Software tools]
An Object-Oriented Testing and Maintenance Environment
Proceedings of the
None
1997
false
[Computer science, System testing, Visualization, Computer displays, Costs, Object oriented modeling, Computational modeling, Permission, Marine vehicles, Power engineering and energy]
The SCR Method for Formally Specifying, Verifying, and Validating Requirements: Tool Support
Proceedings of the
None
1997
false
[Real time systems, Thyristors, Costs, Embedded system, Laboratories, Control systems, Frequency, Robustness, Error correction, Accidents]
GRAIL/KAOS: An Environment for Goal-Driven Requirements Engineering
Proceedings of the
None
1997
false
[Navigation, Taxonomy, Layout, Transportation, Ontologies, Permission, Specification languages, Logic, Kernel]
ADE -An Architecture Design Environment for Component-Based Software Engineering
Proceedings of the
None
1997
false
[components, Scalability, Specification languages, Application software, Middleware, Design engineering, Computer architecture, Permission, formal analysis, Software standards, Assembly, Software engineering, Architectures, middleware]
Package-Oriented Programing of Engineering Tools
Proceedings of the
None
1997
false
[Costs, reuse, mediators, Buildings, package-oriented programming, Computer science, Software packages, COTS, Computer architecture, wrappers, Packaging, integration, software engineering, Large-scale systems, Software tools, Software engineering, Testing, architecture]
Developing Graphical (Software Engineering) Tools with PROGRES
Proceedings of the
None
1997
false
[prototyping, Data structures, Electronic switching systems, Programming environments, Computer science, Computer languages, Prototypes, Data models, Hardware, Software tools, visual programming, Software engineering, graph rewriting systems]
TINA ACE: an environment for Specifying, Developing and Generating TINA Services
Proceedings of the
None
1997
false
[Computer aided software engineering, Computational modeling, Telecommunication standards, Phase detection, Information analysis, CASE, TINA, CORBA, Specification Validation, Behaviour Modelling, Production, Skeleton, Logic, Standards development, Telecommunication services]
Software Process Improvement: Methods and Lessons Learned
Proceedings of the
None
1997
false
[Productivity, Coordinate measuring machines, Job shop scheduling, Investments, Software quality, Programming, Permission, Capability maturity model, Aircraft, Software engineering]
A Realistic, Commercially Robust Process for the Development of Object-Oriented Software Systems
Proceedings of the
None
1997
false
[process, Object oriented modeling, method, frameworks, patterns, Programming, Educational institutions, object-oriented, domain analysis, Application software, case study, Quality assurance, design, Permission, Software systems, Robustness, Iterative methods, Testing]
Software-Reliability-Engineered Testing Practice
Proceedings of the
None
1997
false
[Software testing, Productivity, System testing, Costs, Operational profile, Reliability engineering, Control systems, failure intensity, Failure analysis, failure, Materials reliability, Permission, reliability objective, software-based system, Power system reliability]
Tutorial Java: A Language for Software Engineering
Proceedings of the
None
1997
Provides an abstract for each of the presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[]
Software Interoperability: Principles and Practice
Proceedings of the
None
1997
false
[Java, polylingual, C++, Taxonomy, Programming, OLE, Interoperability, Computer science, CORBA, Network servers, Computer architecture, Permission, Solids, Software systems, Lifting equipment]
Distributed Software Architectures
Proceedings of the
None
1997
false
[Process design, Software testing, Software maintenance, component composition, Software architecture, Computer architecture, Permission, Software systems, Educational institutions, architecture description language, Architecture description languages, Distributed computing]
Effective Use of COTS (Commercial-Off-the-Shelf) Software Components in Long Lived Systems
Proceedings of the
None
1997
false
[Costs, Object oriented databases, Laboratories, Programming, Algorithms, Councils, COTS, Computer architecture, Software systems, Libraries, software components, Software engineering, software process]
Rigorous Requirements for Real-Time Systems: Evolution and Application of the SCR Method
Proceedings of the
None
1997
false
[Real time systems, Thyristors, Costs, requirements, Programming, specification, Application software, Formal specifications, formal specification, consistency checking, Defense industry, real-time, Technology transfer, formal methods, Computer industry, Military aircraft, verification, validation]
Software and Business Process Technology
Proceedings of the
None
1997
false
[Environmental management, Sun, business process engineering, process tools, Operating systems, Insurance, Permission, Broadcasting, Computer industry, Software tools, Business, Context modeling, software process]
An Introduction to OMG/CORBA
Proceedings of the
None
1997
false
[Dynamic Invocation Interface, CORBA Services, Object oriented modeling, Interface Repository, CORBA Facilities, Application software, Middleware, Distributed computing, Interoperability, Computer science, CORBA, IDL, Domain Interfaces, Computer languages, Technology transfer, Permission, Software standards, Hardware]
The Experience Factory: How to Build and Run One
Proceedings of the
None
1997
false
[Costs, software experiments, NASA, Process improvement, Production facilities, Time measurement, measurement, Software packages, Experience Factory, Software quality, Packaging, Permission, Software measurement, Software engineering, Quality Improvement Paradigm]
The Personal Software Process (PSP) A Full-day Tutorial
Proceedings of the
None
1997
false
[Productivity, Software testing, Tutorial, process, data, testing, Maintenance engineering, process improvement, discipline, PSP, measurement, TSP, Engineering management, productivity, Software quality, defects, yield, Computer industry, Software measurement, Capability maturity model, personal process, PROBE, estimating, Software engineering]
Making Requirements Measurable
Proceedings of the
None
1997
false
[requirements engineering, Documentation, Permission, Particle measurements, Educational institutions, Systems engineering and theory, specification, testability, validation, verification, Testing, Context modeling]
Defining Families: The Commonality Analysis
Proceedings of the
None
1997
false
[Production systems, reuse, domain engineering, Buildings, Programming, domain analysis, families, requirements engineering, Permission, Computer industry, Systems engineering and theory, Software systems, software engineering, Performance analysis, Assembly, application-oriented languages, Software engineering, software process]
Tutorial: Evaluating Softiare Technology
Proceedings of the
None
1997
Provides an abstract for each of the presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
[Tutorial, Software maintenance, surveys., Decision making, experiments, Design for experiments, empirical research, case studies, Design engineering, assessment, Metrics, Permission, Software standards, software engineering, Software measurement, Software tools, Software engineering]
A Survey of Object Oriented Analysis and Design Methods
Proceedings of the
None
1997
false
[Design methodology, Unified modeling language, Spine, Object Oriented, Jacobian matrices, Cyclic redundancy check, Design, Software design, UML, Analysis, Packaging, Software systems, Pattern analysis, Patterns, Contracts]
Simplifying the Evolution of Java Programs
Proceedings of the
None
1997
false
[Computer science, Bridges, Java, Object oriented modeling, Unified modeling language, Permission, Educational institutions, Pattern recognition, Programming profession, Context modeling]
A Primer on Empirical Studies
Proceedings of the
None
1997
false
[In vivo, Costs, Laboratories, Collaboration, Permission, Educational institutions, Anatomy, Logic, Statistics, In vitro]
Reverse Engineering Strategies for Software Migration
Proceedings of the
None
1997
false
[Computer interfaces, Software maintenance, Switching systems, Reverse engineering, software migration, Electronic mail, software reengineering, Information systems, software evolution, Computer science, legacy systems, Permission, Software systems, Computer networks]
A Software Process Improvement Approach Tailored for Small Organizations and SmaU Projects
Proceedings of the
None
1997
false
[Couplings, Coordinate measuring machines, Investments, Project management, Time to market, Permission, Concrete, Best practices]
Design Patterns for Object-Oriented Software Development
Proceedings of the
None
1997
false
[Java, Design patterns, frameworks, Programming, Vehicles, Automotive engineering, Design engineering, Computer languages, software architecture, Software design, Software architecture, object-oriented design, Permission, Software reusability]
Formal Methods for Broadband and Multimedia Systems
Proceedings of the
None
1997
false
[Protocols, Multimedia systems, Quality of service, Broadband Communication, Specification, Broadband communication, Application software, Formal Description Techniques, Delay, Implementation, Quality of Service, Distributed Multimedia Systems, TCPIP, EsLelle, Streaming media, Permission, SDL, Real Time, Logic]
Software Engineering Data Analysis Techniques
Proceedings of the
None
1997
false
[Data analysis, Statistical analysis, Databases, Software metrics, Neural networks, Stochastic processes, Regression Modeling; Software Reliability; NHPP; Classification Trees; Neural Networks; Radial Basis Functions; Data Mining; KDD, Software Metrics; Statistical analysis, Data mining, Regression tree analysis, Software engineering, Classification tree analysis]
Comprehension and Evolution of Legacy Software
Proceedings of the
None
1997
false
[Software maintenance, change propagation, Documentation, Inspection, ripple effect, Programming profession, Computer science, incremental reengineering, redocumentation, Investments, Permission, Understanding, Software systems, Software tools, Software engineering]
European And American Software Process Maturity Models And Assessments
Proceedings of the
None
1997
false
[Capability Maturity Model (CMM), Companies, Financial management, Electronic mail, 1S0 9000, Certification, Seminars, Coordinate measuring machines, Software Process Improvemen~ Software Quality, Layout, Software quality, Sotlsvare Process Improvement Models, SPICE, Bootstrap, Capability maturity model]
The Second ISEW Cleanroom Workshop
Proceedings of the
None
1997
false
[Formal Specification, Software Reliability Certification, Cleanroom Software Engineering, Verification, Software Process Improvement, Technology Transfer, Stepwise Refinement, Statistical Usage Testing]
Summary: Process Modelling and Empirical Studies of Software Evolution
Proceedings of the
None
1997
false
[Process modelling, Object oriented modeling, Area measurement, Empirical studies, Design for experiments, Computer science, Metrics, Software metrics, Systems' Evolution, Permission, Collaborative work, Large-scale systems, Software engineering]
Software Engineering (on) the World Wide Web
Proceedings of the
None
1997
false
[Java, Discussion forums, Software architecture, Permission, Lakes, Software systems, Web sites, Web server, Software engineering, Information systems]
Fourth International Workshop on Software Engineering Education (IWSEE4)
Proceedings of the
None
1997
false
[]
Workshop on Softiare Engineering for Parallel and Distributed Systems
Proceedings of the
None
1997
false
[]
ICSE 97 Doctoral Consortium
Proceedings of the
None
1997
false
[]
Reuse Library Interoperability and The World Wide Web
Proceedings of the
None
1997
false
[Software libraries, Government, Europe, Permission, Writing, World Wide Web, Software standards, Explosives, Internet, Web sites]
Reuse of Off-the-Shelf Components in C2-Style Architectures
Proceedings of the
None
1997
false
[messagebased architectures, software reuse, Application software, Computer science, Graphics, Software design, graphkal user interfaces (GUI), architectural styles, Computer architecture, Software quality, Web sites, Graphical user interfaces, component-based development]
Configuring Designs for Reuse
Proceedings of the
None
1997
false
[Computer languages, Design and implementation reuse, configuration of designs, Program processors, Design methodology, product families, User interfaces, Manufacturing, Cultural differences, Radio access networks]
Restructuring OODesigner: a CASE tool for OMT
Proceedings of the 20th International Conference on Software Engineering
None
1998
This report describes our experience acquired when we restructured OODesigner, a computer aided software engineering (CASE) tool for object modeling techniques (OMT). We had developed the version 1.x of OODesigner during 3 years since 1994. Although we had developed this version using OMT and C++, we recognized the potential maintenance problem that originated from the ill-designed class architecture. Thus we totally restructured the old version of OODesigner during 12 months, and obtained a new version that is much easier to maintain than the old one.
[maintenance problem, Computer aided software engineering, class architecture, process goals, Reverse engineering, OODesigner, Tree graphs, Computer architecture, object modeling technique, Robustness, functional requirements, information repository, software tools, software restructuring, object-oriented programming, C++, Object oriented modeling, Documentation, Maintenance engineering, reverse engineering, systems re-engineering, code generation, OMT, Systems engineering and theory, computer aided software engineering, CASE tool, Software engineering, product goals]
How does 3-D visualization work in software engineering?: empirical study of a 3-D version/module visualization system
Proceedings of the 20th International Conference on Software Engineering
None
1998
Version control and module management are very important in practical software development. In UNIX, RCS or SCCS is used in general as version control tools. They, however, have a couple of drawbacks. This paper proposed a solution for these issues by applying 3D visualization. The prototype system, VRCS, was developed. In our system, version information stored in the RCS history file is displayed as a 2D tree by taking the z-axis as time. Other 2D trees are laid out in 3D space in the same way. In our visualization, files which compose a certain release of the software are connected by a line called a relation link. By using GUIs, users can check in/out each version easily and interactively. More importantly, just by choosing the relation link, a certain release is rebuilt automatically. Three comparative experiments between VRCS and RCS were conducted to know the effectiveness of VRCS. The result shows that VRCS is faster in checking in/out than RCS.
[Visualization, Computer aided software engineering, 3D visualization, RCS, graphical user interfaces, VRCS, relation link, experiments, Programming, Control systems, History, Engineering management, Prototypes, data visualisation, 2D tree, software engineering, GUI, three dimensional visualization, software development management, version control, SCCS, graphical user interface, reverse engineering, Software development management, UNIX, configuration management, prototype system, module visualization system, module management, Software systems, visual programming, Software engineering]
Establishing experience factories at Daimler-Benz an experience report
Proceedings of the 20th International Conference on Software Engineering
None
1998
The experience factory concept enables systematic learning and continuous improvement in software development. As with most learning initiatives, it is hard to establish. In our experience, there is a great deal of uncertainty and skepticism about the mission and contents of an experience factory. The starting phase is especially endangered through pitfalls or unexpected delays. As expectations vary and there is pressure to demonstrate success within only a few months, tension arises which may jeopardize the entire enterprise. In the course of a large-scale software improvement program, we have established three experience factories in different environments of the Daimler-Benz AG within two years. At each site, several application projects are involved. We describe how we approached the task, what actions we took, and the lessons we learned.
[application projects, Uncertainty, Programming, Production facilities, Airbus airplanes, Delay, Embedded software, guarantee management, continuous improvement, control systems, Large-scale systems, systematic learning, organizational learning, software development, software improvement program, software development management, reconnaissance systems, Application software, space systems, Daimler-Benz, management of change, Concrete, Continuous improvement, experience factories, Software engineering, Mercedes cars]
Architectural modeling in industry-an experience report
Proceedings of the 20th International Conference on Software Engineering
None
1998
Nokia Research Center (NRC) has been leading Esprit project ARES (Architectural Reasoning for Embedded Systems) for two years. One of the main purposes of ARES is to demonstrate the use of architectural modeling in controlling properties of industrial scale products and product families. Two examples are documented. Forward architecting was applied in modeling the new architecture for our mobile phone family showing how to control the dynamic properties of the software. Reverse architecting was used to tune the performance of our telephone switch software. Both cases show that special purpose models can be constructed even for very large systems and they do give valuable insight to the properties of the software.
[ARES, Industrial control, Communication system control, Switches, Predictive models, Mobile handsets, telecommunication computing, Nokia Research Center, Software architecture, Embedded system, product families, Computer architecture, industrial scale products, Electrical equipment industry, software engineering, forward architecting, software performance evaluation, software architectural modeling, Architectural Reasoning for Embedded Systems, queueing networks, colored Petri net, research initiatives, reverse engineering, experience report, reverse architecting, telephony, mobile phone, Software systems, software performance, Esprit project, telephone switch software]
A worldwide survey of base process activities towards software engineering process excellence
Proceedings of the 20th International Conference on Software Engineering
None
1998
A survey has been designed to seek the practical foundation of base process activities (BPAs) in the software industry and to support research in modelling the software engineering processes. A superset of BPAs compatible with the current software process models, such as SPICE (ISO 15504), CMM, ISO 9000 and BOOTSTRAP, were identified for the construction of the questionnaires. This paper reports the survey findings on BPAs in software engineering processes. A summary of the current software engineering process techniques and practices modelled by 83 BPAs in 10 processes and three categories is given. Each BPA is benchmarked on attributes of mean importance and ratios of significance, practice and effectiveness. Based on the benchmarks, and by comparing with the current practice of the reader's organization, recommendations can be given on which specific areas need to have processes established first, and which areas should be highest priority for process improvement.
[In vitro fertilization, Software testing, System testing, CMM, software engineering process, software development management, Programming, process improvement, Information systems, Coordinate measuring machines, software industry, BOOTSTRAP, ISO 9000, Benchmark testing, SPICE, Systems engineering and theory, software engineering, base process activities, software process model, Software engineering]
Consistency management for complex applications
Proceedings of the 20th International Conference on Software Engineering
None
1998
Consistency management is important in many complex applications, but current languages and database systems inadequately support it. To address this limitation, we defined a consistency management model and incorporated it into the PLEIADES object management system. This paper illustrates some typical consistency management requirements and discusses the requirements in terms of both functionality and cross-cutting concerns that affect how this functionality is provided. It then describes the model and some design and implementation issues that arose in instantiating it. Finally, we discuss user feedback and future research plans.
[object-oriented programming, consistency management model, Laboratories, object-oriented databases, Project management, cross-cutting, functionality, inconsistency management, object management, Application software, consistency management, Environmental management, Engineering management, PLEIADES object management system, Feedback, Database systems, software engineering, Contracts, Software engineering, Testing]
Software architecture recovery of a program family
Proceedings of the 20th International Conference on Software Engineering
None
1998
The concept of software architecture has gained a lot of attention and has found its way into the software development process in industry. Software architecture recovery focuses on the recovery of architectural information from existing systems. This paper presents a framework for recovering the software architecture of a program family. Based on the available system information architectural properties such as safety or system control are recovered using different reverse engineering methods and tools in combination with architectural descriptions. We describe our architecture recovery process and discuss the recovery of system structure as one example of the case study. The framework was developed while working on the recovery of the family architecture of a train control system.
[Knowledge engineering, software development process, Industrial control, rail traffic, Programming, Control systems, reverse engineering, Information technology, program family, case study, Design engineering, Software architecture, traffic control, software architecture recovery, safety, Computer architecture, Software systems, Electrical equipment industry, software engineering, train control system, software tools]
Software size measurement and productivity rating in a large-scale software development department
Proceedings of the 20th International Conference on Software Engineering
None
1998
Some current object-oriented analysis methods provide use cases. Scenarios or similar concepts to describe functional requirements for software systems. The authors introduced the use case point method to measure the size of large-scale software systems based on such requirements specifications. With the measured size and the measured effort the real productivity can be calculated in terms of delivered functionality. In the status report they summarize the experiences made with size metrics and productivity rates at a major Swiss banking institute. They analyzed the quality of requirements documents and the measured use case points in order to test and calibrate the use case point method. Experiences are based on empirical data of a productivity benchmark of 23 measured projects (quantitative analysis), 64 evaluated questionnaires of project members and 11 post benchmark interviews held with selected project managers (qualitative analysis).
[software systems, Programming, object-oriented analysis methods, scenarios, post benchmark interviews, formal specification, large-scale software development department, software size measurement, requirements specifications, Benchmark testing, Software standards, functional requirements, Large-scale systems, object-oriented methods, Software measurement, software productivity rating, size metrics, calibration, Productivity, project management, Object oriented modeling, requirements document quality, Banking, testing, Size measurement, project members, Swiss banking institute, productivity benchmark, empirical data, bank data processing, use case point method, Software engineering, software metrics]
Process improvement towards ISO 9001 certification in a small software organization
Proceedings of the 20th International Conference on Software Engineering
None
1998
Software process improvement in small organizations is a challenging task where the "smallness" brings a number of unique problems. We report the status of our work on creating ISO 9001 compliant quality systems in a small software organization.
[Productivity, Costs, ISO standards, software development management, Programming, software quality, Certification, certification, small software organization, Coordinate measuring machines, ISO 9001, Investments, Software quality, software process improvement, software houses, Capability maturity model, Artificial intelligence, software standards]
Helping the automated validation process of user interfaces systems
Proceedings of the 20th International Conference on Software Engineering
None
1998
This paper describes the prototype of a software environment that was devised for helping the formal validation of user interfaces systems. The paper suggests an approach to include such formal operations in the design process. An abstract and formal representation of the user interface system is produced to perform formal verifications on it. The paper explains why the user interface system can be modelled properly by a dataflow system and how this model can be expressed by using equations of flows in the language Lustre. It describes then some main tools of the environment.
[Process design, Software prototyping, System testing, user interface management systems, program verification, Formal languages, user interfaces systems, Lustre, user interfaces, Application software, formal validation, automated validation, Equations, Human computer interaction, formal verification, dataflow system, Prototypes, data flow computing, User interfaces, Formal verification]
An investigation on the use of machine learned models for estimating correction costs
Proceedings of the 20th International Conference on Software Engineering
None
1998
We present the results of an empirical study in which we have investigated machine learning (ML) algorithms with regard to their capabilities to accurately assess the correctability of faulty software components. Three different families of algorithms have been analyzed. We have used (1) fault data collected on corrective maintenance activities for the Generalized Support Software reuse asset library located at the Flight Dynamics Division of NASA's GSFC and (2) product measures extracted directly from the faulty components of this library.
[Software maintenance, Costs, Machine learning algorithms, reuse asset library, Predictive models, software libraries, predictive software quality model building, Generalized Support Software, learning (artificial intelligence), machine learning algorithms, correction costs estimation, Flight Dynamics Division, Software algorithms, NASA, software development management, Educational institutions, empirical study, software maintenance, corrective maintenance activities, coding guidelines, Software libraries, correctability, Software quality, Machine learning, fault data, software reusability, Error correction, software cost estimation, product measures, faulty software components]
Towards better software projects and contracts: commitment specifications in software development projects
Proceedings of the 20th International Conference on Software Engineering
None
1998
Any software development project requires commitments from its participants. These commitments can include money, resources, deadlines, and specified functionality for the end product. The authors have developed a framework and a set of guidelines to support the specification of such commitments. They have evaluated the framework empirically in a series of case studies. The studies indicated that the framework addresses commitment specification issues that are normally not covered in project contracts and that the specification framework was considered beneficial by project representatives.
[risk management, software contracts, project management, Project management, software development management, Programming, software development project, resources, Scheduling, contracts, formal specification, commitment specifications, deadlines, Guidelines, Intelligent networks, Engineering management, money, Feedback, Information processing, project representatives, specified functionality, Risk management, Contracts]
An incremental project plan: introducing cleanroom method and object-oriented development method
Proceedings of the 20th International Conference on Software Engineering
None
1998
Introducing new technologies into a software development process or project often produces both good and bad effects. If it is well planned it will improve both productivity and quality. We present an incremental development process planning approach (IDPA) which uses the idea of technical dependency-based assessment of the project planning for a small and stable development team shifting slowly but steadily to a new software paradigm that fits the traditional development process. The project plan was a well-connected set of incremental fragments of improvements with the introduction of new technologies or methods to the previous plans. IDPA is a method of assessing each technology or method to be decomposed and scheduled according to the technical dependency at the time of introduction. We also present a case study applying this idea to a real development project, in which object-oriented technology and the cleanroom method were introduced, and present the results of its evaluation.
[Productivity, cleanroom method, software development process, Costs, project management, object-oriented programming, Process planning, Mission critical systems, Laboratories, Project management, Process control, software development management, Scheduling, software quality, software project, IDPA, case study, technical dependency-based assessment, productivity, object-oriented development method, Client server systems, incremental development process planning approach, incremental project plan, Object oriented programming]
Specification and verification of an object request broker
Proceedings of the 20th International Conference on Software Engineering
None
1998
This paper reports the results of specifying, modeling and verifying a safe object request broker. This method has been applied on several case studies by using the SPIN verification tool. An object request broker has been implemented using sC++, a concurrent extension of C++ designed by our team. Liveness and safety properties have been checked on the model to ensure the system behaviour is correct. This application shows the efficiency of using formal methods in building safe applications. It also shows that sC++ is appropriate for developing protocols and communicating systems and is easily translatable from models such as Promela.
[Protocols, program verification, modeling, Laboratories, object request broker specification, temporal logic, liveness, case studies, formal specification, Concurrent computing, Promela, Computer architecture, Computer networks, concurrent extension, Safety, Space exploration, object-oriented methods, communicating systems, Logic, protocols, client-server systems, C++, system behaviour, Buildings, sC++, State-space methods, SPIN verification tool, object request broker verification, CORBA, formal methods, object-oriented languages, safety properties]
Integrating architecture description languages with a standard design method
Proceedings of the 20th International Conference on Software Engineering
None
1998
Software architecture descriptions are high-level models of software systems. Some researchers have proposed special-purpose architectural notations that have a great deal of expressive power but are not well integrated with common development methods. Others have used mainstream development methods that are accessible to developers, but lack semantics needed for extensive analysis. We describe an approach to combining the advantages of these two ways of modeling architectures. We present two examples of extending UML, an emerging standard design notation, for use with two architecture description languages, C2 and Wright. Our approach suggests a practical strategy for bringing architectural modeling into wider use, namely by incorporating substantial elements of architectural models into a standard design method.
[Costs, Design methodology, Object oriented modeling, Unified modeling language, architecture description languages, software systems, standard design notation, Wright, Power system modeling, incremental development, C2, extending UML, software architecture, Software architecture, architectural notations, object-oriented design, constraint languages, Computer architecture, Software systems, Software standards, software engineering, object-oriented methods, Architecture description languages]
Analyzing effects of cost estimation accuracy on quality and productivity
Proceedings of the 20th International Conference on Software Engineering
None
1998
This paper discusses the effects of estimation accuracy for software development cost on both the quality of the delivered code and the productivity of the development team. The estimation accuracy is measured by metric RE (relative error). The quality and productivity are measured by metrics FQ (field quality) and TP (team productivity). Using actual project data on thirty-one projects at a certain company, the following are verified by correlation analysis and testing of statistical hypotheses. There is a high correlation between the faithfulness of the development plan to standards and the value of RE (a coefficient of correlation between them is -0.60). Both FQ and TP are significantly different between projects with -10%<RE<+10% and projects with RE/spl ges/+10% (the level of significance is chosen as 0.05).
[Productivity, Costs, statistical hypotheses testing, software development team, software development management, Programming, software quality, relative error metric, Embedded software, correlation analysis, standards, Software quality, Writing, Marketing and sales, field quality metric, software projects, System buses, software cost estimation, estimation accuracy, team productivity metric, Standards development, statistical analysis, Software engineering, software metrics]
Examples of applying software estimate tool
Proceedings of the 20th International Conference on Software Engineering
None
1998
Although estimating the cost of a software project and the effort involved is very important, improving accuracy and establishing the software as a technology are both difficult. To solve these problems we have tried to use a software estimating tool. This tool has two characteristics. The first is that the tool uses function point analysis (FPA) instead of source lines of code, and the second is that special factors present in software development (usually estimated only from experience) are considered. This report provides some examples of how software project effort estimates can be improved.
[Productivity, software project effort estimation, Costs, project management, software development, software development management, Delay estimation, Programming, software project, function point analysis, Computer languages, Quality assurance, special factors, USA Councils, Software quality, software cost estimation, software estimation tool, Software measurement, Software tools]
The use of goals to surface requirements for evolving systems
Proceedings of the 20th International Conference on Software Engineering
None
1998
This paper addresses the use of goals to surface requirements for the redesign of existing or legacy systems. Goals are widely recognized as important precursors to system requirements, but the process of identifying and abstracting them has not been researched thoroughly. We present a summary of a goal-based method (GBRAM) for uncovering hidden issues, goals, and requirements and illustrate its application to a commercial system, an Intranet-based electronic commerce application, evaluating the method in the process. The core techniques comprising GBRAM are the systematic application of heuristics and inquiry questions for the analysis of goals, scenarios and obstacles. We conclude by discussing the lessons learned through applying goal refinement in the field and the implications for future research.
[evolving systems, software prototyping, Documentation, Educational institutions, surface requirements, Computational Intelligence Society, Electronic commerce, Application software, formal specification, Intranet-based electronic commerce, Feedback, goal-based method, Prototypes, systems analysis, legacy systems, Software systems, system requirements, Risk management, Business]
A systematic approach to domain-oriented software development
Proceedings of the 20th International Conference on Software Engineering
None
1998
We describe our experience with domain-oriented software development in the domain of automatic teller machine applications. We systematically proceeded with development in four phases: domain analysis, domain formalization, domain facility building, and product development. In this development, we built domain facilities consisting of a domain framework and domain CASE (Computer Aided Software Engineering) tools, then employed them for application development. The framework shared about 4% of the application, and the remaining 96% was generated by the CASE tools automatically. Our approach was found to realize effective reuse of design and implementation and to enable domain-oriented development in large domains.
[Process design, Computer aided software engineering, Laboratories, Programming, Research and development, design reuse, product development, software engineering, software tools, object oriented programming, automatic teller machines, object-oriented programming, Object oriented modeling, domain analysis, domain facility building, Application software, domain formalization, CASE, application development, domain oriented software development, automatic teller machine, Software systems, Product development, computer aided software engineering, Software engineering]
Non-intrusive object introspection in C++: architecture and application
Proceedings of the 20th International Conference on Software Engineering
None
1998
We describe the design and implementation of system architecture to support object introspection in C++. In this system, information is collected by parsing class declarations, and used to build a supporting environment for object introspection. Our approach is nonintrusive because it requires no change to the original class declarations and libraries, and it guarantees compatibility between objects before and after the addition of introspective capability. This is critical if one wants to integrate third-party class libraries, which are often supplied as black boxes and allow no modification, into highly dynamic applications. We show two applications: automatic I/O support for C++ objects; and an interactive exercise of dynamically loaded C++ class libraries.
[automatic input output support, C language, program compilers, Delay, software libraries, Runtime, class declaration parsing, object oriented programming, Object oriented programming, Contracts, Java, object-oriented programming, C++, object compatibility, software reuse, Object oriented modeling, system design, nonintrusive object introspection, Application software, Computer languages, Software libraries, Councils, system architecture, object-oriented languages, software reusability, system implementation, dynamically loaded class libraries]
Process assurance audits: lessons learned
Proceedings of the 20th International Conference on Software Engineering
None
1998
During 1997, a large Information System (IS) Division of a Canadian phone company implemented formal process assurance in its Quality Assurance group. The status report presents a new perspective on the measurement of process assurance and the lessons learned after one year of assessing the individual conformance of software development projects to the corporate software development process (CSDP) of the organization. The status report presents the assurance process overview, goals. Benefits and scope, as well as the 1997 results overview, followed by the lessons learned for the 1998 audit program.
[Software maintenance, Quality Assurance group, software development project conformance, Collaborative software, Instruction sets, Programming, process assurance audits, auditing, software quality, telecommunication computing, Information systems, Canadian phone company, process assurance measurement, Quality assurance, corporate software development process, Software quality, Management information systems, Cities and towns, formal process assurance, Information System Division, information systems, Software measurement]
Conceptual module querying for software reengineering
Proceedings of the 20th International Conference on Software Engineering
None
1998
Many tools have been built to analyze source code. Most of these tools do not adequately support reengineering activities because they do not allow a software engineer to simultaneously perform queries about both the existing and the desired source structure. This paper introduces the conceptual module approach that overcomes this limitation. A conceptual module is a set of lines of source that are treated as a logical unit. We show how the approach simplifies the gathering of source information for reengineering tasks, and describe how a tool to support the approach was built as a front-end to existing source analysis tools.
[Data analysis, software reuse, Reverse engineering, Software performance, Data engineering, reverse engineering, source analysis tools, software reengineering, Information analysis, Computer science, systems re-engineering, logical unit, conceptual module querying, source information, software reusability, Performance analysis, software tools, Software tools, source code analysis]
Software process modeling and enactment: an experience report related to problem tracking in an industrial project
Proceedings of the 20th International Conference on Software Engineering
None
1998
The paper provides an overview of process research and the application of research results to practice and then describes process models using FUNSOFT nets and a workflow management system LEU. The example from which the experience is drawn is that of a problem tracking system and the components of the model are described and illustrated. The experience reported comes in three flavors: the appropriateness of FUNSOFT nets as a modeling mechanisms and the alternative use of state transition diagrams; the benefits of the model and its support of the business process; and finally some implications for process research.
[Software prototyping, Object oriented modeling, Humans, software process modeling, diagrams, Application software, Computer science, Technology management, software process enactment, state transition diagrams, business process, industrial project, Computer industry, software engineering, computer aided software engineering, process research, FUNSOFT nets, Software tools, LEU, Workflow management software, Software engineering, workflow management system, problem tracking system]
Software quality analysis and measurement service activity in the company
Proceedings of the 20th International Conference on Software Engineering
None
1998
It is very important to improve software quality using program analysis and measurement tools and the SQA (Software Quality Assurance) method at the appropriate points during the process of development. In many development departments, there is often not enough time to evaluate and use the tools and the SQA method or to accumulate the know-how for effective use. This paper describes the support activity of a software quality analysis and measurement service which is performed by our laboratory team within the company as a third-party independent staff group. We call this activity HQC (High Quality software creation support virtual Center). The purpose of this activity is as follows. First we improve the software quality engineering process in the development department, for example, we help them to increase efficiency of review or testing. To accomplish this, we use program static analysis tools to detect fault-prone software components. Next we assist in starting their own self-improvement process. In addition, we provide service activity to many development departments concurrently. We have been making progress with these activities, and some development departments have begun to establish improvement processes themselves.
[Software testing, program testing, Laboratories, software quality analysis, software review, Electronic mail, software quality, SQA method, Information analysis, static analysis tools, Software Quality Assurance method, program analysis, company, software tools, HQC, Software measurement, Data analysis, program diagnostics, software testing, software quality measurement service, fault-prone software components, Time measurement, Fault detection, laboratory, Software quality, Software tools, software metrics, software improvement processes]
Design components: towards software composition at the design level
Proceedings of the 20th International Conference on Software Engineering
None
1998
Component-based software development has proven effective for systems implementation in well-understood application domains, but is still insufficient for the creation of reusable and changeable software architectures. Design patterns address these shortcomings by capturing the expertise that is necessary for reusable design solutions. However, there is so far no methodical approach to providing these conceptual design building blocks in a tangible and composable form. To address this limitation, we introduce the notion of design components, reified design patterns fit for software composition. In this paper, we define design components and explain their constituents and services. Furthermore, we detail the activities of design composition and illustrate them as a process within a four-dimensional design space. Moreover, we describe a prototype of a design composition environment. A case study helps illustrating our approach.
[Process design, object-oriented programming, software development, Object oriented modeling, Collaborative software, software development management, Application software, component-based, Programming profession, reusable, Software design, Software architecture, Councils, reified design patterns, software composition, software reusability, changeable software architectures, Software reusability, Software engineering, design components]
Modeling and analysis of a virtual reality system with time Petri nets
Proceedings of the 20th International Conference on Software Engineering
None
1998
The design, implementation, and testing of virtual environments is complicated by the concurrency and real-time features of these systems. Therefore, the development of formal methods for modeling and analysis of virtual environments is highly desirable. In the past, Petri net models have led to good empirical results in the automatic verification of concurrent and real-time systems. We applied a timed extension of Petri nets to modeling and analysis of the CAVE/sup TM/ virtual environment at the University of Illinois at Chicago. We report on our time Petri net model and on empirical studies that we conducted with the Cabernet toolset from Politecnico di Milano. Our experiments uncovered a flaw in the way a shared buffer is used by CAVE processes. Due to an erroneous synchronization on the buffer, different CAVE walls can simultaneously display images based on different input information. We conclude from our empirical studies that Petri net-based tools can effectively support the development of reliable virtual environments.
[virtual reality, program verification, program testing, modeling, Petri nets, Glass, Delay, CAVE, Virtual reality, Trademarks, real-time system, Cabernet toolset, shared buffer, Computational modeling, testing, time Petri nets, Power system modeling, concurrency, Computer displays, systems analysis, real-time systems, automatic verification, formal methods, virtual environment, system implementation, synchronization, Frequency synchronization, virtual reality system analysis, Formal verification]
The ramp-up problem in software projects: a case study of how software immigrants naturalize
Proceedings of the 20th International Conference on Software Engineering
None
1998
Joining a software development team is like moving to a new country to start employment; the immigrant has a lot to learn about the job, the local customs, and sometimes a new language. In an exploratory case study, we interviewed four software immigrants, in order to characterize their naturalization process. Seven patterns in four major categories were found. In this paper, these patterns are substantiated, and their implications discussed. The lessons learned from this study can be applied equally to improving the naturalization process, and to the formulation of further research questions.
[Software maintenance, Computer aided software engineering, Costs, software development team, software development management, Programming, Educational institutions, human resource management, Personnel, Management training, case study, Recruitment, Computer science, software immigrants, Software systems, personnel, software projects, naturalization process, job, new employees]
Using simulation to build inspection efficiency benchmarks for development projects
Proceedings of the 20th International Conference on Software Engineering
None
1998
It is difficult for organizations introducing and using software inspections to evaluate how efficient they are. However, it is of practical importance to determine whether they have been efficiently implemented or whether further corrective actions are necessary to bring them up to standard. We present in this paper a procedure for building inspection efficiency benchmarks based on simulation and typical inspection data. Based on most of the data published in the literature, we build an industry-wide benchmark which intends to capture the current practice regarding inspection efficiency. This benchmark construction procedure can also be used to build enterprise specific benchmarks. Last, we assess how robust we can expect them to be by distorting their input distributions to reflect violations of the assumptions made.
[Costs, On the job training, development projects, Buildings, software development management, Inspection, Research and development, inspection efficiency, Construction industry, benchmark construction, inspection efficiency benchmarks, software inspections, Robustness, Software measurement, software performance evaluation, Testing, Software engineering, software metrics]
An empirical study of regression test selection techniques
Proceedings of the 20th International Conference on Software Engineering
None
1998
Regression testing is an expensive maintenance process directed at validating modified software. Regression test selection techniques attempt to reduce the cost of regression testing by selecting tests from a program's existing test suite. Many regression test selection techniques have been proposed. Although there have been some analytical and empirical evaluations of individual techniques, to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed. We conducted an experiment to examine the relative costs and benefits of several regression test selection techniques. The experiment examined five techniques for reusing tests, focusing on their relative abilities to reduce regression testing effort and uncover faults in modified programs. Our results highlight several differences between the techniques, and expose essential tradeoffs that should be considered when choosing a technique for practical application.
[Software testing, Performance evaluation, Software maintenance, System testing, Costs, program testing, Educational institutions, empirical study, regression test selection techniques, Application software, software maintenance, Computer science, maintenance process, modified software, Fault detection, Performance analysis]
Blending Object-Z and Timed CSP: an introduction to TCOZ
Proceedings of the 20th International Conference on Software Engineering
None
1998
Object-Z is an extension to the Z language designed to facilitate specification in an object-oriented style. It is an excellent tool for modeling data and algorithms, but its object semantics are single threaded and operations are atomic. Therefore, it is difficult to use Object-Z to capture the behaviour of concurrent real-time reactive systems. On the other hand, Timed CSP is good at modeling real-time concurrent behaviour, but has little support for modeling the state of a complex system. This paper introduces a blending of Object-Z and Timed CSP, known as TCOZ. The blended notation is particularly suited for specifying complex systems whose components have their own thread of control.
[Real time systems, Algorithm design and analysis, communicating sequential processes, temporal logic, Control systems, Yarn, formal specification, Concurrent computing, Object-Z, complex systems, data modeling, Timed CSP, notation, specification languages, specification language, object semantics, object-oriented programming, Object oriented modeling, Process control, single threaded, object oriented language, concurrent real-time reactive systems, Z language, Information technology, real-time systems, object-oriented languages, Timing, Australia, TCOZ]
Software cost and quality analysis by statistical approaches
Proceedings of the 20th International Conference on Software Engineering
None
1998
This paper describes statistical approaches for analyzing software cost and quality data of Software Development Center of Hitachi (SDCH) for several recent years. We have achieved some highly fitted cost and quality models with r=0.837 to 0.975 multiple correlation. The models may be used for cost and duality estimation and also productivity and quality evaluation.
[Productivity, Costs, Multidimensional systems, Data analysis, software cost analysis, Statistical analysis, Programming, software quality analysis, Vectors, Regression analysis, software quality, duality estimation, cost estimation, quality evaluation, Hitachi Software Development Center, Software quality, Cities and towns, multiple correlation, productivity evaluation, software cost estimation, statistical analysis, statistical approach]
Evaluating the tradeoffs of mobile code design paradigms in network management applications
Proceedings of the 20th International Conference on Software Engineering
None
1998
The question of whether technologies supporting mobile code are bringing significant benefits to the design and implementation of distributed applications is still an open one. Even more difficult is to identify precisely under which conditions a design exploiting mobile code is preferable over a traditional one. In this work, we present an in-depth evaluation of several mobile code design paradigms against the traditional client-server architecture, within the application domain of network management. The evaluation is centered around a quantitative model, which is used to determine precisely the conditions for the selection of a design paradigm minimizing the network traffic related to management.
[Java, application domain, Protocols, Telecommunication traffic, mobile code design paradigms, Information management, Application software, network management, design paradigm, Intelligent networks, computer network management, Mobile agents, Traffic control, Computer networks, software engineering, Mobile computing, quantitative model]
An adaptable generation approach to agenda management
Proceedings of the 20th International Conference on Software Engineering
None
1998
As software engineering efforts move to more complex, distributed environments, coordinating the activities of people and tools becomes very important. While groupware systems address user level communication needs and distributed computing technologies address tool level communication needs, few attempts have been made to synthesize the common needs of both. This paper describes our attempt to do exactly that. We describe a framework for generating an agenda management system (AMS) from a specification of the system's requirements. The framework can meet a variety of requirements and produces a customized AMS appropriate for use by both humans and software tools. The framework and generated system support evolution in several ways, allowing existing systems to be extended as requirements change. We also describe our experiences using this approach to create an AMS to support a process programming environment.
[Software testing, System testing, agenda management, Collaborative software, Humans, software development management, agenda management system, Electronic mail, distributed environments, Application software, Programming profession, distributed computing, process programming environment, groupware, Collaborative work, Software systems, software engineering, software tools, Software engineering]
Validation of the coupling dependency metric as a predictor of run-time failures and maintenance measures
Proceedings of the 20th International Conference on Software Engineering
None
1998
The coupling dependency metric (CDM) is a successful design quality metric. Here we apply it to four case studies: run-time failure data for a COBOL registration system; maintenance data for a C text-processing utility; maintenance data for a C++ patient collaborative care system; and maintenance data for a Java electronic file transfer facility. CDM outperformed a wide variety of competing metrics in predicting run-time failures and a number of different maintenance measures. These results imply that coupling metrics may be good predictors of levels of interaction within a software product.
[run-time failure data, software quality, levels of interaction, design quality metric, Java electronic file transfer facility, Runtime, formal verification, maintenance data, run-time failures, C text-processing utility, C++ patient collaborative care system, Testing, Java, Size measurement, Product design, metrics validation, software maintenance, Computer science, Network-on-a-chip, Collaboration, coupling dependency metric, Position measurement, maintenance measures, COBOL registration system, software product, software metrics]
Designing an application development model for a large banking organization
Proceedings of the 20th International Conference on Software Engineering
None
1998
The German Savings Banks Organization is establishing an application development model as a standard for object-oriented development. In an AD-Model project we developed an object-oriented methodology based on the UML notation. It contains a tailorable life cycle model and several architectural guidelines. The development process is extended with subprocesses for quality assurance, security, project management and reuse.
[large banking organization, tailorable life cycle model, Unified modeling language, Project management, software quality, Security, architectural guidelines, Guidelines, Quality assurance, security, object-oriented development, German Savings Banks Organization, AD Model project, UML notation, Standards development, software project management, object-oriented programming, project management, software reuse, Object oriented modeling, software development management, Banking, object-oriented methodology, application development model design, security of data, Standards organizations, quality assurance, software reusability, Acceleration, bank data processing]
Calibrating the COCOMO II Post-Architecture model
Proceedings of the 20th International Conference on Software Engineering
None
1998
The COCOMO II model was created to meet the need for a cost model that accounted for future software development practices. This paper describes some of the experiences learned in calibrating COCOMO II Post-Architecture model from eighty-three observations. The results of the multiple regression analysis, their implications, and a future calibration strategy are discussed.
[Software maintenance, Costs, software development management, Programming, calibration strategy, Calibration, Regression analysis, Application software, future software development practice, cost model, Embedded software, cost estimation, Computer science, COCOMO II, post-architecture model, Processor scheduling, Computer architecture, software cost estimation, multiple regression analysis, metrics, statistical analysis, calibration]
Object oriented reuse: experience in developing a framework for speech recognition applications
Proceedings of the 20th International Conference on Software Engineering
None
1998
The development of highly interactive software systems with complex user interfaces has become increasingly common, where prototypes are often used as a vehicle for demonstrating visions of innovative systems. Given this trend, it is important for new technology to be based on flexible architectures that do not mandate the understanding of all complexities inherent in a system. In this context, we share our experience with developing an object oriented framework for a specific new technology, i.e. speech recognition. We describe the benefits of the object oriented paradigm rich with design patterns, which provide a natural way to model complex concepts and capture system relationships effectively, along with achieving a high level of software reuse.
[prototypes, Engines, system relationships, speech recognition, flexible architectures, USA Councils, Prototypes, design patterns, interactive systems, speech recognition applications, Productivity, object-oriented programming, object oriented reuse, software reuse, natural language interfaces, Rivers, Application software, Milling machines, Speech recognition, complex user interfaces, User interfaces, software reusability, innovative systems, Software systems, object oriented framework]
Integrating obstacles in goal-driven requirements engineering
Proceedings of the 20th International Conference on Software Engineering
None
1998
Requirements engineering is concerned with the elicitation of high-level goals to be achieved by the system envisioned, the refinement of such goals and their operationalization into services and constraints, and the assignment of responsibilities for the resulting requirements to agents such as humans, devices and software. Requirements engineering processes may often result in requirements and assumptions about agent behaviour that are too ideal; some of them are likely to be violated from time to time in the running system due to unexpected agent behaviour. The lack of anticipation of exceptional behaviors results in unrealistic, unachievable and/or incomplete requirements. As a consequence, the software developed from those requirements will inevitably result in poor performance, sometimes with critical consequences on the environment. This paper proposes systematic techniques for reasoning about obstacles to the satisfaction of goals, requirements, and assumptions elaborated in the requirements engineering process. These techniques are integrated into an existing method for goal-driven requirements elaboration with the aim of deriving more complete and realistic requirements. The concept of obstacle is first defined precisely. Formal techniques and domain-independent heuristics are then proposed for identifying obstacles from goal/assumption formulations and domain properties. The paper then discusses techniques for resolving obstacles by transformation of the goals, requirements and assumptions elaborated so far in the process, or by introduction of new ones. Numerous examples are given throughout the paper to suggest how the techniques can be usefully applied in practice.
[formal techniques, Humans, Interconnected systems, incomplete requirements, unexpected behaviour, formal specification, defensive requirements specification, systematic techniques, performance, obstacle-driven requirements transformation, systems analysis, goal-driven requirements engineering, high-level goal elicitation, Face, domain-independent heuristics, software performance evaluation, Software engineering, Formal verification]
Agile Software Process and its experience
Proceedings of the 20th International Conference on Software Engineering
None
1998
This article proposes a new software process model, ASP (Agile Software Process) and discusses its experience in large-scale software development. The Japanese software factory was a successful model in the development of quality software for large-scale business applications in the 80s. However, the requirements for software development have dramatically changed. Development cycle-time has been promoted to one of the top goals of software development in the 90s. Unlike conventional software process models based on volume, the ASP is a time-based process model which aims at quick delivery of software products by integrating the lightweight processes, modular process structures and incremental and iterative process enaction. The major contributions of APS include: a new process model and its enaction mechanism based on time; a software process model for evolutional delivery; a software process architecture integrating concurrent and asynchronous processes, incremental and iterative process enaction, distributed multi-site processes, and the people-centered processes; a process-centered software engineering environment for ASP; and experience and lessons learned from the use of ASP in the development of a family of large-scale communication software systems for more than five years.
[software products, human factors, Programming, Production facilities, software quality, Agile Software Process model, large-scale software development, people centered process, modular process structures, iterative process enaction, Computer architecture, incremental process enaction, software engineering, Large-scale systems, business applications, software process architecture, ASP software process model, Application specific processors, Large scale integration, Application software, Communication system software, communication software, Software quality, process centered software engineering, Japanese software factory, software development cycle-time, Software engineering, time-based process model]
A conceptual model of software maintenance
Proceedings of the 20th International Conference on Software Engineering
None
1998
Four distinct maintenance systems are studied and synthesised: the SEI quality framework and three industrial systems belonging to ABB, Ellemtel, and Ericsson. The goal is to validate the SEI framework, and to build a "state of the practice" conceptual model of the most fundamental software maintenance concepts. This model can help understand the underlying conditions for managing software maintenance. It can provide guidance to organisations in the process of building or improving their maintenance systems. It also constitutes a common basis for communication, for reasoning about software quality, and for building quality and maintenance models.
[Availability, Software maintenance, ABB, SEI quality framework, software management, Electronic mail, software quality, Ericsson, software maintenance, Research and development, Ellemtel, organisations, Communication industry, industrial systems, Computer industry, Software systems, conceptual model, SEI framework, Software measurement]
Measuring cognitive activities in software engineering
Proceedings of the 20th International Conference on Software Engineering
None
1998
This paper presents an approach to the study of cognitive activities in collaborative software development. This approach has been developed by a multidisciplinary team made up of software engineers and cognitive psychologists. The basis of this approach is to improve our understanding of software development by observing professionals at work. The goal is to derive lines of conduct or good practices based on observations and analyses of the processes that are naturally used by software engineers. The strategy involved is derived from a standard approach in cognitive science. It is based on the videotaping of the activities of software engineers, transcription of the videos, coding of the transcription, defining categories from the coded episodes and defining cognitive behaviors or dialogs from the categories. This project presents two original contributions that make this approach generic in software engineering. The first contribution is the introduction of a formal hierarchical coding scheme, which will enable comparison of various types of observations. The second is the merging of psychological and statistical analysis approaches to build a cognitive model. The details of this new approach are illustrated with the initial data obtained from the analysis of technical review meetings.
[software engineers, multidisciplinary team, Collaborative software, Merging, Psychology, software development management, Programming profession, Videos, Human computer interaction, collaborative software development, cognitive psychologists, Software systems, cognitive activities, software engineering, Cognitive science, Software measurement, Software engineering]
Promises: limited specifications for analysis and manipulation
Proceedings of the 20th International Conference on Software Engineering
None
1998
Structural change in a large system is hindered when information is missing about portions of the system, as is often the case in a distributed development process. An annotation mechanism called promises is described for expressing properties that can enable many kinds of structural change in systems. Promises act as surrogates for an actual component, and thus are analogous to "header" files, but with more specific semantic information. Unlike formal specifications, however, promises are designed to be easily extracted from systems and managed by programmers using automatic analysis tools. Promises are described for effects, unique references, and use properties. By using promises, a component developer can offer additional opportunity for change (flexibility) to clients, but at a potential cost in flexibility for the component itself. This suggests the possibility of using promises as a means to allocate flexibility among the components of a system.
[Encapsulation, limited specifications, Costs, Mechanical factors, distributed development process, Data mining, Formal specifications, formal specification, Programming profession, Information analysis, Computer science, systems analysis, US Government, unique references, Software systems, automatic analysis tools, promises, annotation mechanism]
Reuse-driven interprocedural slicing
Proceedings of the 20th International Conference on Software Engineering
None
1998
To manage the evolution of software systems effectively, software developers must understand software systems, identify and evaluate alternative modification strategies, implement appropriate modifications, and validate the correctness of the modifications. One analysis technique that assists in many of these activities is program slicing. To facilitate the application of slicing to large software systems, we adapted a control flow-based interprocedural slicing algorithm so that it accounts for interprocedural control dependencies not recognized by other slicing algorithms, and reuses slicing information for improved efficiency. Our initial studies suggest that additional slice accuracy and slicing efficiency may be achieved with our algorithm.
[program verification, Control systems, Information science, Databases, program understanding, software modification, program validation, program slicing, interprocedural control dependencies, Embedded computing, Data analysis, dataflow analysis, software reuse, Computational modeling, Software algorithms, control flow-based method, software development management, data flow analysis, reverse engineering, Application software, software maintenance, interprocedural slicing, Software development management, software reusability, Software systems, software evolution management]
Toward computational support for software process improvement activities
Proceedings of the 20th International Conference on Software Engineering
None
1998
Software organizations and projects need guidance on how to improve software process, not just guidelines on what to improve. Several surveys demonstrate that the Capability Maturity Model (CMM) and ISO-9000 only provide the latter. We report our in-depth analysis on a seventeen-month effort in software process improvement (SPI) at OMRON Corporation. The goal of the analysis was to identify issues and challenges of SPI and to design a step-wise practical method to avoid such problems. Major problems we have found include the lack of shared goal among stakeholders, insufficient understanding of the current progress of SPI efforts, and underutilization of a large amount of complex information generated during SPI. We present the method for software organizations and projects for dealing with the problems, and argue for a knowledge-based SPI support system based on the method.
[Productivity, Capability Maturity Model, ISO-9000, computational support, CMM, OMRON Corporation, Software performance, Guidelines, Software development management, knowledge based system, CASE, Coordinate measuring machines, knowledge based systems, Software quality, software process improvement, Computer industry, software engineering, computer aided software engineering, software projects, System buses, software tools, Capability maturity model, software organizations, Software engineering]
Overcoming the NAH syndrome for inspection deployment
Proceedings of the 20th International Conference on Software Engineering
None
1998
Despite considerable evidence to show that inspections can help reduce costs and improve quality, inspections are not widely deployed in the software industry. One of the likely reasons for this is the "not applicable here (NAH)" syndrome-developers and managers believe that in their environment, inspections will not provide the benefits seen by other organizations. One of the big challenges for deploying inspections is to overcome this syndrome. We describe two experiments that can be conducted, with little effort, in an organization to obtain data from the organization to build a case for inspections. By conducting one of these experiments, we were able to effectively overcome the NAH syndrome in our organization-many developers and managers are now ready to try inspections in their projects. Though the purpose of the experiment was to overcome the syndrome, the data from the experiment also shows how code inspections compare with unit testing in terms of defect detection capability, and the effect of inspections on the overall cost of development.
[Productivity, program debugging, Costs, program testing, software costs, not applicable here syndrome, experiments, Inspection, software inspection deployment, software quality, Industrial electronics, NAH syndrome, program unit testing, software industry, Electronics industry, Software quality, Cities and towns, organizations, program defect detection, Computer industry, software projects, Testing, Software engineering]
A learning curve based simulation model for software development
Proceedings of the 20th International Conference on Software Engineering
None
1998
Many of the non conventional software development methodologies (such as object-oriented analysis methodology) and tools (such as visual programming environment) have been applied in real life projects. These projects have been started without sufficient previous training given to the developers. An increment in the productivity has been seen as the projects progress. This paper proposes a simulation model for software development which can deal with variances of developers' productivity during software development. As the proposed model takes into account the developer's learning curve, it can be used to compute a developer's productivity and the quantity of gain to the developer's knowledge in executing an activity. The proposed model has been applied to four typical scenarios in our case study. The results show that it is highly practicable. An outline of a project planning prototype which is based on the proposed model is presented. The prototype can be used to make project plans which take the developer's learning curve into consideration.
[Productivity, Software prototyping, project management, software development, Object oriented modeling, software development management, project planning prototype, Programming, digital simulation, developers' productivity, Application software, Information analysis, Information science, Prototypes, Software quality, simulation model, learning curve based, Software tools, learning curve]
What you see is what you test: a methodology for testing form-based visual programs
Proceedings of the 20th International Conference on Software Engineering
None
1998
Form-based visual programming languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that form-based visual programs often contain faults. We would like to provide at least some of the benefits of formal testing methodologies to the creators of these programs. This paper presents a testing methodology for form-based visual programs. To accommodate the evaluation model used with these programs, and the interactive process by which they are created, our methodology is validation driven and incremental. To accommodate the users of these languages, We provide an interface to the methodology that does not require an understanding of testing theory. We discuss our implementation of this methodology and empirical results achieved in its use.
[Software testing, Performance evaluation, end-user computing, program testing, Computational modeling, form-based visual programs testing, Displays, formal testing methodologies, Engines, Programming profession, Scheduling algorithm, commercial spreadsheets, Computer science, Computer languages, visual programming languages, Feedback, visual programming]
Three dimensional software modelling
Proceedings of the 20th International Conference on Software Engineering
None
1998
Traditionally, diagrams used in software systems modelling have been two dimensional (2D). This is probably because graphical notations, such as those used in object-oriented and structured systems modelling, draw upon the topological graph metaphor, which, at its basic form, receives little benefit from three dimensional (3D) rendering. This paper presents a series of 3D graphical notations demonstrating effective use of the third dimension in modelling. This is done by e.g. connecting several graphs together, or in using the Z co-ordinate to show special kinds of edges. Each notation combines several familiar 2D diagrams, which can be reproduced from 2D projections of the 3D model. 3D models are useful even in the absence of a powerful graphical workstation: even 2D stereoscopic projections can expose more information than a plain planar diagram.
[Solid modeling, Visualization, Computer aided software engineering, object-oriented programming, Object oriented modeling, Unified modeling language, three dimensional software modelling, structured systems modelling, 2D stereoscopic projections, graphical notations, Power system modeling, Flowcharts, Software systems, object-oriented modelling, three dimensional rendering, Z co-ordinate, software engineering, Workstations, topological graph metaphor, Software tools, rendering (computer graphics)]
Assessment of system evolution through characterization
Proceedings of the 20th International Conference on Software Engineering
None
1998
Owing to the growing diffusion of the object-oriented paradigm and the need to keep the process of software development under control, industries are looking for metrics/indicators capable of evaluating system evolution to control quality, reusability, maintainability, etc. Some new metrics are proposed for monitoring system development and maintenance. These metrics are used with a set of histograms to give a clear characterization of the system under development. Histograms call be profitably used to detect critical conditions during the system life-cycle. The semantics of these histograms has been validated against several projects: an example is also reported.
[system development monitoring, Software maintenance, histograms, Industrial control, software development management, object-oriented paradigm, Control systems, reusability, system life-cycle, critical conditions, quality, Histograms, Software quality, Computer industry, Electrical equipment industry, object-oriented methods, system evolution, metrics, Informatics, Software reusability, characterization, Monitoring, maintainability, software metrics]
Reifying configuration management for object-oriented software
Proceedings of the 20th International Conference on Software Engineering
None
1998
Using a solid Software Configuration Management (SCM) is mandatory to establish and maintain the integrity of the products of a software project throughout the project's software life cycle. Even with the help of sophisticated tools, handling the various dimensions of SCM can be a daunting (and costly) task for many projects. The contribution of this paper is to propose a method (based on the use Creational Design Patterns) to simplify SCM by reifying the variants of an object-oriented software system into language-level objects; and to show that newly available compilation technology makes this proposal attractive with respect to performance (memory footprint and execution time) by inferring which classes are needed for a specific configuration and optimizing the generated code accordingly, We demonstrate this idea on an artificial case study intended to be representative of a properly designed OO software. All the performance figures me get are obtained with freely available software, and, since the source code of our case study is also freely available, they are easily reproducible and checkable.
[Software maintenance, object-oriented programming, Project management, software development management, object-oriented software, Software performance, data integrity, Proposals, software project, Design optimization, configuration management, integrity, Engineering management, Solids, Software systems, software life cycle, Capability maturity model, object-oriented software system, Software engineering]
An experimental study of individual subjective effort estimations and combinations of the estimates
Proceedings of the 20th International Conference on Software Engineering
None
1998
The required effort of a task can be estimated subjectively in interviews with experts in an organization in different ways. Interview techniques dealing with which type of questions to ask are evaluated and techniques for combining estimates from individuals into one estimate are compared in an experiment. The result shows that the interview technique is not as important as the combination technique. The estimate which is best with respect to mean value and standard deviation of the effort is based on an equal weighting of all individual estimates. The experiment is performed within the Personal Software Process (PSP).
[Performance evaluation, Costs, Project management, Software performance, Programming, measurement, interview techniques, experimentation, mean value, Cost estimation, software engineering, Software measurement, Personal Software Process, project management, Linear regression, software development management, subjective effort estimations, Time measurement, PSP, personnel, metrics, combination technique, Usability, Software engineering, standard deviation]
Architecture-based runtime software evolution
Proceedings of the 20th International Conference on Software Engineering
None
1998
Continuous availability is a critical requirement for an important class of software systems. For these systems, runtime system evolution can mitigate the costs and risks associated with shutting down and restarting the system for an update. We present an architecture-based approach to runtime software evolution and highlight the role of software connectors in supporting runtime change. An initial implementation of a tool suite for supporting the runtime modification of software architectures, called ArchStudio, is presented.
[Costs, software prototyping, architecture-based runtime software evolution, Mission critical systems, software systems, software architectures, Control systems, Connectors, Computer science, Runtime, Software architecture, Operating systems, Computer architecture, runtime system evolution, Software systems, software engineering, ArchStudio, software connectors]
Extracting concepts from file names; a new file clustering criterion
Proceedings of the 20th International Conference on Software Engineering
None
1998
Decomposing complex software systems into conceptually independent subsystems is a significant software engineering activity which received considerable research attention. Most of the research in this domain considers the body of the source code; trying to cluster together files which are conceptually related. We discuss techniques for extracting concepts (abbreviations) from a more informal source of information: file names. The task is difficult because nothing indicates where to split the file names into substrings. In general, finding abbreviations would require domain knowledge to identify the concepts that are referred to in a name and intuition to recognize such concepts in abbreviated forms. We show by experiment that the techniques we propose allow about 90% of the abbreviations to be found automatically.
[Software maintenance, domain knowledge, independent subsystems, Reverse engineering, Data mining, substrings, Tellurium, research, artificial intelligence, concept extraction, complex software decomposition, program understanding, experiment, software engineering, design recovery, file names, abbreviations, Information resources, Buildings, source code, reverse engineering, software maintenance, Information technology, Organizing, Software systems, file organisation, file clustering criterion, Software tools]
Workflow management based on process model repositories
Proceedings of the 20th International Conference on Software Engineering
None
1998
Workflow management is an area of increasing interest. Nonetheless there are only a few workflow systems which are actually used for supporting business processes in an industrial context. Most of these systems only deal with processes which create and manipulate unstructured pieces of information, like documents and images. Our workflow management approach, called the FUNSOFT net approach, additionally supports the management of structured pieces of information, called objects. In doing so, we use a repository which is used to store information about process models and about individual processes. This repository is particularly useful for processes dealing with highly structured information, such as software processes. We describe the experience in using such a repository. We point out which of these experiences may be useful for developers of other workflow management systems as well.
[Petri nets, software development management, industrial context, Relational databases, Programming, business processes, Transaction databases, relational database, relational databases, FUNSOFT net approach, Computer science, workflow management systems, structured information, Engineering management, process model repositories, Concrete, computer aided software engineering, Workflow management software, Software engineering]
Formalizing design patterns
Proceedings of the 20th International Conference on Software Engineering
None
1998
Design patterns facilitate reuse of good design practices. They are typically given by using conventional notations that lack well-defined semantics and, therefore reasoning about their behaviors requires formalization. Even when formalized, conventional communication abstractions may lead to too laborious formalizations when addressing the temporal behavior of a pattern as a whole instead of behaviors local to its components. We show that rigorous reasoning can be eased by formalizing temporal behaviors of patterns in terms of high-level abstractions of communication, and that by using property-preserving refinements, specifications can be naturally composed by using patterns as building blocks.
[Context, Process design, Design methodology, Laboratories, reasoning, temporal logic, Formal specifications, formal specification, formal specifications, communication abstractions, temporal behavior, design patterns, property-preserving refinements, Writing, Software systems, Skeleton, Concrete, high-level abstractions, temporal behaviors, Software engineering]
A model of noisy software engineering data
Proceedings of the 20th International Conference on Software Engineering
None
1998
Software development data is highly variable, which often results in underlying trends being hidden. In order to address this problem, a method of data analysis, adapted from the financial community, is presented that permits the shape of the curve of some activity to be reduced to a few line segments, called the characteristic curve. This process is used on sample data from the NASA/GSFC Software Engineering Laboratory and has been shown to be a reasonable method to understand the underlying process being plotted.
[Data analysis, Shape, data analysis, NASA, Laboratories, Scattering, software development management, Programming, Educational institutions, Computer science, characteristic curve, NASA/GSFC Software Engineering Laboratory, financial data, Databases, noisy software engineering data, exponential moving average, software development data, financial sector, curve fitting, Software engineering]
Parallel changes in large scale software development: an observational case study
Proceedings of the 20th International Conference on Software Engineering
None
1998
An essential characteristic of large scale software development is parallel development by teams of developers. How this parallel development is structured and supported has a profound effect on both the quality and timeliness of the product. We conduct an observational case study in which me collect and analyze the change and configuration management history of a legacy system to delineate the boundaries of, and to understand the nature of, the problems encountered in parallel development. The results of our studies are: 1) that the degree of parallelism is very high-higher than considered by tool builders; 2) there are multiple levels of parallelism and the data for some important aspects are uniform and consistent for all levels and 3) the tails of the distributions are long, indicating the tail, rather than the mean, must receive serious attention in providing solutions for these problems.
[Computer aided software engineering, parallel development, legacy, software development management, Switches, Programming, Probability distribution, History, Software development management, configuration management, Engineering management, observational case study, management of change, Software systems, software engineering, Large-scale systems, large scale software development, Research and development management]
COBRA: a hybrid method for software cost estimation, benchmarking, and risk assessment
Proceedings of the 20th International Conference on Software Engineering
None
1998
Current cost estimation techniques have a number of drawbacks. For example, developing algorithmic models requires extensive past project data. Also, off-the-shelf models have been found to be difficult to calibrate but inaccurate without calibration. Informal approaches based on experienced estimators depend on estimators' availability and are not easily repeatable, as well as not being much more accurate than algorithmic techniques. We present a method for cost estimation that combines aspects of algorithmic and experiential approaches (referred to as COBRA, COst estimation, Benchmarking, and Risk Assessment). We find through a case study that cost estimates using COBRA show an average ARE of 0.09. Although we do not have the room to describe the benchmarking and risk assessment parts, the reader will find detailed information in (Briand et al., 1997).
[risk management, risk assessment, Costs, hybrid method, benchmarking, Software algorithms, Project management, Thumb, Parametric statistics, Calibration, algorithmic models, case study, COBRA, off-the-shelf models, software cost estimation, Risk management, Resource management, calibration, Contracts, software performance evaluation, Software engineering]
Techniques for trusted software engineering
Proceedings of the 20th International Conference on Software Engineering
None
1998
How do we decide if it is safe to run a given piece of software on our machine? Software used to arrive in shrink-wrapped packages from known vendors. But increasingly, software of unknown provenance arrives over the internet as applets or agents. Running such software risks serious harm to the hosting machine. Risks include serious damage to the system and loss of private information. Decisions about hosting such software are preferably made with good knowledge of the software product itself, and of the software process used to build it. We use the term Trusted Software Engineering to describe tools and techniques for constructing safe software artifacts in a manner designed to inspire trust in potential hosts. Existing approaches have considered issues such as schedule, cost and efficiency; we argue that the traditionally software engineering issues of configuration management and intellectual property protection are also of vital concern. Existing approaches (e.g., Java) to this problem have used static type checking, run-time environments, formal proofs and/or cryptographic signatures; we propose the use of trusted hardware in combination with a key management infrastructure as an additional, complementary technique for trusted software engineering, which offers some attractive features.
[Costs, cryptographic signatures, software reliability, industrial property, Software safety, key management infrastructure, Engineering management, trusted software engineering, static type checking, Protection, applets, formal proofs, cryptography, Packaging machines, agents, configuration management, Software packages, Intellectual property, intellectual property protection, Internet, software product, Software tools, Software engineering, software process, run-time environments]
Exploiting an event-based infrastructure to develop complex distributed systems
Proceedings of the 20th International Conference on Software Engineering
None
1998
The development of complex distributed systems demands for the creation of suitable architectural styles (or paradigms) and related run-time infrastructures. An emerging style that is receiving increasing attention is based on the notion of event. In an event-based architecture, distributed software components interact by generating and consuming events. The occurrence of an event in a component (called source) is asynchronously notified to any other component (called recipient) that has declared some interest in it. This paradigm holds the promise of supporting a flexible and effective interaction among highly reconfigurable distributed software components. We have developed an object-oriented infrastructure, called JEDI (Java Event-based Distributed Infrastructure), to support the development and operation of event-based systems. During the past year, JEDI has been used to implement a significant example of distributed system, namely, the OPSS workflow management system. The paper illustrates JEDI main features and how we have used it to implement the OPSS workflow management system. Moreover, it provides an initial evaluation of our experiences in using an event-based architectural style.
[Java, object-oriented programming, JEDI, distributed software components, OPSS workflow management system, Government, distributed processing, event-based architecture, run-time infrastructures, reconfigurable distributed software, Electronic mail, Telecommunication computing, Distributed computing, Java Event-based Distributed Infrastructure, Convergence, Runtime, architectural styles, object-oriented infrastructure, Computer architecture, Broadcasting, complex distributed systems, Computer networks]
Defect content estimations from review data
Proceedings of the 20th International Conference on Software Engineering
None
1998
Reviews are essential for defect detection and they provide an opportunity to control the software development process. This paper focuses upon methods for estimating the defect content after a review and hence to provide support for process control. Two new estimation methods are introduced as the assumptions of the existing statistical methods are not fulfilled. The new methods are compared with a maximum-likelihood approach. Data from several reviews are used to evaluate the different methods. It is concluded that the new estimation methods provide new opportunities to estimate the defect content.
[Maximum likelihood estimation, Maximum likelihood detection, program debugging, software development process, Statistical analysis, program diagnostics, Process control, Communication system control, software defect content estimations, Inspection, Programming, Control systems, maximum likelihood estimation, software reviews, maximum likelihood approach, Chaotic communication, defect detection, software inspections, Particle measurements, software engineering, statistical methods]
An approach to large-scale collection of application usage data over the internet
Proceedings of the 20th International Conference on Software Engineering
None
1998
Empirical evaluation of software systems in actual usage situations is critical in software engineering. Prototyping, beta testing, and usability testing are widely used to refine system requirements, detect anomalous or unexpected system and user behavior, and to evaluate software usefulness and usability. The World Wide Web enables cheap, rapid, and large-scale distribution of software for evaluation purposes. However, current techniques for collecting usage data have not kept pace with the opportunities presented by Web-based deployment. This paper presents an approach and prototype system that makes large-scale collection of usage data over the Internet a practical possibility. A general framework for comparing software monitoring systems is presented and used to compare the proposed approach to existing techniques.
[Software testing, System testing, software prototyping, prototyping, software monitoring systems, World Wide Web, software usefulness, usability testing, heuristic programming, Prototypes, system requirements, software engineering, Large-scale systems, Software prototyping, software systems evaluation, application usage data, Application software, beta testing, Software systems, Internet, Web sites, Usability, large-scale collection, internet, Software engineering]
Conceptual simplicity meets organizational complexity: case study of a corporate metrics program
Proceedings of the 20th International Conference on Software Engineering
None
1998
A corporate-wide metrics program faces enormous and poorly understood challenges as its implementation spreads out from the centralized planning body across many organizational boundaries into the sites where the data collection actually occurs. This paper presents a case study of the implementation of one corporate-wide program, focusing particularly on the unexpected difficulties of collecting a small number of straightforward metrics. Several mechanisms causing these difficulties are identified, including attenuated communication across organizational boundaries, inertia created by existing data collection systems, and the perceptions, expectations, and fears about how the data will be used. We describe how these factors influence the interpretation of the definitions of the measurements and influence the degree of conformance that is actually achieved. We conclude with lessons learned about both content and mechanisms to help in navigating the tricky waters of organizational dynamics in implementing a company-wide program.
[organizational complexity, Software maintenance, Computer aided software engineering, Data analysis, Navigation, Project management, software development management, organizational dynamics, conformance, Programming, technology transfer, corporate metrics, Software development management, Software metrics, company-wide program, Technology transfer, Planning, software metrics]
Using off-the-shelf middleware to implement connectors in distributed software architectures
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Software architectures promote development focused on modular building blocks and their interconnections. Since architecture-level components often contain complex functionality, it is reasonable to expect that their interactions will also be complex. Modeling and implementing software connectors thus becomes a key aspect of architecture-based development. Software interconnection and middleware technologies such as RMI, CORBA, ILU, and ActiveX provide a valuable service in building applications from components. The relation of such services to software connectors in the context of software architectures, however, is not well understood. To understand the tradeoffs among these technologies with respect to architectures, we have evaluated several off-the-shelf middleware technologies and identified key techniques for utilizing them in implementing software connectors. Our platform for investigation was C2, a component- and message-based architectural style. By encapsulating middleware functionality within software connectors, we have coupled C2's existing benefits such as component interchangeability, substrate independence and structural guidance with new capabilities of multi-lingual, multi-process and distributed application development in a manner that is transparent to architects.
[substrate independence, Protocols, multi-process application development, ActiveX, distributed application development, C2, software architecture, structural guidance, Software architecture, ILU, message-based architectural style, Computer architecture, distributed object management, modular building blocks, client-server systems, Object oriented modeling, LAN interconnection, Application software, Middleware, off-the-shelf middleware, RMI, Connectors, Computer science, CORBA, Computer languages, multi-lingual application development, distributed software architectures, software connectors, component-based architectural style, component interchangeability]
Exploiting ADLs to specify architectural styles induced by middleware infrastructures
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Architecture definition languages (ADLs) enable the formalization of the architecture of software systems and the execution of preliminary analyses on them. These analyses aim at supporting the identification and solution of design problems in the early stages of software development. We have used ADLs to describe middleware-induced architectural styles. These styles describe the assumptions and constraints that middleware infrastructures impose on the architecture of systems. Our work originates from the belief that the explicit representation of these styles at the architectural level can guide designers in the definition of an architecture compliant with a pre-selected middleware infrastructure, or, conversely can support designers in the identification of the most suitable middleware infrastructure for a specific architecture. In this paper we provide an evaluation of ADLs as to their suitability for defining middleware-induced architectural styles. We identify new requirements for ADLs, and we highlight the importance of existing capabilities. Although our experimentation starts from an attempt to solve a specific problem, the results we have obtained provide general lessons about ADLs, learned from defining the architecture of existing, complex, distributed, running systems.
[client-server systems, middleware infrastructures, architecture definition languages, architectural style specification, software development, high level languages, Programming, design problems, Middleware, formal specification, Information analysis, Computer science, software architecture, Software architecture, ADLs, Computer architecture, Permission, complex distributed systems, Software systems, Robustness, formalization, Performance analysis]
A flexible approach to alliances of complex applications
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Complex distributed environments contain thousands of workstations that can run hundreds of applications. Multiple networks are used to connect the workstations to dozens of behind-the-scenes servers, all of which are necessary for the user to perform even simple tasks. Such distributed environments are difficult to design and maintain, and current software engineering practices are not well adapted to deal with this inherent complexity. This paper describes the Single Glass Project in the Boeing Commercial Airplane Group. Single Glass expands the number of both internally developed and commercial-off-the-shelf applications a single user can access from any workstation. The result is in production use by over 6,000 people in both Puget Sound and Wichita. The key to success is a flexible and scaleable architecture that works within a complex, heterogeneous application and delivery system environment. Our work has focused on both computing technology (software and delivery systems) and general processes (primary technology, support, and organization). Agreements on both technology and process improvements are essential to project success. Single Glass is a work in progress, and we describe areas that require additional investigation. Our experience indicates that most technical problems can be addressed. However, significant improvement is needed in software engineering processes and practices to design and build applications and systems that can be tested, delivered, supported, and diagnosed when made available alongside other, independently developed applications and systems.
[workstation clusters, Software maintenance, Airplanes, open systems, Glass, process improvements, Network servers, software architecture, Production, Computer architecture, software process improvement, complex distributed environments, aerospace computing, software engineering, Workstations, workstation networks, flexible architecture, scaleable architecture, delivery systems, Application software, Single Glass Project, Boeing Commercial Airplane Group, Software systems, complex application alliances, Software engineering]
A systematic approach to derive the scope of software product lines
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Product line scoping is a critical activity because it elicits the common realms upon which the different products of a product line can be optimally engineered with respect to economies of scope. This, in turn, upper bounds the overall economic benefits that can be accrued from product line based development. Inherently, product line scoping is difficult because of the complexity of the factors that must be taken into account. Many are not known a priori. Traditional scoping approaches (from domain engineering) have focused on the notion of application domains. However, domains proved difficult to optimally scope and engineer from an enterprise standpoint because a domain captures extraneous elements that are of no interest to an enterprise which must focus on particular products, whether existing, under development, or anticipated. Hence, the domain view provides a flawed economic basis for making a scoping decision. We introduce PuLSE-Eco, a technique especially developed to address the aforementioned issues. Its main characteristics are: a complete product-centric orientation done via product maps, the separation of concerns achieved through the definition and operationalization of strategic business objectives, and last, diverse types of analyses performed upon product maps allowing scoping decisions based on these objectives. We illustrate the technique with a running example.
[application domains, product-centric orientation, Laboratories, Spine, enterprise, product line based development, economic benefits, Application software, Upper bound, economics, Software architecture, software product line scoping, PuLSE-Eco, product maps, cost-benefit analysis, strategic business objectives, Permission, software engineering, Performance analysis, Marine vehicles, business data processing, Software engineering, Business]
A language and environment for architecture-based software development and evolution
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Software architectures have the potential to substantially improve the development and evolution of large, complex, multi-lingual, multi-platform, long-running systems. However, in order to achieve this potential, specific techniques for architecture-based modeling, analysis, and evolution must be provided. Furthermore, one cannot fully benefit from such techniques unless support for mapping an architecture to an implementation also exists. This paper motivates and presents one such approach, which is an outgrowth of our experience with systems developed and evolved according to the C2 architectural style. We describe an architecture description language (ADL) specifically designed to support architecture-based evolution and discuss the kinds of evolution the language supports. We then describe a component-based environment that enables modeling, analysis, and evolution of architectures expressed in the ADL, as well as mapping of architectural models to an implementation infrastructure. The architecture of the environment itself can be evolved easily to support multiple ADLs, kinds of analyses, architectural styles, and implementation platforms. Our approach is fully reflexive: the environment can be used to describe, analyze, evolve, and (partially) implement itself, using the very ADL it supports. An existing architecture is used throughout the paper to provide illustrations and examples.
[Costs, high level languages, software architectures, C2 architectural style, Programming, architecture-based software evolution, Environmental economics, architectural models, implementation platforms, Computer science, Connectors, architecture-based analysis, software architecture, Software architecture, architecture-based software development, Computer architecture, Permission, large complex multi-lingual multi-platform long-running systems, architecture description language, Architecture description languages, Software tools, architecture-based modeling, component-based environment]
Experience with performing architecture tradeoff analysis
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Software architectures, like complex designs in any field, embody tradeoffs made by the designers. However, these tradeoffs are not always made explicitly by the designers and they may not understand the impacts of their decisions. This paper describes the use of a scenario-based and model-based analysis technique for software architectures-called ATAM-that not only analyzes a software architecture with respect to multiple quality attributes, but explicitly considers the tradeoffs inherent in the design. This is a method aimed at illuminating risks in the architecture through the identification of attribute trends, rather than at precise characterizations of measurable quality attribute values. In this paper, the operationalization of ATAM is illustrated via a specific example in which we analyzed a U.S. Army system for battlefield management.
[Availability, risk management, ATAM, software development management, multiple quality attribute, Time measurement, software quality, model-based analysis technique, Security, Risk analysis, software architecture tradeoff analysis, software architecture, Software architecture, risks, battlefield management, Computer architecture, Software quality, scenario-based analysis technique, Performance analysis, Software measurement, military computing, US Army system, Software engineering]
Exploiting Smalltalk modules in a customizable programming environment
Proceedings of the 1999 International Conference on Software Engineering
None
1999
This paper describes how we extended a module structure of the Smalltalk LearningWorks framework to provide a programming environment designed for very large scale technology transfer. The 'module' is what we have termed the LearningBook, a set of classes and persistent objects, including an HTML browser, programming and visualization tools, and microworlds. The context for this development is a distance learning university course in object technology which is enrolling over 5000 mature students per year-making it the largest such course in the world. While promoting a systems building approach, we have successfully added support for programming in the small and the needs of the isolated novice. Two guiding principles have been: (i) the environment and its modules fit into a consistent structure for personal management of learning and (ii) details of complex facilities, such as the class library, are progressively disclosed as knowledge and sophistication grow. The paper shows how these principles have guided the exploitation of LearningBook modules. To provide context, relevant academic background is given. Early informal feedback is reported and a project currently underway to observe in detail how thousands of learners use the programming environment is sketched.
[Visualization, personal learning management, mature students, classes, HTML, distance learning university course, object technology, Environmental management, programming tools, software libraries, Smalltalk modules, Smalltalk, Technology transfer, microworlds, Isolation technology, Large-scale systems, software tools, class library, courseware, computer science education, object-oriented programming, Buildings, customizable programming environment, isolated novice, technology transfer, Knowledge management, visualization tools, Programming environments, distance learning, Computer aided instruction, Smalltalk LearningWorks framework, very large scale technology transfer, program visualisation, programming environments, HTML browser, persistent objects, systems building approach]
Software engineering issues for ubiquitous computing
Proceedings of the 1999 International Conference on Software Engineering
None
1999
In the last decade, we have experienced the advent of the paradigm of ubiquitous computing, with the goal of making computational services so pervasive throughout an environment that they become transparent to the human user. Research in ubiquitous computing raises many challenging issues for computer science in general, but successful research in ubiquitous computing requires the deployment of applications that can survive everyday use, and this in itself presents a great software engineering challenge. In our experience, we have found three features common across many ubiquitous computing applications-transparent interfaces that provide appropriate alternatives to the desktop bound traditional graphical user interface, the ability to modify behavior of a application based on knowledge of its context of use, and the ability to capture live experiences for later recall. Building ubiquitous computing applications with these features raises software engineering problems in toolkit design, software structuring for separation of concerns and component integration. We clarify these problems and discuss our approaches towards their solution.
[Pervasive computing, Buildings, transparent interfaces, Humans, Ubiquitous computing, user interfaces, Application software, ubiquitous computing, toolkit design, Computer science, Software design, component integration, software structuring, computational services, application behaviour modification, software engineering, live experiences, Software tools, Software engineering, Graphical user interfaces]
Avoiding packaging mismatch with flexible packaging
Proceedings of the 1999 International Conference on Software Engineering
None
1999
To integrate a software component into a system, it must interact properly with the system's other components. Unfortunately, the decisions about how a component is to interact with other components are typically committed long before the moment of integration and are difficult to change. This paper introduces the flexible packaging method, which allows a component developer to defer some decisions about component interaction until system integration time. The method divides the component's source into two pieces: the ware, which encapsulates the component's functionality; and the packager, which encapsulates the details of interaction. Both the ware and the packager are independently reusable. A ware, as a reusable part, allows a given piece of functionality to be employed in systems in different architectural styles. A packager, as a reusable part, encapsulates conformance to a component standard, like an ActiveX control or an ODBC database accessor. Because the packager's source code is often formulaic, a tool is provided to generate the packager's source from a high-level description of the intended interaction, a description written in the architectural description language UniCon. The method and tools are evaluated with two case studies, an image viewer and a database updater.
[Heart, System testing, architectural description language, ware, reusable part, packaging mismatch, component standard conformance, software component integration, ActiveX control, system integration, ODBC database accessor, Permission, UniCon, high-level description, source code, Maintenance engineering, component functionality, packager, database updater, Image databases, architectural styles, Packaging, software reusability, flexible packaging, Architecture description languages, Software tools, image viewer]
N degrees of separation: multi-dimensional separation of concerns
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Done well, separation of concerns can provide many software engineering benefits, including reduced complexity, improved reusability, and simpler evolution. The choice of boundaries for separate concerns depends on both requirements on the system and on the kind(s) of decomposition and composition a given formalism supports. The predominant methodologies and formalisms available, however, support only orthogonal separations of concerns, along single dimensions of composition and decomposition. These characteristics lead to a number of well-known and difficult problems. The paper describes a new paradigm for modeling and implementing software artifacts, one that permits separation of overlapping concerns along multiple dimensions of composition and decomposition. This approach addresses numerous problems throughout the software lifecycle in achieving well-engineered, evolvable, flexible software artifacts and traceability across artifacts.
[hyperslices, Software maintenance, complexity, Costs, software prototyping, separate concerns, Laboratories, software lifecycle, reusability, software quality, software engineering benefits, hypermodules, software artifacts, software decomposition, Production, system requirements, Large-scale systems, program slicing, Contracts, orthogonal separations, multi-dimensional separation of concerns, Rivers, evolution, traceability, overlapping concerns, systems analysis, Software quality, software reusability, Software systems, multiple dimensions, programming environments, Software engineering]
An initial assessment of aspect-oriented programming
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The principle of separation of concerns (E. Dijkstra, 1976) has long been used by software engineers to manage the complexity of software system development. Programming languages help software engineers explicitly maintain the separation of some concerns in code. As another step towards increasing the scope of concerns that can be captured cleanly within the code, G. Kiczales et al. (1997) have introduced aspect oriented programming. In aspect oriented programming, explicit language support is provided to help modularize design decisions that cross-cut a functionally decomposed program. Aspect oriented programming is intended to make it easier to reason about, develop, and maintain certain kinds of application code. To investigate these claims, we conducted two exploratory experiments that considered the impact of aspect oriented programming, as found in AspectJ version 0.1, on two common programming activities: debugging and change. Our experimental results provide insights into the usefulness and usability of aspect oriented programming. Our results also raise questions about the characteristics of the interface between aspects and functionally decomposed core code that are necessary to accrue programming benefits. Most notably, the separation provided by aspect oriented programming seems most helpful when the interface is narrow (i.e., the separation is more complete); partial separation does not necessarily provide partial benefit.
[Software maintenance, program debugging, aspect oriented programming, software change, software quality, programming languages, partial separation, software system development, design decisions, Software design, Engineering management, initial assessment, functionally decomposed program, Permission, debugging, object oriented programming, Functional programming, AspectJ, software engineers, object-oriented programming, Maintenance engineering, explicit language support, application code maintenance, Software development management, Computer science, Computer languages, functionally decomposed core code, programming benefits, object-oriented languages, separation of concerns, Usability]
Using the ASTRAL model checker to analyze Mobile IP
Proceedings of the 1999 International Conference on Software Engineering
None
1999
ASTRAL is a high level formal specification language for real time systems. It is provided with structuring mechanisms that allow one to build modularized specifications of complex real time systems with layering. The ASTRAL model checker checks the satisfiability of critical requirements of a specification by enumerating possible runs of transitions within a given time bound. The paper discusses the mechanism of the model checker and how it can be used to analyze encryption protocols. Several classic benchmarks have been investigated, including the Needham-Schroeder public-key authentication protocol (R.M. Needham and M.D. Schroeder, 1978) and the TMN protocol, and a number of attacks were uncovered. The paper focuses on using ASTRAL to specify Mobile IP (C. Perkins, 1996) and testing the specification using the model checker.
[Real time systems, program verification, computability, layering, formal specification, mobile IP analysis, mobile computing, public key cryptography, satisfiability, Prototypes, specification languages, Permission, Cryptography, protocols, ASTRAL model checker, Needham-Schroeder public-key authentication protocol, Testing, critical requirements, high level formal specification language, time bound, Specification languages, Formal specifications, Cryptographic protocols, TMN protocol, modularized specifications, encryption protocols, Public key, real-time systems, message authentication, structuring mechanisms, Internet, real time systems, Mobile computing]
Decoupling synchronization from local control for efficient symbolic model checking of statecharts
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Symbolic model checking is a powerful formal verification technique for reactive systems. We address the problem of symbolic model checking for software specifications written as statecharts. We concentrate on how the synchronization of statecharts relates to the efficiency of model checking. We show that statecharts synchronized in an oblivious manner, such that the synchronization and the local control are decoupled, tend to be easier for symbolic analysis. Based on this insight, the verification of some non-oblivious systems can be optimized by a simple, transparent modification to the model to separate the synchronization from the local control. The technique enabled the analysis of the statecharts model of a fault tolerant electrical power distribution system developed by the Boeing Commercial Airplane Group. The results disclosed subtle modeling and logical flaws not found by simulation.
[Airplanes, program verification, reactive systems, formal verification technique, statecharts model, formal specification, symbolic analysis, Fault tolerance, binary decision diagrams, Boolean functions, Fault tolerant systems, transparent modification, symbolic model checking, software specification, logical flaws, local control, software specifications, Data structures, Power system modeling, software fault tolerance, synchronisation, Computer science, fault tolerant electrical power distribution system, Boeing Commercial Airplane Group, non-oblivious systems, Computer industry, power distribution, statechart synchronization, Power engineering and energy, Formal verification]
Analysis of a scheduler for a CAD framework
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The experience report describes a case study in which a key component of a software system was modeled and analyzed to better understand a proposed algorithm prior to implementation. A Promela model of a linear scheduler for a CAD framework was developed. The Spin simulator was used to debug the model and, later, to illustrate how the algorithm works in different scenarios. Additionally, the Spin verifier was used to check various safety and liveness properties. The study revealed a deficiency with the algorithm, as originally proposed. Subsequently, the modeling tools provided by Spin were used in devising solutions to the problems. Finally, the Promela model was modified and verified to be correct. The actual implementation of the scheduler involves a significant amount of message passing, multiple execution threads, and potentially huge data structures. By focusing on the interfaces between threads, restricting the system scope, and abstracting details of data structures and irrelevant computations, a very simple model was obtained, which nevertheless provides an accurate representation of the communication between threads. The paper describes the steps that were abstracted and highlights the restrictions imposed on the model.
[Algorithm design and analysis, program debugging, program verification, linear scheduler, simple model, irrelevant computations, Yarn, software system, processor scheduling, scheduler analysis, Analytical models, liveness properties, Permission, debugging, data structures, multiple execution threads, message passing, multi-threading, Software algorithms, Citation analysis, CAD framework, Promela model, CAD, Data structures, Spin verifier, Power system modeling, Scheduling algorithm, case study, Processor scheduling, Spin simulator, huge data structures]
Dynamic layout of distributed applications in FarGo
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The design of efficient and reliable distributed applications that operate in large networks, over links with varying capacities and loads, demands new programming abstractions and mechanisms. The conventional static design-time determination of local-remote relationships between components implies that (dynamic) environmental changes are hard if not impossible to address without reengineering. The paper presents a novel programming model that is centered around the concept of "dynamic application layout\
[dynamic application layout, FarGo system, programming abstractions, reflective inter-component reference, Reliability engineering, type theory, Vehicle dynamics, Distributed computing, distributed applications, event based scripting language, application performance, Vehicles, static design-time determination, Intelligent networks, Runtime, mobile computing, authoring languages, programming model, layout programming, Cities and towns, re-location semantics, component location, FarGo applications, distributed programming, Java, Logic programming, event monitoring service, reengineering, Application software, programming language semantics, local-remote relationships, large networks, dynamic environmental changes, application logic, abstraction vehicle, reference types]
A cooperative approach to support software deployment using the Software Dock
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Software deployment is an evolving collection of interrelated processes such as release, install, adapt, reconfigure, update, activate, deactivate, remove, and retire. The connectivity of large networks, such as the Internet, is affecting how software deployment is performed. It is necessary to introduce new software deployment technologies that leverage this connectivity. The Software Dock framework creates a distributed, agent based deployment framework to support the ongoing cooperation and negotiation among software producers themselves and among software producers and software consumers. This deployment framework is enabled by the use of a standardized deployment schema for describing software systems, called the Deployable Software Description (DSD) format. The Software Dock also employs agents to traverse between software producers and consumers in order to perform software deployment activities by interpreting the descriptions of software systems. The Software Dock infrastructure allows software producers to offer their customers high level deployment services that were previously not possible.
[CD-ROMs, Laboratories, Software Dock framework, Software performance, Software Dock infrastructure, evolving collection, mobile computing, Mobile agents, software deployment support, software deployment activities, mobile agents, interrelated processes, Permission, IP networks, distributed programming, software consumers, Java, object-oriented programming, software deployment technologies, software producers, high level deployment services, distributed agent based deployment framework, standardized deployment schema, large networks, configuration management, Software systems, cooperative approach, Internet, Deployable Software Description format, Context modeling, Software engineering]
Information survivability control systems
Proceedings of the 1999 International Conference on Software Engineering
None
1999
We address the dependence of critical infrastructures-including electric power, telecommunications, finance and transportation-on vulnerable information systems. Our approach is based on the notion of control systems. We envision hierarchical, adaptive, multiple model, discrete state distributed control systems to monitor infrastructure information systems and respond to disruptions (e.g., security attacks) by changing operating modes and design configurations to minimize loss of utility. To explore and evaluate our approach, we have developed a toolkit for building distributed dynamic models of infrastructure information systems. We used this toolkit to build a model of a simple subset of the United States payment system and a control system for this model information system.
[software reliability, vulnerable information systems, distributed dynamic models, Control systems, Telecommunication control, Adaptive control, Information systems, telecommunications, infrastructure information systems, information systems, financial data processing, Monitoring, computerised control, information survivability control systems, security attacks, infrastructure information systems monitoring, distributed control, Finance, discrete state distributed control systems, adaptive control, Power system modeling, model information system, electric power, transportation, Programmable control, United States payment system, Information security, Distributed control, design configurations, system monitoring, critical infrastructures, operating modes, government data processing, finance]
Lightweight extraction of object models from bytecode
Proceedings of the 1999 International Conference on Software Engineering
None
1999
A program's object model captures the essence of its design. For some programs, no object model was developed during design; for others, an object model exists but may be out of sync with the code. The paper describes a tool that automatically extracts an object model from the class files of a Java program. Although the tool performs only a simple, heuristic analysis that is almost entirely local, the resulting object model is surprisingly accurate. The paper explains the form of the object model, the assumptions upon which the analysis is based, and its limitations, and evaluates the tool on a suite of sample programs.
[bytecode, Java, Visualization, object-oriented programming, Womble, Blob, program object model, Laboratories, Containers, lightweight extraction, graph layout tool, Computer science, heuristic programming, sample programs, class files, lightweight analysis, Permission, Java program, module dependence diagrams, file organisation, Performance analysis, object model extraction, heuristic analysis]
Highly reliable upgrading of components
Proceedings of the 1999 International Conference on Software Engineering
None
1999
After a system is deployed, fixes, enhancements, and modifications all occur that change the components that make up the system. Unfortunately, new versions of components can introduce new errors and break existing, depended-upon behavior. When this happens, the old component version could have provided the correct behavior, but it is no longer part of the system. We propose a framework, HERCULES, for upgrading system components that, instead of removing the old version of the component, keeps multiple versions of a component running. Doing so allows behavior to be utilized from all versions, and maintains system integrity and correctness even in the presence of newly introduced errors. This framework ensures that the move towards dynamic, configurable software systems does not lessen, but rather provides capabilities to enhance the reliability that software will achieve through the next century.
[Java, object-oriented programming, correctness, highly reliable component upgrading, depended-upon behavior, software reliability, Maintenance, dynamic configurable software systems, software fault tolerance, Computer science, configuration management, Software architecture, HERCULES, system components, Permission, Computer errors, Software systems, Libraries, component version, version management, system integrity, multiple versions]
Dynamically discovering likely program invariants to support program evolution
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Explicitly stated program invariants can help programmers by identifying program properties that must be preserved when modifying code. In practice, however, these invariants are usually implicit. An alternative to expecting programmers to fully annotate code with invariants is to automatically infer invariants from the program itself. This research focuses on dynamic techniques for discovering invariants from execution traces. This paper reports two results. First, it describes techniques for dynamically discovering invariants, along with an instrumenter and an inference engine that embody these techniques. Second, it reports on the application of the engine to two sets of target programs. In programs from Cries's work on program derivation, we rediscovered predefined invariants. In a C program lacking explicit invariants, we discovered invariants that assisted a software evolution task.
[program property preservation, formal specification, Engines, Runtime, inference engine, predefined invariants, code modification, Permission, Pattern analysis, Testing, dynamically discovered program invariants, Pattern recognition, Application software, Formal specifications, software maintenance, inference mechanisms, Programming profession, automatically inferred invariants, Computer science, execution trace, instrumenter, program evolution, explicitly stated program invariants, C program, program derivation]
Pattern-based reverse-engineering of design components
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Many reverse-engineering tools have been developed to derive abstract representations from source code. Yet, most of these tools completely ignore recovery of the all-important rationale behind the design decisions that have lead to its physical shape. Design patterns capture the rationale behind proven design solutions and discuss the trade-offs among their alternatives. We argue that it is these patterns of thought that are at the root of many of the key elements of large-scale software systems, and that, in order to comprehend these systems, we need to recover and understand the patterns on which they were built. In this paper, we present our environment for the reverse engineering of design components based on the structural descriptions of design patterns. We give an overview of the environment, explain three case studies, and discuss how pattern-based reverse-engineering helped gain insight into the design rationale of some of the pieces of three large-scale C++ software systems.
[structural descriptions, Visualization, Java, object-oriented programming, Shape, Reverse engineering, source code, reverse engineering, Application software, design decisions, Design engineering, design patterns, pattern-based reverse engineering, abstract representations, large-scale C++ software systems, Software systems, software engineering, Large-scale systems, object-oriented methods, Pattern analysis, State estimation, design components]
Automatic method refactoring using weighted dependence graphs
Proceedings of the 1999 International Conference on Software Engineering
None
1999
While refactoring makes frameworks more reusable, it is complex to do by hand. This paper presents a mechanism that automatically refactors methods in object-oriented frameworks by using weighted dependence graphs, whose edges are weighted based on the modification histories of the methods. To find the appropriate boundary between frozen spots and hot spots in the methods, the value of the weight varies based on whether the dependence in the original methods has been repeatedly preserved or destroyed in the methods of applications created by programmers. The mechanism constructs both template methods that contain the invariant dependence and hook methods that are separated by eliminating the variant dependence. The new template methods and hook methods tailored to each programmer save him/her from writing superfluous code when reusing a framework. Experimental results show a reduction rate of up to 22% in the number of statements a programmer has to write when creating several applications; this percentage is double that achievable by a conventional refactoring technique.
[automatic method refactoring, object-oriented programming, Collaborative software, Laboratories, weighted dependence graphs, framework reuse, invariant dependence, template methods, modification histories, statements, Application software, History, frozen spots, Programming profession, hook methods, Writing, software reusability, object-oriented frameworks, hot spots, Software systems, Concrete, Software reusability, Object oriented programming]
Identifying objects using cluster and concept analysis
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Many approaches to support (semi-automatic) identification of objects in legacy code take data structures as the starting point for candidate classes. Unfortunately, legacy data structures tend to grow over time, and may contain many unrelated fields at the time of migration. We propose a method for identifying objects by semi-automatically restructuring the legacy data structures. Issues involved include the selection of record fields of interest, the identification of procedures actually dealing with such fields, and the construction of coherent groups of fields and procedures into candidate classes. We explore the use of cluster and concept analysis for the purpose of object identification, and we illustrate their effect on a 100000 LOC Cobol system. Furthermore, we use these results to contrast clustering with concept analysis techniques.
[legacy code, object-oriented programming, semi-automatic restructuring, COBOL, Inspection, Cobol system, Data structures, Entropy, software maintenance, Loans and mortgages, legacy data structures, Permission, Software systems, Lab-on-a-chip, US Department of Commerce, concept analysis, Database systems, Robustness, data structures, object identification, record field selection, cluster analysis, pattern recognition]
Using a goal-driven approach to generate test cases for GUIs
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The widespread use of GUIs for interacting with software is leading to the construction of more and more complex GUIs. With the growing complexity comes challenges in testing the correctness of a GUI and the underlying software. We present a new technique to automatically generate test cases for GUIs that exploits planning, a well developed and used technique in artificial intelligence. Given a set of operators, an initial state and a goal state, a planner produces a sequence of the operators that will change the initial state to the goal state. Our test case generation technique first analyzes a GUI and derives hierarchical planning operators from the actions in the GUI. The test designer determines the preconditions and effects of the hierarchical operators, which are then input into a planning system. With the knowledge of the GUI and the way in which the user will interact with the GUI, the test designer creates sets of initial and goal states. Given these initial and final states of the GUI, a hierarchical planner produces plans, or a set of test cases, that enable the goal state to be reached. Our technique has the additional benefit of putting verification commands into the test cases automatically. We implemented our technique by developing the GUI analyzer and extending a planner. We generated test cases for Microsoft's Word-Pad to demonstrate the viability and practicality of the approach.
[Software testing, verification commands, System testing, Computer aided software engineering, program testing, graphical user interfaces, software, initial state, Microsoft Word-Pad, Application software, goal-driven approach, artificial intelligence, Computer science, planning (artificial intelligence), Automatic testing, Computer bugs, operators, goal state, Permission, GUIs, automatic test case generation, hierarchical planning operators, Software tools, Graphical user interfaces]
Lutess: a specification-driven testing environment for synchronous software
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Several studies have shown that automated testing is a promising approach to save significant amounts of time and money in the industry of reactive software. But automated testing requires a formal framework and adequate means to generate test data. In the context of synchronous reactive software, we have built such a framework and its associated tool-Lutess-to integrate various well-founded testing techniques. This tool automatically constructs test harnesses for fully automated test data generation and verdict return. The generation conforms to different formal descriptions: software environment constraints, functional and safety-oriented properties to be satisfied by the software, software operational profiles and software behavior patterns. These descriptions are expressed in an extended executable temporal logic. They correspond to more and more complex test objectives raised by the first pre-industrial applications of Lutess. This paper concentrates on the latest development of the tool and its use in the validation of standard feature specifications in telephone systems. The four testing techniques which are coordinated in Lutess uniform framework are shown to be well-suited to efficient software testing. The lessons learnt from the use of Lutess in the context of industrial partnerships are discussed.
[Software testing, System testing, program testing, program verification, synchronous reactive software, temporal logic, Software safety, Lutess tool, formal specification, telecommunication computing, specification-driven testing environment, software tools, fully automated test data generation, extended executable temporal logic, validation, software environment constraints, software testing, Telecommunications, software operational profiles, telephone systems, Application software, Formal specifications, fully automated verdict return, functional properties, safety-oriented properties, Automatic testing, standard feature specifications, Software systems, Computer industry, software behavior patterns, formal descriptions, test harnesses, Formal verification]
Residual test coverage monitoring
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Structural coverage criteria are often used as an indicator of the thoroughness of testing, but complete satisfaction of a criterion is seldom achieved. When a software product is released with less than 100% coverage, testers are explicitly or implicitly assuming that executions satisfying the remaining test obligations (the residue) are either infeasible or occur so rarely that they have negligible impact on quality. Violation of this assumption indicates shortcomings in the testing process. Monitoring in the deployed environment, even in the beta test phase, is typically limited to error and sanity checks. Monitoring the residue of test coverage in actual use can provide additional useful information, but it is unlikely to be accepted by users unless its performance impact is very small. Experience with a prototype tool for residual test coverage monitoring of Java programs suggests that, at least for statement coverage, the simple strategy of removing all probes except those corresponding to the residue of coverage testing reduces execution overhead to acceptably low levels.
[Software testing, Performance evaluation, System testing, Java, program testing, Computerized monitoring, Instruments, beta test phase, structural coverage criteria, statement coverage, Quality assurance, Feedback, Software quality, error checks, residual test coverage monitoring, system monitoring, execution overhead, software product, Probes, sanity checks, Java programs]
Model-based testing in practice
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Model-based testing is a new and evolving technique for generating a suite of test cases from requirements. Testers using this approach concentrate on a data model and generation infrastructure instead of hand-crafting individual tests. Several relatively small studies have demonstrated how combinatorial test generation techniques allow testers to achieve broad coverage of the input domain with a small number of tests. We have conducted several relatively large projects in which we applied these techniques to systems with millions of lines of code. Given the complexity of testing, the model-based testing approach was used in conjunction with test automation harnesses. Since no large empirical study has been conducted to measure efficacy of this new approach, we report on our experience with developing tools and methods in support of model-based testing. The four case studies presented here offer details and results of applying combinatorial test-generation techniques on a large scale to diverse applications. Based on the four projects, we offer our insights into what works in practice and our thoughts about obstacles to transferring this technology into testing organizations.
[Software testing, System testing, test case suite generation, data model, Costs, requirements, program testing, generation infrastructure, test automation harnesses, Telecommunications, Maintenance, automatic testing, combinatorial test generation techniques, Automatic testing, Permission, model-based testing, Data models, Large-scale systems, Contracts]
Software architecture classification for estimating the cost of COTS integration
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The use of commercial-off-the-shelf (COTS) products creates a software integration problem, whether a single COTS software component is being integrated into a software system, or the whole system is being built primarily from COTS products. This integration may require considerable effort and affect system quality. A good estimate of integration cost can help in the decision of whether or not to use a COTS solution, the selection of the best COTS products, and determine the amount and type of glueware that needs to be built, in this paper, we introduce a set of variables that have the potential to estimate the integration cost. We present a classification scheme of software architectures with respect to the integration of COTS products. The scheme is based on inter-component interactions within software architectures. The classification scheme allows the comparison of integration costs of different COTS products relative to different software architectures.
[Costs, Programming, Educational institutions, software integration problem, Computer science, software architecture, Software architecture, Databases, inter-component interactions, glueware, integrated software, commercial-off-the-shelf products, Computer architecture, Software quality, software architecture classification, Permission, Software systems, software cost estimation, COTS integration cost estimation, system quality]
Explaining the cost of European space and military projects
Proceedings of the 1999 International Conference on Software Engineering
None
1999
There has been much controversy in the literature on several issues underlying the construction of parametric software development cost models. For example, it has been argued whether (dis)economies of scale exist in software production, what functional form should be assumed between effort and product size, whether COCOMO factors were useful, and whether the COCOMO factors are independent. Answers to such questions should help software organizations define suitable data collection programs and well-specified cost models. We use a data set collected by the European Space Agency to perform such an investigation. To ensure a certain degree of consistency in our data, we focus our analysis on a set of space and military projects that represent an important application domain and the largest subset in the database. These projects have been performed, however, by a variety of organizations. First, our results indicate that two functional forms are plausible between effort and product size: linear and log-linear. This also means that different project subpopulations are likely to follow different functional forms. Second, besides product size, the strongest factor influencing cost appears to be team size. Larger teams result in substantially lower productivity, which is interesting considering this attribute is rarely collected in software engineering cost databases. Third, although some COCOMO factors appear to be useful and significant covariates, they play a minor role in explaining project effort.
[Costs, Programming, data collection programs, COCOMO factors, software production, product size, Databases, productivity, Embedded system, Production, Permission, aerospace computing, European military project cost, military computing, Productivity, project management, European Space Agency, software development management, European space project cost, team size, software engineering cost databases, parametric software development cost models, project subpopulations, Economies of scale, software cost estimation, Time factors, software organizations, Software engineering]
An assessment and comparison of common software cost estimation modeling techniques
Proceedings of the 1999 International Conference on Software Engineering
None
1999
This paper investigates two essential questions related to data-driven, software cost modeling: (1) What modeling techniques are likely to yield more accurate results when using typical software development cost data? and (2) What are the benefits and drawbacks of using organization-specific data as compared to multi-organization databases? The former question is important in guiding software cost analysts in their choice of the right type of modeling technique, if at all possible. In order to address this issue, we assess and compare a selection of common cost modeling techniques fulfilling a number of important criteria using a large multi-organizational database in the business application domain. Namely, these are: ordinary least squares regression, stepwise ANOVA, CART, and analogy. The latter question is important in order to assess the feasibility of using multi-organization cost databases to build cost models and the benefits gained from local, company-specific data collection and modeling. As a large subset of the data in the multi-company database came from one organization, we were able to investigate this issue by comparing organization-specific models with models based on multi-organization data. Results show that the performances of the modeling techniques considered were not significantly different, with the exception of the analogy-based models which appear to be less accurate. Surprisingly, when using standard cost factors (e.g., COCOMO-like factors, Function Points), organization specific models did not yield better results than generic, multi-organization models.
[Programming, ordinary least squares regression, multi-organizational database, Yield estimation, Application software, software cost modeling, Least squares methods, Databases, data-driven, stepwise ANOVA, Cost function, software cost estimation, statistical analysis, Analysis of variance, Regression tree analysis, Software engineering, Business, CART]
Using version control data to evaluate the impact of software tools
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Software tools can improve the quality and maintainability of software, but are expensive to acquire, deploy and maintain, especially in large organizations. We explore how to quantify the effects of a software tool once it has been deployed in a development environment. We present a simple methodology for tool evaluation that correlates tool usage statistics with estimates of developer effort, as derived from a project's change history (version control system). Our work complements controlled experiments on software tools, which usually take place outside the industrial setting, and tool assessment studies that predict the impact of software tools before deployment. Our analysis is inexpensive, non-intrusive and can be applied to an entire software project in its actual setting. A key part of our analysis is how to control confounding variables such as developer work-style and experience in order accurately to quantify the impact of a tool on developer effort. We demonstrate our method in a case study of a software tool called VE, a version-sensitive editor used in BellLabs. VE aids software developers in coping with the rampant use of preprocessor directives (such as if/ endif) in C source files. Our analysis found that developers were approximately 36% more productive when using VE than when using standard text editors.
[Software maintenance, Industrial control, version control, Control systems, History, Statistics, VE, configuration management, development environment, software tool, Software quality, Computer industry, Electrical equipment industry, software tools, Standards development, Software tools, tool evaluation]
Polymorphism measures for early risk prediction
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Polymorphism is an essential feature of the object-oriented paradigm. However, polymorphism induces hidden forms of class dependencies, which may impact software quality. In this paper, we define and empirically investigate the quality impact of polymorphism on OO design. We define measures of two main aspects of polymorphic behaviors provided by the C++ language: polymorphism based on compile time linking decisions (overloading functions for example) and polymorphism based on run-time binding decisions (virtual functions for example). Then, we validate our measures by evaluating their impact on class fault-proneness, a software quality attribute. The results show that our measures are capturing different dimensions than LOC a size measure, as well as they are significant predictors of fault proneness. In fact, we show that they constitute a good complement to the existing OO design measures.
[Encapsulation, object-oriented paradigm, Size measurement, Time measurement, polymorphism, software quality, class fault-proneness, Computer languages, early risk prediction, software quality attribute, Runtime, OO design, Software quality, Permission, Lab-on-a-chip, object-oriented methods, Software measurement, Joining processes, software metrics]
Investigating quality factors in object-oriented designs: an industrial case study
Proceedings of the 1999 International Conference on Software Engineering
None
1999
This paper aims at empirically exploring the relationships between most of the existing coupling and cohesion measures for object-oriented (OO) systems, and the fault-proneness of OO system classes. The underlying goal of such a study is to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. The study described here is a replication of an analogous study conducted in an university environment with systems developed by students. In order to draw more general conclusions and to (dis)confirm the results obtained there, we now replicated the study using data collected on an industrial system developed by professionals. Results show that many of our findings are consistent across systems, despite the very disparate nature of the systems under study. Some of the strong dimensions captured by the measures in each data set are visible in both the university and industrial case study. For example, the frequency of method invocations appears to be the main driving factor of fault-proneness in all systems. However, there are also differences across studies which illustrate the fact that quality does not follow universal laws and that quality models must be developed locally, wherever needed.
[Performance evaluation, Computer aided software engineering, object-oriented programming, Industrial relations, Object oriented modeling, OO system classes, software quality, quality factors, Software quality, Permission, Computer industry, Frequency, industrial case study, OO systems, Software measurement, object-oriented designs, Software engineering]
Agent system development method based on agent patterns
Proceedings of the 1999 International Conference on Software Engineering
None
1999
As wide-area open networks such as the Internet and intranets grow larger, agent technology is attracting more attention. Agents are units of software that can deal with environmental changes and the various requirements of open networks through features such as autonomy, mobility, intelligence, cooperation, and reactivity. However, since the usual development method of the agent systems is not sufficiently investigated, the technology is not yet widespread. This paper proposes a method of agent system development based on agent patterns that represent typical and recurring structures and behaviors of agents. The agent patterns are classified according to their appropriate architectural levels and the degree to which they depend on specific agent platforms. Our method enables developers to design agent systems efficiently since they can construct complicated system architectures and behaviors by dividing the design process into two architectural levels and applying the appropriate agent patterns. In addition, the higher level designs are independent of specific agent platforms and can be therefore easily reused.
[system architectures, Information resources, Microarchitecture, agent system development, Mobile agents, Laboratories, Software systems, Maintenance, agent patterns, software agents, Research and development]
LIME: Linda meets mobility
Proceedings of the 1999 International Conference on Software Engineering
None
1999
LIME is a system designed to assist in the rapid development of dependable mobile applications over both wired and ad hoc networks. Mobile agents reside on mobile hosts and all communication takes place via transiently shared tuple spaces distributed across the mobile hosts. The decoupled style of computing characterizing the Linda model is extended to the mobile environment. At the application level, both agents and hosts perceive movement as a sudden change of context. The set of tuples accessible by a particular agent residing on a given host is altered transparently in response to changes in the connectivity pattern among the mobile hosts. In this paper we present the key design concepts behind the LIME system.
[Pervasive computing, Telecommunication traffic, mobile environment, dependable mobile applications, Ad hoc networks, rapid development, Distributed computing, software agents, Linda, Wireless communication, Network servers, mobile computing, Mobile agents, LIME, mobile agents, Permission, Computer networks, Mobile computing]
Adding more "DL" to IDL: towards more knowledgeable component inter-operability
Proceedings of the 1999 International Conference on Software Engineering
None
1999
In an open component market place, interface description languages (IDLs), such as CORBA's, provide for the consumer only a weak guarantee (concerning type signatures) that a software service will work in a particular context as anticipated. Stronger guarantees, regarding the intended semantics of the service, would help, especially if formalized in a language that allowed effective, automatic and static checking of compatibility between the server and the client's service descriptions. We propose an approach based on a family of formalisms called description logics (DLs), providing three examples of the use of DLs to augment IDL: (1) for the CORBA Cos Relationship service; (2) for capturing information models described using STEP Express, the ISO standard language used in the manufacturing domain (and a basis of the OMG PDM effort); and (3) constraints involving methods. While traditional formal specification techniques are more powerful, DLs offer certain advantages: they have decidable, even efficient reasoning algorithms, yet they still excel at modeling natural domains, and are thus well-suited for specifying application and domain-specific services.
[Context-aware services, decidable, Dictionaries, interface description languages, reasoning, Drives, Application software, Formal specifications, formal specification, natural domains, Computer science, CORBA, description logics, Permission, Software standards, Logic, Virtual manufacturing, compatibility, distributed object management]
A specification matching based approach to reverse engineering
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Specification Matching is a technique that has been used to retrieve reusable components from reuse libraries. The relationship between a query specification and a library specification is typically based on refinement, where a library specification matches a query specification if the library specification is more detailed than the query specification. Reverse engineering is a process of analyzing components and component interrelationships in order to construct descriptions of a system at a higher level of abstraction. In this paper, we define the concept of an abstraction match as a basis for reverse engineering and show how the abstraction match can be used to facilitate a process for generalizing specifications. Finally, we apply the specification generalization technique to a portion of a NASA JPL ground-based mission control system for unmanned flight systems.
[Software maintenance, reusable components, abstraction match, Reverse engineering, NASA, Software performance, Control systems, reverse engineering, software maintenance, formal specification, Computer science, Computer languages, specification matching, reuse libraries, Biomedical applications of radiation, software reusability, query specification, Software systems, Libraries, library specification]
Data flow analysis for checking properties of concurrent Java programs
Proceedings of the 1999 International Conference on Software Engineering
None
1999
In this paper we show how the FLAVERS data flow analysis technique, originally formulated for systems using a rendezvous concurrency model, can be applied to the various concurrency models used in Java programs. The general approach of FLAVERS is based on modeling a concurrent system as a flow graph and, using a data flow analysis algorithm over this graph, statically checking if a property holds on all (or no) executions of the program. The accuracy of this analysis can be iteratively improved, as needed, by supplying additional constraints, represented as finite state automata, to the data flow analysis algorithm. In this paper we present an approach for analyzing Java programs that uses the constraint mechanism to model the possible communications among threads in Java programs, instead of representing them directly in the flow graph model. We also discuss a number of error-prone thread communication patterns that can arise in Java and describe how FLAVERS can be used to check for the presence of these. A preliminary evaluation of this approach is carried out by analyzing some small concurrent Java programs for these error-prone communication patterns and other, program-specific, faults.
[Algorithm design and analysis, Java, System testing, Data analysis, data flow analysis, concurrency theory, FLAVERS, Flow graphs, Yarn, Concurrent computing, Computer science, flow graph model, concurrency model, flow graph, Iterative algorithms, Mathematical model, Java programs]
Patterns in property specifications for finite-state verification
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Model checkers and other finite-state verification tools allow developers to detect certain kinds of errors automatically. Nevertheless, the transition of this technology from research to practice has been slow. While there are a number of potential causes for reluctance to adopt such formal methods, we believe that a primary cause is that practitioners are unfamiliar with specification processes, notations, and strategies. In a recent paper, we proposed a pattern-based approach to the presentation, codification and reuse of property specifications for finite-state verification. Since then, we have carried out a survey of available specifications, collecting over 500 examples of property specifications. We found that most are instances of our proposed patterns. Furthermore, we have updated our pattern system to accommodate new patterns and variations of existing patterns encountered in this survey. This paper reports the results of the survey and the current status of our pattern system.
[System testing, Automation, NASA, finite-state verification, Mathematics, property specification presentation, Formal specifications, Statistics, formal specification, property specification patterns, property specification codification, Computer science, formal verification, model checkers, formal methods, Permission, Computer errors, property specification reuse, object-oriented methods, Logic, automatic error detection]
Call-mark slicing: an efficient and economical way of reducing slice
Proceedings of the 1999 International Conference on Software Engineering
None
1999
When one debugs and maintain large software, it is very important to localize the scope of concern to small program portions. Program slicing is a promising technique for identifying portions of interest. There are many research results on the program slicing method. X static slice, which is a collection of program statements possibly affecting a particular variable's value, limits the scope, but the resulting collections are often still large. A dynamic slice, which is a collection of executed program statements affecting a particular variable's value, generally reduces the scope considerably, but its computation is expensive since the execution trace of the program must be recorded. In this paper, we propose a new slicing technique named call-mark slicing that combines static analysis of a program's structure with lightweight dynamic analysis. The data dependences and control dependences among the program statements are statically analyzed beforehand, and procedure/function invocations (calls) are recorded (marked) during execution. From this information, the dynamic dependences of the variables are explored. This call-mark slicing mechanism has been implemented, and the effectiveness of the method has been investigated.
[Software maintenance, program debugging, data dependences, call-mark slicing, Data mining, lightweight dynamic analysis, Information analysis, program statements, dynamic dependences, Information science, procedure invocation recording, Permission, control dependences, program slicing, Testing, large software debugging, small program portions, large software maintenance, Process control, static analysis, software maintenance, Software debugging, Programming profession, function invocation recording, Software engineering]
System-dependence-graph-based slicing of programs with arbitrary interprocedural control flow
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Many algorithms for automating software engineering tasks require program slices. To be applicable to large software systems, these slices must be computed interprocedurally. Slicing techniques based on the system dependence graph (SDG) provide one approach for computing interprocedural slices, but these techniques are defined only for programs in which called procedures necessarily return to call sites. When applied to programs that contain arbitrary interprocedural control flow, existing SDG-based slicing techniques can compute incorrect slices; this limits their applicability. This paper presents an approach to constructing SDGs, and computing slices on SDGs, that accommodates programs with arbitrary interprocedural control flow. The main benefit of our approach is that it allows the use of the SDG-based slicing technique on a wide class of practical programs to which it did not previously apply.
[Java, Data analysis, Software algorithms, large software systems, Control systems, system-dependence-graph-based program slicing, Application software, Software debugging, Computer science, Permission, software engineering task automation, Software systems, arbitrary interprocedural control flow, computer aided software engineering, program slicing, Software engineering, call sites]
An incremental flow- and context-sensitive pointer aliasing analysis
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Pointer aliasing analysis is used to determine if two object names containing dereferences and/or field selectors (e.g., *P,9->t) may refer to the same location during execution. Such information is necessary for applications such as data-flow-based testers, program understanding tools, and debuggers, but is expensive to calculate with acceptable precision. Incremental algorithms update data flow information after a program change rather than recomputing it from scratch, under the assumption that the change impact will be limited. Two versions of a practical incremental pointer aliasing algorithm have been developed, based on Landi-Ryder flow- and context-sensitive alias analysis. Empirical results attest to the time savings over exhaustive analysis (a six-fold speedup on average), and the precision of the approximate solution obtained (on average same solution as exhaustive algorithm for 75% of the tests.).
[Algorithm design and analysis, program understanding tools, program change, program debugging, Costs, program testing, debuggers, field selectors, Information analysis, approximate solution, Program processors, incremental flow-sensitive pointer aliasing analysis, Permission, Data flow computing, object names, Testing, time saving, data-flow-based testers, dereferences, change impact, incremental context-sensitive pointer aliasing analysis, data flow analysis, Educational institutions, reverse engineering, software maintenance, Landi-Ryder context-sensitive alias analysis, Petroleum, Computer science, Landi-Ryder flow-sensitive alias analysis, incremental algorithms, data flow information update]
Reusing single system requirements from application family requirements
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Reuse and requirements engineering are very important for efficient and successful systems development. However there are many open issues for performing them well in practice, especially raise of requirements. We present a new approach to reusing requirements based on application families i.e. several systems in a given problem domain. In contrast to precious approaches, an application family model in our approach consists of a pool of requirements (linked in a lattice structure), a domain model and a set of discriminants (a special kind of requirement that differentiate one system from another). We focus on how to make an efficient and clean selection of the requirements for a new system from an application family model. We have developed a method for solving this problem and a prototypical tool for supporting it. The method and the prototype were successfully developed under a study contract for ESA (European Space Agency). We present a case study, where we generated commanding requirements for an individual mission from an application family model of commanding requirements for spacecraft control operating systems. As a consequence, we propose this approach for reusing requirements based an application families.
[discriminants, European Space Agency, Lattices, Drives, spacecraft control operating systems, formal specification, Guidelines, systems development, requirements engineering, domain model, Permission, Writing, aerospace computing, software reusability, application family requirements, single system requirements reuse]
Assessing software libraries by browsing similar classes, functions and relationships
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Comparing and contrasting a set of software libraries is useful for reuse related activities such as selecting a library from among several candidates or porting an application from one library to another. The current state of the art in assessing libraries relies on qualitative methods. To reduce costs and/or assess a large collection of libraries, automation is necessary. Although there are tools that help a developer examine an individual library in terms of architecture, style, etc., we know of no tools that help the developer directly compare several libraries. With existing tools, the user must manually integrate the knowledge learned about each library. Automation to help developers directly compare and contrast libraries requires matching of similar components (such as classes and functions) across libraries. This is different than the traditional component retrieval problem in which components are returned that best match a user's query. Rather, we need to find those components that are similar across the libraries under consideration. In this paper, we show how this kind of matching can be done.
[Automation, Costs, reuse related activities, information retrieval, Information retrieval, similar class browsing, similar function browsing, similar relationship browsing, Application software, Sparks, software libraries, Computer science, Software libraries, automation, qualitative methods, Permission, software reusability, computer aided software engineering, software library assessment, Indexing]
CHIME: customizable hyperlink insertion and maintenance engine for software engineering environments
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Source code browsing is an important part of program comprehension. Browsers expose semantic and syntactic relationships (such as between object references and definitions) in GUI-accessible forms. These relationships are derived using tools which perform static analysis on the original software documents. Implementing such browsers is tricky. Program comprehension strategies vary, and it is necessary to provide the right browsing support. Analysis tools to derive the relevant cross-reference relationships are often difficult to build. Tools to browse distributed documents require extensive coding for the GUI, as well as for data communications. Therefore, there are powerful motivations for using existing static analysis tools in conjunction with WWW technology to implement browsers for distributed software projects. The CHIME framework provides a flexible, customizable platform for inserting HTML links into software documents using information generated by existing software analysis tools. Using the CHIME specification language, and a simple, retargetable database interface, it is possible to quickly incorporate a range of different link insertion tools for software documents, into an existing, legacy software development environment. This enables tool builders to offer customized browsing support with a well-known GUI. This paper describes the CHIME architecture, and describes our experience with several re-targeting efforts of this system.
[Software maintenance, graphical user interfaces, HTML link insertion, software documents, distributed software projects, legacy software development environment, Software performance, World Wide Web, HTML, Engines, syntactic relationships, WWW technology, CHIME architecture, source code browsing, Performance analysis, Data communication, Graphical user interfaces, hypermedia markup languages, information resources, program comprehension, retargetable database interface, information retrieval, static analysis, customizable hyperlink insertion engine, reverse engineering, GUI-accessible forms, distributed documents, software engineering environments, cross-reference relationship, CHIME specification language, customizable hyperlink maintenance engine, online front-ends, Software tools, programming environments, semantic relationships, Software engineering]
Prototyping real-time vision systems: an experiment in DSL design
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Describes the enhancement of XVision, a large library of C++ code for real-time vision processing, into FVision (pronounced "fission"), a fully-featured domain-specific language (DSL) embedded in Haskell. The resulting prototype system substantiates the claims of increased modularity, effective code reuse and rapid prototyping that characterize the DSL approach to systems design. It also illustrates the need for judicious interface design: relegating computationally expensive tasks to XVision (pre-existing C++ components) and leaving modular compositional tasks to FVision (Haskell). At the same time, our experience demonstrates how Haskell's advanced language features (specifically, parametric polymorphism, lazy evaluation, higher-order functions and automatic storage reclamation) permit a rapid DSL design that is itself highly modular and easily modified. Overall, the resulting hybrid system exceeded our expectations: visual tracking programs continue to spend most of their time executing low-level image processing code, while Haskell's advanced features allow us to quickly develop and test small prototype systems within a matter of a few days, and to develop realistic applications within a few weeks.
[Real time systems, Computer interfaces, System testing, XVision, C++ library, Image processing, software prototyping, functional programming, domain-specific language design, higher-order functions, systems design, low-level image processing code, real-time vision systems prototyping, software libraries, hybrid system, advanced language features, parametric polymorphism, modularity, Prototypes, Storage automation, modular compositional tasks, Haskell, Libraries, DSL, computationally expensive tasks, interface design, rapid prototyping, C++ language, automatic storage reclamation, lazy evaluation, code reuse, Domain specific languages, Machine vision, real-time systems, computer vision, software reusability, FVision, C++ components, visual tracking programs, functional languages]
Generalizing perspective-based inspection to handle object-oriented development artifacts
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The value of software inspection for uncovering defects early in the development lifecycle has been well documented. Of the various types of inspection methods published to date, experiments have shown perspective-based inspection to be one of the most effective, because of its enhanced coverage of the defect space. However, inspections in general, and perspective-based inspections in particular, have so far been applied predominantly in the context of conventional structured development methods, and then almost always to textual artifacts, such as requirements documents or code modules. Object-oriented models, particularly of the graphical form, have so far not been adequately addressed by inspection methods. This paper tackles this problem by first discussing the difficulties involved in tailoring the perspective-based inspection approach to object-oriented development methods and, second, by presenting a generalization of the approach which overcomes these limitations. The new version of the approach is illustrated in the context of UML-based object-oriented development.
[Costs, object-oriented programming, program diagnostics, Unified modeling language, Inspection, graphical object-oriented models, reading techniques, UML-based object-oriented development, textual artifacts, code modules, Quality assurance, requirements documents, structured development methods, software defect space coverage, Software quality, Permission, generalized perspective-based software inspection, object-oriented development artifacts, object-oriented methods, Unified Modelling Language, Software engineering, inspection]
Coca: an automated debugger for C
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Presents Coca, an automated debugger for C, where the breakpoint mechanism is based on events related to language constructs. Events have semantics, whereas the source lines used by most debuggers do not have any. A trace is a sequence of events. It can be seen as an ordered relation in a database. Users can specify precisely which events they want to see by specifying values for event attributes. At each event, visible variables can be queried. The trace query language is Prolog with a handful of primitives. The trace query mechanism searches through the execution traces using both control flow and data, whereas debuggers usually search according to either control flow or data. As opposed to fully "relational" debuggers which use plain database querying mechanisms, the Coca trace querying mechanism does not require any storage. The analysis is done on-the-fly, synchronously with the traced execution. Coca is therefore more powerful than "source-line" debuggers and more efficient than relational debuggers.
[Parallel languages, program debugging, Costs, automated debugger, Relational databases, execution traces, visible variables, relational debuggers, Coca, C language, trace query mechanism, Database languages, query processing, control flow, Permission, ordered relation, event semantics, database querying mechanisms, event attribute value specification, program diagnostics, program behaviour understanding, Debugging, Data structures, reverse engineering, debugging language, breakpoint mechanism, Uninterruptible power systems, source-line debuggers, data flow, language constructs]
Haemo dialysis software architecture design experiences
Proceedings of the 1999 International Conference on Software Engineering
None
1999
In this paper we present the experiences and architecture from a research project conducted in cooperation with two industry partners. The goal of the project was to reengineer an existing system for haemo dialysis machines into a domain specific software architecture. Our main experiences are (1) architecture design is an iterative and incremental process, (2) software quality requires a context, (3) quality attribute assessment methods are too detailed for use during architectural design, (4) application domain concepts are not the best abstractions, (5) aesthetics guides the architect in finding potential weaknesses in the architecture, (6) it is extremely hard to decide when an architecture design is ready, and (7) documenting software architectures is an important problem. We also present the architecture and design rational to give a basis for our experiences. We evaluated the resulting architecture by implementing a prototype application.
[Process design, Design methodology, biomedical equipment, reengineering, documentation, incremental process, software quality, haemo dialysis software architecture design experiences, iterative process, Application software, aesthetics, blood, Blood, software architecture, systems re-engineering, kidney, Software design, Software architecture, patient treatment, Computer architecture, Software quality, Permission, Computer industry, medical computing]
Architectural framework modeling in telecommunication domain
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Architectural frameworks have been shown to increase the design reusability in large-scale object-oriented systems. Drawing on experience in complex software systems development in the telecommunication domain, we present concepts and techniques for domain partitioning and an architectural framework modeling and layering. In particular, we discuss how a component-based approach, architectural modeling styles, and the systematic usage of architectural and design patterns provide a common framework for product-line development. Two application frameworks based on this model are presented as case studies.
[domain partitioning, component-based approach, telecommunication domain, complex software systems development, Telecommunication traffic, product-line development, telecommunication computing, software architecture, Software architecture, design patterns, Computer architecture, Traffic control, Large-scale systems, object-oriented methods, Pattern analysis, Data analysis, object-oriented programming, design reusability, Object oriented modeling, architectural framework modeling, architectural framework layering, Application software, large-scale object-oriented systems, software reusability, Software systems]
Baseball seasons and dog years
Proceedings of the 1999 International Conference on Software Engineering
None
1999
From 1995 through 1997, Instant Sports used the Internet to provide interactive real-time coverage of Major League Baseball. The changes in Instant Sports core architecture during that time provide some lessons about architectural evolution in the context of rapidly changing technology, including the need to identify fundamental issues rather than trendy ones, the importance of a good domain model, and the role of domain characteristics in balancing computational and communication resources.
[information resources, Java, Technological innovation, Fans, communication resources, architectural evolution, Major League Baseball, Delay, software architecture, interactive real-time coverage, Databases, domain model, computational resources, Web and internet services, rapidly changing technology, Computer architecture, Automatic generation control, Computer networks, Instant Sports, Internet, sport, Context modeling]
Product-line architectures in industry: a case study
Proceedings of the 1999 International Conference on Software Engineering
None
1999
In this paper, a case study investigating the experiences from using product-line architectures is presented involving two Swedish companies, Axis Communications AB and Securitas Larm AB. Key persons in these organizations have been interviewed and information has been collected from documents and other sources. The study identified a collection of problems and issues. The identified problems include the amount of required background knowledge, information distribution, the need for multiple versions of assets, dependencies between assets, use of assets in new contexts, documentation, tool support, management support and effort estimation. Issues collected from the case study are the questioned necessity of domain engineering units, business units versus development departments, time-to-market versus asset quality and common features versus feature superset. For each problem, a problem description, an example, underlying causes, available solutions and research issues are identified whereas for each issue the advantages and disadvantages of each side are discussed.
[Securitas Larm AB, Computer aided software engineering, Industrial relations, system documentation, Programming, business units, domain engineering units, development departments, management support, software architecture, Defense industry, product-line architectures, assets, background knowledge, Computer architecture, feature superset, software tools, effort estimation, Axis Communications AB, time-to-market, software development management, DP industry, Documentation, documentation, asset quality, information distribution, Computer science, tool support, Swedish companies, common features, Software quality, software reusability, Computer industry, Software engineering]
Linux as a case study: its extracted software architecture
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Many software systems do not have a documented system architecture. These are often large, complex systems that are difficult to understand and maintain. One approach to recovering the understanding of a system is to extract architectural documentation from the system implementation. To evaluate the effectiveness of this approach, we extracted architectural documentation from the Linux/sup TM/ kernel. The Linux kernel is a good candidate for a case study because it is a large (800 KLOC) system that is in widespread use and it is representative of many existing systems. Our study resulted in documentation that is useful for understanding the Linux system structure. Also, we learned several useful lessons about extracting a system's architecture.
[Unix, operating system kernels, Computer aided software engineering, Costs, Buildings, system documentation, software systems, Linux kernel, Documentation, architectural documentation extraction, Computer science, software architecture, Software architecture, Linux, Computer architecture, Software systems, Kernel]
An architectural style for multiple real-time data feeds
Proceedings of the 1999 International Conference on Software Engineering
None
1999
We present an architectural style for the integration of multiple real-time data feeds on Windows NT platforms. We motivate the development of this style by highlighting different application areas in which the style has been deployed. We present the requirements that will be met by the architectural style and discuss the design of customizable components that implement the style based on Microsoft's Component Object Model.
[Real time systems, Microsoft Component Object Model, Humans, Control systems, Application software, customizable component design, software architecture, architectural style, multiple real-time data feed integration, Software architecture, network operating systems, real-time systems, Windows NT platforms, Cities and towns, software reusability, Computer industry, Electrical equipment industry, financial data processing, Feeds, Stock markets]
Supporting industrial hyperwebs: lessons in scalability
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Open hypermedia is one approach to managing the relationships that exist in software development projects. A key technical issue in this endeavor is support for scalability. Our experience supporting scalability in open hypermedia has revealed several key insights including the notion of the transitivity of scalability, the need to consider issues of scale in moving from design to implementation, the need to apply multiple techniques in tandem, and the unexpected nontechnical issues that arise when scaling a system to meet the demands of industrial software engineering. These insights are grounded in observations of a development project that scaled an open hypermedia system, Chimera, two orders of magnitude to meet the demands of an industrial user.
[information resources, project management, Industrial relations, open systems, Scalability, Project management, software development management, Documentation, hypermedia, Programming, open hypermedia, industrial software engineering, industrial hyperwebs, multimedia computing, Environmental management, scalability, Software development management, software development projects, Computer industry, Large-scale systems, Software engineering, Chimera]
Software evolution and 'light' semantics
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The motivation for Inscape came from my experience as a programmer, designer and architect. There were two major (and inter-related) problems that I encountered while building software systems where I had to use components built by other people: the pieces often did not fit when I put them together and changing code often produced surprising and unexpected results. The first problem was due primarily to the informality and often incompleteness of component interfaces. The second problem was due ultimately to the complexity of the software and an inability to foresee or determine the consequences of changes. These problems result from three essential and intertwined properties of building software systems: composition, evolution and complexity. In coming to grips with the problem of composition, using formal interface specifications is the obvious choice. Enhancing the syntactic interfaces with semantic information is one way of expressing the intent of the interface provider and enabling the user to have all the information necessary to its correct and effective use. How to attack the problem of evolution is not as obvious. The approach I took in the Inscape experiment was to use the specifications constructively in order to determine and maintain semantic dependencies. Keeping track semantically as to how the interfaces are used is the analog of expressing the interface creator's intent: it is capturing the users intent. Given that both interfaces and implementations evolve, keeping track of the dependencies enables the environment to help in understanding the effects of changes and where those effects take place.
[semantic dependencies, complexity, formal interface specifications, light semantics, Buildings, software systems, Data structures, user interfaces, software maintenance, formal specification, Programming profession, software evolution, composition, Pressing, Permission, Software systems, Hardware, syntactic interfaces, Pins, component interfaces, semantic information, Inscape]
A practical method for verifying event-driven software
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Formal verification methods are used only sparingly in software development. The most successful methods to date are based on the use of model checking tools. To use such tools, the user must first define a faithful abstraction of the application (the model), specify how the application interacts with its environment, and then formulate the properties that it should satisfy. Each step in this process can become an obstacle. To complete the verification process successfully often requires specialized knowledge of verification techniques and a considerable investment of time. In this paper we describe a verification method that requires little or no specialized knowledge in model construction. It allows us to extract models mechanically from the source of software applications, securing accuracy. Interface definitions and property specifications have meaningful defaults that can be adjusted when the checking process becomes more refined. All checks can be executed mechanically, even when the application itself continues to evolve. Compared to conventional software testing, the thoroughness of a check of this type is unprecedented.
[Software testing, interface definitions, model construction, program testing, software development, software testing, Manuals, abstraction, accuracy, Concurrency control, Application software, model checking tools, Programming profession, software applications, formal verification, defaults, Investments, Computer bugs, formal verification methods, Telephony, Permission, property specifications, Formal verification, event-driven software verification]
Software process maturity: is level five enough?
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Productivity, Process design, Coordinate measuring machines, Computer architecture, Programming, Product development, Continents, Capability maturity model, Hemorrhaging, Software development management]
Reuse technologies and their niches
Proceedings of the 1999 International Conference on Software Engineering
None
1999
This article characterizes various categories of reuse technologies in terms of their underlying architectures, the kinds of problems that they handle well, and the kinds of problems that they do not handle well. It describes their operational envelopes and niches. The emphasis is on generative reuse technologies.
[architectures, Humans, Middleware, Organizing, Delay, Computer languages, software architecture, Character generation, Permission, software reusability, Concrete, Libraries, Large-scale systems, generative reuse technologies]
Architecting for large-scale systematic component reuse
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Organizations building highly complex business and technical systems must architect families of systems and implement these with large-scale component reuse. Without carefully architecting the systems, components, organizations and processes for reuse, reuse will not succeed. The talk explains our experience with component-based software reuse and a systematic reuse process using UML, and object-oriented business and system modeling.
[Object oriented modeling, Unified modeling language, Companies, Programming, Business process re-engineering, object-oriented, Application software, component-based software reuse, software architecture, component reuse, UML, Computer architecture, Software quality, software reusability, Software systems, Large-scale systems, business and system modeling]
Designing safe software for medical devices
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Safety devices, Software design, Software quality, Programming, Computer industry, Hazards, Software safety, Risk management, Risk analysis, Medical diagnostic imaging]
A systems engineering view of requirements management for software-intensive systems
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The importance of requirements management is widely understood and appreciated. Nevertheless, the rules used to guide the process, while recognized, but rarely followed in a consistent way. Requirements management often exists as a vague, undefined collection of process fragments. This presentation explores the system decomposition function of systems engineering. Transition Zones are defined that mark the boundaries where, during the decomposition process, the change of technical disciplines is needed. The success of the requirements management process here is defined as the Systems Engineer's ability to recognize and manage these transitions.
[Electric breakdown, Project management, Life testing, Product design, system decomposition, requirements management, Design engineering, Transition Zones, Engineering management, systems analysis, Permission, Systems engineering and theory, systems engineering, Hardware, Concurrent engineering]
The extension of systems architecting to the architecting of organizations
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Cyclic redundancy check, Context, Art, Navigation, Software architecture, Buildings, Permission, Systems engineering and theory, Aerospace engineering]
ICSE 1999 presentation overview and outline "successfully deliver internet applications using rapid software techniques"
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Summary form only given. The author will be presenting on the key aspects of C-bridge's business practices which allows it to successfully deliver Internet applications using rapid software development techniques. C-bridge develops custom, transaction-oriented Internet application for global 2000 companies with focuses on retail, insurance and distribution. C-bridge customers seek competitive advantages by being first to market with customer and business to business systems and require fast development and implementation cycles. Cbridge provides this capability through investments in three primary areas: A methodology that is tailored to the technologies and solutions that are developed; A technical framework that provides architectural solutions to challenges such as scalability, reliability and availability; and Ongoing research into existing components, thereby leveraging the technical strength of an entire market of technology developers. During his presentation Mr. Leinbach will discuss key practices within these three areas of C-bridge's practice demonstrating how they allow C-bridge to deliver tangible business benefits within short windows of opportunity. Mr. Leinbach will share specific architectural and process details, information tools that Cbridge currently employees, and discuss specific business benefits that are the result of these techniques. Specific examples will also be given from C-bridge engagements.
[Bridges, Java, Investments, Project management, Insurance, Permission, Programming, Internet, Application software]
Impact of Commercial Off-The-Shelf (COTS) software on the interface between systems and software engineering
Proceedings of the 1999 International Conference on Software Engineering
None
1999
This presentation examines the changes in the interface between systems engineering and software engineering which are required to make effective use of commercial off-the-shelf software products, especially in large or complex systems.
[Software maintenance, Missiles, Costs, Power generation economics, Licenses, formal specification, software requirements, Design engineering, Software systems, Systems engineering and theory, systems engineering, system requirements, Hardware, software engineering, Software engineering]
InSight-reverse engineer CASE tool
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Building upon existing software systems while maintaining or improving software quality is a major goal of software engineering. To achieve this, every software engineering phase (requirements analysis, software design, implementation, testing and maintenance) must produce good quality outputs for the next phase. We believe that a next generation reverse engineering tool can assist in realizing this need. InSight is a new and powerful CASE toolset for software developers and architects, designed to support the development and maintenance of software throughout the complete lifecycle. The inSight tool suite provides software architects and developers with new capabilities of understanding existing software systems, allowing them to ask their day-to-day questions directly to the tool and receive up-to-date answers while interacting with the tool. It includes data mining capabilities such as code flow extraction, and software interaction extraction such as operating system and application specific messaging, uses, needs, etc. The inSight tool suite also includes powerful graphical interfaces for presenting and manipulating the extracted information. Control flow is presented as editable flowcharts. InSight supports Protel, ANSI C, and C++. InSight uses a patent pending reverse engineering technique to perform these extractions.
[Software maintenance, Computer aided software engineering, Reverse engineering, Buildings, software design, data mining, Maintenance engineering, reverse engineering, software quality, Data mining, requirements analysis, code flow extraction, Software quality, CASE toolset, Software systems, reverse engineer, computer aided software engineering, software tools, Software tools, inSight tool suite, Software engineering]
Behavioral analysis of software architectures using LTSA
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The LTSA (Labeled Transition System Analyzer) is a tool for modeling and analyzing the behavior of concurrent systems. The demonstration will focus on the use of architectural descriptions in developing behavioral models and on the analysis that can be performed on these models. Three concurrent architecture examples; filter pipeline, supervisor-worker and announcer-listener which each use a different type of connector are used to illustrate the capabilities of the tool.
[Pipelines, software architectures, concurrent architecture, Control systems, Educational institutions, finite state machines, Concurrent computing, Connectors, software architecture, Filters, Software architecture, LTSA, concurrent systems, Computer architecture, Permission, Performance analysis, Labeled Transition System Analyzer]
OU LearningWorks: a customized programming environment for Smalltalk modules
Proceedings of the 1999 International Conference on Software Engineering
None
1999
We have exploited and adapted Goldberg's LearningWorks framework to produce an environment with new programming tools, visualization tools, and system simulations. The environment is designed to be used via plug-in modules, called LearningBooks, sets of classes and persistent objects, for which we have developed a pedagogic standard that includes, for example, an HTML browser and various of the aforementioned tools and systems. The context for this development has been a distance learning degree-level course in object technology which is enrolling over 5000 mature students per year, mostly in the UK, Western Europe and Singapore. The course, M206, Computing: An Object-oriented Approach from the Open University (OU), will soon be introduced into the USA. While adhering to the original conception of LearningWorks that it promote a software engineering approach of systems building, we have successfully added support for the needs of the distance learning neophyte. By showing various microworlds and programming tools these notes outline the environment we have implemented and deployed.
[Visualization, Europe, HTML, visualization tools, programming tools, Programming profession, LearningWorks, Programming environments, distance learning, Smalltalk modules, Computer aided instruction, Smalltalk, Permission, software engineering, Books, Standards development, system simulations, programming environments, Software engineering]
Software process support over the Internet
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The MILOS system supports software development processes over the Internet. It integrates process modeling with project planning and enactment. Our flexible workflow engine allows refining and changing process models during project execution. The built-in traceability component supports change notifications and helps the project participants to ensure that the project plan as well as the state of the enactment engine reflect the "real world" development process. Tool integration is accomplished by using the built-in capabilities of Web browsers.
[Art, enactment, Process planning, Project management, Humans, Programming, flexible workflow, MILOS system, Virtual groups, workflow engine, Search engines, Permission, software engineering, Internet, Web browsers, Capability maturity model, software development processes, process modeling, project planning]
WinWin: a system for negotiating requirements
Proceedings of the 1999 International Conference on Software Engineering
None
1999
WinWin is a system that aids in the capture and recording of system requirements. It also assists in negotiation. The WinWin system has been available for several years and is being used by dozens of software development groups. In this presentation we go over the capabilities of the system and discuss how it might be used on your software development project.
[Terminology, software development, Multimedia systems, Taxonomy, Programming, formal specification, Organizing, Computer science, WinWin, Voting, system requirements, software engineering, negotiation tool, Software engineering]
MoHCA-Java: a tool for C++ to Java conversion support
Proceedings of the 1999 International Conference on Software Engineering
None
1999
As Java increases in popularity and maturity, many people find it desirable to convert legacy C++ or C programs to Java. Our hypothesis is that a tool which performs rigorous analysis on a C++ program, providing detailed output on the changes necessary, will make conversion a much more efficient and reliable process. MoHCA-Java is such a tool. It performs detailed analysis on a C++ abstract syntax tree; the parameters of the analysis can be specified and extended very quickly and easily using a rule-based language. We have found that MoHCA-Java is very useful for identifying and implementing source code changes, and that its extensibility is a very important factor, specially to adapt the tool to assist in the conversion of C++ code that makes extensive use of libraries to Java code that uses similar libraries.
[Computer science, Java, C++, Citation analysis, Permission, Libraries, MoHCA-Java, Performance analysis, software tools, C++ language, Java conversion support, source code changes]
Leap: a "personal information environment" for software engineers
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The Leap toolkit is designed to provide Lightweight, Empirical, Anti-measurement dysfunction, and Portable approaches to software developer improvement. Using Leap, software engineers gather and analyze personal data concerning time, size, defects, patterns, and checklists. They create and maintain definitions describing their software development procedures, work products, and project attributes, including document types, defect types, severities, phases, and size definitions. Leap also supports asynchronous software review and facilitates integration of this group-based data with individually collected data. The Leap toolkit provides a "reference model" for a personal information environment to support skill acquisition and improvement for software engineers.
[software engineers, Portable computers, Data analysis, software development, Collaborative software, Time to market, Programming, asynchronous software review, Data engineering, Leap toolkit, Environmental economics, Design engineering, personal information environment, software engineering, software tools, Software tools, Pattern analysis]
FarGo: a system for mobile component-based application development
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The design of efficient and reliable distributed applications that need to operate over various machines which are networked by wide area and/or low-bandwidth connections, demands new programming abstractions and mechanisms. In particular, the conventional static design-time determination of local-remote relationships between components implies that dynamic environmental changes are hard if not impossible to address without reengineering the application. The FARGO system presents a novel programming model that is centered around the concept of "dynamic application layout\
[dynamic application layout, Logic programming, Terminology, Printers, application development, Runtime, Program processors, mobile computing, component mobility, programming model, FarGo, Permission, Cities and towns, Dynamic programming, software tools, Joining processes, Monitoring]
The RMT (Recursive Multi-Threaded) tool: a computer aided software engineering tool for monitoring and predicting software development progress
Proceedings of the 1999 International Conference on Software Engineering
None
1999
A number of software life-cycles for object-oriented software development (Fountain Model, Recursive/Parallel Model, McGregor and Sykes Model, and Chaos Model Life Cycle) exist today. However, these existing life-cycles have little or no support for estimating and monitoring progress during the development of the software. The ability to measure progress during the development is significant because it allows both the managers and the developers to determine whether a project is on schedule or not. Identifying that a project is behind schedule allows managers and developers to notify appropriate individuals of any scheduling and/or budgetary impacts at an early stage during the development and to determine appropriate course of action. This demonstration presents the Recursive Multi-Threaded (RMT) software life-cycle and the implemented computer aided software engineering tool based on RMT. The RMT Tool supports the monitoring of progress during development, addresses the specific needs of the developing object-oriented software, and attempts to resolve deficiencies found in many existing software lifecycles. What makes RMT unique from existing software life-cycles is its use of a "thread" for partitioning and organizing software development activities. Threads support iteration and recursion, which are critical concepts for the development of the software.
[Chaos, software life-cycle, Computer aided software engineering, project support environments, Recursive Multi-Threaded, Computerized monitoring, Object oriented modeling, Project management, software development management, Programming, Yarn, object-oriented software development, Processor scheduling, Life estimation, software life-cycles, computer aided software engineering, monitoring progress, Software tools]
Distributed objects
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Computer science, Computer languages, Network servers, Java, Object oriented modeling, Buildings, Permission, Educational institutions, Middleware, Network operating systems]
Practical software measurement
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Performance evaluation, Data analysis, project management, software development, Area measurement, Project management, Programming, Scheduling, Risk analysis, measurement, Performance analysis, Software measurement, Monitoring]
Personal software engineering project management process
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Productivity, personal software engineering project management process, software estimating, Process planning, Linear regression, Project management, PPMP, PSP, Statistics, software project tracking, Counting circuits, personal productivity, personal software process, Education, software project planning, Permission, Probes, Software engineering]
Defining families - Commonality analysis
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Costs, variabilities, Maintenance engineering, domain analysis, Application software, families, use reuse, commonality analysis, Feedback, Computer industry, Software systems, Systems engineering and theory, Hardware, Performance analysis, feedback domain engineering, commonalities, product line engineering, Software engineering]
Verification and validation of requirements for mission critical systems
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Mission critical systems, NASA, Space shuttles, software requirements specifications, Formal specifications, Space stations, Embedded software, Embedded system, V&amp;V, embedded systems, lightweight formal methods, Permission, Software systems, Software tools, requirements modelling]
Software interoperability: principles and practice
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Java, polylingual, C++, Taxonomy, Programming, interoperability, Computer science, CORBA, Network servers, java, Computer architecture, Permission, Solids, Software systems, Lifting equipment]
Managing by the numbers: a tutorial on quantitative measurement and control of software projects
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Tutorial, project management, Electric breakdown, technical performance measurement, Project management, Software performance, Time measurement, control, process improvement, incremental development, measurement, corrective action, earned value, Current measurement, Packaging, Solids, Software measurement, binary tracking, Testing]
Risk management in software development: a technology overview and the riskit method
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Computer science, Intelligent networks, risk management, project management, Project management, Europe, Programming, Permission, Concrete, Risk management, Software development management, Guidelines]
Failure and success factors in reuse programs: a synthesis of industrial experiences
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Terminology, software reuse, Europe, Humans, empirical studies, Computer industry, Software systems, Solids, survey, Performance analysis, Software reusability, Bonding, Software engineering]
Round-trip engineering with design patterns, UML, java and C++
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Java, Computer aided software engineering, Unified modeling language, Pattern recognition, Code standards, Design engineering, UML, Collaboration, design patterns, Software systems, Libraries, round-trip engineering, Qualifications]
Adaptable components
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[domain-specific, Costs, reuse, metaprogram, Taxonomy, Documentation, abstraction, generator, Application software, component, File systems, adaptable, Investments, Permission, Product development, family, Software reusability, Testing]
Using subject-oriented programming to overcome common problems in object-oriented software development/evolution
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Software testing, Costs, Object oriented modeling, subject-oriented programming, Large scale integration, evolution, Code standards, Runtime, Genetic programming, object-oriented development, Writing, Object oriented programming, component adaptation, Software engineering, object composition]
Tutorial: A quick introduction to software reliability modeling
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Software testing, Tutorial, models, Data analysis, Statistical analysis, software reliability, testing, Reliability engineering, Data engineering, Software reliability, Sun, Software quality, Permission, statistics]
Modeling and analyzing software architectures
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Protocols, software design, architectural modeling, Throughput, Delay, software architecture, Software design, Software architecture, Bibliographies, architectural analysis, Computer architecture, architecture design languages, acme, Permission, Software systems, Books]
ICSE'99 Workshop on Web Engineering
Proceedings of the 1999 International Conference on Software Engineering
None
1999
The workshop would assess the problems of Web-based application systems development, argue the need for Web Engineering approach for developing scalable, quality, large-scale Web-based systems. It would also identify key Web Engineering activities and propose approaches and methods for systematic development of Webbased applications. Further, it would review ongoing work in this area, discuss case studies and best practices, and pave directions further work.
[]
Software change and evolution (SCE'99)
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Algorithm design and analysis, concept location, Software maintenance, software comprehension, Change detection algorithms, validation of change, Software algorithms, software development maintenance, change, Programming, change request, change planning, restructuring, Computer science, Pathology, Permission, Iterative algorithms, Software tools, change impact analysis, propagation of change]
ICSE 99 Workshop on Software Engineering over the Internet
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[software engineering, internet]
International workshop on testing distributed component-based systems
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[]
First workshop on economics-driven software engineering research
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[economics, decision theory, software engineering, finance]
International workshop on software transformation systems (STS'99)
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[]
The international symposium on software engineering for parallel and distributed systems PDSE'99
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[]
Engineering distributed objects (EDO 99) workshop summary
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[]
The first international symposium on constructing software engineering tools (CoSET'99)
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[]
Summary: empirical studies of software development and evolution
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Computer science, Statistical analysis, Object oriented modeling, Programming, Educational institutions, Design for experiments, Control systems, empirical software engineering, Large-scale systems, Software measurement, Software engineering, systems evolution]
Science and engineering for software development: a recognition of Harlan D. Mills' legacy
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[Computer science, Vacuum systems, Fitting, Software quality, Vacuum technology, Programming, Drives, Educational institutions, Milling machines, Software engineering]
International workshop on component-based software engineering
Proceedings of the 1999 International Conference on Software Engineering
None
1999
false
[component-based software, component-based development]
Automatic verification of real-time designs
Proceedings of the 1999 International Conference on Software Engineering
None
1999
We are working on an automatic approach to verify real-time distributed designs for complex timing requirements which go beyond schedulability. We focus our analysis on designs which adhere to the hypothesis of known analytical theory for fixed-priority scheduling. Unlike previous formal approaches, we profit from that theory and build rather simple and tractable formal models based on Timed Automata. Therefore, we are integrating scheduling analysis techniques into the framework of automatic formal verification.
[Real time systems, scheduling analysis, real-time designs, distributed designs, automata theory, Citation analysis, distributed processing, Distributed computing, Delay, timing requirements, Runtime, Processor scheduling, formal verification, Automata, real-time systems, scheduling, Constraint theory, Timing, verification, Formal verification]
Research summary for dynamic detection of program invariants
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Explicitly stated program invariants can help programmers by identifying program properties that must be preserved when modifying code; invariants also play a number of other valuable roles in program development and evolution. In practice, however, these invariants are usually implicit. An alternative to expecting programmers to fully annotate code with invariants is to automatically infer invariants from the program itself. This research aims to develop and evaluate dynamic techniques for discovering invariants from execution traces. Our hypothesis is that such techniques are effective at extracting invariants from programs and that the extracted invariants are useful to programmers. Experiments with our prototype implementation provide preliminary support for this hypothesis.
[Software maintenance, program invariants, execution traces, Data structures, modifying code, Programming profession, Computer science, program development, Computer languages, prototype implementation, Prototypes, Programmable logic arrays, Permission, User interfaces, data structures, software engineering, Logic arrays, extracted invariants]
Development of object oriented frameworks for spatio-temporal information systems
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Domain specific Information Systems (IS) have traditionally been developed using conventional structured analysis and design. This investigation looks into the various aspects of designing and developing an Environmental Information System (EIS) using Object Technology and in the process identifying new, generic and specific design patterns which can be used in developing object oriented frameworks for the environment domain.
[Geographic Information Systems, object-oriented programming, Object oriented modeling, Data processing, Remote sensing, Information systems, Information analysis, Environmental Information System, Jacobian matrices, Earth, object oriented frameworks, spatio-temporal information systems, environmental science computing, design patterns, information systems, Spatial resolution, Monitoring]
The Internet as a medium for software engineering experiments
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Empirical software engineering often faces the challenge of large variability of results among individual subjects. Variability can be reduced by using a larger group of subjects, but such group quickly becomes too expensive. Another challenge is finding a group of subjects that is representative of some relevant population of software engineers. This paper explores the potential of using the Internet as the medium for software engineering experiments to address the problems of sample size and representativeness.
[Costs, Programming, software engineering, Internet, Face detection, variability, Web server, software engineering experiments, Software engineering, Recruitment]
Specification, analysis, and prototyping of mobile systems
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Mobile code offers new strategies for the development of systems. I adopt a formal approach to study advantages, limitations, classification, and future trends of mobile code technologies.
[Java, software prototyping, prototyping, Formal languages, mobile systems, Automatic logic units, formal specification, Computer science, mobile computing, formal verification, Mobile agents, Prototypes, Tail, Permission, mobile code, formalization, Performance analysis, Safety, distributed programming, verification]
Project LEAP: personal process improvement for the differently disciplined
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Software developers and managers have faced the problem of producing quality software since the beginning of the computer age. Many people have studied the software quality problem and have proposed many solutions. We can categorize these different solutions into two groups: (1) "Topdown" solutions, that focus on software development as a group effort and (2) "Bottom-up" solutions, that focus on the individual software developer. Some of the many Top-down solutions include: the Capability Maturity Model, Clean Room development, software quality assurance groups, and Formal. Technical Review. These top down methods help improve the quality of the software, however they may not be enough.
[software development, Collaborative software, Laboratories, Area measurement, software development management, Switches, Programming, quality software, top down methods, process improvement, software quality, LEAP, group effort, Management training, Software quality, Software measurement, Research and development management, Quality management]
Algorithm development in the mobile environment
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Mobility is emerging as a new research field with its own characteristic problems, models, and algorithms. Fixed networks are commonplace but the challenges of mobility such as the transient nature of connections, reduced bandwidth, and limited processing power have made incorporating mobility into this existing environment challenging. Our contribution to the rapid integration of mobility is the design and development of algorithms in the mobile environment which will serve as a foundation for mobile applications. In this paper, we describe two models for mobility, outline the challenges of each environment, and provide approaches to algorithm development within each model.
[Algorithm design and analysis, Base stations, mobility, mobile environment, Drives, algorithm development, Power system modeling, Computer science, Intelligent networks, Multicast algorithms, mobile computing, Bandwidth, Distributed algorithms, Mobile computing, distributed programming]
A flexible approach to decentralized software evolution
Proceedings of the 1999 International Conference on Software Engineering
None
1999
Reducing the costs and risks associated with changing complex software systems has been a principal concern of software engineering research and development. One facet of this effort concerns decentralized software evolution (DSE), which, simply stated, enables third-parties to evolve a software application independent of the organization that originally developed it. Popular approaches to DSE include application programming interfaces or APIs, software plug-ins, and scripting languages. Application vendors employ DSE as a means of attracting additional users to their applications-and, consequentially, increasing their market share-since it opens up the possibility that a third-party modified version of the application would satisfy the needs of end-users unsatisfied with the original version. This benefits everyone involved: the original application vendor sells more product since customization constitutes use; third-party developers deliver a product in less time and with lower cost by reusing software as opposed to building it from scratch; and customers receive a higher quality product, customized to suit their needs, in less time and with lower cost. By increasing the opportunity for buying and customizing software instead of building it from scratch, DSE attacks Brook's "essential" difficulties of software development.
[Costs, Buildings, software systems, Programming, Application software, Research and development, software evolution, Computer science, Software quality, Permission, Software systems, software engineering, distributed programming, Software engineering, decentralized software evolution]
Configurable software architecture in support of configuration management and software deployment
Proceedings of the 1999 International Conference on Software Engineering
None
1999
In this research article we introduce the Menage project. Menage is based on the vision that the notion of software architecture, extended with the concept of versioning, can be used as an organizing abstraction for some of the activities in the software life cycle. In particular, we are investigating how two of those activities, namely configuration management and software deployment, can benefit from the availability of an explicit architectural representation that is enhanced with versioning capabilities.
[versioning, Menage project, Merging, Project management, software deployment, configurable software architecture, Organizing, Computer science, configuration management, software architecture, Software architecture, explicit architectural representation, Computer architecture, Disaster management, Permission, organizing abstraction, software life cycle, Architecture description languages, Software tools]
Contextual programming
Proceedings of the 1999 International Conference on Software Engineering
None
1999
When information external to a component is of no importance to the implementation of that component, but is present within it as an artifact of design or programming mechanisms, system structure suffers, resulting in greater difficulties in software evolution and reuse. I am investigating an approach to lessening the effects of such extraneous embedded knowledge through the use of dynamic execution information and static structural information, which comprise the concept of context.
[Encapsulation, static structural information, software reuse, design mechanisms, contextual programming, Computational Intelligence Society, programming mechanisms, software evolution, Computer science, Runtime, Genetic programming, Permission, software engineering, Dynamic programming, system structure, programming, embedded knowledge, dynamic execution information]
Toward precise measurements using software normalization
Proceedings of the 1999 International Conference on Software Engineering
None
1999
There has been a lot of interest in defining appropriate ways of measuring the attributes of software. We think these measures should accurately represent those attributes which they quantify. However, these software measures are usually not precise enough because they measure elements that need not be included. Reducing redundant structures of the program before they are measured is a simple yet important concept. We propose a strategy of software normalization for enhancing software measurement. Most importantly, this program normalization allows those metrics to measure what is meant to be measured and avoid imprecise results.
[software attribute measurement, Entropy, Appropriate technology, software normalization, Programming profession, Computer science, Fluid flow measurement, Software metrics, Permission, Lab-on-a-chip, Software measurement, Testing, software metrics]
Is the new economy socially sustainable?
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Summary form only given. The complete presentation was not made available for publication as part of the conference proceedings. At the turn of the millennium, the revolution in information technology has ushered in a new economy. This economy, originated in the United States, and more specifically in the American West Coast, is spreading throughout the world, in an uneven, yet dynamic pattern. It is essentially characterized by the key role of knowledge and information in spurring productivity and enhancing competitiveness; by its global reach; and by its networked form of business organization. Well managed, this new economy may yield an extraordinary harvest of human creativity and social well being. However, several major contradictions threaten the stability of this new economy: the volatility of global financial markets; the institutional rigidity of business, legislation, and governments in many countries; increasing social inequality and social exclusion throughout the world, limiting market expansion and triggering social tensions; and the growing opposition to globalization without representation on behalf of alternative values, and legitimate concerns on the environmental and social costs of this model of growth. Information technology offers great potential in helping to supersede these contradictions at the dawn of an emerging socio-economic system. But the speed of technological innovation requires the parallel development of institutional and cultural innovation, away from bureaucracy but closer to people, to ensure the sustainability of the new economy, and to spur the new wave of technological creativity.
[Productivity, Technological innovation, Stability, Sociology, Government, Humans, Legislation, Globalization, Information technology, Organizational aspects]
The future of software
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Friction, Humans, Software quality, Programming, Fuel economy, Internet, Environmental economics, Physics, Software engineering]
Dot com versus bricks and mortar the impact of portal technology
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Programmable control, Mortar, US Department of Transportation, Internet, Portals]
Requirements engineering in the year 00: a research perspective
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Requirements engineering (RE) is concerned with the identification of the goals to be achieved by the envisioned system, the operationalization of such goals into services and constraints, and the assignment of responsibilities for the resulting requirements to agents such as humans, devices, and software. The processes involved in RE include: domain analysis, elicitation, specification, assessment, negotiation, documentation, and evolution. Getting high quality requirements is difficult and critical. Recent surveys have confirmed the growing recognition of RE as an area of utmost importance in software engineering research and practice. The paper presents a brief history of the main concepts and techniques developed to date to support the RE task, with a special focus on modeling as a common denominator to all RE processes. The initial description of a complex safety-critical system is used to illustrate a number of current research trends in RE-specific areas such as goal oriented requirements elaboration, conflict management, and the handling of abnormal agent behaviors. Opportunities for goal based architecture derivation are also discussed together with research directions to let the field move towards more disciplined habits.
[goal based architecture derivation, Costs, modeling, software engineering research, Humans, safety-critical software, RE processes, History, abnormal agent behaviors, formal specification, conflict management, Computer architecture, Permission, research perspective, high quality requirements, RE task, goal oriented requirements elaboration, Documentation, documentation, specification, history, domain analysis, bibliographies, requirements engineering, complex safety-critical system, systems analysis, Software quality, Error correction, operationalization, Software engineering, Accidents, responsibility assignment, research trends]
A case study: demands on component-based development
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Building software systems with reusable components brings many advantages. The development becomes more efficient, the reliability of the products is enhanced, and the maintenance requirement is significantly reduced. Designing, developing and maintaining components for reuse is, however, a very complex process which places high requirements not only on the component functionality and flexibility, but also on the development organization. The authors discuss the different levels of component reuse, and certain aspects of component development, such as component generality and efficiency, compatibility problems, the demands on development environment, maintenance, etc. The evolution of requirements for products generates new requirements for components, if components are not general and mature enough. This dynamism determines the component life cycle where the component first reaches its stability and later degenerates in an asset that is difficult to use, difficult to adapt and maintain. When reaching this stage, the component becomes an obstacle for efficient reuse and should be replaced. Questions related to use of standard and de-facto standard components are addressed specifically. As an illustration of reuse issues, we present a successful implementation of a component based system which is widely used for industrial process control.
[industrial process control, Computer aided software engineering, reusable components, Industrial control, software systems design, very complex process, maintenance requirement, development organization, component generality, component based system, component based development, Electrical equipment industry, reuse issues, Power generation, Chemical industry, Automation, object-oriented programming, Process control, de-facto standard components, component functionality, component life cycle, Maintenance, software maintenance, case study, Electrical products industry, development environment, component reuse, process control, software reusability, Software systems, software standards, compatibility problems]
Investigating and improving a COTS-based software development process
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The work described in the paper is an investigation of COTS based software development within a particular NASA environment, with an emphasis on the processes used. Fifteen projects using a COTS based approach were studied and their actual process was documented. This process is evaluated to identify essential differences in comparison to traditional software development. The main differences, and the activities for which projects require more guidance, are requirements definition and COTS selection, high level design, integration and testing. Starting from these empirical observations, a new process and guidelines for COTS based development are developed and briefly presented. The new process is currently under experimentation.
[Software maintenance, COTS based approach, COTS selection, Programming, traditional software development, COTS-based software development process, requirements definition, high level design, software packages, commercial off-the-shelf products, component based software, Permission, aerospace computing, software tools, Standards development, NASA environment, NASA, software development management, Educational institutions, empirical observations, Software packages, Packaging, computer aided software engineering, Software tools, Software engineering]
PPT: a COTS integration case study
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
T. Rowe Price Investment Technologies built the Product and Project Tracking System (PPT) to reduce the human resources needed to track and forecast information technology projects. Instead of developing or purchasing a new system, the need was met by integrating commercial-off-the-shelf (COTS) products already used and licensed by the company. The conclusion can be made that this approach reduces development costs while providing more flexibility than a single vendor solution. The paper describes the process used and issues encountered in building a system from software products generally intended for stand-alone applications. It discusses the rationale behind the system, the choice of products, the software engineering process used, the handling of changes, and modifications made in business practice. A discussion is conducted on the initial return on investment and ongoing support requirements.
[software products, Computer aided software engineering, software engineering process, business practice, Project management, Humans, Companies, DP management, Information management, information technology project tracking, COTS integration case study, Investments, PPT, software packages, Technology forecasting, single vendor solution, financial data processing, Business, development costs, project management, human resources, Information technology, stand-alone applications, ongoing support requirements, systems analysis, commercial-off-the-shelf products, return on investment, Product and Project Tracking System, Software engineering]
Requirements engineering for product families
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
In search of improved software quality and high productivity, software reuse has become a key research area. One of the most promising reuse approaches is product families. However, current practices in requirements engineering do not support product families. The paper describes a definition hierarchy method for requirements capturing, structuring, analysis and documentation. This method helps to identify architectural drivers of the product family and shows how different products in the family vary.
[Productivity, software reuse, Laboratories, Globalization, Documentation, documentation, Product design, software quality, reuse approaches, Cultural differences, requirements capture, formal specification, Design engineering, requirements engineering, productivity, systems analysis, product families, architectural drivers, Software quality, Permission, software reusability, definition hierarchy method, Product development]
Extending requirement specifications using analogy
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Creating the specifications for a new system is a labour intensive task. Analogical reasoning provides a flexible mechanism to retrieve and adapt past specifications. Previous work in applying analogical reasoning to requirement specifications has departed from the psychological foundations of analogical reasoning, introducing specific ontologies and abstract templates to constrain the reasoning process. We argue that similar results can be obtained without introducing domain specific constraints and that using analogical reasoning engines based on well-established psychological theories, such as the structure-mapping engine, will lead to better results and scale up more effectively.
[System testing, past specification retrieval, Computational modeling, Buildings, psychological theories, Psychology, analogical reasoning, Ontologies, formal specification, Engines, Convergence, past specification adaptation, abstract templates, psychology, requirement specifications, Permission, Constraint theory, case-based reasoning, structure-mapping engine, ontologies]
'It's engineering Jim...but not as we know it'. Software engineering-solution to the software crisis, or part of the problem?
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This paper considers the impact and role of the 'engineering' metaphor, and argues that it is time to reconsider its impact on software development practice.
[Costs, Maintenance engineering, Programming, Reliability engineering, software crisis, Information systems, Computer science, software development practice, Education, Permission, software engineering, Informatics, Software engineering]
Producing more reliable software: mature software engineering process vs. state-of-the-art technology?
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
A customer of high assurance software recently sponsored a software engineering experiment in which a real-time software system was developed concurrently by two popular software development methodologies. One company specialized in the state-of-the-practice waterfall method rated at a Capability Maturity Model Level 4. A second developer employed his mathematically based formal method with automatic code generation. As specified in separate contracts, C++ code plus development documentation and process and product metrics (errors) were to be delivered. Both companies were given identical functional specs and agreed to a generous and equal cost, schedule, and explicit functional reliability objectives. At conclusion of the experiment an independent third party determined through extensive statistical testing that neither methodology was able to meet the user's reliability objectives within cost and schedule constraints. The metrics collected revealed the strengths and weaknesses of each methodology and why they were not able to reach customer reliability objectives. This paper explores the specification for the system under development, the two competing development processes, the products and metrics captured during development, the analysis tools and testing techniques by the third party, and the results of a reliability and process analysis.
[Real time systems, System testing, reliable software production, program testing, software reliability, process analysis, Programming, process metrics, automatic code generation, formal specification, program compilers, product metrics, mature software engineering process, real-time software system, analysis tools, Cost function, Capability maturity model, statistical testing, Contracts, cost constraints, Statistical analysis, C++ code, Documentation, testing, state-of-the-art technology, high assurance software, mathematically based formal method, waterfall method, Software systems, schedule constraints, development documentation, Software engineering, software metrics]
Improving problem-oriented mailing list archives with MCS
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Developers often use electronic mailing lists when seeking assistance with a particular software application. The archives of these mailing lists provide a rich repository of problem-solving knowledge. Developers seeking a quick answer to a problem find these archives inconvenient, because they lack efficient searching mechanisms, and retain the structure of the original conversational threads which are rarely relevant to the knowledge seeker. We present a system called MCS which improves mailing list archives through a process called condensation. Condensation involves several tasks: extracting only messages of longer-term relevance, adding metadata to those messages to improve searching, and potentially editing the content of the messages when appropriate to clarify. The condensation process is performed by a human editor (assisted by a tool), rather than by an artificial intelligence (AI) system. We describe the design and implementation of MCS, and compare it to related systems. We also present our experiences condensing a 1428 message mailing list archive to an archive containing only 177 messages (an 88% reduction). The condensation required only 1.5 minutes of editor effort per message. The condensed archive was adopted by the users of the mailing list.
[System testing, meta data, project management, metadata, Collaborative software, Laboratories, Humans, information retrieval, searching, Programming, Application software, Yarn, message extraction, problem-oriented mailing list archives, Permission, condensation, software engineering, problem-solving knowledge, Problem-solving, MCS, software application, Artificial intelligence, message content editing, electronic mailing lists]
Broad-spectrum studies of log file analysis
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This paper reports on research into applying the technique of log file analysis for checking test results to a broad range of testing and other tasks. The studies undertaken included applying log file analysis to both unit- and system-level testing and to requirements of both safety-critical and non-critical systems, and the use of log file analysis in combination with other testing methods. The paper also reports on the technique of using log file analyzers to simulate the software under test, both in order to validate the analyzers and to clarify requirements. It also discusses practical issues to do with the completeness of the approach, and includes comparisons to other recently-published approaches to log file analysis.
[Software testing, software simulation, System testing, program testing, safety-critical software, Programming, Inspection, unit-level testing, log file analysis, Software safety, Code standards, formal specification, Computer science, Analytical models, safety-critical systems, noncritical systems, formal verification, Failure analysis, Permission, system-level testing, test results checking]
Multivariate visualization in observation-based testing
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
We explore the use of multivariate visualization techniques to support a new approach to test data selection, called observation-based testing. Applications of multivariate visualization are described, including: evaluating and improving synthetic tests; filtering regression test suites; filtering captured operational executions; comparing test suites; and assessing bug reports. These applications are illustrated by the use of correspondence analysis to analyze test inputs for the GNU GCC compiler.
[Software testing, synthetic tests, Data analysis, Filtering, program testing, Instruments, correspondence analysis, test data selection, bug report assessment, observation-based testing, Application software, program visualization, captured operational execution filtering, Runtime, regression test suite filtering, Message passing, Data visualization, GNU GCC compiler, Permission, test suite comparison, multivariate visualization, Graphical user interfaces]
An empirical study of regression test application frequency
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Regression testing is an expensive maintenance process used to revalidate modified software. Regression test selection (RTS) techniques try to lower the cost of regression testing by selecting and running a subset of the existing test cases. Many such techniques have been proposed and initial studies show that they can produce savings. We believe, however, that issues such as the frequency with which testing is done have a strong effect on the behavior of these techniques. Therefore, we conducted an experiment to assess the effects of test application frequency on the costs and benefits of regression test selection techniques. Our results expose essential tradeoffs that should be considered when using these techniques over a series of software releases.
[Software testing, Performance evaluation, System testing, Software maintenance, Costs, program testing, Educational institutions, regression test application frequency, Application software, software maintenance, Computer science, maintenance process, modified software revalidation, Permission, Frequency]
Testing levels for object-oriented software
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
One of the characteristics of object-oriented software is the complex dependency that may exist between classes due to inheritance, association and aggregation relationships. Hence, where to start testing and how to define an integration strategy are issues that require further investigation. This paper presents an approach to define a test order by exploiting a model produced during design stages (e.g., using OMT, UML), namely the class diagram. Our goal is to minimize the number of stubs to be constructed in order to decrease the cost of testing. This is done by testing a class after the classes it depends on. The novelty of the test order lies in the fact that it takes account of: (i) dynamic (polymorphism) dependencies; (ii) abstract classes that cannot be instantiated, making some testing levels infeasible. The test order is represented by a graph showing which testing levels must be done in sequence and which ones may be done independently. It also provides information about the classes involved in each level and how they are involved (e.g., instantiation or not). The approach is implemented in a tool called TOONS (testing level generator for object-oriented software). It is applied to an industrial case study from the avionics domain.
[Software testing, System testing, Costs, program testing, Unified modeling language, object-oriented software, Aerospace electronics, inheritance, association, Permission, aggregation relationships, abstract classes, object-oriented programming, class diagram, Object oriented modeling, abstract data types, testing, testing levels, Aerodynamics, avionics, Character recognition, test order, dynamic dependencies, Software tools, TOONS]
Software evolution in componentware using requirements/assurances contracts
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
In practice, pure top-down and refinement based development processes are not sufficient. Usually, an iterative and incremental approach is applied instead. Existing methodologies, however, do not support such evolutionary development processes very well. We present the basic concepts of an overall methodology based on componentware and software evolution. The foundation of our methodology is a novel, well founded model for component based systems. This model is sufficiently powerful to handle the fundamental structural and behavioral aspects of componentware and object orientation. Based on the model, we are able to provide a clear definition of a software evolution step. During development, each evolution step implies changes of an appropriate set of development documents. In order to model and track the dependencies between these documents, we introduce the concept of requirements/assurances contracts. These contracts can be rechecked whenever the specification of a component evolves, enabling us to determine the impacts of the respective evolution step. Based on the proposed approach, developers are able to track and manage the software evolution process and to recognize and avoid failures due to software evolution. A short example shows the usefulness of the presented concepts and introduces a practical description technique for requirements/assurances contracts.
[requirements/assurances contracts, evolution step, Reliability engineering, development documents, software quality, contracts, formal specification, software evolution, software architecture, refinement based development processes, Software architecture, component based systems, Computer architecture, evolutionary development processes, Iterative methods, Contracts, object-oriented programming, Buildings, object orientation, Power system modeling, Programming profession, Software development management, incremental approach, systems analysis, behavioral aspects, Software engineering, componentware]
An integrated cost model for software reuse
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Several cost models have been proposed in the past for estimating, predicting, and analyzing the costs of software reuse. The authors analyze existing models, explain their variance, and propose a tool-supported comprehensive model that encompasses most of the existing models.
[Productivity, component engineering, integrated cost model, software reuse, domain engineering, application engineering, Predictive models, investment, Asset management, Application software, Investments, return on investment, Software quality, COCOMO, software reusability, tool-supported comprehensive model, Economic forecasting, Cost function, software cost estimation, Software reusability, Analysis of variance]
Data mining library reuse patterns using generalized association rules
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
It is shown how data mining can be used to discover library reuse patterns in existing applications. Specifically, we consider the problem of discovering library classes and member functions that are typically reused in combination by application classes. The paper improves upon earlier research using "association rules" (A. Michail, 1999) by taking into account the inheritance hierarchy using "generalized association rules". This turns out to be a non-trivial but worthwhile endeavor. By browsing generalized association rules, a developer can discover patterns in library usage in a way that takes into account inheritance relationships. For example, such a rule might tell us that application classes that inherit from a particular library class often instantiate another class or one of its descendents. We illustrate the approach using our tool, CodeWeb, by demonstrating characteristic ways in which applications reuse classes in the KDE application framework.
[data mining, association rules, Data engineering, inheritance, library classes, Data mining, Open source software, software libraries, inheritance relationships, Permission, generalized association rules, CodeWeb, KDE application framework, object-oriented programming, library reuse patterns, inheritance hierarchy, member functions, Association rules, Application software, application classes, Computer science, Software libraries, Writing, software reusability, library class, Paints]
Towards a taxonomy of software connectors
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Software systems of today are frequently composed of prefabricated, heterogeneous components that provide complex functionality and engage in complex interactions. Existing research on component based development has mostly focused on component structure, interfaces, and functionality. Recently, software architecture has emerged as an area that also places significant importance on component interactions, embodied in the notion of software connectors. However, the current level of understanding and support for connectors has been insufficient. This has resulted in their inconsistent treatment and a notable lack of understanding of what the fundamental building blocks of software interaction are and how they can be composed into more complex interactions. The paper attempts to address this problem. It presents a comprehensive classification framework and taxonomy of software connectors. The taxonomy is obtained through an extensive analysis of existing component interactions. The taxonomy is used both to understand existing software connectors and to suggest new, unprecedented connectors. We demonstrate the use of the taxonomy on the architecture of a large, existing system.
[complex functionality, client-server systems, unprecedented connectors, Taxonomy, software systems, Programming, software interaction, Middleware, prefabricated heterogeneous components, Connectors, Computer science, software connector taxonomy, software architecture, component based development, Software architecture, fundamental building blocks, complex interactions, Computer architecture, Packaging, Software systems, Software standards, component interactions, classification framework]
A formal approach for designing CORBA based applications
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The design of distributed applications in a CORBA based environment can be carried out by means of an incremental approach, which starts from the specification and leads to the high level architectural design. This is done by introducing in the specification all typical elements of CORBA and by providing a methodological support to the designers. The paper discusses a methodology to transform a formal specification written in TRIO into a high level design document written using an extension of TRIO named TC. The TC language is suited to formally describe the high level architecture of a CORBA based application. The methodology and the associated language are presented by means of an example involving a real supervision and control system.
[high level design document, TRIO, Design methodology, Control systems, TC language, Distributed computing, formal specification, distributed applications, formal approach, Computer architecture, Permission, Hardware, high level architectural design, high level architecture, distributed object management, distributed programming, Banking, computerized control, specification, Logic design, Formal specifications, Information technology, supervision and control system, incremental approach, methodological support, CORBA based application design]
Simulation in software engineering training
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Simulation is frequently used for training in many application areas like aviation and economics, but not in software engineering. We present the SESAM project which focuses on software engineering education using simulation. In the SESAM project a simulator was developed. Using this simulator, a student can take the role of a software project manager. The simulated software project can be finished within a couple of hours because it is simulated in "quick-motion" mode. The background and goals of the SESAM project are presented. A new simulation model, the so called QA model, is introduced. The model behavior is demonstrated by investigating and comparing different strategies for software development. The results of experiments based on the QA model are reported. Finally, conclusions are drawn from the experiments and future work is outlined.
[software engineering training, student, Project management, Programming, teaching, digital simulation, application areas, Quality assurance, software project manager, SESAM project, computer based training, quick-motion mode, Disaster management, QA model, Computer science education, Systems engineering education, model behavior, computer science education, project management, software development, Computational modeling, Computer simulation, software development management, Computer science, simulated software project, simulation model, future work, software engineering education, Software engineering]
Twenty dirty tricks to train software engineers
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Many employers find that graduates and sandwich students come to them poorly prepared for the every day problems encountered at the workplace. Although many university students undertake team projects at their institutions, an educational environment has limitations that prevent the participants experiencing the full range of problems encountered in the real world. To overcome this, action was taken on courses at the Plessey Telecommunications company and Loughborough University to disrupt the students' software development progress. These actions appear mean and vindictive, and are labeled 'dirty tricks' in the paper, but their value has been appreciated by both the students and their employers. The experiences and learning provided by twenty 'dirty tricks' are described and their contribution towards teaching. Essential workplace skills are identified. The feedback from both students and employers has been mostly informal but the universally favourable comments received give strong indications that the courses achieved their aim of preparing the students for the workplace. The paper identifies some limitations on the number and types of 'dirty tricks' that can be employed at a university and concludes that companies would benefit if such dirty tricks were employed in company graduate induction programmes, as well as in university courses.
[dirty tricks, Programming, educational environment, teaching, software development progress, university students, Employment, Feedback, university courses, Permission, software engineering, Computer science education, Educational programs, computer science education, employers, Plessey Telecommunications company, Computational modeling, employment, workplace skills, Management training, Loughborough University, Computer science, every day problems, team projects, company graduate induction programmes, software engineering education, Software engineering]
Deriving test plans from architectural descriptions
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The paper presents an approach for deriving test plans for the conformance testing of a system implementation with respect to the formal description of its software architecture (SA). The SA describes a system in terms of its components and connections, therefore the derived test plans address the integration testing phase. We base our approach on a labelled transition system (LTS) modeling the SA dynamics, and on suitable abstractions of it, the Abstract Labelled Transition Systems (ALTSs). ALTSs offer specific views of the SA dynamics by concentrating on relevant features and abstracting away from uninteresting ones. ALTS is a tool we provide to the software architect that lets him/her focus on relevant behavioral patterns and more easily identify those that are meaningful for validation purposes. Intuitively, deriving an adequate set of functional test classes means deriving a set of paths appropriately covering the ALTS. We describe our approach in the scope of a real world case study and discuss in detail all the steps of our methodology, from ALTS identification to test plan generation.
[Software testing, System testing, test plan generation, program testing, Unified modeling language, SA dynamics, conformance testing, formal specification, software architecture, real world case study, Software architecture, integration testing phase, software architect, Production, Permission, Abstract Labelled Transition Systems, ALTS identification, formal description, derived test plans, Application software, behavioral patterns, LTS modeling, functional test classes, labelled transition system, test plan derivation, system implementation, Software systems, architectural descriptions, Software tools, Software engineering]
WYSIWYT testing in the spreadsheet paradigm: an empirical evaluation
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Is it possible to achieve some of the benefits of formal testing within the informal programming conventions of the spreadsheet paradigm? We have been working on an approach that attempts to do so via the development of a testing methodology for this paradigm. Our "What You See Is What You Test" (WYSIWYT) methodology supplements the convention by which spreadsheets provide automatic immediate visual feedback about values by providing automatic immediate visual feedback about "testedness". In previous work we described this methodology (G. Rothermal et al., 1998). We present empirical data about the methodology's effectiveness. Our results show that the use of the methodology was associated with significant improvement in testing effectiveness and efficiency, even with no training on the theory of testing or test adequacy that the model implements. These results may be due at least in part to the fact that use of the methodology was associated with a significant reduction in overconfidence.
[Software maintenance, program testing, testedness, Displays, spreadsheet programs, WYSIWYT testing, Engines, test adequacy, empirical evaluation, informal programming conventions, Feedback, Permission, Large-scale systems, spreadsheet paradigm, automatic immediate visual feedback, formal testing, testing effectiveness, What You See Is What You Test, testing methodology, Programming profession, Computer science, Computer languages, Automatic testing, overconfidence, empirical data, visual programming]
Integrating UML diagrams for production control systems
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The paper proposes to use SDL block diagrams, UML class diagrams, and UML behavior diagrams like collaboration diagrams, activity diagrams, and statecharts as a visual programming language. We describe a modeling approach for flexible, autonomous production agents, which are used for the decentralization of production control systems. In order to generate a (Java) implementation of a production control system from its specification, we define a precise semantics for the diagrams and we define how different (kinds of) diagrams are combined to a complete executable specification. Generally, generating code from UML behavior diagrams is not well understood. Frequently, the semantics of a UML behavior diagram depends on the topic and the aspect that is modeled and on the designer that created it. In addition, UML behavior diagrams usually model only example scenarios and do not describe all possible cases and possible exceptions. We overcome these problems by restricting the UML notation to a subset of the language that has a precise semantics. In addition, we define which kind of diagram should be used for which purpose and how the different kinds of diagrams are integrated to a consistent overall view.
[statecharts, Production systems, Unified modeling language, Control systems, Production facilities, Java implementation, visual languages, SDL block diagrams, executable specification, Embedded system, specification languages, UML notation, flexible autonomous production agents, Flexible manufacturing systems, precise semantics, example scenarios, UML behavior diagrams, Object oriented modeling, UML class diagrams, production control, production control systems, activity diagrams, programming language semantics, Computer science, collaboration diagrams, Collaboration, Production control, visual programming language, modeling approach]
Dragonfly: linking conceptual and implementation architectures of multiuser interactive systems
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Software architecture styles for developing multiuser applications are usually defined at a conceptual level, abstracting such low-level issues of distributed implementation as code replication, caching strategies and concurrency control policies. Ultimately, such conceptual architectures must be cast into code. The iterative design inherent in interactive systems implies that significant evolution will take place at the conceptual level. Equally, however, evolution occurs at the implementation level in order to tune performance. This paper introduces Dragonfly, a software architecture style that maintains a tight, bidirectional link between conceptual and implementation software architectures, allowing evolution to be performed at either level. Dragonfly has been implemented in the Java-based TeleComputing Developer (TCD) toolkit.
[Computer interfaces, user interface management systems, Dragonfly, Collaborative software, implementation architectures, multiuser interactive systems, Telecommunication computing, Application software, iterative design, caching strategies, software architecture, conceptual architectures, Java-based TeleComputing Developer toolkit, Software architecture, Interactive systems, concurrency control, Computer architecture, groupware, User interfaces, interactive systems, Collaborative work, code replication, Joining processes, software architecture styles]
A case study of open source software development: the Apache server
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
According to its proponents, open source style software development has the capacity to compete successfully, and perhaps in many cases displace, traditional commercial development methods. In order to begin investigating such claims, we examine the development process of a major open source application, the Apache web server. By using email archives of source code change history and problem reports we quantify aspects of developer participation, core team size, code ownership, productivity, defect density, and problem resolution interval for this OSS project. This analysis reveals a unique process, which performs well on important measures. We conclude that hybrid forms of development that borrow the most effective techniques from both the OSS and commercial worlds may lead to high performance software processes.
[Productivity, code ownership, Computer aided software engineering, Job shop scheduling, search engines, open source software development, Apache web server, Programming, defect density, Application software, History, Open source software, System-level design, Computer science, email archives, productivity, software process improvement, software engineering, Web server]
Multiple mass-market applications as components
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Truly successful models for component-based software development continue to prove elusive. One of the few is the use of operating system, database and similar programs in many systems. We address three related problems in this paper. First, we lack needed models. Second, we do not know the conditions under which such models can succeed. In particular, it is unclear whether the notable success with operating systems can be replicated. Third, we do not know whether certain specific models can succeed. We are addressing these problems by evaluating a particular model that shares important characteristics with the successful operating system example: using compatible PC packages as components. Our approach to evaluating such a model is to engage in a case study that aims to build an industrially successful system representative of an important class of systems. We report on our use of the model to develop a computational tool for reliability engineering. We draw two conclusions. First, this kind of model has the potential to succeed. Second, even today, the model can produce significant returns, but it clearly carries considerable risks.
[Modular construction, multiple mass-market applications, operating system, Programming, Reliability engineering, component-based software development, compatible PC packages, Application software, Computer science, database, Databases, Software packages, Operating systems, reliability engineering, Packaging, Permission, software engineering]
Developing and deploying software engineering courseware in an adaptable curriculum framework
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
We describe an effort to design an adaptable framework for teaching and learning in software engineering. We are developing a repository of asynchronous, multimedia courseware that facilitates the rapid incorporation of new advances in research and technology, enables courses to be tailored to individual student needs and interests, leverages innovations in educational technology and encourages innovation in teaching and in student learning. Our emphasis is on developing composable multi-level "knowledge and topic units" (KU/TUs) that can be employed to tailor course content and depth to fit the needs of a diverse student population. We have developed "live" and online course material for KU/TUs in software engineering and taught courses using this material. The framework was deployed in three software engineering courses (previously taught concurrently) and provides quite different learning environments for the students in each course and, to some extent, tailors the courses to individual students within the classes based on their skills, objectives and backgrounds. We describe efforts at formative evaluation. Student satisfaction is high and available measures of success, e.g., student performance, have improved markedly. We also describe a project now beginning to build on this prototype that will be accompanied by more extensive formative and summative evaluation.
[Costs, course content, individual students, student needs, Drives, teaching, software engineering courses, multimedia computing, educational technology, summative evaluation, KU/TUs, student satisfaction, adaptable curriculum framework, Education, Permission, online course material, software engineering, knowledge and topic units, courseware, Courseware, student learning, Educational programs, Technological innovation, computer science education, learning environments, Educational technology, diverse student population, Computer science, multimedia courseware, formative evaluation, software engineering courseware, student performance, Software engineering]
Achieving industrial relevance with academic excellence: lessons from the Oregon master of software engineering
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Many educational institutions are developing graduate programs in software engineering targeted to working professionals. These educators face the dilemma of providing programs with both industrial relevance and academic excellence. This paper describes our experience and lessons learned in developing such a program, the Oregon Master of Software Engineering (OMSE). It describes a structured approach to curriculum design, curriculum design principles and methods that can be applied to develop a quality professional program.
[Educational programs, computer science education, Oregon master of software engineering, Design methodology, academic excellence, graduate programs, professional program, Programming, industrial relevance, Educational institutions, Computer science, Information science, Software design, OMSE, educational courses, Computer industry, software engineering, Computer science education, curriculum design, Software engineering]
Inference of message sequence charts
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Software designers draw Message Sequence Charts for early modeling of the individual behaviors they expect from the concurrent system under design. Can they be sure that precisely the behaviors they have described are realizable by some implementation of the components of the concurrent system? If so, can one automatically synthesize concurrent state machines realizing the given MSCs? If, on the other hand, other unspecified and possibly unwanted scenarios are "implied" by their MSCs, can the software designer be automatically warned and provided the implied MSCs? In this paper we provide a framework in which all these questions are answered positively. We first describe the formal framework within which one can derive implied MSCs, and we then provide polynomial-time algorithms for implication, realizability, and synthesis. In particular, we describe a novel algorithm for checking deadlock-free (safe) realizability.
[Algorithm design and analysis, message sequence charts, Unified modeling language, Citation analysis, concurrent system, Software design, Automata, concurrency control, System recovery, Permission, concurrent state machines, polynomial-time algorithms, Polynomials, software engineering, Timing, formal framework, deadlock-free realizability, Pattern analysis]
Generating statechart designs from scenarios
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This paper presents an algorithm for automatically generating UML statecharts from a collection of UML sequence diagrams. Computer support for this transition between requirements and design is important for a successful application of UML's highly iterative, distributed software development process. There are three main issues which must be addressed when generating statecharts from sequence diagrams. Firstly, conflicts arising from the merging of independently developed sequence diagrams must be detected and resolved. Secondly, different sequence diagrams often contain identical or similar behaviors. For a true interleaving of the sequence diagrams, these behaviors must be recognized and merged. Finally, generated statecharts usually are only an approximation of the system and thus must be hand-modified and refined by designers. As such, the generated artifact should be highly structured and readable. In terms of statecharts, this corresponds to the introduction of hierarchy. Our algorithm successfully tackles all three of these aspects and will be illustrated in this paper with a well-known ATM example.
[Algorithm design and analysis, Software prototyping, Unified modeling language, NASA, Merging, statechart designs generation, Programming, UML statecharts, scenarios, Application software, distributed software development process, Prototypes, UML sequence diagrams, Interleaved codes, software engineering, Software engineering]
Object model resurrection-an object oriented maintenance activity
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This paper addresses the problem of reengineering object-oriented systems that have incurred increased maintenance cost due to long development time-span and project lifecycle. When an Incremental Approach is used to develop an object-oriented system, there is a risk that the class design and the overall object model will deteriorate in quality with each increment. A recent research work suggested a process activity (Class Deterioration Detection and Resurrection-CDDR process activity) and a technique for the detection and resurrection of deteriorated classes. That work focussed on one particular aspect of object-oriented software maintenance-Class Quality Deterioration due to lack of cohesion induced by high coupling. This paper addresses the problem of deteriorating object-oriented design due to code and class growth (increase in the number of classes) within a system. A Code/Class Growth Control process activity (CGC) is suggested to avoid and eliminate Repetitious Code and Classes within the evolving system. The CDDR and CGC process activities are used to build an evolving Maintenance process model for object-oriented systems. The presented maintenance process model is an effective way to periodically assess and resurrect the quality of an object-oriented design during incremental development.
[Software maintenance, Costs, object-oriented programming, Object oriented modeling, Process control, Control systems, software quality, software maintenance, incremental development, object oriented maintenance activity, project lifecycle, Design optimization, Degradation, systems re-engineering, maintenance cost, object model resurrection, object-oriented systems reengineering, Permission, Lakes, Software systems, class design]
Action Language: a specification language for model checking reactive systems
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
We present a specification language called Action Language for model checking software specifications. Action Language forms an interface between transition system models that a model checker generates and high level specification languages such as Statecharts, RSML and SCR-similar to an assembly language between a microprocessor and a programming language. We show that Action Language translations of Statecharts and SCR specifications are compact and they preserve the structure of the original specification. Action Language allows specification of both synchronous and asynchronous systems. It also supports modular specifications to enable compositional model checking.
[microprocessor, Thyristors, SCR, model checking reactive systems, RSML, Software safety, formal specification, Embedded system, Statecharts, compositional model checking, specification languages, Permission, Hardware, programming language, Logic, model checking software specifications, specification language, transition system models, modular specifications, Specification languages, Action Language, SCR specifications, Computer bugs, Software systems, Power system reliability]
Three approximation techniques for ASTRAL symbolic model checking of infinite state real-time systems
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
ASTRAL is a high-level formal specification language for real-time systems. It has structuring mechanisms that allow one to build modularized specifications of complex real-time systems with layering. Based upon the ASTRAL symbolic model checker, three approximation techniques to speed-up the model checking process for use in debugging a specification are presented. The techniques are random walk, partial image and dynamic environment generation. Ten mutation tests on a railroad crossing benchmark are used to compare the performance of the techniques applied separately and in combination. The test results are presented and analyzed.
[Real time systems, Software testing, program debugging, formal specification, random walk, Tree graphs, specification languages, Benchmark testing, Permission, debugging, Libraries, ASTRAL symbolic model checking, railroad crossing benchmark, symbolic model checker, Debugging, infinite state real-time systems, partial image, mutation tests, dynamic environment generation, Formal specifications, Computer science, modularized specifications, real-time systems, approximation techniques, structuring mechanisms, Timing, high-level formal specification language]
Component design of retargetable program analysis tools that reuse intermediate representations
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Interactive program analysis tools are often tailored to one particular representation of programs, making adaptation to a new language costly. One way to ease adaptability is to introduce an intermediate abstraction-an adaptation layer-between an existing language representation and the program analysis tool. This adaptation layer translates the tool's queries into queries on the particular representation. Our experiments with this approach on the StarTool program analysis tool resulted in low-cost retargets for C, Tcl/Tk, and Ada. Required adjustments to the approach, however, led to insights for improving a client's retargetability. First, retargeting was eased by having our tool import a tool-centric (i.e.,client-centric) interface rather than a general-purpose, language-neutral representation interface. Second, our adaptation layer exports two interfaces, a representation interface supporting queries on the represented program and a language interface that the client queries to configure itself suitably for the given language. Straightforward object-oriented extensions enhance reuse and ease the development of multilanguage tools.
[retargetable program analysis tools, program analysis tool, object-oriented programming, StarTool program analysis tool, language-neutral representation interface, Drives, Data structures, multilanguage tools, Programming profession, Information analysis, Computer science, Design engineering, intermediate representations reuse, Software design, component design, object-oriented extensions, interactive program analysis tools, systems analysis, Permission, software reusability, software tools, Software tools]
Light-weight context recovery for efficient and accurate program analyses
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
To compute accurate information efficiently for programs that use pointer variables, a program analysis must account for the fact that a procedure may access different sets of memory locations when the procedure is invoked under different callsites. This paper presents light-weight context recovery, a technique that can efficiently determine whether a memory location is accessed by a procedure under a specific callsite. The paper also presents a technique that uses this information to improve the precision and efficiency of program analyses. Our empirical studies show that (1) light-weight context recovery can be quite precise in identifying the memory locations accessed by a procedure under a specific callsite and (2) distinguishing memory locations accessed by a procedure under different callsites can significantly improve the precision and the efficiency of program analyses on programs that use pointer variables.
[Software testing, Costs, Programming, Educational institutions, Throughput, Data mining, Software debugging, system recovery, Delay, memory locations, Information analysis, program analyses, systems analysis, lightweight context recovery, light-weight context recovery, memory location, Software tools, program slicing, pointer variables]
A replicated assessment and comparison of common software cost modeling techniques
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Delivering a software product on time, within budget, and to an agreed level of quality is a critical concern for many software organizations. Underestimating software costs can have detrimental effects on the quality of the delivered software and thus on a company's business reputation and competitiveness. On the other hand, overestimation of software cost can result in missed opportunities to funds in other projects. In response to industry demand, a myriad of estimation techniques has been proposed during the last three decades. In order to assess the suitability of a technique from a diverse selection, its performance and relative merits must be compared. The current study replicates a comprehensive comparison of common estimation techniques within different organizational contexts, using data from the European Space Agency. Our study is motivated by the challenge to assess the feasibility of using multi-organization data to build cost models and the benefits gained from company-specific data collection. Using the European Space Agency data set, we investigated a yet unexplored application domain, including military and space projects. The results showed that traditional techniques, namely, ordinary least-squares regression and analysis of variance outperformed analogy-based estimation and regression trees. Consistent with the results of the replicated study no significant difference was found in accuracy between estimates derived from company-specific data and estimates derived from multi-organizational data.
[Performance evaluation, Costs, replicated assessment, software costs, least-squares regression, common software cost modeling, Application software, Aerospace industry, Software quality, Permission, software cost estimation, Australia, software product, Regression tree analysis, Analysis of variance, software organizations, Business, cost models]
Characterization of risky projects based on project managers evaluation
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
During the process of software development, senior managers often find indications that projects are risky and take appropriate actions to recover them from this dangerous status. If senior managers fail to detect such risks, it is possible that such projects may collapse completely. In this paper, we propose a new scheme for the characterization of risky projects based on an evaluation by the project manager. In order to acquire the relevant data to make such an assessment, we first designed a questionnaire from five view-points within the projects: requirements, estimations, team organization, planning capability and project management activities. Each of these viewpoints consisted of a number of concrete questions. We then analyzed the responses to the questionnaires as provided by project managers by applying a logistic regression analysis. That is, we determined the coefficients of the logistic model from a set of the questionnaire responses. The experimental results using actual project data in Company A showed that 27 projects out of 32 were predicted correctly. Thus we would expect that the proposed characterizing scheme is the first step toward predicting which projects are risky at an early phase of the development.
[risk management, project management, project managers evaluation, requirements, logistic regression analysis, software development, Project management, software development management, Programming, team organization, Regression analysis, Software development management, risky projects, Engineering management, Permission, Concrete, Risk management, planning capability, Logistics, Software engineering]
Implementing incremental code migration with XML
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
We demonstrate how XML and related technologies can be used for code mobility at any granularity, thus overcoming the restrictions of existing approaches. By not fixing a particular granularity for mobile code, we enable complete programs as well as individual lines of code to be sent across the network. We define the concept of incremental code mobility as the ability to migrate and add, remove, or replace code fragments (i.e., increments) in a remote program. The combination of fine-grained and incremental migration achieves a previously unavailable degree of flexibility. We examine the application of incremental and fine-grained code migration to a variety of domains, including user interface management, application management on mobile thin clients, for example PDAs, and management of distributed documents.
[Java, network computers, Scalability, code mobility, mobile thin clients, Educational institutions, PDA, fine-grained code migration, user interface management, incremental code migration, Computer science, Network servers, application management, XML, granularity, Bandwidth, User interfaces, Permission, mobile code, distributed document management, Personal digital assistants, distributed programming, hypermedia markup languages]
Principled design of the modern Web architecture
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The World Wide Web has succeeded in the large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia system. The modern Web architecture emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. We introduce the Representational State Transfer (REST) architectural style, developed as an abstract model of the Web architecture to guide our redesign and definition of the Hypertext Transfer Protocol and Uniform Resource Identifiers. We describe the software engineering principles guiding REST and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. We then compare the abstract model to the currently deployed Web architecture in order to elicit mismatches between the existing protocols and the applications they are intended to support.
[information resources, Protocols, Scalability, Service oriented architecture, hypermedia, World Wide Web, Security, Delay, scalability, software architecture, distributed hypermedia system, security, Software architecture, transport protocols, Uniform Resource Identifiers, Computer architecture, legacy systems, Representational State Transfer, Hypertext Transfer Protocol, component interactions, software engineering, Internet, Representational state transfer, Web sites]
A study on exception detection and handling using aspect-oriented programming
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Aspect oriented programming (AOP) is intended to ease situations that involve many kinds of code tangling. The paper reports on a study to investigate AOP's ability to ease tangling related to exception detection and handling. We took an existing framework written in Java/sup TM/, the JWAM framework, and partially reengineered its exception detection and handling aspects using AspectJ/sup TM/, an aspect oriented programming extension to Java. We found that AspectJ supported implementations that drastically reduced the portion of the code related to exception detection and handling. In one scenario, we were able to reduce that code by a factor of 4. We also found that, with respect to the original implementation in plain Java, AspectJ provided better support for different configurations of exceptional behaviors, more tolerance for changes in the specifications of exceptional behaviors, better support for incremental development, better reuse, automatic enforcement of contracts in applications that use the framework, and cleaner program texts. We also found some weaknesses of AspectJ that should be addressed in the future.
[exception detection, aspect oriented programming, Laboratories, Humans, exception handling, code tangling, specifications, JWAM framework, Permission, Contracts, AspectJ, Java, object-oriented programming, AOP, Application software, incremental development, program texts, Computer science, systems re-engineering, Computer errors, Software systems, automatic contract enforcement, partial reengineering, exceptional behaviors, Usability]
A case study in root cause defect analysis
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
There are three interdependent factors that drive our software development processes: interval, quality and cost. As market pressures continue to demand new features ever more rapidly, the challenge is to meet those demands while increasing, or at least not sacrificing, quality. One advantage of defect prevention as an upstream quality improvement practice is the beneficial effect it can have on interval: higher quality early in the process results in fewer defects to be found and repaired in the later parts of the process, thus causing an indirect interval reduction. We report a retrospective root cause defect analysis study of the defect Modification Requests (MRs) discovered while building, testing, and deploying a release of a transmission network element product. We subsequently introduced this analysis methodology into new development projects as an in-process measurement collection requirement for each major defect MR. We present the experimental design of our case study discussing the novel approach we have taken to defect and root cause classification and the mechanisms we have used for randomly selecting the MRs to analyze and collecting the analyses via a Web interface. We then present the results of our analyses of the MRs and describe the defects and root causes that we found, and delineate the countermeasures created to either prevent those defects and their root causes or detect them at the earliest possible point in the development process. We conclude with lessons learned from the case study and resulting ongoing improvement activities.
[Web interface, Computer aided software engineering, transmission network element product, software reliability, Project management, Optical computing, countermeasures, Optical fiber networks, Drives, Programming, software quality, telecommunication computing, upstream quality improvement practice, Intelligent networks, retrospective root cause defect analysis study, process improvement activities, software process improvement, root cause defect analysis, indirect interval reduction, Hardware, Computer networks, software development processes, development projects, Optical network units, market pressures, Modification Requests, case study, in-process measurement collection requirement, root cause classification, quality assurance, defect MR, defect prevention, analysis methodology]
Bandera: extracting finite-state models from Java source code
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Finite-state verification techniques, such as model checking, have shown promise as a cost-effective means for finding defects in hardware designs. To date, the application of these techniques to software has been hindered by several obstacles. Chief among these is the problem of constructing a finite-state model that approximates the executable behavior of the software system of interest. Current best-practice involves hand construction of models which is expensive (prohibitive for all but the smallest systems), prone to errors (which can result in misleading verification results), and difficult to optimize (which is necessary to combat the exponential complexity of verification algorithms). The authors describe an integrated collection of program analysis and transformation components, called Bandera, that enables the automatic extraction of safe, compact finite-state models from program source code. Bandera takes as input Java source code and generates a program model in the input language of one of several existing verification tools; Bandera also maps verifier outputs back to the original source code. We discuss the major components of Bandera and give an overview of how it can be used to model check correctness properties of Java programs.
[program specialization, Bandera, program verification, input language, Humans, verifier outputs, best-practice, Data mining, finite state machines, software system, finite-state verification techniques, integrated collection, verification tools, correctness properties, program transformation components, program analysis, verification algorithms, model extraction, Hardware, Manufacturing, finite-state models, Mathematical model, Logic, Java programs, Java, Java source code, abstract interpretation, hardware design defects, Explosions, executable behavior, exponential complexity, Computer science, Computer languages, model checking, system monitoring, program source code, automatic extraction]
Quickly detecting relevant program invariants
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Explicitly stated program invariants can help programmers by characterizing certain aspects of program execution and identifying program properties that must be preserved when modifying code. Unfortunately, these invariants are usually absent from code. Previous work showed how to dynamically detect invariants from program traces by looking for patterns in and relationships among variable values. A prototype implementation, Daikon, accurately recovered invariants from formally-specified programs, and the invariants it detected in other programs assisted programmers in a software evolution task. However, Daikon suffered from reporting too many invariants, many of which were not useful, and also failed to report some desired invariants. The paper presents, and gives experimental evidence of the efficacy of, four approaches for increasing the relevance of invariants reported by a dynamic invariant detector. One of them (exploiting unused polymorphism), adds desired invariants to the output. The other three (suppressing implied invariants, limiting which variables are compared to one another, and ignoring unchanged values), eliminate undesired invariants from the output and also improve runtime by reducing the work done by the invariant detector.
[Costs, program testing, software reliability, quick detection, unused polymorphism, Data mining, formal specification, Engines, Daikon, implied invariants, Runtime, relevant program invariant detection, unchanged values, Detectors, Permission, software evolution task, Testing, program properties, Java, programming theory, program execution, formally-specified programs, Programming profession, Computer science, dynamic invariant detector, program traces, explicitly stated program invariants, prototype implementation, system monitoring, undesired invariants, invariant detector]
Characterizing implicit information during peer review meetings
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Disciplines like software engineering evolve over time by studying some practices and feeding back those results to improve the practice. The empirical approach presented in the paper is used to analyze the nature of the information shared during peer review meetings (PRMs) held in an industrial software engineering project. The results obtained show that although a PRM is categorized as a verification practice, it is also a golden opportunity for project personnel to share information about technical solutions, a decision's rationale or quality guidelines. The contribution of PRMs is not restricted to the rapid detection of anomalies; they also provide the opportunity for project team members to share implicit information. PRM efficiency cannot solely be measured through an anomaly detection rate.
[technical solutions, human factors, Software performance, Programming, implicit information, PRM efficiency, Information management, software quality, Personnel, project team members, Information analysis, Guidelines, verification practice, project personnel, Permission, peer review meetings, quality guidelines, project management, Collaborative software, software development management, anomaly detection rate, empirical approach, industrial software engineering project, Computer industry, personnel, Software engineering]
Object-oriented inspection in the face of delocalisation
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Software inspection is now widely accepted as an effective technique for defect detection. This acceptance is largely based on studies using procedural program code. The paper presents empirical evidence that raises significant questions about the application of inspection to object oriented code. A detailed analysis of the "hard to find" defects during an inspection experiment shows that many of them can be characterised as "delocalised": the information needed to recognise the defect is distributed throughout the software. The paper shows that key features of object oriented technology are likely to exaggerate delocalisation. As a result, it is argued that new methods of inspection for object oriented code are required. These must address: partitioning code for inspection ("what to read"), reading strategies ("how to read"), and support for understanding what isn't read, i.e., "localising the delocalisation".
[Java, object-oriented programming, procedural program code, code partitioning, Poles and towers, object oriented code, software inspection, hard to find defects, Inspection, Paper technology, Application software, Character recognition, Information analysis, Computer science, inspection experiment, object oriented inspection, Software libraries, defect detection, Permission, system monitoring, reading strategies, object oriented technology, delocalisation, inspection]
An inheritance-based technique for building simulation proofs incrementally
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The paper presents a technique for incrementally constructing safety specifications, abstract algorithm descriptions, and simulation proofs showing that algorithms meet their specifications. The technique for building specifications (and algorithms) allows a child specification (or algorithm) to inherit from its parent by two forms of incremental modification: (a) interface extension, where new forms of interaction are added to the parent's interface, and (b) specialization (subtyping), where new data, restrictions, and effects are added to the parent's behavior description. The combination of interface extension and specialization constitutes a powerful and expressive incremental modification mechanism for describing changes that do not override the behavior of the parent, although it may introduce new behavior. Consider the case when incremental modification is applied to both a parent specification S and a parent algorithm A. A proof that the child algorithm A' implements the child specification S' can be built incrementally upon a simulation proof that algorithm A implements specification S. The new work required involves reasoning about the modifications, but does not require repetition of the reasoning in the original simulation proof. The paper presents the technique mathematically, in terms of automata. The technique has already been used to model and validate a full-fledged group communication system (I. Keidar and R. Khazan, 1999); the methodology and results of that experiment are summarized.
[program verification, automata theory, incremental simulation proof design, parent algorithm, reasoning, parent behavior description, inheritance, child specification, expressive incremental modification mechanism, type theory, abstract algorithm descriptions, formal specification, parent specification, Safety, Large-scale systems, theorem proving, Object oriented modeling, Computational modeling, Computer simulation, inheritance based technique, Power system modeling, subtyping, Computer science, interface extension, safety specifications, group communication system, child algorithm, Software systems, specialization, incremental modification, Software engineering, Context modeling]
Verification of time partitioning in the DEOS scheduler kernel
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The paper describes an experiment to use the Spin model checking system to support automated verification of time partitioning in the Honeywell DEOS real time scheduling kernel. The goal of the experiment was to investigate whether model checking could be used to find a subtle implementation error that was originally discovered and fixed during the standard formal review process. To conduct the experiment, a core slice of the DEOS scheduling kernel was first translated without abstraction from C++ into Promela (the input language for Spin). We constructed an abstract "test-driver" environment and carefully introduced several abstractions into the system to support verification. Several experiments were run to attempt to verify that the system implementation adhered to the critical time partitioning requirements. During these experiments, the known error was rediscovered in the time partitioning implementation. We believe this case study provides several insights into how to develop cost-effective methods and tools to support the software design and implementation review process.
[Software testing, Real time systems, System testing, standard formal review process, program verification, Spin model checking system, input language, Spin, software design, DEOS scheduler kernel, Promela, Operating systems, Honeywell DEOS real time scheduling kernel, scheduling, Safety, Kernel, abstract test-driver environment, operating system kernels, automatic programming, C++, Collaborative software, NASA, implementation review process, case study, program interpreters, time partitioning verification, subtle implementation error, critical time partitioning requirements, model checking, Automatic testing, real-time systems, system implementation, automated verification, Software engineering]
Graphical animation of behavior models
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Graphical animation is a way of visualizing the behavior of design models. This visualization is of use in validating a design model against informally specified requirements and in interpreting the meaning and significance of analysis results in relation to the problem domain. The authors describe how behavior models specified by Labeled Transition Systems (LTS) can drive graphical animations. The semantic framework for the approach is based on Timed Automata. Animations are described by an XML document that is used to generate a set of JavaBeans. The elaborated JavaBeans perform the animation actions as directed by the LTS model.
[Visualization, program verification, finite automata, computational linguistics, design model behavior visualization, design model validation, analysis results, program visualization, computer animation, Permission, graphical animations, Network address translation, distributed object management, hypermedia markup languages, behavior models, Java, informally specified requirements, Buildings, semantic framework, Labeled Transition Systems, Educational institutions, graphical animation, problem domain, animation actions, Graphics, JavaBeans, LTS model, Automata, XML, XML document, Animation, Timed Automata]
Towards the principled design of software engineering diagrams
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Diagrammatic specification, modelling and programming languages are increasingly prevalent in software engineering and, it is often claimed, provide natural representations which permit intuitive reasoning. A desirable goal of software engineering is the rigorous justification of such reasoning, yet many formal accounts of diagrammatic languages confuse or destroy any natural reading of the diagrams; hence they cannot be said to be intuitive. The answer, we feel, is to examine seriously the meaning and accuracy of the terms: "natural" and "intuitive" in this context. The paper highlights, and illustrates by means of examples taken from industrial practice, an ongoing research theme of the authors. We take a deeper and more cognitively informed consideration of diagrams which leads us to a more natural formal underpinning that permits: (i) the formal justification of informal intuitive arguments, without placing the onus of formality upon the engineer constructing the argument; and (ii) a principled approach to the identification of intuitive (and counter-intuitive) features of diagrammatic languages.
[modelling languages, Unified modeling language, industrial practice, Humans, diagrams, type theory, visual languages, programming languages, formal specification, Embedded software, rigorous justification, Software design, Software architecture, natural formal underpinning, formal accounts, specification languages, natural representations, Permission, software engineering, diagrammatic languages, software engineering diagrams, Informatics, formal justification, Logic programming, natural reading, diagrammatic specification languages, principled approach, informal intuitive arguments, ongoing research theme, Computer languages, intuitive reasoning, Software engineering, principled design]
From MCC to CMM: technology transfers bright and dim
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Describes lessons learned during the author's five lives in technology transfer. The author's first life came in General Electric's Space Division, where he performed research on software metrics and structured programming, and transferred technology to the pages of technical journals. His second life came at ITT's Programming Technology Center where he was responsible for transferring software measurement practices into common use across ITT's worldwide software operations. Some measurement initiatives survived, but most were short-lived. His third life came in the Human Interface Laboratory and Software Technology Program of the Microelectronics and Computer Technology Corporation (MCC). MCC's member companies were only occasionally able to transfer the advanced technology they challenged MCC to produce. His fourth life came in directing the Software Process Program at the Software Engineering Institute (SEI) where he led the team that produced the Capability Maturity Model (CMM). Although the CMM's transfer was occasionally too rapid to control, the CMM suggested that you should transfer no technology before its time. The author's fifth and current life involves co-founding TeraQuest and helping companies to improve their software development capability. The paper includes 25 lessons in technology transfer and a career transfer model.
[TeraQuest, Humans, software measurement practices, biographies, structured programming, ITT Programming Technology Center, General Electric Space Division, MCC Human Interface Laboratory and Software Technology Program, change management, Microelectronics and Computer Technology Corporation, Software metrics, Space technology, Technology transfer, software process improvement, Second Life, software engineering, Software measurement, Capability maturity model, software design process, Capability Maturity Model, technology adoption, worldwide software operations, Extraterrestrial measurements, technology transfer, employment, Software Process Program, career development, Programming profession, Software Engineering Institute, Coordinate measuring machines, technical journals, software measurement initiatives, software development capability improvement, career transfer model, software metrics]
Fraunhofer: the German model for applied research and technology transfer
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The Fraunhofer Gesellschaft e.V. in Germany is Europe's largest and most successful organization for applied research and technology transfer. Its 48 institutes cover all areas of technology and engineering, ranging from materials and production technology to information and communication technology and solar energy. The Fraunhofer Institute for Experimental Software Engineering (IESE) in Kaiserslautern, Germany, focuses on software engineering methods, software product and process management, and learning organization concepts for software. It applies an experiment- or feedback-based transfer model, which has led to many successful and sustained improvements in the industrial practice of software development. In this paper, the underlying transfer model, key business areas and core competencies of Fraunhofer IESE, as well as examples of industrial transfer projects are illustrated. The presentation concludes with arguments as to why this transfer approach is well-suited for software development and why it is a prerequisite for the professionalization of software development.
[Fraunhofer Institute for Experimental Software Engineering, solar energy, feedback-based transfer model, information technology, core competencies, industrial practice, Programming, key business areas, learning organization concepts, industrial transfer projects, Engineering management, Fraunhofer Gesellschaft, Technology transfer, Production, Communications technology, software engineering, professionalization, applied research, software development, materials technology, Europe, Kaiserslautern, technology transfer, experiment-based transfer model, software process management, research and development management, Solar energy, production technology, Computer industry, software product management, communication technology, Germany, Software engineering, Power engineering and energy, software engineering methods]
Software development engineer in Microsoft. A subjective view of soft skills required
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This paper is a position statement. There are important requirements on software development engineers that go beyond the normal academic qualifications and technical skills, and which quite often receive a lower priority in education and training. Some of the soft skills that are listed in the job descriptions for software development engineers at Microsoft are: change management; self-development; composure and stress management; problem-solving skills; drive for results; communication skills; and interpersonal skills. This paper focuses on the first four of these. The exact description of the soft skills needed changes depending on the level of the position.
[Career development, education, composure, Programming, interpersonal skills, training, self-development, change management, subjective view, Industrial training, Design engineering, Software design, Engineering management, software houses, job position, software engineering, Microsoft, technical skills, soft skill requirements, problem-solving skills, communication skills, computer science education, academic qualifications, Management training, Stress, job descriptions, software development engineers, Computer industry, drive for results, personnel, stress management, Qualifications]
Software needs engineering - a position paper
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
When the general press refers to 'software' in its headlines, then this is often not to relate a success story, but to expand on yet another 'software-risk-turned-problem-story'. For many people, the term 'software' evokes the image of an application package running either on a PC or some similar stand-alone usage. Over 70% of all software, however, are not developed in the traditional software houses as part of the creation of such packages. Much of this software comes in the form of products and services that end-users would not readily associate with software. These can be complex systems with crucial connections made through software, such as telecommunication or banking systems, or the logistics systems of airports; or they can be end-user products with software embedded, ranging from battery management systems in electric shavers, to mobile phones, to engine management and safety systems in cars. E-commerce systems fall into this category too. Yes, there is software that works reliably and as expected, and there are professional approaches to create such products - one can engineer software, in the right environment, with the right people.
[professional approaches, software reliability, Banking, Airports, application packages, Mobile handsets, software risk, Software safety, Application software, Embedded software, professional aspects, Software packages, problem stories, standalone usage, complex systems, embedded software, Battery management systems, Packaging, software houses, software product development, software engineering, end-user products, Logistics]
Is software education narrow-minded? - A position paper
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The content of computer science and software engineering courses needs to be examined so that students are better prepared to cope with the challenges of a rapidly changing software industry.
[Software testing, program testing, course content, challenges, Companies, software education, software engineering courses, computer science courses, Robustness, software engineering, Computer science education, creativity, narrow-mindedness, Quality management, Business, rapidly changing software industry, computer science education, software testing, DP industry, Computer science, educational courses, Software quality, student preparation, Computer industry, Software engineering]
An approach to architectural analysis of product lines
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Addresses the issue of how to perform architectural analysis on an existing software product-line architecture. The contribution of the paper is to identify and demonstrate a repeatable product-line architecture analysis process. The approach defines a "good" product-line architecture in terms of those quality attributes required by the particular product line under development. It then analyzes the architecture against these criteria by both manual and tool-supported methods. The phased approach described in this paper provides a structured analysis of an existing product-line architecture using (1) formal specification of the high-level architecture, (2) manual analysis of scenarios to exercise the architecture's support for required variabilities, and (3) model checking of critical behaviors at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain and evaluate the approach.
[Laboratories, Drives, spaceborne telescopes, repeatable process, scenarios, required variabilities, software product-line architecture, formal specification, software architecture, Software architecture, critical behaviors, astronomy computing, Computer architecture, software packages, product development, Propulsion, Permission, computerized instrumentation, Performance analysis, high-level architecture, manual methods, Application software, interferometry software, Computer science, tool-supported methods, model checking, astronomical telescopes, quality attributes, phased approach, Telescopes, software architectural analysis, structured analysis]
Introducing a software modeling concept in a medium-sized company
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Describes, using the Quality Improvement Paradigm (QIP), how an improvement project aimed at improving the modeling and documentation approach of a medium-sized company (Markant Su/spl uml/dwest Software und Dienstleistungs GmbH) was conducted. We discuss the new modeling approach, which may serve for other companies as a template for deriving their own adapted approach. Further, we illustrate our insights from this project that can help in future technology transfer projects. A major characteristic of this project was that it was embedded in a long-term consulting relationship.
[Industrial relations, system documentation, Programming, software quality, long-term consulting relationship, Technology transfer, software process improvement, Permission, software houses, software modeling, technology transfer projects, Mirrors, medium-sized company, software documentation, software development management, Documentation, technology transfer, software maintenance support, software improvement project, Software quality, Computer industry, software product families, Markant Sudwest Software und Dienstleistungs GmbH, consultancies, software requirements modeling, Software engineering, Context modeling, Quality Improvement Paradigm]
From research to reward: challenges in technology transfer
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Over a five-year period, the Applied Science & Technology Group of IBM's Hursley Laboratory in England turned itself from a fully-funded research organisation into an entirely self-funded technology transfer group. Practical experience and insight was gained into the questions of: what the obstacles are that need to be overcome in successful technology transfer; how to find a match between the technology and the customer; and how best to manage risks and expectations. To be successful, a technology transfer group needs to be correctly positioned within its sponsoring organisation, to use management processes that provide flexibility and control, and to develop a sophisticated engagement model for working with its customers.
[Costs, Laboratories, management processes, Appropriate technology, Applied Science and Technology Group, Technology management, flexibility, sponsoring organisation, Technology transfer, engagement model, expectation management, Testing, risk management, technology-customer matching, DP industry, customer management, Winches, Educational institutions, technology transfer, control, IBM Hursley Laboratory, research and development management, research organisation, self-funded technology transfer group, Product development, Risk management, research exploitation]
Technology transfer macro-process. A practical guide for the effective introduction of technology
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
In our efforts to increase software development productivity, we have worked to introduce numerous software development techniques and technologies into various target organizations. Through these efforts, we have come to understand the difficulties involved in technical transfer. Some of the major hurdles that these organizations face during technical transfers are tight schedules and budgets. We have made efforts to lighten this load by using various customization techniques and have defined an overall process called the "technology transfer macro-process" that we can use to introduce a wide variety of software development techniques and technologies into a target organization. This paper introduces this simple and practical process, along with important methods and concepts, such as the "process plug-in method" and the "process warehouse\
[Computer aided software engineering, process plug-in method, Humans, Programming, software development productivity, process warehouse, Technology transfer, software process improvement, schedules, Permission, object-oriented methods, Face, Productivity, software development techniques, development projects, software development management, consulting, object-oriented technology, technology transfer, mentoring, target organizations, technology introduction, Employee welfare, customization techniques, Processor scheduling, budgets, initial productivity loss, technology transfer macro-process, technical transfer, Software tools, consultancies]
When the project absolutely must get done: marrying the organization chart with the precedence diagram
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Presents a new project planning technique to marry the organization chart with a project's task precedence diagram. This permits one to simulate the project at a micro-scale, project-specific level. One can perform "what-if" scenarios related to organizational structures, the deployment of specific individuals and skills, and the structure of information flow and exception handling in a project. The tool used, ViteProject, was developed in a Stanford University laboratory, where substantial results have been achieved when it was applied to design activities other than software. The author presents his real-world experience with several software projects, where ViteProject has improved project visibility and allowed projects to be rationally optimized in a way that has hitherto been impossible.
[Costs, rational optimization, Laboratories, Project management, exception handling, diagrams, organization chart, individual deployment, Electronic mail, task analysis, information flow, Permission, micro-estimation, software projects, task precedence diagram, Testing, micro-scale project-specific simulation, project management, design activities, Buildings, software development management, ViteProject tool, organizational structures, corporate modeling, planning, project visibility, Life estimation, project planning technique, skills deployment, Organization Charts, Software tools, what-if scenarios]
An evaluation of the paired comparisons method for software sizing
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This paper evaluates the accuracy, precision and robustness of the paired comparisons method for software sizing and concludes that the results produced by it are superior to the so called "expert" approaches.
[Vocabulary, Costs, expert approaches, Project management, software development management, Size measurement, paired comparisons method, software sizing, estimation methods, Permission, Lab-on-a-chip, software measurement, Robustness, Software measurement, software metrics]
Grow fast, grow global: how the Irish software industry evolved to this business model
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The paper discusses the evolution of the Irish software industry and the key components of the new business model. The first wave consisted of small service companies that in this period transformed into product companies and began to export (UK mainly). The second wave was characterised by the emergence of larger companies and a big increase in the number of companies. The third wave is the age of the Internet and the need for new business models.
[Europe, DP industry, Companies, small service companies, large companies, Telecommunications, Research and development, product companies, Communication industry, Irish software industry, Venture capital, Permission, software houses, Computer industry, Hardware, Internet, business model, export, Business]
The making of Orbix and the iPortal suite
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
IONA released the first full implementation of the CORBA standard in August 1992, and our first product, Orbix, has become the most successful object request broker, capturing almost 70-percent of this market. It has spawned many follow-on products from IONA and from partner companies. This development followed nearly ten years of research in the area of distributed object systems within Trinity College Dublin, centered on language support for developers of distributed systems. This paper captures some of the lessons we have learned in the transition from academia to business. We have had to learn many software engineering skills. We have had to find the right mix between engineering, marketing and sales expertise. We have had to learn how to release new products while staying committed to current ones. Most recently, we have had to learn how to become an Internet-company in order to deliver the iPortal Suite. For IONAians, it has been a fascinating journey.
[Orbix, distributed object systems, Object oriented modeling, Internet company, Companies, Educational institutions, Trinity College Dublin, Security, Distributed computing, Programming profession, Research and development, sales, CORBA, Computer languages, marketing, object request broker, Prototypes, iPortal suite, remote procedure calls, software engineering, Internet, IONA, distributed object management]
Improvement of a configuration management system
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The company CAD-UL AG develops software tools for embedded systems. Single tools such as compilers, linkers and debuggers are offered as well as complete development tool chains for the software development process. In contrast to application software for personal computers, embedded systems require very specialized software of highly optimized and exhaustively tested code. Since the previously existing configuration management was not efficient in comparison to the state-of-the-art in software engineering, an improvement was implemented by the introduction of a modern configuration management (CM) system (Cederqvist, 1998). In the paper, CAD-UL intends to show the results and the experiences of the European Systems and Software Initiative Process Improvement Experiment (ESSI-PIE) ICMS with the new configuration management system.
[Software testing, System testing, software development, European Systems and Software Initiative, program debuggers, research initiatives, software development management, Programming, Microcomputers, Process Improvement Experiment, Application software, program compilers, Embedded software, configuration management, CAD-UL AG, Engineering management, Embedded system, embedded systems, software tools, configuration management system, Software tools, program linkers, Software engineering]
Applying and adjusting a software process improvement model in practice: the use of the IDEAL model in a small software enterprise
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Software process improvement is a demanding and complex undertaking. To support the constitution and implementation of software process improvement schemes the Software Engineering Institute (SEI) proposes a framework, the so-called IDEAL model. This model is based on experiences from large organizations. The aim of the research described was to investigate the suitability of the model for small software enterprises. It has therefore been deployed and adjusted for successful use in a small Danish software company. The course of the project and the application of the model are presented and the case is reflected on the background of current knowledge about managing software process improvement as organizational change.
[IDEAL model, Project management, Europe, software development management, Companies, Danish software company, Knowledge management, Application software, software process improvement model, organizational change, Software Engineering Institute, software process improvement, Permission, software houses, small software enterprise, Continuous improvement, Informatics, Constitution, Software engineering]
European experiences with software process improvement
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This paper provides a brief overview of the status of software process improvement (SPI) in Europe, its history, current situation and future direction. Four case studies are presented covering a diverse range of business domains, organisational sizes and approaches to SPI. The author has worked closely with each of the organisations involved in support of their SPI programmes. The case studies show the starting position of each company, the approach taken, results achieved and lessons learnt. A number of themes such as assessment approach used, cultural/people issues, etc. are used to explore the experiences of the various companies. The four case studies are: NewWorld Commerce (formerly Cunav Technologies), Motorola Cork, Silicon and Software Systems and Allied Irish Bank. Assessment models used include SPICE (ISO/IEC TR 15504) and Software Engineering Institute's CMM (one organisation also achieved ISO9001 certification).
[ISO certification, assessment approach, ISO, business, Europe, ISO standards, Companies, future direction, NewWorld Commerce, history, case studies, History, Cultural differences, Allied Irish Bank, Software Engineering Institute, IEC standards, organisations, software process improvement, Motorola Cork, Software systems, SPICE, Silicon, Silicon and Software Systems, Business]
Software process improvement by object technology (ESSI PIE 27785-SPOT)
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The paper describes the on-going experience of Calio Informatica Srl in a process improvement experiment (PIE) project sponsored by the community's ESSI (European Systems and Software Initiative) program. The experiment concerns the improvement of two primary software life cycle (SLC) processes, namely analysis and design, by adopting object oriented technology, and in particular the UML method. Rational Rose is the technology which is supporting the improvement. The PIE is being done on top of a strategic baseline project, being deployed by Calio Informatica in the business domain of enterprise management applications. The main benefits achieved concern: increased professional skills and technical capability of Calio's personnel; achievement of higher customer satisfaction; better resource allocation in software projects; improvement of product quality and robustness, because of better modularization and structuring; set up of a reusable software components library.
[Unified modeling language, Project management, Personnel, software libraries, resource allocation, software life cycle processes, UML method, customer satisfaction, Customer satisfaction, product quality, software process improvement, specification languages, business domain, strategic baseline project, Robustness, software projects, Software reusability, object-oriented programming, ESSI PIE 27785, SPOT, Rational Rose, professional skills, Application software, professional aspects, process improvement experiment, enterprise management applications, reusable software components library, Software quality, software reusability, Software systems, personnel, Resource management, object oriented technology]
Daily build and feature development in large distributed projects
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Daily build is a software development paradigm that originated in the PC industry to get control of the development process, while still allowing the focus on end user requirements and code. The PC industry used daily build to avoid chaos in increasingly larger applications in an environment without a strong development process. Ericsson Radio Systems has chosen to implement daily build to increase the focus on end user requirements and code, but from a different starting point with a traditionally strong development process. The authors discuss their experiences with daily build and feature oriented development in this context. They also relate their experience to the concept of extreme programming, arguing that their ideas can help extend the applicability of extreme programming beyond small co-located projects.
[feature oriented development, Industrial control, Programming, Control systems, large distributed projects, extreme programming, software development paradigm, strong development process, Electrical equipment industry, software engineering, Radio control, user centred design, Context, Base stations, project management, feature development, daily build, computer telephony integration, Application software, small co-located projects, development process, Chaotic communication, Ericsson Radio Systems, Computer industry, PC industry, end user requirements, cellular radio]
Component-based software engineering and the issue of trust
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Software testing, Computer science, Coordinate measuring machines, Software quality, Production, Programming, Application software, Software reusability, Information technology, Software engineering]
Lessons learned from teaching reflective software engineering using the Leap toolkit
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
After using and teaching the Personal Software Process (PSP) (W.S. Humphrey, 1995) for over four years, the author came to appreciate the insights and benefits that it produced. However, there were some general problems with the PSP. These problems led him to begin work on an alternative software process improvement method called reflective software engineering. Like the PSP, reflective software engineering is based upon a simple idea: people learn best from their own experience. Reflective software engineering supports experience based improvement in developers' professional activities by helping the developer structure their experience, record it, and analyze it. Unlike the PSP, reflective software engineering is designed around the presence of extensive automated support. The support is provided by a Java based toolkit called "Leap" <http://csdl.ics.hawaii.edu/Research/LEAP/LEAP.html>. The kinds of structured insights and experiences users can record with Leap include: the size of the work product; the time it takes to develop it; the defects that the user or others find in it; the patterns that they discover during the development of it; checklists that they use during or design as a result of the project; estimates for time or size that they generate during the project; and the goals, questions, and measures that the user uses to motivate the data recording.
[checklists, Laboratories, Software performance, reflective software engineering, Java based toolkit, alternative software process improvement method, teaching, Leap toolkit, experience based improvement, software process improvement, Computer science education, Software measurement, work product, Java, automatic programming, computer science education, Personal Software Process, Data analysis, project management, Collaborative software, PSP, professional activities, data recording, software reusability, Computer industry, automated support, Software tools, Software engineering]
Can quality graduate software engineering courses really be delivered asynchronously on-line?
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The article briefly presents a case study in online asynchronous course delivery. It sketches the design of a graduate computer science course entitled "Software Design and Quality\
[teaching, learning by doing, educational technology, Videoconference, quality online education, Software Design and Quality, Software design, critical thinking, graduate computer science course, software engineering, information resources, computer science education, Educational institutions, streaming media presentations, online asynchronous course delivery, case study, distance learning, Computer science, asynchronous online courses, Computer aided instruction, Collaboration, educational courses, Software quality, quality graduate software engineering courses, Streaming media, Web sites, Software engineering, team collaboration]
Multibook's test environment
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Well engineered Web based courseware and exercises provide flexibility and added value to the students, which goes beyond the traditional text book or CD-ROM based courses. The Multibook project explores the boundaries of customized learning materials by composing learning trails dynamically as learners have set their profile to access a course. The authors first give an overview of the core project ideas and illustrate them along their Software Engineering course. Then they present a novel extension to the project's exercise environment with a graph editing component that particularly fits the needs of structure-related assignments.
[user profile, CD-ROMs, students, teaching, Web based courseware, learning trails, dynamic composition, Permission, Multibook test environment, Multibook project, online courseware, software engineering, Books, courseware, core project ideas, Testing, self assessment, Courseware, information resources, Java, computer science education, structure-related assignments, Software Engineering course, graph editing component, Application software, distance learning, exercise environment, Software libraries, educational courses, Computer applications, customized learning materials, Software engineering]
E-Slate: a software architectural style for end-user programming
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The paper describes E-Slate (http://E-Slate.cti.gr), an exploratory learning environment. It builds on a component based approach, to enable end users to create educational software constructions themselves by wiring components, using the Plug and Synapse metaphors.
[teaching, authoring systems, Guidelines, Wiring, software architecture, component wiring, Software architecture, Computer architecture, Libraries, component based approach, Assembly, courseware, software architectural style, educational software constructions, Plugs, computer science education, object-oriented programming, exploratory learning environment, Educational technology, personal computing, E-Slate, Programming profession, end user programming, educational software, component software architectures, Software engineering]
An interactive multimedia software house simulation for postgraduate software engineers
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The Open University's M880 Software Engineering is a postgraduate distance education course aimed at software professionals. The case study element of the course (approximately 100 hours of study) is presented through an innovative interactive multimedia simulation of a software house Open Software Solutions (OSS). The student 'joins' OSS as an employee and performs various tasks as a member of the company's project teams. The course is now in its sixth presentation and has been studied by over 1500 students. The authors present the background to the development, and a description of the environment and student tasks.
[TV, Distance learning, teaching, digital simulation, multimedia computing, employee, M880 Software Engineering, Radiofrequency interference, Videos, Design engineering, Open University, Permission, software houses, interactive systems, software engineering, courseware, Audio tapes, software professionals, computer science education, interactive multimedia software house simulation, postgraduate distance education course, OSS, distance learning, Human computer interaction, postgraduate software engineers, educational courses, software house Open Software Solutions, Animation, student tasks, project teams, Software engineering, case study element]
LIGHTVIEWS-visual interactive Internet environment for learning OO software testing
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The Internet has been recognised not only as a tool for communication in the 21st century but also as an environment for enabling changes in the paradigm of teaching and learning. The paper describes our development effort, sponsored by the Committee of University Teaching Development (CUTSD98) Grant, in designing educational material on object oriented (OO) testing in an Internet environment. The aim of the work is to enhance the state of the art in learning OO testing by visualizing the testing process and interactive courseware in virtual communities. We have endeavoured to create an effective Internet based courseware known as LIGHTVIEWS which contains OO testing case studies described by visual images, animation, and interactive lessons, to assist active participation by learners to result in better understanding and knowledge retention. Our approach employs appropriate UML diagrams, makes the diagrams test ready by including details of constraints as part of state/event transitions, and provides interactive lessons for learning OO software testing. We have used four case studies to explore the various test selection techniques. We have included black-box testing at unit level in case study 1, and at the system level in case study 3. Case study 2 was used to illustrate event based testing by visually representing the dynamics of Java applets at work, and using interactivity to learn how to test Java applets, threads, and applet communication. Case study 4 explores the various aspects of distributed component testing. More details are available on the project Web page http://www.sd.monash.edu.au/sitar/se-educ-proj.
[Software testing, OO software testing education, Materials testing, Visualization, System testing, testing process, knowledge retention, program testing, educational material, Unified modeling language, visual images, applet communication, UML diagrams, event based testing, teaching, visual interactive Internet environment, distributed component testing, program visualization, interactive courseware, virtual communities, system level, object oriented testing, Education, interactive systems, Java applets, courseware, OO software testing, Courseware, Java, computer science education, object-oriented programming, LIGHTVIEWS, active participation, black-box testing, interactive lessons, development effort, distance learning, animation, OO testing case studies, state/event transitions, Internet based courseware, Animation, Internet]
The ICSE2000 doctoral workshop
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Training, Conferences, Medical services, Software, Proposals, Information technology, Software engineering]
A logical framework for design composition
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The design of a large component based software system typically involves the composition of different components. The lack of rigorous reasoning about the correctness of composition is an important barrier towards the promise of "plug and play". The author describes a rigorous logic framework to reason about component compositions. We focus our analysis on design components such as design patterns, which have been used by a large number of applications. We also propose methods to verify structural and behavioral composition correctness.
[Assembly systems, object-oriented programming, correctness, behavioral composition correctness, program verification, rigorous reasoning, Programming, type theory, design composition, Application software, plug and play, Computer science, design patterns, Permission, Software systems, rigorous logic framework, component based software system, Logic, Software reusability, Pattern analysis, logical framework, component compositions, Software engineering, design components]
Algorithmic cost estimation for software evolution
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The study addresses the problem of cost estimation in the context of software evolution by building a set of quantitative models and assessing their predictive power. The models aim at capturing the relationship between effort, productivity and a suite of metrics of software evolution extracted from empirical data sets.
[Productivity, quantitative models, Software maintenance, Costs, Software algorithms, Predictive models, Data mining, software maintenance, software evolution, predictive power, Processor scheduling, productivity, empirical data sets, Aging, algorithmic cost estimation, software cost estimation, metrics, Monitoring, Context modeling, software metrics]
Estimating software fault-proneness for tuning testing activities
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The article investigates whether a correlation exists between the fault-proneness of software and the measurable attributes of the code (i.e. the static metrics) and of the testing (i.e. the dynamic metrics). The article also studies how to use such data for tuning the testing process. The goal is not to find a general solution to the problem (a solution may not even exist), but to investigate the scope of specific solutions, i.e., to what extent homogeneity of the development process, organization, environment and application domain allows data computed on past projects to be projected onto new projects. A suitable variety of case studies is selected to investigate a methodology applicable to classes of homogeneous products, rather than investigating if a specific solution exists for few cases.
[Software testing, testing process, application domain, Costs, measurable attributes, program testing, software quality, case studies, Fault diagnosis, Fluid flow measurement, Software metrics, dynamic metrics, Software measurement, Monitoring, Stability, past projects, software fault tolerance, development process, software fault-proneness estimation, homogeneous products, static metrics, Software quality, Logistics, software metrics, testing activity tuning]
Formal verification applied to Java concurrent software
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Applying existing finite-state verification tools to software systems is not yet easy for a variety of reasons. The research activity presented aims to integrate formal verification with programming languages currently used in software development. In particular, it focuses on elaborating a formal method for the specification and validation of temporal logic properties concerning the behavior of Java concurrent programs.
[program verification, software systems, Java concurrent software, temporal logic, finite state machines, programming languages, formal specification, finite-state verification tools, parallel programming, Concurrent computing, research activity, formal verification, Java concurrent programs, formal method, Java, Logic programming, temporal logic properties, Reasoning about programs, specification, Computer languages, Memory management, System recovery, Software systems, Formal verification, Software engineering]
Supporting dynamic distributed work processes with a component and event based approach
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
There is a long standing tension between efforts to provide coherent models of work processes and the support for the way in which work actually takes place. The well laid groundwork of extensive research on formalisms has led to a more recent focus on supporting dynamic ad-hoc work activities that are nonetheless driven by a notion of work process (P.J. Kammer et al., 2000). The widely distributed character of the Internet expands the need for dynamic and adaptive capabilities. Process enactment may extend not only across widely distributed organizational groups, but across organizational boundaries, requiring the interaction of independently executing processes in a manageable way and support for fluidly created relationships between process participants, be they individuals, groups, or organizations. The paper proposes an approach based on lightweight ubiquitous technologies and a dynamic component based view of process, to support distributed and adaptive enactment of work activities for software development. We use the term "process architecture" to describe the run-time infrastructure relating the various process elements, for example people, artifacts, automated processes, and resources. These elements, including traditional process engines, interact though layered event based mechanisms that allow dynamic composition of process architectures on-the-fly.
[coherent models, Adaptive systems, distributed adaptive enactment, dynamic component based view, work processes, Programming, distributed processing, automated processes, dynamic composition, process participants, event based approach, software architecture, Runtime, process architectures, dynamic ad-hoc work activities, software process improvement, widely distributed organizational groups, Permission, Search engines, organizational boundaries, run-time infrastructure, layered event based mechanisms, object-oriented programming, lightweight ubiquitous technologies, software development, Fluid dynamics, software development management, traditional process engines, process enactment, dynamic distributed work process support, work activities, process architecture, Computer science, Programmable control, process elements, independently executing processes, Internet, Context modeling]
Platform-independent and tool-neutral test descriptions for automated software testing
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Current automatic test execution techniques are sensitive to changes in program implementation. Moreover, different test descriptions are required by different testing tools. As a result, it is difficult to maintain or port test descriptions. To address this problem, we developed TestTalk, a comprehensive testing language. TestTalk test descriptions are platform-independent and tool-neutral. The same software test in TestTalk can be automatically executed by different testing tools on different platforms. The goal of TestTalk is to make software test descriptions, which represent a significant portion of a software project, last as long as the software project.
[Software testing, automatic test execution techniques, automatic programming, TestTalk, Automation, program testing, Computational modeling, program implementation, automated software testing, Application software, software test descriptions, software project, Computer science, Automatic testing, testing tools, comprehensive testing language, Writing, User interfaces, Computer industry, Software tools, tool-neutral test descriptions, hypermedia markup languages]
Contribution to simplifying the mobile agent programming
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The paper introduces an experimental framework for mobile agents. It utilizes expressiveness and formal foundation of concurrent constraint programming to solve the problem of system support for dynamic re-binding of non-transferable resources and inter-agent collaboration based on logic variables. Proposed solutions help to make the agent based programming easier and more straightforward and at the same time offer a basis for more sophisticated multi agent systems.
[Mobile communication, non-transferable resources, dynamic re-binding, multi agent systems, mobile computing, Mobile agents, expressiveness, concurrent constraint programming, Computer networks, mobile agent programming, Dynamic programming, constraint handling, Informatics, distributed programming, Multiagent systems, object-oriented programming, Logic programming, agent based programming, Logic design, inter-agent collaboration, system support, software agents, logic variables, formal foundation, Collaboration, Collaborative work]
Spontaneous software: a Web-based, object computing paradigm
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The author introduces the concept of spontaneous software, a Web based object computing paradigm for supporting on-demand, dynamic distribution and integration of distributed reusable software artifacts on user environments during execution time. The model is supported by a framework, Software Operating System (SOS), which provides execution and distribution models combining the power of both hypertext based Web technologies and mobile code paradigms. Like the Web, which moves distributed resources to clients, SOS allows software systems to locate, retrieve, install and execute remotely available software artifacts on user desktops. In such a scenario, cataloging, publishing, retrieving and executing distributed components is easily and efficiently achieved by adherent platforms, which allow users to acquire software licenses and have them automatically installed and running. Hardware devices, if designed over a compliant platform, could have drivers automatically installed and running after connection. Therefore, spontaneous software is a true way for manufacturers to provide genuine plug-and-play software and hardware.
[hardware devices, user desktops, Web based object computing paradigm, software systems, Licenses, plug-and-play hardware, spontaneous software, on-demand dynamic distribution, Distributed computing, remotely available software artifacts, hypertext based Web technologies, distributed components, mobile computing, Operating systems, Space technology, user environments, distribution models, Permission, cataloging, Software Operating System, software licenses, plug-and-play software, Software reusability, publishing, mobile code paradigm, information resources, object-oriented programming, Power system modeling, distributed reusable software artifacts, distributed resources, software reusability, Software systems, operating systems (computers), compliant platform, Internet, Software engineering]
Automated refactoring to introduce design patterns
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Software systems have to be flexible in order to cope with evolving requirements. However, since it is impossible to predict with certainty what future requirements will emerge, it is also impossible to know exactly what flexibility to build into a system. Design patterns are often used to provide this flexibility, so this question frequently reduces to whether or not to apply a given design pattern. We address this problem by developing a methodology for the construction of automated transformations that introduce design patterns. This enables a programmer to safely postpone the application of a design pattern until the flexibility it provides becomes necessary. Our approach deals with the issues of reuse of existing transformations, preservation of program behaviour and the application of the transformations to existing program code.
[Costs, evolving requirements, software reuse, Educational institutions, Production facilities, Application software, Programming profession, Computer science, program behaviour preservation, Design engineering, design patterns, Software quality, Permission, software reusability, Software systems, automated program refactoring, software tools, automated transformations]
High-integrity code generation for state-based formalisms
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
We are attempting to create a translator for a formal state-based specification language (RSML/sup -e/) that is suitable for use in safety-critical systems. For such a translator, there are two main concerns: the generated code must be shown to be semantically equivalent to the specification, and it must be fast enough to be used in the intended target environment. We address the first concern by providing a formal proof of the translation, and by keeping the implementation of the tool as simple as possible. The second concern is addressed through a variety of methods: decomposing a specification into parallel subtasks; providing provably-correct optimizations; and making worst case performance guarantees on the generated code.
[Optimization methods, RSML, safety-critical software, program translator, Control systems, Software safety, program compilers, formal specification, formal state-based specification language, optimization, performance guarantees, Production, specification languages, Permission, software performance evaluation, formal proof, Specification languages, Formal specifications, Computer science, program interpreters, safety-critical systems, Software systems, high-integrity code generation, parallel subtasks, Error correction codes]
Alcoa: the Alloy constraint analyzer
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Alcoa is a tool for analyzing object models. It has a range of uses. At one end, it can act as a support tool for object model diagrams, checking for consistency of multiplicities and generating sample snapshots. At the other end, it embodies a lightweight formal method in which subtle properties of behaviour can be investigated. Alcoa's input language, Alloy, is a new notation based on Z. Its development was motivated by the need for a notation that is more closely tailored to object models (in the style of UML), and more amenable to automatic analysis. Like Z, Alloy supports the description of systems whose state involves complex relational structure. State and behavioural properties are described declaratively, by conjoining constraints. This makes it possible to develop and analyze a model incrementally, with Alcoa investigating the consequences of whatever constraints are given. Alcoa works by translating constraints to boolean formulas, and then applying state-of-the-art SAT solvers. It can analyze billions of states in seconds.
[SAT solvers, relational algebra, Unified modeling language, Laboratories, diagrams, boolean formula, Alcoa, formal specification, program compilers, object model diagrams, Boolean functions, File systems, Alloy constraint analyzer, constraint satisfaction, notation, specification languages, Permission, relational logic, software tools, Logic, constraint handling, formal method, object-oriented programming, object models, Data structures, Topology, Risk analysis, Formal specifications, Z language, complex relational structure, Computer science, software analysis, UML, program compiler]
Hyper/J/sup TM/: multi-dimensional separation of concerns for Java/sup TM/
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Hyper/J/sup TM/ supports flexible, multi-dimensional separation of concerns for Java/sup TM/ software. This demonstration shows how to use Hyper/J in some important development and evolution scenarios, emphasizing the software engineering benefits it provides.
[Java, object-oriented programming, multi-dimensional separation of concerns, Scattering, Software safety, Organizing, Hyper/J, Permission, Software standards, software engineering, software tools, object oriented programming, Standards development, Object oriented programming, Software tools]
A software engineering approach and tool set for developing Internet applications
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
If a business built a plant to produce products without first designing a process to manufacture them, the risk would be lack of capacity without significant plant redesign. Similarly, lacking a software engineering approach and tools for designing e-business connections before creating them, can risk: designing the business partnership incorrectly; not implementing the connection quickly enough; or having operations that cannot adapt to changes in business direction. This paper presents a software engineering tool for developing process-oriented Internet applications that implement e-business connections. It gives an approach for using this tool in conjunction with standard commercial IDEFO tools to create adaptable connections. It is organized to match a formal demonstration that shows the step-by-step usage of these tools, and cites software engineering principles that, when applied, ensure adaptability.
[Process design, Computer aided manufacturing, Supply chains, hypermedia, Internet applications, adaptability, Application software, e-business, Design engineering, tool set, software tool, IDEFO tools, Computer architecture, Permission, application generators, Internet, software engineering approach, programming environments, Software engineering, Business, electronic commerce]
The FUJABA environment
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Fujaba is an acronym for From UML to Java And Back Again. The Fujaba environment aims to provide round-trip engineering support for UML and Java. The main distinction to other UML tools is its tight integration of UML class and UML behavior diagrams to a visual programming language. Our use of UML allows to model operations on graph-like object structures on a high-level of abstraction and leverages the user from programming with plain references at code level. Code generation from class diagrams is widely known and supported by most modern CASE tools. However, code generation from class diagrams creates class frames and method declarations without bodies, only. The actual work remains unsupported. The paper considers how Fujaba generates code from collaboration diagrams. It presents an example session.
[Unified modeling language, diagrams, visual languages, Yarn, program compilers, From UML to Java And Back Again, specification languages, Permission, software tools, graph-like object structures, Java, UML behavior diagrams, Navigation, UML class diagrams, Fujaba environment, Computer science, Computer languages, CASE tools, collaboration diagrams, code generation, Collaboration, object-oriented languages, Collaborative work, Nickel, visual programming language]
Managing software artifacts on the Web with Labyrinth
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Software developers are increasingly exploiting the Web as a document management system. However, the Web has some limitations, since it is not aware of the structure and semantics associated to pieces of information (e.g., the fact that a document is a requirement specification) and of the semantics of relationships between pieces of information (e.g., the fact that a requirement specification document may be associated to some design specification document). In the Labyrinth project we enhance the capabilities of the Web as a document management system by means of a semantic model (called schema, in analogy with database schemas), which is associated to Web documents. This model is itself a Web document and can be accessed and navigated through a simple Web browser.
[information resources, document handling, schema, Navigation, document management system, Project management, Programming, Labyrinth, software management, Knowledge management, semantics, formal specification, semantic model, software libraries, requirement specification, Software development management, Technology management, Databases, Web browser, Permission, Software systems, Web software artifact management, Internet]
Galileo: a tool built from mass-market applications
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
We present Galileo, an innovative engineering modeling and analysis tool built using an approach we call package-oriented programming (POP). Galileo represents an ongoing evaluation of the POP approach, where multiple large, architecturally coherent components are tightly integrated in an overall software system. Galileo utilizes Microsoft Word, Internet Explorer and Visio to provide a low cost, richly functional fault tree modeling superstructure. Based on the success of previous prototypes of the tool, we are now building a version for industrial use under an agreement with NASA Langley Research Center.
[mass-market applications, NASA Langley Research Center, package-oriented programming, Visio, fault tree modeling, software architecture, Prototypes, software packages, Internet Explorer, Cost function, engineering modeling tool, software components, Fault trees, document handling, Software prototyping, Buildings, NASA, engineering computing, Galileo tool, engineering analysis tool, fault trees, Application software, Microsoft Word, COTS, Packaging, software reusability, Software systems, Internet]
Little-JIL/Juliette: a process definition language and interpreter
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Little-JIL, a language for programming coordination in processes is an executable, high-level language with a formal (yet graphical) syntax and rigorously defined operational semantics. The central abstraction in Little-JIL is the step, which is the focal point for coordination, providing a scoping mechanism for control, data and exception flow and for agent and resource assignment. Steps are organized into a static hierarchy, but can have a highly dynamic execution structure including the possibility of recursion and concurrency. Little-JIL is based on two main hypotheses. The first is that coordination structure is separable from other process language issues. Little-JIL provides rich control structures while relying on separate systems for resource, artifact and agenda management. The second hypothesis is that processes are executed by agents that know how to perform their tasks but benefit from coordination support. Accordingly, each Little-JIL step has an execution agent (human or automated) that is responsible for performing the work of the step. This approach has proven effective in supporting the clear and concise expression of agent coordination for a wide variety of software, workflow and other processes.
[execution agent, Laboratories, process definition language, operational semantics, Humans, Communication system control, high level languages, Control systems, high-level language, control structures, formal specification, Centralized control, formal syntax, resource assignment, specification languages, Permission, Juliette, Little-JIL, recursion, process coordination programming, program control structures, agenda management, static hierarchy, High level languages, concurrency, Computer science, program interpreters, Computer languages, agent coordination, program interpreter, Resource management, graphical syntax]
Analyzing software architectures with Argus-I
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This formal research demonstration presents an approach to develop and assess architecture and component-based systems based on specifying software architecture augmented by statecharts representing component behavioral specifications (Dias et al., 2000). The approach is applied for the C2 style (Medvidovic et al., 1999) and associated ADL and is supported within a quality-focused environment, called Argus-I, which assists specification-based analysis and testing at both the component and architecture levels.
[Software testing, statecharts, program testing, component-based systems, software testing, Citation analysis, Unified modeling language, ADL, Topology, software quality, Risk analysis, C2 style, formal specification, Argus-I, Computer science, Analytical models, software architecture, Software architecture, component behavioral specifications, Computer architecture, Permission, software reusability, programming environments]
Bandera: a source-level interface for model checking Java programs
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Despite emerging tool support for assertion-checking and testing of object-oriented programs, providing convincing evidence of program correctness remains a difficult challenge. This is especially true for multi-threaded programs. Techniques for reasoning about finite-state systems have been developing rapidly over the past decade and have the potential to form the basis of powerful software validation technologies. We have developed the Bandera toolset to harness the power of existing model checking tools to apply them to reason about correctness requirements of Java programs. Bandera provides tool support for defining and managing collections of requirements for a program, for extracting compact finite-state models of the program to enable tractable analysis, and for displaying analysis results to the user through a debugger-like interface. This paper describes and illustrates the use of Bandera's source-level user interface for model checking Java programs.
[Bandera, assertion-checking, debugger-like interface, program verification, program testing, assertion testing, reasoning, finite-state systems, finite state machines, Engines, Sequential analysis, source-level interface, software validation, Feedback, Permission, object-oriented programs, multi-threaded programs, software engineering, software tools, Java programs, Testing, program correctness, Java, object-oriented programming, multi-threading, Object oriented modeling, Power system modeling, user interface, tool support, model checking, User interfaces, tractable analysis, Error correction]
Developing mobile computing applications with LIME
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Mobile computing defines a very dynamic and challenging scenario for which software engineering practices are still largely in their initial developments. LIME is a middleware designed to enable the rapid development of dependable applications in the mobile environment. The model underlying LIME allows for the coordination of physical and logical mobile units by exploiting a reactive, transiently shared tuple space whose contents changes according to connectivity. We report on initial experiences in developing applications for physical mobility using LIME.
[dependable applications, physical mobility, reactive transiently shared tuple space, Mobile communication, Linda in a Mobile Environment, Application software, mobile computing, Space technology, Physics computing, mobile computing applications, Mobile agents, LIME, Computer applications, Permission, software engineering, programming environments, Mobile computing, Assembly, distributed programming, mobile units, Software engineering, middleware]
Component composition
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This paper depicts a novel approach to document components in a uniform and abstract way. Every use of component is expressed with a specific type of message sequence chart (MSG), using a limited set of standard primitives with predefined semantics. These primitives are mapped on the actual API of the component(s). This documentation is used to find compatible components and to detect conflicts when composing components. Because of the standard set of primitives, components from different sources can be matched and developers do not have to rely on the concrete API. The behavioural flavour of MSCs is suited to document, as a set of usage scenarios, how a component expects to interact with other components when configured in an application. This complements existing documentation techniques.
[Vocabulary, application program interfaces, system documentation, predefined semantics, formal specification, software architecture, message sequence chart, Standards development, Assembly, object-oriented programming, Natural languages, Documentation, patterns, Application software, Middleware, microarchitectures, standard primitives, component composition, Software libraries, document components, compatible components, documentation techniques, Concrete, API, usage scenarios, Software engineering]
Third Eye - specification-based analysis of software execution traces
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
In the Third Eye project we have defined a methodology for tracing execution of software by reporting events meaningful in the application domain or essential from the implementation point of view. The implemented portable prototype of the Third Eye framework includes reusable software components for event definition and reporting and stand-alone tools for storage and query of event traces, constraint specification and trace analysis. Third Eye can be used for debugging, monitoring, specification validation, and performance measurements. These scenarios use typed events, a concept simple and yet expressive enough to be shared by product designers and developers. The Third Eye has an open architecture allowing easy replacement of third-party tools, including databases, analysis and validation tools. Third Eye is a practical framework for specification-based analysis and adaptive execution tracing of software systems.
[Measurement, software execution traces, application domain, event definition, event traces, specification-based analysis, trace analysis, stand-alone tools, formal specification, Third Eye, Databases, specification validation, execution tracing, Computer architecture, debugging, software tools, Software reusability, Monitoring, Software prototyping, reusable software components, Debugging, meaningful events, Product design, open architecture, Application software, constraint specification, monitoring, performance measurements, software reusability, system monitoring, Software tools, software execution tracing]
Empirical investigation of a novel approach to check the integrity of software engineering measuring processes
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
We present an empirical investigation of the applicability of Benford's Law (1958) and Digital Statistics (Nigrine, 1995) in the context of software engineering metrics analysis and process validation. We have conducted an investigation to determine under what circumstances various software metrics follow Benford's Law, and whether any special characteristics, or irregularities, in the data can be uncovered if the data are found not to follow the law. Lists were formed from three software metrics extracted from 100 public domain industrial Java projects. These metrics were Lines of Code (LOC), Fan-Out (FO) and McCabe Cyclomatic Complexity (MCC). The results indicate that the first digits of numbers in lists of LOC metrics extracted from the projects closely followed the probabilities predicted by Benford's Law. The FO and MCC metrics did not follow the standard Benford's Law as closely as the LOC metrics.
[Java, Statistical analysis, public domain industrial Java projects, empirical investigation, Drives, Fan-Out, Data mining, McCabe Cyclomatic Complexity, software engineering measuring processes, process validation, Software metrics, Benford Law, Electric variables measurement, Lab-on-a-chip, Software measurement, Lines of Code, Stock markets, Digital Statistics, software engineering metrics analysis, Software engineering, software metrics, number theory]
The implication of different learning styles on the modeling of object-oriented systems
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This paper reports on work in progress on the implication of thinking and learning styles on the modelling of object oriented systems. In particular, analyses of learning modalities are presented and then considered in light of using the Unified Modeling Language (UML) a tool for system modelling.
[computer science education, Computer aided software engineering, object-oriented programming, Unified Modeling Language, Object oriented modeling, Educational products, Unified modeling language, object-oriented systems, learning modalities, Computer science, CASE tools, learning styles, UML, specification languages, Permission, software engineering education, computer aided software engineering, Systems engineering education, Software tools, thinking styles, system modelling, Testing, Software engineering]
A culture-centered multilevel software process cycle model
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
A culture-centered multilevel software process cycle model (MPCM) is presented. This model interrelates the socio-cultural, scientific/technological and paradigmatological environments. The proposed model is composed of three environments made up of the ecological universe, engineering, management, development and evaluation levels which represent the process cycle, and the lines of "what\
[Knowledge engineering, culture-centered multilevel software process cycle model, Humans, human factors, Strategic planning, ecological universe, Environmental management, cultural comprehensive procedure, evaluation, Engineering management, software process improvement, software engineering, socio-economic effects, cultural procedure, Symbiosis, development, Biological system modeling, Decision making, engineering, Cultural differences, organizational culture, process engineering, socio-cultural environment, management, software process models]
Using application states in software testing
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Application states are abstract states of application software characterized only by properties interesting to test designers in a particular situation. Application states can be used in software testing to break down test cases into manageable pieces, generate new test cases, and control the number of test cases in a test suite. They can also help test designers to conceptualize test design.
[Software testing, Performance evaluation, application software, Design automation, program testing, coverage analysis, software testing, test automation, abstract states, test cases, Application software, test suite, test designers, Information analysis, Computer science, Automatic testing, Permission, software engineering, application states, test design]
Effort estimation from change records of evolving software
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Algorithmic cost estimation in the context of software evolution is being addressed as part of the FEAST/2 project with encouraging results from an industrial case study. The paper considers the approach and case study.
[Software maintenance, Costs, Software algorithms, Predictive models, Educational institutions, Data mining, software maintenance, Power system modeling, case study, software evolution, algorithmic cost estimation, Lab-on-a-chip, software cost estimation, Software measurement, effort estimation, FEAST 2 project, Software engineering, software metrics, change records]
Modeling deployment and configuration of CORBA systems with UML
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
An area of CORBA-based distributed systems which has been difficult to design and document is that of deployment of server components and configuration information. This poster shows by way of an example taken from a health-care system how UML deployment diagrams can be used to model configuration in a system based on Iona Technologies' Orbix. Using the UML models we compare several different centralized and distributed approaches. We conclude by examining how extensions made to UML in recent revisions enhance the utility of our approach.
[centralized approach, Scalability, Unified modeling language, Standardization, deployment diagrams, Electromagnetic compatibility, diagrams, server components, Fault tolerance, Ovens, specification languages, Iona Technologies, Permission, distributed systems, Large-scale systems, distributed object management, health care system, Orbix, system configuration, CORBA, Software packages, UML, Packaging, object-oriented languages, medical computing]
As strong as possible mobility
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
In Java there is no close coupling between a thread and the objects from which they were created. The use of a container abstraction allows us to group threads and their respective objects into a single structure. A container that holds threads whose variables are all housed within the container is a perfect candidate for strong migration. To achieve this we propose a combination of three techniques to allow the containers to migrate in a manner that approaches strong mobility yet does not resort to retaining bindings to resources across distant and unreliable networks.
[Java, object-oriented programming, strong migration, Containers, Data structures, Educational institutions, thread, Printers, container abstraction, Yarn, Computer science, Permission, unreliable networks, Internet, object oriented programming, Resource management, distributed programming]
Hybrid Domain Representation Archive (HyDRA) for requirements model synthesis across viewpoints
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
IEEE's recommended practices for Software Requirements Specifications (Std 830-1993) require traceability for requirements artifacts. However, common practice, commercial tools, and even popular modeling methodologies hinder the capture and maintenance of traceability links. The creation of a requirements model (explicitly representing functional, data and timing requirements) typically involves accommodating viewpoints from multiple system stakeholders (e.g. multiple end-users and system maintainers). While most acknowledge the need for multiple requirement sources, domain-modeling methodologies and CASE tools poorly address how to construct a single model given input from a variety of sources and how to maintain traceability through the synthesis process. Viewpoint identification has been proposed by other researchers to ensure the capture of requirements imposed by all user perspectives. The paper discusses the Hybrid Domain Representation Archive (HyDRA) tool developed to support viewpoint model resolution.
[Computer aided software engineering, Costs, Laboratories, requirements traceability, Ontologies, Programming, Hybrid Domain Representation Archive, Software Requirements Specifications, HyDRA, formal specification, requirements model synthesis, CASE tools, viewpoint identification, Bibliographies, Permission, IEEE standard, Systems engineering and theory, Timing, IEEE standards, software tools, viewpoint model resolution, Hybrid intelligent systems, software standards]
The use of task analysis methods in support of the development of interactive systems
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Capturing and analysis of user requirements and tasks are concepts that have frequently been suggested to address central problems within system development in recent years. In this research the use of a variety of task analysis (TA) methods has been used to assess the adequacy of a proposed design for a World Wide Web (WWW) system within an interactive multimedia (IMM) context, domain and environment which will help research students conduct their doctoral program as carried out at Salford University, Manchester, UK. The findings have shown that TA methods have a number of weaknesses in the contributions that they make and therefore questions of how the methods can be improved to increase their capability were considered.
[Process design, information resources, System testing, Multimedia systems, Design methodology, interactive multimedia, Programming, World Wide Web, user interfaces, interactive systems development, task analysis, multimedia computing, Salford University, System analysis and design, Computer science, Interactive systems, Permission, interactive systems, user requirements analysis, human computer interaction]
DeBOT-an approach for constructing high performance, scalable distributed object systems
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This poster describes a practical and novel approach to the construction of distributed object systems that exhibit high performance and good scalability features. Such distributed applications are important especially in the present day e-commerce world, where performance and scalability are critical issues in attracting and keeping Web clients. The session illustrates the DeBOT approach through a case study of building an online stock ordering system using a CORBA compliant Object Transaction Monitor WebLogic Enterprise from BEA Systems.
[System testing, Scalability, Web clients, Delay, scalability, Software architecture, Permission, Object Transaction Monitor, software engineering, e-commerce, high performance, Monitoring, distributed object management, Business, information resources, Buildings, DeBOT approach, stock control data processing, online stock ordering system, case study, CORBA, scalable distributed object systems, Internet, Resource management, Australia, WebLogic Enterprise]
Exploring O-O framework usage
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Object-oriented application frameworks are becoming an increasingly popular part of software development but there has been little work on studying how they are actually used. An object-oriented application framework comprises a design and an object-oriented implementation of that design meant to apply to a broad range of applications, or subsystems within a single domain, such as graphical user interfaces. FrameScan is an ongoing study with the goal of understanding how developers can effectively and efficiently understand and deploy framework technology to construct and evolve their applications. In order to study how frameworks are used, 34 students of a senior year software engineering course were divided up into six teams of five or six students each. Each team had three months to design and implement a small client-server application of their choosing with the requirement that a framework for client-server computing called CSF (Client-Server Framework) be used as part of the project. The conclusions of the study are presented.
[computer science education, client-server systems, object-oriented programming, software development, Object oriented modeling, graphical user interfaces, software design, FrameScan, Documentation, software engineering course, client-server computing, Programming, students, Client-Server Framework, Application software, object oriented application framework usage, CSF, Concrete, software engineering, Software engineering, Graphical user interfaces]
Tracking, predicting and assessing software reuse costs: an automated tool
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
In a companion paper (Mili et al., 2000), we propose an ROI-based cost model for software reuse, and discuss in what sense and to what extent this model encompasses many relevant aspects of existing software reuse cost models. In this paper, we present an automated tool that supports the model, and we briefly illustrate its operation. The tool which supports the proposed model has two main functions, which we review; we call this tool RCA, for Reuse Cost Analyzer.
[ROI-based cost model, software reuse cost model, Data engineering, Cost benefit analysis, RCA tool, automated software tool, Reuse Cost Analyzer, Databases, Investments, Permission, software reusability, Cost function, Frequency, software cost estimation, software tools, Software tools, Software engineering]
Holmes: a system to support software product lines
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The paper discusses Holmes tool designed to support the Sherlock (Predonzani et al., 2000) software product line methodology. Holmes attempts to provide comprehensive support for product line development, from market and product strategy analysis to modeling, designing, and developing the resulting system. The paper shows the overall architecture of Holmes. It centres on the use of JavaSpaces as a distributed blackboard of objects.
[Java, Vocabulary, object-oriented programming, JavaSpaces, Data structures, product strategy analysis, Software development management, Information analysis, distributed blackboard, software architecture, Sherlock, Databases, software product line methodology, Computer architecture, Holmes tool, Permission, Software systems, software tools, Software tools]
Supporting dynamic composition of components
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The Internet creates new opportunities for component distribution. Infrastructure for dynamic, Web-based composition of software components appears to be a very impelling need. The demonstration focuses on a Web-based system that supports dynamic component composition.
[information resources, Java, Web-based software components, Drives, LAN interconnection, Mediation, software component distribution, Dynamic compiler, Computer languages, Permission, software reusability, dynamic component composition, Skeleton, Internet, Joining processes, distributed object management]
Prompter-a project planning assistant
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
The aim of the Prompter project was to develop the Prompter tool, a "decision-support tool to assist in the planning and managing of a software project". Prompter has the ability to help software project planners assimilate best practice and 'know how' in the field of software project planning and incorporate expert critiquing which will assist project planners in solving the complex problems associated with the planning of a software project.
[project management, Decision making, Process planning, Project management, software development management, Switches, Prompter tool, software agents, Intelligent agent, Best practices, decision support systems, project planning assistant, Software development management, planning, decision support tool, software project planning, Computer applications, Permission, expert critiquing, Software tools, software project management, best practice]
Visualizing software release histories with 3DSoftVis
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This paper briefly introduces a 3D visualization tool (3DSoftVis) that has been developed for the analysis of the evolution of an industrial software system.
[Navigation, Laboratories, Control systems, industrial software system evolution, History, software release history visualisation, program visualization, Programming profession, 3DSoftVis, Data visualization, Computer industry, Software systems, Concrete, software engineering, data visualization, 3D visualization tool, Software tools]
Legacy systems migration in CELLEST
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
Most research on legacy user interface migration has adopted code understanding as the means for system modeling and reverse engineering. The methodological assumption underlying the CELLEST project is that the purpose of system migration is to enable, and possibly optimize, its current uses on a new platform. This is why CELLEST uses traces of the system-user interaction to reverse engineer the legacy interface, extract its current uses and generate GUIs on new platforms as wrappers for it.
[Protocols, code understanding, Navigation, graphical user interfaces, Reverse engineering, Optimization methods, Communication system control, legacy systems migration, reverse engineering, Spatial databases, legacy user interface migration, system modeling, system-user interaction, Prototypes, wrappers, User interfaces, Permission, GUIs, software engineering, Graphical user interfaces]
Process engineering with Spearmint/sup TM//EPG
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
This paper presents the Spearmint process modeling tool and the Electronic Process Guide (EPG) generator. Together they enable process engineers to elicit, model, analyze and document software processes and then to automatically generate Web-based guidebooks based on the documented processes.
[Process design, Printing, information resources, Electronic Process Guide, Costs, project support environments, Spearmint process modeling tool, system documentation, Certification, Best practices, Guidelines, process engineering, Industrial electronics, software process documentation, Electronics industry, Computer industry, computer aided software engineering, World Wide Web-based guidebook generator, Internet, software tools, EPG]
An overview of the ICSE 2000 workshop program
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Computer science, Conferences, Servers]
Second ICSE workshop on web engineering
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[]
The first international workshop on automated program analysis, testing and verification
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[]
Beg, borrow, or steal: using multidisciplinary approaches in empirical software engineering research
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Computer science, Data analysis, Councils, Psychology, Permission, Educational institutions, Constraint theory, Cultural differences, Application software, Software engineering]
The second international symposium on constructing software engineering tools (CoSET2000)
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[]
Design, specification, and verification of interactive systems
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Human computer interaction, Interactive systems, Computer architecture, User interfaces, Permission, Application software, Usability, Least squares approximation, Context modeling, Software engineering]
Workshop on standard exchange format (WoSEF)
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[]
3rd workshop on software engineering over the internet
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[]
Workshop on multi-dimensional separation of concerns in software engineering
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[]
The 2/sup nd/ international workshop on economics-driven software engineering research
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[]
WISE/sup 3/: the third international workshop on intelligent software engineering
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[]
Software product lines: economics, architectures, and applications
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Technological innovation, Costs, Software architecture, Computer architecture, Software quality, Permission, Educational institutions, Application software, Software engineering, Business]
Agent-oriented software engineering
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Computer science, Protocols, Software architecture, Permission, Software agents, Character recognition, Application software, Yarn, Research and development, Software engineering]
Specifying and measuring quality in use
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Productivity, System testing, IEC standards, Ergonomics, ISO standards, Software quality, Software safety, Software measurement, Product safety, Usability]
Designing and analyzing software architectures using AIBASs
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Availability, Software testing, System testing, Software design, Object oriented modeling, Computer architecture, Software performance, Software quality, Usability, Software engineering]
Building modular object-oriented systems with reusable collaborations
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Computer science, Printing, Object oriented modeling, Unified modeling language, Collaboration, Permission, Programming, Educational institutions, Logic, Business]
Introduction to CORBA
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Transport protocols, Heart, Computer languages, Java, Operating systems, Scalability, Skeleton, Hardware, Internet, Communication standards]
Moving from ISO9000 to higher levels of the CMM
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Computer science, Software maintenance, Production systems, Coordinate measuring machines, Quality assurance, Standards organizations, Application software, Capability maturity model, Certification, Guidelines]
Planning realistic schedules using software architecture
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Estimation error, Job shop scheduling, Software design, Software architecture, Process planning, Project management, Computer architecture, Programming, Educational institutions, Software development management]
Improving design and source code modularity using aspectJ/sup TM/
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Computer science, Java, Protocols, Software design, Terminology, Laboratories, Permission, Concrete, Computer security, Contracts]
Scalability issues in CORBA-based systems
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Scalability]
Intellectual property protection for software in the united states and europe: the changing roles of patents and copyrights
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[International trade, Technological innovation, Europe, Intellectual property, Copyright protection, Needles, Software, Explosives, Patent law, Legal factors]
Software process improvement: best practices and lessons learned
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Productivity, Coordinate measuring machines, Standards organizations, Time to market, Permission, Computer industry, Cost function, SPICE, Cultural differences, Best practices]
Designing real-time and distributed applications with the UML
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Real time systems, Software design, Object oriented modeling, Design methodology, Unified modeling language, Collaboration, Permission, Performance analysis, Application software, Software engineering]
System development using application services over the net
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Computer science, Costs, Extranets, Application specific processors, Educational institutions, Hardware, Enterprise resource planning, Message service, Application software, Computer network management]
Software reliability: basic concepts and assessment methods
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Software testing, Uncertainty, Battery powered vehicles, Software quality, Computer industry, Economic forecasting, Software reliability, Software measurement, Application software, Software tools]
Product-line architectures, aspects, and reuse
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Software design, Microscopy, Refining, Buildings, Computer architecture, Programmable logic arrays, Permission, Application software, Software reusability, Assembly]
Advanced visual modeling: beyond UML
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Visual system, IEEE Computer Society Press, Unified modeling language, Laboratories, Collaboration, Cities and towns, Software systems, Computer industry, Gas insulated transmission lines, Contracts]
Understanding code mobility
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Java, Technology management, Shape, Taxonomy, Mobile agents, Permission, Mobile communication, Application software, Security, Artificial intelligence]
Fault tolerance via diversity against design faults: design principles and reliability assessment
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Fault tolerance, Fault tolerant systems, Battery powered vehicles, Railway safety, Permission, Rail transportation, Robustness, Software reliability, Application software, Software engineering]
Improving software inspections by using reading techniques
Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
None
2000
false
[Bioreactors, Feedback, Inspection, Programming, Permission, Educational institutions, Software systems, Surges, Software engineering, Testing]
Babel: representing business rules in XML for application integration
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
In this paper, we discuss Babel, a prototype tool for integrating multiple heterogeneous applications, by wrapping them and by specifying the logic of their interoperation in XML.
[business rules representation, open systems, Babel, prototype tool, task analysis, Runtime, Databases, application integration, Prototypes, specification languages, software tools, Logic, Monitoring, hypermedia markup languages, Process control, wrapping, Wrapping, interoperation logic specification, XML, integrated software, User interfaces, computer aided software engineering, heterogeneous applications, Portals, business data processing]
On the syllogistic structure of object-oriented programming
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Recent works by J.F. Sowa (2000) and D. Rayside and G.T. Campbell (2000) demonstrate that there is a strong connection between object-oriented programming and the logical formalism of the syllogism, first set down by Aristotle in the Prior Analytics (1928). In this paper, we develop an understanding of polymorphic method invocations in terms of the syllogism, and apply this understanding to the design of a novel editor for object-oriented programs. This editor is able to display a polymorphic call graph, which is a substantially more difficult problem than displaying a non-polymorphic call graph. We also explore the design space of program analyses related to the syllogism, and find that this space includes Unique Name, Class Hierarchy Analysis, Class Hierarchy Slicing, Class Hierarchy Specialization, and Rapid Type Analysis.
[object-oriented programming, Logic programming, Humans, syllogistic structure, Programming profession, Computer displays, Mood, Animal structures, polymorphic method invocations, Concrete, polymorphic call graph, Space exploration, Object oriented programming, logical formalism]
Scientific rigour, an answer to a pragmatic question: a linguistic framework for software engineering
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Discussions of the role of mathematics in software engineering are common and have probably not changed much over the last few decades. There is now much discussion about the "intuitive" nature of software construction and analogies are drawn (falsely) with graphic design, (conventional) architecture, etc. The conclusion is that mathematics is an unnecessary luxury and that, like these other disciplines, it is not needed in everyday practice. We attempt to refute these arguments by recourse to ideas from the philosophy of science developed over the past century. We demonstrate why these ideas are applicable, why they establish a framework (in the sense of Carnap) in which many central ideas in software engineering can be formalised and organised, why they refute the simplistic recourse to "intuition\
[linguistic framework, philosophy of science, Buildings, mathematics, frameworks, Carnap, systems design, Mathematics, philosophical aspects, Graphics, Computer science, Design engineering, requirements engineering, software construction, epistemology, Computer architecture, Software systems, intuitive methods, software engineering, Informatics, Software engineering, Mathematical programming, software process]
Holmes: an intelligent system to support software product line development
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Holmes is a software product line tool that supports all core activities of software product line analysis and development. Holmes integrates its tools using a blackboard architecture based on a Linda tuple space. A novel feature is the use of a critiquing system to provide semantic support. This is demonstrated with an example.
[Java, expert systems, Subscriptions, semantic support, software product line development, Information analysis, software product line analysis, software tool, Software metrics, critiquing system, Coupled mode analysis, Data visualization, Computer architecture, product development, Software systems, computer aided software engineering, intelligent system, software tools, Intelligent systems, Software tools, Holmes, Linda tuple space, blackboard architecture]
Supporting program comprehension using semantic and structural information
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Focuses on investigating the combined use of semantic and structural information of programs to support the comprehension tasks involved in the maintenance and reengineering of software systems. "Semantic information" refers to the domain-specific issues (both the problem and the development domains) of a software system. The other dimension, structural information, refers to issues such as the actual syntactic structure of the program, along with the control and data flow that it represents. An advanced information retrieval method, latent semantic indexing, is used to define a semantic similarity measure between software components. Components within a software system are then clustered together using this similarity measure. Simple structural information (i.e. the file organization) of the software system is then used to assess the semantic cohesion of the clusters and files with respect to each other. The measures are formally defined for general application. A set of experiments is presented which demonstrates how these measures can assist in the understanding of a nontrivial software system, namely a version of NCSA Mosaic.
[semantic similarity measure, latent semantic indexing, Data mining, software systems maintenance, syntactic program structure, control flow, structural information, information retrieval method, Software measurement, program comprehension, software development domain, program control structures, indexing, Natural languages, NCSA Mosaic, Documentation, data flow analysis, information retrieval, Information retrieval, reverse engineering, software component clustering, Application software, software maintenance, problem domain, file organization, domain-specific issues, Computer science, Computer languages, systems re-engineering, online front-ends, Software systems, software systems reengineering, semantic cohesion, semantic information, data flow, Indexing, software metrics]
A component-based approach to building formal analysis tools
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Automatic-verification capability tends to be packaged into stand-alone tools, as opposed to components that are easily integrated into a larger software-development environment. Such packaging complicates integration because it involves translating internal representations into a form compatible with the stand-alone tool. By contrast, lightweight-analysis components package analysis capability in a form that does not involve such a translation. Borrowing ideas from GenVoca and object-oriented design patterns, we developed a domain model and an automatic generation framework for lightweight-analysis components. The generated components operate directly over the internal form of a specification without requiring a change in representation. Moreover, the domain model identifies several "useful subsets" that can be used to customize analysis capability to a particular application. We validated this domain model by generating lightweight analyzers for temporal logic and the behavioral subset of Lotos.
[Thyristors, Computer aided software engineering, component-based approach, Unified modeling language, object-oriented design patterns, temporal logic, stand-alone tools, GenVoca, Software design, formal verification, domain model, behavioral subset, formal analysis tools, Logic, lightweight-analysis components, object-oriented programming, Object oriented modeling, automatic generation framework, Formal specifications, Computer science, Lotos, Packaging, Software engineering, automatic-verification capability, software-development environment]
Reengineering analysis of object-oriented systems via duplication analysis
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
All software systems, no matter how they are designed, are subject to continuous evolution and maintenance activities to eliminate defects and extend their functionalities. This is particularly true for object-oriented systems, where we may develop different software systems using the same internal library or framework. These systems may evolve in quite different directions in order to cover different functionalities. Typically, there is the need to analyze their evolution in order to redefine the library or framework boundaries. This is a typical problem of software reengineering analysis. In this paper, we describe metrics, based on duplication analysis, that contribute to the process of reengineering analysis of object-oriented systems. These metrics are the basic elements of a reengineering analysis method and tool. Duplication analyses at the file, class and method levels have been performed. A structural analysis using metrics that capture similarities in class structure has been also exploited. In order to identify the best approach for the reengineering analysis of object-oriented systems, a comparison between the two approaches is described. In this paper, a case study based on real cases is presented, in which the results obtained by using a reengineering process with and without the analysis tool are described. The purpose of this study is to discover which method is the most powerful and how much time reduction can be obtained by its use.
[frameworks, Software performance, internal libraries, software evolution, class-level analysis, structural analysis, class structure similarities, Genetic programming, software defect elimination, Performance analysis, object-oriented methods, method-level analysis, Functional programming, Informatics, Object oriented programming, object-oriented programming, clone detection, Cloning, object-oriented systems, Application software, software maintenance, case study, file-level analysis, systems re-engineering, Software libraries, time reduction, Software systems, code duplication analysis, software functionality extension, software reengineering analysis, software metrics]
A scalable formal method for design and automatic checking of user interfaces
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
The paper addresses the formal specification, design and implementation of the behavioral component of graphical user interfaces. Dialogs are specified by means of modular, communicating grammars called VEG (Visual Event Grammars), which extend traditional BNF grammars to make the modeling of dialogs more convenient. A VEG specification is independent of the actual layout of the GUI, but it can be easily integrated with various layout design toolkits. The specification may be verified with the model checker Spin, in order to test consistency and correctness, to detect deadlocks and unreachable states, and also to generate test cases for validation purposes. Efficient code is automatically generated by the VEG toolkit, based on compiler technology. Realistic applications have been specified, verified and implemented, like a Notepad-style editor, a graph construction library and a large real application to medical software. The complete VEG toolkit is going to be available soon as free software.
[user interface management systems, medical software, Design methodology, graphical user interfaces, VEG specification, Spin, scalable formal method, test cases, compiler technology, user interfaces, formal specification, program compilers, behavioral component, unreachable states, Notepad-style editor, model checker, deadlocks, VEG toolkit, GUI design, validation, Graphical user interfaces, Testing, BNF grammars, large real application, Application software, Formal specifications, graph construction library, Visual Event Grammars, HCI, Software libraries, human-computer interaction, model checking, Layout, formal methods, User interfaces, System recovery, VEG, layout design toolkits, automatic checking, Software tools, modular communicating grammars]
Commitment development in software process improvement: critical misconceptions
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
It has been well-established in the software process improvement (SPI) literature and practice that, without commitment from all organizational levels to SPI, the initiative will most likely fail or the results will not be far-reaching. In this paper, the 'commitment' construct is explored, and three forms of commitment are introduced: affective, continuance and normative commitment. Analysis shows that current models of commitment development lack scientific validity and are based on four misconceptions: (1) the assumption of linearity of the human cognitive process (i.e. commitment in this case), (2) the controllability of this process, (3) the notion of a singular commitment construct, and (4) the sole utility perspective on the commitment phenomenon. Implications of these findings for SPI research and practice are discussed.
[human cognitive process linearity, commitment development, continuance commitment, commitment construct, human factors, affective commitment, Human factors, Programming, scientific validity, controllability, psychology, Investments, software process improvement, Controllability, Modems, sole utility perspective, misconceptions, Productivity, normative commitment, software development management, organizational levels, Linearity, Software quality, Information processing, Computer industry, human cognitive process controllability]
Finding failures by cluster analysis of execution profiles
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
We experimentally evaluate the effectiveness of using cluster analysis of execution profiles to find failures among the executions induced by a set of potential test cases. We compare several filtering procedures for selecting executions to evaluate for conformance to requirements. Each filtering procedure involves a choice of a sampling strategy and a clustering metric. The results suggest that filtering procedures based on clustering are more effective than simple random sampling for identifying failures in populations of operational executions, with adaptive sampling from clusters being the most effective sampling strategy. The results also suggest that clustering metrics that give extra weight to industrial profile features are most effective. Scatter plots of execution populations, produced by multidimensional scaling, are used to provide intuition for these results.
[Software testing, program debugging, program testing, execution profiles, scatter plots, random sampling, Personnel, experiment, Failure analysis, sampling strategy, software failures, cluster analysis, adaptive sampling, Multidimensional systems, Filtering, Instruments, software testing, Scattering, Computer science, Automatic testing, pattern clustering, clustering metric, Sampling methods, filtering procedures, multidimensional scaling]
A case study of the evolution of Jun: an object-oriented open-source 3D multimedia library
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Jun is a large open-source graphics and multimedia library. It is object-oriented and supports 3D geometry, topography and multimedia. This paper reviews the development of the Jun library from five perspectives: open-source, software evolution processes, development styles, technological support, and development data. It concludes with lessons learned from the perspective of a for-profit company providing open-source object-oriented software to the community.
[Computer aided software engineering, object-oriented programming, Jun, 3D geometry, software library, topography, software development styles, Programming, Topology, Surface topography, multimedia computing, Open source software, software libraries, case study, software evolution, Graphics, Geometry, Software libraries, computer graphics, software reusability, Motion pictures, Rendering (computer graphics), object-oriented 3D multimedia library, open-source graphics library]
Dynamic and selective combination of extensions in component-based applications
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Support for dynamic and client-specific customization is required in many application areas. We present a (distributed) application as consisting of a minimal functional core, implemented as a component based system, and an unbound set of potential extensions that can be selectively integrated within this core functionality. An extension to this core may be a new service due to new requirements of end users. Another important category of extensions we consider are non-functional services such as authentication, which typically introduce interaction refinements at the application level. In accordance with the separation of concerns principle, each extension is implemented as a layer of mixin-like wrappers. Each wrapper incrementally adds behavior and state to a core component instance from the outside, without modifying the component's implementation. The novelty of this work is that the composition logic, responsible for integrating extensions into the core system, is externalized from the code of clients, core system and extensions. Clients (end users, system integrators) can customize this composition logic on a per collaboration basis by 'attaching' high-level interpretable extension identifiers to their interactions with the core system.
[interaction refinements, non-functional services, distributed application, formal logic, composition logic, Runtime, Software architecture, separation of concerns principle, Production, extension combination, client-specific customization, Logic, Object oriented programming, distributed programming, component based applications, authentication, object-oriented programming, Object oriented modeling, potential extensions, high-level interpretable extension identifiers, minimal functional core, Application software, core system, unbound set, mixin-like wrappers, core component instance, Computer science, end users, selective combination, Authentication, systems analysis, Collaborative work, system integrators, end user requirements]
Reuse of verification efforts and incomplete specifications in a formalized, iterative and incremental software process
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
The possibility of verifying systems during any phase of the software development process is one of the most significant advantages of using formal methods. Model checking is considered to be the broadest-used formal verification technique, even though a great quantity of computing resources are needed to verify medium-large and large systems. As verification is present over the whole software process, this amount of resources is more critical in incremental and iterative life-cycles. Our proposal focuses on reusing incomplete models and their verification results - which are obtained from a model-checking algorithm - in order to improve this kind of life-cycle. Making good use of these previous verification results can reduce the formal verification costs by minimizing the set of requirements and the set of system states where the properties must be verified. The unspecification that is inherent to incomplete systems is used to provide an approximate and content-oriented retrieval which is supplemented by suggestions to match the desired specifications.
[software development process, Costs, software prototyping, formal verification technique, Multivalued logic, incomplete systems, Proposals, formal specification, approximate content-oriented retrieval, formal verification, verification costs, computing resources, Software reusability, system state minimization, Natural languages, incomplete specifications reuse, Information retrieval, Logic design, content-based retrieval, Organizing, software verification effort reuse, large systems, Software libraries, formalized iterative incremental software process, model checking, unspecification, formal methods, Iterative algorithms, requirements minimization]
Software product lines: organizational alternatives
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Software product lines are enjoying increasingly wide adoption in the software industry. Most authors focus on the technical and process aspects, and assume an organizational model consisting of a domain engineering unit and several application engineering units. In our cooperation with several software development organizations applying software product-line principles, we have identified several other organizational models that are employed as well. In this article, we present a number of organizational alternatives, organized around four main models, viz. a development department, business units, domain engineering unit and hierarchical domain engineering units. For each model, its characteristics, applicability, advantages and disadvantages are discussed, as well as an example. Based on an analysis of these models, we present three factors that influence the choice of the organizational model, viz. product-line assets, responsibility levels and the type of organizational units.
[Process design, organizational unit types, Programming, business units, Proposals, organizational alternatives, development department, Software design, software industry, Computer architecture, software packages, software houses, responsibility levels, Software reusability, software product lines, hierarchical units, corporate modelling, DP industry, software development management, product-line assets, Application software, application engineering units, software development organizations, Software systems, Computer industry, organizational models, domain engineering unit, Software engineering]
Improving validation activities in a global software development
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Global software development challenges traditional techniques of software engineering, such as peer reviews or teamwork. Effective teamwork and coaching of engineers contribute highly towards successful projects. In this case study, we evaluate experiences with validation activities in a global setting within Alcatel's switching and routing business. We investigate three hypotheses related to the effects of (a) collocated inspections, (b) intensive coaching and (c) feature-oriented development teams on globally distributed projects. As all these activities mean an initial investment compared to a standard process with scattered activities, the major validation criteria for the three hypotheses are: (1) cost reduction due to earlier defect detection and (2) less defects introduced. The data is taken from a sample of over 60 international projects of various sizes, from which we have collected all types of product and process metrics over the past four years.
[switching, validation criteria, international projects, Costs, program verification, peer reviews, Programming, training, feature-oriented development teams, process metrics, software quality, telecommunication computing, routing, product metrics, global software development, software process improvement, software engineering techniques, Marketing and sales, software engineering, scattered activities, Contracts, inspection, collocated inspections, Context, cost reduction, nonquality cost, efficiency, Scattering, software validation activities improvement, Inspection, Routing, early defect detection, globally distributed projects, incremental development, case study, intensive coaching, Alcatel, teamwork, software defects, Teamwork, Software engineering, software metrics]
Investigating the cost-effectiveness of reinspections in software development
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Software inspection is one of the most effective methods to detect defects. Reinspection repeats the inspection process for software products that are suspected to contain a significant number of undetected defects after an initial inspection. As a reinspection is often believed to be less efficient than an inspection an important question is whether a reinspection justifies its cost. In this paper we propose a cost-benefit model for inspection and reinspection. We discuss the impact of cost and benefit parameters on the net gain of a reinspection with empirical data from an experiment in which 31 student teams inspected and reinspected a requirements document. Main findings of the experiment are: a) For reinspection benefits and net gain were significantly lower than for the initial inspection. Yet, the reinspection yielded a positive net gain for most teams with conservative cost-benefit assumptions. B) Both the estimated benefits and number of major defects are key factors for reinspection net gain, which emphasizes the need for appropriate estimation techniques.
[Costs, requirements document, program testing, software development, Project management, Software performance, Programming, Inspection, reinspections, Scheduling, software quality, Yield estimation, Phase detection, Investments, systems analysis, Software quality, cost effectiveness, software cost estimation, inspection, cost-benefit model]
A framework for multi-valued reasoning over inconsistent viewpoints
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
In requirements elicitation, different stakeholders often hold different views of how a proposed system should behave, resulting in inconsistencies between their descriptions. Consensus may not be needed for every detail, but it can be hard to determine whether a particular disagreement affects the critical properties of the system. We describe the Xbel framework for merging and reasoning about multiple, inconsistent state machine models. Xbel permits the analyst to choose how to combine information from the multiple viewpoints, where each viewpoint is described using an underlying multi-valued logic. The different values of our logics typically represent different levels of agreement. Our multi-valued model checker, Xchek, allows us to check the merged model against properties expressed in a temporal logic. The resulting framework can be used as an exploration tool to support requirements negotiation, by determining what properties are preserved for various combinations of inconsistent viewpoints.
[Vocabulary, agreement, inconsistent viewpoints, Merging, Xchek, temporal logic, Multivalued logic, stakeholders, multi-valued model checker, formal specification, multi-valued logic, multi-valued reasoning, Information analysis, Computer science, requirements engineering, Databases, systems analysis, multiple inconsistent state machine models, merged model, software tools, requirements elicitation, Xbel framework, Software tools, requirements negotiation]
Fast formal analysis of requirements via "topoi diagrams"
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Early testing of requirements can decrease the cost of removing errors in software projects. However unless done carefully, that testing process can significantly add to the cost of requirements analysis. We show that requirements expressed as topoi diagrams can be built and tested cheaply /sup s/ing our SP2 algorithm, the formal temporal properties of a large class of topoi can be proven very quickly, in time nearly linear in the number of nodes and edges in the diagram. There are two limitations to our approach. Firstly, topoi diagrams cannot express certain complex concepts such as iteration and sub-routine calls. Hence, our approach is more useful for requirements engineering than for traditional model checking domains. Secondly, our approach is better for exploring the temporal occurrence of properties than the temporal ordering of properties. Within these restrictions, we can express a useful range of concepts currently seen in requirements engineering, and a wide range of interesting temporal properties.
[Software testing, System testing, testing process, Costs, program verification, NASA, Laboratories, State-space methods, Personnel, SP2 algorithm, Logic testing, formal specification, Recruitment, formal temporal properties, requirements engineering, fast formal requirements analysis, topoi diagrams, software projects, Performance analysis]
Engineering mobile-agent applications via context-dependent coordination
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Mobility introduces peculiar coordination problems in agent-based Internet applications. First, it suggests the exploitation of an infrastructure based on a multiplicity of local interaction spaces. Second, it may require coordination activities to be adapted both to the characteristics of the execution environment where they occur and to the needs of the application to which the coordinating agents belong. This paper introduces the concept of context-dependent coordination based on programmable interaction spaces. On the one hand, interaction spaces associated to different execution environments may be independently programmed so as to lead to differentiated, environment-dependent, behaviors. On the other hand, agents can program the interaction spaces of the visited execution environments to obtain an application-dependent behavior of the interaction spaces themselves. Several examples show how an infrastructure for context-dependent coordination can be exploited to simplify the design of Internet applications based on mobile agents. In addition, the MARS coordination infrastructure is presented as an example of a system in which the concept of context-dependent coordination has found a clean and efficient implementation.
[Protocols, Internet applications, Electronic mail, programmable interaction spaces, Environmental management, context-dependent coordination, distributed program development, Mars, Mobile agents, coordination infrastructure, Software agents, software engineering, local interaction spaces, distributed programming, Knowledge management, Application software, software agents, coordination problems, synchronisation, mobile-agent applications, infrastructure, MARS, agent-based Internet applications, Internet, Resource management, coordination activities]
Efficient filtering in publish-subscribe systems using binary decision diagrams
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Implicit invocation or publish-subscribe has become an important architectural style for large-scale system design and evolution. The publish-subscribe style facilitates developing large-scale systems by composing separately developed components because the style permits loose coupling between various components. One of the major bottlenecks in using publish-subscribe systems for very large scale systems is the efficiency of filtering incoming messages, i.e., matching of published events with event subscriptions. This is a very challenging problem because in a realistic publish subscribe system the number of subscriptions can be large. We present an approach for matching published events with subscriptions which scales to a large number of subscriptions. Our approach uses binary decision diagrams, a compact data structure for representing Boolean functions which has been successfully used in verification techniques such as model checking. Experimental results clearly demonstrate the efficiency of our approach.
[Subscriptions, data structure, shared variables, verification techniques, binary decision diagrams, software architecture, efficient filtering, Boolean functions, very large scale systems, asynchronous message passing, Operating systems, Filtering algorithms, software engineering, Large-scale systems, Contracts, incoming messages, separately developed components, Java, client-server systems, message passing, loose coupling, Data structures, large-scale systems, published events, Multicast algorithms, event subscriptions, architectural styles, Publish-subscribe, implicit invocation, remote procedure calls, publish-subscribe systems]
Analysis and testing of Web applications
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
The economic relevance of Web applications increases the importance of controlling and improving their quality. Moreover, the newly available technologies for their development allow the insertion of sophisticated functions, but often leave the developers responsible for their organization and evolution. As a consequence, a high demand is emerging for methodologies and tools for the quality assurance of Web-based systems. In this paper, a UML model of Web applications is proposed for their high-level representation. Such a model is the starting point for several analyses, which can help in the assessment of the static site structure. Moreover, it drives Web application testing, in that it can be exploited to define white-box testing criteria and to semi-automatically generate the associated test cases. The proposed techniques were applied to several real-world Web applications. The results suggest that automatic support for verification and validation activities can be extremely beneficial. In fact, it guarantees that all paths in the site which satisfy a selected criterion are properly exercised before delivery. The high level of automation that is achieved in test case generation and execution increases the number of tests that are conducted and simplifies the regression checks.
[Software testing, System testing, program testing, program verification, economic relevance, Unified modeling language, World Wide Web application testing, application quality control, regression checks, validation activities, code analysis, white-box testing criteria, Quality assurance, static site structure assessment, high-level representation, verification activities, Feedback, specification languages, World Wide Web application analysis, information resources, UML model, Automation, semi-automatic test case generation, program diagnostics, reverse engineering, Application software, Automatic testing, quality assurance, Software systems, Computer industry]
Lightweight analysis of operational specifications using inference graphs
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
The Amalia framework generates lightweight components that automate the analysis of operational specifications and designs. A key concept is the step analyzer, which enables Amalia to automatically tailor high-level analyses, such as behavior simulation and model checking, to different specification languages and representations. A step analyzer uses a new abstraction, called an inference graph, for the analysis. It creates and evaluates an inference graph on-the-fly during a top-down traversal of a specification to deduce the specification's local behaviors (called steps). The nodes of an inference graph directly reify the rules in an operational semantics, enabling Amalia to automatically generate a step analyzer from an operational description of a notation's semantics. Inference graphs are a clean abstraction that can be formally defined. The paper provides a detailed but informal introduction to inference graphs. It uses example specifications written in LOTOS for purposes of illustration.
[Algorithm design and analysis, LOTOS, operational semantics, specification local behavior, Automatic test pattern generation, representations, formal specification, Engines, step analyzer, inference graphs, lightweight components, Design engineering, Analytical models, graphs, formal verification, high-level analyses, design patterns, specification languages, Pattern analysis, notation semantics, software testing, automated software generator, operational description, Amalia framework, Test pattern generators, inference mechanisms, formally defined abstraction, top-down specification traversal, Computer science, operational specifications, model checking, Automatic testing, formal methods, Packaging, application generators, behavior simulation, computer aided software engineering, subroutines, lightweight specification analysis]
Incorporating varying test costs and fault severities into test case prioritization
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. In previous work (S. Elbaum et al., 2000; G. Rothermel et al., 1999), we provided a metric, APFD, for measuring rate of fault detection, and techniques for prioritizing test cases to improve APFD, and reported the results of experiments using those techniques. This metric and these techniques, however, applied only in cases in which test costs and fault severity are uniform. We present a new metric for assessing the rate of fault detection of prioritized test cases that incorporates varying test case and fault costs. We present the results of a case study illustrating the application of the metric. This study raises several practical questions that might arise in applying test case prioritization; we discuss how practitioners could go about answering these questions.
[Software testing, System testing, performance goal, testing process, Costs, Computer aided software engineering, program testing, regression testing, prioritized test cases, fault costs, fault severities, test case prioritization, varying test case costs, fault severity, fault detection rate, Job shop scheduling, Application software, case study, Computer science, Processor scheduling, APFD, Fault detection, varying test costs, Frequency, software cost estimation, software metrics]
Modeling and controlling the software test process
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
A novel approach for modeling and control of the software test process is presented. The approach is based on the concept of state variables and uses techniques from the well-established field of automatic control theory. An initial model of the software test phase is described and the results of a case study analysis are presented.
[Software testing, System testing, State feedback, Costs, program testing, control theory, software test process control, feedback control, Programming, Predictive models, Feedback control, case study analysis, automatic control theory, Automatic testing, Differential equations, Automatic control, state variables, software test process modeling]
Evaluating the reverse engineering capabilities of Web tools for understanding site content and structure: a case study
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
This paper describes an evaluation of the reverse engineering capabilities of three Web tools for understanding site content and structure. The evaluation is based on partitioning Web sites into three classes (static, interactive, and dynamic), and is structured using an existing reverse engineering environment framework (REEF). This case study also represents an initial evaluation of the applicability of the REEF in the related but qualitatively different domain of Web sites. The case study highlights several shortcomings of current Web tools in the context of aiding understanding to support evolution. For example, most Web tools are geared towards new page design and development, not to understanding detailed page content or overall site structure. The evaluation also identified some aspects of the REEF that might benefit from refinement to better reflect Web tool capabilities that support common evolution tasks. For example, Web server log file analysis as a specialized form of data gathering and subsequence information presentation.
[information resources, Web page design, Web tools, Web server log file analysis, Reverse engineering, Web site content, Companies, reverse engineering, Application software, case study, Information analysis, Computer science, reverse engineering environment framework, REEF, data gathering, Computer architecture, Libraries, Large-scale systems, Internet, Web site reverse engineering, Web server]
Encoding program executions
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Dynamic analysis is based on collecting data as the program runs. However, raw traces tend to be too voluminous and too unstructured to be used directly for visualization and understanding. We address this problem in two phases: the first phase selects subsets of the data and then compacts it, while the second phase encodes the data in an attempt to infer its structure. Our major compaction/selection techniques include gprof-style N-depth call sequences, selection based on class, compaction based on time intervals, and encoding the whole execution as a directed acyclic graph. Our structure inference techniques include run-length encoding, context free grammar encoding, and the building of finite state automata.
[gprof-style N-depth call sequences, run-length encoding, directed acyclic graph, Displays, data encoding, Compaction, compaction/selection techniques, finite state machines, data subset collection, program execution encoding, context-free grammars, Libraries, Performance analysis, Navigation, data analysis, program diagnostics, Buildings, finite state automata, reverse engineering, Encoding, dynamic analysis, structure inference techniques, encoding, Computer science, context free grammar encoding, directed graphs, Data visualization, Software systems, raw traces, time intervals]
Describing software architecture with UML
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
The presence of a solid architectural vision is a key discriminator in the success or failure of a software project. This tutorial examines what software architecture is and what it is not. It discusses and illustrates how to describe architecture through a set of design viewpoints and views and how to express these views in UML (Rumbaugh et al., 1998), in the spirit of the new IEEE Standard: Recommended practice for architectural description. The tutorial shows of how architectures drive the development process and how to capture architectural design patterns using UML (Unified Modeling Language). It is illustrated by several widely applicable architectural patterns in different domains.
[software development process, object-oriented programming, Unified Modeling Language, Software requirements and specifications, Specification languages, software project, design viewpoints, formal specification, software architecture description, software architecture, IEEE Standard, UML, design patterns, specification languages, Software standards, IEEE standards, Object oriented programming, software standards]
Quantifying the costs and benefits of architectural decisions
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
The benefits of a software system are assessable only relative to the business goals the system has been developed to serve. In turn, these benefits result from interactions between the system's functionality and its quality attributes (such as performance, reliability and security). Its quality attributes are, in most cases, dictated by its architectural design decisions. Therefore, we argue that the software architecture is the crucial artifact to study in making design tradeoffs and in performing cost-benefit analyses. A substantial part of such an analysis is in determining the level of uncertainty with which we estimate both costs and benefits. We offer an architecture-centric approach to the economic modeling of software design decision making called CBAM (Cost Benefit Analysis Method), in which costs and benefits are traded off with system quality attributes. We present the CBAM, the early results from applying this method in a large-scale case study, and discuss the application of more sophisticated economic models to software decision making.
[Uncertainty, economic modeling, cost benefit analysis, Cost benefit analysis, software system, software architecture, Software architecture, CBAM, cost-benefit analysis, Computer architecture, economic models, cost-benefit analyses, Performance analysis, architectural design decisions, architecture-centric approach, business goals, system quality attributes, software design decision making, Decision making, Buildings, architectural decisions, design tradeoffs, Public policy, system functionality, quality attributes, Software systems, software cost estimation, large-scale case study, Cost Benefit Analysis Method, Software engineering]
Design and evaluation of the mobile agent architecture for distributed consistency management
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
The proposed mobile agent architecture for carrying out incremental consistency checks between sets of distributed software engineering documents is described and evaluated. Functionality of architectural components and collaboration between them throughout the consistency check are described. Architecture simulation, based on concurrent "execution" of state chart models of components, is used for evaluation of scalability in a number of system configurations. This work represents the first part of a thesis, which aims to establish applicability of mobile agent technology to the domain of distributed consistency management.
[document handling, Scalability, Project management, software architecture simulation, Educational institutions, state chart models, software agents, mobile agent architecture, distributed consistency management, distributed software engineering documents, scalability, Computer science, software architecture, Technology management, Engineering management, Mobile agents, XML, Computer architecture, incremental consistency checks, distributed programming, Software engineering]
jMocha: a model checking tool that exploits design structure
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Model checking is a practical tool for automated debugging of embedded software. In model checking, a high-level description of a system is compared against a logical correctness requirement to discover inconsistencies. Since model checking is based on exhaustive state-space exploration and the size of the state space of a design grows exponentially with the size of the description, scalability remains a challenge. We have thus developed techniques for exploiting modular design structure during model checking, and the model checker jMocha (Java MOdel-CHecking Algorithm) is based on this theme. Instead of manipulating unstructured state-transition graphs, it supports the hierarchical modeling framework of reactive modules. jMocha is a growing interactive software environment for specification, simulation and verification, and is intended as a vehicle for the development of new verification algorithms and approaches. It is written in Java and uses native C-code BDD libraries from VIS. jMocha offers: (1) a GUI that looks familiar to Windows/Java users; (2) a simulator that displays traces in a message sequence chart fashion; (3) requirements verification both by symbolic and enumerative model checking; (4) implementation verification by checking trace containment; (5) a proof manager that aids compositional and assume-guarantee reasoning; and (6) SLANG (Scripting LANGuage) for the rapid and structured development of new verification algorithms. jMocha is available publicly at <http://www.eecs.berkeley.edu//spl sim/mocha>; it is a successor and extension of the original Mocha tool that was entirely written in C.
[Algorithm design and analysis, interactive software environment, program debugging, enumerative model checking, scripting language, Scalability, simulation, Binary decision diagrams, requirements verification, logical correctness requirement, formal specification, publicly available software, scalability, Embedded software, Vehicles, reactive modules, inconsistencies discovery, formal verification, embedded systems, native C-code BDD libraries, GUI, software tools, implementation verification, formal verification algorithms, symbolic model checking, compositional reasoning, Java, jMocha, message sequence charts, Software algorithms, trace containment checking, Debugging, automated debugging, high-level system description, State-space methods, modular design structure, structured algorithm development, Software libraries, model checking tool, embedded software, proof manager, trace display, SLANG, hierarchical modeling framework, computer aided software engineering, state-space exploration, assume-guarantee reasoning]
A scenario-driven approach to traceability
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Design traceability has been widely recognized as being an integral aspect of software development. In the past years this fact has been amplified due to the increased use of legacy systems and COTS (commercial-off-the-shelf) components mixed with the growing use of elaborate "upstream" software modeling techniques such as the Unified Modeling Language (UML). The more intensive emphasis on upstream (non-programming) software development issues has, however, widened the gap between software components (e.g., subsystems, modules) and software models (e.g., class diagrams, data flow diagrams), creating the need for a better understanding of the intricacies and interrelationships between the two. This paper demonstrates how observable run-time information of software systems can be used to detect traceability information between software systems and their models. We do this by employing a technique that evaluates the "footprints" that usage scenarios (e.g., test cases) make during the execution of software systems. Those footprints can be compared, resulting in additional traceability information among modeling elements associated with those scenarios. Our approach is tool supported.
[Software testing, System testing, Dictionaries, scenario-driven approach, commercial-off-the-shelf, program testing, software development, Unified Modeling Language, Unified modeling language, data flow diagrams, Programming, traceability, Stress, Information analysis, Runtime, Software design, specification languages, software models, software modeling, Software systems, design traceability, software engineering, traceability information]
An architecture for heterogeneous groupware applications
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
The proliferation of wireless networks and small portable computing devices raises the need for applications that are adaptable to heterogeneous computing and communication environments and the contexts in which they are used. However, most current groupware systems as well as other software applications are not well prepared to handle the heterogeneity. The Manifold framework presented provides a software architecture for synchronous groupware applications to deal with heterogeneity. The framework's main characteristic is data centricity. The users collaborate on and exchange data, and the data is dynamically transformed to adapt to the particular computing/network platform. The design is based on a multi-tier architecture and uses eXtensible Markup Language (XML) as a generic means for information exchange. The resulting design is simple yet very powerful and scalable. Manifold is implemented and tested by developing several complex groupware applications.
[Context, Portable computers, Collaborative software, synchronous groupware applications, distributed processing, wireless networks, Application software, heterogeneous groupware applications, small portable computing devices, software architecture, Software architecture, multi-tier architecture, Wireless networks, Manifold framework, XML, Computer architecture, groupware, Collaborative work, Computer networks, user collaboration, data centricity, hypermedia markup languages, eXtensible Markup Language]
The coming-of-age of software architecture research
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Over the past decade, software architecture research has emerged as the principled study of the overall structure of software systems, especially the relations among subsystems and components. From its roots in qualitative descriptions of useful system organizations, software architecture has matured to encompass broad explorations of notations, tools, and analysis techniques. Whereas initially the research area interpreted software practice, it now offers concrete guidance for complex software design and development. We can understand the evolution and prospects of software architecture research by examining the research paradigms used to establish its results. These are, for the most part, the paradigms of software engineering. We advance our fundamental understanding by posing research questions of several kinds and applying appropriate research techniques, which differ from one type of problem to another, yield correspondingly different kinds of results, and require different methods of validation. Unfortunately, these paradigms are not recognized explicitly and are often not carried out correctly; indeed not all are consistently accepted as valid. This retrospective on a decade-plus of software architecture research examines the maturation of the software architecture research area by tracing the types of research questions and techniques used at various stages. We will see how early qualitative results set the stage for later precision, formality, and automation and how results build up over time. This generates advice to the field and projections about future impact.
[research paradigms, qualitative results, software systems, future impact, Machinery, subsystems, software architecture, complex software design, Software design, Software architecture, qualitative descriptions, research maturation, software architecture research, software practice, Automation, project management, research initiatives, principled study, system organizations, retrospective, Computer science, Computer languages, software engineering paradigms, Software systems, Concrete, Software engineering]
Tool-supported program abstraction for finite-state verification
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Numerous researchers have reported success in reasoning about properties of small programs using finite-state verification techniques. We believe, as do most researchers in this area, that in order to scale those initial successes to realistic programs, aggressive abstraction of program data will be necessary. Furthermore, we believe that to make abstraction-based verification usable by non-experts significant tool support will be required. In this paper we describe how several different program analysis and transformation techniques are integrated into the Bandera toolset to provide facilities for abstracting Java programs to produce compact, finite-state models that are amenable to verification for example via model checking. We illustrate the application of Bandera's abstraction facilities to analyze a realistic multi-threaded Java program.
[Java, program verification, program data, abstraction-based verification, Object oriented modeling, NASA, multi-threaded Java program, reasoning, Aerospace electronics, finite-state verification, finite state machines, Sun, tool-supported program abstraction, program analysis and transformation, Bandera toolset, model checking, USA Councils, Hardware, Logic, aggressive abstraction, Java programs, Contracts, Formal verification]
An empirical study of global software development: distance and speed
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Global software development is rapidly becoming the norm for technology companies. Previous qualitative research suggests that multi-site development may increase the development cycle time. We use both survey data and data from the source code change management system to model the extent of delay in a multi-site software development organization, and explore several possible mechanisms for this delay. We also measure differences in same-site and cross-site communication patterns, and analyze the relationship of these variables to delay. Our results show that, compared to same-site work, cross-site work takes much longer and requires more people for work of equal size and complexity. We also report a strong relationship between delay in cross-site work and the degree to which remote colleagues are perceived to help out when workloads are heavy. We discuss the implications of our findings for collaboration technology for distributed software development.
[delay mechanisms, distance, Programming, human resource management, collaboration technology, development cycle time, Delay, speed, global software development, Engineering management, multi-site software development organization, awareness, informal communication, remote colleagues, Pattern analysis, Collaborative software, heavy workloads, software development management, source code change management system, International collaboration, distributed software development, cross-site communication patterns, Software development management, same-site communication patterns, technology companies, delays, Collaborative work, survey, Continents, perceived help, Software engineering]
Case study: extreme programming in a university environment
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Extreme programming (XP) is a new and controversial software process for small teams. A practical training course at the University of Karlsruhe led to the following observations about the key practices of XP. First, it is unclear how to reap the potential benefits of pair programming, although pair programming produces high-quality code. Second, designing in small increments appears to be problematic but ensures rapid feedback about the code. Third, while automated testing is helpful, writing test cases before coding is a challenge. Last, it is difficult to implement XP without coaching. This paper also provides some guidelines for those starting out with XP.
[training course, Computer aided software engineering, program testing, high-quality code, test case writing, Programming, training, automated testing, Guidelines, extreme programming, feedback, pair programming, Feedback, coaching, programming, Karlsruhe University, computer science education, small teams, Documentation, Inspection, case study, Computer science, Automatic testing, educational courses, Writing, incremental design, Software engineering, software process]
Generating wrappers for command line programs: the Cal-Aggie Wrap-O-Matic project
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Software developers writing new software have strong incentives to make their products compliant to standards such as CORBA, COM, and Java Beans. Standards compliance facilitates interoperability, component based software assembly, and software reuse, thus leading to improved quality and productivity. Legacy software, on the other hand, is usually monolithic and hard to maintain and adapt. Many organizations, saddled with entrenched legacy software, are confronted with the need to integrate legacy assets into more modern, distributed, componentized systems that provide critical business services. Thus, wrapping legacy systems for interoperability has been an area of considerable interest. Wrappers are usually constructed by hand which can be costly and error-prone. We specifically target command-line oriented legacy systems and describe a tool framework that automates away some of the drudgery of constructing wrappers for these systems. We describe the Cal-Aggie Wrap-O-Matic system (CAWOM), and illustrate its use to create CORBA wrappers for: a) the JDB debugger, thus supporting distributed debugging using other CORBA components; and b) the Apache Web server, thus allowing remote Web server administration, potentially mediated by CORBA-compliant security services. While CORBA has some limitations, in several relatively common settings it can produce better wrappers at lower cost.
[Software maintenance, program debugging, open systems, JDB debugger, software developers, critical business services, legacy assets, CORBA components, Apache Web server, component based software assembly, file servers, command-line oriented legacy systems, remote Web server administration, CAWOM, wrapper generation, Java Beans, Software standards, Standards development, Web server, Assembly, distributed object management, distributed componentized systems, Productivity, COM, Java, automatic programming, software reuse, standards compliance, interoperability, software maintenance, Wrapping, legacy software, CORBA-compliant security services, Cal-Aggie Wrap-O-Matic system, Software quality, Writing, CORBA wrappers, Internet, Cal-Aggie Wrap-O-Matic project, tool framework, distributed debugging, command line programs]
Traceability for system families
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
System families are an idea of software reuse in a specific problem domain. Existing methods have little requirements engineering support for system family development. This paper proposes a requirements meta-model for system family development. Traceability throughout the model elements is a necessary pre-condition for preserving the consistency of the complete family model during development and is a main issue in this paper as well as for software development in general. Family development based on the meta-model guarantees traceability by the inclusion of all development artifacts in a single and consistent model.
[software development artifacts, software reuse, Object oriented modeling, Unified modeling language, Programming, World Wide Web, traceability, configuration management, system family development, systems analysis, problem domains, Computer architecture, software reusability, Systems engineering and theory, Software systems, Manufacturing, model consistency preservation, Iterative methods, requirements engineering support, Contracts]
Evaluating the accuracy of defect estimation models based on inspection data from two inspection cycles
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Defect content estimation techniques (DCETs), based on defect data from inspection, estimate the total number of defects in a document to evaluate the development process. For inspections that yield few data points DCETs reportedly underestimate the number of defects. If there is a second inspection cycle, the additional defect data is expected to increase estimation accuracy. In this paper we consider 3 scenarios to combine data sets from the inspection-reinspection process. We evaluate these approaches with data from an experiment in a university environment where 31 teams inspected and reinspected a software requirements document. Main findings of the experiment were that reinspection data improved estimation accuracy. With the best combination approach all examined estimators yielded on average estimates within 20% around the true value, all estimates stayed within 40% around the true value.
[Decision support systems, project management, Biological system modeling, software development management, Inspection, inspection cycles, Yield estimation, Statistics, software requirements, defect estimation models, defect data, inspection data, Quality assurance, Animals, Software quality, Chromium, defect content estimation techniques, university environment, Context modeling, inspection]
Applying WinWin to quality requirements: a case study
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Describes the application of the WinWin paradigm to identify and resolve conflicts in a series of real-client, student-developer digital library projects. The paper is based on a case study of the statistical analysis of 15 projects and an in-depth analysis of one representative project. These analyses focus on the conflict resolution process, stakeholders' roles and their relationships to quality artifacts, and tool effectiveness. We show that stakeholders tend to accept satisfactory rather than optimal resolutions. Users and customers are more proactive in stating win conditions, whereas developers are more active in working toward resolutions. Further, we suggest that knowledge-based automated aids have potential to significantly enhance process effectiveness and efficiency. Finally, we conclude that such processes and tools have theoretical and practical implications in the quest for better software requirements elicitation.
[Computer aided software engineering, Costs, WinWin paradigm, software quality, acceptability, Degradation, conflict resolution, digital library projects, knowledge-based automated aids, knowledge based systems, proactive users, software process effectiveness, software process improvement, software tools, quality artifacts, software quality requirements, software requirements elicitation, software process efficiency, software cost analysis, Statistical analysis, digital libraries, Application software, student software developers, case study, stakeholder roles, Computer science, software tool effectiveness, Software libraries, requirements engineering, software quality attributes, proactive customers, conflict identification, Software quality, computer aided software engineering, risk, win conditions, software cost estimation, statistical analysis, Software tools, Software engineering]
Functional paleontology: system evolution as the user sees it
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
It has long been accepted that requirements analysis should precede architectural design and implementation, but in software evolution and reverse engineering this concern with black-box analysis of function has necessarily been de-emphasized in favor of code-based analysis and designer-oriented interpretation. We redress this balance by describing 'functional paleontology': an approach to analyzing the evolution of user-visible features or services independent of architecture and design intent. We classify the benefits and burdens of interpersonal communication services into core and peripheral categories and investigate the telephony services available to domestic subscribers over a fifty-year period. We report that services were introduced in discrete bursts, each of which emphasized different benefits and burdens. We discuss the general patterns of functional evolution that this "fossil record" illustrates and conclude by discussing their implications for forward engineering of software products.
[software products, Reverse engineering, Anthropometry, Programming, functional evolution, formal specification, software evolution, Design engineering, Computer architecture, Telephony, software engineering, Software measurement, user-visible features, forward engineering, functional paleontology, telephony services, Educational institutions, reverse engineering, code-based analysis, interpersonal communication services, software maintenance, HCI, Human computer interaction, requirements analysis, human-computer interaction, requirements engineering, systems analysis, designer-oriented interpretation, Software systems, system evolution, software metrics]
Conceptual modeling through linguistic analysis using LIDA
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Despite the advantages that object technology can provide to the software development community and its customers, the fundamental problems associated with identifying objects, their attributes, and methods remain: it is a largely manual process driven by heuristics that analysts acquire through experience. While a number of methods exist for requirements development and specification, very few tools exist to assist analysts in making the transition from textual descriptions to other notations for object-oriented analysis and other conceptual models. We describe a methodology and a prototype tool. Linguistic Assistant for Domain Analysis (LIDA), which provide linguistic assistance in the model development process. We first present our methodology to conceptual modeling through linguistic analysis. We give an overview of LIDA's functionality and present its technical design and the functionality of its components. We also provide a comparison of LIDA's functionality with that of other research prototypes. Finally, we present an example of how LIDA is used in a conceptual modeling task.
[Computer aided software engineering, Unified modeling language, textual descriptions, Manuals, Programming, conceptual models, object technology, formal specification, Information analysis, Cyclic redundancy check, heuristics, model development process, Prototypes, LIDA, object-oriented methods, software tools, Books, linguistic assistance, software development community, Object oriented modeling, linguistic analysis, specification, object-oriented analysis, requirements development, Software tools, Linguistic Assistant for Domain Analysis, conceptual modeling]
Consistent group membership in ad hoc networks
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
The design of ad hoc mobile applications often requires the availability of a consistent view of the application state among the participating hosts. Such views are important because they simplify both the programming and verification tasks. Essential to constructing a consistent view is the ability to know what hosts are within proximity of each other, i.e., form a group in support of the particular application. We propose an algorithm that allows hosts within communication range to maintain a consistent view of the group membership despite movement and frequent disconnections. The novel features of this algorithm are its reliance on location information and a conservative notion of logical connectivity that creates the illusion of announced disconnection. Movement patterns and delays are factored in the policy that determines which physical connections are susceptible to disconnection.
[ad hoc mobile applications, consistent group membership, location information, logical connectivity, Mobile communication, Multicast protocols, Data structures, group membership, Ad hoc networks, conservative notion, announced disconnection, Application software, Programming profession, consistent view, configuration management, Intelligent networks, mobile computing, Broadcasting, Space exploration, ad hoc networks, Mobile computing, distributed programming, programming, verification]
The right algorithm at the right time: comparing data flow analysis algorithms for finite state verification
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Finite-state verification is emerging as an important technology for proving properties about software. In our experience, we have found that analysts have different expectations at different times. When an analyst is in an exploratory mode, initially formulating and verifying properties, analyses usually find inconsistencies because of flaws in the properties or in the software artifacts being analyzed. Once an inconsistency is found, the analyst begins to operate in a fault-finding mode, during which meaningful counter-example traces are needed to help determine the cause of the inconsistency. Eventually, systems become relatively stable, but still require re-verification as evolution occurs. During such periods, the analyst is operating in a maintenance mode and would expect re-verification to usually report consistent results. Although it could be that one algorithm suits all three of these modes of use, the hypothesis explored in this paper is that each would be best served by an algorithm optimized for the expectations of the analyst.
[Algorithm design and analysis, program verification, maintenance mode, software verification, fault-finding mode, Laboratories, human factors, counter-example traces, data flow analysis algorithms, finite state machines, Engines, Counting circuits, software evolution, inconsistencies, exploratory mode, software artifacts, analyst expectations, software properties, system stability, Data analysis, Software algorithms, Debugging, data flow analysis, finite-state verification, software maintenance, Computer science, reverification, Software systems, Software engineering]
Using software component generators to construct a meta-weaver framework
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Several new modularity technologies have been proposed that improve separation of concerns in programming languages. The initial efforts to demonstrate these technologies are usually focused on a single programming language. Since we live in a polyglot world, this proposal addresses the goal of being able to take these new powerful technologies to other languages. The approach uses software generators that create new "weavers" from meta-specifications of programming languages.
[Java, aspect oriented programming, Merging, high level languages, software component generators, Proposals, programming languages, Information technology, program compilers, formal specification, Computer languages, Intersymbol interference, Embedded system, polyglot world, Software systems, program compiler, Books, Contracts, meta-weaver framework, modularity technology, meta-specifications]
Survivability analysis of networked systems
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Survivability is the ability of a system to continue operating despite the presence of abnormal events such as failures and intrusions. Ensuring system survivability has increased in importance as critical infrastructures have become heavily dependent on computers. We present a systematic method for performing survivability analysis of networked systems. An architect injects failure and intrusion events into a system model and then visualizes the effects of the injected events in the form of scenario graphs. Our method enables further global analyses, such as reliability, latency, and cost-benefit analyses, where mathematical techniques used in different domains are combined in a systematic manner. We illustrate our ideas on an abstract model of the United States Payment System.
[Visualization, systematic method, software reliability, graph theory, Humans, abnormal events, survivability analysis, Telecommunication computing, networked systems, Delay, abstract model, cost-benefit analysis, injected events, cost-benefit analyses, scenario graphs, Pervasive computing, Automation, Finance, computer networks, Banking, Computer crashes, system model, Power system modeling, United States Payment System, mathematical techniques, global analyses, critical infrastructures, system survivability, bank data processing]
Maintenance support tools for Java programs: CCFinder and JAAT
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
This paper describes two software maintenance support tools, CCFinder (Code Clone Finder) and JAAT (Java Alias Analysis Tool), for Java programs. CCFinder identifies code clones in Java programs, while JAAT executes alias analysis for Java programs.
[Algorithm design and analysis, Java, Software maintenance, software maintenance support tools, Cloning, Maintenance engineering, JAAT, software maintenance, code clone identification, Information science, CCFinder, Software systems, Computer industry, Large-scale systems, software tools, alias analysis, Detection algorithms, Java programs]
A Web-oriented architectural aspect for the emerging computational tapestry
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
An emerging tapestry of computations will soon integrate systems around the globe. It will evolve without central control. Its complexity will be vast. We need new ideas, tools and methods to help map, understand and manage this tapestry. We contribute a light-weight architectural aspect that designers can use without compromising their own architectural preferences. Widespread use could help. The idea is for objects to provide Web-based interfaces to object-specific meta-data, state, and monitoring and control services. We discuss applications, implementation, scalability, performance, tradeoffs, and related work.
[information resources, Costs, Scalability, Communication system control, Web-oriented architecture, Application software, Web-based interfaces, scalability, Centralized control, Uniform resource locators, Computer science, software architecture, computational tapestry, Runtime, Communication system software, performance, object-specific meta-data, Internet, Monitoring]
A workbench for synthesising behaviour models from scenarios
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Scenario-based specifications such as Message Sequence Charts (MSCs) are becoming increasingly popular as part of a requirements specification. Our objective is to facilitate the development of behaviour models in conjunction with scenarios. In this paper, we first present an MSC language with semantics in terms of labelled transition systems and parallel composition. The language integrates existing languages based on the use of high-level MSCs (hMSCs) and on identifying component states. This integration allows stakeholders to break up scenario specifications into manageable parts using hMCSs and to explicitly introduce additional information and domain-specific or other assumptions using state labels. Secondly, we present an algorithm, implemented in Java, which translates scenarios into a specification in the form of Finite Sequential Processes. This can then be fed to the labelled transition system analyser for model checking and animation. Finally we show how many of the assumptions embedded in existing synthesis approaches can be translated into our approach. Thus we provide the basis of a common workbench for supporting MSC specifications, behaviour synthesis and analysis.
[Java, message sequence charts, behaviour models, Unified modeling language, Educational institutions, semantics, labelled transition systems, formal specification, behaviour models synthesis, animation, computer animation, component states, model checking, behaviour synthesis, MSC language, requirements specification, parallel composition, scenario-based specifications, Animation, labelled transition system analyser, workbench, state labels, Software engineering]
Theory of software reliability based on components
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
We present a foundational theory of software system reliability based on components. The theory describes how component developers can design and test their components to produce measurements that are later used by system designers to calculate composite system reliability, without implementation and test of the system being designed. The theory describes how to make component measurements that are independent of operational profiles, and how to incorporate the overall system-level operational profile into the system reliability calculations. In principle, the theory resolves the central problem of assessing a component, which is: a component developer cannot know how the component will be used and so cannot certify it for an arbitrary use; but if the component buyer must certify each component before using it, component based development loses much of its appeal. This dilemma is resolved if the component developer does the certification and provides the results in such a way that the component buyer can factor in the usage information later without repeating the certification. Our theory addresses the basic technical problems inherent in certifying components to be released for later use in an arbitrary system. Most component research has been directed at functional specification of software components; our theory addresses the other equally important side of the coin: component quality.
[Software testing, System testing, software reliability, arbitrary system, software quality, Software design, component based development, operational profiles, component developers, component quality, component developer, component buyer, Software standards, system designers, object-oriented programming, component measurements, Interconnected systems, system-level operational profile, Reliability theory, usage information, composite system reliability, Software reliability, Application software, certification, functional specification, software reliability theory, software system reliability, system reliability calculations, component research, Software quality, Software systems, software metrics]
Designing components versus objects: a transformational approach
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
A good object oriented design does not necessarily make a good component based design, and vice versa. What design principles do components introduce? The paper examines component based programming and how it expands the design space in the context of an event based component architecture. We present a conceptual model for addressing new design issues these components afford, and we identify fundamental design decisions in this model that are not a concern in conventional object oriented design. We use JavaBeans based examples to illustrate concretely how expertise in component based design, as embodied in a component taxonomy and implementation space, impacts both design and the process of design. The results are not exclusive to JavaBeans; they can apply to any comparable component architecture.
[Process design, transformational approach, Taxonomy, object oriented design, software quality, event based component architecture, Boolean functions, distributed object management, Java, object-oriented programming, Object oriented modeling, JavaBeans based examples, fundamental design decisions, Educational institutions, component architecture, component based programming, Connectors, Computer science, Component architectures, component based design, design issues, conceptual model, component taxonomy, implementation space, design principles, design space, Software engineering]
Model processing tools in UML
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
The Unified Modeling Language (UML) provides several diagram types, viewing a system from different perspectives. In this research, we exploit the logical relationships between different UML models. We propose operations to compare, merge, slice and synthesize UML diagrams based on these relationships. In a formal demonstration, we show how statechart diagrams can be synthesized semi-automatically from a set of sequence diagrams using an interactive algorithm called MAS. We also demonstrate how a class diagram, annotated with pseudocode presentations of key operations, can be synthesized from sequence diagrams, and how class diagrams and sequence diagrams can be sliced against each other.
[Process design, pseudocode presentations, Unified modeling language, Laboratories, Programming, diagrams, sequences, specification languages, Software standards, Standards development, statechart diagrams, Unified Modeling Language, sequence diagrams, Object oriented modeling, diagram types, logical relationships, model processing tools, diagram slicing, diagram merging, interactive algorithm, UML, diagram comparison, Software systems, Computer industry, Inference algorithms, annotated class diagram, diagram synthesis, MAS]
A general framework for formalizing UML with formal languages
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Informal and graphical modeling techniques enable developers to construct abstract representations of systems. Object-oriented modeling techniques further facilitate the development process. The Unified Modeling Language (UML), an object-oriented modeling approach, could be broad enough in scope to represent a variety of domains and gain widespread use. Currently, UML comprises several different notations with no formal semantics attached to the individual diagrams. Therefore, it is not possible to apply rigorous automated analysis or to execute a UML model in order to test its behavior: short of writing code and performing exhaustive testing. We introduce a general framework for formalizing a subset of UML diagrams in terms of different formal languages based on a homomorphic mapping between meta models describing UML and the formal language. This framework enables the construction of a consistent set of rules for transforming UML models into specifications in the formal language. The resulting specifications derived from UML diagrams enable either execution through simulation or analysis through model checking, using existing tools. This paper describes the use of this framework for formalisms UML to model and analyze embedded systems. A prototype system for generating the formal specifications and results from an industrial case study are also described.
[Performance evaluation, rigorous automated analysis, homomorphic mapping, Unified modeling language, Formal languages, formal specification, object-oriented modeling, specifications, Analytical models, formal semantics, Embedded system, Prototypes, embedded systems, specification languages, industrial case study, Performance analysis, object-oriented methods, formal languages, Unified Modeling Language, Object oriented modeling, meta models, formal specifications, model checking, Automatic testing, UML, abstract representations, Writing]
MAS - an interactive synthesizer to support behavioral modeling in UML
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
The Minimally Adequate Synthesizer (MAS) is an interactive algorithm that synthesizes UML statechart diagrams from sequence diagrams. It follows D. Angluin's (1987) framework of a minimally adequate teacher to infer the desired statechart diagram by consulting the user. To minimize the number of consultations needed, MAS keeps track of the interaction with the user. Together with its general knowledge about sequence diagrams, this makes MAS operate mostly without the user's help, but allowing him to guide the process at critical points. In this paper, we discuss the MAS algorithm and its practical implementation, integrated with a real-world UML modeling tool, the Nokia TDE ("The Design Environment"). Moreover, we discuss the interaction between the algorithm and the user as a medium for improving the algorithm and for further reducing the number of user consultations needed. Furthermore, we show how MAS can be used to incrementally synthesize sequence diagrams into an edited or a manually constructed statechart diagram. Totally automatic synthesis algorithms may result in a state machine that contains undesired generalizations. Because MAS consults the user during the synthesis process, the user can be confident that such generalizations do not appear in the resulting statechart diagram.
[Protocols, Synthesizers, Unified modeling language, Laboratories, algorithm-user interaction, diagrams, sequences, specification languages, interactive systems, Software standards, Minimally Adequate Synthesizer, minimally adequate teacher, sequence diagrams, Object oriented modeling, user interaction tracking, user consultation, Automata, UML statechart diagram synthesis, Software systems, Computer industry, Inference algorithms, behavioral modeling, Nokia TDE, MAS, state machine generalizations]
Static checking of interrupt-driven software
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Resource-constrained devices are becoming ubiquitous. Examples include cell phones, Palm Pilots and digital thermostats. It can be difficult to fit the required functionality into such a device without sacrificing the simplicity and clarity of the software. Increasingly complex embedded systems require extensive brute-force testing, making development and maintenance costly. This is particularly true for system components that are written in assembly language. Static checking has the potential of alleviating these problems, but until now there has been little tool support for programming at the assembly level. In this paper, we present the design and implementation of a static checker for interrupt-driven Z86-based software with hard real-time requirements. For six commercial microcontrollers, our checker has produced upper bounds on interrupt latencies and stack sizes, as well as verified fundamental safety and liveness properties. Our approach is based on a known algorithm for the model checking of pushdown systems and produces a control-flow graph annotated with information about time, space, safety and liveness. Each benchmark is approximately 1000 lines of code, and the checking is done in a few seconds on a standard PC. Our tool is one of the first to give an efficient and useful static analysis of assembly code. It enables increased confidence in code correctness, significantly reduced testing requirements and support for maintenance throughout the system life-cycle.
[assembly language, System testing, Assembly systems, program verification, software tool support, hard real-time requirements, software maintenance cost, interrupt latencies, Delay, device functionality, microprogramming, embedded systems, cellular telephones, interrupts, software tools, stack sizes, microcontrollers, static analysis, software maintenance, annotated control-flow graph, personal computer, resource-constrained devices, Cellular phones, Z86-based software, Palm Pilots, software development cost, program testing requirements, program testing, microcomputer applications, digital thermostats, complex embedded systems, Embedded system, liveness properties, Benchmark testing, Safety, static checking, system lifecycle, Microcontrollers, program diagnostics, pushdown systems, flow graphs, brute-force testing, commercial microcontrollers, Upper bound, model checking, code correctness, safety properties, Thermostats, interrupt-driven software]
Supporting the deployment of object-oriented frameworks
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Frameworks are usually large and complex, and typically reusers need to understand them well enough to effectively use them. This research concentrates on verifying applications built on top of OO frameworks. The idea is to get framework builders to specify a set of constraints for the correct usage of the framework and check them using static analysis techniques.
[Protocols, object-oriented programming, program verification, software reuse, program diagnostics, static analysis, application verification, Specification languages, Prototypes, software reusability, object-oriented frameworks, Protection, Object oriented programming]
XAS: a system for accessing componentized, virtual XML documents
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
XML is emerging as an important format for describing the schema of documents and data to facilitate integration of applications in a variety of industry domains. An important issue that naturally arises is the requirement to generate, store and access XML documents. It is important to reuse existing data management systems and repositories for this purpose. We describe the XML Access Server (XAS), a general purpose XML based storage and retrieval system which provides the appearance of a large set of XML documents while retaining the data in underlying federated data sources that could be relational, object-oriented, or semi-structured. XAS automatically maps the underlying data into virtual XML components when mappings between DTDs and underlying schemas are established. The components can be presented as XML documents or assembled into larger components. XAS manages the relationship between XML components and the mapping in the form of document composition logic. The versatility in its ways to generate XML documents enables XAS to serve a large number of XML components and documents efficiently and expediently.
[XML Access Server, XAS system, XML based storage, Humans, data repository, distributed databases, Software agents, semi-structured database, Logic, Assembly, Monitoring, hypermedia markup languages, document handling, Content management, component-based virtual XML documents, object-oriented databases, federated data sources, Information retrieval, relational database, Application software, relational databases, object-oriented database, document composition logic, XML, Internet]
The specification and testing of quantified progress properties in distributed systems
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
There are two basic parts to the behavioral specification of distributed systems: safety and progress. In earlier work, we developed a tool to monitor progress properties of CORBA components specified using the temporal operator transient. In this paper, we address the specification and testing of transient properties that are quantified (over both bounded and unbounded domains). We categorize typical quantifications that arise in practical systems and discuss possible implementation strategies. We define functional transients, a subclass of quantified transient properties that can be monitored in constant space and time. We outline the design and implementation of a tool for testing these properties in CORBA components.
[Software testing, System testing, Java, functional transients, program testing, Computerized monitoring, testing, transient properties, specification, CORBA components, Distributed computing, formal specification, behavioral specification, implementation strategies, Concurrent computing, Information science, quantified progress properties, Prototypes, safety, distributed systems, Safety, temporal operator transient, Transient analysis, distributed object management]
Exploiting the map metaphor in a tool for software evolution
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Software maintenance and evolution are the dominant activities in the software lifecycle. Modularization can separate design decisions and allow them to be independently revolved, but modularization often breaks down and complicated global changes are required. Tool support can reduce the costs of these unfortunate changes, but current tools are limited in their ability to manage information for large-scale software evolution. We argue that the map metaphor can serve as an organizing principle for the design of effective tools for performing global software changes. We describe the design of Aspect Browser, developed around the map metaphor, and discuss a case study of removing a feature from a 500000 line program written in Fortran and C.
[Software maintenance, Costs, C, software prototyping, Aspect Browser, software lifecycle, organizing principle, map metaphor, Information management, software evolution tool, Fortran, design decisions, Large-scale systems, software tools, global software changes, software maintenance, Programming profession, Organizing, case study, Computer science, tool support, Software systems, computer aided software engineering, Software tools, Pattern matching, large-scale software evolution, global changes]
Hyper/J/sup TM/: multi-dimensional separation of concerns for Java/sup TM/
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Hyper/J supports a new approach to constructing, integrating and evolving software, called multi-dimensional separation of concerns. Developers can decompose and organize code and other artifacts according to multiple arbitrary criteria (concerns) simultaneously - even after the software has been implemented - and synthesize or integrate the pieces into larger-scale components and systems. Hyper/J facilitates several common development and evolution activities non-invasively, including: adaptation and customization, mix-and-match of features, reconciliation and integration of multiple domain models, reuse, product line management, extraction or replacement of existing parts of software, and on-demand remodularization. Hyper/J works with standard Java software, not requiring special compilers or environments. This demonstration shows it in action in a number of software engineering scenarios at different stages of the software life-cycle.
[Encapsulation, software life-cycle, Additives, software engineering scenarios, customization, domain model integration, adaptation, Software safety, code decomposition, code organization, software evolution, Hyper/J, software construction, software artifacts, arbitrary criteria, on-demand remodularization, feature mix-and-match, software component replacement, Object oriented programming, domain model reconciliation, Java, software reuse, multi-dimensional separation of concerns, software component extraction, software maintenance, Organizing, Software development management, software integration, larger-scale components, integrated software, software reusability, computer aided software engineering, subroutines, product line management, Software engineering]
Model checking distributed objects design
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Potential advantages brought about by distributed system architectures has given rise to the number of applications being based around it. Advantages include an increase in fault tolerance due to replicated components and achieving cost effective scalability by distributing the execution of a task over several relatively cheap hosts rather than a central mainframe. The construction of distributed applications via the direct use of network operating system primitives is no longer feasible and middleware technologies are fast becoming the alternative approach. We note that amongst the different form of middleware, distributed object middleware offers the richest support to application designers and incorporates primitives for distributed transaction management and asynchronous message passing. From the set of distributed object middleware approaches, we concentrate on CORBA because it offers the richest set of synchronization and threading primitives.
[Computer aided software engineering, Scalability, Unified modeling language, scalability, Algebra, asynchronous message passing, distributed transaction management, distributed object management, middleware, threading primitives, message passing, fault tolerance, Object oriented modeling, distributed object design, Educational institutions, Application software, Middleware, Power system modeling, synchronisation, distributed system architectures, Computer science, CORBA, model checking, synchronization, replicated components]
Visualization and interpretation of analysis results within the context of formalized UML diagrams
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
This project complements and extends previous work that has focused on attaching formal semantics to informal graphical object-oriented modeling notations in order to automatically generate formal specifications for a number of target languages (Bourdeau and Cheng, 1995; Wang et al., 1997). My research (Campbell and Cheng, 2000; Campbell et al., 2000; Cheng et al., 2000) builds upon the informal and formal integration work by investigating how commonly used automated analysis techniques, such as simulators, model checkers, rewriting systems, and theorem provers can be combined and used in tandem. The main artifacts that are being analyzed are the formal specifications generated from the informal diagrams. The automated analysis enables a developer to check the system design for various properties, such as freedom from deadlock or constraint satisfaction (i.e, specific conditions are satisfied). The formal semantics for the diagrams also makes it possible to execute the graphical models, via the formal specifications, in order to validate the behavior of the system design.
[Visualization, Unified modeling language, UML diagrams, diagrams, deadlock, formal specification, Information analysis, Analytical models, formal semantics, Feedback, constraint satisfaction, Prototypes, specification languages, theorem provers, object-oriented methods, rewriting systems, automated analysis techniques, Object oriented modeling, Formal specifications, formal specifications, simulators, Computer science, model checkers, object-oriented modeling notations, Joining processes]
Using the Web for document versioning: an implementation report for DeltaV
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
The current suite of systems that offer client/server capabilities for document versioning relies on proprietary protocols for communicating between a central versioning repository and a remote client. In order to support better document authoring via the Web, the DeltaV working group of the WebDAV (WWW Distributed Authoring and Versioning) project of the Internet Engineering Task Force is working on a standard protocol for versioning over HTTP. The authors present a prototype of DeltaV based on the 04.5 draft. This system demonstrates that, though important aspects of the protocol need to be revised, versioning via the Web can be a practical means of supporting remote access to a central versioning repository.
[hypermedia, World Wide Web, central versioning repository, Design engineering, WebDAV project, Prototypes, protocols, Web server, information resources, document handling, client-server systems, Globalization, client server system, Access protocols, Web document versioning, HTTP, Internet Engineering Task Force, transport protocols, DeltaV, XML, User interfaces, Internet, Web sites, document authoring, remote client]
Separating features in source code: an exploratory study
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Most software systems are inflexible. Reconfiguring a system's modules to add or to delete a feature requires substantial effort. This inflexibility increases the costs of building variants of a system, amongst other problems. New languages and tools that are being developed to provide additional support for separating concerns show promise to help address this problem. However applying these mechanisms requires determining how to enable a feature to be separated from the codebase. We investigate this problem through an exploratory study conducted in the context of two existing systems: gnu.regexp and jFTPd. The study consisted of applying three different separation of concern mechanisms: Hyper/J/sup TM/ AspectJ/sup TM/ and a lightweight, lexically-based approach, to separate features in the two packages. We report on the study, providing contributions in two areas. First, we characterize the effect different mechanisms had on the structure of the codebase. Second, we characterize the restructuring process required to perform the separations. These characterizations can help researchers to elucidate how the mechanisms may be best used, tool developers to design support to aid the separation process, and early adopters to apply the techniques.
[Costs, exploratory study, Delay systems, software systems, Time to market, Separation processes, system module reconfiguration, lexically based approach, Hyper/J, software tools, AspectJ, early adopters, Java, object-oriented programming, Delay effects, source code, tool developers, software maintenance, Computer science, separation of concern mechanisms, restructuring process, codebase, Packaging, Software systems, jFTPd, feature separation, separation process]
Web engineering device independent Web services
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Today's Web services not only have to be flexible, but also have to be device independent to support mobile devices such as WAP and PDAs. Supporting multiple Web formats (e.g., WML, HTML, etc.) is still an open challenge. Most sites have to provide a separate application for every format and reuse is not common. We are working on a methodology and a tool to support the Web developer in building flexible, device independent Web services.
[information resources, Web engineering, PDAs, Scalability, Buildings, WAP, HTML, Web developer support, Web services, Databases, XML, systems analysis, WML, mobile devices, Page description languages, Internet, Personal communication networks, Personal digital assistants, Mobile computing, multiple Web formats, cellular radio, hypermedia markup languages, device independent Web services]
Comparing frameworks and layered refinement
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Object-oriented frameworks are a popular mechanism for building and evolving large applications and software product lines. We describe an alternative approach to software construction, Java Layers (JL), and evaluate JL and frameworks in terms of flexibility, ease of use, and support for evolution. Our experiment compares Schmidt's (1998) ACE framework against a set of ACE design patterns that have been implemented in JL. We show how problems of framework evolution and overfeaturing can be avoided using JL's component model, and we demonstrate that JL scales better than frameworks as the number of possible application features increases. Finally, we describe how constrained parametric polymorphism and a small number of language features can support JL's model of loosely coupled components and stepwise program refinement.
[JL, constrained parametric polymorphism, stepwise program refinement, loosely coupled components, Runtime, Java Layers, Measurement standards, experiment, layered refinement, design patterns, Computer architecture, software product lines, Java, object-oriented programming, ACE framework, Object oriented modeling, Buildings, framework evolution, component model, Application software, Computer languages, ease of use, software reusability, object-oriented frameworks, Usability]
Evolving legacy systems using feature engineering and CBSE
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
This dissertation explores the relationships between feature engineering, CBSE, and software evolution. Software end-users and developers have different perspectives of a software system, resulting in a complexity gap between user expectations and the software functionality. This gap together with aging code has resulted in lost assets for many organizations. By combining feature engineering and CBSE, legacy code can be modernized so that many organizations can benefit from this technique. In our approach, we identify the legacy system's features through test cases and test suites. We then apply our code carving techniques to identify the code associated with those features and extract the code to create components. We validate our results in two ways. First, by inserting the component(s) back into the legacy system to continue functioning. Second, by measuring the cost of adding a new feature after applying the methodology.
[Software testing, System testing, Software maintenance, Costs, program testing, software testing, Life testing, user expectations, CBSE, test suites, software evolution, systems re-engineering, software functionality, legacy systems, Aging, software reusability, feature engineering, Systems engineering and theory, Software systems, Feature extraction, Software engineering, component based software engineering]
An explorative journey from architectural tests definition downto code tests execution
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Connectors, System testing, Assembly systems, Software architecture, Object oriented modeling, Modems, Concrete, Power system modeling]
Tigra- an architectural style for enterprise application integration
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Computer science, Computer languages, Markup languages, Operating systems, XML, Optical wavelength conversion, Educational institutions, Hardware, Application software, Middleware]
ICSE workshop on software visualization
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
A formal approach to component-based software engineering: education and evaluation
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Summarizes an approach for introducing component-based software engineering (CBSE) early in the undergraduate computer science curriculum, and an evaluation of the impact of the approach at two institutions. Principles taught include a modular style of software development, an emphasis on human understanding of component behavior even while using formal specifications, and the importance of maintainability, as well as classical issues such as efficiency analysis and reasoning. Qualitative and quantitative evaluations of student outcomes and end-to-end changes in student attitudes show mostly positive results that are statistically significant, confirming that: (1) it is possible to teach CBSE principles without displacing "classical" principles usually taught in introductory courses, (2) students can understand and reuse formally specified components without knowing their implementations, and (3) student attitudes towards software engineering can be altered in directions heretofore often assumed to be difficult to achieve.
[Assembly systems, statistical significance, student outcomes, Humans, reasoning, qualitative evaluations, Programming, software maintainability, human understanding, formal specification, undergraduate computer science curriculum, quantitative evaluations, end-to-end student attitude changes, software engineering, Computer science education, Software reusability, computer science education, component-based software engineering, introductory courses, impact evaluation, efficiency analysis, Formal specifications, Statistics, formal specifications, Computer science, component behavior, Software systems, software engineering education, subroutines, statistical analysis, modular software development style, Software engineering]
Educating software engineering students to manage risk
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
In 1996, the University of Southern California (USC) switched its core two-semester software engineering course from a hypothetical-project, homework-and-exam course based on the Bloom taxonomy of educational objectives (knowledge, comprehension, application, analysis, synthesis and evaluation). The revised course is a real-client team-project course based on the CRESST (Center for Research on Evaluation, Standards and Student Testing) model of learning objectives (content understanding, problem solving, collaboration, communication and self-regulation). We used the CRESST cognitive demands analysis to determine the necessary student skills required for software risk management and the other major project activities, and have been refining the approach over the last four years of experience, including revised versions for one-semester undergraduate and graduate project courses at Columbia University. This paper summarizes our experiences in evolving the risk management aspects of the project courses. These have helped us mature more general techniques, such as risk-driven specifications, domain-specific simplifier and complicator lists, and the SAIV (schedule as an independent variable) process model. The largely positive results in terms of review pass/fail rates, client evaluations, product adoption rates and hiring manager feedback are summarized as well.
[SAIV process model, software engineering course, content understanding, review pass/fail rates, Communication standards, CRESST model, graduate project course, software engineering, communication, product adoption rates, risk management, computer science education, project management, real-client team-project course, software development management, property models, Risk analysis, Columbia University, educational courses, project activities, risk-driven specifications, Risk management, Problem-solving, Bloom taxonomy, success models, management education, software engineering students, Taxonomy, independent variable, client evaluations, student skills, Engineering management, product models, Southern California University, learning objectives, educational objectives, problem solving, cognitive demands analysis, Application software, software risk management education, schedule, process models, Automatic testing, Collaboration, collaboration, domain-specific simplifier lists, self-regulation, hiring manager feedback, undergraduate project course, domain-specific complicator lists, Software engineering]
Workshop on software engineering and mobility
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
Software engineering and the internet
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Service oriented architecture, Computer architecture, Capacitive sensors, Internet, Application software, Fuels, Middleware, Software tools, Artificial intelligence, Software engineering]
Specification and modeling: an industrial perspective
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Costs, Humans, Switches, Programming, Computer industry, Software systems, Mathematics, Mathematical model, Industrial accidents, Software engineering]
From software requirements to architectures
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Software architecture, Buildings, Computer architecture, Educational institutions, Software systems, Safety, Security, Software tools, Software engineering, Context modeling]
4/sup th/ ICSE workshop on "software engineering over the internet"
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
An efficient set of software degree programs for one domain
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
There is increasing urgency to put software engineering (SE) programs in place at universities in North America. For years, the computer science and professional engineering communities neglected the area, but both are now paying serious attention. There is creative tension as efforts accelerate to define the field faster than is possible. This paper discusses a set of four software degree programs that have evolved over 14 years at a small university with close ties to one software community. The context is computer engineering in a department of electrical and computer engineering, so the natural domain is software that is close to the hardware. This means an emphasis on real-time, embedded, and, to a lesser extent, safety critical issues. The newest of the four programs is a Ph.D. program. It demonstrates that Ph.D. programs can be created with limited resources, given the right circumstances. If similar circumstances exist in other small universities, the rate of Ph.D. production in software engineering may be able to be increased, while maintaining quality. This paper describes the four degree programs, how they are related to each other, and how the programs have evolved. It makes limited comparisons to programs at other universities.
[computer science education, Military computing, software degree programs, Maintenance engineering, Educational institutions, North America, Computer science, software community, PhD programs, educational courses, Production, university education, embedded system, software engineering education, safety critical system, Hardware, software engineering, Safety, Acceleration, real-time system, Software engineering]
Systematic object-oriented inspection - an empirical study
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Computer science, Poles and towers, Feedback, Software performance, Inspection, Libraries]
The coming-of-age of software architecture research
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Computer languages, Software design, Automation, Software architecture, Software systems, Concrete, Commercialization, Software engineering]
Workshop to consider global aspects of software engineering professionalism
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
The second international workshop on automated program analysis, testing and verification
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
David L. Parnas symposium
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
Academic software engineering: what is and what could be? Results of the first annual survey for international SE programs
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
According to data received from an international survey, almost 6800 students are enrolled in software engineering degree programs in 11 countries, as of January, 2001. A total of 94 academic programs in software engineering are in place at 60 universities with 350 full-time faculty and nearly 200 part-time faculty teaching hundreds of undergraduate and graduate courses in the discipline. Over 5500 people have obtained degrees in software engineering since 1979. The authors are conducting the first of an ongoing annual survey of international academic software engineering programs, as a joint ACM/IEEE-CS project. This status report covers: history, audience, initial survey, initial partial results available on the WWW, request for evaluation of WWW-site, request for additional questions for next version of survey, time-line for next version of the survey, "lessons learned," and some future directions. The annual report and survey results will be posted on a wide variety of Web pages. A more current report, based on the sabbatical of the first author, will be presented at the conference. The sabbatical involves the initial development of an "International Software Engineering University Consortium-ISEUC". A sample scenario for an employee in industry who becomes a student in ISEUC is given.
[computer science education, ACM IEEE project, Educational institutions, World Wide Web, software engineering degrees, teaching, History, employee, Postal services, Computer science, Information science, Databases, annual survey, undergraduate courses, Education, software engineering degree programs, Web pages, educational courses, graduate courses, software engineering, Software engineering]
Software engineering challenges a CIO's perspective
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Software maintenance, Software quality, Computer architecture, Maintenance engineering, Reliability engineering, Systems engineering and theory, Communication system security, Software reusability, Software engineering, Information systems]
Challenges and achievements in software engineering (CHASE)
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Software testing, Computer science, Engineering drawings, Roads, USA Councils, Shipbuilding industry, Computer industry, Software systems, Software tools, Software engineering]
Seminal software engineering using metaheuristic innovative algorithms
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Software testing, Computer science, Algorithm design and analysis, Design engineering, Software algorithms, Data engineering, Genetic engineering, Application software, Software engineering, Genetic algorithms]
2nd international workshop on living with inconsistency
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
Architecture-oriented programming using FRIED
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Java, Software prototyping, Instruments, Object oriented modeling, Prototypes, Computer architecture, Documentation, Code standards, Programming environments, Adaptive coding]
Corrective maintenance maturity model (CM/sup 3/): maintainer's education and training
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
What is the point of improving maintenance processes if the most important asset, people, is not properly utilised? Knowledge of the product(s) maintained, maintenance processes and communications skills is very important for achieving quality software and for improving maintenance and development processes. We present CM/sup 3/: Maintainer's Education and Training-a maturity model for educating and training maintenance engineers. This model is the result of a comparative study of two industrial processes utilised at ABB, and of process models such as IEEE 1219, ISO/IEC 12207, CMM, People CMM, and TickIT.
[Software maintenance, CMM, Educational products, communications skills, Laboratories, TickIT, Maintenance engineering, corrective maintenance maturity model, training, software quality, People CMM, software maintenance, Robotics and automation, Industrial training, software personnel, Coordinate measuring machines, industrial processes, IEEE 1219, ISO IEC 12207, Software quality, personnel, Computer science education, Capability maturity model]
Invited industry presentations (IIP)
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Web services, Collaborative software, Collaborative tools, Companies, Programming, Computer industry, Mobile handsets, Performance analysis, Best practices, Software engineering]
3/sup rd/ international workshop on net-centric computing (NCC 2001) Theme: migrating to the Web
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
Second ISCE workshop on software product lines: economics, architectures, and applications
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
Understanding IV&amp;IV in a safety critical and complex evolutionary environment: the nasa space shuttle program
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[NASA, Space shuttles, Software quality, Software performance, Software systems, Educational institutions, Aerospace safety, Software safety, Space exploration, Software engineering]
The Software Factory: combining undergraduate computer science and software engineering education
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
Industry often complains that current university curricula fail to address the practical issues of real software development. This paper outlines a proposal for an innovative core curriculum for a Bachelor of Science in computer science. The proposed core curriculum contains elements of traditional computer science programs combined with software engineering via a team-oriented, hands-on approach to large-scale software development. In addition to traditional lecture/project/exam courses, students are required to take an eight-semester sequence of "Software Factory" courses. Software Factory courses put the students' newly acquired skills to work in a real software organization staffed and managed by all students in the program. Students from all courses in the Software Factory sequence meet simultaneously to fulfil their roles in the software organization. We expect the students will be better-prepared software engineering practitioners after completing a curriculum that combines traditonal courses with practical Software Factory experience.
[computer science education, Programming, Educational institutions, Production facilities, team-oriented approach, Computer science, Industrial training, Software Factory, Bachelor of Science, educational courses, undergraduate computer science education, Software quality, university curricula, Computer industry, software engineering education, software engineering, software organization, Computer science education, Large-scale systems, Software engineering]
"Tolerating inconsistency" revisited
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Computer science, Instruments, Humans, Distributed databases, Computer architecture, Programming, Software systems, Data engineering, Deductive databases, Wrapping]
Third international workshop on economics-driven software engineering research
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
New software engineering faculty symposium
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
Reuse that pays
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Computer science, Real time systems, Productivity, System testing, Diesel engines, Programming, Software systems, Control systems, Marine vehicles, Software engineering]
Composition patterns: an approach to designing reusable
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Computer science, Software maintenance, Computer languages, Software design, Runtime, Object oriented modeling, Unified modeling language, Scattering, Educational institutions, Software systems]
Workshop on advanced separation of concerns in software engineering
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
Generative techniques for product lines
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Costs, Terminology, Time to market, Web pages, Software quality, DSL, Application software, Software debugging, Domain specific languages, Software development management]
Frontiers of software practice (FoSP)
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Computer science, Runtime, Publishing, Collaborative software, Space technology, USA Councils, Bandwidth, Programming, Computer industry, Software engineering]
4/sup th/ ICSE workshop on component-based software engineering: component certification and system prediction
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
(W18) 1st workshop on open source software engineering
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
Adaptive feedback scheduling of incremental and design-to time tasks
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Real time systems, Adaptive scheduling, Job shop scheduling, Processor scheduling, Feedback, Quality of service, Dynamic scheduling, Sensor systems, Resource management, Software engineering]
Using OCL-Queries for debugging C++
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Computer science, Encapsulation, Fault detection, Instruments, Unified modeling language, Application software, Software debugging, Programming profession, Contracts, Testing]
SCM-10 tenth international workshop on software configuration management new practices, new challenges, and new boundaries
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[]
XML technologies and software engineering
Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001
None
2001
false
[Computer science, Software architecture, XML, Programming, Collaborative work, Paper technology, Security, Application software, Middleware, Software engineering]
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The following topics are dealt with: software specification; empirical methods; requirements engineering; software testing; software process; architecture and implementation; software evaluation; software architecture; dynamic program analysis; design recovery; mobile and distributed computing; software maintenance; concurrency; program analysis; software presentation; real time systems; product lines; and technology trends.
[software specification, empirical methods, dynamic program analysis, software testing, software maintenance, technology trends, distributed computing, concurrency, software presentation, software architecture, requirements engineering, mobile computing, program analysis, software engineering, design recovery, product lines, real time systems, software evaluation, Software engineering, software process]
Transforming and extending the enterprise through IT
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. In this time of economic turmoil, IT is emerging as a foundation for transition to the new economy. This presentation provides a high-level perspective on the key business and technology megatrends shaping the future of IT, as well as the key management initiatives required to harness and exploit IT effectively. Key issues include: i) What are the key trends and events that will drive new IT investments during the next five years? ii) How will technology advances and changes impact IT deployment decisions? iii) How can organizations harness and exploit IT despite ever-increasing complexity and volatility?.
[Data analysis, Job shop scheduling, Educational products, information technology, business, management initiatives, technology megatrends, investments, technological forecasting, Technology management, Databases, Processor scheduling, Investments, Asia, Education, new economy, future, organizations, Marketing and sales, business data processing, economic turmoil]
Systems engineering: an essential engineering discipline for the 21st Century
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. The engineering of systems in the 21st Century demands robust use of the systems approach given the nature of our times, as well as the systems being created. The global marketplace, changing competition dynamics, shorter life cycles, and increasing complexity characterize our environment. We are building systems that are much larger than ever before and we are building systems that are infinitely smaller than ever before. Maturity of technical, management, and infrastructure processes are competitive discriminators. Systems engineering, both as a profession and as practiced by multi-discipline practitioners, is key to addressing these challenges. Over the past decade, there have been frequent debates on whether systems engineering is an approach or a formal field of engineering. Given the technical, management, and environmental challenges of this century, I believe that systems engineering must be an essential engineering discipline for the 21st Century. This talk discusses the state of the art and practice of systems engineering, and several initiatives focused on its evolution as a formal engineering discipline. Systems engineering and software engineering must each evolve as unique engineering disciplines to address the engineering problems of the 21st Century. We must ensure their evolution results in shared knowledge, and highly collaborative approaches and methods drawing on the unique strengths of each discipline.
[Shape, global marketplace, Engineering drawings, systems approach, 21st Century, environmental challenges, formal engineering discipline, Environmental management, Engineering management, management, Collaboration, commercial product development, Systems engineering and theory, systems engineering, Product development, Robustness, Hardware, software engineering, Software engineering]
Living with COTS
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. Computer usage has evolved from small special purpose applications to large commercial off the shelf software (COTS) products that dominate the landscape. These COTS products present major challenges for our traditional software assistance paradigm. This talk explores those challenges and suggests research opportunities for software assistance in extending COTS products, in integrating them with other components, and in using them.
[software products, software reuse, Instruments, computer usage, Programming, Application software, Distributed computing, Wrapping, software assistance paradigm, Computer science, small special purpose applications, research opportunities, Software architecture, COTS, Distributed databases, software packages, software reusability, commercial off the shelf software, software components, Artificial intelligence, Software engineering]
PROPEL: an approach supporting property elucidation
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Property specifications concisely describe what a software system is supposed to do. It is surprisingly difficult to write these properties correctly. There are rigorous mathematical formalisms for representing properties, but these are often difficult to use. No matter what notation is used, however, there are often subtle, but important, details that need to be considered. PROPEL aims to make the job of writing and understanding properties easier by providing templates that explicitly capture these details as options for commonly-occurring property patterns. These templates are represented using both "disciplined" natural language and finite-state automata, allowing the specifier to easily move between these two representations.
[disciplined natural language, Natural languages, PROPEL, finite state automata, templates, finite state machines, formal specification, software system, property elucidation, Computer science, Automata, Propulsion, Permission, Writing, Software systems, natural languages, commonly-occurring property patterns, Hardware, software tools, Logic, property specifications, Testing]
Performance-related completions for software specifications
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
To evaluate a software specification for its performance potential, it is necessary to supply additional information, not required for functional specification. Examples range from the execution cost of operations and details of deployment, up to missing subsystems and layers. The term "completions" is used here to include all such additions, including annotations, component insertions, environment infrastructure, deployment, communication patterns, design refinements and scenario or design transformations which correspond to a given deployment style. Completions are related to the purpose of evaluation, so they are tailored to describing the performance at a suitable level of detail. Completions for evaluating other attributes such as reliability or security are also possible. The paper describes how completions are added to a specification regardless of the language used (provided that it describes the system behaviour as well as its structure), and experience with completions in Use Case Maps.
[Petri nets, Unified modeling language, Software performance, communication patterns, annotations, component insertions, formal specification, design refinements, execution cost, Use Case Maps, security, Algebra, Prototypes, Computer architecture, specification languages, Permission, Performance analysis, software tools, performance-related completions, software specifications, Formal specifications, UML, Systems engineering and theory, environment infrastructure, design transformations]
Investigating the readability of state-based formal requirements specification languages
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The readability of formal requirements specification languages is hypothesized as a limiting factor in the acceptance of formal methods by the industrial community. An empirical study was conducted to determine how various factors of state-based requirements specification language design affect readability using aerospace applications. Six factors were tested in all, including the representation of the overall state machine structure, the expression of triggering conditions, the use of macros, the use of internal broadcast events, the use of hierarchies, and transition perspective (going-to or coming-from). Subjects included computer scientists as well as aerospace engineers in an effort to determine whether background affects notational preferences. Because so little previous experimentation on this topic exists on which to build hypotheses, the study was designed as a preliminary exploration of what factors are most important with respect to readability. It can serve as a starting point for more thorough and carefully controlled experimentation in specification language readability.
[macros, state-based formal requirements specification languages, Humans, Maintenance engineering, triggering conditions, Mathematics, Specification languages, Formal specifications, formal specification, Aerospace industry, Aerospace engineering, state machine structure, specification language readability, aerospace applications, internal broadcast events, specification languages, formal methods, Permission, aerospace computing, Computer industry, Logic]
Further investigations into the development and evaluation of reading techniques for object-oriented code inspection
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Describes the development and experimental evaluation of a rigorous approach for effective object-oriented (OO) code inspection. Since their development, inspections have been shown to be powerful defect detection strategies but little research has been done to investigate their application to OO systems, which have very different structural and execution models compared to procedural systems. Previous investigations have demonstrated that the delocalised nature of OO software - the resolution of frequent non-local references and the incongruous relationship between its static and dynamic representations - are primary inhibitors to its effective inspection. The experiment investigates a set of three complementary code-reading techniques devised specifically to address these problems: one based on a checklist adapted to address the identified problems of OO inspections, one focused on the systematic construction of abstract specifications, and the last one centered on the dynamic slice that a use case takes through a system. The analysis shows that there is a significant difference in the number of defects found between the three reading techniques. The checklist-based technique emerges as the most effective approach, but the other techniques also have noticeable strengths, and so, for the best results in a practical situation, a combination of techniques is recommended.
[Encapsulation, code-reading techniques, Industrial control, static representation, abstract specifications, dynamic representation, object-oriented code inspection, structural models, nonlocal reference resolution, Permission, software engineering, Testing, inspection, Java, object-oriented programming, Object oriented modeling, program diagnostics, use-case dynamic slice, Inspection, check list, Application software, Power system modeling, defect detection strategies, execution models, Inhibitors, delocalised software]
Empirical interval estimates for the defect content after an inspection
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
We present a novel method for estimating the number of defects contained in a document using the results of an inspection of the document. The method is empirical, being based on observations made during past inspections of comparable documents. The method yields an interval estimate, that is, a whole range of values which is likely to contain the true value of the number of defects in the document. We also derive point estimates from the interval estimate. The method is validated using a known empirical inspection dataset and clearly outperforms existing approaches for estimating the defect content after inspections.
[document handling, interval estimate, Linear regression, Stochastic processes, document management, document, Inspection, Yield estimation, defect content estimation, Permission, software inspections, parameter estimation, curve fitting, Curve fitting, document inspection]
Lessons learned from 25 years of process improvement: the rise and fall of the NASA software engineering laboratory
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
For 25 years the NASA/GSFC Software Engineering Laboratory (SEL) has been a major resource in software process improvement activities. But due to a changing climate at NASA, agency reorganization, and budget cuts, the SEL has lost much of its impact. In this paper we describe the history of the SEL and give some lessons learned on what we did right, what we did wrong, and what others can learn from our experiences. We briefly describe the research that was conducted by the SEL, describe how we evolved our understanding of software process improvement, and provide a set of lessons learned and hypotheses that should enable future groups to learn from and improve on our quarter century of experiences.
[overview, NASA, Laboratories, Programming, Educational institutions, process improvement, History, Personnel, Computer science, historical perspective, NASA/GSFC Software Engineering Laboratory, research and development management, software process improvement, Permission, Software measurement, Software engineering]
Requirements, domain and specifications: a viewpoint-based approach to requirements engineering
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Viewpoint-based Requirements Engineering (VBRE) is based on the fact that there is a multiplicity of stakeholders that take part in any requirements process. This will inevitably lead to conflicts and inconsistencies that, if adequately managed, can be used to improve the process, as they are sources of requirements. There comes a time in every VBRE process when different viewpoints need to be compared to discrepancies (conflicts, inconsistencies). But in Requirements Engineering (RE) we also deal with other categories of statements, apart from "requirements\
[viewpoint-based requirements engineering, Programming, Control systems, Proposals, discrepancy detection, formal specification, requirements engineering, formal verification, systems analysis, VBRE, Permission, Aging, software engineering, discrepancy classification]
Detection of conflicting functional requirements in a use case-driven approach
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
In object-oriented software development, requirements of different stakeholders are often manifested in use case models which complement the static domain model by dynamic and functional requirements. In the course of development, these requirements are analyzed and integrated to produce a consistent overall requirements specification. Iterations of the model may be triggered by conflicts between requirements of different parties. However, due to the diversity, incompleteness, and informal nature, in particular of functional and dynamic requirements, such conflicts are difficult to find. Formal approaches to requirements engineering, often based on logic ' attack these problems, but require highly specialized experts to write and reason about such specifications. We propose a formal interpretation of use case models consisting of UML use case, activity, and collaboration diagrams. The formalization, which is based on concepts from the theory of graph transformation, allows to make precise the notions of conflict and dependency between functional requirements expressed by different use cases. Then, use case models can be statically analyzed, and conflicts or dependencies detected by the analysis can be communicated to the modeler by annotating the model. An implementation of the static analysis within a graph transformation tool is presented.
[Terminology, Unified modeling language, diagrams, conflicting functional requirements detection, use case-driven approach, formal specification, object-oriented software development, graph grammars, graph transformation tool, specification languages, requirements specification, Permission, Software standards, Logic, Standards development, object-oriented programming, Object oriented modeling, program diagnostics, static domain model, static analysis, activity diagrams, Programming profession, Phase detection, collaboration diagrams, UML, Software systems]
A history-based test prioritization technique for regression testing in resource constrained environments
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Regression testing is an expensive and frequently executed maintenance process used to revalidate modified software. To improve it, regression test selection (RTS) techniques strive to lower costs without overly reducing effectiveness by carefully selecting a subset of the test suite. Under certain conditions, some can even guarantee that the selected test cases perform no worse than the original test suite. This ignores certain software development realities such as resource and time constraints that may prevent using RTS techniques as intended (e.g., regression testing must be done overnight, but RTS selection returns two days worth of tests). In practice, testers work around this by prioritizing the test cases and running only those that fit within existing constraints. Unfortunately this generally violates key RTS assumptions, voiding RTS technique guarantees and making regression testing performance unpredictable. Despite this, existing prioritization techniques are memoryless, implicitly assuming that local choices can ensure adequate long run performance. Instead, we propose a new technique that bases prioritization on historical execution data. We conducted an experiment to assess its effects on the long run performance of resource constrained regression testing. Our results expose essential tradeoffs that should be considered when using these techniques over a series of software releases.
[Software testing, Performance evaluation, System testing, Software maintenance, costs, Costs, program testing, program verification, regression testing, resource constrained environments, Programming, Educational institutions, software maintenance, Computer science, software releases, performance, modified software revalidation, experiment, Permission, history-based test prioritization, Time factors]
The impact of test suite granularity on the cost-effectiveness of regression testing
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Regression testing is an expensive testing process used to validate software following modifications. The cost-effectiveness of regression testing techniques varies with characteristics of test suites. One such characteristic, test suite granularity, involves the way in which test inputs are grouped into test cases within a test suite. Various cost-benefit tradeoffs have been attributed to choices of test suite granularity, but almost no research has formally examined these tradeoffs. To address this lack, we conducted several controlled experiments, examining the effects of test suite granularity on the costs and benefits of several regression testing methodologies across six releases of two non-trivial software systems. Our results expose essential tradeoffs to consider when designing test suites for use in regression testing evolving systems.
[Software testing, test inputs, System testing, Computer vision, cost-effectiveness, cost-benefit tradeoffs, Costs, program testing, regression testing, test cases, Control systems, Computer science, essential tradeoffs, test suite granularity, Fault detection, software validation, cost-benefit analysis, Permission, Software systems, Libraries]
Automated test case generation for spreadsheets
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Spreadsheet languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that spreadsheets often contain faults. Thus, in previous work, we presented a methodology that assists spreadsheet users in testing their spreadsheet formulas. Our empirical studies have shown that this methodology can help end-users test spreadsheets more adequately and efficiently; however, the process of generating test cases can still represent a significant impediment. To address this problem, we have been investigating how to automate test case generation for spreadsheets in ways that support incremental testing and provide immediate visual feedback. We have utilized two techniques for generating test cases, one involving random selection and one involving a goal-oriented approach. We describe these techniques, and report results of an experiment examining their relative costs and benefits.
[program debugging, Computer aided software engineering, end-user computing, program testing, incremental testing, spreadsheet programs, visual feedback, cost benefit analysis, Programming profession, random selection, Computer science, spreadsheets, Automatic testing, experiment, cost-benefit analysis, Tail, goal-oriented approach, Permission, Impedance, Output feedback, automated test case generation, methodology, Software engineering, Business]
Deriving executable process descriptions from UML
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
In the recent past, a relevant effort has been devoted to the definition of process modeling languages (PMLs). The resulting languages and environments, although technically successful, did not receive much attention from industry. On the contrary, researchers and practitioners have recently started experimenting with the usage of UML as a PML. Being so popular and widely used, UML has an important competitive advantage compared to any specialized PML. However, it has also a main limitation. While most PMLs are executable by some process engine, UML was conceived as a non-executable, semi-formal language. The work described aims at assessing the possibility of employing a subset of UML as an executable PML. The article proposes a formalization of the semantics of the UML subset and presents the translation of UML process models into code, which can be enacted in the OPSS process-centered environment. The paper also presents a case study to validate the approach. We expect that process modeling by means of UML is easier and available to a larger community of software process managers. Moreover, process enactment makes the process more efficient, reliable, predictable and controllable, as widely shown by previous research.
[Unified modeling language, Petri nets, Humans, executable process descriptions, process-centered environment, nonexecutable semi-formal language, semantics, formal specification, Engines, specification languages, Permission, object oriented programming, workflow management software, Workflow management software, object-oriented programming, OPSS, Knowledge based systems, PML, case study, Computer languages, UML, process modeling languages, Software systems, Context modeling, workflow management system]
Verification support for workflow design with UML activity graphs
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
We describe a tool that supports verification of workflow models specified in UML activity graphs. The tool translates an activity graph into an input format for a model checker according to a semantics we published earlier. With the model checker arbitrary propositional requirements can be checked against the input model. If a requirement fails to hold an error trace is returned by the model checker. The tool automatically translates such an error trace into an activity graph trace by highlighting a corresponding path in the activity graph. One of the problems that is dealt with is that model checkers require a finite state space whereas workflow models in general have an infinite state space. Another problem is that strong fairness is necessary to obtain realistic results. Only model checkers that use a special model checking algorithm for strong fairness are suitable for verifying workflow models. We analyse the structure of the state space. We illustrate our approach with some example verifications.
[Unified modeling language, Petri nets, graph theory, Throughput, World Wide Web, Enterprise resource planning, diagrams, semantics, infinite state space, formal specification, strong fairness, UML activity graphs, specification languages, finite state space, Performance analysis, object oriented programming, workflow management software, model checker, object-oriented programming, error trace, State-space methods, workflow design verification support, Computer science, workflow management, Resource management]
Progressive open source
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The success of several Open Source/sup TM/ software systems, e.g., Apache, Bind, Emacs, and Linux, has recently sparked interest in studying and emulating the software engineering principles underlying this innovative development and use model. Certain aspects of the Open Source development method are having fundamental and far reaching consequences on general software engineering practices. To leverage such Open Source methods and tools, we have defined an innovative software engineering paradigm for large corporations: progressive open source (POS). POS leverages the power of Open Source methods and tools for large corporations in a progressive manner: starting from completely within the corporation, to include partner businesses, and eventually complete Open Source. In this paper we present the design goals and principles for POS. We illustrate POS with two programs in HP: Corporate Source and the collaborative development program. We present early results from both these programs suggesting the power and necessity of POS for all modern large corporations.
[Open Source, Buildings, software systems, Programming, progressive open source, Open source software, corporations, Linux, Collaboration, Computer architecture, Permission, Software systems, operating systems (computers), software engineering, Standards development, business data processing, Software engineering]
ArchJava: connecting software architecture to implementation
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Software architecture describes the structure of a system, enabling more effective design, program understanding, and formal analysis. However, existing approaches decouple implementation code from architecture, allowing inconsistencies, causing confusion, violating architectural properties, and inhibiting software evolution. ArchJava is an extension to Java that seamlessly unifies software architecture with implementation, ensuring that the implementation conforms to architectural constraints. A case study applying ArchJava to a circuit-design application suggests that ArchJava can express architectural structure effectively within an implementation, and that it can aid in program understanding and software evolution.
[Java, object-oriented programming, application program interfaces, Circuits, ArchJava, software evolution, Computer science, Design engineering, software architecture, Software architecture, Computer architecture, Permission, Software systems, Java code, software life cycle, Architecture description languages, Joining processes]
Mixin' up components
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Recently, we (2001) proposed a language called ACOEL (a component-oriented extension language) for abstracting and composing software components. Components in ACOEL are black-box components, and each component consists of: (1) an internal implementation containing classes, methods, and fields that is hidden to the external world; and (2) an external contract consisting of a set of typed input and output ports. Components in ACOEL interact with each other only via these ports. In this paper we extend ACOEL in two directions: (1) use mixins to customize the services provided by a component without exposing its internal implementation, (2) add support for virtual types and sub-type relation among components. We show how mixins and virtual types together allows us to build adaptable applications based on black-box component principles.
[abstracting, object-oriented programming, file transfer, software development, black-box components, ACOEL, Programming, File servers, Application software, Computer crime, component-oriented extension language, mixins, Collaboration, Permission, User interfaces, object-oriented languages, file organisation, Large-scale systems, Remote monitoring, Contracts]
Static and dynamic structure in design patterns
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Design patterns are a valuable mechanism for emphasizing structure, capturing design expertise, and facilitating restructuring of software systems. Patterns are typically applied in the context of an object-oriented language and are implemented so that the pattern participants correspond to object instances that are created and connected at run-time. The paper describes a complementary realization of design patterns, in which many pattern participants correspond to statically instantiated and connected components. Our approach separates the static parts of the software design from the dynamic parts of the system behavior. This separation makes the software design more amenable to analysis, thus enabling more effective and domain-specific detection of system design errors, prediction of run-time behavior, and more effective optimization. This technique is applicable to imperative, functional, and object-oriented languages: we have extended C, Scheme, and Java with our component model. We illustrate our approach in the context of the OSKit, a collection of operating system components written in C.
[Software maintenance, C, Scheme, object-oriented language, run-time behavior, Runtime, Software design, dynamic structure, OSKit, Operating systems, design patterns, Permission, Cities and towns, design expertise, Monitoring, object instances, domain-specific detection, Java, object-oriented programming, Object oriented modeling, statically instantiated components, software systems restructuring, system design errors, pattern participants, static structure, operating system components, object-oriented languages, software reusability, Software systems, operating systems (computers), system behavior]
Goal-oriented software assessment
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Companies that engage in multi-site, multi-project software development continually face the problem of how to understand and improve their software development capabilities. We have defined and applied a goal-oriented process that enables such a company to assess the strengths and weaknesses of those capabilities. Our goals are to help (a) to decrease the time and cost to develop software, (b) to decrease the time needed to make changes to existing software, (c) to improve software quality, (d) to attract and retain a talented engineering staff, and (e) to facilitate more predictable management of software projects. In response to the variety of product requirements, market needs and development environments, we selected a goal-oriented process, rather than a criteria-oriented process, to advance our strategy and ensure relevance of the results. We describe the design of the process, discuss the results achieved and present vulnerabilities of the methodology. The process includes both interviews with projects' personnel and analysis of change data. Several common issues have emerged from the assessments across multiple projects, enabling strategic investments in software technology. Teams report satisfaction with the outcome in that they act on the recommendations, ask for additional future assessments, and recommend the process to sibling organizations.
[Process design, Costs, Project management, predictable software project management, Programming, goal-oriented software assessment, software quality, Personnel, software changes, recommendations, sibling organizations, Engineering management, software product requirements, software process improvement, Quality management, software development capabilities, change data analysis, Data analysis, project management, strategic investments, software technology, market needs, software development environments, software development management, multi-site multi-project software development, Software development management, software development time, future assessments, Software quality, team satisfaction, talented engineering staff, vulnerabilities, project personnel interviews, software development cost]
Security attribute evaluation method: a cost-benefit approach
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Conducting cost-benefit analyses of architectural attributes such as security has always been difficult, because the benefits are difficult to assess. Specialists usually make security decisions, but program managers are left wondering whether their investment in security is well spent. The paper summarizes the results of using a cost-benefit analysis method called SAEM to compare alternative security designs in a financial and accounting information system. The case study presented starts with a multi-attribute risk assessment that results in a prioritized list of risks. Security specialists estimate countermeasure benefits and how the organization's risks are reduced. Using SAEM, security design alternatives are compared with the organization's current selection of security technologies to see if a more cost-effective solution is possible. The goal of using SAEM is to help information-system stakeholders decide whether their security investment is consistent with the expected risks.
[architectural attributes, Information management, Cost benefit analysis, SAEM, Technology management, Investments, program managers, security specialists, cost-benefit analysis, multi-attribute risk assessment, Permission, countermeasure benefits, cost-benefit approach, cost-benefit analyses, financial data processing, financial/accounting information system, risk management, Data security, security decisions, security attribute evaluation method, Risk analysis, information-system stakeholders, Computer science, alternative security designs, security of data, prioritized risks, Information security, Risk management, expected risks]
An empirical evaluation of fault-proneness models
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Planning and allocating resources for testing is difficult and it is usually done on an empirical basis, often leading to unsatisfactory results. The possibility of early estimation of the potential faultiness of software could be of great help for planning and executing testing activities. Most research concentrates on the study of different techniques for computing multivariate models and evaluating their statistical validity, but we still lack experimental data about the validity of such models across different software applications. The paper reports on an empirical study of the validity of multivariate models for predicting software fault-proneness across different applications. It shows that suitably selected multivariate models can predict fault-proneness of modules of different software packages.
[Software testing, statistical validity, Predictive models, software management, Delay, empirical evaluation, multivariate models, Software metrics, resource allocation, software packages, Permission, logistic regression, software performance evaluation, potential software faultiness, Application software, fault-proneness models, software applications, Software packages, software fault-proneness, Resource management, testing activities, principal component analysis, experimental data, Logistics, Principal component analysis, software metrics]
Building product populations with software components
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Two trends have made reuse of embedded software for consumer electronics an urgent issue: the software of individual products becomes more and more complex, and the market demands a larger variety of products at an increasing rate. For that reason, various business groups within Philips organize their products as product families. A third trend is the integration of functions that until now were only found in separate products (e.g. a TV with Dolby Digital sound and a built-in DVD player). This requires software reuse between product families, which (when organized systematically), leads to a product population approach. We have set up such a product population approach, and applied it in various business groups within our organization. We use a component technology that stimulates context independence, and allows the composition of new products out of existing parts. We use an architectural description language to explicitly describe the architecture, and also to generate efficient bindings. We have aligned our development process and organization with the new 'compositional' way of working. The paper outlines our approach and reports on our experiences with it.
[TV, consumer electronics, Laboratories, architectural description language, business groups, Embedded software, component based development, Software architecture, product. families, specification languages, Permission, context independence, Hardware, software components, Business, object-oriented programming, software reuse, product populations, product population approach, development process, configuration management, DVD, embedded software, software reusability, component technology, Architecture description languages, Consumer electronics]
An infrastructure for the rapid development of XML-based architecture description languages
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Research and experimentation in software architectures over the past decade (1992-2002) have yielded a plethora of software architecture description languages (ADLs). Continuing innovation indicates that it is reasonable to expect more new ADLs, or at least ADL features. This research process is impeded by the difficulty and cost associated with developing new notations. An architect in need of a unique set of modeling features must either develop a new architecture description language from scratch or undertake the daunting task of modifying an existing language. In either case, it is unavoidable that a significant effort will be expended in building or adapting tools to support the language. To remedy this situation, we have developed an infrastructure for the rapid development of new architecture description languages. Key aspects of the infrastructure are its XML-based modular extension mechanism, its base set of reusable and customizable architectural modeling constructs, and its equally important set of flexible support tools. The paper introduces the infrastructure and demonstrates its value in the context of several real-world applications.
[Technological innovation, Electronic design automation and methodology, Costs, real-world applications, Buildings, reusable customizable architectural modeling constructs, XML-based architecture description languages, Application software, research process, XML-based modular extension mechanism, Connectors, software architecture, Software architecture, ADLs, specification languages, Permission, software reusability, software architecture experimentation, Architecture description languages, Impedance, modeling features, software architecture description languages, hypermedia markup languages, flexible support tools]
Advanced control flows for flexible graphical user interfaces or, growing GUIs on trees or, bookmarking GUIs
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Web and GUI programs represent two extremely common and popular modes of human-computer interaction. Many GUI programs share the Web's notion of browsing through data and decision trees. The paper compares the user's browsing power in the two cases and illustrates that many GUI programs fall short of the Web's power to clone windows and bookmark applications. It identifies a key implementation problem that GUI programs must overcome to provide this power. It then describes a theoretically well-founded programming pattern, which we have automated, that endows GUI programs with these capabilities. The paper provides concrete examples of the transformation in action.
[Web programs, graphical user interfaces, data trees, advanced control flows, automated programming pattern, Tree graphs, Permission, Carbon capture and storage, GUI programs, Graphical user interfaces, bookmark applications, information resources, automatic programming, browsing, Automatic programming, Cloning, information retrieval, Application software, browsing power, Programming profession, Graphics, human-computer interaction, implementation problem, decision trees, Concrete, flexible graphical user interfaces]
Tracking down software bugs using automatic anomaly detection
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Introduces DIDUCE, a practical and effective tool that aids programmers in detecting complex program errors and identifying their root causes. By instrumenting a program and observing its behavior as it runs, DIDUCE dynamically formulates hypotheses of invariants obeyed by the program. DIDUCE hypothesizes the strictest invariants at the beginning, and gradually relaxes the hypothesis as violations are detected to allow for new behavior. The violations reported help users to catch software bugs as soon as they occur. They also give programmers new visibility into the behavior of the programs such as identifying rare corner cases in the program logic or even locating hidden errors that corrupt the program's results. We implemented the DIDUCE system for Java programs and applied it to four programs of significant size and complexity. DIDUCE succeeded in identifying the root causes of programming errors in each of the programs quickly and automatically. In particular, DIDUCE is effective in isolating a timing-dependent bug in a released JSSE (Java Secure Socket Extension) library, which would have taken an experienced programmer days to find. Our experience suggests that detecting and checking program invariants dynamically is a simple and effective methodology for debugging many different kinds of program errors across a wide variety of application domains.
[program debugging, program testing, Predictive models, timing-dependent bug, Java Secure Socket Extension library, software libraries, complex program errors, Permission, Hardware, software tools, DIDUCE tool, object oriented programming, Java, object-oriented programming, program behavior, automatic anomaly detection, Debugging, Documentation, Sun, Programming profession, Computer bugs, Software systems, JSSE library, software debugging tool, program logic]
Semantic anomaly detection in online data sources
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Much of the software we use for everyday purposes incorporates elements developed and maintained by someone other than the developer. These elements include not only code and databases but also dynamic data feeds from online data sources. Although everyday software is not mission critical, it must be dependable enough for practical use. This is limited by the dependability of the incorporated elements. It is particularly difficult to evaluate the dependability of dynamic data feeds, because they may be changed by their proprietors as they are used. Further, the specifications of these data feeds are often even sketchier than the specifications of software components. We demonstrate a method of inferring invariants about the normal behavior of dynamic data feeds. We use these invariants as proxies for specifications to perform on-going detection of anomalies in the data feed. We show the feasibility of our approach and demonstrate its usefulness for semantic anomaly detection: identifying occasions when a dynamic data feed is delivering unreasonable values, even though its behavior may be superficially acceptable (i.e., it is delivering parsable results in a timely fashion).
[information resources, Software maintenance, databases, semantic anomaly detection, software reliability, Weather forecasting, Cardiac arrest, data integrity, formal specification, formal specifications, Computer science, Databases, online data sources, Permission, Systems engineering and theory, Feeds, software components, Portfolios, Software engineering, dynamic data feed dependability]
Role-based exploration of object-oriented programs
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
We present a new technique for helping developers understand heap properties of object-oriented programs and how the actions of the program affect these properties. Our dynamic analysis uses the aliasing properties of objects to synthesize a set of roles; each role represents an abstract object state intended to be of interest to the developer. We allow the developer to customize the analysis to explore the object states and behavior of the program at multiple different and potentially complementary levels of abstraction. The analysis uses roles as the basis for three abstractions: role transition diagrams, which present the observed transitions between roles and the methods responsible for the transitions; role relationship diagrams, which present the observed referencing relationships between objects playing different roles; and enhanced method interfaces, which present the observed roles of method parameters. Together, these abstractions provide useful information about important object and data structure properties and how the actions of the program affect these properties. We have used our implemented role analysis to explore the behavior of several Java programs. Our experience indicates that, when combined with a powerful graphical user interface, roles are a useful abstraction for helping developers explore and understand the behavior of object-oriented programs.
[heap properties, graphical user interfaces, Laboratories, data structure, diagrams, Data mining, Information analysis, role-based program exploration, abstract object state, Permission, object-oriented programs, enhanced method interfaces, data structures, Marine vehicles, Java programs, Contracts, Java, object-oriented programming, graphical user interface, Data structures, dynamic analysis, role relationship diagrams, Computer science, role transition diagrams, aliasing properties]
Browsing and searching source code of applications written using a GUI framework
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Nowadays, applications are typically written using an object-oriented GUI framework. We explore the possibility of using the GUI of such applications to guide browsing and search of their source code. Such a tool would be helpful for software maintenance and reuse, particularly when the application source is unfamiliar. Generally, the GUI framework is in control and makes calls into the application code to handle various events, thus providing fundamental entry points into the application code, namely the callbacks. Of course, this is a property of frameworks in general but GUI frameworks have one additional advantage: the GUI is visible to the end-user and contains text messages describing what the application can do. Thus, we have an explicit connection between an informal specification fragment visible in the GUI and its precise entry point to the implementation in the source. We demonstrate our approach, which takes advantage of this connection, on KDE applications written using the KDE GUI framework.
[Software maintenance, object-oriented programming, software reuse, graphical user interfaces, callbacks, Application software, software maintenance, text messages, source code searching, object-oriented application framework, informal specification fragment, application code, Potential well, Permission, software reusability, source code browsing, software tools, Australia, GUI framework, Software reusability, Software tools, KDE applications, Graphical user interfaces]
Towards pattern-based design recovery
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
A method and a corresponding tool is described which assist design recovery and program understanding by recognising instances of design patterns semi-automatically. The approach taken is specifically designed to overcome the existing scalability problems caused by many design and implementation variants of design pattern instances. Our approach is based on a new recognition algorithm which works incrementally rather than trying to analyse a possibly large software system in one pass without any human intervention. The new algorithm exploits domain and context knowledge given by a reverse engineer and by a special underlying data structure, namely a special form of an annotated abstract syntax graph. A comparative and quantitative evaluation of applying the approach to the Java AWT and JGL libraries is also given.
[Algorithm design and analysis, domain knowledge, Scalability, Reverse engineering, Humans, data structure, software libraries, JGL library, program understanding, context knowledge, design patterns, Libraries, data structures, software tools, object oriented programming, Java, object-oriented programming, Software algorithms, pattern-based design recovery, Data structures, reverse engineering, Pattern recognition, annotated abstract syntax graph, AWT library, software tool, scalability problems, Software systems]
Architecture recovery of Web applications
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Web applications are the legacy software of the future. Developed under tight schedules, with high employee turnover, and in a rapidly evolving environment, these systems are often poorly structured and poorly documented. Maintaining such systems is problematic. This paper presents an approach to recover the architecture of such systems, in order to make maintenance more manageable. Our lightweight approach is flexible and retargetable to the various technologies that are used in developing Web applications. The approach extracts the structure of dynamic Web applications and shows the interaction between their various components such as databases, distributed objects, and Web pages. The recovery process uses a set of specialized extractors to analyze the source code and binaries of Web applications. The extracted data is manipulated to reduce the complexity of the architectural diagrams. Developers can use the extracted architecture to gain a better understanding of Web applications and to assist in their maintenance.
[information resources, dynamic Web applications, databases, Protocols, Service oriented architecture, Documentation, employee turnover, source code, reverse engineering, diagrams, Application software, Data mining, software maintenance, legacy software, software architecture, Web pages, Distributed databases, Computer architecture, distributed objects, Internet, Web server, Web application architecture recovery]
Network abstractions for context-aware mobile computing
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Context-aware computing is characterized by the ability of a software system to continuously adapt its behavior to a changing environment over which it has little or no control. Previous work along these lines presumed a rather narrow definition of context, one that was centered on resources immediately available to the component in question, e.g., communication bandwidth, physical location, etc. We explore context-aware computing in the setting of ad hoc networks consisting of numerous mobile hosts that interact with each other opportunistically via transient wireless interconnections. We extend the context to encompass awareness of an entire neighborhood within the ad hoc network. A formal abstract characterization of this new perspective is proposed. The result is a specification method and associated context maintenance protocol. The former enables an application to define an individualized context, one that extends across multiple mobile hosts in the ad hoc network. The latter makes it possible to delegate the continuous reevaluation of the context and the performance of operations on it to some middleware operating below the application level. This relieves application development of the obligation of explicitly managing mobility and its implications on the component's behavior.
[network abstractions, context maintenance protocol, Communication system control, Control systems, Mobile communication, mobile computing, mobile hosts, Bandwidth, Computer networks, protocols, distributed object management, middleware, Context-aware services, Context, Ad hoc networks, transient wireless interconnections, individualized context, application development, component behavior, specification method, Software systems, wireless LAN, ad hoc networks, context-aware mobile computing, Mobile computing]
A programming model and system support for disconnected-aware applications on resource-constrained devices
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The emergence of networked lightweight portable computing devices can potentially enable accessibility to a vast array of remote applications and data. In order to cope with shortage of local resources such as memory, CPU and bandwidth, such applications are typically designed as a thin-client thick-server applications. However, another highly desirable yet conflicting requirement is to support disconnected operation, due to the low quality and high cost of online connectivity. We present a novel programming model and a runtime infrastructure that addresses these requirements by automatically reconfiguring the application to operate in disconnected mode of operation, when voluntary disconnection is requested, and automatically resorting to normal distributed operation, upon reconnection. The programming model enables developers to design disconnected aware applications by providing a set of component reference annotations with special disconnection and reconnection semantics. Using these annotations, designers can identify critical components, priorities, dependencies, local component alternatives with reduced functionality, and state merging policies. The runtime infrastructures carries out dis- and re-connection semantics using component mobility and dynamic application layout. The disconnected operation framework, FarGo-DA, is an extension of FarGo, a mobile component framework for distributed applications.
[runtime infrastructure, networked lightweight portable computing devices, Costs, memory, Merging, CPU, Electronic mail, Network servers, Runtime, mobile computing, reconnection semantics, programming model, disconnected-aware applications, remote applications, Cities and towns, Permission, Web server, distributed object programming, distributed object management, client-server systems, Automatic programming, object-oriented programming, bandwidth, state merging policies, runtime infrastructures, component reference annotations, Optical arrays, thin-client thick-server applications, resource-constrained devices, FarGo-DA, wireless LAN, online connectivity, portable computers]
A dynamic pair-program sending architecture for industrial remote operations
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Remote operations such as maintenance, diagnoses, and command executions are more and more needed in industrial automation domains. Remote operation software that flexibly responds to changes in requirements from factory, chemical plants, or remote-side operators is desired. We developed a dynamic program-sending and automatic starting architecture for this purpose. In the architecture, a pair of programs appears in one remote operation context at a time. One program called a "Worker" is dynamically sent to a plant side and another called a "WorkerGUI" is dynamically sent to a remote operator side. Both programs are simultaneously started and communicate each other using Java/RMI. The remote-side operator's commands via the "WorkerGUI" are sent and executed in the plant side "Worker" program and the execution results are sent back to the operator side "WorkerGUI". By using this architecture, a remote-side operator is able to select best match programs whenever he or she needs, and dynamically send and start them. Thus, the architecture establishes flexible remote operation environments. We explain our architecture first, and then, report the evaluation results through experiences of architecture development, three prototype application developments, and using the applications in a real remote plant operation environment.
[graphical user interfaces, World Wide Web, dynamic pair-program sending architecture, Production facilities, remote plant operation environment, Research and development, Network servers, software architecture, Computer architecture, Automatic control, Permission, Computer networks, industrial remote operations, evaluation results, Manufacturing automation, industrial plants, Java, client-server systems, Chemicals, RMI, telecontrol, automatic starting architecture, industrial control, WorkerGUI, remote procedure calls, prototype application development]
Reporting about industrial strength software engineering courses for undergraduates
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
How do you organize an "industrial strength" one semester educational programming project for up to 200 second year students? This paper reports on four years of experience with such projects at the University of Paderborn and the University of Braunschweig. Key properties of our project design are: starting with an existing large application, regular hard deadlines with peer reviews and presentations to a large audience, working in groups, applying project and configuration management tools, a standard system architecture with interchangeable components and competing software agents, quality assurance and standard conformance testing through final overall system integration spanning all groups, and exposure to real-world project threats.
[computer science education, project management, Project management, peer reviews, educational programming project, Application software, conformance testing, software agents, Programming profession, University courses, configuration management, Quality assurance, undergraduate courses, industrial strength software engineering courses, educational courses, quality assurance, Computer architecture, Computer industry, Software standards, Software agents, software engineering, standard system architecture, Software engineering, Quality management]
Concern graphs: finding and describing concerns using structural program dependencies
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Many maintenance tasks address concerns, or features, that are not well modularized in the source code comprising a system. Existing approaches available to help software developers locate and manage scattered concerns use a representation based on lines of source code, complicating the analysis of the concerns. In this paper, we introduce the concern graph representation that abstracts the implementation details of a concern and makes explicit the relationships between different parts of the concern. The abstraction used in a Concern Graph has been designed to allow an obvious and inexpensive mapping back to the corresponding source code. To investigate the practical tradeoffs related to this approach, we have built the feature exploration and analysis tool (FEAT) that allows a developer to manipulate a concern representation extracted from a Java system, and to analyze the relationships of that concern to the code base. We have used this tool to find and describe concerns related to software change tasks. We have performed case studies to evaluate the feasibility, usability, and scalability of the approach. Our results indicate that concern graphs can be used to document a concern for change, that developers unfamiliar with concern graphs can use them effectively, and that the underlying technology scales to industrial-sized programs.
[Performance evaluation, Java, Scalability, structural program dependencies, Scattering, software developers, structured programming, concern graphs, maintenance tasks, Computer science, lines of source code, software change tasks, Abstracts, feature exploration and analysis tool, Permission, software engineering, Marine vehicles, Software tools, Usability, Java system]
Evolving legacy system features into fine-grained components
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
There is a constant need for practical, efficient, and cost-effective software evolution techniques. We propose a novel evolution methodology that integrates the concepts of features, regression tests, and component-based software engineering (CBSE). Regression test cases are untapped resources, full of information about system features. By exercising each feature with its associated test cases using code profilers and similar tools, code can be located and refactored to create components. These components are then inserted back into the legacy system, ensuring a working system structure. This methodology is divided into three parts. Part one identifies the source code associated with features that need evolution. Part two deals with creating components and part three measures results. By applying this methodology, AFS has successfully restructured its enterprise legacy system and reduced the costs of future maintenance. Additionally, the components that were refactored from the legacy system are currently being used within a web-enabled application.
[Software testing, System testing, Costs, code profilers, component-based software engineering, fine-grained components, Application software, software maintenance, legacy system features, Computer science, Investments, Permission, Hardware, software engineering, cost-effective software evolution, regression tests, Software reusability, Software engineering]
Software model checking in practice: an industrial case study
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
We present an application of software model checking to the analysis of a large industrial software product: Lucent Technologies' CDMA call-processing library. This software is deployed on thousands of base stations in wireless networks world-wide, where it sets up and manages millions of calls to and from mobile devices everyday. Our analysis of this software was carried out using VeriSoft, a tool developed at Bell Laboratories that implements model-checking algorithms for systematically testing concurrent reactive software. VeriSoft has now been used for over a year for analyzing several releases and versions of the CDMA call-processing software. Although we started this work with a fairly robust version of the software, the application of model checking exposed several problems that had escaped traditional testing. Model checking also helped developers maintain a high degree of confidence in the library as it evolved through its many releases and versions. To our knowledge, software model checking has rarely been applied to software systems of this scale. In this paper, we describe our experience in applying this technology in an industrial environment.
[Software testing, Algorithm design and analysis, Base stations, VeriSoft, CDMA call-processing software, program testing, Software algorithms, concurrent reactive software, industrial software product, wireless networks, code division multiple access, CDMA call-processing library, Application software, Multiaccess communication, software maintenance, software model checking, Software libraries, Wireless networks, Computer industry, Software tools]
Invariant-based specification, synthesis, and verification of synchronization in concurrent programs
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Concurrency is used in modern software systems as a means of addressing performance, availability, and reliability requirements. Using current technologies developers are faced with a tension between correct synchronization and performance. Unfortunately, simple approaches can result in significant run-time overhead. Implementing more sophisticated synchronization policies may improve run-time performance and satisfy synchronization requirements, but fundamental difficulties in reasoning about concurrency make it difficult to assess their correctness. The paper describes an approach to automatically synthesizing complex synchronization implementations from formal high-level specifications. Moreover, the generated code is designed to be processed easily by software model-checking tools such as Bandera. This enables the generated synchronization solutions to be verified for important system correctness properties. We believe this is an effective approach because the tool-support provided makes it simple to use, it has a solid semantic foundation, it is language independent, and we have demonstrated that it is powerful enough to solve numerous challenging synchronization problems.
[Process design, software model-checking tools, software systems, reliability, availability, concurrent programs, multiple independently executing components, formal specification, invariant-based verification, Concurrent computing, Runtime, formal verification, System performance, invariant-based synthesis, Modems, software tools, Availability, invariant-based specification, formal high-level specifications, system correctness properties, Application software, synchronisation, concurrency, component execution, performance, Collaboration, concurrency control, Software systems, synchronization, Software tools]
Assuring and evolving concurrent programs: annotations and policy
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Assuring and evolving concurrent programs requires understanding the concurrency-related design decisions used in their implementation. In Java-style shared-memory programs, these decisions include which state is shared, how access to it is regulated, the roles of threads, and the policy that distinguishes desired concurrency from race conditions. We use case studies from production Java code to explore the costs and benefits of a new annotation-based approach for expressing design intent. Our intent is both to assist in establishing "thread safety" attributes in code and to support tools that safely restructure code. The annotations we use express "mechanical" properties such as lock-state associations, uniqueness of references, and encapsulation of state into named aggregations. Our analyses revealed race conditions in our case study samples, drawn from open-source projects and library code. The novel technical features of this approach include (1) flexible encapsulation via aggregations of state that can cross object boundaries, (2) the association of locks with state aggregations, (3) policy descriptions for allowable method interleavings, and (4) the incremental process for inserting, validating, and exploiting annotations.
[Encapsulation, software reliability, hazards and race conditions, annotations, concurrent programs, open-source projects, Yarn, Open source software, Concurrent computing, production Java code, lock splitting, concurrency-related design decisions, Production, Java-style shared-memory programs, shared memory systems, Libraries, Safety, software tools, design intent, policy, Java, multi-threading, critical section boundaries, library code, Mechanical factors, lock-state associations, synchronisation, allowable method interleavings, concurrency control, thread safety attributes, Interleaved codes, race conditions, state encapsulation, state aggregations]
Visualization of test information to assist fault localization
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
One of the most expensive and time-consuming components of the debugging process is locating the errors or faults. To locate faults, developers must identify statements involved in failures and select suspicious statements that might contain faults. The paper presents a technique that uses visualization to assist with these tasks. The technique uses color to visually map the participation of each program statement in the outcome of the execution of the program with a test suite, consisting of both passed and failed test cases. Based on this visual mapping, a user can inspect the statements in the program, identify statements involved in failures, and locate potentially faulty statements. The paper also describes a prototype tool that implements our technique along with a set of empirical studies that use the tool for evaluation of the technique. The empirical studies show that, for the subject we studied, the technique can be effective in helping a user locate faults in a program.
[program statement, Visualization, program debugging, visual mapping, Costs, visualization, program testing, failures, color, fault localization, Debugging, Programming, Educational institutions, test information, test suite, Fault diagnosis, Prototypes, Permission, Computer errors, debugging, potentially faulty statements, program visualisation, suspicious statements, Testing]
Efficient path conditions in dependence graphs
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Program slicing combined with constraint solving is a powerful tool for software analysis. Path conditions are generated for a slice or chop, which-when solved for the input variables-deliver compact "witnesses" for dependences or illegal influences between program points. We show how to make path conditions work for large programs. Aggressive engineering, based on interval analysis and BDDs, is shown to overcome the potential combinatoric explosion. Case studies and empirical data demonstrate the usefulness of path conditions for practical program analysis.
[constraint solving, dependence graphs, Input variables, illegal influences, large programs, safety-critical software, computability, Software safety, efficient path conditions, binary decision diagrams, Boolean functions, program analysis, Permission, program slicing, program points, abstract data types, data flow analysis, Data structures, Explosions, Application software, Combinatorial mathematics, BDDs, software analysis, interval analysis, dependences, Software tools, Arithmetic]
Specifying multithreaded Java semantics for program verification
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Most current work on multithreaded Java program verification assumes a model of execution that is based on interleaving of the operations of the individual threads. However, the Java language specification supports a weaker model of execution, called the Java Memory Model (JMM). The JMM allows certain reordering of operations within a thread and thus permits more behaviors than the interleaving based execution model. Therefore, programs verified by assuming interleaved thread execution may not behave correctly for certain Java multithreading implementations. The main difficulty with the JMM is that it is informally described in an abstract rule-based declarative style, which is unsuitable for formal verification. We develop an equivalent formal executable specification of the JMM. Our specification is operational and uses guarded commands. We then use this executable model to verify popular software construction idioms for multithreaded Java. Our prototype verifier tool detects a bug in the widely used "Double Checked Locking" idiom, which verifiers based on interleaving execution model cannot possibly detect.
[Java, formal executable specification, multi-threading, program verification, Java Memory Model, guarded commands, Yarn, formal specification, Computer science, Computer languages, Software design, Multithreading, multithreaded Java semantics, software construction idioms, Permission, Interleaved codes, Hardware, multithreading, Formal verification, operations reordering]
Expertise Browser: a quantitative approach to identifying expertise
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Finding relevant expertise is a critical need in collaborative software engineering, particularly in geographically distributed developments. We introduce a tool, called Expertise Browser (ExB), that uses data from change management systems to locate people with desired expertise. It uses a quantification of experience, and presents evidence to validate this quantification as a measure of expertise. The tool enables developers, for example, to easily distinguish someone who has worked only briefly in a particular area of the code from someone who has more extensive experience, and to locate people with broad expertise throughout large parts of the product, such as modules or even subsystems. In addition, it allows a user to discover expertise profiles for individuals or organizations. Data from a deployment of the tool in a large software development organization shows that newer, remote sites tend to use the tool for expertise location more frequently. Larger, more established sites used the tool to find expertise profiles for people or organizations. We conclude by describing extensions that provide continuous awareness of ongoing work and an interactive, quantitative resume/spl acute/.
[Communication effectiveness, software products, software tool extensions, expertise identification, continuous awareness, Programming, human resource management, subsystems, Design engineering, Permission, experience quantification, Global communication, ongoing work, Floors, quantitative approach, Collaborative software, Resumes, software development management, geographically distributed software development, interactive quantitative resume, software development organization, Cultural differences, modules, remote sites, management of change, change management systems, expertise profile discovery, computer aided software engineering, collaborative software engineering, Continents, Expertise Browser]
Supporting reuse by delivering task-relevant and personalized information
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Technical, cognitive and social factors inhibit the widespread success of systematic software reuse. Our research is primarily concerned with the cognitive and social challenges faced by software developers: how to motivate them to reuse software, and how to reduce the difficulty of locating components from a large reuse repository. Our research has explored a new interaction style between software developers and reuse repository systems enabled by information delivery mechanisms. Instead of passively waiting for software developers to explore the reuse repository with explicit queries, information delivery autonomously locates and presents components by using the developers' partially written programs as implicit queries. We have designed, implemented, and evaluated a system called CodeBroker, which illustrates different techniques to address the essential challenges in information delivery: to make the delivered information relevant to the task at hand and personalized to the background knowledge of an individual developer. Empirical evaluations of CodeBroker show that information delivery is effective in promoting reuse.
[Laboratories, cognitive factors, Programming, CodeBroker, user modelling, interaction style, software libraries, Investments, background knowledge, technical factors, Permission, information relevance, software tools, implicit queries, partially written programs, Productivity, reuse repository systems, software reuse, personalized information delivery, social factors, Technology social factors, software component location, software developer motivation, task-relevant information delivery, relevance feedback, Software quality, System recovery, software reusability, Software systems, computer aided software engineering, Software tools]
Towards large-scale information integration
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Software engineers confront many challenges during software development. One challenge is managing the relationships that exist between software artifacts. We refer to this task as information integration, since establishing a relationship between documents typically implies that an engineer must integrate information from each of the documents to perform a development task. In the past, we have applied open hypermedia techniques and technology to address this challenge. We now extend this work with the development of an information integration environment. We present the design of our environment along with details of its first prototype implementation. Furthermore, we describe our efforts to evaluate the utility of our approach. Our first experiment involves the discovery of keyword relationships between text-based software artifacts. Our second experiment examines the code of an open source project and generates a report on how its module relationships have evolved over time. Finally, our third experiment develops the capability to link code claiming to implement W3C (World Wide Web Consortium) standards with the XHTML (eXtensible HTML) representation of the standards themselves. These experiments combine to demonstrate the promise of our approach. We conclude by asserting that the process of software development can be significantly enhanced if more tools made their relationships available for integration.
[System testing, project support environments, open source project, large-scale information integration, system documentation, report generation, Programming, Code standards, Open source software, information integration environment, keyword relationships, document relationships, Permission, software engineering, software tools, module relationships evolution, Standards development, open hypermedia techniques, hypermedia markup languages, software artifact relationships management, software development, text-based software artifacts, W3C standards, Large scale integration, Software development management, large-scale systems, Computer science, XHTML representation, subroutines, Software tools, software standards]
Distributed component technologies and their software engineering implications
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
In this state-of-the-art report, we review advances in distributed component technologies, such as the Enterprise JavaBeans (EJB) specification and the CORBA component model (CCM). We assess the state of industrial practice in the use of distributed components. We show several architectural styles for whose implementation distributed components have been used successfully. We review the use of iterative and incremental development processes and the notion of a model-driven architecture. We then assess the state of the art in research into novel software engineering methods and tools for the modelling, reasoning and deployment of distributed components. The open problems identified during this review result in the formulation of a research agenda that will contribute to the systematic engineering of distributed systems based on component technologies.
[software prototyping, industrial practice, systematic engineering, reasoning, incremental development processes, reviews, review, Computer architecture, Permission, software engineering, software tools, Software reusability, Assembly, distributed object management, component deployment, Java, software architectural styles, Educational institutions, state of the art, iterative development processes, distributed systems engineering, CORBA component model, research agenda, distributed component technologies, Enterprise JavaBeans, Computer science, Systems engineering and theory, Software systems, model-driven architecture, subroutines, component modelling, Software engineering]
Safety critical systems: challenges and directions
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Safety-critical systems are those systems whose failure could result in loss of life, significant property damage or damage to the environment. There are many well-known examples in application areas such as medical devices, aircraft flight control, weapons and nuclear systems. Many modern information systems are becoming safety-critical in a general sense because financial loss and even loss of life can result from their failure. Future safety-critical systems will be more common and more powerful. From a software perspective, developing safety-critical systems in the numbers required and with adequate dependability is going to require significant advances in areas such as specification, architecture, verification and the software process. The very visible problems that have arisen in the area of information system security suggests that security is a major challenge too.
[software verification, property damage, safety-critical software, Software safety, losses, formal specification, systems failure, software architecture, security, Weapons, Surgery, Modems, software dependability, software engineering, information systems, Application software, Power system security, financial loss, Aerospace control, safety-critical systems, security of data, Information security, Pacemakers, loss of life, environmental damage, Aircraft, software process]
Software component quality assessment in practice: successes and practical impediments
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
This paper describes the authors' experiences of initiating and sustaining a project at CSIRO (Commonwealth Scientific and Industrial Research Organization), aimed at accelerating the successful adoption of COTS (commercial off-the-shelf) middleware technologies in large business and scientific information systems. The project's aims are described, along with example outcomes and an assessment of what is needed for wide-scale software component quality assessments to succeed.
[Enterprise resource planning, project outcomes, software quality, Information systems, COTS middleware technology adoption acceleration, software packages, distributed databases, Large-scale systems, Business, client-server systems, Commonwealth Scientific and Industrial Research Organization, software component quality assessment, management information systems, Application software, Middleware, large-scale systems, commercial off-the-shelf technologies, Software quality, Computer industry, scientific information systems, Quality assessment, subroutines, CSIRO project, Impedance, business data processing, business information systems]
Accelerating software development through collaboration
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
In early 1999, VA Software launched a project to understand how the Internet development community had been able to produce software such as Linux, Apache and Sarnba that was generally developed faster and with higher quality than comparable commercially available alternatives. Our goal was simple: determine how to make more software development projects successful. We discovered that successful Internet community projects employed a number of practices that were not well characterized by traditional software engineering methodologies. We now refer to those practices as collaborative software development or CSD. Late in 1999 we developed the SourceForge platform to make it easy for even small software development projects to employ those practices, and in November of 1999 launched the SourceForge.net Web site based on the SourceForge platform. The site was an overwhelming success, and in less than two years, grew to support more than 27,000 software development projects and over a quarter million software developers worldwide. SourceForge.net affords us an unequaled test bed for understanding CSD. In response to demand from companies seeking to enable CSD within their organizations, we announced a commercial version of the SourceForge platform, SourceForge Enterprise Edition, in August 2001. This paper describes the principles of CSD, the software development pain points those principles address, and our experience enabling CSD with the SourceForge platform.
[Collaborative software, public domain software, open source software, Companies, Programming, software engineering methodologies, collaborative software development, Pain, Linux, Software quality, VA Software, software engineering, SourceForge platform, Internet, Acceleration, Web site, Software engineering, Testing]
The CycleFree methodology: a simple approach to building reliable, robust, real-time systems
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
This paper introduces a new programming methodology for building real-time systems that allows the construction of concurrent programs without the explicit creation and synchronization of threads. The approach requires the program to have an acyclic invocation structure. This restriction allows an underlying CycleFree Kernel to implicitly schedule units of concurrency and synchronize access to objects. Deadlock is avoided by the hierarchical access to objects, and programmers are freed from the traditional worries of explicit task creation and synchronization. The paper discusses real world, commercial experiences, both with explicit multi-threaded applications and with CycleFree applications. The potential pitfalls associated with programming concurrent processes are well known to those skilled in the art. The issues of race conditions, reentrancy, and cyclic deadlock can lead to transient program failure. The CycleFree methodology eliminates these sources of transient program failure, leading to more reliable and robust software.
[Real time systems, Art, CycleFree methodology, cyclic deadlock, programming methodology, concurrent programs, acyclic invocation structure, Yarn, parallel programming, deadlock avoidance, hierarchical access, Concurrent computing, Robustness, software engineering, software tools, Kernel, transient program failure, Buildings, Application software, Programming profession, real-time systems, concurrency control, System recovery, race conditions, explicit task creation]
Function point measurement from Java programs
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Function point analysis (FPA) was proposed to help measure the functionality of software systems. It is used to estimate the effort required for the software development. However, it has been reported that since function point measurement involves judgment on the part of the measurer, differences for the same product may occur even in the same organization. Also, if an organization tries to introduce FPA, FP will have to be measured from the past software developed there, and this measurement is cost-consuming. We examine the possibility to measure FP from source code automatically. First, we propose measurement rules to count data and transactional functions for an object-oriented program based on the IFPUG method and develop the function point measurement tool. Then, we apply the tool to practical Java programs in a computer company and examine the difference between the FP values obtained by the tool and those of an FP measurement specialist. The results show that the number of data and transactional functions extracted by the tool is similar to those by the specialist, although for the classification of each function there is some difference between them.
[Programming, Data mining, software system functionality, FP measurement specialist, Permission, Software measurement, Java programs, measurement rules, Java, IFPUG method, object-oriented programming, software development, source code, computer company, Size measurement, transactional functions, object-oriented program, Computer languages, Image analysis, FPA, Lab-on-a-chip, Software systems, FP values, software cost estimation, function point measurement, function point measurement tool, software metrics]
Experiences in assessing product family software architecture for evolution
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Software architecture assessments are a means to detect architectural problems before the bulk of development work is done. They facilitate planning of improvement activities early in the lifecycle and allow limiting the changes on any existing software. This is particularly beneficial when the architecture has been planned to (or already does) support a whole product family, or a set of products that share common requirements, architecture, components or code. As the family requirements evolve and new products are added, the need to assess the evolvability of the existing architecture is vital. The author illustrates two assessment case studies in the mobile telephone software domain: the Symbian operating system platform and the network resource access control software system. By means of simple experimental data, evidence is shown of the usefulness of architectural assessment as rated by the participating stakeholders. Both assessments have led to the identification of previously unknown architectural defects, and to the consequent planning of improvement initiatives. In both cases, stakeholders noted that a number of side benefits, including improvement of communication and architectural documentation, were also of considerable importance. The lessons learned and suggestions for future research and experimentation are outlined.
[Access control, architectural assessment, family requirements, software lifecycle, Symbian operating system platform, architectural documentation, software architecture, assessment case studies, Software architecture, Operating systems, Computer architecture, software process improvement, Telephony, Permission, software performance evaluation, architectural defects, network resource access control software system, product family software architecture assessment, Documentation, computer telephony integration, Middleware, evolvability, Communication system software, architectural problems, common requirements, mobile telephone software domain, Software systems, operating systems (computers), improvement activity planning, cellular radio]
A comprehensive product line scoping approach and its validation
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Product line engineering is a recent approach to software development that specifically aims at exploiting commonalities and systematic variabilities among functionally overlapping systems in terms of large scale reuse. Taking full advantage of this potential requires adequate planning and management of the reuse approach as otherwise huge economic benefits will be missed due to an inappropriate alignment of the reuse infrastructure. Key in product line planning is the scoping activity, which aims at focussing the reuse investment where it pays. Scoping actually happens on several levels in the process: during the domain analysis step (analysis of product line requirements) a focusing needs to happen just like during the decision of what to implement for reuse. The latter decision also has important ramifications for the development of an appropriate reference architecture as it provides the reusability requirements for this step. We describe an integrated approach that has been developed, improved, and validated over the last few years. The approach fully covers the scoping activities of domain scoping and reuse infrastructure scoping and was validated in several industrial case studies.
[comprehensive product line scoping approach, reference architecture, large scale reuse, Time to market, Programming, economic benefits, reuse infrastructure scoping, industrial case studies, product line requirements, Investments, systematic variabilities, Permission, scoping activity, reuse approach, reuse investment, Large-scale systems, reuse infrastructure, product line engineering, domain analysis step, reusability requirements, Portfolios, domain scoping, software development, systems analysis, integrated approach, software reusability, functionally overlapping systems, software standards, product line planning, Software engineering]
Integrating hundred's of products through one architecture - the industrial IT architecture
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
During the last few years, software product line engineering has gained significant interest as a way for creating software products faster and cheaper. But what architecture is needed to integrate huge amounts of products from different product lines? The paper describes such an architecture and its support processes and tools. Through cases it is illustrated how the architecture is used to integrate new and old products in such diverse integration projects as vessel motion control, airport baggage handling systems, pulp and paper and oil and gas, in a very large organization. However, in a large organization it is a challenge to make everyone follow an architecture. Steps taken to ensure global architectural consistency are presented. It is concluded that a single architecture can be used to unify development in a huge organization where the distributed development practices otherwise may prohibit integration of various products.
[Base stations, software product line engineering, Costs, distributed development practices, software development management, Radio link, Airports, Control systems, Mobile handsets, software product design, diverse integration projects, vessel motion control, industrial IT architecture, airport baggage handling systems, software architecture, large organization, global architectural consistency, support processes, Computer architecture, Permission, Motion control, Radio control, software standards]
Recognizing and responding to "bad smells" in extreme programming
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The agile software development process called Extreme Programming (XP) is a set of best practices which, when used, promises swifter delivery of quality software than one finds with more traditional methodologies. In this paper, we describe a large software development project that used a modified XP approach, identifying several unproductive practices that we detected over its two-year life that threatened the swifter project completion we had grown to expect. We have identified areas of trouble in the entire life cycle, including analysis, design, development, and testing. For each practice we identify, we discuss the solution we implemented to correct it and, more importantly, examine the early symptoms of those poor practices ("bad smells") that project managers, analysts, and developers need to look out for in order to keep an XP project on its swifter track.
[software development process, poor practices, project management, life cycle, Life testing, Project management, Programming, quality software, Asset management, software quality, Best practices, Software quality, Extreme Programming, best practices, Signal processing, Permission, XP, software engineering, Books, Contracts]
Building enterprise portals: principles to practice
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Primary objective of this paper is to offer an exclusive view of constructing and deploying enterprise portals by using a component-based development approach. As the dot-com hype dies down, most companies are forced to revisit their enterprisewide Web integration strategies. This paper offers a pragmatic roadmap that these companies may follow in their upcoming enterprise portal deployment initiatives. The academic world plays a significant role in the advances of the portal technology. In this paper, we address the challenges faced in building enterprise portals as a new principle of software engineering. We also explain how the academia will play a significant role in meeting most of these challenges.
[enterprise portal deployment initiatives, Scalability, Buildings, internetworking, Companies, Business communication, enterprise portals, Web integration, academia, Permission, Cities and towns, Collaborative work, software engineering, Internet, Portals, Software engineering, electronic commerce]
Brazilian software quality in 2002
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Brazil aims to achieve international standards on quality and productivity in the software sector. From 1993 onwards there are strategies and projects to reach the Brazilian objective on software quality. Since 1995 there have been nationwide surveys on software quality every 2 years. This paper highlights the main trends on software quality in Brazil based both on the results of four surveys (1995, 1997, 1999, and 2001) and on other pieces of evidence. The paper concludes that the software quality in Brazil is continuously improving.
[Productivity, software, ISO standards, software development management, software quality, quality, standards, IEC standards, international standards, productivity, Software quality, Permission, Brazil, SPICE, Computer industry, Software standards, Capability maturity model, Software engineering]
Software inspections, reviews and walkthroughs
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Presents some of the history of software inspections, walkthroughs and reviews. This shows that inspections are related to research efforts back in the 1970s. An example success story is briefly described to illustrate how research has impacted industrial software development practice in this area. The success story was the result of researchers and practitioners working closely together. Finally, challenges and questions as well as areas for further work are outlined. As a summary statement, one can say that, in the inspection area, research did have and still has impact on industrial practice.
[Software maintenance, program diagnostics, success story, Humans, Inspection, Programming, Educational institutions, history, History, software reviews, Milling machines, research impact, industrial software development practice, Permission, software inspections, Computer industry, software engineering, Software engineering, inspection, software walkthroughs]
Impact of the research community for the field of software configuration management
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Software configuration management (SCM) is an important discipline in professional software development and maintenance. The importance of SCM has increased as programs have become larger, more complex, and more mission/life-critical. This paper presents a brief summary of a full report that discusses the evolution of SCM technology from the early days of software development to present, and the specific impact university and industrial research has had along the way.
[Software maintenance, software development, software development management, Programming, Control systems, software maintenance, software configuration management, Software development management, Computer science, configuration management, Engineering management, research impact, Permission, Computer industry, Electrical equipment industry, software engineering, Software engineering]
Web services engineering: promises and challenges
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Web services are emerging technologies to reuse software as services over the Internet by wrapping underlying computing models with XML. Web services are rapidly evolving and are expected to change the paradigms of both software development and use. This panel will discuss the current status and challenges of Web services technologies.
[current status, reuse, software development, workflow, Programming, Application specific processors, Web services technologies, Application software, Simple object access protocol, business transactions, Web services, Web and internet services, XML, Permission, software reusability, Internet, workflow management software, Software engineering, Business]
Lightweight vs. heavyweight processes: is this even the right question?
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
false
[Coordinate measuring machines, Programming, Permission, Software measurement, Personnel, Capability maturity model]
Software engineering for large-scale multi-agent systems - SELMAS'2002
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
false
[Production systems, Multiagent systems, Scalability, Object oriented modeling, Permission, Software agents, Large-scale systems, Maintenance, Security, Software engineering]
5t" ICSE workshop on component-based software engineering: benchmarks for predictable assembly
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
false
[]
Principles of software evolution: 5th international workshop on principles of software evolution (IWPSE 2002)
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
false
[]
Scenarios and state machines: models, algorithms, and tools
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
In a wide range of application areas, system behavior is modeled using variations of scenarios and state machines. In telecommunications, ITU standards SDL (Specification and Description Language) and MSC (Message Sequence Charts) are used for formal specifications and descriptions. Also, for behavioral modeling of object-oriented software systems, UML interaction diagrams (sequence or collaboration diagrams) and statechart diagrams are commonly used. Use cases can be conveniently refined using interaction diagrams, which in turn aid in recognizing class operations and associations and in building statechart specifications of object behaviour. The aim of this workshop is to build a shared understanding on the relation between scenarios and state machines and to gain insight on techniques and tools that may leverage from combining these approaches to behaviour modeling. In this workshop, we will discuss the usage of scenario-based approaches in software engineering. An emphasis area is the relation between scenarios and state-machines. Areas of interest of this one-day workshop include, but are not limited to the following three main discussion topics: 1) Models and notations (requirements for different application areas, shortcomings in current notations, new suggestions for models or notations, categorizations); 2) Algorithms (e.g., synthesizing state machines from scenarios, implied scenarios, generating scenarios from state machines, consistency checks); 3) Tools (tool support for the issues above, different application areas). Moreover, we will identify the limitations and shortcomings of current approaches and outline possible directions for future research in the domain. A related workshop (scenario-based roundtrip eng.) was organized at OOPSLA 2000.
[Computer science, Object oriented modeling, Software algorithms, Laboratories, Permission, Software systems, Educational institutions, Telecommunication standards, Application software, Formal specifications]
Third ICSE workshop on web engineering
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The objectives of the workshop cover perspectives on Web Engineering, navigation and adaptivity, design aspects, acceptance criteria for Web-based systems, development and management of Web-based systems, Web metrics and case studies. Researchers and practitioners interested in WebApps development were invited to submit papers describing their work and methodological approaches to WebApps development. Each submission was reviewed by at least two members of the Programme Committee. The selected papers will be presented by the speakers and will form the basis of discussions all round. The workshop will close with a panel discussion.
[]
The fourth international workshop on economics-driven software engineering research (EDSER-4)
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
false
[]
Software product lines: economics, architectures, and implications
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
false
[Design engineering, Software architecture, Industrial economics, Investments, Computer architecture, Permission, Computer industry, Hardware, Software engineering, Business]
Workshop on global software development
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
false
[]
Meeting challenges and surviving success: the 2nd workshop on open source software engineering
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
false
[]
Workshop on software quality
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
false
[]
ICSE 2002 workshop on architecting dependable systems
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Presents the title page of the proceedings record.
[]
Workshop on methods and techniques for software architecture review and assessment (SARA)
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
false
[]
Pre-workshop summary: workshop on iterative, adaptive, and agile processes
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
false
[]
Building systems from commercial components
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The question of which design methods are appropriate for component-based development (CBD) is complicated by different understandings of the end objectives of CBD. A further complication is different understandings of what is meant by "component". These differences lead to entirely distinct classes of design problem. The aim of this tutorial is firstly to outline the different classes of design problem that span these differing interpretations of CBD, and secondly to outline the required methodological responses to these design problems.
[Java, system design methods, Design methodology, commercial components, methodological responses, software systems construction, design problem classes, Design optimization, Guidelines, Construction industry, Operating systems, COTS components, software packages, end objectives, Permission, Computer industry, software engineering, subroutines, Standards development, interpretations, Software engineering, component-based development]
Architecture-centric software engineering
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Many software organizations are in the transition from project-centric to architecture-centric engineering of software. This tutorial addresses this development by providing an overview and in-depth treatment of the issues surrounding the architecture-centric engineering of software. Topics include software architecture design in the presence of existing components and infrastructure (top-down versus bottom-up), architecture evaluation and assessment, software artefact variability management, software product lines and the role of the software architect. These topics are, in addition to the technical perspective, discussed from process and organizational viewpoints. The topics are extensively illustrated by examples and experiences from many industrial cases. The tutorial presents our experiences, reflections and research results concerning architecture-centric software engineering.
[software infrastructure, architecture-centric software engineering, Companies, Programming, software architecture design, organizational viewpoint, software artefact variability management, software architecture, Software architecture, Computer architecture, top-down design, software architecture evaluation, software architect role, Permission, process viewpoint, tutorial, software components, Software reusability, technical perspective, industrial cases, software product lines, bottom-up design, Reflection, Software quality, software architecture assessment, Computer industry, Software engineering]
Software engineering economics: background, current practices, and future directions
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The field of software economics seeks to develop technical theories, guidelines, and practices of software development based on sound, established, and emerging models of value and value-creation - adapted to the domain of software development as necessary. The premise of the field is that software development is an ongoing investment activity - in which developers and managers continually make investment decisions requiring the expenditure of valuable resources, such as time, talent, and money. The overriding aim of this activity is to maximize the value added subject to an equitable distribution among the participating stakeholders. The goal of the paper is to expose the audience to this line of thinking and introduce the tools pertinent to its pursuit. The paper is designed to be self-contained and will cover concepts from introductory to advanced. Both practitioners and researchers with an interest in the impact of value considerations in software decision-making will benefit from attending it.
[software development, software engineering economics, Decision making, Programming, Financial management, Software development management, Computer science, Software design, Investments, Software quality, Permission, guidelines, software engineering, software cost estimation, Software engineering]
Dependability of embedded systems
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Almost all modem, engineered systems depend on computers for their correct operation. The cost, size, and power requirements of microprocessors have dropped to a point where it is both feasible and desirable to embed computers into everything from people to toasters. Obviously computers embedded in people must operate correctly because they are providing what is presumably some form of medical service. But even computers embedded in toasters have to operate correctly because incorrect operation might cause a fire. A serious though less obvious issue with embedded computers, even in appliances, is the financial loss from failure. Recalling a mass-produced appliance to repair a defective embedded computer, correcting the software for example, can be financially devastating for the manufacturer. Embedded systems control external equipment, and significant physical damage can occur as a result of defective software. It is important that software engineers understand the major elements of current technology in the field of dependability as it applies to embedded systems, yet this material tends to be unfamiliar to researchers and practitioners alike. Researchers are often concerned in one way or another with some aspect of what is mistakenly called software "reliability". All practitioners are concerned with the "reliability" of the software that they produce but researchers and practitioners tend not to understand fully the broader impact of their work. A lot of research, such as that on testing, is concerned directly with software dependability. Understanding dependability more fully allows researchers to be more effective. Similarly, practitioners can direct their efforts during development more effectively if they have a better understanding of dependability. Software by itself is benign. Only when it is being used in an application system does the role of software become fully defined. Thus software is just a component of a system, and the dependability of an embedded system depends on how systems engineering information is used in software specification and development.
[engineered systems, Embedded computing, application system, software specification, Costs, Computer aided manufacturing, software reliability, defective software, mass-produced appliance, formal specification, Embedded software, Home appliances, Power engineering computing, systems engineering information, Embedded system, embedded systems, embedded systems dependability, Systems engineering and theory, Modems, power requirements, microprocessors, Power engineering and energy]
Meeting the challenges of Web application development: the web engineering approach
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The Web has very rapidly become central to many applications in diverse areas. As our reliance on Web-based applications continues to increase and the Web systems supporting these applications become more complex, there is growing concern about the manner in which the Web-based systems/applications are created and their quality, integrity and maintainability. The development of Web-based systems has generally been ad hoc, resulting in poor quality and maintainability. In the recent times, there have been many failures of Web applications due to a variety of problems and causes. The way the developers address these problems is critical to deploying successful large-scale Web applications. This paper addresses these issues and offers a holistic approach to managing the complexity of development of Web-based systems and Web applications. It highlights the various real-world issues, challenges and considerations in development of large Web applications, compared to traditional software development, and recommends the Web engineering approach that Web/software developers could follow.
[information resources, Web application development, holistic approach, software development, Web systems, real-world issues, Project management, Maintenance engineering, Programming, Application software, Web engineering approach, Software development management, integrity, Permission, software engineering, Internet, Australia, Research and development management, maintainability, Software engineering, Quality management]
Hyper/J/spl trade/: multi-dimensional separation of concerns for Java/spl trade/
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
This paper describes how to accomplish multi-dimensional separation of concerns (MDSOC) with Hyper/J/sup TM/, a tool available for free download, in the context of standard Java/sup TM/ development. Hyper/J does not require any special languages or language variants, compilers, development environments, or processes - it works with standard Java, using any compiler, development environment, methodology and process. It can be used at any stage of the software lifecycle - from design and initial development of code, to the evolution of pre-existing code (whether or not it was developed using Hyper/J), to the adaptation and integration of separately developed components.
[Java, software lifecycle, Large scale integration, Java/spl trade/, standard Java/sup TM/ development, Application software, multidimensional separation of concerns, compilers, Software design, Hyper/J/spl trade/, Permission, Software systems, software engineering, Standards development, Software engineering]
Component technologies: Java beans, COM, CORBA, RMI, EJB and the CORBA component model
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
This paper is aimed at software engineering practitioners and researchers, who are familiar with object-oriented analysis, design and programming and want to obtain an overview of the technologies that are enabling component-based development. We introduce the idea of component-based development by defining the concept and providing its economic rationale. We describe how object-oriented programming evolved into local component models, such as Java Beans and distributed object technologies, such as the Common Object Request Broker Architecture (CORBA), Java Remote Method Invocation (RMI) and the Component Object Model (COM). We then address how these technologies matured into distributed component models, in particular Enterprise Java Beans (EJB) and the CORBA Component Model (CCM). We give an assessment of the maturity of each of these technologies and sketch how they are used to build distributed architectures.
[Encapsulation, application program interfaces, software engineering practitioners, Enterprise Java Beans, Computer architecture, Permission, Java Beans, Object oriented programming, distributed object management, component-based development, COM, Java, client-server systems, object-oriented programming, distributed component models, Object oriented modeling, component technologies, Educational institutions, object-oriented analysis, CORBA component model, RMI, Computer science, Computer industry, remote procedure calls, EJB, Software engineering]
Tutorial: describing software architecture with UML
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The presence of a solid architectural vision is a key discriminator in the success or failure of a software project. This paper examines what software architecture is and what it is not. It discusses and illustrates how to describe architecture through a set of design viewpoints and views and how to express these views in the UML, in the spirit of the new IEEE Standard 1471:2000: Recommended practice for architectural description. The paper shows of how architectures drive the development process and how to capture architectural design patterns using the UML. It is illustrated by several widely applicable architectural patterns in different domain.
[Real time systems, Unified modeling language, Programming, Palladium, software project, Tellurium, software architecture, Software architecture, USA Councils, UML, Computer architecture, specification languages, Permission, architectural patterns, Solids, solid architectural vision]
Introduction to agile processes and extreme programming
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Extreme programming is one of the most discussed subjects in the software development community. But what makes XP extreme? And how does it fit into the New World of agile methodologies? This paper establishes the underpinnings of agile methodology and explains why you might want to try one. It shows how XP uses a set of practices to build an effective software development team that produces quality software in a predictable and repeatable manner.
[Production systems, Costs, software development community, software development team, quality software, Programming profession, Communication standards, agile processes, extreme programming, Feedback, Permission, Writing, software engineering, Books, Testing, Business]
Advanced visual modelling: beyond UML
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
With the adoption of UML by the OMG and industry as the lingua-franca of visual systems modelling, one begins to ponder what will come next in this field? This tutorial brings a vision for visual modelling beyond UML. We present and consolidate radical new notations, proposed in a series of research papers and with quickly increasing adoption by industry, for the specification of complex systems in an intuitive visual, yet precise manner. The recurring theme of these notations is the upgrading of familiar diagrams into a powerful visual language. Spider diagrams considerably extend Venn diagrams to the specification of object-oriented (OO) systems. Most familiar OO concepts are translated into set-theoretical terms: classes into sets of objects, inheritance corresponding to subsets, and even statecharts are interpreted as the set of objects in that state. Constraint diagrams enhance the arrow notation to describe static system invariants which cannot be described by UML class-object diagrams. Reasoning rules are developed for the notation and strong completeness results are given. Finally, 3D diagrams show how the third dimension and VRML modelling can be used for a conceptual modelling of dynamic system behaviour. Much of the tutorial is based on a case study developed in industry, illustrating how the new notations are combined with those of UML, including OCL.
[statecharts, Unified modeling language, Laboratories, class-object diagrams, notations, Virtual Reality Modeling Language, inheritance, diagrams, set theory, completeness, visual languages, Gas insulated transmission lines, object classes, Visual system, visual language, arrow notation, complex systems specification, specification languages, Permission, Cities and towns, reasoning rules, conceptual modelling, industrial case study, object-oriented methods, tutorial, constraint diagrams, dynamic system behaviour, object-oriented systems specification, virtual reality languages, Object Constraint Language, static system invariants, Unified Modeling Language, 3D diagrams, Object Management Group, OMG, Computer crashes, Spider diagrams, Computer science, diagram upgrading, VRML, UML, Venn diagrams, object-oriented languages, Software systems, Computer industry, OCL, visual systems modelling, subsets]
Non-functional requirements: from elicitation to modelling languages
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Although Non-Functional Requirements (NFRs) have been present in many software development methods, they have been presented as a second or even third class type of requirement, frequently hidden inside notes and therefore, frequently neglected or forgotten. Surprisingly, despite the fact that non-functional requirements arc among the most expensive and difficult to deal with there are still few works that focus on NFRs as first class requirements. Although these Works have brought a contribution on how to represent and deal with NFRs, two aspects remain not sufficiently explored: how to elicit NFRs and how to merge these NFRs with conceptual models. Our work aims at filling this gap, proposing a strategy to elicit NFRs and to integrate them into conceptual models We focus our attention on conceptual models expressed using UML, and therefore, we propose extensions to UML such that NFRs can be expressed. More precisely, we will show how to integrate NFRs to the Class, Sequence and Collaboration Diagrams. We will also show how Use Cases and Scenarios can be adapted to deal with NFRs. This work was validated by three case studies and their results suggest that by using our proposal we can improve the quality of UML models.
[software development process, software development methods, Unified modeling language, Software performance, conceptual models, Software safety, Proposals, Security, formal specification, Computer science, nonfunctional requirements, Permission, Collaborative work, Filling, software engineering, NFRs, Context modeling]
Refactoring
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
A common phenomenon to software systems is that of software entropy, which suggests that over time the design integrity of software decays under the accumulated pressure of modifications, enhancements, and bug fixes. Refactoring is a technique to stem and even reverse this process. This tutorial is an example driven introduction to refactoring: a disciplined approach to changing the design of an existing code base.
[Manuals, software entropy, Turning, Entropy, software design integrity, Software debugging, Software design, Algorithms, Automatic testing, refactoring, Computer bugs, software modifications, Permission, Software systems, software engineering, software bug fixes, code base redesign]
Information systems architecture
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
This tutorial provides an overview of patterns and principles that we have found useful in designing business information systems.
[Heart, patterns, business information systems design, Displays, Information systems, information systems architecture, software architecture, Operating systems, Computer architecture, Permission, Software systems, information systems, tutorial, Logic, Books, Web sites, business data processing]
Tutorial: introduction to the Rational Unified Process/sup /spl reg//
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. The Rational Unified Process/sup /spl reg// (RUP/sup /spl reg//) is a software engineering process framework. It captures many of the best practices in modern software development in a form that is suitable for a wide range of projects and organizations. It embeds object-oriented techniques and uses UML as the principal notation for the several models that are built during the development. The RUP is also an open process framework that allows software organizations to tailor the process to their specific need, and to capture their own specific process know-how in the form of process components. Many process components are now developed by various organizations to cover different domains, technologies, tools, or type of development, and these components can be assembled to rapidly compose a suitable process. This tutorial introduces the basic concepts and principles, which lie under the RUP framework, and show concrete examples of its usage.
[Software maintenance, Unified modeling language, Rational Unified Process, Programming, formal specification, Best practices, Jacobian matrices, object-oriented techniques, notation, specification languages, Permission, Modems, software engineering, software tools, Assembly, object-oriented programming, software development, Unified Modeling Language, Object oriented modeling, open process framework, UML, software engineering process framework, software process components, software organizations, Software engineering]
Tutorial: mastering design patterns
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. Design patterns are about the reuse of excellent, established design ideas, best practice formulas from experience object oriented developers. This tutorial is an introduction to design patterns used in the design of object-oriented software applications.
[costs, Costs, object-oriented programming, Scalability, Programming, Application software, Seminars, Software design, Software architecture, design patterns, Permission, software reusability, object-oriented software design, Robustness, Product development, software design reuse]
Observing timed systems by means of Message Sequence Chart Graphs
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. Tools that feature MSC do not have the ability to check model or implementation executions against the specified behavior. We present a method for observing the behavior of timed systems specified using Message Sequence Chart Graphs (MSC-Graphs) (a simplified version of ITU Z.120 notation). We believe that a log-analyzer and a run-time monitor based on MSC-Graphs are practical and powerful tools to improve the quality of real-time systems. On one hand, the log analyzer can play the role of an Oracle while testing non-functional requirements. On the other hand, the run-time monitor can help in the verification of protocol assertions given in terms of message interchange annotated with time constraints. The work is built over a formal definition of the syntax and semantics of MSC-Graphs, which is similar to (Alur and Yannakakis, 1999) (i.e. based on partial orders). Those MSC-Graphs are enriched with timers and delay intervals in a similar way to (Ben-Abdallah and Leue, 1997) and (Li and Lilius, 1999).
[Real time systems, Protocols, diagrams, semantics, Message Sequence Chart Graphs, formal specification, Delay, log-analyzer, run-time monitor, Runtime, Software architecture, timed systems, message interchange, real-time systems, Permission, syntax, Timing, software tools, Time factors, protocol assertion verification, Monitoring, Testing, MSC-Graphs]
Reasoning about the correctness of software development process
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. During the object-oriented software development process, a variety of models of the system is built. All these models are not independent, but they are related to each other. Elements in one model have trace dependencies to other models; they are semantically overlapping and together represent the system as a whole. It is necessary to have a precise definition of the syntax and semantics of the different models and their relationships, since the lack of accuracy in their definition can lead to wrong model interpretations and inconsistency between models. The paper considers the notion of formal contract regulating the activities in the software development process. It defines the concept of software process contract (sp-contract).
[sp-contracts, object-oriented programming, Object oriented modeling, Process planning, Programming, semantics, agents, object-oriented software development, software development process correctness, Collaboration, Permission, Software systems, syntax, Software agents, Libraries, software engineering, formal contract, software process contract, Software tools, Contracts]
Software validation using power profiles
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. While historically software validation focused on the functional requirements, recent approaches also encompass the validation of quality requirements; for example, system reliability, performance or usability. Application development for mobile platforms opens an additional area of quality - power consumption. In PDAs or mobile phones, power consumption varies depending on the hardware resources used, making it possible to specify and validate correct or incorrect executions. Tools to find the power inconsistencies and to validate software from the energy point of view are needed. We obtained the power consumption and global event traces of mobile phone applications and matched them against the power consumption model providing the first evidence that software can be validated for the energy consumption.
[Energy consumption, program verification, Software performance, mobile phone applications, Mobile handsets, Application software, PDA, power inconsistencies, power consumption, application development, mobile platforms, computer power supplies, global event traces, software validation, Software quality, quality requirements, Hardware, functional requirements, Reliability, Usability, Personal digital assistants, Software tools]
An architecture-centric approach to the development of a distributed model-checker for timed automata
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given, as follows. Research in model checking is focused on increasing the size of the problems that tools can deal with. The ultimate wave has been the use of distributed computing, where a cluster of computers work together to solve the problem. In our work, we present a distributed model checker that is evolved from the Kronos tool and that can handle backwards computation of TCTL (timed computation tree logic) reachability formulae over timed automata. Our proposal, including the arguments of its correctness, is based on software architectures, using a notation adapted from C. Hofmeister et al. (1999). We find such an approach to be a natural and general way to address the development of complex tools that need to incorporate new features and optimizations as they evolve. We introduce some interesting features, such as a-priori graph partitioning (using METIS, a standard library for graph partitioning), sophisticated machinery to reach optimum performance (communication piggybacking and delayed messaging) and dead-time utilization, where every processor uses time intervals of inactivity to perform auxiliary, time-consuming tasks that will later speed up the rest of the computation. The correctness proof strategy combines an architecture evolution with the theoretical results about fix-point calculation developed by P. Cousot (1978).
[program verification, automata theory, problem size, computer cluster, temporal logic, Proposals, a-priori graph partitioning, Distributed computing, Machinery, architecture-centric software development, software libraries, distributed computing, timed computation tree logic, Communication standards, fix-point calculation, software architecture, Software architecture, Computer architecture, Libraries, Logic, distributed model checker, timed automata, distributed programming, backwards computation, software architecture evolution, reachability analysis, Delay effects, complex tools, dead-time utilization, graph partitioning library, TCTL-reachability formulae, delayed messaging, auxiliary time-consuming tasks, optimum performance, correctness proof strategy, Automata, computation speedup, communication piggybacking, METIS, Kronos, inactive time intervals, optimizations, software tool evolution]
Enterprise application integration by means of a generic CORBA LDAP gateway
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. Telecommunication applications are inherently distributed and the interface provided to third party applications is often complex and also distributed. Usually, these third party components need only a subset of the provided data, therefore a simple and standardized access method would be preferred. Such an interface is provided by the Lightweight Directory Access Protocol (LDAP) and we designed an LDAP to CORBA (Common Object Request Broker Architecture) gateway acting as a bridge between the involved technologies.
[application program interfaces, third party components, Access protocols, access method, access protocols, Distributed computing, Bridges, Lightweight Directory Access Protocol, Network servers, enterprise application integration, Common Object Request Broker Architecture, Veins, XML, Prototypes, Permission, telecommunication applications, CORBA LDAP gateway, Artificial intelligence, Web server, business data processing, distributed object management]
XMIDDLE: information sharing middleware for a mobile environment
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. The capabilities of modern mobile devices enable new classes of applications to exploit the ability to form ad-hoc workgroups and exchange data in a very dynamic fashion. They also present, however, new challenges to application developers, related with the scarcity of resources that need to be exploited efficiently. Moreover network connectivity may be interrupted instantaneously and network bandwidth remains by orders of magnitude lower than in wired networks. To address such issues, we have designed and implemented XMIDDLE, which advances mobile computing middleware approaches by choosing a more powerful underlying data structure (XML) and by supporting offline data manipulation.
[client-server systems, Protocols, XMIDDLE, data exchange, offline data manipulation, data structure, Educational institutions, Data structures, Application software, network bandwidth, Middleware, information sharing middleware, Computer science, mobile computing, XML, Bandwidth, Permission, ad-hoc workgroups, Personal digital assistants, Mobile computing, hypermedia markup languages]
The CommUnity Workbench
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
CommUnity is a parallel program design language and framework that has been extended to provide a formal platform for the architectural design of open, reactive, reconfigurable systems. CommUnity programs are in the style of Unity programs, but they also combine elements from interacting processes. CommUnity also has a richer coordination model and it requires interactions between components to be made explicit. The CommUnity Workbench is being developed as a proof of concept of the framework, hiding the underlying "mathematical machinery" from the user. Currently, the tool provides a graphical integrated development environment to write CommUnity programs, draw a configuration, automatically calculate its co-limit and run it. The workbench prevents the creation of ill-formed configurations and gives great flexibility in testing CommUnity programs.
[Software testing, program testing, open systems, Input variables, Conferences, formal platform, software architecture, Quantum computing, Software design, Software architecture, Feedback, specification languages, Permission, configuration drawing, co-limit calculation, parallel languages, ill-formed configurations, Java, software architectural design, graphical integrated development environment, CommUnity Workbench, open reactive reconfigurable systems, hidden mathematical machinery, parallel program design language, interacting processes, component interactions, Software tools, programming environments, coordination model]
Coordination contracts for Java applications
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. The authors consider coordination contracts, a modelling primitive based on methodological and mathematical principles, that facilitates the evolution of software systems. Coordination contracts encapsulate the coordination aspects, i.e., the way components interact, and as such may capture the business rules or the protocols that govern interactions within the application and between the application and its environment. For this approach to be usable in real applications, it requires a tool to support system development and evolution using coordination contracts. The Coordination Development Environment (CDE) helps programmers to develop Java applications using coordination contracts.
[Java, object-oriented programming, Java applications, business rules, Mechanical factors, LAN interconnection, Coordination Development Environment, Application software, coordination contracts, modelling, software tool, Computer architecture, system development, Permission, Software systems, Systems engineering and theory, software tools, object oriented programming, Contracts, Business]
Human capacities in the software process: empiric validation
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
In this paper, an empirical validation of a person-to-role allocation process is presented. In this process, the allocation of people to fulfil roles is made according to the capacities that the people possess and those required by the roles in the software process. A set of experiments are carried out dealing with the development of the initiation, planning and estimation process, domain study process, requirements analysis process and design process of eight projects. It was proved that the estimated time deviation, as well as the errors found in the technical reviews of requirements specification, were less when the people fulfilling the roles of planning engineer, domain analyst, requirements specifier and designer were allocated, according to the proposed process, by considering the set of critical human capacities.
[Process design, domain analyst, Humans, human factors, planning engineer, Programming, critical human capacities, planning process, human resource management, Design engineering, empirical validation, software process improvement, Permission, design process, software projects, State estimation, estimation process, requirements analysis process, Testing, initiation process, technical reviews, Process planning, software development management, person-to-role allocation process, planning, system designer, requirements specifier, requirements specification errors, Capacity planning, software process, domain study process, estimated time deviation]
Smartweaver: an agent-based approach for aspect-oriented development
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. Proposes an approach for enhancing aspect-oriented software development considering aspects as first-class design entities. The proposal puts together lines of research coming from different fields, namely: aspect-oriented frameworks, aspect models extending UML models, knowledge-driven framework documentation and agent-based planning. The concept of smart-weaving promotes essentially an early incorporation of aspects in the development cycle, so that designers are able to specify their designs by means of aspect models, reuse parts of these models, and also provide different strategies to map generic aspect structures to specific implementations. With this purpose, we have built an experimental environment called Smartweaver aiming to support this process. The kind of assistance provided by the tool relies on the Smartbooks method, a method extending traditional techniques for framework documentation. Smartbooks includes a special planning agent that is able to derive the sequence of activities that should be executed to implement a given functionality from a target framework.
[model reuse, implementation mapping, Computer aided software engineering, Unified modeling language, Smartbooks method, framework documentation, aspect-oriented software development, knowledge-driven framework documentation, Programming, aspect-oriented frameworks, planning agent, Proposals, Technology planning, Engines, planning (artificial intelligence), Permission, object-oriented programming, activity sequence derivation, Aspect-Moderator Framework, Smartweaver, agent-based planning, Documentation, software agents, first-class design entities, Bridges, UML models, software design specification, generic aspect structures, computer aided software engineering, aspect models]
An object-oriented bridge among architectural styles, aspects and frameworks
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Summary form only given. Proposes an architecture-driven design approach based on the concept of proto-frameworks, aiming to provide an intermediate stage in the transition from architectural models to object-oriented frameworks or applications. The approach relies on an object-oriented materialization of domain-specific architectures derived from domain models, i.e. the production of concrete computational representations of abstract architectural descriptions using object-oriented terminology. A proto-framework materializes, in object-oriented terms, the infrastructure required for cooperation and communication of each architectural component type. The framework gives abstract hooks to map specific domain components into a class hierarchy in a white-box fashion. This mapping can produce a specific application, but it can also produce new domain-specific frameworks that adopt the underlying architectural model. In the proposed approach, we can basically identify two stages. First, developers should figure out the problem architecture; aspects are initially mapped to architectural constructs, instead of being coded using framework language constructs. Second, the approach enables a materialization into a proto-framework, and then several kinds of frameworks implementations. These frameworks retain the properties inherited from the original architecture.
[proto-frameworks, Assembly systems, domain models, inheritance, Distributed computing, abstract architectural descriptions, software architecture, architectural component types, Computer architecture, Production, class hierarchy, Permission, object-oriented materialization, aspect-oriented programming, object-oriented methods, software architecture-driven design approach, software architectural styles, object-oriented programming, Object oriented modeling, Documentation, object-oriented terminology, architectural models, Bridges, object-oriented bridge, domain-specific architectures, object-oriented frameworks, computational representations, Concrete, Context modeling]
A representation for describing and analyzing concerns in source code
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Existing approaches that are available to help software developers locate and manage the scattered implementation of concerns use a representation based on lines of source code. Because they do not explicitly express program structure, concern representations based on source code have inherent limitations when finding, describing and analyzing concerns. To address these problems, we are trying to find a concern representation that captures as close as possible the amount and precision level of information that software developers need to efficiently plan a change, and that supports queries that can provide this information. As a proposed solution, we introduced the Concern Graph representation that abstracts the implementation details of a concern and makes explicit the relationships between different parts of the concern. The Concern Graph abstraction has also been designed to allow an obvious and expensive mapping back to the corresponding source code. To investigate the practical tradeoffs related to this approach, we have built the Feature Exploration and Analysis Tool (FEAT) that allows a developer to navigate over an extracted model of a Java program, to build up the subset of the model that corresponds to a concern of interest, and to analyze the relationships of that concern to the code base.
[practical tradeoffs, concern representation, concern-code base relationships, Information analysis, graphs, FEAT, Java program model navigation, Abstracts, Permission, Feature Exploration and Analysis Tool, software engineering, implementation details, Marine vehicles, source-code concerns, Java, program control structures, Navigation, software development, Concern Graph representation, Scattering, model subset, change planning, queries, Programming profession, program structure, Software development management, Computer science]
Mobile computing middleware for context-aware applications
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The hypothesis of this paper is that a new form of middleware can be developed that delivers better quality of service to mobile applications. This middleware should maintain in its internal data structures an updated representation of context information, and make it available to executing applications so that they can listen to changes in the context (i.e. inspection of the middleware) and influence the behaviour of the middleware accordingly (i.e. adaptation of the middleware). The main contribution of this research is an investigation of the underlying principles of mobile computing middleware, and in particular of reflection as a means to allow applications to dynamically inspect and adapt middleware behaviour according to the current execution context.
[Quality of service, middleware adaptation, Mobile communication, Batteries, updated context information representation, mobile computing, Bandwidth, context-aware applications, internal data structures, execution context changes, Personal digital assistants, inspection, Context-aware services, Context, client-server systems, service quality, program diagnostics, reflection, quality of service, Application software, Middleware, adaptive systems, mobile computing middleware, middleware inspection, executing applications, Mobile computing]
Use of software inspection inputs in practice
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The effects in the empirical study of different kinds of inputs on the software inspection process can be classified into explicit inputs and implicit inputs. Explicit inputs are the software artifacts to be inspected, the documentation and inspection aids used by the inspectors. Implicit inputs include inspectors' expertise, norms, beliefs and values.
[program diagnostics, explicit inputs, Process planning, Documentation, documentation, inspector expertise, values, Inspection, empirical study, beliefs, Technology management, implicit inputs, software artifacts, inspection aids, norms, software inspection inputs, Management information systems, Permission, Computer industry, Meeting planning, software engineering, Australia, Business, inspection]
Fuzzy logic based interactive recovery of software design
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
This paper presents an approach to semi-automatically detect pattern instances and their implementations in a software system. Design patterns are currently best practice in software development; they provide solutions for nearly all granularities of software design, and this makes them suitable for representing design knowledge. The proposed approach overcomes a number of scalability problems as they exist in other approaches by using fuzzy logic, user interaction and a learning component.
[design knowledge representation, Scalability, Unified modeling language, Programming, user interaction, Best practices, scalability, learning component, Software design, scaling phenomena, interactive programming, design patterns, software design granularity, interactive systems, object-oriented methods, pattern instance implementations, object-oriented programming, software development, interactive software design recovery, Documentation, fuzzy logic, Pattern recognition, Fuzzy logic, Computer science, knowledge representation, Software systems, computer aided software engineering, best practice, semi-automatic pattern instance detection]
Holistic framework for establishing interoperability of heterogeneous software development tools and models
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
This research is an initial investigation into the development of the Holistic Framework for Software Engineering (HFSE), which establishes mechanisms by which existing software development tools and models can interoperate. The HFSE captures and uses dependency relationships among heterogeneous software development artifacts, the results of which can be used by software engineers to improve software processes and product integrity.
[Software testing, Software maintenance, Software prototyping, software development artifacts, Costs, open systems, heterogeneous software development tools, Software performance, Programming, interoperability, dependency relationships, Risk analysis, software product integrity improvement, Design engineering, heterogeneous software development models, Holistic Framework for Software Engineering, software process improvement, computer aided software engineering, software engineering, Performance analysis, software tools, Software engineering]
A compliance notation for verifying concurrent systems
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The compliance notation provides a practical system where both formal and informal techniques can be employed in software verification. The notation has been successfully applied in verifying some industrial safety-critical systems, but currently it has no support for verifying concurrent systems. This research aims to extend the compliance notation with appropriate support for verifying concurrent systems.
[formal techniques, multiprocessing programs, program verification, software verification, concurrent systems verification, Calculus, informal techniques, Specification languages, Sparks, History, Formal specifications, Programming profession, Computer science, Computer languages, Software design, Permission, industrial safety-critical systems, compliance notation]
Research abstract for semantic anomaly detection in dynamic data feeds with incomplete specifications
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
Everyday software must be dependable enough for its intended use. Because this software is not usually mission-critical, it may be cost-effective to detect improper behavior and notify the user or take remedial action. Detecting improper behavior requires a model of proper behavior. Unfortunately, specifications of everyday software are often incomplete and imprecise. The situation is exacerbated when the software incorporates third-party elements such as commercial-off-the-shelf software components, databases, or dynamic data feeds from online data sources. We want to make the use of dynamic data feeds more dependable. We are specifically interested in semantic problems with these feeds-cases in which the data feed is responsive, it delivers well-formed results, but the results are inconsistent, out of range, incorrect, or otherwise unreasonable. We focus on a particular facet of dependability: availability or readiness for usage, and change the fault model from the traditional "fail-silent" (crash failures) to "semantic". We investigate anomaly detection as a step towards increasing the semantic availability of dynamic data feeds.
[incomplete specifications, semantic anomaly detection, normal behavior, data mining, availability, Databases, useful characteristics, Training data, Permission, learning (artificial intelligence), Stock markets, semantic problems, dependability, Computer crashes, Face detection, inference mechanisms, Computer science, fault model, missing specifications, online data sources, Machine learning, dynamic data feeds, proper behavior, Feeds, readiness for usage]
Making software knowledgeable
Proceedings of the 24th International Conference on Software Engineering. ICSE 2002
None
2002
The goal of this research is to express domain knowledge in software applications explicitly and as separated as possible from the implementation strategy. Although some (domain) knowledge is notoriously hard to elicit and capture, as was discovered in building expert systems, the domain knowledge we intend to make explicit is quite tangible as is illustrated by examples. In fact, the domain knowledge is currently "implemented" using a (object-oriented) programming language. When expressed in a suitable medium, domain knowledge consists of concepts and relations between the concepts, constraints on the concepts and the relations, and rules that state how to infer new concepts and relations.
[TV, reuse, understandability, knowledge management, distributed applications, Radio spectrum management, knowledge-intensive domains, Engineering management, Permission, aspect-oriented programming, knowledge acquisition, Knowledge management, adaptability, Application software, Radio broadcasting, software maintenance, software applications, Hospitals, software domains, software engineering practices, object-oriented languages, software reusability, Computer industry, maintenance, Software engineering]
Consistency management with repair actions
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Comprehensive consistency management requires a strong mechanism for repair once inconsistencies have been detected In this paper we present a repair framework for inconsistent distributed documents. The core piece of the framework is a new method for generating interactive repairs from full first order logic formulae that constrain these documents. We present a full implementation of the components in our repair framework, as well as their application to the UML and related heterogeneous documents such as EJB deployment descriptors. We describe how our approach can be used as an infrastructure for building higher-level, domain specific frameworks and provide an overview of related work in the database and software development environment community.
[Unified modeling language, Programming, Educational institutions, Data engineering, data integrity, Application software, software maintenance, programming language semantics, consistency management, first order logic formulae, Computer science, Databases, formal verification, software repair, Engineering management, UML, XML, specification languages, distributed document management, software tools, Logic]
Palantir: raising awareness among configuration management workspaces
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Current configuration management systems promote workspaces that isolate developers from each other. This isolation is both good and bad It is good, because developers make their changes without any interference from changes made concurrently by other developers. It is bad, because not knowing which artifacts are changing in parallel regularly leads to problems when changes are promoted from workspaces into a central configuration management repository. Overcoming the bad isolation, while retaining the good isolation, is a matter of raising awareness among developers, an issue traditionally ignored by the discipline of configuration management. To fill this void, we have developed Palantir, a novel workspace awareness tool that complements existing configuration management systems by providing developers with insight into other workspaces. In particular, the tool informs a developer of which other developers change which other artifacts, calculates a simple measure of severity of those changes, and graphically displays the information in a configurable and generally non-obtrusive manner. To illustrate the use of Palantir, we demonstrate how it integrates with two representative configuration management systems.
[Visualization, project support environments, Filtering, Merging, Project management, Interference, Displays, Computer science, configuration management, Palantir workspace awareness tool, groupware, Particle measurements, configuration management system, Joining processes]
Tools for understanding the behavior of telecommunication systems
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Many methods and tools for the reengineering of software systems have been developed so far However, the domain-specific requirements of telecommunication Systems have not been addressed sufficiently. These systems are designed in a process- rather than in a data-centered way. Furthermore, analyzing and visualizing dynamic behavior is a key to system understanding. In this paper, we report on tools for the reengineering of telecommunication systems which we have developed in close cooperation with an industrial partner These tools are based on a variety of techniques for understanding behavior such as visualization of link chains, recovery of state diagrams from the source code, and visualization of traces by different kinds of diagrams. Tool support has been developed step by step in response to the requirements and questions stated by telecommunication experts at Ericsson Eurolab Germany.
[GSM, Visualization, Software prototyping, telecommunication system, Reverse engineering, Telecommunication switching, Mobile communication, telecommunication computing, software system reengineering, Communication switching, Computer science, systems re-engineering, software tool, Prototypes, Ericsson Eurolab Germany, Software systems, software tools, cellular radio]
Data flow testing as model checking
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper presents a model checking-based approach to dataflow testing. We characterize dataflow oriented coverage criteria in temporal logic such that the problem of test generation is reduced to the problem of finding witnesses for a set of temporal logic formulas. The capability of model checkers to construct witnesses and counterexamples allows test generation to be fully automatic. We discuss complexity issues in minimal cost test generation and describe heuristic test generation algorithms. We illustrate our approach using CTL as temporal logic and SMV as model checker.
[Software testing, Data analysis, Object oriented modeling, model checking-based approach, Automatic logic units, data flow analysis, temporal logic, data flow graphs, Data engineering, data flow testing, Flow graphs, Logic testing, Information technology, heuristic test generation algorithm, formal verification, Automatic testing, Character generation, model checker]
Toward an understanding of the motivation of open source software developers
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
An Open Source Software (OSS) project is unlikely to be successful unless there is an accompanied community that provides the platform for developers and users to collaborate. Members of such communities are volunteers whose motivation to participate and contribute is of essential importance to the success of OSS projects. In this paper, we aim to create an understanding of what motivates people to participate in OSS communities. We theorize that learning is one of the motivational forces. Our theory is grounded in the learning theory of Legitimate Peripheral Participation, and is supported by analyzing the social structure of OSS communities and the co-evolution between OSS systems and communities. We also discuss practical implications of our theory for creating and maintaining sustainable OSS communities as well as for software engineering research and education.
[open source software development, legitimate peripheral participation, Collaborative software, public domain software, human factors, motivation, Open source software, Computer science, Computer languages, Collaboration, software engineering, learning theory, Software engineering]
Scaling step-wise refinement
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Step-wise refinement is a powerful paradigm for developing a complex program from a simple program by adding features incrementally. We present the AHEAD (Algebraic Hierarchical Equations for Application Design) model that shows how step-wise refinement scales to synthesize multiple programs and multiple non-code representations. AHEAD shows that software can have an elegant, hierarchical mathematical structure that is expressible as nested sets of equations. We review a tool set that supports AHEAD. As a demonstration of its viability, we have bootstrapped AHEAD tools solely from equational specifications, generating Java and non-Java artifacts automatically, a task that was accomplished only by ad hoc means previously.
[algebraic hierarchical equations for application design, Java, Unified modeling language, Application software, Equations, Jacobian matrices, Refining, Collaboration, Production, specification languages, hierarchical mathematical structure, software tools, DSL, step-wise refinement, Power generation]
Software variability management
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
During recent years, the amount of variability that has to be supported by a software artifact is growing considerably and its management is developing as a main challenge during development, usage, and evolution of software artifacts. Successful management of variability in software artifacts leads to better customizable software products that are in turn likely to result in higher market success. The aim of this workshop is to study software variability management both from a 'problems' and from a 'solutions' perspective by bringing together people from industrial practice and from applied research in academia to present and discuss their respective experience. Issues to be addressed include, but are not limited to, technological, process, and organizational aspects as well as notation, assessment, design, and evolution aspects.
[configuration management, software artifacts, software variability management, software adaptation, software development management, software configuration, software customization, Software development management]
Problems and Programmers: an educational software engineering card game
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Problems and Programmers is an educational card game that we have developed to help teach software engineering. It is based on the observation that students, in a typical software engineering course, gain little practical experience in issues regarding the software process. The underlying problem is time: any course faces the practical constraint of only being able to involve students in at most a few small software development projects. Problems and Programmers overcomes this limitation by providing a simulation of the software process. In playing the game, students become aware of not only general lessons, such as the fact that they must continuously make tradeoffs among multiple potential next steps, but also specific issues such as the fact that inspections improve the quality of code but delay its delivery time. We describe game play of Problems and Programmers, discuss its underlying design, and report on the results of a small experiment in which twenty-eight students played the game.
[Software testing, Educational programs, computer science education, Delay effects, Computational modeling, Computer simulation, software engineering course, programmer, Inspection, software development project, Programming profession, Computer science, educational card game, Employment, student experiments, computer games, educational courses, software engineering, programming, Software engineering]
Hipikat: recommending pertinent software development artifacts
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
A newcomer to a software project must typically come up-to-speed on a large, varied amount of information about the project before becoming productive. Assimilating this information in the open-source context is difficult because a newcomer cannot rely on the mentoring approach that is commonly used in traditional software developments. To help a newcomer to an open-source project become productive faster, we propose Hipikat, a tool that forms an implicit group memory from the information stored in a project's archives, and that recommends artifacts from the archives that are relevant to a task that a newcomer is trying to perform. To investigate this approach, we have instantiated the Hipikat tool for the Eclipse open-source project. In this paper we describe the Hipikat tool, we report on a qualitative study conducted with a Hipikat mock-up on a medium-sized in-house project, and we report on a case study in which Hipikat recommendations were evaluated for a task on Eclipse.
[Hipikat software tool, software project development, Programming, reverse engineering, Electronic mail, formal specification, Open source software, Employee welfare, Computer science, Eclipse open-source project, Databases, Computer bugs, Prototypes, software tools]
DADO: enhancing middleware to support crosscutting features in distributed, heterogeneous systems
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Some "non-" or "extra-functional" features, such as reliability, security, and tracing, defy modularization mechanisms in programming languages. This makes such features hard to design, implement, and maintain. Implementing such features within a single platform, using a single language, is hard enough With distributed, heterogeneous (DH) systems, these features induce complex implementations which cross-cut different languages, OSs, and hardware platforms, while still needing to share data and events. Worse still, the precise requirements for such features are often locality-dependent and discovered late (e.g., security policies). The DADO/sup 1/ approach helps program cross-cutting features by improving DH middleware. A DADO service comprises pairs of adaplets which are explicitly modeled in IDL. Adaplets may be implemented in any language compatible with the target application, and attached to stubs and skeletons of application objects in a variety of ways. DADO supports flexible and type-checked interactions (using generated stubs and skeletons) between adaplets and between objects and adaplets. Adaplets can be attached at run-time to an application object. We describe the approach and illustrate its use for several cross-cutting features, including performance monitoring, caching, and security. We also discuss software engineering process, as well as run-time performance implications.
[Data security, performance monitoring, data security, software engineering process, program crosscutting features, Maintenance, Application software, Middleware, caching, adaplets, Computer languages, Runtime, distributed heterogeneous systems, DH-HEMTs, object-oriented languages, Skeleton, Hardware, software engineering, run-time performance implications, Monitoring, distributed object management, middleware]
An empirical study of predicate dependence levels and trends
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Many source code analyses are closely related to and strongly influenced by interdependence among program components. This paper reports results from an empirical study of the interdependences involving program predicates and the formal parameters and global variables which potentially affect them. The findings show that it is possible to eliminate from consideration approximately 30% of the formal parameters, 50% of the 'touched' global variables, and 97% of the 'visible' global variables. Another important and encouraging finding is a strong inverse correlation between the number of formal parameters and dependence level. The fact that no such correlation was found for global variables provides evidence to support the conjecture that global variables are harmful.
[Visualization, Array signal processing, graph theory, Humans, Data compression, Educational institutions, Application software, formal parameters, inverse correlation, Automatic testing, program predicate dependence levels, Digital signal processing, global variables, software engineering, program slicing, Logic arrays, Software engineering, computational complexity]
Bridging the gaps between software engineering and human-computer interaction
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The First International Workshop on the Relationships between Software Engineering and Human-Computer Interaction was held on May 3-4, 2003 as part of the 2003 International Conference on Software Engineering, in Portland, OR, U.S.A. This workshop was motivated by a perception among researchers, practitioners, and educators that the fields of Human-Computer Interaction and Software Engineering were largely ignoring each other and that they needed to work together more closely and to understand each other better. This paper describes the motivation, goals, organization, and outputs of the workshop.
[Vocabulary, Educational programs, workshop motivation, human factors, Educational institutions, Proposals, Best practices, Human computer interaction, human-computer interaction, User interfaces, Software systems, Computer industry, software engineering, human computer interaction, workshop goal, Software engineering]
Tricks and traps of initiating a product line concept in existing products
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Many industries are hampered with introducing the product line concept into already existing products. Though appealing, the concept is very difficult to introduce specifically into a legacy environment. All too often the impacts and risks are not considered adequately. This article describes the introduction of a product line approach in Alcatel's S12 Voice Switching System Business Unit. Practical impacts during the introduction are described as well as tricks and traps. The article not only summarizes the key software engineering principles, but also provides empirical evidence and practical techniques on which to build.
[Switching systems, ISO standards, Telecommunication switching, process improvement, Marketing management, formal specification, Bridges, Alcatel S12 voice switching system business unit, Intelligent networks, Coordinate measuring machines, Computer architecture, software process improvement, software reusability, software engineering principle, portfolio management, product line concept, Portfolios, Software engineering]
Beyond the Personal Software Process: Metrics collection and analysis for the differently disciplined
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Pedagogues such as the Personal Software Process (PSP) shift metrics definition, collection, and analysis from the organizational level to the individual level. While case study research indicates that the PSP can provide software engineering students with empirical support for improving estimation and quality assurance, there is little evidence that many students continue to use the PSP when no longer required to do so. Our research suggests that this "PSP adoption problem" may be due to two problems: the high overhead of PSP-style metrics collection and analysis, and the requirement that PSP users "context switch" between product development and process recording. This paper overviews our initial PSP experiences, our first attempt to solve the PSP adoption problem with the LEAP system, and our current approach called Hackystat. This approach fully automates both data collection and analysis, which eliminates overhead and context switching. However, Hackystat changes the kind of metrics data that is collected, and introduces new privacy-related adoption issues of its own.
[Context-aware services, computer science education, Data analysis, Collaborative software, Laboratories, educational pedagogies, Information analysis, Quality assurance, personal software process, privacy-related adoption issues, Character generation, metrics collection, Product development, Software tools, courseware, Software engineering, software metrics, software engineering students]
A compositional formalization of connector wrappers
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Increasingly systems are composed of parts: software components, and the interaction mechanisms (connectors) that enable them to communicate. When assembling systems front independently developed and potentially mismatched parts, wrappers may be used to overcome mismatch as well as to remedy extra-functional deficiencies. Unfortunately the current practice of wrapper creation and use is ad hoc, resulting in artifacts that are often hard to reuse or compose, and whose impact is difficult to analyze. What is needed is a more principled basis for creating, understanding, and applying wrappers. Focusing on the class of connector wrappers (wrappers that address issues related to communication and compatibility), we present a means of characterizing connector wrappers as protocol transformations, modularizing them, and reasoning about their properties. Examples are drawn from commonly practiced dependability enhancing techniques.
[Connectors, protocol transformations, object-oriented programming, compositional formalization, connector wrappers, software components, remedy extra-functional deficiencies, formal specification]
Quality of service engineering with UML, .NET, and CORBA
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The concern for non-functional properties of software components and distributed applications has increased significantly in recent years. Non-functional properties are often subsumed under the term Quality of Service (QoS). It refers to quality aspects of a software component or service such as real-time response guarantees, availability and fault-tolerance, the degree of data consistency, the precision of some computation, or the level of security. Consequently, the specification and implementation of QoS mechanisms has become an important concern in the engineering of distributed applications. In this tutorial the attendees will learn how non-functional requirements can be engineered in a systematic way into applications on top of distribution platforms such as CORBA and .NET The tutorial focuses on two major subjects areas: (1) Specification of QoS properties and (2) implementation of QoS mechanisms in middleware. We present a comprehensive, model-driven approach. It starts with a platform-independent model (PIM) in UML that captures the application QoS requirements. This model is mapped by a tool to a platform-specific model (PSM) tailored for a specific middleware, which is extended with the corresponding QoS mechanisms. Finally, the PSM is translated to code. Participants in this tutorial will get a thorough understanding of general QoS requirements, QoS modeling alternatives and QoS mechanism integration in respect to popular distributed object middleware. Furthermore, we will discuss the pros and cons of CORBA and .NET for QoS engineering. A tool will be demonstrated that eases substantially the modeling stages and the code generation.
[Unified modeling language, Quality of service, software quality, formal specification, program compilers, software component, Fault tolerance, specification languages, .NET, software tools, platform-independent model, distributed object management, middleware, Availability, fault tolerance, Data security, platform-specific model, distributed object middleware, Mechanical factors, quality of service, Application software, Middleware, data consistency, CORBA, code generation, UML, Software quality, Systems engineering and theory]
Industrial-strength software product-line engineering
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Software product-line engineering is one of the few approaches to software engineering that shows promise of improving software productivity by factors of 5 to 10. There are still few examples of its successful application on a large scale, partly because of the complexity of initiating a product-line engineering project and the many factors that must be addressed for such a project to be successful. This tutorial draws on experiences in introducing and sustaining product-line engineering in Lucent Technologies and in Avaya. The objective is to convey to participants the obstacles involved in transitioning to product line engineering and how to overcome such obstacles, particularly in large software development organizations. Participants will learn both technical and organizational aspects of the problem. Participants will leave the tutorial with many ideas on how to introduce product line engineering into an organization in a systematic way.
[project management, software productivity, Lucent Technologies, organizational aspects, technical aspects, Computer industry, software engineering, software development organizations, organisational aspects, software product-line engineering]
Positive experiences with an open project assignment in an introductory programming course
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The course SIF8005 Object-Oriented Programming at the NTNU is in many respects taught in a quite traditional manner, with a well-known textbook, lectures in huge auditoria, and compulsory exercises that the students have to deliver to be allowed to sit a final written exam. The exercise part of the course is a mixture of individual exercises on a weekly basis, and a somewhat larger project to be done by groups of 4 students. This paper particularly discusses the project, which has become a huge success after some notable changes were made for the 2001 offering of the course. Then the assignment profile was changed from one of fixed requirements set by staff to an open assignment, where each group made a computer game according to their own preferences. This change eliminated many of the problems present in earlier offerings and increased student inspiration.
[Knowledge engineering, Java, computer science education, object-oriented programming, computer game, Relational databases, Calendars, HTML, teaching, Information science, student inspiration, Web pages, Collaboration, educational courses, computer games, open project assignment, introductory programming course, Object oriented programming, positive experiences]
XVCL: XML-based variant configuration language
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
XVCL (XML-based Variant Configuration Language) is a meta-programming technique and tool that provides effective reuse mechanisms [2]. XVCL is an open source software (http://fxvcl.sourceforge.net) developed at the National University of Singapore. Being a modem and versatile version of Bassett's frames [1], a technology that has achieved substantial gains in industry, the underlying principles of the XVCL have been thoroughly tested in practice. Unlike original frames, XVCL blends with contemporary programming paradigms and complements other design techniques. XVCL uses "composition with adaptation" rules to generate a specific program from generic, reusable meta-components. Program generation rules are 100% transparent to a programmer, who retains full control over fine-tuning the generated code. Despite its simplicity, XVCL can effectively manage a wide range of program variants from a compact base of metacomponents, structured for effective reuse.
[hardware devices, meta-programming technique, Open source software, XVCL, software evolution, software architecture, Computer architecture, Hardware, Testing, versatile version, meta data, software reuse, open source software, Asset management, Bassett frames, Application software, program code, Programming profession, program generation, Computer science, reuse mechanisms, XML, Information processing, software reusability, XML-based variant configuration language, software product]
Design recovery of interactive graphical applications
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Nowadays, the majority of productivity applications are interactive and graphical in nature. In this paper, we explore the possibility of taking advantage of these two characteristics in a design recovery tool. Specifically, the fact that an application is interactive means that we can identify distinct execution bursts corresponding closely to "actions" performed by the user The fact that the application is graphical means that we can describe those actions visually from a fragment of the application display itself. Combining these two ideas, we obtain an explicit mapping from high-level actions performed by a user (similar to use case scenarios/specification fragments) to their low-level implementation. This mapping can be used for design recovery of interactive graphical applications. We demonstrate our approach using L/sub Y/X, a scientific word processor.
[Productivity, scientific word processor, graphical user interfaces, interactive systems, Displays, productivity applications, software engineering, design recovery tool, interactive graphical applications, word processing]
Java program analysis projects in osaka university: aspect-based slicing system ADAS and ranked-component search system SPARS-J
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
In our research demonstration, we show two development support systems for Java programs. One is an Aspect-oriented Dynamic Analysis and Slice calculation system named ADAS, and another is a Software Product archiving, Analyzing, and Retrieving System for Java named SPARS-J.
[Java, SPARS-J, Costs, Data analysis, slice calculation system, ranked-component search system, software development management, Debugging, aspect-oriented dynamic analysis, Virtual machining, ADAS, Data mining, Programming profession, Information analysis, Information science, aspect-based slicing system, Java program analysis projects, retrieving system, information retrieval systems, software product archive, Osaka University, Dynamic programming, development support systems, program slicing]
Fragment class analysis for testing of polymorphism in Java software
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Adequate testing of polymorphism in object-oriented software requires coverage of all possible bindings of receiver classes and target methods at call sites. Tools that measure this coverage need to use class analysis to compute the coverage requirements. However, traditional whole-program class analysis cannot be used when testing partial programs. To solve this problem, we present a general approach for adapting whole-program class analyses to operate on program fragments. Furthermore, since analysis precision is critical for coverage tools, we provide precision measurements for several analyses by determining which of the computed coverage requirements are actually feasible. Our work enables the use of whole-program class analyses for testing of polymorphism in partial programs, and identifies analyses that compute precise coverage requirements and therefore are good candidates for use in coverage tools.
[Software testing, Java, object-oriented programming, program testing, object-oriented software, fragment class analysis, Programming profession, Computer science, Information science, Fault detection, Automatic testing, Collaboration, polymorphism testing, Java software, Software tools, Contracts]
Second workshop on scenarios and state machines: models, algorithms, and tools
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Following the success of the "First Workshop on Scenarios and State Machines: Models, Algorithms, and Tools" held at ICSE 2002 in Orlando [1], this workshop aims at bringing together researchers and practitioners to build a shared understanding on the relation between scenarios and state machines and to gain insight into techniques and tools that may leverage the combination of these approaches to enhance our means for behavior modeling.
[scenario-based notations, state machine-based technique, specification languages, Software requirements and specifications, software engineering, Specification languages, software tools, Software tools, formal specification, UML statechart diagram, behavior modeling]
Empowering software engineers in human-centered design
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Usability is about to become the quality measure of today's interactive software including Web sites, and mobile appliances. User-centered design approach emerges from this need for developing more usable products. However, interactive systems are still designed and tested by software and computer engineers unfamiliar with UCD and the related usability engineering techniques. While most software developers may have been exposed with basic concepts such as GUI design guidelines, few developers are able to understand the human/user-centered design (UCD) toolbox at a level that allows them to incorporate it into the software development lifecycle. This paper describes an approach for skilling developers and students enrolled in an engineering program in critical user-centered design techniques and tools. The proposed approach starts from the analysis of the usability and software engineer's work context, identifies critical UCD skills and then associates relevant learning resources with each of the identified skills. Our approach suggests a list of patterns for facilitating the integration the UCD skills into the software engineering lifecycle.
[Software testing, human-centered design, System testing, computer science education, interactive software, graphical user interfaces, User centered design, GUI design guidelines, Design engineering, Home appliances, usability engineering technique, software development lifecycle, quality measure, Interactive systems, Software quality, software engineering, Software measurement, Usability, Software tools, user centred design]
Architectural interaction diagrams: AIDs for system modeling
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper develops a modeling paradigm called Architectural Interaction Diagrams, or AIDs, for the high-level design of systems containing concurrent, interacting components. The novelty of AIDs is that they introduce interaction mechanisms, or buses, as first-class entities into the modeling vocabulary. Users then have the capability, in their modeling, of using buses whose behavior captures interaction at a higher level of abstraction than that afforded by modeling notations such as Message Sequence Charts or process algebra, which typically provide only one fixed interaction mechanism. This paper defines AIDs formally by giving them an operational semantics that describes how buses combine subsystem transitions into system-level transitions. This semantics enables AIDs to be simulated; to incorporate subsystems given in different modeling notations into a single system model; and to use testing, debugging and model checking early in the system design cycle in order to catch design errors before they are implemented.
[software architecture, formal verification, Acquired immune deficiency syndrome, Petri nets, operational semantics, system bus, architectural interaction diagram, diagrams, Modeling, formal specification, software modeling notation, Petri net]
Computer-assisted assume/guarantee reasoning with VeriSoft
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
We show how the state space exploration tool VeriSoft can be used to analyze parallel C/C++ programs compositionally. VeriSoft is used to check assume/guarantee specifications of parallel processes automatically. The analysis is meant to complement standard assume/guarantee reasoning which is usually carried out solely with "pencil and paper". While a successful analysis does not always imply the general correctness of the specification, it increases the confidence in the verification effort. An unsuccessful analysis always produces a counterexample which can be used to correct the specification or the program. VeriSoft's optimization and visualization techniques make the analysis relatively efficient and effective.
[Visualization, VeriSoft, Automation, program verification, Computational modeling, State-space methods, formal specification, state space exploration tool, parallel programming, Concurrent computing, formal verification, Feedback, Embedded system, optimization, Automata, Space exploration, parallel processes, program visualisation, Software tools, visualization techniques]
Must there be so few? Including women in CS
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Women's under-representation in academic computer science is described for the U.S. and internationally. Conditions that contribute to this situation are identified, and motivations for increasing women's participation in computer science are discussed According to recent research in the US., effective interventions at the undergraduate level include: actively recruiting women, encouraging women to persist, and mentoring for the purpose of overcoming under-representation. The latter two practices are easily implemented.
[computer science education, motivation, Data engineering, Mathematics, Biology, Statistics, female participation, Recruitment, Computer science, Employee welfare, Databases, educational courses, CS women under-representation, Computer industry, academic discipline, educational statistics, Computer science education, gender issues]
Cadena: an integrated development, analysis, and verification environment for component-based systems
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The use of component models such as Enterprise Java Beans and the CORBA Component Model (CCM) in application development is expanding rapidly. Even in real-time safety/mission-critical domains, component-based development is beginning to take hold as a mechanism for incorporating non-functional aspects such as real-time, quality-of-service, and distribution. To form an effective basis for development of such systems, we believe that support for reasoning about correctness properties of component-based designs is essential. In this paper, we present Cadena - an integrated environment for building and modeling CCM systems. Cadena provides facilities for defining component types using CCM IDL, specifying dependency information and transition System semantics for these types, assembling systems from CCM components, visualizing various dependence relationships between components, specifying and verifying correctness properties of models of CCM systems derived from CCM IDL, component assembly information, and Cadena specifications, and producing CORBA stubs and skeletons implemented in Java. We are applying Cadena to avionics applications built using Boeing's Bold Stroke framework.
[Real time systems, Java, Assembly systems, program verification, Object oriented modeling, component-based systems, Mission critical systems, Buildings, Aerospace electronics, enterprise Java Beans, avionics, Application software, CORBA component model, Middleware, formal specification, formal verification, Safety, distributed object management, middleware]
Component technology - what, where, and how?
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Software components, if used properly, offer many software engineering benefits. Yet, they also pose many original challenges starting from quality assurance and ranging to architectural embedding and composability. In addition, the recent movement towards services, as well as the established world of objects, causes many to wonder what purpose components might have. This extended abstract summarizes the main points of my Frontiers of Software Practice (FOSP) talk at ICSE 2003. The topics covered aim to offer an end-to-end overview of what role components should play, where they should be used, and how this can be achieved Some key open problems are also pointed out.
[software architecture, Frontiers of Software Practice, Quality assurance, object-oriented programming, quality assurance, software engineering, software quality, architectural embedding, software components, formal specification, Software engineering]
Precise dynamic slicing algorithms
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Dynamic slicing algorithms can greatly reduce the debugging effort by focusing the attention of the user on a relevant subset of program statements. In this paper we present the design and evaluation of three precise dynamic slicing algorithms called the full preprocessing (FP), no preprocessing (NP) and limited preprocessing (LP) algorithms. The algorithms differ in the relative timing of constructing the dynamic data dependence graph and its traversal for computing requested dynamic slices. Our experiments show that the LP algorithm is a fast and practical precise slicing algorithm. In fact we show that while precise slices can be orders of magnitude smaller than imprecise dynamic slices, for small number of slicing requests, the LP algorithm is faster than an imprecise dynamic slicing algorithm proposed by Agrawal and Horgan.
[Algorithm design and analysis, Costs, Data analysis, limited preprocessing algorithm, Heuristic algorithms, graph theory, Debugging, Computer science, Sequential analysis, Computer aided instruction, dynamic data dependence graph, precise dynamic slicing algorithms, full preprocessing algorithm, no preprocessing algorithm, software engineering, Timing, program slicing, computational complexity]
Goal-oriented requirements engineering: from system objectives to UML models to precise software specifications
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This tutorial presents a comprehensive overview of state-of-the-art techniques for eliciting, modeling, specifying, analyzing and documenting high-quality system requirements.
[goal-oriented requirements engineering, UML models, Unified modeling language, systems analysis, specification languages, software specifications, software modeling, Systems engineering and theory, formal specification, software agents]
Evaluating the quality of information models: empirical testing of a conceptual model quality framework
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper conducts an empirical analysis of a semiotics-based quality framework for quality assuring information models. 192 participants were trained in the concepts of the quality framework, and used it to evaluate models represented in an extended Entity Relationship (ER) language. A randomised, double-blind design was used, in which each participant independently reviewed multiple models and each model was evaluated by multiple reviewers. A combination of quantitative and qualitative analysis techniques were used to evaluate the results, including reliability analysis, validity analysis, interaction analysis, influence analysis, defect pattern analysis and task accuracy analysis.. An analysis was also conducted of the framework's likelihood of adoption in practice. The study provides strong support for the validity of the framework and suggests that it is likely to be adopted in practice, but raises questions about its reliability and the ability of participants to use it to accurately identify defects. The research findings provide clear directions for improvement of the framework. The research methodology used provides a general approach to empirical validation of quality frameworks.
[software reliability, semiotics-based quality framework, entity relationship language, influence analysis, empirical testing, software quality, conceptual model quality framework, interaction analysis, defect pattern analysis, validity analysis, reliability analysis, quality assuring information models, quality assurance, task accuracy analysis, entity-relationship modelling, Testing]
6th ICSE workshop on component-based software engineering: automated reasoning and prediction
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Component-based technologies and processes have been deployed in many organizations and in many fields over the past several years. However, modeling, reasoning about, and predicting component and system properties remains challenging in theory and in practice. CBSE6 builds on previous workshops in the ICSE/CBSE series, and in 2003 is thematically centered on automated composition theories. Composition theories support reasoning about, and predicting, the runtime properties of assemblies of components. Automation is a practical necessity for applying composition theories in practice. Emphasis is placed in this workshop on composition theories that are well founded theoretically, are verifiable or falsifiable, automated by tools, and supported by practical evaluation.
[object-oriented programming, component-based software engineering, automated composition theories, automated prediction, system modeling, System analysis and design, system analysis, automated reasoning, systems analysis, quality attributes, reasoning about programs, software tools, Object oriented programming, Software tools]
Towards systematic recycling of systems requirements
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Many (technical) systems are not developed from scratch but as an evolution of existing systems. Consequently, a large portion of the system requirements employed can be recycled when building the next version of the product. Usually, this recycling step is performed unsystematically, i.e. simply by copying and modifying complete requirements documents. In this paper, we present in a case study a lightweight requirements recycling approach which evolved from observations and concrete needs of projects at DaimlerChrysler Passenger Car Development. The basic idea of the approach is separation of model-dependent from model-independent requirements on the same level of abstraction. This notion is supported by document structures, criteria for identifying reusable requirements and tool support. The paper presents the core elements of the approach and provides observations and valuable experiences we made in the projects.
[Production systems, Protocols, Instruments, reusable requirement, Displays, requirement document, formal specification, model-independent requirement, model-dependent requirement, Collaboration, Character generation, software reusability, Concrete, Recycling, Manufacturing, Daimler-Chrysler passenger car development, Testing, system requirement, systematic recycling]
Using a web-based project process throughout the software engineering curriculum
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
In order to facilitate the study and use of software process, which is essential to the education of future software professionals, a standard and tailorable project process has been developed over the last five years at Texas Tech University for use in both undergraduate and graduate curricula, with a total 12 courses involved The process is entirely web-based, and includes a complete set of HTML document templates in order to facilitate the creation of project artifacts which are posted to the course web page. This method enhances communication between team members, including distance education students, and between the project team and client. The project process has received positive feedback from all stakeholders involved This paper discusses the benefits of the web-based project process, its relation to curriculum models, and plans for a more formal assessment of the process. The portability of process to other institutions is also discussed, with an example provided.
[computer science education, formal assessment, educational courses, Web-based project process, software engineering, educational administrative data processing, HTML document templates, software engineering curriculum, Software engineering]
A component architecture for an extensible, highly integrated context-aware computing infrastructure
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Ubiquitous context-aware computing systems present several challenges in their construction. Principal among them is the tradeoff between easily providing new context-aware services to users and the tight integration of those services, as demanded by the small form factor of the devices typically found in ubiquitous computing environments. Performance issues further complicate the management of this tradeoff. Mechanisms have been proposed and toolkits developed for aiding the construction of context-aware systems, but there has been little consideration of how to specialize, organize, and compose these mechanisms to meet the above requirements. We motivate and describe a software architecture that provides the desired integration and extensibility of services in a context-aware application infrastructure. A key result is the fissioning of intuitive class organizations, both across layers and within layers, to achieve the required integration of services and separation of concerns.
[Context-aware services, object-oriented programming, intuitive class organization fissioning, Navigation, ubiquitous context-aware computing systems, Buildings, Ubiquitous computing, Sensor systems, Application software, ubiquitous computing, Computer science, Component architectures, software architecture, Software architecture, Computer architecture, entity-relationship modelling, software component architecture]
Designing software architectures for usability
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Usability is increasingly recognized as a quality attribute that one has to design for. The conventional alternative is to measure usability on a finished system and improve it. The disadvantage of this approach is, obviously, that the cost associated with implementing usability improvements in a fully implemented system are typically very high and prohibit improvements with architectural impact. In this tutorial, we present the insights gained, techniques developed and lessons learned in the EU-IST project STATUS (SofTware Architectures That supports USability). These include a forward-engineering perspective on usability, a technique for specifying usability requirements, a method for assessing software architectures for usability and, finally, for improving software architectures for usability. The topics are extensively illustrated by examples and experiences from many industrial cases.
[software architecture, Software design, EU-IST project, Computer architecture, software process improvement, requirements specification, software reusability, software quality, Usability, formal specification, software usability]
A software process scheduling simulator
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
To cut development cost and meet tight deadlines in short staffed software projects, managers must optimize the project schedule. Scheduling a software project is extremely difficult, though, because the time needed to complete a software development activity is hard to estimate. Often, the completion of a task is delayed because of unanticipated rework caused by feedback between activities in the process.
[probabilistic scheduling model, project management, Project management, software development management, Programming, process simulation, software project, software process scheduling simulator, Delay, Software development management, Software design, Feedback, scheduling, Cost function, Software measurement, discrete event simulation, Testing, Software engineering, discrete-time simulator]
Pattern-oriented distributed system architectures
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This tutorial describes how to apply patterns and middleware frameworks to alleviate the complexity of developing software for distributed systems. These patterns and framework components have been used successfully by the presenter on production communication software projects at many commercial companies for telecommunication systems, network management for personal communication systems, electronic medical imaging systems, real-time avionics and aerospace systems, distributed interactive simulations, and automated stock trading.
[Real time systems, Production systems, object-oriented language, Project management, pattern-oriented distributed system architecture, Aerospace electronics, Telecommunication network management, Middleware, concurrency, software architecture, object-oriented design techniques, advanced operating system, Communication system software, reusable communication software, Computer architecture, software reusability, Software systems, operating systems (computers), object-oriented methods, Biomedical imaging, middleware]
Documenting software architectures: views and beyond
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This lecture maps the concepts and templates explored in this tutorial with well-known architectural prescriptions, including the 4+1 approach of the Rational Unified Process, the Siemens Four Views approach, and the ANSI/IEEE-1471-2000 recommended best practice for documenting architectures for software-intensive systems. The lecture concludes by re-capping the highlights of the tutorial, and asking for feedback.
[rational unified process, software development, system documentation, Documentation, software architecture documentation, Programming, Pattern recognition, software engineering institute, Computer science, Design engineering, software architecture, Siemens four view approach, Software architecture, Machine vision, Computer architecture, Robustness, Software engineering]
The impact of pair programming on student performance, perception and persistence
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This study examined the effectiveness of pair programming in four lecture sections of a large introductory programming course. We were particularly interested in assessing how the use of pair programming affects student performance and decisions to pursue computer science related majors. We found that students who used pair programming produced better programs, were more confident in their solutions, and enjoyed completing the assignments more than students who programmed alone. Moreover, pairing students were significantly more likely than non-pairing students to complete the course, and consequently to pass it. Among those who completed the course, pairers performed as well on the final exam as non-pairers, were significantly more likely to be registered as computer science related majors one year later, and to have taken subsequent programming courses. Our findings suggest that not only does pairing not compromise students' learning, but that it may enhance the quality of their programs and encourage them to pursue computer science degrees.
[Algorithm design and analysis, computer science education, Logic programming, Psychology, student perception, Logic design, Programming profession, Computer science, pair programming, student experiments, Collaboration, Keyboards, educational courses, Mice, introductory programming course, programming, student performance, Testing]
Requirements discovery during the testing of safety-critical software
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper describes the role of requirements discovery during the testing of a safety-critical software system. Analysis of problem reports generated by the integration and system testing of an embedded, safety-critical software system identified four common mechanisms for requirements discovery and resolution during testing: (1) Incomplete requirements, resolved by changes to the software, (2) Unexpected requirements interactions, resolved by changes to the operational procedures, (3) Requirements confusion by the testers, resolved by changes to the documentation, and (4) Requirements confusion by the testers, resolved by a determination that no change was needed The experience reported here confirms that requirements discovery during testing is frequently due to communication difficulties and subtle interface issues. The results also suggest that "false positive" problem reports from testing (in which the software behaves correctly but unexpectedly) provide a rich source of requirements information that can be used to reduce operational anomalies in critical systems.
[Software testing, System testing, program testing, requirements discovery, Laboratories, Documentation, safety-critical software, safety-critical software testing, Software safety, formal specification, Embedded software, Space technology, Propulsion, Software systems, Accidents]
Understanding and predicting effort in software projects
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
We set out to answer a question we were asked by software project management: how much effort remains to be spent on a specific software project and how will that effort be distributed over time? To answer this question we propose a model based on the concept that each modification to software may cause repairs at some later time and investigate its theoretical properties and application to several projects in Avaya to predict and plan development resource allocation. Our model presents a novel unified framework to investigate and predict effort, schedule, and defects of a software project. The results of applying the model confirm a fundamental relationship between the new feature and defect repair changes and demonstrate its predictive properties.
[Data analysis, Information resources, project management, software reliability, Project management, Predictive models, Information retrieval, software management, Application software, software change effort estimation, Software development management, software project schedule, Databases, resource allocation, defect prediction, Software quality, software cost estimation, Resource management, software project management]
Software engineering for large-scale multi-agent systems - SELMAS'2003
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Objects and agents are abstractions that exhibit Points of similarity, but the development of multi-agent systems (MASs) poses other challenges to Software Engineering since software agents are inherently more complex entities. In addition, a large MAS needs to satisfy multiple stringent requirements such as reliability, trustability, security, interoperability, scalability, reusability, and maintainability. This workshop brings together researchers and practitioners to discuss the current state of the art and the future research directions in software engineering for large-scale MASs. A particular interest is to understand those issues in the agent technology that make it difficult and/or improve the production of complex distributed systems.
[Multiagent systems, multi-agent systems, Collaborative software, Scalability, software reliability, distributed processing, interoperability, Application software, Security, software maintenance, software agents, Computer science, agent technology, security, software reusability, Software systems, distributed systems, Software agents, software engineering, Large-scale systems, Software engineering, multiagent systems]
ICSE workshop on remote analysis and measurement of software systems (RAMSS)
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The goal of this workshop is to bring together researchers and practitioners interested in exploring how the characteristics of today's area of computing (e.g., high connectivity substantial computing power for the average user, higher demand for and expectation of frequent software updates) can be leveraged to improve software quality and performance.
[quality assurance, Software quality, software process improvement, software system measurement, software engineering, software quality, software performance]
Automotive software engineering
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Information technology has become the driving force of innovation in many areas of technology and also in cars. Embedded software controls the functions of cars, supports and assists the driver and realizes systems for information and entertainment. Software in automobiles is today one of the great challenges for software engineering. On modem cars we find all issues of software systems in a nutshell. It is a challenge for software and systems engineering.
[Technological innovation, automobiles, Control systems, Automobiles, Information technology, Automotive engineering, Embedded software, embedded software systems, embedded systems, automotive engineering, Software systems, Modems, Systems engineering and theory, systems engineering, software engineering, automotive software engineering, Software engineering]
Research demonstrations and posters
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Automation plays an important role in software engineering. ne transition from research breakthroughs to usable tools is neither linear nor easy, and requires a fortunate combination of many elements difficult to identify and merge. In this hard transition process, research prototypes are an important step. They allow researchers to early experience ideas and results on real case studies, and to learn both difficulties in applying new theoretical results and limitations of the theory; and enable practitioners, who can see beyond the frontiers of current state of practice, to identify new technology for improving the software development process and spot advances applicable in the near future.
[Software prototyping, Java, Visualization, software development process, software verification, software development management, Programming, Application software, Embedded software, formal verification, research prototype, Prototypes, software process improvement, software engineering, Risk management, Software tools, Software engineering]
Architectural level risk assessment tool based on UML specifications
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Recent evidences indicate that most faults in software systems are found in only a few of a system's components [1]. The early identification of these components allows an organization to focus on defect detection activities on high risk components, for example by optimally allocating testing resources [2], or redesigning components that are likely to cause field failures. This paper presents a prototype tool called Architecture-level Risk Assessment Tool (ARAT) based on the risk assessment methodology presented in [3]. The ARAT provides risk assessment based on measures obtained from Unified Modeling Language (UML) artifacts [4]. This tool can be used in the design phase of the software development process. It estimates dynamic metrics [5] and automatically analyzes the quality of the architecture to produce architectural-level software risk assessment [3].
[software development process, Unified modeling language, Programming, prototype tool, software quality, architectural level risk assessment tool, software system, risk component, testing resource, software architecture, Prototypes, Computer architecture, specification languages, dynamic metrics, UML specification, software tools, Testing, Software prototyping, risk management, software development management, Risk analysis, component redesign, unified modeling language, design phase, detection activity, Software systems, Risk management, Resource management, system component, software metrics]
On the uniformity of software evolution patterns
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Preparations for Y2K reminded the software engineering community of the extent to which long-lived software systems are embedded in our daily environments. As systems are maintained and enhanced throughout their lifecycles they appear to follow generalized behaviors described by the laws of software evolution. Within this context, however, there is some question of how and why systems may evolve differently. The objective of this work is to answer the question: do systems follow a set of identifiable evolutionary patterns? In this paper we use software volatility to describe the lifecycle evolution of a portfolio of 23 software systems. We show by example that a vector of software volatility levels can represent lifecycle behavior of a software system. We further demonstrate that the portfolio's 23 software volatility vectors can be grouped into four distinguishable patterns. Thus, we show by example that there are different patterns of system lifecycle behavior, i.e. software evolution.
[Software maintenance, Production systems, program debugging, Costs, long-lived software systems, software evolution patterns, Programming profession, software lifecycle evolution, software volatility, software engineering community, Software systems, Computer industry, software engineering, Dynamic programming, Pattern analysis, Portfolios, Software engineering]
Software technology in an automotive company - major challenges
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The automotive industry is one of the sectors which has been affected significantly by the industrial software revolution during the last few years. Competitive challenges, which are of major relevance for the growing number of software-based automotive innovations, are presented by a dramatic growth in system complexity, rising time and cost pressure, and high quality demands. All these requirements lead to a number of software-related challenges which will be of significant competitive importance for the automotive industry in the future. Asides from the question of whether an automotive company decides to develop software in-house or whether this is carried out by suppliers, every automotive key player has to hold or to build up specific software competencies. The most important competence areas are the software development process, software quality management including supplier cooperation, the overall architecture of the in-vehicle software, as well as the ability to specify, to integrate and to test the system.
[Software testing, Technological innovation, software development process, Costs, automotive industry, Programming, automobile industry, Automotive engineering, Software development management, software quality management, in-vehicle software architecture, Software quality, Computer architecture, automotive electronics, software-based automotive innovation, Computer industry, software engineering, Quality management]
An effective layout adaptation technique for a graphical modeling tool
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Editing graphic models always entails layout problems. Inserting and deleting items requires tedious manual work for shifting existing items and rearranging the diagram layout. Hence, techniques that automatically expand a diagram when space is required for insertion and contract it when free space becomes available are highly desirable. Existing layout generation algorithms are no good solution for that problem: they may completely rearrange a diagram after an editing operation, while users want to preserve the overall visual appearance of a diagram. We have developed a technique which automatically expands or contracts a diagram layout when items are inserted or removed while preserving its overall shape, i.e. the positions of the items relative to each other Our technique has been implemented in a prototype tool. We are using it not just for simplifying editing, but primarily for implementing an aspect-oriented visualization concept.
[Visualization, Shape, editing operation, software prototyping, graphical modeling tool, prototype tool, Displays, layout adaptation technique, Floods, diagram layout generation algorithms, aspect-oriented visualization concept, Graphics, Layout, Heating, Prototypes, visual appearance, Control system synthesis, object-oriented methods, program visualisation, Contracts]
Quantifying the value of architecture design decisions: lessons from the field
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper outlines experiences with using economic criteria to make architecture design decisions. It briefly describes the CBAM (Cost Benefit Analysis Method) framework applied to estimate the value of architectural strategies in a NASA project, the ECS. This paper describes the practical difficulties and experiences in applying the method to a large realworld system. It concludes with some lessons learned from the experience.
[Availability, architecture design decision, Uncertainty, NASA, economic criteria, File servers, Cost benefit analysis, NASA project, software architecture, Engineering management, CBAM, Investments, cost-benefit analysis, Computer architecture, Marketing and sales, cost benefit analysis method, software cost estimation, Software engineering]
Improving test suites via operational abstraction
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper presents the operational difference technique for generating, augmenting, and minimizing test suites. The technique is analogous to structural code coverage techniques, but it operates in the semantic domain of program properties rather than the syntactic domain of program text. The operational difference technique automatically selects test cases; it assumes only the existence of a source of test cases. The technique dynamically generates operational abstractions (which describe observed behavior and are syntactically identical to formal specifications)from test suite executions. Test suites can be generated by adding cases until the operational abstraction stops changing. The resulting test suites are as small, and detect as many faults, as suites with 100% branch coverage, and are better at detecting certain common faults. This paper also presents the area and stacking techniques for comparing test suite generation strategies; these techniques avoid bias due to test suite size.
[Software testing, stacking techniques, System testing, program verification, program testing, Stacking, program text syntactic domain, test suite generation strategies, Formal specifications, programming language semantics, formal specification, formal specifications, Computer science, Runtime, operational abstractions, test suite executions, Automatic testing, Fault detection, Detectors, structural code coverage techniques, operational difference technique, Software tools]
Automated support for classifying software failure reports
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper proposes automated support for classifying reported software failures in order to facilitate prioritizing them and diagnosing their causes. A classification strategy is presented that involves the use of supervised and unsupervised pattern classification and multivariate visualization. These techniques are applied to profiles of failed executions in order to group together failures with the same or similar causes. The resulting classification is then used to assess the frequency and severity of failures caused by particular defects and to help diagnose those defects. The results of applying the proposed classification strategy to failures of three large subject programs are reported These results indicate that the strategy can be effective.
[Visualization, Estimation error, pattern classification, program debugging, supervised pattern classification, Terminology, Instruments, program diagnostics, Humans, software failure, Computer crashes, Frequency estimation, software maintenance, software diagnosis, software fault tolerance, program visualisation, multivariate visualization, unsupervised pattern classification]
New directions on agile methods: a comparative analysis
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Agile software development methods have caught the attention of software engineers and researchers worldwide. Scientific research is yet scarce. This paper reports results from a study, which aims to organize, analyze and make sense out of the dispersed field of agile software development methods. The comparative analysis is performed using the method's life-cycle coverage, project management support, type of practical guidance, fitness-for-use and empirical evidence as the analytical lenses. The results show that agile software development methods, without rationalization, cover certain/different phases of the software development life-cycle and most of them do not offer adequate support for project management. Yet, many methods still attempt to strive for universal solutions (as opposed to situation appropriate) and the empirical evidence is still very limited. Based on the results, new directions are suggested In principal, it is suggested to place emphasis on methodological quality - not method quantity.
[Project management, Programming, Application software, project management support, Information analysis, agile software development methods, comparative analysis, software development life-cycle, Feedback, Information processing, Computer industry, software engineering, Performance analysis, Internet, Lenses]
Sound methods and effective tools for engineering modeling and analysis
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Modeling and analysis is indispensable in engineering. To be safe and effective, a modeling method requires a language with a validated semantics; feature-rich, easy-to-use, dependable tools; and low engineering costs. Today we lack adequate means to develop such methods. We present a partial solution combining two techniques: formal methods for language design, and package-oriented programming for function and usability at low cost. We have evaluated the approach in an end-to-end experiment. We deployed an existing reliability method to NASA in a package-oriented tool and surveyed engineers to assess its usability. We formally specified, improved, and validated the language. To assess cost, we built a package-based tool for the new language. Our data show that the approach can enable cost-effective deployment of sound methods by effective tools.
[program verification, NASA, package-oriented programming, package-oriented tool, Reliability engineering, Educational institutions, programming language semantics, formal specification, Computer science, Design engineering, Acoustical engineering, formal methods, Packaging, Cost function, software tools, Functional programming, Usability]
ICSE 2003 workshop on software architectures for dependable systems
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This workshop summary gives a brief overview of a one-day workshop on "Software Architectures for Dependable Systems" held in conjunctions with ICSE 2003.
[software architecture, ICSE workshop, dependable system]
Evaluating individual contribution toward group software engineering projects
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
It is widely acknowledged that group or team projects are a staple of undergraduate and graduate software engineering courses. Such projects provide students with experiences that better prepare them for their careers, so teamwork is often required or strongly encouraged by accreditation agencies. While there are a multitude of educational benefits of group projects, they also pose considerable challenge in fairly and accurately discerning individual contribution for evaluation purposes. Issues, approaches, and best practices for evaluating individual contribution are presented from the perspectives of the University of Kentucky, University of Ottawa, University of Southern California, and others. The techniques utilized within a particular course generally are a mix of (1) the group mark is everybody's mark, (2) everybody reports what they personally did, (3) other group members report the relative contributions of other group members, (4) pop quizzes on project details, and (5) cross-validating with the results of individual work.
[computer science education, Engineering profession, Life testing, Programming, software engineering courses, Best practices, educational perspectives, group projects, teamwork, educational courses, groupware, Accreditation, software engineering, Teamwork, individual contribution, Software engineering]
Comparison of two component frameworks: the FIPA-compliant multi-agent system and the web-centric J2EE platform
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This work compares and contrasts two component frameworks: (1) the web-centric Java 2 Enterprise Edition (J2EE) framework and (2) the HPA-compliant multi-agent system (MAS). FIPA, the Foundation for Intelligent Physical Agents, provides specifications for agents and agent platforms. Both frameworks are component frameworks; servlets and Enterprise Java Beans (EJBs) in the case of J2EE and software agents in the case of MAS. Both frameworks are specification based. Both frameworks mandate platform responsibilities towards their respective component(s). We develop a framework with which to structure the comparison of the component frameworks. We apply this comparison structure in the context of a 'Data Access' scenario to application development in the respective component frameworks. Furthermore, we have prototyped this scenario in each of the two component frameworks. We conclude with a discussion of the benefits, drawbacks, and issues of developing new applications in each of the component frameworks.
[Availability, Java, Multiagent systems, multi-agent systems, intelligent physical agents, FIPA-compliant multiagent system, Java 2 Enterprise Edition framework, Web-centric J2EE platform, software maintainability, Enterprise Java Beans, Application software, Security, software maintenance, software agents, formal specification, Intelligent agent, Component architectures, Prototypes, Computer architecture, data access, servlets, Software agents, distributed object management]
About the development of a point of sale system: an experience report
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This report comprises some experiences, which were made during the development of a point of sale (POS) system. Specific about the project is the fact that it started as an attempt to develop customizable standard software, and then was restructured to deliver a unique project solution. This report details the situation before and after the restructuring, and discusses the experiences made through the ongoing development. These experiences relate mostly to three areas, which are: software project management, prototyping, and testing.
[Software testing, Java, Software prototyping, project management, program testing, software prototyping, software testing, Project management, software development management, Middleware, Insurance, Marketing and sales, Software standards, Standards development, software project management, Portfolios, point of sale system development, customizable standard software development]
Teaching contract programming concepts to future software engineers
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Current research in software engineering at Karlstad University is concentrated on non-formal software design methods with a focus on semantics. One goal is to produce methods, which may be applied in both industry and academia. In concrete terms, ideas from contract programming, including preand postconditions have been introduced into the first year curriculum. This paper presents results taken from three surveys of the same group of first-year students during their second semester, in an attempt to ascertain how well the students have internalised these and other programming concepts. The results show that the majority of the students are aware of the concepts but are still at various stages of understanding. A good understanding of terminology emerges as one key area of focus for future courses. The results are a reasonable reflection of reality, given the limited time in which the students are expected to absorb these ideas, and provide feedback for further integration and development of the related programming courses.
[computer science education, Terminology, software engineering course, Reflection, teaching, educational administrative data processing, contract programming concept, Computer science, Software design, Education, Feedback, educational curriculum, Writing, nonformal software design method, Concrete, software engineering, Contracts, programming, Software engineering]
Modular verification of software components in C
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
We present a new methodology for automatic verification of C programs against finite state machine specifications. Our approach is compositional, naturally enabling us to decompose the verification of large software systems into subproblems of manageable complexity. The decomposition reflects the modularity in the software design. We use weak simulation as the notion of conformance between the program and its specification. Following the abstract-verify-refine paradigm, our tool MAGIC first extracts a finite model from C source code using predicate abstraction and theorem proving. Subsequently, simulation is checked via a reduction to Boolean satisfiability. MAGIC is able to interface with several publicly available theorem provers and SAT solvers. We report experimental results with procedures from the Linux kernel and the OpenSSL toolkit.
[Visualization, Protocols, program verification, SAT solvers, Unified modeling language, Linux kernel, call graphs, Programming, computability, C language, formal specification, Boolean satisfiability, modular verification, Software design, MAGIC tool, theorem provers, theorem proving, software components, OpenSSL toolkit, Kernel, flow graphs, Linux, Automata, C programs, Software systems, Software engineering]
Report on the ICSE 2003 doctoral symposium
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The ICSE Doctoral Symposium is a forum for Ph.D. students to publicly discuss their research goals, methods, and results prior to completion of their doctorates. The Symposium aims to provide useful guidance for completion of the dissertation research and initiation of a research career. The Symposium and ICSE also provide an opportunity for student participants to interact with established researchers and others in the wider software engineering community.
[ICSE doctoral symposium, software engineering, research career, Ph.D. students, Software engineering]
Pluggable reflection: decoupling meta-interface and implementation
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Reflection remains a second-class citizen in current programming models, where it's assumed to be imperative and tightly bound to its implementation. In contrast, most object-oriented APIs allow interfaces to vary independently of their implementations. Components take this separation a step further by describing unforeseeable attributes-the key to pluggable third-party components. This paper describes how reflection can benefit from a similar evolutionary path.
[Java, Software prototyping, meta data, object-oriented programming, application program interfaces, Object oriented modeling, Programming, Educational institutions, pluggable reflection, Reflection, Printers, programming models, Information science, object-oriented API, meta-interface, Assembly, Testing]
Source Viewer 3D (sv3D) - a framework for software visualization
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Source Viewer 3D is a software visualization framework that uses a 3D metaphor to represent software system and analysis data. The 3D representation is based on the SeeSoft pixel metaphor. It extends the original metaphor by rendering the visualization in a 3D space. New, object-based manipulation methods and simultaneous alternative mappings are available to the user.
[Data analysis, Filtering, data analysis, Containers, user interaction, user interfaces, software visualization, software system, Computer science, source viewer 3D, Simultaneous localization and mapping, object-based manipulation method, Data visualization, data visualisation, Software systems, Cameras, software engineering, program visualisation, 3D metaphor, SeeSoft pixel metaphor, Software engineering, Engine cylinders]
Recovering documentation-to-source-code traceability links using latent semantic indexing
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
An information retrieval technique, latent semantic indexing, is used to automatically identify traceability links from system documentation to program source code. The results of two experiments to identify links in existing software systems (i.e., the LEDA library, and Albergate) are presented. These results are compared with other similar type experimental results of traceability link identification using different types of information retrieval techniques. The method presented proves to give good results by comparison and additionally it is a low cost, highly flexible method to apply with regards to preprocessing and/or parsing of the source code and documentation.
[Costs, indexing, latent semantic indexing, Natural languages, system documentation, traceability link identification, Documentation, information retrieval, Information retrieval, Information analysis, Computer science, information retrieval technique, Software libraries, Software systems, computer aided software engineering, program source code, Indexing, Software engineering]
The 3rd workshop on open source software engineering
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Building on the success of "Making Sense of the Bazaar" and "Meeting Challenges and Surviving Success" - the 1/sup st/ and 2/sup nd/ Workshops on Open Source Software Engineering (ICSE 2001 and ICSE 2002)-this workshop ("Taking Stock of the Bazaar') brings together researchers and practitioners for the purpose of discussing the diverse array of techniques - as well as supporting tools and social/organizational contexts which can be observed in the domain of open source software.
[organizational context, public domain software, workshop proceedings, software engineering, open source software engineering, software tools, Software engineering]
The grand challenge of trusted components
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Reusable components equipped with strict guarantees of quality can help reestablish software development on a stronger footing, by taking advantage of the scaling effect of reuse to justify the extra effort of ensuring impeccable quality. This discussion examines work intended to help the concept of Trusted Component brings its full potential to the software industry, along two complementary directions: a "low road" leading to qualification of existing components, and a "high road" aimed at the production of components with fully proved correctness properties.
[object-oriented programming, reusable components, software development, Roads, trusted components, Production, Programming, software reusability, Computer industry, software engineering, software quality, Qualifications]
Architecture, design, implementation
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The terms architecture, design, and implementation are typically used informally in partitioning software specifications into three coarse strata of abstraction. Yet these strata are not well-defined in either research or practice, causing miscommunication and needless debate. To remedy this problem we formalize the Intension and the Locality criteria, which imply that the distinction between architecture, design, and implementation is qualitative and not merely quantitative. We demonstrate that architectural styles are intensional and non-local; that design patterns are intensional and local; and that implementations are extensional and local.
[Algorithm design and analysis, software design theory, Unified modeling language, software specifications, Data structures, formal specification, Computer science, software architecture, Software design, Software architecture, Computer architecture, Software systems, Architecture description languages, Software engineering]
ViewPoints: meaningful relationships are difficult!
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The development of complex systems invariably involves many stakeholders who have different perspectives on the problem they are addressing, the system being developed, and the process by which it is being developed. The ViewPoints framework was devised to provide an organisational framework in which these different. perspectives, and their relationships, could be explicitly represented and analysed The framework acknowledges the inevitability of multiple inconsistent views, promotes separation of concerns, and encourages decentralised specification while providing support for integration through relationships and composition. In this paper, we reflect on the ViewPoints framework, current work and future research directions.
[Heart, Software prototyping, software development, Programming, viewpoints framework, formal specification, organisational framework, Software development management, Computer science, Design engineering, Engineering management, decentralised specification, Software engineering]
Patterns, frameworks, and middleware: their synergistic relationships
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The knowledge required to develop complex software has historically existed in programming folklore, the heads of experienced developers, or buried deep in the code. These locations are not ideal since the effort required to capture and evolve this knowledge is expensive, time-consuming, and error-prone. Many popular software modeling methods and tools address certain aspects of these problems by documenting how a system is designed However they only support limited portions of software development and do not articulate why a system is designed in a particular way, which complicates subsequent software reuse and evolution. Patterns, frameworks, and middleware are increasingly popular techniques for addressing key aspects of the challenges outlined above. Patterns codify reusable design expertise that provides time-proven solutions to commonly occurring software problems that arise in particular contexts and domains. Frameworks provide both a reusable product-line architecture [1] - guided by patterns - for a family of related applications and an integrated set of collaborating components that implement concrete realizations of the architecture. Middleware is reusable software that leverages patterns and frameworks to bridge the gap between the functional requirements of applications and the underlying operating systems, network protocol stacks, and databases. This paper presents an overview of patterns, frameworks, and middleware, describes how these technologies complement each other to enhance reuse and productivity, and then illustrates how they have been applied successfully in practice to improve the reusability and quality of complex software systems.
[databases, object-oriented programming, software development, Programming, software patterns, operating systems, software quality, Application software, Middleware, Bridges, software modeling methods, network protocol stacks, Operating systems, Collaboration, Computer architecture, software reusability, operating systems (computers), complex software systems, Concrete, reusable software, protocols, Software tools, Software reusability, middleware]
Usage-centered software engineering: an agile approach to integrating users, user interfaces, and usability into software engineering practice
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Usage-centered design is a systematic, model-driven approach to visual and interaction design with an established record of effectiveness in a wide variety of settings and areas of application. The tutorial introduces the models and methods of usage-centered design and explores the integration of usage-centered approaches into software engineering practice. Agile approaches to modeling will be emphasized, with the focus on use cases, which are central to usage-centered design and serve as a common thread throughout an integrated usage-centered software engineering process.
[agile modeling, User interfaces, software engineering, user interfaces, model-driven approach, Usability, usage-centered software engineering, Software engineering, user centred design, software usability]
Whole program path-based dynamic impact analysis
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Impact analysis, determining when a change in one part of a program affects other parts of the program, is time-consuming and problematic. Impact analysis is rarely used to predict the effects of a change, leaving maintainers to deal with consequences rather than working to a plan. Previous approaches to impact analysis involving analysis of call graphs, and static and dynamic slicing, exhibit several tradeoffs involving computational expense, precision, and safety, require access to source code, and require a relatively large amount of effort to re-apply as software evolves. This paper presents a new technique for impact analysis based on whole path profiling, that provides a different set of cost-benefits tradeoffs - a set which can potentially be beneficial for an important class of predictive impact analysis tasks. The paper presents the results of experiments that show that the technique can predict impact sets that are more accurate than those computed by call graph analysis, and more precise (relative to the behavior expressed in a program's profile) than those computed by static slicing.
[Software maintenance, Costs, path-based dynamic impact analysis, Instruments, static slicing, Inspection, Software safety, software maintenance, Computer science, cost-benefits tradeoffs, directed graphs, cost-benefit analysis, call graphs analysis, Signal processing, Software systems, whole path profiling, Performance analysis, software cost estimation, Signal analysis, program slicing, dynamic slicing]
Writing good software engineering research papers
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to XSE 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.
[technical writing, research paradigms, software engineering research papers, software profession, Statistics, Physics, Vehicles, Design engineering, Acoustical engineering, Abstracts, Writing, research project design, Concrete, Software standards, software engineering, research validation, Software engineering]
CLIME: An environment for constrained evolution demonstration description
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
We are building a software development environment that uses constraints to ensure the consistency of the different artifacts associated with software. This approach to software development makes the environment responsible for detecting most inconsistencies between software design, specifications, documentation, source code, and test cases. The environment provides facilities to ensure that these various dimensions remain consistent as the software is written and evolves. The environment works with the wide variety of artifacts typically associated with a large software system. It handles both the static and dynamic aspects of software. Moreover, it works incrementally so that consistency information is readily available to the developer as the system changes. The demonstration will show this environment and its capabilities.
[Software testing, constrained evolution, System testing, Software maintenance, software specification, project support environments, software design, Project management, software development management, Documentation, source code, Programming, consistency information, CLIME, Data mining, software system, software architecture, Databases, Computer architecture, software development environment, Software systems, programming environments]
Computing systems dependability
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Computer systems provide us with a wide range of services upon which we have come to depend. Many computers are embedded in more complex devices that we use such as automobiles, aircraft, appliances, and medical devices. Others are part of sophisticated systems that provide us with a variety of important facilities such as financial services, telecommunications, transportation systems, and energy production and distribution networks. Engineering systems to be as dependable as we require them to be is a significant challenge and requires a variety of analysis and development techniques. It is important that computer engineers, software engineers, project managers, and users understand the major elements of current technology in the field of dependability, yet this material tends to be unfamiliar to researchers and practitioners alike. Researchers are often concerned in one way or another with some aspect of what is mistakenly called software "reliability". All practitioners are concerned with the "reliability" of the software that they produce but researchers and practitioners tend not to understand fully the broader impact of their work. A lot of research, such as that on testing, is concerned directly with software dependability. Understanding dependability more fully allows researchers to be more effective. Similarly, practitioners can direct their efforts during development more effectively if they have a better understanding of dependability.
[Embedded computing, Production systems, software specification, fault tolerance, software reliability, Transportation, Reliability engineering, Telecommunication computing, Automobiles, Aircraft propulsion, formal specification, software fault tolerance, Aerospace engineering, Home appliances, formal verification, systems engineering, software dependability, software engineering, systems dependability, Power engineering and energy]
Introducing software engineering by means of extreme programming
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper reports on experience from teaching basic software engineering concepts by using Extreme Programming in a second year undergraduate course taken by 107 students. We describe how this course fits into a wider programme on software engineering and technology and report our experience from running and improving the course. Particularly important aspects of our setup includes team coaching (by older students) and "team-in-one-room". Our experience so far is very positive and we see that students get a good basic understanding of the important concepts in software engineering, rooted in their own practical experience.
[Educational institutions, team coaching, teaching, undergraduate course, Vehicles, Automotive engineering, Computer science, extreme programming, Design engineering, Education, Feedback, educational courses, software engineering, Springs, Software engineering, Testing]
Cost estimation for web applications
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
In this paper, we investigate the application of the COBRA/spl trade/ method (Cost Estimation, Benchmarking, and Risk Assessment) in a new application domain, the area of web development. COBRA combines expert knowledge with data on a small number of projects to develop cost estimation models, which can also be used for risk analysis and benchmarking purposes. We modified and applied the method to the web applications of a small Australian company, specializing in web development. In this paper we present the modifications made to the COBRA method and results of applying the method In our study, using data on twelve web applications, the estimates derived from our Web-COBRA model showed a Mean Magnitude of Relative Error (MMRE) of 0.17. This result significantly outperformed expert estimates from Allette Systems (MMRE 0.37). A result comparable to Web-COBRA was obtained when applying ordinary least squares regression with size in terms of Web Objects as an independent variable (MMRE 0.23).
[Costs, COBRA application, Buildings, Software performance, regression analysis, Web applications, mean magnitude of relative error, Application software, Web-COBRA model, Trademarks, Internet, software cost estimation, Australia, least squares regression, Analysis of variance, Regression tree analysis, distributed object management, Software engineering]
Fifth international workshop on economics-driven software engineering research (EDSER-5) "The search for value in engineering decisions"
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The series of EDSER workshops are a unique forum to discuss and advance the state-of-the-art research and practice in economics driven software engineering. The EDSER-5 will bring together leading researchers and practitioners to provide further understanding how economic and business considerations affect - and should affect - software engineering decisions.
[Economics, economics, economics driven software engineering research, decision making, software engineering, commerce, Software engineering, Business, software engineering decisions]
Addressing the challenges of software industry globalization: the workshop on global software development
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The goal of this workshop is to provide an opportunity for researchers and industry practitioners to explore both the state-of-the art and the state-of-the-practice in global software development (GSD). Increased globalization of software development creates software engineering challenges due to the impact of temporal, geographical and cultural differences, and requires development of techniques and technologies to address these issues. The workshop will foster interaction between practitioners and researchers and help grow a community of interest in this area. Practitioners experiencing challenges in GSD will share their concerns and successful solutions and learn from research about current investigations. Researchers addressing GSD will gain a better understanding of the key issues facing practitioners and share their work in progress with others in the field.
[workshop description, software industry, global software development, DP industry, software development management, Computer industry, software engineering, globalisation, Software development management]
Demonstration of AGENDA tool set for testing relational database applications
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Database systems play an important role in nearly every modern organization, yet relatively little research effort has focused on how to test them. AGENDA, A (test) GENerator for Database Applications, is a research prototype tool set for testing DB application programs. In testing such applications, the states of the database before and after execution play an important role, along with the user's input and system output. AGENDA components populate the database, generate inputs, and check aspects of the correctness of output and new DB state.
[relational database application, test data, program testing, research prototype, software prototyping, software testing, Relational databases, software tools, AGENDA tool set, relational databases, Testing]
Experiences on defining and evaluating an adapted review process
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper presents our experiences with the introduction of a review process for software requirements documents in a development project of the Passenger Car Development business unit (PCD) of DaimlerChrylser AG. As software quality management and quality assurance must be carefully integrated into existing company processes or business cultures, we had to substantially adapt the review process to the specific needs of the above project and its context characteristics. The chief difference to the traditional procedure for conducting reviews as, for example, set out in [1] or [2] is that our review process has a strong workshop character with an emphasis on one of the most crucial success factors for reviews: sufficient time for the individual preparation of the reviewers. We also describe the phase of analyzing the effort and defect data captured with respect to cost benefit aspects and the quality of the reviewed documents in relation to each other.
[Project management, Control systems, software management, automobile industry, software quality, cost benefit analysis, formal specification, Automotive engineering, Software development management, software quality management, Quality assurance, Passenger Car Development business unit, Software quality, Software systems, Meeting planning, Hardware, software requirement specification, Quality management]
Assessing test-driven development at IBM
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
In a software development group of IBM Retail Store Solutions, we built a non-trivial software system based on a stable standard specification using a disciplined, rigorous unit testing and build approach based on the test-driven development (TDD) practice. Using this practice, we reduced our defect rate by about 50 percent compared to a similar system that was built using an ad-hoc unit testing approach. The project completed on time with minimal development productivity impact. Additionally, the suite of automated unit test cases created via TDD is a reusable and extendable asset that will continue to improve quality over the lifetime of the software system. The test suite will be the basis for quality checks and will serve as a quality contract between all members of the team.
[Software testing, Productivity, automated unit testing, System testing, project management, program testing, ad-hoc unit testing, Life testing, software development management, Programming, software quality, software development group, formal specification, formal verification, software system quality, Automatic testing, test-driven development, Software systems, Software standards, Standards development, Contracts, IBM retail store solution]
Using benchmarking to advance research: a challenge to software engineering
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Benchmarks have been used in computer science to compare the performance of computer systems, information retrieval algorithms, databases, and many other technologies. The creation and widespread use of a benchmark within a research area is frequently accompanied by rapid technical progress and community building. These observations have led us to formulate a theory of benchmarking within scientific disciplines. Based on this theory, we challenge software engineering research to become more scientific and cohesive by working as a community to define benchmarks. In support of this challenge, we present a case study of the reverse engineering community, where we have successfully used benchmarks to advance the state of research.
[benchmarking, software engineering research, Software algorithms, Reverse engineering, information retrieval algorithms, Information retrieval, reverse engineering, reverse engineering community, Guidelines, Computer science, Computer languages, computer system performance, Databases, community building, Collaboration, software engineering, Books, benchmark testing, technical progress, Software engineering]
Fault-tolerance in a distributed management system: a case study
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Our case study provides the most important conceptual lessons learned from the implementation of a Distributed Telecommunication Management System (DTMS), which controls a networked voice communication system. Major requirements for the DTMS are fault-tolerance against site or network failures, transactional safety, and reliable persistence. In order to provide distribution and persistence both transparently and fault-tolerant we introduce a two-layer architecture facilitating an asynchronous replication algorithm. Among the lessons learned are: component based software engineering poses a significant initial overhead but is worth it in the long term; a fault-tolerant naming service is a key requirement for fail-safe distribution; the reasonable granularity for persistence and concurrency control is one whole object; asynchronous replication on the database layer is superior to synchronous replication on the instance level in terms of robustness and consistency; semi-structured persistence with XML has drawbacks regarding consistency, performance and convenience; in contrast to an arbitrarily meshed object model, a accentuated hierarchical structure is more robust and feasible; a query engine has to provide a means for navigation through the object model; finally the propagation of deletion operation becomes more complex in an object-oriented model. By incorporating these lessons learned we are well underway to provide a highly available, distributed platform for persistent object systems.
[object-oriented programming, networked voice communication system, Object oriented modeling, fault-tolerance, Communication system control, Control systems, Telecommunication network management, Telecommunication control, two-layer architecture, software fault tolerance, Robust control, Fault tolerance, distributed telecommunication management system, asynchronous replication algorithm, Fault tolerant systems, XML, distributed databases, Safety, voice communication, object oriented programming, Telecommunication network reliability, distributed object management, naming service, component based software engineering]
Efficient authoring of software documentation using RaPiD7
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper presents a method developed in Nokia, for efficient document authoring in software development projects. The method is called RaPiD7 (Rapid Production of Documentation, 7 steps). The method improves the traditional approach for document authoring in which the work is typically started by informal initiative, and the actual writing of a document is a task performed by a single individual. Traditional document authoring usually relies on inspections to verify the quality of the documentation. RaPiD7 addresses the document authoring problem by getting people involved in the documentation work earlier as a team in order to guarantee quality, calendar time efficiency, commitment and improved communication. This paper also compares RaPiD7 with some other similar approaches and presents results from a large-scale experiment with RaPiD7.
[document handling, Life testing, Documentation, Calendars, Inspection, Programming, software document authoring, authoring systems, software development projects, rapid production of documentation, Manufacturing processes, Software quality, Production, Writing, software engineering, Large-scale systems]
On the supervision and assessment of part-time postgraduate software engineering projects
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper describes existing practices in the supervision and assessment of projects undertaken by part-time, postgraduate students in Software Engineering. It considers this aspect of the learning experience, and the educational issues raised, in the context of existing literature-much of which is focussed upon the experience of full-time, undergraduate students. The importance of these issues will increase with the popularity of part-time study at a postgraduate level; the paper presents a set of guidelines for project supervision and assessment.
[Heart, Educational programs, computer science education, software engineering course, International collaboration, Reflection, educational administrative data processing, project assessment, Guidelines, Computer science, project supervision, educational issues, part-time postgraduate students, educational courses, Intellectual property, Computer industry, software engineering, Computer science education, Software engineering]
Design pattern rationale graphs: linking design to source
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
A developer attempting to evolve a system in which design patterns have been applied can benefit from knowing which code implements which design pattern. For instance, the developer may be able to understand the purpose, or to assess the flexibility of the code, more quickly. The degree to which the developer benefits depends upon their understanding of the pattern. Achieving an in-depth understanding of even a simple pattern can be difficult as pattern descriptions span several pages of text, and discuss interrelated design concepts and choices. To enable a developer to effectively trace the design goals associated with a pattern to and from source, we have developed the Design Pattern Rationale Graph (DPRG) approach and associated tool. A DPRG makes explicit the relationships between design concepts in a design pattern, provides a graphical representation of the design pattern text, and supports the linking of those concepts to implementing code. In this paper, we introduce the DPRG approach and tool, and present case studies to show that a DPRG can, at low-cost, help a developer identify design goals in a pattern, and can improve a developer's confidence about how those goals are realized in a code base.
[object-oriented programming, design pattern rationale graphs, Buildings, graph theory, Abstracts, Documentation, graphical representation, Software systems, Encoding, Pattern recognition, Joining processes, Pattern analysis]
Effective experience repositories for software engineering
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Software development and acquisition require knowledge and experience in many areas of software engineering. Experience helps people to make decisions under uncertainty, and to find better compromises. Experience-based process improvement considers experience as a prerequisite for competent behavior in software development. There is usually a repository to store experiences and to make it available for reuse. At DaimlerChrysler, we have been building those repositories for more than five years. We learned to concentrate on certain properties that seem to be key success factors for experience repositories. During our experience-based work in business units, five key quality aspects have been identified that determine the chances for success of an experience repository. The quality criteria can be used to analyze a given repository; or they can be applied to guide the construction of more effective experience repositories.
[Uncertainty, software development, experience repository, software reuse, ISO standards, Programming, Production facilities, process improvement, Constraint optimization, Coordinate measuring machines, Databases, software process improvement, software reusability, SPICE, software engineering, Risk management, Software engineering]
New software engineering faculty symposium (NSEFS 03)
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The challenge of becoming a new Assistant Professor at a research university is one that most people accept with enthusiasm and energy, but also with some trepidation. One's tenure track appointment is a signal accomplishment and the culmination of many years of hard work. But, simultaneously, it signals the start of another round of hard work and challenges that are new and often unfamiliar. Among these new challenges are establishing an independent research program, learning how to teach, learning how to mentor, and deciding how to balance career and personal life. For a new software engineering faculty member there are additional challenges such as obtaining financial support, supervising the development of software systems, and dealing with skeptical faculty colleagues from other disciplines. The purpose of NSEFS 03 is to help new software engineering faculty members to feel more comfortable and confident in dealing with these many challenges. NSEFS 03 will feature presentations by leading academic software engineering researchers who will provide advice and guidance based upon their personal experiences and insights into the contemporary academic software engineering community. There will also be ample time for informal and small group interactions with the presenters and other attendees. NSEFS 03 is designed to be a service to those who are within two years of (either before or after) their initial tenure track academic appointment, although the symposium may be of significant interest and value to others.
[software engineering faculty, research university, software engineering, teaching, educational institutions, Software engineering]
Component rank: relative significance rank for software component search
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Collections of already developed programs are important resources for efficient development of reliable software systems. In this paper, we propose a novel method of ranking software components, called Component Rank, based on analyzing actual use relations among the components and propagating the significance through the use relations. We have developed a component-rank computation system, and applied it to various Java programs. The result is promising such that non-specific and generic components are ranked high. Using the Component Rank system as a core part, we are currently developing Software Product Archiving, analyzing, and Retrieving System named SPARS.
[Java, software component search, object-oriented programming, software reliability, directed graphs, software product archiving, Software systems, reliable software systems, Java programs, component-rank computation system]
DRT: A tool for design recovery of interactive graphical applications
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Nowadays, the majority of productivity applications are interactive and graphical in nature. In this demonstration, we explore the possibility of taking advantage of these two characteristics in a design recovery tool. Specifically, the fact that an application is interactive means that we can identify distinct execution bursts corresponding closely to "actions" performed by the user. The fact that the application is graphical means that we can describe those actions visually from a fragment of the application display itself. Combining these two ideas, we obtain an explicit mapping from high-level actions performed by a user (similar to use case scenarios/specification fragments) to their low-level implementation. This mapping can be used for design recovery of interactive graphical applications. We demonstrate our approach using L/sub Y/X, a scientific word processor.
[Productivity, productivity application, interactive graphical application, mapping, scientific word processor, graphical user interfaces, interactive systems, Displays, software engineering, design recovery tool, program visualisation, word processing]
ICSE 2003 workshop on software engineering for high assurance systems: synergies between process, product, and profiling (SEHAS 2003)
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
A critical issue in software engineering is how to construct high assurance software systems, i.e., software Systems where compelling evidence is required that the system delivers its services in a manner satisfying critical properties, such as safety and security. This two-day XSE workshop, the third in a series of workshops on high assurance systems, will provide a forum for researchers and practitioners to exchange ideas and experiences relevant to the development of software for aerospace systems, medical systems, systems controlling nuclear power plants, and other critical systems. Participants of the SEHAS 2003 workshop will explore the opportunities for, and benefits of, synergies between three important themes-product, process, and profiling-each theme reflecting an important aspect of software development for high assurance systems.
[software development, workshop themes, software engineering, assurance system, Software engineering]
Cybersecurity
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
As more business activities are being automated and an increasing number of computers are being used to store sensitive information, the need for secure computer systems becomes more apparent. This need is even more apparent as systems and applications are being distributed and accessed via an insecure network, such as the Internet. The Internet itself has become critical for governments, companies, financial institutions, and millions of everyday users. Networks of computers support a multitude of activities whose loss would all but cripple these organizations. As a consequence, cybersecurity issues have become national security issues. Protecting the Internet is a difficult task. Cybersecurity can be obtained only through systematic development; it can not be achieved through haphazard seat-of-the-pants methods. Applying software engineering techniques to the problem is a step in the right direction. However, software engineers need to be aware of the risks and security issues associated with the design, development, and deployment of network-based software. This paper introduces some known threats to cybersecurity, categorizes the threats, and analyzes protection mechanisms and techniques for countering the threats. Approaches to prevent, detect, and respond to cyber attacks are also discussed.
[financial institutions, network-based software, Design engineering, authorisation, computer crime, software engineering techniques, distributed systems, Computer networks, software engineering, access control, IP networks, Computer security, Protection, National security, national security, cyber attacks, Government, business activities, Application software, cybersecurity, haphazard seat-of-the-pants methods, message authentication, Internet, Software engineering, secure computer system]
Internet security and intrusion detection
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The tutorial presents the principal attack techniques that are used in the Internet today and possible countermeasures. In particular, intrusion detection techniques are analyzed in detail. This tutorial mixes a practical character with a discussion of the current research in the field. Participants will learn how the security of networks is violated and how such violations can be detected and prevented.
[security of data, transport protocols, Internet security, Intrusion detection, intrusion detection, Internet, ubiquitous computing, TCP/IP network, network-based software]
Practical software measurement
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This tutorial provides a one-day overview of Practical Software Measurement (PSM). The measurement approach focuses on satisfying the information needs of project managers. Innovative elements of PSM include the measurement process model and the measurement information model. Rather than discussing specific numerical techniques in detail, this course emphasizes basic concepts.
[Performance evaluation, measurement process model, project management, Decision making, Project management, ISO standards, measurement information model, numerical techniques, practical software measurement, IEC standards, decision making, software engineering, Performance analysis, Software measurement, Books, Capability maturity model, Software engineering]
JIVE: visualizing Java in action demonstration description
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Dynamic software visualization should provide a programmer with insights as to what the program is doing. Most current dynamic visualizations either use program traces to show information about prior runs, slow the program down substantially, show only minimal information, or force the programmer to indicate when to turn visualizations on or off. We have developed a dynamic Java visualizer that provides a view of a program in action with low enough overhead that it can be used almost all the time by programmers to understand what their program is doing while it is doing it.
[Java, trace data, dynamic software visualization, Displays, JIVE, Yarn, software system, Programming profession, Computer science, Feedback, Data visualization, Packaging, Software systems, Libraries, program visualisation]
eXtreme programming at universities - an educational perspective
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
To address the problems of traditional software development, recent years have shown the introduction of more light-weight or "agile" development processes (eXtreme Programming being the most prominent one). These processes are intended to support early and quick production of working code by structuring the development into small release cycles and focus on continual interaction between developers and customers. As such software development processes become more popular there is a growing demand from industry to introduce agile development practices in tertiary education. This is not a straightforward task as the corresponding practices may run counter to educational goals or may not be adjusted easily to a learning environment. In this paper, we discuss some of these issues and reflect on the problems of teaching agile processes in tertiary education.
[tertiary education, computer science education, software development, Educational institutions, software engineering, teaching, eXtreme programming, educational perspective, Programming profession, programming, agile processes]
ICSE workshop on dynamic analysis (WODA 2003)
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Dynamic analysis of software systems has long proven to be a practical approach to gain understanding of the operational behavior of the system. This workshop will bring together researchers in the field of dynamic analysis to discuss the breadth of the field, order the field along logical dimensions, expose common issues and approaches, and stimulate synergistic collaborations among the participants.
[Software testing, program testing, software system dynamic analysis, static analysis, system monitoring, software engineering, data description, data collection, Software engineering]
/spl chi/Chek: A model checker for multi-valued reasoning
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper describes our multi-valued symbolic model-checker XChek. XChek is a generalization of an existing symbolic model-checking algorithm for a multi-valued extension of the temporal logic CTL. Multi-valued model-checking supports reasoning with values other than just TRUE and FALSE.
[Java, Visualization, Lattices, temporal logic, Specification languages, Boolean algebra, Application software, Engines, quasiBoolean algebra, multivalued reasoning, formal verification, multivalued logic, XML, symbolic model-checking algorithm, Packaging, software engineering, Performance analysis, Logic]
An analysis of the fault correction process in a large-scale SDL production model
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Improvements in the software development process depend on our ability to collect and analyze data drawn from various phases of the development life cycle. Our design metrics research team was presented with a largescale SDL production model plus the accompanying problem reports that began in the requirements phase of development. The goal of this research was to identify and measure the occurrences of faults and the efficiency of their removal by development phase in order to target software development process improvement strategies. The number and severity of problem reports were tracked by development phase and fault class. The efficiency of the fault removal process using a variety of detection methods was measured Through our analysis of the system data, the study confirms that catching faults in the phase of origin is an important goal. The faults that migrated to future phases are on average eight times more costly to repair. The study also confirms that upstream faults are the most critical faults and more importantly it identifies detailed design as the major contributor of faults, including critical faults.
[software development process, Phase measurement, Data analysis, Target tracking, Programming, Fault location, formal specification, software fault tolerance, Fault diagnosis, software development process improvement, design metrics research team, Fault detection, requirement phase, Production, software process improvement, specification languages, software fault correction, SDL production model, Large-scale systems, Software measurement, software metrics]
An empirical study of an informal knowledge repository in a medium-sized software consulting company
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Numerous studies have been conducted on design and architecture of knowledge repositories. This paper addresses the need for looking at practices where knowledge repositories are actually used in concrete work situations. This insight should be used when developing knowledge repositories in the future. Through methods inspired by ethnography this paper investigates how an unstructured knowledge repository is used for different purposes by software developers and managers in a medium-sized software consulting company. The repository is a part of the company's knowledge management tool suite on the Intranet. We found five distinct ways of using the tool, from solving specific technical problems to getting an overview of competence in the company. We highlight the importance of informal organization and the social integration of the tool in the daily work practices of the company.
[knowledge repositories, medium-sized software consulting company, DP industry, Companies, software developers, social integration, Knowledge management, Telecommunications, knowledge management, intranets, company knowledge management tool, Software development management, intranet, Communication industry, Computer architecture, Computer industry, Concrete, software engineering, informal organization, Informatics, consultancies, organisational aspects, ethnography, Software engineering]
The deployer's problem: configuring application servers for performance and reliability
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Frameworks such as J2EE are designed to simplify the process of developing enterprise applications by handling much of the complexity of concurrency, transaction, and persistence management. An application server that supports such a framework implements these concerns, freeing the application developer to focus on the task of implementing the business logic aspect of the application. In such frameworks, the deployer, the individual(s) who configures the application server to manage concurrency, transaction and persistence correctly and efficiently, plays a central role. A deployer has few tools to assist with performing this complicated task. Incorrect configuration can lead to application failure or severe underperformance. We outline the problems facing the deployer of applications, present a methodology that can assist the programmer with the task of configuring application servers, and present two case studies that validate the usefulness of our methodology.
[Concurrent computing, configuration management, application server, Java, software reliability, transaction management, business logic, J2EE, concurrency management, Internet, Logic, Programming profession]
Using process technology to control and coordinate software adaptation
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
We have developed an infrastructure for end-to-end run-time monitoring, behavior/performance analysis, and dynamic adaptation of distributed software. This infrastructure is primarily targeted to pre-existing systems and thus operates outside the target application, without making assumptions about the target's implementation, internal communication/computation mechanisms, source code availability, etc. This paper assumes the existence of the monitoring and analysis components, presented elsewhere, and focuses on the mechanisms used to control and coordinate possibly complex repairs/reconfigurations to the target system. These mechanisms require lower-level effectors somehow attached to the target system, so we briefly sketch one such facility (elaborated elsewhere). Our main contribution is the model, architecture, and implementation of Workflakes, the decentralized process engine we use to tailor, control, coordinate, etc. a cohort of such effectors. We have validated the Workflakes approach with case studies in several application domains. Due to space restrictions we concentrate primarily on one case study, briefly discuss a second, and only sketch others.
[Availability, Java, Process control, Communication system control, Software performance, Control systems, hardware description languages, Application software, Engines, end-to-end run-time monitoring, software architecture, Runtime, software process improvement, software process technology, distributed software dynamic adaptation, architecture description language, Performance analysis, decentralized process engine, Monitoring, distributed object management, Workflakes architecture]
The software engineering impacts of cultural factors on multi-cultural software development teams
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This paper is based on our experiences in trying to apply software engineering practices to development projects staffed by developers from three distinct cultures; Japan, India, and the United States. The development of commercial software products has always been difficult. The standard balancing act that occurs between features, schedules, and resources is at the core of the difficulty. We found that cultural differences also had a large impact on our software engineering work Much has been written and said about software engineering methods that can be applied to development projects to reduce and control these core difficulties. Methods that were thought to be "best practices" turned out to be ineffective or very difficult to implement. Our understanding of the possible root causes for these difficulties greatly increased when we began to study some of the cultural dynamics within the team. This paper describes our observations in terms of how these cultural factors impacted the software engineering techniques used on the projects.
[Laboratories, Project management, Programming, software development project, software engineering impact, Cultural differences, cultural factor, multicultural software development team, commercial software product, software engineering, Books, Software measurement, socio-economic effects, Kernel, Contracts, Power generation, Software engineering]
SE Pioneers Symposium
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The Pioneers' Symposium provides a forum in which Software Engineering's next generation of researchers, faculty, and practitioners have the opportunity to interact with and learn from some of the field's pioneering thinkers. The invited speakers are individuals whose works are not only seminal, but have demonstrated enduring value to both the research and development communities. Attendees will have the opportunity to understand and discuss the speakers' views on (1) what software engineering professionals should know, (2) thinking critically about the field, and (2) what it takes to do work of lasting value.
[professional aspects, research and development, pioneer symposium, software engineering, societies, Research and development, software engineering professionals, Software engineering]
The Vienna component framework enabling composition across component models
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The Vienna Component Framework (VCF) supports the interoperability and composability of components across different component models, a facility that is lacking in existing component models. The VCF presents a unified component model-implemented by a facade component to the application programmer The programmer may write new components by composing components from different component models, accessed through the VCF The model supports common component features, namely, methods, properties, and events. To support a component model within the VCF a plugin component is needed that provides access to the component model. The paper presents the VCF's design, implementation issues, and evaluation. Performance measurements of VCF implementations of COM, Enterprise JavaBeans, CORBA distributed objects, and JavaBeans show that the overhead of accessing components through the VCF is negligible for distributed components.
[Measurement, open systems, application program interfaces, application programmer, Code standards, distributed components, Web and internet services, Abstracts, Cost function, Software standards, software engineering, distributed object management, COM, Java, meta data, object-oriented programming, interoperability, Vienna component framework, Enterprise JavaBeans, Programming profession, JavaBeans, CORBA distributed objects, Writing, unified component model, Software engineering, facade component]
3/sup rd/ international workshop on adoption-centric software engineering ACSE 2003
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The key objective of this workshop is to explore innovative approaches to the adoption of software engineering tools and practices-in particular by embedding them in extensions of Commercial Off-The-Shelf (COTS) software products and/or middleware technologies. The workshop aims to advance the understanding and evaluation of adoption of software engineering tools and practices by bringing together researchers and practitioners who investigate novel solutions to software engineering adoption issues.
[software engineering, software tools, adoption-centric software engineering, middleware technology, commercial off-the shell product, Software engineering]
FEAT a tool for locating, describing, and analyzing concerns in source code
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Developers working on existing programs repeatedly have to address concerns, or aspects, that are not well modularized in the source code comprising a system. In such cases, a developer has to first locate the implementation of the concern in the source code comprising the system, and then document the concern sufficiently to be able to understand it and perform the actual change task.
[Java, cross-reference database, integrated software development, graphical user interfaces, Buildings, Scattering, source code, Programming, Containers, reverse engineering, code browsers, lexical searching tool, Software development management, address concern, Computer science, FEAT, Databases, integrated software, Computer architecture, software engineering, Graphical user interfaces]
Second international workshop on from SofTware requirements to architectures (STRAW'03)
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The Second International Workshop on From SofTware Requirements to Architectures (STRAW'03) was held in Portland, Oregon, USA on 9 May 2003 just after the Twenty-Fifth International Conference on Software Engineering (ICSE'03). This brief paper outlines the motivation, goals, and organization of the workshop.
[software architecture, workshop motivation, systems analysis, software requirement engineering, Software requirements and specifications, formal specification, System analysis and design]
Trustworthy and sustainable operations in marine environments
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
In order to address challenges and opportunities of engineering information systems for network-centric warfare, we have developed a prototype for trustworthy and sustainable operations in marine environments (TWOSOME). The system developed addressed qualities such as information fusion, target acquisition, and self-organization in open computational systems; comprised of distributed services. As such, the system prototype executes on a service-oriented layered architecture for communicating entities (SOLACE) and, furthermore, different perspectives of the prototype are visualized by means of a distributed interaction system for complex entity relation networks (DISCERN).
[system prototype, Military computing, open computational systems, software prototyping, sensor fusion, distributed interaction system, entity relation network, Design engineering, Intelligent networks, Prototypes, Computer networks, engineering information systems, Marine technology, target acquisition, Software prototyping, communicating entity, information fusion, Instruments, marine systems, network-centric warfare, distributed service, trustworthy operation, Intelligent sensors, marine environment, sustainable operation, service-oriented layered architecture, Systems engineering and theory]
End-user software engineering with assertions in the spreadsheet paradigm
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
There has been little research on end-user program development beyond the activity of programming. Devising ways to address additional activities related to end-user program development may be critical, however, because research shows that a large proportion of the programs written by end users contain faults. Toward this end, we have been working on ways to provide formal "software engineering" methodologies to end-user programmers. This paper describes an approach we have developed for supporting assertions in end-user software, focusing on the spreadsheet paradigm. We also report the results of a controlled experiment, with 59 end-user subjects, to investigate the usefulness of this approach. Our results show that the end users were able to use the assertions to reason about their spreadsheets, and that doing so was tied to both greater correctness and greater efficiency.
[Software testing, Software maintenance, program debugging, program verification, Reliability engineering, spreadsheet programs, end-user program development, Environmental economics, formal specification, Programming profession, Acoustical engineering, Automatic testing, Web pages, software engineering, Cognitive science, spreadsheet paradigm, Software engineering]
From scenarios to code: an air traffic control case study
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Two high profile workshops at OOPSLA and ICSE, an IEEE Computer article by David Harel and a growing number of research papers have all suggested algorithms that translate scenarios of a system's behavior into state machines. One of the uses of such algorithms is in the transition from requirements scenarios to component design. To date, however, most efforts have concentrated on the algorithmic details of the proposed translations. Less work has been done on evaluating these algorithms on a realistic case study. In this paper, we do exactly that for the algorithm presented in [10]. Our study is a component of an air traffic advisory system developed at NASA Ames Research Center.
[Algorithm design and analysis, Computer aided software engineering, message passing, object-oriented programming, Conferences, NASA, Unified modeling language, component based software design, Control systems, Air traffic control, formal specification, air traffic control, software architecture, Software design, NASA Ames Research Center, UML, specification languages, Automatic control, state machine, Concrete, specification language]
Why can't they create architecture models like "Developer X"? An experience report
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
A large financial company, struggling with legacy systems that did not interoperate, performed a pilot project to teach software architecture to an enthusiastic application development team. Experienced mentors, including the author, worked with the application team for seven months to complete their engineering goal successfully. However, the mentors were unsuccessful in their attempt to train any of the six members of the application team to create architecture models on their own, though they were able to create them collaboratively with the mentors. This surprising result is due to the application team's strong preference for concrete artifacts over abstract ones. Even more surprising, an application developer from a different project, "Developer X\
[Documentation, Programming, teaching, pilot project, Application software, Employee welfare, Computer science, software architecture, architecture model, application development team, Software architecture, Education, Computer architecture, Collaborative work, Concrete, Developer X]
Constructing test suites for interaction testing
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Software system faults are often caused by unexpected interactions among components. Yet the size of a test suite required to test all possible combinations of interactions can be prohibitive in even a moderately sized project. Instead, we may use pairwise or t-way testing to provide a guarantee that all pairs or t-way combinations of components are tested together This concept draws on methods used in statistical testing for manufacturing and has been extended to software system testing. A covering array, CA(N; t, k, v), is an N/spl times/k array on v symbols such that every N x t sub-array contains all ordered subsets from v symbols of size t at least once. The properties of these objects, however do not necessarily satisfy real software testing needs. Instead we examine a less studied object, the mixed level covering array and propose a new object, the variable strength covering array, which provides a more robust environment for software interaction testing. Initial results are presented suggesting that heuristic search techniques are more effective than some of the known greedy methods for finding smaller sized test suites. We present a discussion of an integrated approach for finding covering arrays and discuss how application of these techniques can be used to construct variable strength arrays.
[Software testing, System testing, heuristic search technique, Statistical analysis, program testing, software system testing, interaction testing, Printers, software fault tolerance, Computer science, variable strength covering array, optimisation, software system faults, Linux, ISDN, t-way testing, Software systems, greedy methods, Manufacturing, software interaction testing, statistical testing, Local area networks, mixed level covering array]
Embedded architect: a tool for early performance evaluation of embedded software
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Embedded Architect is a design automation tool that embodies a static performance evaluation technique to support early, architecture-level design space exploration for component-based embedded systems. A static control flow characterization, called an evaluation scenario, is specified based on an incremental refinement of software source code, from which a pseudo-trace of operations is generated in combination with architecture mapping and several component parameters, a software performance metric is estimated The novel contribution is the implementation of a tool that automates specification of an evaluation scenario, which sets the context for a rapid performance evaluation of distinct candidate architectures.
[Algorithm design and analysis, architecture-level design space exploration, Design automation, software source code, Software performance, Embedded software, design automation tool, software architecture, embedded software, static performance evaluation, Embedded system, embedded systems, Computer architecture, Automatic control, embedded architect, Hardware, static control flaw characterization, Space exploration, Software tools, component-based embedded systems, software performance evaluation, software metrics, software performance metric]
The co-evolution of a hype and a software architecture: experience of component-producing large-scale EJB early adopters
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
abaXX. components was one of the first AN software products fully based on Enterprise JavaBeans/spl trade/ (EJB) technology. We describe the evolution of its architecture as it moved from simply taking the initial EJB hype for the truth, through several intermediate stages, to using EJB simply as one of several encapsulated implementation techniques. So far, the public perception of how to use EJB properly evolved along a similar path, lagging 6 to 12 months behind.
[Java, Automatic programming, application program interfaces, Companies, Containers, API software product, Application software, abaXX.component, software architecture, Software architecture, Computer architecture, enterprise JavaBeans technology, Large-scale systems, EJB hype, Resource management, Functional programming, distributed object management, EJB technology]
Improving web application testing with user session data
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Web applications have become critical components of the global information infrastructure, and it is important that they be validated to ensure their reliability. Therefore, many techniques and tools for validating web applications have been created. Only a few of these techniques, however, have addressed problems of testing the functionality of web applications, and those that do have not fully considered the unique attributes of web applications. In this paper we explore the notion that user session data gathered as users operate web applications can be successfully employed in the testing of those applications, particularly as those applications evolve and experience different usage profiles. We report results of an experiment comparing new and existing test generation techniques for web applications, assessing both the adequacy of the generated tests and their ability to detect faults on a point-of-sale web application. Our results show that user session data can produce test suites as effective overall as those produced by existing white-box techniques, but at less expense. Moreover the classes of faults detected differ somewhat across approaches, suggesting that the techniques may be complimentary.
[System testing, Web application testing, program testing, program verification, fault detection, Reliability engineering, Application software, Proposals, software fault tolerance, test generation techniques, Computer science, user session data, Fault detection, Automatic testing, global information infrastructure, Software systems, Marketing and sales, Internet, white-box techniques, Medical diagnostic imaging]
Proceedings 25th International Conference on Software Engineering
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
The following topics are dealt with: software components; software testing; formal methods; software design; program analysis; software architecture; software engineering education; software fault correction.
[Software testing, computer science education, object-oriented programming, program verification, program testing, program diagnostics, software testing, Software verification and validation, software reliability, software design, Software requirements and specifications, Software reliability, software quality, formal specification, software architecture, Software fault diagnosis, Software quality, formal methods, program analysis, software fault correction, software engineering education, Computer science education, software components, Object oriented programming]
A tutorial on feature oriented programming and product-lines
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
Feature Oriented Programming (FOP) is a design methodology and tools for program synthesis. The goal is to specify a target program in terms of the features that it offers, and to synthesize an efficient program that meets these specifications. FOP has been used to develop product-lines in disparate domains, including compilers for extensible Java dialects [3], fire support simulators for the U.S. Army [5], high-performance network protocols [1], and program verification tools [14].
[Tutorial, Java, Protocols, object-oriented programming, product-lines, program verification, Design methodology, program verification tools, program synthesis, HTML, formal specification, program compilers, compilers, program specification, network protocols, Refining, Fires, Packaging, Network synthesis, Large-scale systems, object-oriented methods, software tools, fire support simulators, feature oriented programming]
Tutorial h2: an overview of UML 2.0
25th International Conference on Software Engineering, 2003. Proceedings.
None
2003
This half-day tutorial covers the salient aspects of the first major revision of the Unified Modeling Language, UML 2.0. It includes background information on what drove the requirements and the design rationale-from the point of view of one of its primary designers. The overall structure of UML 2.0 is described followed by a more detailed description of the most prominent new modeling features illustrated with many examples. The ability of UML 2.0 to deal with the needs of model-driven development methods is also covered.
[specification languages, Software requirements and specifications, model-driven architecture, Specification languages, UML 2.0 design, unified modeling language, formal specification]
Architectures and technologies for enterprise application integration
Proceedings. 26th International Conference on Software Engineering
None
2004
Architects are faced with the problem of building enterprise scale information systems, with streamlined, automated internal business processes and Web-enabled business functions, all across multiple legacy applications. The underlying architectures for such systems are embodied in a range of diverse products known as enterprise application integration (EAI) technologies. In this paper, we highlight some of the major problems, approaches and issues in designing EAI architectures and selecting appropriate supporting technology. The tutorial presents a range of the common architectural patterns frequently used for EAI applications. It also explains service-oriented architectures as the current best practice architectural framework for EAI. It then describes the state-or-the-art in EAI technologies that support these architectural styles, and discusses some of the key design trade-offs involved when selecting an appropriate integration technology (including buy versus build decisions).
[legacy applications, Buildings, Service oriented architecture, Relational databases, World Wide Web, Application software, Appropriate technology, Best practices, Information systems, software architecture, enterprise application integration, systems analysis, Computer architecture, service-oriented architectures, Concrete, information systems, Australia]
Extending the Representational State Transfer (REST) architectural style for decentralized systems
Proceedings. 26th International Conference on Software Engineering
None
2004
Because it takes time and trust to establish agreement, traditional consensus-based architectural styles cannot safely accommodate resources that change faster than it takes to transmit notification of that change, nor resources that must be shared across independent agencies. The alternative is decentralization: permitting independent agencies to make their own decisions. Our definition contrasts with that of distribution, in which several agents share control of a single decision. Ultimately, the physical limits of network latency and the social limits of independent agency call for solutions that can accommodate multiple values for the same variable. Our approach to this challenge is architectural: proposing constraints on the configuration of components and connectors to induce particular desired properties of the whole application. Specifically, we present, implement, and evaluate variations of the World Wide Web's Representational State Transfer (REST) architectural style that support distributed and decentralized systems.
[Software maintenance, decentralized systems, Communication system control, REST architectural style, independent agencies, World Wide Web, notification transmission, Application software, Delay, Connectors, software architecture, Software design, Software architecture, Computer architecture, Representational State Transfer, distributed systems, Representational state transfer, Internet, object-oriented methods, distributed programming, network latency, Software engineering]
Verifying DAML+OIL and beyond in Z/EVES
Proceedings. 26th International Conference on Software Engineering
None
2004
Semantic Web, the next generation of Web, gives data well-defined and machine-understandable meaning so that they can be processed by remote intelligent agents cooperatively. Ontology languages are the building blocks of Semantic Web as they prescribe how data are defined and related. The existing reasoning and verification tools for Semantic Web are improving however still elementary. We believe that Semantic Web can be a novel application domain for software modeling languages and tools. Z is a formal modeling language for specifying software systems and Z/EVES is a proof tool for Z. In this paper, we firstly present Z semantics for ontology language DAML+OIL. This semantic model is embedded as a Z section daml2zin Z/EVES, which serves as an environment for checking and verifying Web ontologies. Then we present a tool for automatically transforming ontology documents into the specialized Z codes understood by Z/EVES. Finally, we use a recent real application, the military plan ontologies, to demonstrate the different reasoning tasks that Z/EVES can perform. Furthermore, undiscovered errors in the original ontologies were found by Z/EVES and some of these errors are even beyond Semantic Web modeling and reasoning capabilities.
[Laboratories, remote intelligent agents, Ontologies, Z code transformation, Z section daml2zin, Microstrip, software system specification, reasoning tasks, formal specification, software modeling tools, ontology languages, verification tools, formal modeling language, software modeling languages, specification languages, Web ontology verification, Software agents, ontology language, Logic, Z semantics, reasoning tools, formal languages, semantic Web, semantic Web modeling, OWL, Application software, programming language semantics, Z language, proof tool, military plan ontologies, semantic model, Intelligent agent, Semantic Web, DAML+OIL verification, semantic Web reasoning, Software tools, Z/EVES, ontology documents]
Fault localization using visualization of test information
Proceedings. 26th International Conference on Software Engineering
None
2004
Attempts to reduce the number of delivered faults in software are estimated to consume 50% to 80% of the development and maintenance effort according to J.S. Collofello ans S.N. Woodfield (1989). Among the tasks required to reduce the number of delivered faults, debugging is one of the most time-consuming according to T. Ball and S.G. Eick and Telcordia Technologies, and locating the errors is the most difficult component of this debugging task according to I. Vessey (1985). Clearly, techniques that can reduce the time required to locate faults can have a significant impact on the cost and quality of software development and maintenance.
[Software testing, Visualization, Software maintenance, program debugging, software debugging, Costs, software development, fault localization, Debugging, Programming, test information visualization, Displays, software quality, software maintenance, software fault tolerance, Information analysis, software faults, Fault diagnosis, Failure analysis, program visualisation]
Formal concept analysis in software engineering
Proceedings. 26th International Conference on Software Engineering
None
2004
Given a binary relationship between objects and attributes, concept analysis is a powerful technique to organize pairs of related sets of objects and attributes into a concept lattice, where higher level concepts represent general features shared by many objects, while lower level concepts represent the object-specific features. Concept analysis was recently applied to several software engineering problems, such as: restructuring the code into more cohesive components, identifying class candidates, locating features in the code by means of dynamic analysis, reengineering class hierarchies. This paper provides the background knowledge required by such applications. Moreover, the methodological issues involved in the different applications of this technique are considered by giving a detailed presentation of three of them: module restructuring, design pattern inference and impact analysis based on decomposition slicing. The paper is concluded by an overview on other kinds of applications.
[Software maintenance, program diagnostics, Lattices, code restructuring, reverse engineering, dynamic analysis, design pattern inference, Application software, formal specification, Information analysis, impact analysis, class hierarchies reengineering, systems analysis, concept lattice, Software systems, concept analysis, software engineering, class candidates, module restructuring, Books, Pattern analysis, program slicing, decomposition slicing, Software engineering]
Software modeling techniques and the semantic Web
Proceedings. 26th International Conference on Software Engineering
None
2004
Following the success of XML, W3C envisions the semantic Web (Berners-Lee et al., 2001) as the next generation of Web in which data are given well-defined and machine-understandable semantics so that they can be processed by intelligent software agents. Semantic Web can be regarded as an emerging area from the knowledge representation and the Web communities. The software engineering community can also play an important role in the semantic Web development. Modeling and verification techniques can be useful at many stages during the design, maintenance and deployment of semantic Web ontology. We believe semantic Web will be a new research and application domain for software modeling techniques and tools. For example, recent research results have shown that UML, Z (Woodcock and Davis, 1996) and Alloy (Jackson, 2002) can provide modeling, reasoning and consistency checking services for semantic Web.
[semantic Web, Knowledge representation, intelligent software agents, Ontologies, Application software, formal specification, software agents, consistency checking, Intelligent agent, Machine intelligence, Semantic Web, formal verification, knowledge representation, XML, software modeling, Software agents, software engineering, Software tools, Software engineering]
Efficient decentralized monitoring of safety in distributed systems
Proceedings. 26th International Conference on Software Engineering
None
2004
We describe an efficient decentralized monitoring algorithm that monitors a distributed program's execution to check for violations of safety properties. The monitoring is based on formulae written in PT-DTL, a variant of past time linear temporal logic that we define. PT-DTL is suitable for expressing temporal properties of distributed systems. Specifically, the formulae of PT-DTL are relative to a particular process and are interpreted over a projection of the trace of global states that represents what that process is aware of. A formula relative to one process may refer to other processes' local states through remote expressions and remote formulae. In order to correctly evaluate remote expressions, we introduce the notion of Knowledge Vector and provide an algorithm which keeps a process aware of other processes' local states that can affect the validity of a monitored PT-DTL formula. Both the logic and the monitoring algorithm are illustrated through a number of examples. Finally, we describe our implementation of the algorithm in a tool called DIANA.
[past time linear temporal logic, temporal logic, program execution monitoring, PT-DTL, security of data, DIANA tool, algorithm theory, distributed systems, distributed system safety, Safety, distributed program, Knowledge Vector, Monitoring, distributed programming, decentralized monitoring]
GlueQoS: middleware to sweeten quality-of-service policy interactions
Proceedings. 26th International Conference on Software Engineering
None
2004
A holy grail of component-based software engineering is write-once, reuse everywhere. However, in modern distributed, component-based systems supporting emerging application areas such as service-oriented e-business (where Web services are viewed as components) and peer-to-peer computing, this is difficult. Non-functional requirements (related to quality-of-service (QoS) issues such as security, reliability, and performance) vary with deployment context, and sometimes even at run-time, complicating the task of re-using components. In this paper, we present a middleware-based approach to managing dynamically changing QoS requirements of components. Policies are used to advertise non-functional capabilities and vary at run-time with operating conditions. We also provide middleware enhancements to match, interpret, and mediate QoS requirements of clients and servers at deployment time and/or runtime.
[Quality of service, software component management, Runtime, distributed systems, dynamically changing QoS requirements, distributed object management, middleware, object-oriented programming, peer-to-peer computing, Peer to peer computing, component-based software engineering, component-based systems, GlueQoS, service-oriented e-business, quality of service, Application software, quality-of-service policy interactions, Middleware, Web services, nonfunctional requirements, Information security, middleware-based approach, software reusability, Software systems, Joining processes, Software engineering]
The CommUnity workbench
Proceedings. 26th International Conference on Software Engineering
None
2004
CommUnity proposes a formal approach to software architecture. It uses a parallel program design language in the style of Unity programs (Chandy and Misra, 1988), combining elements from IP (Francez and Forman, 1996). The concepts of software architecture - including configuration, connection, connector, component, instantiation - are clearly defined. CommUnity was initially developed to show how programs fit into Goguen's categorical approach to general systems theory. Since then, the language and its framework have been extended to provide a formal platform for the architectural design of open and reconfigurable systems (Fiadeiro et al., 2003). This paper describes an extension of a previous demo we presented at ICSE'02 (Wermelinger and Oliveira, 2002). This extension concerns: connectors; a graphical mode to visualize and/or update interactions; an expert utility to save the whole architecture or just some connectors as a textual specification which can then be easily read without the tool; the distribution and mobility constructs.
[Visualization, Client-server systems, open systems, Scholarships, systems theory, Calculus, reconfigurable systems, textual specification, Topology, Printers, Distributed computing, formal specification, Goguen categorical approach, graphical visualization, Connectors, software architecture, Software architecture, parallel program design language, Computer architecture, program visualisation, CommUnity workbench, Unity programs]
Research abstract: semantic concepts for the specification of non-functional properties of component-based software
Proceedings. 26th International Conference on Software Engineering
None
2004
In my research I try to define a framework which can be used to provide semantics for non-functional specifications of component-based systems. Some of the key questions driving my work are: What is the fundamental difference between functional and non-functional specifications? What formal apparatus is required to provide for this difference; What effects need to be taken into consideration when composing components with non-functional properties; What is the dependency between functional and nonfunctional specifications; What evaluation or analysis algorithms will be applied to a non-functional specification? What information must be extractable from the semantics of a nonfunctional specification?.
[Algorithm design and analysis, functional specifications, nonfunctional properties, component-based software, object-oriented programming, Computational modeling, component-based systems, nonfunctional specifications, Programming, language definition, analysis algorithms, Specification languages, Data mining, formal specification, Admission control, specification languages, Software systems, semantic concepts, Software measurement, Resource management, Software engineering]
Statistical techniques for software engineering practice
Proceedings. 26th International Conference on Software Engineering
None
2004
Many factors are combining to promote the use of quantitative and statistical methods by practicing software engineers. While these techniques are not new to industry in general, they are relatively new to the software industry. Consequently, there is significant uncertainty in the community about their difficulty and applicability. This paper provides an introduction to basic concepts and shows how they can be applied to help solve common software engineering problems.
[Uncertainty, Statistical analysis, statistical techniques, Decision making, Application software, Engineering management, Statistical distributions, Control charts, Computer industry, software engineering, statistical analysis, Software engineering, Quality management]
Software engineering challenges in bioinformatics
Proceedings. 26th International Conference on Software Engineering
None
2004
Data from biological research is proliferating rapidly and advanced data storage and analysis methods are required to manage it. We introduce the main sources of biological data available and outline some of the domain specific problems associated with automated analysis. We discuss two major areas in which we are likely experience software engineering challenges over the next ten years: data integration and presentation.
[Data analysis, Sequences, data analysis, biological data, Genomics, Memory, Information retrieval, biological research, Crystallography, Proteins, data presentation, data storage, biology computing, DNA, bioinformatics, software engineering, Bioinformatics, data integration, Software engineering]
Using event-based translation to support dynamic protocol evolution
Proceedings. 26th International Conference on Software Engineering
None
2004
All systems built from distributed components involve the use of one or more protocols for inter-component communication. Whether these protocols are based on a broadly used standard or are specially designed for a particular application, they are likely to evolve. The goal of the work described here is to contribute techniques that can support protocol evolution. We are concerned not with how or why a protocol might evolve, or even whether that evolution is in some sense correct. Rather, our concern is with making it possible for applications to accommodate protocol changes dynamically. Our approach is based on a method for isolating the syntactic details of a protocol from the semantic concepts manipulated within components. Protocol syntax is formally specified in terms of tokens, message structures, and message sequences. Event-based translation techniques are used in a novel way to present to the application the semantic concepts embodied by these syntactic elements. We illustrate our approach by showing how it would support an HTTP 1.1 client interacting with an HTTP 1.0 server.
[message sequences, inter-component communication, software prototyping, protocol syntax, event-based translation, HTTP, Application software, dynamic protocol evolution, formal specification, Computer science, program interpreters, distributed components, Databases, Software architecture, Message passing, transport protocols, message structures, Routing protocols, tokens, Logic, protocols, Web server, distributed programming, Software engineering]
Precise service level agreements
Proceedings. 26th International Conference on Software Engineering
None
2004
SLAng is an XML language for defining service level agreements, the part of a contract between the client and provider of an Internet service that describes the quality attributes that the service is required to possess. We define the semantics of SLAng precisely by modelling the syntax of the language in UML, then relating the language model to a model that describes the structure and behaviour of services. The presence of SLAng elements imposes behavioural constraints on service elements, and the precise definition of these constraints using OCL constitutes the semantic description of the language. We use the semantics to define a notion of SLA compatibility, and an extension to UML that enables the modelling of service situations as a precursor to analysis, implementation and provisioning activities.
[Supply chain management, Unified modeling language, Mission critical systems, SLA compatibility, contracts, Web and internet services, specification languages, precise service level agreements, service level agreement definition, Contracts, Internet service, programming language model, behavioural constraints, Application specific processors, Educational institutions, XML language, quality of service, programming language semantics, semantic description, Computer science, Web services, SLAng, UML, XML, quality attributes, OCL, Internet]
/spl chi/-SCTL/MUS: a formal methodology to evolve multi-perspective software requirements specifications
Proceedings. 26th International Conference on Software Engineering
None
2004
The objective of this thesis is to extend the formal methodology of refinement of requirements specifications SCTL/MUS to a multi-perspective environment where coexist requirements specifications which belong to each stakeholder involved in the software development of the system. To reach this goal, the new methodology (referred to as /spl chi/-SCTL/MUS) bets on using a viewpoint-based approach which allows to gather and maintain (possibly inconsistent and incomplete) information gathered from multiple sources. It explicitly separates the descriptions provided by different stakeholders, and concentrates on identifying and resolving conflicts between them.
[software development, software prototyping, requirements refinement, specification evolution, Programming, Multivalued logic, multi-perspective software requirements specifications, Proposals, Environmental management, formal specification, formal methodology, Telematics, Software systems, Large-scale systems, Software engineering, /spl chi/-SCTL/MUS]
Design and implementation of distributed crosscutting features with DADO
Proceedings. 26th International Conference on Software Engineering
None
2004
Some "non-" or "extra-functional" features, such as reliability, security, and tracing, defy modularization mechanisms in programming languages. With distributed, heterogeneous (DH) systems, these features induce complex implementations which crosscut different languages, OSs, and hardware platforms, while still needing to share data and events. The DADO approach helps program crosscuting features by improving DH middleware. A DADO service comprises pairs of adaplets which are explicitly modeled in IDL. DADO supports flexible and type-checked interactions (using generated stubs and skeletons) between adaplets and between objects and adaptlets. Adaptlets can be attached at run-time to an application object.
[object-oriented programming, nonfunctional features, event sharing, DH middleware, extrafunctional features, programming languages, type-checked interactions, adaptlets, distributed heterogeneous systems, DADO, distributed crosscutting features, data sharing, Software engineering, middleware]
Developing new approaches for software design quality improvement based on subjective evaluations
Proceedings. 26th International Conference on Software Engineering
None
2004
This research abstract presents two approaches for utilizing the developers' subjective design quality evaluations during the software lifecycle. In process-based approach developers study and improve their system's structure at fixed intervals. Tool-based approach uses subjective evaluations as input to tool analysis. These approaches or their combination are expected to improve software design and promote organizational learning about software design.
[Productivity, subjective evaluations, Humans, software lifecycle, design quality evaluations, software quality, software design quality, software maintenance, tool analysis, Programming profession, Design engineering, software architecture, Software design, Software quality, design quality improvement, Software systems, software performance evaluation, organizational learning, Software engineering]
Using compressed bytecode traces for slicing Java programs
Proceedings. 26th International Conference on Software Engineering
None
2004
Dynamic slicing is a well-known program debugging technique. Given a program P and input I, it finds all program statements which directly/indirectly affect the values of some variables' occurrences when P is executed with I. Dynamic slicing algorithms often proceed by traversing the execution trace of P produced by input I (or a dependence graph which captures control/data flow in the execution trace). Consequently, it is important to develop space efficient representations of the execution trace. In this paper, we use results from data compression to compactly represent bytecode traces of sequential Java programs. The major space savings come from the optimized representation of data (instruction) addresses used by memory reference (branch) bytecodes as operands. We give detailed experimental results on the space efficiency and time overheads for our compact trace representation. We then show how dynamic slicing algorithms can directly traverse our compact traces without resorting to costly decompression. We also develop an extension of dynamic slicing which allows us to explain omission errors (i.e. why some events did not happen during program execution).
[program debugging, Heuristic algorithms, sequential Java programs, Data compression, compressed bytecode traces, Java program slicing, program statements, dynamic slicing algorithms, dependence graph, control flow, program slicing, Java, data compression, representation data addresses, Debugging, data flow analysis, Read-write memory, program execution, Information retrieval, Flow graphs, compact traces, Programming profession, trace representation, execution trace, space savings, memory reference bytecodes, data flow, Software engineering]
Testing object-oriented software
Proceedings. 26th International Conference on Software Engineering
None
2004
The best approach to testing object-oriented software depends on many factors: the application-under-test, the development approach, the organization of the development and quality assurance teams, the criticality of the application, the development environment and the implementation language(s), the use of design and language features, project timing and resource constraints. Nonetheless, we can outline a general approach that works in stages from independent consideration of classes and their features to consideration of their interactions. A coherent strategy would include three main phases: intraclass, interclass, and system and acceptance testing.
[Software testing, Encapsulation, System testing, object-oriented programming, program testing, Object oriented modeling, software testing, object-oriented software, system testing, Technology planning, Application software, Computer science, Concurrent computing, Quality assurance, Software quality, acceptance testing]
Breaking the ice for agile development of embedded software: an industry experience report
Proceedings. 26th International Conference on Software Engineering
None
2004
A software engineering department in a Daimler-Chrysler business unit was highly professional at developing embedded software for busses and coaches. However, customer specific add-ons were a regular source of hassle. Simple as they are, those individual requirements have to be implemented in hours or days rather than weeks or months. Poor quality or late upload into the bus hardware would cause serious cost and overhead. Established software engineering methods were considered inadequate and needed to be cut short. Agile methods offer guidance when quality, flexibility and high speed need to be reconciled. However, we did not adopt any full agile method, but added single agile practices to our process improvement toolbox. We suggested a number of classical process improvement activities (such as more systematic documentation and measurement) and combined them with agile elements (e.g. Test First Process). This combination seemed to foster acceptance of agile ideas and may help us to break the ice for a cautious extension of agile process improvement.
[Costs, Documentation, Ice, automobile industry, software quality, manufacturing data processing, Embedded software, Coordinate measuring machines, agile process improvement, embedded software, software flexibility, embedded systems, software process improvement, Computer industry, Hardware, software engineering, Software measurement, agile software development, Software engineering, Testing]
Elaborating security requirements by construction of intentional anti-models
Proceedings. 26th International Conference on Software Engineering
None
2004
Caring for security at requirements engineering time is a message that has finally received some attention recently. However, it is not yet very clear how to achieve this systematically through the various stages of the requirements engineering process. The paper presents a constructive approach to the modeling, specification and analysis of application-specific security requirements. The method is based on a goal-oriented framework for generating and resolving obstacles to goal satisfaction. The extended framework addresses malicious obstacles (called anti-goals) set up by attackers to threaten security goals. Threat trees are built systematically through anti-goal refinement until leaf nodes are derived that are either software vulnerabilities observable by the attacker or anti-requirements implementable by this attacker. New security requirements are then obtained as countermeasures by application of threat resolution operators to the specification of the anti-requirements and vulnerabilities revealed by the analysis. The paper also introduces formal epistemic specification constructs and patterns that may be used to support a formal derivation and analysis process. The method is illustrated on a Web-based banking system for which subtle attacks have been reported recently.
[Mission critical systems, Banking, Application software, Communication system security, Computer crime, formal specification, requirements engineering, security of data, goal satisfaction, Surgery, formal analysis, Web-based banking, Internet, Cryptography, security requirements, Pattern analysis, bank data processing, Biomedical engineering]
Panel MDA in practice
Proceedings. 26th International Conference on Software Engineering
None
2004
false
[Computer science, Computer aided software engineering, Web services, Computer architecture, Programming, Educational institutions, Application software, Books, Logic, Software engineering]
Team-based fault content estimation in the software inspection process
Proceedings. 26th International Conference on Software Engineering
None
2004
The main objective of software inspection is to detect faults within a software artifact. This helps to reduce the number of faults and to increase the quality of a software product. However, although inspections have been performed with great success, and although the quality of the product is increased, it is difficult to estimate the quality. During the inspection process, attempts with objective estimations as well as with subjective estimations have been made. These methods estimate the fault content after an inspection and give a hint of the quality of the product. This paper describes an experiment conducted throughout the inspection process, where the purpose is to compare the estimation methods at different points. The experiment evaluates team estimates from subjective and objective fault content estimation methods integrated with the software inspection process. The experiment was conducted at two different universities with 82 reviewers. The result shows that objective estimates outperform subjective when point and confidence intervals are used. This contradicts the previous studies in the area.
[objective estimations, program testing, estimation theory, software fault detection, Meetings, software inspection, Inspection, software quality, software fault tolerance, team-based fault content estimation, Fault detection, capture-recapture, software artifact, software product, subjective estimations, Software engineering]
Software variability management
Proceedings. 26th International Conference on Software Engineering
None
2004
During recent years, the amount of variability that has to be supported by a software artefact is growing considerably and its management is evolving into a major challenge during development, usage, and evolution of software artefacts. Successful management of variability in software leads to better customizable software products that are in turn likely to result in higher market success. The aim of this paper is to present software variability management both from a 'problems' and from a 'solutions' perspective by discussing experiences from industrial practice and from applied research in academia. Issues that are addressed include, but are not limited to, technological, process, and organizational aspects as well as notation, assessment, design, and evolution aspects.
[software variability management, Programming, software management, Enterprise resource planning, Environmental management, software customization, Software development management, Embedded software, Management information systems, software reusability, Software systems, Computer industry, Resource management, Software reusability, software artefact]
Using data versioning in database application development
Proceedings. 26th International Conference on Software Engineering
None
2004
Database applications such as enterprise resource planning systems and customer relationship management systems are widely used software systems. Development and testing of database applications is difficult because the program execution depends on the persistent state stored in the database. In this paper we show that how versioning of the persistent data stored in the database can solve some critical problems in the development and testing of database applications can be solved by versioning the data stored in the database. Our solution framework is based on long transaction management, a well-researched branch of database systems. We also present empirical results that show the proposed framework's effectiveness in practice.
[Software testing, Java, program testing, software development, software testing, software systems, enterprise resource planning systems, Enterprise resource planning, enterprise resource planning, Transaction databases, Application software, database management systems, data versioning, customer relationship management systems, long transaction management, customer relationship management, Customer relationship management, database application development, Software systems, Hardware, Database systems, software engineering, Software engineering]
Controlling the complexity of software designs
Proceedings. 26th International Conference on Software Engineering
None
2004
Our research has focused on identifying techniques to develop software that is amenable to refactoring and change. The Law of Demeter (LoD) was one contribution in this effort. But it led to other problems. With the current state of the art focused on aspect-oriented software development (AOSD), it is useful to revisit the general objectives of the LoD and adapt it to the new ideas. Hence we introduce the Law of Demeter for Concerns and discuss the important intersection of these approaches with traversals. We explore the ramifications of the Laws of Demeter (LoD and LoDC) to achieve better separation of concerns through improved software processes. They are supported by language mechanisms that are implemented using novel applications of automata theory.
[Encapsulation, LoD, AOSD, object-oriented programming, Object oriented modeling, automata theory, software design complexity, LoDC, aspect-oriented software development, Law of Demeter for Concerns, Programming, Educational institutions, Application software, Information science, Software design, language mechanisms, Automata, software process improvement, software processes, separation of concerns, software refactoring, Software engineering, software metrics]
Evaluating object-oriented designs with link analysis
Proceedings. 26th International Conference on Software Engineering
None
2004
The hyperlink induced topic search algorithm, which is a method of link analysis, primarily developed for retrieving information from the Web, is extended in this paper, in order to evaluate one aspect of quality in an object-oriented model. Considering the number of discrete messages exchanged between classes, it is possible to identify "God" classes in the system, elements which imply a poorly designed model. The principal eigenvectors of matrices derived from the adjacency matrix of a modified class diagram, are used to identify and quantify heavily loaded portions of an object-oriented design that deviate from the principle of distributed responsibilities. The non-principal eigenvectors are also employed in order to identify possible reusable components in the system. The methodology can be easily automated as illustrated by a Java program that has been developed for this purpose.
[Algorithm design and analysis, Java, hyperlink, object-oriented programming, Object oriented modeling, Scalability, information retrieval, Information retrieval, World Wide Web, Electronic mail, message exchanging, Information analysis, eigenvalues and eigenfunctions, matrix algebra, adjacency matrix, eigenvectors, link analysis, Internet, Web sites, Informatics, topic search algorithm, search problems, object-oriented designs, Software engineering]
Compositional verification of middleware-based software architecture descriptions
Proceedings. 26th International Conference on Software Engineering
None
2004
In this paper we present a compositional reasoning to verify middleware-based software architecture descriptions. We consider a nowadays typical software system development, namely the development of a software application A on a middleware M. Our goal is to efficiently integrate verification techniques, like model checking, in the software life cycle in order to improve the overall software quality. The approach exploits the structure imposed on the system by the software architecture in order to develop an assume-guarantee methodology to reduce properties verification from global to local. We apply the methodology on a non-trivial case study namely the development of a Gnutella system on top of the SIENA event-notification middleware.
[compositional verification, software life-cycle, program verification, middleware-based software architecture descriptions, verification techniques, Data structures, Explosions, software quality, Application software, Middleware, software system development, software architecture, Software architecture, formal verification, model checking, Software quality, Computer architecture, Software systems, System software, assume-guarantee methodology, software application, Context modeling, middleware]
Toward a software testing and reliability early warning metric suite
Proceedings. 26th International Conference on Software Engineering
None
2004
The field reliability is measured too late for affordably guiding corrective action to improve the quality of the software. Software developers can benefit from an early warning of their reliability while they can still affordably react. This early warning can be built from a collection of internal metrics. An internal metric, such as the number of lines of code, is a measure derived from the product itself. An external measure is a measure of a product derived from assessment of the behavior of the system. For example, the number of defects found in test is an external measure. The ISO/IEC standard states that [i]nternal metrics are of little value unless there is evidence that they are related to external quality. Internal metrics can be collected in-process and more easily than external metrics. Additionally, internal metrics have been shown to be useful as early indicators of externally-visible product quality. For these early indicators to be meaningful, they must be related (in a statistically significant and stable way) to the field quality/reliability of the product. The validation of such metrics requires the convincing demonstration that (1) the metric measures what it purports to measure and (2) the metric is associated with an important external metric, such as field reliability, maintainability, or fault-proneness. Software metrics have been used as indicators of software quality and fault proneness. There is a growing body of empirical results that supports the theoretical validity of the use of higher-order early metrics, such as OO metrics defined by Chidamber-Kemerer (CK) and the MOOD OO metric suites as predictors of field quality. However, general validity of these metrics (which are often unrelated to the actual operational profile of the product) is still open to criticism.
[Software testing, System testing, program testing, software development, software testing, software reliability, Programming, product reliability, Software reliability, software quality, Computer science, Software metrics, Automatic testing, Feedback, product quality, early warning metric suite, Software quality, software processes, empirical software engineering, Software engineering, software metrics]
Behavior capture and test for verifying evolving component-based systems
Proceedings. 26th International Conference on Software Engineering
None
2004
Component-based system (CBS) technology supports rapid development of complex heterogeneous evolving systems by enhancing reuse and adaptability. CBSs can be extended and adapted by modifying one or more components. The same component can be used in several systems, and the same system can be deployed in many configurations that differ for some components. Traditional test and analysis techniques make little use of quality information about components and subsystems when testing the whole system. Thus, reusability for quality assessment and reduction of quality related costs are not fully exploited. Moreover, verification of CBSs is hardened by the frequent lack of information about components that are provided by third parties without source code and with incomplete documentation. This framework reduces the applicability of many traditional testing and analysis techniques for CBSs. Main goal of my PhD research is the definition and experimentation of testing and analysis techniques that allow to efficiently test CBSs in presence of limited information about design and code by reusing behavioral information that can be gathered from previous usage of the components.
[System testing, Java, Costs, object-oriented programming, Filtering, program testing, program verification, run-time verification, component-based systems, Documentation, Information analysis, Fault diagnosis, system adaptability, Runtime, Automatic testing, quality assessment, software reusability, Quality assessment, system evolution, system reusability]
Making resource decisions for software projects
Proceedings. 26th International Conference on Software Engineering
None
2004
Software metrics should support managerial decision making in software projects. We explain how traditional metrics approaches, such as regression-based models for cost estimation fall short of this goal. Instead, we describe a causal model (using a Bayesian network) which incorporates empirical data, but allows it to be interpreted and supplemented using expert judgement. We show how this causal model is used in a practical decision-support tool, allowing a project manager to trade-off the resources used against the outputs (delivered functionality, quality achieved) in a software project. The model and toolset have evolved in a number of collaborative projects and hence capture significant commercial input. Extensive validation trials are taking place among partners on the EC funded project MODIST (this includes Philips, Israel Aircraft Industries and QinetiQ) and the feedback so far has been very good. The estimates are sensible and the causal modelling approach enables decision-makers to reason in a way that is not possible with other project management and resource estimation tools. To ensure wide dissemination and validation a version of the toolset with the full underlying model is being made available for free to researchers.
[expert judgement, resource decisions, Costs, estimation theory, public domain software, Project management, managerial decision making support, cost estimation, resource estimation tools, Software metrics, project MODIST, causal modelling, software projects, causal model, Bayesian network, Quality management, Collaborative tools, Decision making, software development management, decision support systems, Bayesian methods, Software quality, empirical data, Bayes methods, Resource management, regression-based models, decision-support tool, project management tools, Software tools, software metrics]
Visual timed event scenarios
Proceedings. 26th International Conference on Software Engineering
None
2004
Formal description of real-time requirements is a difficult and error prone task. Conceptual and tool support for this activity plays a central role in the agenda of technology transference from the formal verification engineering community to the real-time systems development practice. In this article we present VTS, a visual language to define complex event-based requirements such as freshness, bounded response, event correlation, etc. The underlying formalism is based on partial orders and supports real-time constraints. The problem of checking whether a timed automaton model of a system satisfies these sort of scenarios is shown to be decidable. Moreover, we have also developed a tool that translates visually specified scenarios into observer timed automata. The resulting automata can be composed with a model under analysis in order to check satisfaction of the stated scenarios. We show the benefits of applying these ideas to some case studies.
[Real time systems, automata theory, Control systems, timed automaton model, visual languages, formal specification, Aerospace industry, formal verification, visual language, VTS, Embedded system, Electrical equipment industry, software tools, Logic, conceptual support, real-time requirements, check satisfaction, visual timed event scenarios, real-time constraints, formal description, Application software, observer timed automata, tool support, event-based requirements, real-time systems development, Electronics industry, Automata, real-time systems, Formal verification]
Component-based self-adaptability in peer-to-peer architectures
Proceedings. 26th International Conference on Software Engineering
None
2004
Current peer-to-peer architectures are hardly resistant against unanticipated exceptions such as the failure of single peers. This can be justified by the absence of sophisticated models for detecting and handling exception in peer-to-peer architectures. On the other hand, existing models for such self-adaptable architectures are rather generic and less practical for end-users. In this work, a component-based self-adaptability model for peer-to-peer architectures is presented that supports end-users in the handling of exceptions during use time. Support is also provided to handle exceptions during deployment and adaptation of an application. All these approaches are integral parts of DeEvolve, a peer-to-peer runtime environment for component-based peer services.
[Runtime environment, object-oriented programming, Peer to peer computing, exception handling, distributed processing, DeEvolve, Application software, HCI, Computer science, Human computer interaction, peer-to-peer architectures, software architecture, composition languages, Software architecture, component-based peer services, Web and internet services, Computer architecture, Concrete, component-based self-adaptability, component technology, human computer interaction, Personal communication networks, distributed programming]
GAMMATELLA: visualization of program-execution data for deployed software
Proceedings. 26th International Conference on Software Engineering
None
2004
To investigate the program-execution data efficiently, we must be able to view the data at different levels of detail. In our visualization approach, we represent software systems at three different levels: statement level, file level, and system level. At the statement level, we represent the actual code. The representation at the file level provides a miniaturized view of the source code similar to the one used in the SeeSoft system (Eick et al., 1992). The system level uses treemaps (Shneiderman, 1992 and Bruls et al., 2000) to represent the software and is the most abstracted level in our visualization. At each level, coloring is used to represent one- or two-dimensional information about the code, using the colors' hue and brightness components. The coloring technique that we apply is a generalization of the coloring technique defined for fault-localization by Jones and colleagues (2001). GAMMATELLA is a toolset that implements our visualization approach and provides capabilities for instrumenting the code, collecting program-execution data from the field, and storing and retrieving the data locally. GAMMATELLA is written in Java, supports the monitoring of Java programs, and consists of three main components: an instrumentation, execution, and coverage tool, a data collection daemon, and a program visualizer.
[data retrieval, program testing, software systems, file level, coverage tool, Software safety, Optimization, Quality assurance, system level, data visualisation, statement level, Performance analysis, data visualization, Java programs, Monitoring, Testing, treemaps, Java, Data analysis, Instruments, source code, Educational institutions, SeeSoft system, GAMMATELLA, program visualizer, program-execution data, coloring technique, Data visualization, system monitoring, data collection daemon, program visualisation, fault-localization]
Calculating architectural reliability via modeling and analysis
Proceedings. 26th International Conference on Software Engineering
None
2004
We present a software architecture-based approach to compositional estimation of system's reliability. Our approach is applicable to early stages of development when the implementation artifacts are not yet available, and exact execution profile is unknown. The uncertainty of the execution profile is modeled using stochastic processes with unknown parameters. The compositional approach calculates overall reliability of the system as a function of the reliability of its constituent components and their (complex) interactions. Sensitivity analysis to identify critical components and interactions will be provided.
[Protocols, Costs, Buildings, software reliability, Stochastic processes, architectural reliability, Predictive models, system reliability, Frequency estimation, modelling, Computer science, software architecture, systems analysis, Software systems, Reliability, stochastic processes, Software engineering]
Parametric analysis of real-time embedded systems with abstract approximation interpretation
Proceedings. 26th International Conference on Software Engineering
None
2004
My research area is fundamental of formal analysis of real-time embedded systems. The main objective of this research is the theoretical and practical development of a verification algorithm for the formal analysis of real-time embedded systems based on the combination of real-time model checking and abstract interpretation of real-time models. The objective of the proposed combination is an improved behavior both in time and space requirement of the resulting algorithm. One of drawbacks of all current real-time model-checking tools is the limited size of the systems that can be analyzed. By combination of state-space exploration with abstract interpretation we expect to scale up the size of applications.
[Real time systems, Algorithm design and analysis, abstract approximation interpretation, abstract interpretation, Explosions, Mathematics, State-space methods, real-time embedded systems, Application software, Power system modeling, formal specification, Computer science, formal verification, Embedded system, embedded systems, real-time model checking, parametric analysis, formal analysis, Hardware, verification algorithm, state-space exploration]
AcmeStudio: supporting style-centered architecture development
Proceedings. 26th International Conference on Software Engineering
None
2004
Software architectural modeling is crucial to the development of high-quality software. Tool support is required for this activity, so that models can be developed, viewed, analyzed, and refined to implementations. This support needs to be provided in a flexible and extensible manner so that the tools can fit into a company's process and can use particular, perhaps company-defined, domain-specific architectural styles. In this research demonstration, we describe AcmeStudio, a style-neutral architecture development environment that can be easily specialized for architectural design in different domains.
[architectural design, Vocabulary, high-quality software, Protocols, Computational modeling, style-neutral architecture development, Programming, software quality, style-centered architecture development, Application software, domain-specific architectural styles, Connectors, tool support, Analytical models, software architecture, Computer architecture, Software quality, Performance analysis, software tools, AcmeStudio]
Agile process tailoring and problem analysis (APTLY)
Proceedings. 26th International Conference on Software Engineering
None
2004
Developing software using a well-defined, well-understood process improves the likelihood of delivering a product with the required quality. Enhancing that process to meet recognised process standards, such as CMMI and ISO 9000, can further facilitate the development of complex systems in a repeatable and predictable way. There are tradeoffs involved, however. In particular, because projects differ in their scale, scope, and technical challenge, the same process will not suit all circumstances. Agile approaches to development, such as Extreme Programming (XP), SCRUM and Crystal Methodologies, recognise this dilemma and suggest that processes be tailored to each situation. The research problem for postgraduate investigation is to determine in detail how this can be achieved successfully. This will include a consideration of how best to define, maintain and give access to a knowledge base recording details of process concepts, techniques and experience.
[Process design, Crystal Methodologies, software development, ISO standards, Programming, software quality, SCRUM, Best practices, Information analysis, extreme programming, software architecture, problem analysis, ISO 9000, Software quality, software process improvement, process standards, XP, SPICE, CMMI, Standards development, agile process tailoring, Monitoring, software standards, Quality management]
Efficient forward computation of dynamic slices using reduced ordered binary decision diagrams
Proceedings. 26th International Conference on Software Engineering
None
2004
Dynamic slicing algorithms can greatly reduce the debugging effort by focusing the attention of the user on a relevant subset of program statements. Recently, algorithms for forward computation of dynamic slices have been proposed which maintain dynamic slices of all variables as the program executes. An advantage of this approach is that when a request for a slice is made, it is already available. The main disadvantage of using such an algorithm for slicing realistic programs is that the space and time required to maintain a large set of dynamic slices corresponding to all program variables can be very high. In this paper, we analyze the characteristics of dynamic slices and identify properties that enable space efficient representation of a set of dynamic slices. We show that by using reduced ordered binary decision diagrams (roBDDs) to represent a set of dynamic slices, the space and time requirements of maintaining dynamic slices are greatly reduced. In fact, not only can the latest dynamic slices of all variables be easily maintained, but rather all dynamic slices of all variables throughout a program's execution can be maintained. Our experiments show that our roBDD based algorithm for forward computation of dynamic slices can maintain 107-217 million dynamic slices arising during long program runs using only 28-392 megabytes of storage. In addition, the performance of the roBDD based forward computation method compares favorably with the performance of the LP backward computation algorithm.
[Software testing, Software maintenance, program debugging, Data analysis, Heuristic algorithms, Debugging, program execution, Data structures, program variables, Application software, software maintenance, program statements, Computer science, binary decision diagrams, dynamic slices computation, Boolean functions, dynamic slicing algorithm, LP backward computation algorithm, Software measurement, reduced ordered binary decision diagrams, program slicing]
Case studies for software engineers
Proceedings. 26th International Conference on Software Engineering
None
2004
The topic of this paper was the correct use and interpretation of case studies as an empirical research method. Using an equal blend of lecture and discussion, it gave a foundation for conducting, reviewing, and reading case studies. There were lessons for software engineers as researchers who conduct and report case studies, reviewers who evaluate papers, and practitioners who are attempting to apply results from papers. The main resource for the course was the book Case Study Research: Design and Methods by Robert K. Yin. This text was supplemented with positive and negative examples from the literature.
[software engineers, Computer aided software engineering, Data analysis, Grounding, Design methodology, empirical research, Paper technology, case studies, History, Solids, software engineering, Books, Software engineering]
Finding latent code errors via machine learning over program executions
Proceedings. 26th International Conference on Software Engineering
None
2004
This paper proposes a technique for identifying program properties that indicate errors. The technique generates machine learning models of program properties known to result from errors, and applies these models to program properties of user-written code to classify and rank properties that may lead the user to errors. Given a set of properties produced by the program analysis, the technique selects a subset of properties that are most likely to reveal an error. An implementation, the fault invariant classifier, demonstrates the efficacy of the technique. The implementation uses dynamic invariant detection to generate program properties. It uses support vector machine and decision tree learning tools to classify those properties. In our experimental evaluation, the technique increases the relevance (the concentration of fault-revealing properties) by a factor of 50 on average for the C programs, and 4.8 for the Java programs. Preliminary experience suggests that most of the fault-revealing properties do lead a programmer to an error.
[fault diagnosis, Laboratories, C language, fault-revealing properties, user-written code, dynamic invariant detection, program analysis, decision tree learning tools, machine learning models, Decision trees, learning (artificial intelligence), Java programs, Testing, Classification tree analysis, program properties, latent code errors, Java, support vector machines, program diagnostics, Programming profession, Support vector machines, Computer science, support vector machine, Support vector machine classification, Machine learning, decision trees, fault invariant classifier, C programs, Computer errors, program executions]
A hybrid architectural style for distributed parallel processing of generic data streams
Proceedings. 26th International Conference on Software Engineering
None
2004
Immersive, interactive applications grouped under the concept of Immersipresence require on-line processing and mixing of multimedia data streams and structures. One critical issue seldom addressed is the integration of different solutions to technical challenges, developed independently in separate fields, into working systems, that operate under hard performance constraints. In order to realize the Immersipresence vision, a consistent, generic approach to system integration is needed, that is adapted to the constraints of research development. This paper introduces SAI, a new software architecture model for designing, analyzing and implementing applications performing distributed, asynchronous parallel processing of generic data streams. SAI provides a universal framework for the distributed implementation of algorithms and their easy integration into complex systems that exhibit desirable software engineering qualities such as efficiency, scalability, extensibility, reusability and interoperability. The SAI architectural style and its properties are described. The use of SAI and of its supporting open source middleware (MFSM) is illustrated with integrated, distributed interactive systems.
[integrated interactive systems, Scalability, distributed processing, multimedia computing, distributed parallel processing, parallel processing, multimedia data streams, Software architecture, system integration, open source middleware, interactive programming, software architecture model, MFSM, Parallel processing, software engineering, Performance analysis, multimedia data structures, middleware, on-line processing, Software algorithms, distributed interactive systems, hybrid architectural style, Application software, Middleware, asynchronous parallel processing, Interactive systems, Immersipresence vision, Streaming media, generic data streams, SAI, interactive applications, immersive applications, Software engineering]
Oil and water? High performance garbage collection in Java with MMTk
Proceedings. 26th International Conference on Software Engineering
None
2004
Increasingly popular languages such as Java and C# require efficient garbage collection. This paper presents the design, implementation, and evaluation of MMTk, a Memory Management Toolkit for and in Java. MMTk is an efficient, composable, extensible, and portable framework for building garbage collectors. MMTk uses design patterns and compiler cooperation to combine modularity and efficiency. The resulting system is more robust, easier to maintain, and has fewer defects than monolithic collectors. Experimental comparisons with monolithic Java and C implementations reveal MMTk has significant performance advantages as well. Performance critical system software typically uses monolithic C at the expense of flexibility. Our results refute common wisdom that only this approach attains efficiency, and suggest that performance critical software can embrace modular design and high-level languages.
[Java, compiler cooperation, Buildings, high level languages, memory management toolkit, Virtual machining, C++ language, garbage collection, Petroleum, Computer science, systems development, storage management, MMTk, Memory management, design patterns, Java language, Robustness, Hybrid power systems, System software, high-level languages, C# language, software performance evaluation, Software engineering]
Managing commitments and risks: challenges in distributed agile development
Proceedings. 26th International Conference on Software Engineering
None
2004
Software development is always a challenging undertaking and it requires high commitments from individuals who participate in it. Software development often involves new technology, challenging or unknown requirements, and tight schedules - making it particularly prone to several types of risk. These challenges are even more paramount in agile development and in distributed development, where the need for efficient information sharing is important, yet the distributed development makes it very difficult. This paper uses innovative learning methods to explore and to learn about these challenges and how to deal with them. The paper is partially based on presentations given by authors, but a major element in the paper is the case study that is introduced and in which will involve all the participants. The learning in the paper is strongly facilitated by participants' discussions and the insights generated in concrete problem solving situations.
[risk management, software development, information sharing, Project management, Programming, distributed development, agile development, Software development management, Learning systems, Technology management, Concrete, software engineering, Risk management, Problem-solving, Research and development management, Software engineering]
Skoll: distributed continuous quality assurance
Proceedings. 26th International Conference on Software Engineering
None
2004
Quality assurance (QA) tasks, such as testing, profiling, and performance evaluation, have historically been done in-house on developer-generated workloads and regression suites. Since this approach is inadequate for many systems, tools and processes are being developed to improve software quality by increasing user participation in the QA process. A limitation of these approaches is that they focus on isolated mechanisms, not on the coordination and control policies and tools needed to make the global QA process efficient, effective, and scalable. To address these issues, we have initiated the Skoll project, which is developing and validating novel software QA processes and tools that leverage the extensive computing resources of worldwide user communities in a distributed, continuous manner to significantly and rapidly improve software quality. This paper provides several contributions to the study of distributed continuous QA. First, it illustrates the structure and functionality of a generic around-the-world, around-the-clock QA process and describes several sophisticated tools that support this process. Second, it describes several QA scenarios built using these tools and process. Finally, it presents a feasibility study applying these scenarios to a 1MLOC+ software package called ACE+TAO. While much work remains to be done, the study suggests that the Skoll process and tools effectively manage and control distributed, continuous QA processes. Using Skoll we rapidly identified problems that had taken the ACE+TAO developers substantially longer to find and several of which had previously not been found. Moreover, automatic analysis of QA task results often provided developers information that quickly led them to the root cause of the problems.
[1MLOC+ software package, software profiling, program verification, software testing, software quality, Distributed computing, Information analysis, around-the-clock QA process, ACE+TAO, Quality assurance, Software packages, Skoll, quality assurance, Software quality, distributed continuous quality assurance, distributed continuous QA, Distributed control, Automatic control, around-the-world QA process, Software tools, distributed programming, software performance evaluation, Testing, Quality management]
Towards safe distributed application development
Proceedings. 26th International Conference on Software Engineering
None
2004
Distributed application development is overly tedious, as the dynamic composition of distributed components is hard to combine with static safety with respect to types (type safety) and data (encapsulation). Achieving such safety usually goes through specific compilation to generate the glue between components, or making use of a single programming language for all individual components with a hardwired abstraction for the distributed interaction. In this paper, we investigate general-purpose programming language features for supporting third-party implementations of programming abstractions for distributed interaction among components. We report from our experiences in developing a stock market application based on type-based publish/subscribe (TPS) implemented (1) as a library in standard Java as well as with (2) a homegrown extension of the Java language augmented with specific primitives for TPS, motivated by the lacks of former implementation. We then revisit the library approach, investigating the impact of genericity, reflective features, and the type system, on the implementation of a satisfactory TPS library. We then discuss the impact of these features also on other distributed programming abstractions, and hence on the engineering of distributed applications in general, pointing out lacks of mainstream programming environments such as Java as well as .NET.
[Laboratories, TPS library, programming languages, dynamic composition, distributed components, type safety, type-based publish/subscribe, .NET, Libraries, distributed programming abstractions, Safety, Stock markets, distributed programming, Java, safe distributed application development, Logic programming, software library, stock market application, Middleware, Sun, Connectors, Computer languages, distributed interaction, distributed component interaction, Java language, general-purpose programming language features, data encapsulation, static safety]
Supporting reflective practitioners
Proceedings. 26th International Conference on Software Engineering
None
2004
The theme and title for this panel is inspired by Donald Schon's writings about the reflective practitioner in which he describes professional practice as being a process of reflection in action. Ill-defined problems including design decisions lead to breakdowns, which become opportunities for reflection and modification of practice. This panel seeks to provide ICSE attendees with a broad cross section of the history, state of the art, and open issues with some of the methods and tools directed at supporting reflective software practitioners.
[Donald Schon writing, Electric breakdown, human factors, Programming, reflective practitioner support, History, Computer science, Software design, professional practice, Writing, Software systems, design decision, software engineering, Optical reflection, Informatics, Software engineering]
Precise modeling of design patterns in UML
Proceedings. 26th International Conference on Software Engineering
None
2004
Prior research attempts to formalize the structure of object-oriented design patterns for a more precise specification of design patterns. It also allows automation support to be developed for user-defined design patterns in the future CASE tools. Targeting to a particular type of automation (e.g. verification of pattern instances), previous specification approaches over-specify pattern structures to a certain extend. Over-specification makes pattern specification ambiguous and disallows the specification language to be used for specifying compound patterns. In this paper, we present the structural properties of design patterns which reveal the true abstract nature of pattern structures. To support these properties so as to solve the over-specification problem, we propose an extension to UML 1.5 (basically UML 1.4 with Action semantics). The specialization and refining mechanism of UML provides also a smooth support for the instantiation, refinement and integration of pattern structures specified in UML. Our work makes no significant extension to the UML 1.5 meta-model but more in a UML Profile approach to ease the migration of our work to UML 2.0, which has not yet officially released by OMG during this work.
[Design automation, Computer aided software engineering, program verification, Unified modeling language, object-oriented design patterns, over-specification problem, Programming, formal specification, Design engineering, UML 1.5 meta-model, specification languages, design pattern precise modeling, automation support, UML 2.0, specification language, Technological innovation, object-oriented programming, Unified Modeling Language, Action semantics, Object oriented modeling, pattern structure specification, Documentation, Specification languages, user-defined design patterns, CASE tools, pattern structures, precise specification, Software systems, computer aided software engineering, design pattern specification, UML Profile approach]
Balancing agility and discipline: evaluating and integrating agile and plan-driven methods
Proceedings. 26th International Conference on Software Engineering
None
2004
Rapid change and increasing software criticality drive successful development and acquisition organizations to balance the agility and discipline of their key processes. The emergence of agile methods in the software community is raising the expectations of customers and management, but the methods have shortfalls and their compatibility with traditional plan-driven methods such as those represented by CMMI, ISO-15288, and UK-DefStan-00-55 is largely unexplored. This paper pragmatically examines the aspects of agile and plan-driven methods and provides an approach to balancing through examples and case studies.
[ISO-15288, Stability, ISO standards, agile development, History, UK-DefStan-00-55, Engineering management, Customer satisfaction, Disaster management, software process improvement, Innovation management, Permission, Variable speed drives, software criticality, CMMI, software engineering, software standards, Business, Software engineering]
Feature-based decomposition of inductive proofs applied to real-time avionics software: an experience report
Proceedings. 26th International Conference on Software Engineering
None
2004
The hardware and software in modern aircraft control systems are good candidates for verification using formal methods: they are complex, safety-critical, and challenge the capabilities of test-based verification strategies. We have previously reported on our use of model checking to verify the time partitioning property of the Deos/spl trade/ real-time operating system for embedded avionics. The size and complexity of this system have limited us to analyzing only one configuration at a time. To overcome this limit and generalize our analysis to arbitrary configurations we have turned to theorem proving. This paper describes our use of the PVS theorem prover to analyze the Deos scheduler. In addition to our inductive proof of the time partitioning invariant, we present a feature-based technique for modeling state-transition systems and formulating inductive invariants. This technique facilitates an incremental approach to theorem proving that scales well to models of increasing complexity, and has the potential to be applicable to a wide range of problems.
[Real time systems, Software testing, System testing, program verification, embedded avionics, safety-critical software, Aerospace electronics, inductive proofs, time partitioning invariant, inductive invariant formulation, aerospace control, Software safety, complex software, state-transition system modeling, test-based verification, formal verification, Operating systems, feature-based decomposition, aircraft control systems, Hardware, theorem proving, Deos real-time operating system, PVS theorem prover, NASA, feature-based technique, avionics, Application software, Deos scheduler analysis, model checking, real-time systems, formal methods, operating systems (computers), inductive proof, time partitioning property verification, real-time avionics software, Software engineering]
Mining version histories to guide software changes
Proceedings. 26th International Conference on Software Engineering
None
2004
We apply data mining to version histories in order to guide programmers along related changes: "Programmers who changed these functions also changed. . . ". Given a set of existing changes, such rules (a) suggest and predict likely further changes, (b) show up item coupling that is indetectable by program analysis, and (c) prevent errors due to incomplete changes. After an initial change, our ROSE prototype can correctly predict 26% of further files to be changed - and 15% of the precise functions or variables. The topmost three suggestions contain a correct location with a likelihood of 64%.
[Navigation, program diagnostics, data mining, Documentation, software management, ROSE, HTML, History, Data mining, Association rules, software maintenance, software changes, Programming profession, Programming environments, configuration management, version histories, Prototypes, program analysis, Books]
The Dublo architecture pattern for smooth migration of business information systems: an experience report
Proceedings. 26th International Conference on Software Engineering
None
2004
While the importance of multi-tier architectures for enterprise information systems is widely accepted and their benefits are well published, the systematic migration from monolithic legacy systems toward multi-tier architectures is known to a much lesser extent. In this paper we present a pattern on how to re-use elements of legacy systems within multi-tier architectures, which also allows for a smooth migration path. We report on experience we made with migrating existing municipal information systems towards a multitier architecture. The experience is generalized by describing the underlying pattern such that it can be re-used for similar architectural migration tasks. The emerged Dublo pattern is based on the partial duplication of business logic among legacy system and newly deployed application server. While this somehow contradicts the separation-of-concerns principle, it offers a high degree of flexibility in the migration process and allows for a smooth transition. Experience with the combination of outdated database technology with modern server-side component and Web services technologies is discussed. In this context, we also report on technology and architecture selection processes.
[Dublo architecture pattern, multitier architectures, Knowledge management, Information systems, software architecture, architectural migration, Databases, Web services, Software architecture, Computer architecture, Management information systems, legacy systems, User interfaces, business logic, enterprise information systems, information systems, Logic, business data processing, business information systems, Software engineering]
Static checking of dynamically generated queries in database applications
Proceedings. 26th International Conference on Software Engineering
None
2004
Many data-intensive applications dynamically construct queries in response to client requests and execute them. Java servlets, e.g., can create string representations of SQL queries and then send the queries, using JDBC, to a database server for execution. The servlet programmer enjoys static checking via Java's strong type system. However, the Java type system does little to check for possible errors in the dynamically generated SQL query strings. Thus, a type error in a generated selection query (e.g., comparing a string attribute with an integer) can result in an SQL runtime exception. Currently, such defects must be rooted out through careful testing, or (worse) might be found by customers at runtime. In this paper, we present a sound, static, program analysis technique to verify the correctness of dynamically generated query strings. We describe our analysis technique and provide soundness results for our static analysis algorithm. We also describe the details of a prototype tool based on the algorithm and present several illustrative defects found in senior software-engineering student-team projects, online tutorial examples, and a real-world purchase order system written by one of the authors.
[Algorithm design and analysis, Java, Software prototyping, database server, program diagnostics, selection query, Application software, Programming profession, SQL, Computer science, query processing, Runtime, Databases, Web services, Java servlet, program analysis, database querying, SQL query strings, software engineering, static checking, Testing]
Getting results from search-based approaches to software engineering
Proceedings. 26th International Conference on Software Engineering
None
2004
Like other engineering disciplines, software engineering is typically concerned with near optimal solutions or those which fall within a specified applicable tolerance. More recently, search-based techniques have started to find application in software engineering problem domains. This area of search-based software engineering has its origins in work on search-based testing, which began in the mid 1990s. Already, search-based solutions have been applied to software engineering problems right through the development life cycle.
[Software testing, Algorithm design and analysis, Software maintenance, optimising compilers, Evolutionary computation, Application software, Genetic algorithms, Constraint optimization, software development life cycle, optimization, search-based software engineering, search-based testing, Robustness, Genetic engineering, software engineering, search problems, Software engineering]
Validating the unit correctness of spreadsheet programs
Proceedings. 26th International Conference on Software Engineering
None
2004
Financial companies, engineering firms and even scientists create increasingly larger spreadsheets and spreadsheet programs. The creators of large spreadsheets make errors and must track them down. One common class of errors concerns unit errors, because spreadsheets often employ formulas with physical or monetary units. In this paper, we describe XeLda, our tool for unit checking Excel spreadsheets. The tool highlights cells if their formulas process values with incorrect units and if derived units clash with unit annotations. In addition, it draws arrows to the sources of the formulas for debugging. The tool is sensitive to many of the intricacies of Excel spreadsheets including tables, matrices, and even circular references. Using XeLda, we have detected errors in some published scientific spreadsheets.
[software debugging, program verification, Excel spreadsheets, Debugging, spreadsheet programs, Spreadsheet programs, Programming profession, Postal services, Design engineering, Databases, unit correctness validation, Education, Prototypes, XeLda tool, software tools, Software engineering, Testing]
Assume-guarantee verification of source code with design-level assumptions
Proceedings. 26th International Conference on Software Engineering
None
2004
Model checking is an automated technique that can be used to determine whether a system satisfies certain required properties. To address the "state explosion" problem associated with this technique, we propose to integrate assume-guarantee verification at different phases of system development. During design, developers build abstract behavioral models of the system components and use them to establish key properties of the system. To increase the scalability of model checking at this level, we have previously developed techniques that automatically decompose the verification task by generating component assumptions for the properties to hold. The design artifacts are subsequently used to guide the implementation of the system, but also to enable more efficient reasoning of the source code. In particular, we propose to use assumptions generated for the design to similarly decompose the verification of the actual system implementation. We demonstrate our approach on a significant NASA application, where design models were used to identify and correct a safety property violation, and the generated assumptions allowed us to check successfully that the property was preserved by the implementation.
[component assumption generation, program verification, Scalability, abstract behavioral models, Humans, Programming, Software safety, design-level assumptions, source code reasoning, software architecture, Space technology, system development, aerospace computing, Aerospace safety, NASA application, NASA, system verification, design artifacts, Application software, Computer science, model checking, state explosion problem, assume-guarantee verification, safety property violation, Software systems]
A flexible software process model
Proceedings. 26th International Conference on Software Engineering
None
2004
The development of software products is a complex activity with a large number of factors involved in defining success. As real-world experimentation is difficult and costly, researchers have used various techniques in an attempt to model the development process. This field of software process simulation has received substantial attention over the last twenty years. The aims have been to better understand the software development process and to mitigate the problems that continue to occur in the industry by providing support for management decision making.
[Kirk field collapse effect, Decision making, software development management, Programming, digital simulation, software process modelling, Environmental economics, Software development management, Vehicles, software process improvement, decision making, Computer industry, software product development, software process simulation, management decision making, Software engineering]
Software architecture reconstruction
Proceedings. 26th International Conference on Software Engineering
None
2004
Architecture reconstruction is the reverse engineering process that aims at recovering the past design decisions that have been made about the software architecture of a system. To be a successful activity, we need to identify the proper architecturally significant information and to extract it from the artefacts. How to identify extract/present/analyse it? What are the critical issues that have to be considered? How to manage the reconstruction process in a product family? What tools are available? This paper will address these and other questions that are relevant for the development of large and complex software systems. We introduce the key concepts of a software architecture description and the context of the architecture reconstruction activity. We present our architecture reconstruction method with a strong emphasis on its practical aspects and the tools supporting it. The extraction of architecturally significant information and its analysis are the key goals of our approach that will be demonstrated with a set of examples taken from real cases. We derive our experience mainly from the telecommunication domain. However, we believe the same general principles can be applied to other domains. The paper addresses software engineers and project managers that are involved in the development of complex software systems.
[Reverse engineering, Project management, Reconstruction algorithms, reverse engineering, Data mining, Information analysis, Software development management, software architecture, Software architecture, Engineering management, Computer architecture, software architecture reconstruction, Software systems, design recovery, software systems development]
Statestep: a tool for systematic, incremental specification
Proceedings. 26th International Conference on Software Engineering
None
2004
Statestep is an interactive tool for editing and checking specifications based on the finite state machine (FSM) model. The tabular notation supported is a novel yet simple one, first developed to specify the external behaviour of a series of audio compact disc recorders. The technique helps to describe system behaviour in a systematic manner, intended principally to ensure that no unusual scenarios, or corner cases, are overlooked at the specification stage. The notation is readily understandable and can reduce or eliminate the need for internal events or other structuring primitives. It supports a naturally incremental approach to specification and seems especially suited to dealing with the kind of complexity that can arise in embedded user interfaces.
[checking specifications, Java, program verification, systematic specification, Displays, user interfaces, Personnel, finite state machines, incremental specification, formal specification, interactive tool, Statestep, audio CD recorders, Automata, User interfaces, software tools, finite state machine, editing specifications, Software engineering]
Adding high availability and autonomic behavior to Web services
Proceedings. 26th International Conference on Software Engineering
None
2004
Rapid acceptance of the Web Services architecture promises to make it the most widely supported and popular object-oriented architecture to date. One consequence is that a wave of mission-critical Web Services applications will certainly be deployed in coming years. Yet the reliability options available within Web Services are limited in important ways. To use a term proposed by IBM, Web Services systems need to become far more autonomic, configuring themselves, diagnosing faults, and managing themselves. High availability applications need more attention. Moreover, the scenarios in which such issues arise often entail very large deployments, raising questions of scalability. In this paper we propose a path by which the architecture could be extended in these respects.
[Availability, Scalability, Mission critical systems, Web service reliability, Service oriented architecture, mission-critical Web Services, safety-critical software, Web services systems, self-configuring system, Security, Delay, software fault tolerance, self-managing system, Computer science, software architecture, autonomic system, fault-diagnosing system, Web services, Web services architecture, object-oriented architecture, IBM, Computer architecture, Computer applications, Internet]
Software engineering for large-scale multi-agent systems - SELMAS'04
Proceedings. 26th International Conference on Software Engineering
None
2004
The development of multiagent systems (MAS) is not a trivial task. In addition, with the advances in Internet technologies, MAS are undergoing a transition from closed to open architectures composed of a huge number of autonomous agents, which operate and move across different environments. In fact, openness introduces additional complexity to the system modeling, design and implementation. It also impacts on most quality attributes of MAS, including scalability, interoperability, reliability and adaptability. This paper brings together researchers and practitioners to discuss the current state and future direction of research in software engineering for open MAS. A particular interest is to understand those issues in the agent technology that make it difficult and/or improve the production of large open systems.
[Production systems, Multiagent systems, multi-agent systems, open systems, Scalability, Modeling, SELMAS04, Computer architecture, Open systems, Autonomous agents, software engineering, Large-scale systems, Internet, Software engineering, multiagent systems]
Improving UML design tools by formal games
Proceedings. 26th International Conference on Software Engineering
None
2004
The Unified Modeling Language (UML) is a standard language for modelling the design of object oriented software systems. The currently available UML design tools mainly provide support for drawing the UML diagrams, i.e. for recording a chosen design, but not for choosing a design. The design of a system is a non-trivial, iterative process and errors which are introduced at this level are usually very expensive to fix. Hence we argue that UML design tools should provide more support for the design activity as such. Ideally a UML design tool should allow the modeller to explore different design options, provide feedback about the design in its current state, and even make suggestions for improvements where this is possible. The usage of such a tool would be highly interactive and very much like a game, played repeatedly between modeller and tool. We claim that this similarity makes formal games a natural and intuitive choice for the definition of tool concepts. Since formal games can be used for verification, a game-based tool can provide feedback about flaws in the design that is formally founded. Games as used in verification normally require a complete formal model of the software system, and a formal specification of the property that is to be verified. Instead of this we would like to let the designer play a game directly on the basis of the UML model, even though a UML model is often incomplete and informally defined. We also want to allow the modeller explore variations of the design while the game is being played. The research hypothesis for this work is that formal games are a suitable technique for more advanced UML design tools which point the modeller to flaws in the design, help to improve the design and provide support for making design decisions.
[formal games, object-oriented programming, Law, Unified Modeling Language, Object oriented modeling, Unified modeling language, Laboratories, software design, object-oriented software, game theory, UML design tools, game-based tool, Application software, formal specification, Computer science, Software design, formal verification, specification languages, formal methods, Software systems, software tools, Informatics, Legal factors]
A fast assembly level reverse execution method via dynamic slicing
Proceedings. 26th International Conference on Software Engineering
None
2004
One of the most time consuming parts of debugging is trying to locate a bug. In this context, there are two powerful debugging aids which shorten debug time considerably: reverse execution and dynamic slicing. Reverse execution eliminates the need for repetitive program restarts every time a bug location is missed. Dynamic slicing, on the other hand, isolates code parts that influence an erroneous variable at a program point. In this paper, we present an approach which provides assembly level reverse execution along a dynamic slice. In this way, a programmer not only can find the instructions relevant to a bug, but also can obtain runtime values of variables in a dynamic slice while traversing the slice backwards in execution history. Reverse execution along a dynamic slice skips recovering unnecessary program state; therefore, it is potentially faster than full-scale reverse execution. The experimental results with four different benchmarks show a wide range of speedups from 1.3X for a small program with few data inputs to six orders of magnitude (1,928,500X) for 400x400 matrix multiply. Furthermore, our technique is very memory efficient. Our benchmark measurements show between 3.4X and 2240X memory overhead reduction as compared to our implementation of the same features using traditional approaches.
[program debugging, execution history, debugging aids, program point, reverse execution, Debugging, memory overhead reduction, Educational institutions, reverse engineering, repetitive program restarts, History, Programming profession, bug location, Power engineering computing, Runtime, Writing, program slicing, code parts, Assembly, dynamic slicing, Power engineering and energy]
Unifying artifacts and activities in a visual tool for distributed software development teams
Proceedings. 26th International Conference on Software Engineering
None
2004
In large projects, software developers struggle with two sources of complexity - the complexity of the code itself, and the complexity of the process of producing it. Both of these concerns have been subjected to considerable research investigation, and tools and techniques have been developed to help manage them. However, these solutions have generally been developed independently, making it difficult to deal with problems that inherently span both dimensions. We describe Augur, a visualization tool that supports distributed software development processes. Augur creates visual representations of both software artifacts and software development activities, and, crucially, allows developers to explore the relationship between them. Augur is designed not for managers, but for the developers participating in the software development process. We discuss some of the early results of informal evaluation with open source software developers. Our experiences to date suggest that combining views of artifacts and activities is both meaningful and valuable to software developers.
[visualization tool, public domain software, Programming, distributed software development teams, visual tool, visual representations, software artifacts, open source software developers, complexity management, Augur, software tools, program visualisation, distributed programming, Software engineering]
ProSim'04-the 5th international workshop on software process simulation and modeling
Proceedings. 26th International Conference on Software Engineering
None
2004
false
[]
Theme: an approach for aspect-oriented analysis and design
Proceedings. 26th International Conference on Software Engineering
None
2004
Aspects are behaviours that are tangled and scattered across a system. In requirements documentation, aspects manifest themselves as descriptions of behaviours that are intertwined, and woven throughout. Some aspects may be obvious, as specifications of typical crosscutting behaviour. Others may be more subtle, making them hard to identify. In either case, it is difficult to analyse requirements to locate all points in the system where aspects should be applied. These issues lead to problems achieving traceability of aspects throughout the development lifecycle. To identify aspects early in the software lifecycle, and establish sufficient traceability, developers need support for aspect identification and analysis in requirements documentation. To address this, we have devised the Theme approach for viewing the relationships between behaviours in a requirements document, identifying and isolating aspects in the requirements, and modelling those aspects using a design language. This paper describes the approach, and illustrates it with a case study and analysis.
[Process design, object-oriented programming, Prefetching, Unified modeling language, system documentation, Scattering, Documentation, Debugging, Educational institutions, crosscutting behavior, formal specification, Computer science, requirements documentation, Operating systems, aspect-oriented design, aspect-oriented analysis, Software engineering]
Agile development: evaluation and experience
Proceedings. 26th International Conference on Software Engineering
None
2004
Agile methods such as Extreme Programming, Crystal, Scrum, and others have attracted a lot of attention recently. Agile methods stress early and continuous delivery of software, welcome changing requirements, and value early feedback from customers. Agile methods seek to cut out inefficiency, bureaucracy, and anything that adds no value to a software product. Proponents of agile methods often see software specification and documentation as adding no value, which has led observers to conclude that agile development is nothing but unprincipled hacking, perhaps even an anarchic counter-reaction to bureaucratic, heavyweight software processes that demand ever more intermediate deliverables from developers. The purpose of this panel is to discuss under what circumstances agile methods work and don't work. Some of the key practices of agile methods are: scheduling according to feature priorities, incremental delivery of software, feedback from expert users, emphasis on face-to-face communication, pair development, minimalist design combined with refactoring, test-driven development, automated regression testing, daily integration, self-organizing teams, and periodic tuning of the methods. Working software is the primary measure of success. Find out what the latest practical experience with agile methods is and learn about the latest thinking in this area.
[Software testing, software specification, program documentation, program testing, Documentation, agile development, automated regression testing, Computer crime, Stress, Tuning, face-to-face communication, software incremental delivery, Automatic testing, test-driven development, Feedback, Extreme Programming, scheduling, software engineering, Scrum, Software measurement, Software engineering, Crystal]
Evidence-based software engineering
Proceedings. 26th International Conference on Software Engineering
None
2004
Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to determine how technologies will behave once deployed. Software engineering would benefit from adopting what it can of the evidence approach provided that it deals with the specific problems that arise from the nature of software engineering.
[Technological innovation, Costs, Laboratories, Psychology, Medical services, software engineeringskill factor, Best practices, EBSE, Computer science, EBM, software engineeringlifecycle factor, software engineering, Australia, Psychiatry, medical computing, evidence-based medicine, Software engineering, evidence-based software engineering]
Generating tests from counterexamples
Proceedings. 26th International Conference on Software Engineering
None
2004
We have extended the software model checker BLAST to automatically generate test suites that guarantee full coverage with respect to a given predicate. More precisely, given a C program and a target predicate p, BLAST determines the set L of program locations which program execution can reach with p true, and automatically generates a set of test vectors that exhibit the truth of p at all locations in L. We have used BLAST to generate test suites and to detect dead code in C programs with up to 30 K lines of code. The analysis and test vector generation is fully automatic (no user intervention) and exact (no false positives).
[Software testing, automatic programming, software model checker, program locations, program verification, program testing, Computerized monitoring, test vector generation, program execution, counterexamples, target predicate, Security, C language, Computer science, Condition monitoring, test generation, Automatic testing, Linux, Automata, Concrete, C program, Safety, BLAST, test suite generation]
Responsibilities and rewards: specifying design patterns
Proceedings. 26th International Conference on Software Engineering
None
2004
Design patterns provide guidance to system designers on how to structure individual classes or groups of classes, as well as constraints on the interactions among these classes, to enable them to implement flexible and reliable systems. Patterns are usually described informally. While such informal descriptions are useful and even essential, if we want to be sure that designers precisely and unambiguously understand the requirements that must be met when applying a given pattern, and be able to reliably predict the behaviors the resulting system exhibits, we also need formal characterizations of the patterns. In this paper, we develop an approach to formalizing design patterns. The requirements that a designer must meet with respect to the structures of the classes, as well as with respect to the behaviors exhibited by the relevant methods, are captured in the responsibilities component of the pattern's specification; the benefits that results by applying the pattern, in terms of specific behaviors that the resulting system is guaranteed to exhibit, are captured in the rewards component. One important aspect of many design patterns is their flexibility; our approach is designed to ensure that this flexibility is retained in the formalization of the pattern. We illustrate the approach by applying it to a standard design pattern.
[Information science, rewards component, object-oriented programming, pattern specification, pattern formalization, Unified modeling language, design patterns, pattern characterization, Power system reliability, formal specification]
The evaluation of large, complex UML analysis and design models
Proceedings. 26th International Conference on Software Engineering
None
2004
This paper describes techniques for analyzing large UML models. The first part of the paper describes heuristics and processes for creating semantically correct UML analysis and design models. The second part of the paper briefly describes the internal DesignAdvisor research tool that was used to analyze Siemens models. The results are presented and some interesting conclusions are drawn.
[large UML models, Unified Modeling Language, Unified modeling language, Humans, Inspection, Cognition, design model evaluation, Embedded software, Guidelines, heuristic programming, formal verification, semantically correct UML analysis, Siemens model analysis, systems analysis, specification languages, semantically correct UML design models, complex UML analysis evaluation, DesignAdvisor research tool, software performance evaluation, Testing, Business, Software engineering, large UML analysis evaluation]
Software visualization for object-oriented program comprehension
Proceedings. 26th International Conference on Software Engineering
None
2004
Software visualisation is the process of modelling software systems for comprehension. The comprehension of software systems both during and after development is a crucial component of the software process. The complex interactions inherent in the object-oriented paradigm make visualisation a particularly appropriate comprehension technique, and the large volume of information typically generated during visualisation necessitates tool support. In order to address the disadvantages with current visualisation techniques, an approach is proposed that integrates abstraction, structural and behavioural perspectives, and statically and dynamically extracted information. The aim of this research is to improve the effectiveness of visualisation techniques for large-scale software understanding based on the use of abstraction. interrelated facets and the integration of statically and dynamically extracted information.
[Visualization, object-oriented programming, Object oriented modeling, Reverse engineering, object-oriented software, Software performance, software modelling, Large scale integration, Data mining, software visualization, object-oriented program, information extraction, Computer architecture, software processes, Software systems, software engineering, Large-scale systems, program visualisation, Software tools]
A model driven approach for software systems reliability
Proceedings. 26th International Conference on Software Engineering
None
2004
The main contribution of this research is to provide platform-independent means to support reliability design following the principles of a model driven approach. The contribution aims to systematically address dependability concerns from the early to the late stages of software development. MDA appears to be a suitable framework to assess these concerns and, therefore, semantically integrate analysis and design models into one environment.
[Software testing, object-oriented programming, model driven architecture, software development, Unified modeling language, software reliability, Educational institutions, Reliability engineering, Software safety, model-driven approach, MDA, Computer science, software architecture, design models, Software design, Computer architecture, Software systems, software systems reliability, Software engineering]
A constraint architectural description approach to self-organising component-based software systems
Proceedings. 26th International Conference on Software Engineering
None
2004
We propose a constraint-based structural architectural description language for specifying and reasoning about self-organising software architectures and for guiding their evolution. We also introduce a notion of tactics added into the new language so as to mitigate the NP-complete problem of constraint satisfaction used for self-organising the specified architectures.
[Runtime environment, component-based software, object-oriented programming, software prototyping, Software algorithms, constraint-based description language, Educational institutions, self-adjusting systems, NP-complete problem, software evolution, selforganising software architecture, Component architectures, software architecture, Software architecture, constraint satisfaction, Computer architecture, specification languages, Software systems, Architecture description languages, constraint handling, Software engineering, computational complexity]
JDBC checker: a static analysis tool for SQL/JDBC applications
Proceedings. 26th International Conference on Software Engineering
None
2004
In data-intensive applications, it is quite common for the implementation code to dynamically construct database query strings and execute them. For example, a typical Java servlet Web service constructs SQL query strings and dispatches them over a JDBC connector to an SQL-compliant database. The servlet programmer enjoys static checking via Java's strong type system. However, the Java type system does little to check for possible errors in the dynamically generated SQL query strings. For example, a type error in a generated selection query (e.g., comparing a string attribute with an integer) can result in an SQL runtime exception. Currently, such defects must be rooted out through careful testing, or (worse) might be found by customers at runtime. In this paper, we describe JDBC Checker, a sound static analysis tool to verify the correctness of dynamically generated query strings. We have successfully applied the tool to find known and unknown defects in realistic programs using JDBC. We give a short description of our tool in this paper.
[JDBC checker, database management systems, query processing, SQL application, JDBC connector, Runtime, SQL-compliant database, Java type system, Databases, SQL query strings, SQL runtime exception, static checking, Testing, Java, database query strings, program diagnostics, selection query, JDBC application, Application software, Programming profession, Java servlet Web service, SQL, Computer science, Connectors, Web services, Automata, implementation code, static analysis tool, type error]
A weakly constrained approach to software change coordination
Proceedings. 26th International Conference on Software Engineering
None
2004
The development of a software system - of any reasonable size - from initial conception through ongoing maintenance and evolution accrues significant coordination overheads. Often the mechanisms used to manage change and coordination detract from the time developers have to pursue the principal goal of constructing the desired system. This is one of the motivators behind the emerging 'agile' methodologies. By permitting people to work as independently as possible and yet be aware of each other's dependencies and constraints, it is believed that these secondary costs can be minimised. The position taken in the research summarised here is that better support can be provided for this type of weakly constrained coordination by enhancing the awareness, automated traceability, and constraint checking capabilities of software configuration management systems. Current progress in the research and plans for future work are described.
[Productivity, Software maintenance, Costs, software development, software prototyping, Process planning, Communication system control, software development management, Documentation, software change coordination, software maintenance, constraint checking, software configuration management, Software development management, software evolution, agile methods, configuration management, weakly constrained coordination, Software systems, Contracts, Software engineering]
One more step in the direction of modularized integration concerns
Proceedings. 26th International Conference on Software Engineering
None
2004
Component integration creates value by automating the costly and error-prone task of imposing desired behavioral relationships on components manually. Requirements for component integration, however, complicate software design and evolution in several ways: first, they lead to coupling among components; second, the code that implements various integration concerns in a system is often scattered over and tangled with the code implementing the component behaviors. Straightforward software design techniques map integration requirements to scattered and tangled code, compromising modularity in ways that dramatically increase development and maintenance costs.
[IEEE news, Costs, Protocols, object-oriented programming, software development, software prototyping, software design, Scattering, Control systems, software maintenance, component integration requirements, software evolution, software architecture, Software design, modularized integration concerns, Usability, Testing, Software engineering]
Feature-oriented programming and the AHEAD tool suite
Proceedings. 26th International Conference on Software Engineering
None
2004
Feature oriented programming (FOP) is an emerging paradigm for application synthesis, analysis, and optimization. A target application is specified declaratively as a set of features, like many consumer products (e.g., personal computers, automobiles). FOP technology translates such declarative specifications into efficient programs.
[Java, AHEAD tool suite, Automatic programming, object-oriented programming, declarative specifications, feature-oriented programming, application optimization, Application software, target application, Design optimization, Domain specific languages, application analysis, FOP technology, Algebra, Query processing, Prototypes, application synthesis, Large-scale systems, software tools, Software engineering]
Empirical studies on requirement management measures
Proceedings. 26th International Conference on Software Engineering
None
2004
The goal of this research is to demonstrate that a subset of a set of 38 requirements management measures are good predictors of stability and volatility of requirements and change requests. At the time of writing we have theoretically validated ten of these 38 measures. We are currently planning and performing an industrial case study where we want to reach the goal described above.
[Costs, Stability, Project management, software management, empirical research, Time measurement, requirement stability, formal specification, requirements engineering, requirement management measures, Writing, software measurement, requirement volatility, Error correction, Software measurement, Size control, Quality management, Software engineering, software metrics]
An empirical comparison of dynamic impact analysis algorithms
Proceedings. 26th International Conference on Software Engineering
None
2004
Impact analysis - determining the potential effects of changes on a software system - plays an important role in software engineering tasks such as maintenance, regression testing, and debugging. In previous work, two new dynamic impact analysis techniques, CoverageImpact and PathImpact, were presented. These techniques perform impact analysis based on data gathered about program behavior relative to specific inputs, such as inputs gathered from field data, operational profile data, or test-suite executions. Due to various characteristics of the algorithms they employ, CoverageImpact and PathImpact are expected to differ in terms of cost and precision; however, there have been no studies to date examining the extent to which such differences may emerge in practice. Since cost-precision tradeoffs may play an important role in technique selection and further research, we wished to examine these tradeoffs. We therefore designed and performed an empirical study, comparing the execution and space costs of the techniques, as well as the precisions of the impact analysis results that they report. This paper presents the results of this study.
[Algorithm design and analysis, Software testing, Performance evaluation, System testing, program debugging, CoverageImpact, Costs, program testing, program behavior, Heuristic algorithms, program diagnostics, regression testing, Debugging, software maintenance, dynamic impact analysis algorithms, software system, operational profile data, Software systems, software engineering, Performance analysis, PathImpact, Software engineering]
Object-oriented reengineering patterns
Proceedings. 26th International Conference on Software Engineering
None
2004
The rapid growth of object-oriented development over the past twenty years has given rise to many object-oriented systems that are large, complex and hard to maintain. These systems exhibit a range of problems, effectively preventing them from satisfying the evolving requirements imposed by their customers. In our paper, we address problem of understanding and reengineering such object-oriented legacy systems. The material is presented as a set of "reengineering patterns" - recurring solutions that experts apply while reengineering and maintaining object-oriented systems. The patterns distill successful techniques in planning a reengineering project, reverse-engineering, problem detection, migration strategies and software redesign. The principles and techniques described have been observed and validated in a number of industrial projects, and reflect best practice in object-oriented reengineering.
[object-oriented programming, problem detection, Maintenance engineering, Strategic planning, reengineering patterns, reverse engineering, reverse-engineering, object-oriented systems, Application software, software maintenance, Best practices, Design engineering, systems re-engineering, object-oriented development, legacy systems, Software systems, Computer industry, Books, Object oriented programming, object-oriented reengineering, Software engineering, software redesign]
DiscoTect: a system for discovering architectures from running systems
Proceedings. 26th International Conference on Software Engineering
None
2004
One of the challenging problems for software developers is guaranteeing that a system as built is consistent with its architectural design. In this paper, we describe a technique that uses run time observations about an executing system to construct an architectural view of the system. With this technique, we develop mappings that exploit regularities in system implementation and architectural style. These mappings describe how low-level system events can be interpreted as more abstract architectural operations. We describe the current implementation of a tool that uses these mappings, and show that it can highlight inconsistencies between implementation and architecture.
[architectural design, software development, Inspection, Throughput, Reliability engineering, Research and development, Design engineering, software architecture, Runtime, Databases, Software architecture, architecture discovery, DiscoTect, abstract architectural operations, Computer architecture, system monitoring, software tools, Monitoring]
Design of large-scale polylingual systems
Proceedings. 26th International Conference on Software Engineering
None
2004
Building systems from existing applications written in two or more languages is common practice. Such systems are polylingual. Polylingual systems are relatively easy to build when the number of APIs needed to achieve language interoperability is small. However, when the number of distinct APIs become large, maintaining and evolving polylingual systems becomes a notoriously difficult task. In this paper, we present a simple, practical, and effective way to develop, maintain, and evolve large-scale polylingual systems. Our approach relies on recursive type systems whose instances can be manipulated by reflection. Foreign objects (i.e. objects that are not defined in a host programming language) are abstracted as graphs and path expressions are used for accessing and manipulating data. Path expressions are implemented by type reification - turning foreign type instances into first-class objects and enabling access to and manipulation of them in a host programming language. Doing this results in multiple benefits, including coding simplicity and uniformity that we demonstrate in a complex commercial project.
[large-scale polylingual systems, application program interfaces, open systems, software prototyping, recursive type systems, Buildings, application programming interface, system design, Turning, Reflection, Application software, Data mining, software maintenance, Computer languages, software architecture, system maintenance, Web pages, system development, Software systems, language interoperability, API, Large-scale systems, system evolution, Software engineering]
An experimental, pluggable infrastructure for modular configuration management policy composition
Proceedings. 26th International Conference on Software Engineering
None
2004
Building a configuration management (CM) system is a difficult endeavor that regularly requires tens of thousands of lines of code to be written. To reduce this effort, several experimental infrastructures have been developed that provide reusable repositories upon which to build a CM system. In this paper, we push the idea of reusability even further. Whereas existing infrastructures only reuse a generic CM model (i.e., the data structures used to capture the evolution of artifacts), we have developed an experimental infrastructure, called MCCM, that additionally allows reuse of CM policies (i.e., the rules by which a user evolves artifacts stored in a CM system). The key contribution underlying MCCM is that a CM policy is not a monolithic entity; instead, it can be composed from small modules that each addresses a unique dimension of concern. Using the pluggable architecture and base set of modules of MCCM, then, the core of a desired new CM system can be rapidly composed by choosing appropriate existing modules and implementing any remaining modules only as needed. We demonstrate our approach by showing how the use of MCCM significantly reduces the effort involved in creating several representative CM systems.
[Computer science, configuration management, pluggable infrastructure, MCCM, policy composition, experimental infrastructure, software reusability, Data structures, data structures, Informatics, Graphical user interfaces, Software engineering]
Comparison of software product line architecture design methods: COPA, FAST, FORM, KobrA and QADA
Proceedings. 26th International Conference on Software Engineering
None
2004
Product line architectures (PLAs) have been under continuous attention in the software research community during the past few years. Although several methods have been established to create PLAs there are not available studies comparing PLA methods. Five methods are known to answer the needs of software product lines: COPA, FAST, FORM, KobrA and QADA. In this paper, an evaluation framework is introduced for comparing PLA design methods. The framework considers the methods from the points of view of method context, user, structure and validation. Comparison revealed distinguishable ideologies between the methods. Therefore, methods do not overlap even though they all are PLA design methods. All the methods have been validated on various domains. The most common domains are telecommunication infrastructure and information domains. Some of the methods apply software standards; at least OMG's MDA for method structures, UML for language and IEEE Std-1471-2000 for viewpoint definitions.
[COPA, Terminology, Design methodology, Unified modeling language, architecture design, FORM, QADA, Product design, Application software, software architecture, software product line, Software architecture, Computer architecture, Programmable logic arrays, KobrA, Software standards, software engineering, product line architectures, FAST, software standards, Software engineering]
An introduction to computing system dependability
Proceedings. 26th International Conference on Software Engineering
None
2004
It is important that computer engineers, software engineers, project managers, and users understand the major elements of current technology in the field of dependability, yet this material tends to be unfamiliar to researchers and practitioners alike. Researchers are often concerned in one way or another with some aspect of what is mistakenly called software "reliability". All practitioners are concerned with the "reliability" of the software that they produce but researchers and practitioners tend not to understand fully the broader impact of their work. A lot of research, such as that on testing, is concerned directly with software dependability. Understanding dependability more fully allows researchers to be more effective. Similarly, practitioners can direct their efforts during development more effectively if they have a better understanding of dependability.
[software reliability, Switches, Circuit faults, software maintenance, software fault tolerance, Degradation, Wiring, Computer science, Fault tolerant systems, Pacemakers, systems analysis, software dependability, Hardware, Error correction, computing system dependability, Software engineering]
A tool for writing and debugging algebraic specifications
Proceedings. 26th International Conference on Software Engineering
None
2004
Despite their benefits, programmers rarely use formal specifications, because they are difficult to write and they require an up front investment in time. To address these issues, we present a tool that helps programmers write and debug algebraic specifications. Given an algebraic specification, our tool instantiates a prototype that can be used just like any regular Java class. The tool can also modify an existing application to use the prototype generated by the interpreter instead of a hand-coded implementation. The tool improves the usability of algebraic specifications in the following ways: (i) A programmer can run an algebraic specification to study its behavior. The tool reports in which way a specification is incomplete for a client application. (ii) The tool can check whether a specification and a hand-coded implementation behave the same for a particular run of a client application. (iii) A prototype can be used when a hand-coded implementation is not yet available. Two case studies demonstrate how to use the tool.
[Java, program debugging, Debugging, Documentation, Containers, Java class, Formal specifications, formal specifications, Programming profession, software tool, Prototypes, algebraic specification debugging tool, Writing, software tools, algebraic specification, algebraic specification writing tool, Software engineering, Testing]
Requirements engineering tools go mobile
Proceedings. 26th International Conference on Software Engineering
None
2004
Software tools that support or automate software engineering tasks are typically available on traditional desktop-based workstations. In contrast, mobile tools for requirements engineers offer considerable potential. In the last few years the capabilities of mobile devices such as personal digital assistants (PDAs) have advanced considerably. These devices now provide faster processing, increased storage, and improved connectivity. Hence, mobile computing will become a dominant computing paradigm. These developments let us envision mobile tools for RE that are sophisticated enough to be used in real-world projects.
[Design automation, Audio recording, PDA, software engineering tools, Human computer interaction, Design engineering, mobile computing, requirements engineering, Engineering management, mobile devices, Systems engineering and theory, computer aided software engineering, Workstations, notebook computers, software tools, mobile tools, Personal digital assistants, Software tools, Software engineering, personal digital assistants]
Polyphony in architecture
Proceedings. 26th International Conference on Software Engineering
None
2004
Based on interviews with a number of architects and managers from a wide range of organizations, we characterize how architecture is perceived in practice. We identify three groups of organizations that differ with respect to their level of architectural thinking and the alignment of business and IT on architectural issues. Analysis of the interviews further indicates that these three groups differ in the architecture aspects and critical success factors they emphasize. Our results provide a starting point for assessing architecture maturity and alignment within organizations, and can be used to help harmonize different architectural tunes played within organizations.
[Visualization, Instruments, architectural thinking, Buildings, Government, architecture alignment, IT, software architecture, architecture maturity, Software architecture, Insurance, business alignment, Computer architecture, Management information systems, Software systems, Software engineering]
An empirical study of software reuse vs. defect-density and stability
Proceedings. 26th International Conference on Software Engineering
None
2004
The paper describes results of an empirical study, where some hypotheses about the impact of reuse on defect-density and stability, and about the impact of component size on defects and defect-density in the context of reuse are assessed, using historical data (data mining) on defects, modification rate, and software size of a large-scale telecom system developed by Ericsson. The analysis showed that reused components have lower defect-density than non-reused ones. Reused components have more defects with highest severity than the total distribution, but less defects after delivery, which shows that that these are given higher priority to fix. There are an increasing number of defects with component size for non-reused components, but not for reused components. Reused components were less modified (more stable) than non-reused ones between successive releases, even if reused components must incorporate evolving requirements from several application products. The study furthermore revealed inconsistencies and weaknesses in the existing defect reporting system, by analyzing data that was hardly treated systematically before.
[Productivity, Data analysis, object-oriented programming, Stability, data analysis, software reuse, Laboratories, Time to market, data mining, Telecommunications, Ericsson, Application software, defect-density, Information science, component reuse, historical data, defect reporting system, Software quality, software reusability, Large-scale systems, large-scale telecom system, stability]
Automated support for development, maintenance, and testing in the presence of implicit flow control
Proceedings. 26th International Conference on Software Engineering
None
2004
Although object-oriented languages can improve programming practices, their characteristics may introduce new problems for software engineers. One important problem is the presence of implicit control flow caused by exception handling and polymorphism. Implicit control flow causes complex interactions, and can thus complicate software-engineering tasks. To address this problem, we present a systematic and structured approach, for supporting these tasks, based on the static and dynamic analyses of constructs that cause implicit control flow. Our approach provides software engineers with information for supporting and guiding development and maintenance tasks. We also present empirical results to illustrate the potential usefulness of our approach. Our studies show that, for the subjects considered, complex implicit control flow is always present and is generally not adequately exercised.
[Software testing, software-engineering tasks, Software maintenance, Java, automatic programming, Automatic programming, program testing, software development, Industrial control, software testing, data flow analysis, exception handling, Educational institutions, Control systems, polymorphism, software maintenance, implicit flow control, Automatic testing, Automatic control, object-oriented languages, automated support, Object oriented programming]
Traits: tools and methodology
Proceedings. 26th International Conference on Software Engineering
None
2004
Traits are an object-oriented programming language construct that allow groups of methods to be named and reused in arbitrary places in an inheritance hierarchy. Classes can use methods from traits as well as defining their own methods and instance variables. Traits thus enable a new style of programming, in which traits rather than classes are the primary unit of reuse. However, the additional sub-structure provided by traits is always optional: a class written using traits can also be viewed as a flat collection of methods, with no change in its semantics. This paper describes the tool that supports these two alternate views of a class, called the traits browser, and the programming methodology that we are starting to develop around the use of traits.
[Java, Protocols, object-oriented programming, programming methodology, inheritance, object-oriented programming language, inheritance hierarchy, Programming profession, traits browser, object-oriented languages, Functional programming, Object oriented programming, Software engineering]
Revisiting statechart synthesis with an algebraic approach
Proceedings. 26th International Conference on Software Engineering
None
2004
The idea of synthesizing statecharts out of a collection of scenarios has received a lot of attention in recent years. However due to the poor expressive power of first generation scenario languages, including UML 1.x sequence diagrams, the proposed solutions often use ad hoc tricks and suffer from many shortcomings. The recent adoption in UML 2.0 of a richer scenario language, including interesting composition operators, now makes it possible to revisit the problem of statechart synthesis with a radically new approach. Inspired by the way UML 2.0 sequence diagrams can be algebraically composed, we first define an algebraic framework for composing statecharts. Then we show how to leverage the algebraic structure of UML 2.0 sequence diagrams to get a direct algorithm for synthesizing a composition of statecharts out of them. The synthesized statecharts exhibit interesting properties that make them particularly useful as a basis for the detailed design process. Beyond offering a systematic and semantically well founded method, another interest of our approach lies in its flexibility: the modification or replacement of a given scenario has a limited impact on the synthesis process, thus fostering a better traceability between the requirements and the detailed design.
[Process design, Unified modeling language, Humans, poor expressive power, ad hoc tricks, UML 1.x sequence diagrams, History, formal specification, statechart synthesis, algebraic approach, algebraic framework, statechart composition, algebraic structure, composition operators, UML 2.0 sequence diagrams, specification languages, first generation scenario languages, direct algorithm, algebraic specification, Power generation, Software engineering]
Using simulation to empirically investigate test coverage criteria based on statechart
Proceedings. 26th International Conference on Software Engineering
None
2004
A number of testing strategies have been proposed using state machines and statecharts as test models in order to derive test sequences and validate classes or class clusters. Though such criteria have the advantage of being systematic, little is known on how cost effective they are and how they compare to each other. This article presents a precise simulation and analysis procedure to analyze the cost-effectiveness of statechart-based testing techniques. We then investigate, using this procedure, the cost and fault detection effectiveness of adequate test sets for the most referenced coverage criteria for statecharts on three different representative case studies. Through the analysis of common results and differences across studies, we attempt to draw more general conclusions regarding the costs and benefits of using the criteria under investigation.
[Software testing, System testing, Costs, program testing, Computational modeling, Laboratories, simulation, fault detection, statechart-based testing, cost detection, finite state machines, Analytical models, Fault detection, state machines, Software quality, Systems engineering and theory, Performance analysis, test coverage criteria]
Imposing a memory management discipline on software deployment
Proceedings. 26th International Conference on Software Engineering
None
2004
The deployment of software components frequently fails because dependencies on other components are not declared explicitly or are declared imprecisely. This results in an incomplete reproduction of the environment necessary for proper operation, or in interference between incompatible variants. In this paper, we show that these deployment hazards are similar to pointer hazards in memory models of programming languages and can be countered by imposing a memory management discipline on software deployment. Based on this analysis, we have developed a generic, platform and language independent, discipline for deployment that allows precise dependency verification; exact identification of component variants; computation of complete closures containing all components on which a component depends; maximal sharing of components between such closures; and concurrent installation of revisions and variants of components. We have implemented the approach in the Nix deployment system, and used it for the deployment of a large number of existing Linux packages. We compare its effectiveness to other deployment systems.
[Unix, Software maintenance, deployment hazards, Nix deployment system, component variants identification, programming languages, Concurrent computing, storage management, memory management, Linux packages, pointer hazards, software engineering, software components, Java, Interference, Independent component analysis, software deployment, Hazards, Application software, dependency verification, Computer languages, Linux, Memory management, Packaging, language independent, software installations]
Usability-supporting architectural patterns
Proceedings. 26th International Conference on Software Engineering
None
2004
Software architects have techniques to deal with many quality attributes such as performance, reliability, and maintainability. Usability, however, has traditionally been concerned primarily with presentation and not been a concern of software architects beyond separating the user interface from the remainder of the application. In this paper, we present usability-supporting architectural patterns. Each pattern describes a usability concern that is not supported by separation alone. For each concern, a usability-supporting architectural pattern provides the forces from the characteristics of the task and environment, the human, and the state of the software to motivate an implementation independent solution cast in terms of the responsibilities that must be fulfilled to satisfy the forces. Furthermore, each pattern includes a sample solution implemented in the context of an overriding separation based pattern such as J2EE Model View Controller.
[Software maintenance, Java, J2EE Model View Controller, NASA, Humans, Software performance, user interfaces, Application software, user interface, software usability, Computer science, software architecture, Software quality, User interfaces, architectural patterns, Usability, Software engineering]
Architecting in the face of uncertainty: an experience report
Proceedings. 26th International Conference on Software Engineering
None
2004
Understanding an application's functional and non-functional requirements is normally seen as essential for developing a robust product suited to client needs. This paper describes our experiences in a project that, by necessity, commenced well before concrete client requirements could be known. After a first version of the application was successfully released, emerging requirements forced an evolution of the application architecture. The key reasons for this are explained, along with the architectural strategies and software engineering practices that were adopted. The resulting application architecture is highly flexible, modifiable and scalable, and therefore should provide a solid foundation for the duration of the application's lifetime.
[Uncertainty, project management, Laboratories, Glass, client requirements, application architecture, Application software, formal specification, Information analysis, software architecture, Operating systems, nonfunctional requirements, software engineering practices, Computer architecture, Robustness, Concrete, functional requirements, Workstations]
SNIAFL: towards a static non-interactive approach to feature location
Proceedings. 26th International Conference on Software Engineering
None
2004
To facilitate software maintenance and evolution, a helpful step is to locate features concerned in a particular maintenance task. In the literature, both dynamic and interactive approaches have been proposed for feature location. In this paper, we present a static and non-interactive method for achieving this objective. The main idea of our approach is to use the information retrieval (IR) technology to reveal the basic connections between features and computational units in source code. Due to the characteristics of the retrieved connections, we use a static representation of the source code named BRCG to further recover both the relevant and the specific computational units for each feature. Furthermore, we recover the relationships among the relevant units for each feature. A premise of our approach is that programmers should use meaningful names as identifiers. We perform an experimental study based on a GNU system to evaluate our approach. In the experimental study, we present the detailed quantitative experimental data and give the qualitative analytical results.
[Performance evaluation, Software maintenance, Costs, software prototyping, interactive approaches, Humans, Optical computing, static representation, software evolution, noninteractive method, feature extraction, dynamic approaches, static noninteractive approach, information retrieval, source code, Information retrieval, feature location, BRCG, software maintenance, Sun, Programming profession, static method, SNIAFL, Software systems, Software engineering]
Automated generation of test programs from closed specifications of classes and test cases
Proceedings. 26th International Conference on Software Engineering
None
2004
Most research on automated specification-based software testing has focused on the automated generation of test cases. Before a software system can be tested, it must be set up according to the input requirements of the test cases. This setup process is usually performed manually, especially when testing complex data structures and databases. After the system is properly set up, a test execution tool runs the system according to the test cases and pre-recorded test scripts to obtain the outputs, which are evaluated by a test evaluation tool. This paper complements the current research on automated specification-based testing by proposing a scheme that combines the setup process, test execution, and test validation into a single test program for testing the behavior of object-oriented classes. The test program can be generated automatically given the desired test cases and closed specifications of the classes. With closed specifications, every class method is defined in terms of other methods which are, in turn, defined in their own class specifications. The core of the test program generator is a partial-order planner which plans the sequence of instructions required in the test program. The planner is, in turn, implemented as a tree-search algorithm. It makes function calls to the Omega Calculator library, which solves the constraints given in the test cases. A first-cut implementation of the planner has been completed, which is able to handle simple arithmetics and existential quantifications in the class specifications. A soundness and completeness proof sketch of the planner is also provided in this paper.
[Software testing, Performance evaluation, System testing, test validation, program testing, automated test program generation, formal specification, Databases, test cases specifications, Libraries, data structures, Omega Calculator library, automatic programming, automatic test pattern generation, Automatic programming, object-oriented programming, software testing, Data structures, tree searching, automated specification-based testing, tree-search algorithm, classes specifications, Automatic testing, object-oriented classes, Software systems, Arithmetic]
An open framework for dynamic reconfiguration
Proceedings. 26th International Conference on Software Engineering
None
2004
Dynamic reconfiguration techniques appear promising for building systems that have requirements for adaptability and/or high availability. Current systems that support dynamic reconfiguration tend to use a single, fixed, reconfiguration algorithm to manage the change process. Furthermore, existing change management systems lack support for measuring the impact of reconfiguration on a running system. In this paper, we introduce OpenRec, an open framework for managing dynamic reconfiguration which addresses these drawbacks. Using OpenRec, developers can observe the costs, in terms of time and disturbance, associated with making a particular run-time change. In addition, OpenRec employs an extensible set of reconfiguration algorithms where one algorithm can be substituted for another. Developers can thus make an informed decision as to which algorithm to use based on comparative analysis. Finally, OpenRec is itself dynamically reconfigurable.
[Availability, Algorithm design and analysis, Costs, Adaptive systems, Protocols, reconfiguration algorithm, Mobile communication, Telecommunication computing, OpenRec, software maintenance, Communication switching, change management, Computer science, configuration management, comparative analysis, Runtime, run-time change, management of change, dynamic reconfiguration]
Using Web service technologies to create an information broker: an experience report
Proceedings. 26th International Conference on Software Engineering
None
2004
This paper reports on our experiences with using the emerging Web service technologies and tools to create a demonstration information broker system as part of our research into information management in a distributed environment. To provide a realistic context, we chose to study the use of information in the healthcare domain, and this context sets some challenging parameters and constraints for our research and for the demonstration system. In this paper, we both report on the extent to which existing Web service technologies have proved to be mature enough to meet these requirements, and also assess their current limitations.
[Web service technologies, information use, information management, Medical services, distributed environment, healthcare domain, Web service tools, Technology planning, information industry, Distributed computing, Environmental management, Computer science, software architecture, Technology management, Web services, Prototypes, Health information management, Software systems, Internet, information broker system, health care]
Bi-criteria models for all-uses test suite reduction
Proceedings. 26th International Conference on Software Engineering
None
2004
Using bi-criteria decision making analysis, a new model for test suite minimization has been developed that pursues two objectives: minimizing a test suite with regard to a particular level of coverage while simultaneously maximizing error detection rates. This new representation makes it possible to achieve significant reductions in test suite size without experiencing a decrease in error detection rates. Using the all-uses inter-procedural data flow testing criterion, two binary integer linear programming models were evaluated, one a single-objective model, the other a weighted-sums bi-criteria model. The applicability of the bi-criteria model to regression test suite maintenance was also evaluated. The data show that minimization based solely on definition-use association coverage may have a negative impact on the error detection rate as compared to minimization performed with a bi-criteria model that also takes into account the ability of test cases to reveal error. Results obtained with the bi-criteria model also indicate that test suites minimized with respect to a collection of program faults are effective at revealing subsequent program faults.
[Software testing, Performance evaluation, Minimization methods, program testing, integer programming, software testing, Decision making, data flow analysis, Educational institutions, data flow testing, linear programming, NP-complete problem, Application software, error detection, bicriteria decision making, test suite reduction, Automatic testing, Integer linear programming, minimisation, Mathematical programming, test suite minimization]
Dynamic configuration of resource-aware services
Proceedings. 26th International Conference on Software Engineering
None
2004
An important emerging requirement for computing systems is the ability to adapt at run time, taking advantage of local computing devices, and coping with dynamically changing resources. Three specific technical challenges in satisfying this requirement are to (1) select an appropriate set of applications or services to carry out a user's task, (2) allocate (possibly scarce) resources among those applications, and (3) reconfigure the applications or resource assignments if the situation changes. In this paper, we show how to provide a shared infrastructure that automates configuration decisions given a specification of the user's task. The heart of the approach is an analytical model and an efficient algorithm that can be used at run time to make near-optimal (re)configuration decisions. We validate this approach both analytically and by applying it to a representative scenario.
[Availability, Heart, Quality of service, Ubiquitous computing, resource-aware services, reconfiguration decisions, ubiquitous computing, Vehicle dynamics, formal specification, user specification, Computer science, configuration management, Analytical models, resource allocation, Bandwidth, Writing, computing systems, Resource management, dynamic configuration, resource assignments]
Autonomous adaptation to dynamic availability using a service-oriented component model
Proceedings. 26th International Conference on Software Engineering
None
2004
This paper describes a project, called Gravity, that defines a component model, where components provide and require services (i.e., functionality) and all component interaction occurs via services. This approach introduces service-oriented concepts into a component model and execution environment. The goal is to support the construction and execution of component-based applications that are capable of autonomously adapting at run time due to the dynamic availability of the services provided by constituent components. In this component model the execution environment manages an application that is described as an abstract composition that can adapt and evolve at run time depending on available functionality. The motivation of Gravity is to simplify the construction of applications where dynamic availability arises, ranging from modern extensible systems to novel computing approaches, such as context-aware applications.
[Availability, dynamic availability, abstract composition, object-oriented programming, resource allocation, service-oriented component model, multifidelity applications, software engineering, ubiquitous computing, component interaction, service composition, Gravity project]
DMS/spl reg/: program transformations for practical scalable software evolution
Proceedings. 26th International Conference on Software Engineering
None
2004
While a number of research systems have demonstrated the potential value of program transformations, very few of these systems have made it into practice. The core technology for such systems is well understood; what remains is integration and more importantly, the problem of handling the scale of the applications to be processed. This paper describes DMS, a practical, commercial program analysis and transformation system, and sketches a variety of tasks to which it has been applied, from redocumenting to large-scale system migration. Its success derives partly from a vision of design maintenance and the construction of infrastructure that appears necessary to support that vision. DMS handles program scale by careful space management, computational scale via parallelism and knowledge acquisition scale via domains.
[space management, Java, Software maintenance, design maintenance, Knowledge acquisition, program diagnostics, software prototyping, knowledge acquisition, Knowledge management, Paper technology, software maintenance, program transformations, software reengineering, DMS/spl reg/, software evolution, Concurrent computing, systems re-engineering, Space technology, program analysis, Parallel processing, Large-scale systems, Software tools]
Heuristic-based model refinement for FLAVERS
Proceedings. 26th International Conference on Software Engineering
None
2004
FLAVERS is a finite-state verification approach that allows an analyst to incrementally add constraints to improve the precision of the model of the system being analyzed. Except for trivial systems, however, it is impractical to compute which constraints should be selected to produce precise results for the least cost. Thus, constraint selection has been a manual task, guided by the intuition of the analyst. In this paper, we investigate several heuristics for selecting task automaton constraints, a kind of constraint that tends to reduce infeasible task interactions. We describe an experiment showing that one of these heuristics is extremely effective at improving the precision of the analysis results without significantly degrading performance.
[Costs, Data analysis, program verification, Laboratories, finite-state verification, FLAVERS, finite state machines, constraint selection, task interactions, model refinement, Information analysis, Computer science, Degradation, task automaton constraints, heuristic programming, Automata, Performance analysis, constraint handling, Software engineering]
Letter from the chairs
Proceedings. 26th International Conference on Software Engineering
None
2004
Welcome to ICSE 2004, the 26th International Conference on Software Engineering! ICSE takes place this year in the lively and historic city of Edinburgh, Scotland, at the modern, purpose-built Edinburgh International Conference Centre.
[]
Conference organization
Proceedings. 26th International Conference on Software Engineering
None
2004
Provides a listing of current committee members and society officers.
[]
Program Committee
Proceedings. 26th International Conference on Software Engineering
None
2004
Provides a listing of current committee members.
[]
Program Committee
Proceedings. 26th International Conference on Software Engineering
None
2004
Provides a listing of current committee members.
[]
Grid small and large: distributed systems and global communities
Proceedings. 26th International Conference on Software Engineering
None
2004
Summary form only given. Grid technologies seek to enable collaborative problem solving and resource sharing within distributed, multi-organizational virtual organizations. Two characteristics of Grid environments make the engineering of systems and applications particularly challenging. First, we face the familiar difficulties that arise when developing software that must provide reliability, performance, and security in environments that may be heterogeneous, unpredictable, unreliable, and hostile; second, we must allow this software to be deployed, operated, and evolved in an environment characterized by multiple participants with different and perhaps conflicting views on system function and design. The author presents the work that is being done to address these challenges.
[Protocols, software development, Earthquake engineering, software reliability, grid computing, distributed virtual organizations, Software performance, collaborative problem solving, Application software, grid environments, Couplings, Computer science, Web services, resource sharing, Writing, Collaborative work, Grid computing, distributed systems, software engineering, software security, global communities, software performance, multiorganizational virtual organizations]
BoF: new directions in UK software engineering research
Proceedings. 26th International Conference on Software Engineering
None
2004
A large number of UK researchers in software engineering are expected to attend ICSE 2004. The BoF session will use this opportunity to bring many of them together to consider ways of significantly improving the impact of UK software engineering research.
[Computer science, Profitability, Education, Buildings, Technology transfer, Computer industry, Systems engineering and theory, Australia, Software engineering, Testing]
4th international workshop on adoption-centric software engineering
Proceedings. 26th International Conference on Software Engineering
None
2004
The ACSE series of events aims to advance the adoption of software engineering tools and techniques by bringing together researchers and practitioners who investigate novel approaches to fostering the transition between limited-use research prototypes and broadly applicable practical solutions. One proven technique to aid adoption is to leverage existing commercial platforms and infrastructure. The key objective of ACSE 2004 is to explore innovative approaches to the adoption of proofof- concept systems by embedding them in extensions of Commercial Off-The-Shelf (COTS) products and/or using middleware technologies to integrate the prototypes into existing toolsets.
[]
Third international workshop on distributed event-based systems DEBS '04
Proceedings. 26th International Conference on Software Engineering
None
2004
Event-based systems are made of reactive components that cooperate by exchanging information and control in the form of events. The occurrence of an event, as well as the data characterizing that event, may trigger the execution of one or more components, which may in turn generate other events. In a distributed event-based system, where components are physically distributed over a network, events must be signaled to remote components through some form of communication mechanism. This workshop is about (1) the engineering methods that support the design of eventbased applications, and (2) the design of the communication mechanisms that support the event-based interaction of components over a network.
[]
1st international workshop on advances and applications of problem frames
Proceedings. 26th International Conference on Software Engineering
None
2004
Software problems originate from real world problems. A software solution must address its real world problem in a satisfactory way. A software engineer must therefore understand the real world problem that their software intends to address. To be able to do this, the software engineer must understand the problem context and how it is to be affected by the proposed software, expressed as the requirements. Without this knowledge the engineer can only hope to chance upon the right solution for the right problem. Application of the Problem Frames approach may well be a way of meeting this need.
[]
The 3rd international workshop on global software development
Proceedings. 26th International Conference on Software Engineering
None
2004
The goal of this workshop is to provide an opportunity for researchers and industry practitioners to explore both the state-of-the art and the state-of-thepractice in global software development (GSD). Increased globalization of software development creates software engineering challenges due to the impact of temporal, geographical and cultural differences, and requires development of techniques and technologies to address these issues. The workshop will foster interaction between practitioners and researchers and help grow a community of interest in this area. Practitioners experiencing challenges in GSD will share their concerns and successful solutions and learn from research about current investigations. Researchers addressing GSD will gain a better understanding of the key issues facing practitioners and share their work in progress with others in the field.
[]
Twin workshops on architecting dependable systems (WADS 2004)
Proceedings. 26th International Conference on Software Engineering
None
2004
This workshop summary gives a brief overview on the workshop on &#x0201C;Architecting Dependable Systems&#x0201D; held in conjunction with ICSE 2004. It is organised as a twin workshop to another to be held in conjunction with the International Conference on Dependable Systems and Network (DSN 2004). This is an ambitious project that aims to promote crossfertilization between the communities of software architectures and dependability. Both communities will benefit from the clarification of approaches that have been previously tried and succeeded, as well as those that have been tried but have not yet shown to be successful.
[]
Models and processes for the evaluation of COTS components
Proceedings. 26th International Conference on Software Engineering
None
2004
This workshop summary presents an overview of the one-day International Workshop on Models and Processes for the Evaluation of COTS Components (MPEC; &#x02019;04), held in conjunction with the 26th International Conference on Software Engineering (ICSE; &#x02019;04). Details about MPEC; &#x02019;04 may be found at http://www.lsi.upc.es/events/mpec/.
[Bridges, Performance evaluation, Costs, Software metrics, Atmosphere, Software quality, Software systems, Proposals, Open source software, Software engineering]
The sixth international workshop on economics-driven software engineering research (EDSER-6)
Proceedings. 26th International Conference on Software Engineering
None
2004
Traditionally, the study of software engineering has been primarily a technical endeavor with minimal attention given to its economic context. Design and implementation methods are proposed based on technical merits without making adequate links to economic considerations. Engineering seeks to create value relative to resources invested in a given context, whether commercial or not. Software development essentially is an irreversible capital investment and software should add value to the organization just as any other capital expenditure that creates a net benefit.
[]
Second international workshop on dynamic analysis (WODA 2004)
Proceedings. 26th International Conference on Software Engineering
None
2004
Dynamic analysis techniques reason over program executions and show promise in aiding the development of robust and reliable large-scale systems. It has become increasingly clear that limitations of static analysis can be overcome by integrating static and dynamic analyses, and that the performance and value of dynamic analysis can be improved by static analysis. Hence, a key focus of the workshop will be on hybrid analyses that involve both static and dynamic components.
[]
Collaboration, conflict and control: the 4th workshop on open source software engineering
Proceedings. 26th International Conference on Software Engineering
None
2004
Building on the success of the first three workshops in the series, which were held at ICSE 2001 (Toronto), ICSE 2002 (Orlando) and ICSE 2003 (Portland), the 4th Workshop on Open Source Software Engineering, ("Collaboration, Conflict and Control") brings together researchers and practitioners for the purpose of discussing the platforms and tools, techniques and processes, and the organizational structures that are used to support and sustain communication, collaboration and conflict resolution within and between open source software communities.
[]
Third workshop on scenarios and state machines: models, algorithms, and tools (SCESM'04)
Proceedings. 26th International Conference on Software Engineering
None
2004
This third workshop in the SCESM (SCEnarios and State Machines) series is motivated by the very successful preceding workshops on this topic. This effort started with a workshop on scenario-based round-trip engineering at OOPSLA 2000. Two SCESM workshops at ICSE 2002 [1] and ICSE 2003 [2], and a Dagstuhl Seminar in September 2003 [3] followed. SCESM&#x02019;04 continues this tradition by bringing together researchers and practitioners interested in advancing models, algorithms and tools for scenario- and state-oriented approaches to software and systems engineering.
[]
Workshop on directions in software engineering environments (WoDiSEE)
Proceedings. 26th International Conference on Software Engineering
None
2004
The goal of this workshop was is to bring together researchers and practitioners with an interest in developing, extending, deploying and using software engineering tools. Theis workshop will provides an interactive forum for the exchange of ideas and discussion about future trends in software engineering environment research and development. The outcomes of this workshop will beare a summary of the state of the art in software engineering environment research and development, and the identification of key directions for future research in this area.
[]
MSR 2004 international workshop on mining software repositories
Proceedings. 26th International Conference on Software Engineering
None
2004
The goal of this one-day workshop is to bring together researchers and practitioners to consider methods that use data stored in software repositories (such as source control systems, defect tracking systems, and archived project communications) to further understanding of software development practices.
[]
Workshop on software engineering for high performance computing system (HPCS) applications
Proceedings. 26th International Conference on Software Engineering
None
2004
High performance computing systems are used to develop software for wide variety of domains including nuclear physics, crash simulation, satellite data processing, fluid dynamics, climate modelling, bioinformatics, and financial modelling. The TOP500 website (http://www.top500.org/) lists the top 500 high performance computing systems along with their specifications and owners. The diversity of government, scientific, and commercial organizations present on this list illustrates the growing prevalence and impact of HPCS applications on modern society.
[]
Bridging the gaps II: bridging the gaps between software engineering and human-computer interaction
Proceedings. 26th International Conference on Software Engineering
None
2004
The Second International Workshop on the Relationships between Software Engineering and Human- Computer Interaction was held on May 24-25, 2004 as part of the 2004 International Conference on Software Engineering, in Edinburgh, Scotland. This workshop was the second at ICSE and the fourth in a series held at international conferences in the past two years. It was motivated by a perception among researchers, practitioners, and educators that the fields of Human-Computer Interaction and Software Engineering were largely ignoring each other and that they needed to work together more closely and to understand each other better. This report describes the motivation, goals, organization, and outputs of the workshop.
[Human computer interaction, Bridges, Vocabulary, Educational programs, Conferences, User interfaces, Software systems, Computer industry, Best practices, Software engineering]
Second ICSE workshop on remote analysis and measurement of software systems (RAMSS)
Proceedings. 26th International Conference on Software Engineering
None
2004
The goal of this workshop is to bring together researchers and practitioners interested in exploring how the characteristics of today&#x02019;s area of computing (e.g., high connectivity, substantial computing power for the average user, higher demand for and expectation of frequent software updates) can be leveraged to improve software quality and performance.
[]
ICSE workshop: software engineering for automotive systems
Proceedings. 26th International Conference on Software Engineering
None
2004
Software is taking a leading role in automotive development and innovation. It is commonly estimated that software and electronics account for 90% of all innovations. Already today cost for software and electronics in current premium cars make up 40% of the overall cost. The applications are not anymore limited to classical embedded control systems, such as airbag control software, but cover a broad range from mission critical embedded systems in the X-by-wire field, driver assistance to infotainment and personalization in the MMI (Man Machine Interface) area.
[]
Second workshop on software quality
Proceedings. 26th International Conference on Software Engineering
None
2004
Software products are a critical and strategic asset in an organizations' business. They are becoming larger, more sophisticated and more complex. The challenge is to develop more complicated software products within the constraints of time and resources without the sacrifice of quality. Quality standards, methodologies and techniques have been continually proposed by researchers and used by software engineers in the industry. The Second Workshop on Software Quality aims to bring together academic, industrial and commercial communities interested in software quality topics to discuss the different technologies being defined and used in the software quality area.
[]
A Message from the General Chair
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Presents the welcome message from the conference proceedings.
[]
Foreword
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Presents the welcome message from the conference proceedings.
[]
ICSE 2005 organization
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Provides a listing of current committee members and society officers.
[]
Transitions in programming models
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Summary form only given. The future of programming languages is not what it used to be. From the 50's to the 90's, richer, more flexible, and more robust structures were imposed on raw computation. Generally, new models of data and control managed to subsume older ones. But now, as programs and applications expand beyond a single local network and a single administrative domain, the very nature of data and control changes, and many long-lasting conceptual invariants are disrupted. We discuss three of these disruptive changes, which seem to be happening all at the same time, and for related reasons: asynchronous concurrency, semistructured data, and (in much less detail) security abstractions. We outline research project that address issues in those areas, mostly as examples of much larger territories yet to explore.
[semistructured data, Object oriented modeling, Computational modeling, Data security, programming languages, programming models, asynchronous concurrency, Concurrent computing, Computer science, Computer languages, security abstractions, security of data, concurrency control, Modems, Robustness, Logic, Mobile computing, programming]
Agile, open source, distributed, and on-time - inside the Eclipse development process
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Summary form only given. Eclipse is a widely recognized open source project dedicated to providing a platform for developing integrated tools. Throughout the history of Eclipse the development team was successful in hitting projected delivery dates with precision and quality. This isn't possible without a team strongly committed to ship quality software. How is this really done? How does Eclipse achieve quality and just-in-time delivery? This paper sheds light on the key practices of the Eclipse development process - from the development mantras "always beta\
[Java, project management, public domain software, Project management, software development management, History, integrated tools development, multiple feedback loops, Open source software, Stress, Feedback loop, Eclipse development process, Software quality, agile open source distributed on-time programming, software tools, Books, Marine vehicles, Software reusability, programming environments, just-in-time delivery, distributed programming]
Addressing software dependability with statistical and machine learning techniques
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Summary form only given. Our ability to design and deploy large complex systems is outpacing our ability to understand their behavior. How do we detect and recover from "heisenbugs\
[Computerized monitoring, program diagnostics, statistical techniques, software reliability, software failure forecasting, Glass, machine learning, Bridges, Condition monitoring, software failure diagnosis, software failure detection, configuration troubleshooting, Machine learning, software dependability, system monitoring, Error correction, Internet, learning (artificial intelligence), statistical analysis]
System challenges for ubiquitous &amp; pervasive computing
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The terms ubiquitous and pervasive computing were first coined at the beginning of the 90's, by Xerox PARC and IBM respectively, and capture the realization that the computing focus was going to change from the PC to a more distributed, mobile and embedded form of computing. Furthermore, it was predicted by some researchers that the true value of embedded computing would come from the orchestration of the various computational components into a much richer and adaptable system than had previously been possible. Now, we have made progress towards these aims. The hardware platforms used to implement these systems encapsulate significant computation capability in a small form-factor, consume little power and have a small cost. However, the system software capabilities have not advanced at a pace that can take full advantage of this infrastructure. This paper describes where software and hardware have combined to enable ubiquitous computing, where these systems have limitations and where the biggest challenges still remain.
[Pervasive computing, Embedded computing, microcomputers, Power system management, embedded computing, Ubiquitous computing, Educational institutions, Application software, ubiquitous computing, pervasive computing, Distributed computing, distributed computing, mobile computing, embedded systems, power management, user interface adaptation, Hardware, Energy management, Mobile computing]
Research challenges of autonomic computing
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Autonomic computing is a grand-challenge vision in which computing systems manage themselves in accordance with high-level objectives specified by humans. The IT industry recognizes that meeting this challenge is imperative; otherwise, IT systems will soon become virtually impossible to administer. But meeting this challenge is also extremely difficult, and requires a worldwide collaboration among the best minds of academia and industry. In the hope of motivating researchers in relevant areas to apply their expertise to this vitally important problem, the author outlines some of the main scientific and engineering challenges that collectively make up the grand challenge of autonomic computing, and provide pointers to initial efforts to address these challenges.
[Computer vision, IT industry, Humans, self-adjusting systems, Application software, Distributed computing, self-management, Computer science, IT systems, Collaboration, Management information systems, Computer applications, research and development, Computer industry, computing systems, fault tolerant computing, autonomic computing, information systems, Artificial intelligence]
Beyond computer science
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Computer science is necessary but not sufficient to understand and overcome the problems we face in software engineering. We need to understand not only the properties of the software itself, but also the limitations and competences humans bring to the engineering task. Rather than rely on commonsense notions, we need a deep and nuanced view of human capabilities in order to determine how to enhance them. The author discusses what he regards as promising examples of cognitive and organizational theories and proposes research directions to develop new ways of representing run-time behavior and ways of thinking about project coordination. He concludes with observations on creating an interdisciplinary culture.
[Computer science, Pathogens, Silver, Runtime, computer science, Humans, Permission, Behavioral science, Software systems, software engineering, Software engineering, Diseases]
Clinical requirements engineering
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
In this paper, the author makes a case for integration of requirements engineering (RE) with clinical disciplines. To back his case, he looks at two examples that employ a clinical RE approach, first, that of introducing email into the life of a brain-injured individual, and second, introducing digital darkroom tools into his life. The former uses a Brownfield approach by starting with an existing clinical process, cognitive rehabilitation, and then defining an RE process that fits. The latter uses a Greenfield approach that postulates a new clinical RE process that focuses on the problems some of us have using digital darkroom tools.
[clinical requirements engineering, goal attainment scale, Target tracking, skill assessment, Human factors, requirements monitoring, Electronic mail, Application software, cognitive rehabilitation, Computer science, Greenfield approach, Design engineering, Brownfield approach, systems analysis, Packaging, Software tools, Monitoring, Software engineering]
Silver bullet or fool's gold: supporting usability in open source software development
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Summary form only given. At first glance it can look like open source software development violates many, if not all, of the precepts of decades of careful research and teaching in software engineering. One could take a classic SE textbook and compare the activities elaborated and advocated in the various chapters with what is actually done in plain sight in the public logs of an OSS project in say SourceForge. For a professor of software engineering this might make for rather depressing reading. Are the principles of SE being rendered obsolete? Has OSS really discovered Brooks' silver bullet? Or is it just a flash in the pan or fool's gold? In this paper, the author mainly looks at one aspect of open source development, the 'problem' of creating usable interfaces, particularly for non-technical end-users. Any approach involves the challenge of how to coordinate distributed collaborative interface analysis and design, given that in conventional software development this is usually done in small teams and almost always face to face. Indeed all the methods in any HCI text just assume same-time same-place work and don't map to distributed work, let alone the looser mechanisms of OSS development. Instead what is needed is a form of participatory usability involving the coordination of end users and developers in a constantly evolving redesign process.
[Gold, open source software development, Collaborative software, public domain software, Programming, user interfaces, Open source software, HCI, Silver, participatory usability, Education, groupware, Rendering (computer graphics), Collaborative work, software engineering, Usability, Software engineering]
How software can help or hinder human decision making (and vice-versa)
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Summary form only given. Developments in computing offer experts in many fields specialised support for decision making under uncertainty. However, the impact of these technologies remains controversial. In particular, it is not clear how advice of variable quality from a computer may affect human decision makers. Here the author reviews research showing strikingly diverse effects of computer support on expert decision-making. Decisions support can both systematically improve or damaged the performance of decision makers in subtle ways depending on the decision maker's skills, variation in the difficulty of individual decisions and the reliability of advice from the support tool. In clinical trials decision support technologies are often assessed in terms of their average effects. However this methodology overlooks the possibility of differential effects on decisions of varying difficulty, on decision makers of varying competence, of computer advice of varying accuracy and of possible interactions among these variables. Research that has teased apart aggregated clinical trial data to investigate these possibilities has discovered that computer support was less useful for - and sometimes hindered - professional experts who were relatively good at difficult decisions without support; at the same time the same computer support tool helped those experts who were less good at relatively easy decisions without support. Moreover, inappropriate advice from the support tool could bias decision makers' decisions and, predictably, depending on the type of case, improve or harm the decisions.
[computer support, Uncertainty, human decision making, Collaborative software, Decision making, Humans, Psychology, Clinical trials, Educational institutions, Software reliability, decision support, decision support systems, Toxicology, decision making]
Why use the model driven architecture to design and build distributed applications?
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
OMG's model driven architecture/spl reg/ (MDA/spl reg/) unifies and simplifies modeling, design, implementation, and integration of applications - including large and complex ones - by defining software fundamentally at the model level, expressed in OMG's standard Unified Modeling Language/spl reg/ (UML/spl reg/). An MDA-based development goes through three steps - two producing models, one producing code - and typically iterates through these several times.
[Algorithm design and analysis, model driven architecture, Unified Modeling Language, Unified modeling language, platform-specific model, Documentation, distributed processing, Reliability engineering, Application software, Middleware, distributed application development, software architecture, Technology management, Engineering management, UML, systems analysis, Computer architecture, Software standards, platform-independent model]
Moving from a plan driven culture to agile development
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Summary form only given. Plan driven cultures are characterized by a strong belief in the plannability and predictability of software development projects. The SEI-CMM, software process improvement initiatives, and software metrics programs are some of the hallmarks of this school of thought. The trend towards agile development places the emphasis on constantly adapting to a project's changing goals rather than on detailed upfront planning. The majority of reports from practitioners of agile development are positive and confirm the advantages of this approach. However, moving from a plan driven culture to agile development is not easy. Making the transition requires changes to many established practices and may even touch core values held by stakeholders. Areas affected are requirements and change management, user involvement, willingness to take on responsibility, contract management, and the ability to live with many uncertainties. This paper looks at what it takes to make the transition and presents lessons learned from organizations and projects which have successfully completed the switch to agile development.
[contract management, Uncertainty, software development, software development management, human factors, Switches, Human factors, Programming, Educational institutions, agile development, change management, Software development management, requirements management, Software metrics, Engineering management, Contracts, Software engineering]
Journey of enlightenment: the evolution of development at Microsoft
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Like many software companies, Microsoft has been doing distributed application development for many years. However, changes in the market have altered the rules, both in terms of customer expectations and programming models for ubiquitous interconnected smart devices. These changes have provoked two dramatic shifts in the way we develop software. The first is the creation and use of the .NET framework as a simple, secure, and robust platform for device-independent software development, data manipulation, and communications. The second is an agile yet highly disciplined approach to designing, testing, implementing, and verifying our software which presumes all bugs are unacceptable and must be found and fixed early before they impact internal groups, external partners, and eventually our customers. This paper discusses the nature and impact of these two dramatic shifts to the development practices at Microsoft.
[Software testing, program testing, program verification, .NET framework, software verification, software reliability, software design, Human factors, Programming, test driven development, agile development, Security, ubiquitous computing, WSDL, distributed application development, network operating systems, Permission, software houses, Robustness, Microsoft, distributed programming, software testing, data manipulation, Application software, Software debugging, SOAP, software implementation, Computer bugs, systems analysis, XML, Software quality, device-independent software development, software security, programming environments]
Software architecture in an open source world
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Summary form only given. In spite of the hype and hysteria surrounding open source software development, there is very little that can be said of open source in general. Open source projects range in scope from the miniscule, such as the thousands of non-maintained code dumps left behind at the end of class projects, dissertations, and failed commercial ventures, to the truly international, with thousands of developers collaborating, directly or indirectly, on a common platform. One characteristic that is shared by the largest and most successful open source projects, however, is a software architecture designed to promote anarchic collaboration through extensions while at the same time preserving centralized control over the interfaces. This paper features a survey of the state-of-the-practice in open source development in regards to software architecture, with particular emphasis on the modular extensibility interfaces within several of the most successful projects, including Apache httpd, Eclipse, Mozilla Firefox, Linux kernel, and the World Wide Web (which few people recognize as an open source project in itself). These projects fall under the general category of collaborative open source software development, which emphasizes community aspects of software engineering in order to compensate for the often-volunteer nature of core developers and take advantage of the scalability obtainable through Internet-based virtual organizations.
[operating system kernels, collaborative open source software development, Collaborative software, public domain software, Apache httpd, Eclipse, Linux kernel, World Wide Web, International collaboration, Open source software, Centralized control, modular extensibility interfaces, Mozilla Firefox, software architecture, Software design, Software architecture, Linux, online front-ends, software engineering, Internet, Web sites, Kernel, Software engineering, Internet-based virtual organizations]
Where do you go when you're through the turnstile? [telecommunication software development]
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Summary form only given. Years ago, our paper described how a very small system might be developed to control a coin-operated turnstile in a zoo (Jackson and Zave, 1995). It arose out of our efforts to understand how requirements, domain knowledge and specifications fit together, and how specifications could be derived systematically. A particular goal was to understand requirements and specifications for telecommunication systems well enough to handle the feature interaction problem that plagues telecommunication software. A year later we published a more comprehensive version of our requirements framework (Zave and Jackson, 1997), and began to develop the distributed feature composition (DFC) architecture for telecommunication services (Jackson and Zave, 1998). We continued to work on it together until mid-2002. DFC has proven successful in practice, and is now being used in a commercial voice-over-IP service. Since publishing the requirements framework, we have worked on different kinds of system, and have been confronted with important differences in how its principles apply. It has taken all this time to achieve the goal of understanding requirements for telecommunication services and other connection services. In this domain, requirements are heavily influenced by the fact that services are assemblies of components added to a basic network infrastructure. Our work in another direction has led to a focus on system interaction with the human and physical world, recognising the varying roles of formalisation and formal reasoning in problems of different kinds, and the need for a stronger grip on the relationship between engineering of software and engineering in the world. The common lesson of these experiences is that the requirements framework of years ago is a good generalization, but specific domains and situations require their own specializations of it.
[telecommunication services, Humans, Programming, Control systems, Telecommunication control, formal reasoning, telecommunication computing, Publishing, systems analysis, Computer architecture, distributed feature composition, software engineering, reasoning about programs, Internet telephony, Digital-to-frequency converters, Telecommunication services, Assembly, telecommunication software]
Introduction to research papers
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Presents the introductory welcome message from the conference proceedings.
[]
Aspect-oriented programming and modular reasoning
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Aspects cut new interfaces through the primary decomposition of a system. This implies that in the presence of aspects, the complete interface of a module can only be determined once the complete configuration of modules in the system is known. While this may seem anti-modular, it is an inherent property of crosscutting concerns, and using aspect-oriented programming enables modular reasoning in the presence of such concerns.
[Computer languages, object-oriented programming, Shape, modular reasoning, Packaging, Permission, aspect-oriented programming, reasoning about programs]
Classpects: unifying aspect- and object-oriented language design
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The contribution of this paper is the design, implementation, and early evaluation of a programming language that unifies classes and aspects. We call our new module construct the classpect. We make three basic claims. First, we can realize a unified design without significantly compromising the expressiveness of current aspect languages. Second, such a design improves the conceptual integrity of the programming model. Third, it significantly improves the compositionality of aspect modules, expanding the program design space from the two-layered model of AspectJ-like languages to include hierarchical structures. To support these claims, we present the design and implementation of Eos-U, an AspectJ-like language based on C# that supports classpects as the basic unit of modularity. We show that Eos-U supports layered designs in which classpects separate integration concerns flexibly at multiple levels of composition. The underpinnings of our design include support for aspect instantiation under program control, instance-level advising, advising as a general alternative to object-oriented method invocation and overriding, and the provision of a separate join-point-method binding construct.
[object-oriented programming, Object oriented modeling, Design methodology, Human factors, AspectJ-like languages, Programming profession, Computer science, Computer languages, Design engineering, object-oriented language design, join-point-method binding, Permission, Packaging, object-oriented languages, Classpects, programming language, aspect-oriented language design, Object oriented programming, Eos-U language, C# language]
Towards aspect weaving applications
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Software must be adapted to accommodate new features in the context of changing requirements. In this paper, we illustrate how applications with aspect weaving capabilities can be easily and dynamically adapted with unforeseen features. Aspects were used at three levels: in the context of semantic analysers, within a BPEL engine that orchestrates Web services, and finally within BPEL processes themselves. Each level uses its own tailored domain-specific aspect language that is easier to manipulate than a general-purpose one (close to the programming language) and the pointcuts are independent from the implementation.
[object-oriented programming, Design methodology, BPEL processes, Software performance, Educational institutions, Application software, Engines, Computer science, Software design, semantic analysers, Web services, software adaptability, Permission, BPEL engine, aspect-oriented programming, Weaving, Internet, aspect weaving applications, domain-specific aspect language, Business]
Testing database transactions with AGENDA
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
AGENDA is a tool set for testing relational database applications. An earlier prototype was targeted to applications consisting of a single query and included components for populating a database with data suitable for testing the application, generating inputs to the query, and checking relatively simple aspects of the results of executing the query. This paper describes substantial extensions to AGENDA, allowing it to test transactions with multiple queries and with complex intended behavior. The paper introduces a technique for checking complex properties of the database state transition performed by the transaction under test, as well as an improved input generation heuristic. Results of using AGENDA to test three applications with seeded faults are presented.
[Software testing, Performance evaluation, transaction processing, program testing, software testing, Relational databases, input generation heuristic, Transaction databases, State-space methods, AGENDA tool set, Application software, relational databases, database state transition checking, query processing, Prototypes, Distributed databases, Permission, database transaction testing, software tools, multiple queries, relational database testing, Software engineering]
SQL DOM: compile time checking of dynamic SQL statements
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Most object oriented applications that involve persistent data interact with a relational database. The most common interaction mechanism is a call level interface (CLI) such as ODBC or JDBC. While there are many advantages to using a CLI - expressive power and performance being two of the most key - there are also drawbacks. Applications communicate through a CLI by constructing strings that contain SQL statements. These SQL statements are only checked for correctness at runtime, tend to be fragile and are vulnerable to SQL injection attacks. To solve these and other problems, we present the SQL DOM: a set of classes that are strongly-typed to a database schema. Instead of string manipulation, these classes are used to generate SQL statements. We show how to extract the SQL DOM automatically from an existing database schema, demonstrate its applicability to solve the mentioned problems, and evaluate its performance.
[object-oriented programming, Object oriented databases, Data security, object oriented applications, Relational databases, data flow analysis, relational database, relational databases, SQL DOM, program compilers, Engines, impedance mismatch, SQL, compile time checking, Computer science, strongly-typed classes, Runtime, Utility programs, dynamic SQL statements, Permission, Impedance, Software engineering]
Safe query objects: statically typed objects as remotely executable queries
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Developers of data-intensive applications are increasingly using persistence frameworks such as EJB, Hibernate and JDO to access relational data. These frameworks support both transparent persistence for individual objects and explicit queries to efficiently search large collections of objects. While transparent persistence is statically typed, explicit queries do not support static checking of types or syntax because queries are manipulated as strings and interpreted at runtime. This paper presents safe query objects, a technique for representing queries as statically typed objects while still supporting remote execution by a database server. Safe query objects use object-relational mapping and reflective metaprogramming to translate query classes into traditional database queries. The model supports complex queries with joins, parameters, existentials, and dynamic criteria. A prototype implementation for JDO provides a type-safe interface to the full query functionality in the JDO 1.0 standard.
[remotely executable queries, Java, Software prototyping, statically typed objects, Relational databases, type theory, Application software, relational databases, explicit queries, reflective metaprogramming, query processing, relational data, Computer languages, Runtime, Engineering management, Prototypes, object-relational mapping, transparent persistence, safe query objects, Marine animals, type-safe interface, Software engineering, persistent objects]
Helping users avoid bugs in GUI applications
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
In this paper, we propose a method to help users avoid bugs in GUI applications. In particular, users would use the application normally and report bugs that they encounter to prevent anyone - including themselves - from encountering those bugs again. When a user attempts an action that has led to problems in the past, he/she will receive a warning and will be given the opportunity to abort the action - thus avoiding the bug altogether and keeping the application stable. Of course, bugs should be fixed eventually by the application developers, but our approach allows application users to collaboratively help each other avoid bugs - thus making the application more usable in the meantime. We demonstrate this approach using our "Stabilizer" prototype. We also include a preliminary evaluation of the Stabilizer's bug prediction.
[Software testing, Software prototyping, program debugging, GUI applications, software fault evasion, graphical user interfaces, software testing, software bug prediction, software bug avoidance, Application software, software fault tolerance, Computer science, Computer bugs, Collaboration, Prototypes, Permission, software tools, software bug tracking system, Australia, Graphical user interfaces]
Using structural context to recommend source code examples
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
When coding to a framework, developers often become stuck, unsure of which class to subclass, which objects to instantiate and which methods to call. Example code that demonstrates the use of the framework can help developers make progress on their task. In this paper, we describe an approach for locating relevant code in an example repository that is based on heuristically matching the structure of the code under development to the example code. Our tool improves on existing approaches in two ways. First, the structural context needed to query the repository is extracted automatically from the code, freeing the developer from learning a query language or from writing their code in a particular style. Second, the repository can be generated easily from existing applications. We demonstrate the utility of this approach by reporting on a case study involving two subjects completing four programming tasks within the Eclipse integrated development environment framework.
[example repository, Protocols, object-oriented programming, Application software, Database languages, Programming profession, Programming environments, Computer science, source code example recommendation, Utility programs, structural context, development environment framework, Writing, Permission, software tools, Software tools]
Eliciting design requirements for maintenance-oriented IDEs: a detailed study of corrective and perfective maintenance tasks
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Several innovative tools have found their way into mainstream use in modern development environments. However, most of these tools have focused on creating and modifying code, despite evidence that most of programmers' time is spent understanding code as part of maintenance tasks. If new tools were designed to directly support these maintenance tasks, what types would be most helpful? To find out, a study of expert Java programmers using Eclipse was performed. The study suggests that maintenance work consists of three activities: (1) forming a working set of task-relevant code fragments; (2) navigating the dependencies within this working set; and (3) repairing or creating the necessary code. The study identified several trends in these activities, as well as many opportunities for new tools that could save programmers up to 35% of the time they currently spend on maintenance tasks.
[Java, Software maintenance, corrective maintenance, design requirements elicitation, Navigation, maintenance-oriented IDE, Eclipse, Human factors, reverse engineering, perfective maintenance, software maintenance, Programming profession, Programming environments, expert Java programmers, systems analysis, Permission, Modems, software tools, Software tools, programming environments, Software engineering]
Automatic generation and maintenance of correct spreadsheets
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Existing spreadsheet systems allow users to change cells arbitrarily, which is a major source of spreadsheet errors. We propose a system that prevents errors in spreadsheets by restricting spreadsheet updates to only those that are logically and technically correct. The system is based on the concept of templates that describe the principal structure of the initial spreadsheet and all of its future versions. We have developed a program generator that translates a template into an initial spreadsheet together with customized update operations for changing cells and inserting/deleting rows and columns for this particular template. We have designed a type system for templates that ensures the following form of "spreadsheet maintenance safety": Update operations that are generated from a type-correct template are proved to transform the spreadsheet only according to the template and to never produce any omission, reference, or type errors. Finally, we have developed a prototype as an extension to Excel, which has been shown by a preliminary usability study to be well accepted by end users.
[Software prototyping, automatic programming, automatic spreadsheet maintenance, Automatic programming, end-user software engineering, spreadsheet programs, software maintenance, automatic spreadsheet generation, program generation, Excel, Prototypes, type system, Computer errors, Permission, Error correction, Safety, Usability, spreadsheet maintenance safety, Testing, Software engineering]
A framework of greedy methods for constructing interaction test suites
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Greedy algorithms for the construction of software interaction test suites are studied. A framework is developed to evaluate a large class of greedy methods that build suites one test at a time. Within this framework are many instantiations of greedy methods generalizing those in the literature. Greedy algorithms are popular when the time for test suite construction is of paramount concern. We focus on the size of the test suite produced by each instantiation. Experiments are analyzed using statistical techniques to determine the importance of the implementation decisions within the framework. This framework provides a platform for optimizing the accuracy and speed of "one-test-at-a-time" greedy methods.
[Software testing, Greedy algorithms, System testing, Portable computers, program testing, greedy algorithms, Optimization methods, pair-wise interaction coverage, mixed-level covering arrays, one-test-at-a-time greedy methods, Computer science, Linux, Permission, interaction test suite construction, Hardware, DSL, software interaction testing]
Demand-driven structural testing with dynamic instrumentation
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Producing reliable and robust software has become one of the most important software development concerns. Testing is a process by which software quality can be assured through the collection of information. While testing can improve software reliability, current tools typically are inflexible and have high overheads, making it challenging to test large software projects. In this paper, we describe a new scalable and flexible framework for testing programs with a novel demand-driven approach based on execution paths to implement test coverage. This technique uses dynamic instrumentation on the binary code that can be inserted and removed on-the-fly to keep performance and memory overheads low. We describe and evaluate implementations of the framework for branch, node and def-use testing of Java programs. Experimental results for branch testing show that our approach has, on average, a 1.6 speed up over static instrumentation and also uses less memory.
[Software testing, Java, program testing, software development, Instruments, software testing, software reliability, Programming, software quality, def-use testing, dynamic instrumentation, Computer science, Computer languages, Software quality, branch testing, node testing, Robustness, demand-driven structural testing, Probes, Software tools, Java programs]
An adaptive object model with dynamic role binding
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
To achieve the goal of realizing object adaptation to environments, a new role-based model Epsilon and a language EpsilonJ is proposed. In Epsilon, an environment is defined as a field of collaboration between roles and an object adapts to the environment assuming one of the roles. Objects can freely enter or leave environments and belong to multiple environments at a time so that dynamic adaptation or evolution of objects is realized. Environments and roles are the first class constructs at runtime as well as at model description time so that separation of concerns is not only materialized as a static structure but also observed as behaviors. Environments encapsulating collaboration are independent reuse components to be deployed separately from objects. In this paper, the Epsilon model and the language are explained with some examples. The effectiveness of the model is illustrated by a case study on the problem of integrated systems. Implementation of the language is also reported.
[object adaptation, Runtime environment, object-oriented programming, role-based model, adaptive object model, dynamic role binding, Object oriented modeling, Adaptation model, Humans, Software design, Collaboration, independent reuse components, Permission, software reusability, object-oriented languages, Epsilon, Manufacturing, Object oriented programming, Software engineering]
Data structure repair using goal-directed reasoning
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Data structure repair is a promising technique for enabling programs to execute successfully in the presence of otherwise fatal data structure corruption errors. Previous research in this field relied on the developer to write a specification to explicitly translate model repairs into concrete data structure repairs, raising the possibility of 1) incorrect translations causing the supposedly repaired concrete data structures to be inconsistent, and 2) repaired models with no corresponding concrete data structure representation. We present a new repair algorithm that uses goal-directed reasoning to automatically translate model repairs into concrete data structure repairs. This new repair algorithm eliminates the possibility of incorrect translations and repaired models with no corresponding representation as concrete data structures.
[Software testing, goal-directed reasoning, Debugging, Data structures, Mechanical factors, software fault tolerance, Computer science, Computer languages, Computer errors, data structure repair, Concrete, data structures, reasoning about programs, Artificial intelligence, Software engineering]
Verifying safety policies with size properties and alias controls
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Many software properties can be analysed through a relational size analysis on each function's inputs and outputs. Such relational analysis (through a form of dependent typing) has been successfully applied to declarative programs, and to restricted imperative programs; but it has been elusive for object-based programs. The main challenge is that objects may mutate and they may be aliased. In this paper, we show how safety policies of programs can be analysed by tracking size properties of objects and be enforced by objects' invariants and the preconditions of methods. We propose several new ideas to allow both mutability and sharing of objects, whilst aiming for precision in our analysis. We introduce the concept of size-immutability to facilitate sharing, and also a set of alias controls to track unaliased objects whose size properties may change. We formalise our results through a set of advanced type checking rules for an object-based imperative language. We re-affirm the utility of the proposed type system by showing how a variety of software properties can be automatically verified according to size-inspired safety policies.
[object-oriented programming, program verification, software verification, type checking, Reasoning about programs, Data structures, type theory, Software safety, Computer science, safety policies verification, security of data, Aggregates, systems analysis, object-based imperative language, relational size analysis, Permission, Automatic control, Size control, Logic, Software engineering]
Verification and change-impact analysis of access-control policies
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Sensitive data are increasingly available on-line through the Web and other distributed protocols. This heightens the need to carefully control access to data. Control means not only preventing the leakage of data but also permitting access to necessary information. Indeed, the same datum is often treated differently depending on context. System designers create policies to express conditions on the access to data. To reduce source clutter and improve maintenance, developers increasingly use domain-specific, declarative languages to express these policies. In turn, administrators need to analyze policies relative to properties, and to understand the effect of policy changes even in the absence of properties. This paper presents Margrave, a software suite for analyzing role-based access-control policies. Margrave includes a verifier that analyzes policies written in the XACML language, translating them into a form of decision-diagram to answer queries. It also provides semantic differencing information between versions of policies. We have implemented these techniques and applied them to policies from a working software application.
[Software maintenance, decision diagrams, semantic differencing information, Access protocols, XACML language, Control systems, Margrave language, Application software, Programming profession, formal verification, Operating systems, authorisation, data access, Permission, access-control policy verification, protocols, role-based access-control, National security, change-impact analysis, decision diagram, Testing, Software engineering]
Explicit assumptions enrich architectural models
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Design for change is a well-known adagium in software engineering. We separate concerns, employ well-designed interfaces, and the like to ease evolution of the systems we build. We model and build in changeability through parameterization and variability points (as in product lines). These all concern places where we explicitly consider variability in our systems. We conjecture that it is helpful to also think of and explicitly model invariability, things in our systems and their environment that we assume will not change. We give examples from the literature and our own experience to illustrate how evolution can be seriously hampered because of tacit assumptions made. In particular, we show how we can explicitly model assumptions in an existing product family. From this, we derive a metamodel to document assumptions. Finally, we show how this type of modeling adds to our understanding of the architecture and the decisions that led to it.
[Documentation, Knowledge management, knowledge management, design for change, Computer science, Connectors, software architecture, Software architecture, Computer architecture, Permission, Robustness, software engineering, Software engineering]
An infrastructure for development of object-oriented, multi-level configuration management services
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
In an integrated development environment, the ability to manage the evolution of a software system in terms of logical abstractions, compositions, and their interrelations is crucial to successful software development. This paper presents a novel framework and infrastructure, Molhado, upon which to build object-oriented software configuration management (SCM) services in a SCM-centered integrated development environment. Key contributions of this paper include a product versioning model, an extensible, logical, and object-oriented system model, and a reusable product versioning SCM infrastructure, that allow new types of objects to be implemented as extensions of the system model's basic entities. Versions and configurations of objects are managed at different levels of abstraction and granularity. A new SCM-centered editing environment or development environment for a specific development paradigm can be rapidly realized by re-using Molhado's infrastructure and implementing new object types and their associated tools. This paper also demonstrates our approach in creating prototypes of SCM-centered development environments for different paradigms.
[Software maintenance, object-oriented programming, Molhado, software development, Object oriented modeling, Programming, reusable product versioning, Environmental management, Software development management, Computer science, configuration management, product versioning model, Engineering management, multilevel configuration management services, object-oriented software configuration management, object-oriented development, integrated development environment, Permission, software reusability, Software systems, software engineering, object-oriented system model, programming environments, Software engineering]
Predictors of customer perceived software quality
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Predicting software quality as perceived by a customer may allow an organization to adjust deployment to meet the quality expectations of its customers, to allocate the appropriate amount of maintenance resources, and to direct quality improvement efforts to maximize the return on investment. However, customer perceived quality may be affected not simply by the software content and the development process, but also by a number of other factors including deployment issues, amount of usage, software platform, and hardware configurations. We predict customer perceived quality as measured by various service interactions, including software defect reports, requests for assistance, and field technician dispatches using the afore mentioned and other factors for a large telecommunications software system. We employ the non-intrusive data gathering technique of using existing data captured in automated project monitoring and tracking systems as well as customer support and tracking systems. We find that the effects of deployment schedule, hardware configurations, and software platform can increase the probability of observing a software failure by more than 20 times. Furthermore, we find that the factors affect all quality measures in a similar fashion. Our approach can be applied at other organizations, and we suggest methods to independently validate and replicate our results.
[Software maintenance, project management, Predictive models, large telecommunications software system, customer perceived software quality prediction, project tracking systems, software quality, customer support, automated project monitoring, Investments, Software quality, Permission, Software systems, nonintrusive data gathering, Hardware, Software measurement, Resource management, software performance evaluation, Business]
Automated support for process-aware definition and execution of measurement plans
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Some of the problems with process measurement are generally due to the fact that the definition of measurement plans does not rely on a reference model of the development process that can drive and explain the measuring activities. One of the most popular methodologies addressing the definition of process measurement plans is the GQM (goal/question/metrics). This paper discusses how to support the creation of GQM plans by means of an explicit model of the process being measured. Such a model guides the GQM process, and makes it possible to define precisely - if not formally - the metrics involved. A tool supporting the proposed method is also illustrated.
[software process measurement, Computer aided software engineering, process-aware definition automated support, Process control, software development management, software process modeling, goal-question-metrics methodology, Time measurement, Software metrics, Engineering management, Permission, Particle measurements, computer aided software engineering, Software measurement, Software tools, Software engineering, software metrics]
A quality-driven systematic approach for architecting distributed software applications
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Architecting distributed software applications is a complex design activity. It involves making decisions about a number of inter-dependent design choices that relate to a range of design concerns. Each decision requires selecting among a number of alternatives; each of which impacts differently on various quality attributes. Additionally, there are usually a number of stakeholders participating in the decision-making process with different, often conflicting, quality goals, and project constraints, such as cost and schedule. To facilitate the architectural design process, we propose a quantitative quality-driven approach that attempts to find the best possible fit between conflicting stakeholders' quality goals, competing architectural concerns, and project constraints. The approach uses optimization techniques to recommend the optimal candidate architecture. Applicability of the proposed approach is assessed using a real system.
[Process design, Costs, quality-driven systematic approach, distributed processing, software architecture design, software quality, distributed software application architecting, Application software, Design optimization, Design engineering, software architecture, Software design, Software architecture, optimization, Software quality, Computer architecture, Australia]
Object naming analysis for reverse-engineered sequence diagrams
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
UML sequence diagrams are commonly used to represent object interactions in software systems. This paper considers the problem of extracting UML sequence diagrams from existing code for the purposes of software understanding and testing. A static analysis for such reverse engineering needs to map the interacting objects from the code to sequence diagram objects. We propose an interprocedural dataflow analysis algorithm that determines precisely which objects are the receivers of certain messages, and assigns the appropriate diagram objects to represent them. Our experiments indicate that the majority of message receivers can be determined exactly, resulting in highly-precise object naming for reverse-engineered sequence diagrams.
[Algorithm design and analysis, Software testing, Software maintenance, Java, object-oriented programming, Logic programming, Unified Modeling Language, program testing, software testing, Reverse engineering, Unified modeling language, data flow analysis, static analysis, reverse engineering, Computer science, Runtime, software understanding, systems analysis, reverse-engineered sequence diagrams, UML sequence diagrams, interprocedural dataflow analysis, Permission, object naming analysis]
Binary refactoring: improving code behind the scenes
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
We present binary refactoring: a software engineering technique for improving the implementation of programs without modifying their source code. While related to regular refactoring in preserving a program's functionality, binary refactoring aims to capture modifications that are often applied to source code, although they only improve the performance of the software application and not the code structure. We motivate binary refactoring, present a binary refactoring catalogue, describe the design and implementation of BARBER - our binary refactoring browser for Java, and demonstrate the usefulness of binary refactoring through a series of benchmarks.
[bytecode engineering, Java, Software maintenance, Optimizing compilers, Reverse engineering, Software performance, Educational institutions, reverse engineering, Application software, software maintenance, binary refactoring, BARBER browser, software evolution, Layout, Permission, software engineering, Software engineering]
CatchUp! Capturing and replaying refactorings to support API evolution
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Library developers who have to evolve a library to accommodate changing requirements often face a dilemma: Either they implement a clean, efficient solution but risk breaking client code, or they maintain compatibility with client code, but pay with increased design complexity and thus higher maintenance costs over time. We address this dilemma by presenting a lightweight approach for evolving application programming interfaces (APIs), which does not depend on version control or configuration management systems. Instead, we capture API refactoring actions as a developer evolves an API. Users of the API can then replay the refactorings to bring their client software components up to date. We present CatchUp!, an implementation of our approach that captures and replays refactoring actions within an integrated development environment semi-automatically. Our experiments suggest that our approach could be valuable in practice.
[Software maintenance, Java, Costs, application program interfaces, application programming interfaces, Control systems, Lighting control, API evolution, Application software, software maintenance, software libraries, API refactoring, Software libraries, integrated development environment, Permission, computer aided software engineering, Software reusability, programming environments, Software engineering, software library development]
Use of relative code churn measures to predict system defect density
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Software systems evolve over time due to changes in requirements, optimization of code, fixes for security and reliability bugs etc. Code churn, which measures the changes made to a component over a period of time, quantifies the extent of this change. We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn. Using statistical regression models, we show that while absolute measures of code chum are poor predictors of defect density, our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore, our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.
[Density measurement, regression analysis, system defect density prediction, statistical regression, Control systems, Time measurement, History, Computer science, systems analysis, Software quality, Automatic control, Software systems, multiple regression, Software measurement, relative code churn measures, principal component analysis, Software engineering, software metrics]
Main effects screening: a distributed continuous quality assurance process for monitoring performance degradation in evolving software systems
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Developers of highly configurable performance-intensive software systems often use a type of in-house performance-oriented "regression testing" to ensure that their modifications have not adversely affected their software's performance across its large configuration space. Unfortunately, time and resource constraints often limit developers to in-house testing of a small number of configurations and unreliable extrapolation from these results to the entire configuration space, which allows many performance bottlenecks and sources of QoS degradation to escape detection until systems are fielded. To improve performance assessment of evolving systems across large configuration spaces, we have developed a distributed continuous quality assurance (DCQA) process called main effects screening that uses in-the-field resources to execute formally designed experiments to help reduce the configuration space, thereby allowing developers to perform more targeted in-house QA. We have evaluated this process via several feasibility studies on several large, widely-used performance-intensive software systems. Our results indicate that main effects screening can detect key sources of performance degradation in large-scale systems with significantly less effort than conventional techniques.
[Performance evaluation, Software testing, System testing, program testing, main effects screening, in-house QA, Software performance, performance-oriented regression testing, software quality, Degradation, Extrapolation, Quality assurance, quality assurance, distributed continuous quality assurance, performance degradation monitoring, evolving software systems, design of experiment theory, Software systems, Time factors, Monitoring, software performance evaluation, design of experiments]
Effort estimation of use cases for incremental large-scale software development
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
This paper describes an industrial study of an effort estimation method based on use cases, the use case points method. The original method was adapted to incremental development and evaluated on a large industrial system with modification of software from the previous release. We modified the following elements of the original method: a) complexity assessment of actors and use cases, and b) the handling of non-functional requirements and team factors that may affect effort. For incremental development, we added two elements to the method: c) counting both all and the modified actors and transactions of use cases, and d) effort estimation for secondary changes of software not reflected in use cases. We finally extended the method to: e) cover all development effort in a very large project. The method was calibrated using data from one release and it produced an estimate for the successive release that was only 17% lower than the actual effort. The study identified factors affecting effort on large projects with incremental development. It also showed how these factors can be calibrated for a specific context and produce relatively accurate estimates.
[team factors, Computer aided software engineering, project management, Unified modeling language, Laboratories, software development management, use case points, Programming, Educational institutions, team working, Information science, incremental large-scale software development, nonfunctional requirements, Permission, Computer industry, Large-scale systems, effort estimation, complexity assessment, Software engineering]
Automatic discovery of API-level exploits
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
We argue that finding vulnerabilities in software components is different from finding exploits against them. Exploits that compromise security often use several low-level details of the component, such as layouts of stack frames. Existing software analysis tools, while effective at identifying vulnerabilities, fail to model low-level details, and are hence unsuitable for exploit-finding. We study the issues involved in exploit-finding by considering application programming interface (API) level exploits. A software component is vulnerable to an API-level exploit if its security can be compromised by invoking a sequence of API operations allowed by the component. We present a framework to model low-level details of APIs, and develop an automatic technique based on bounded, infinite-state model checking to discover API-level exploits. We present two instantiations of this framework. We show that format-string exploits can be modeled as API-level exploits, and demonstrate our technique by finding exploits against vulnerabilities in widely-used software. We also use the framework to model a cryptographic-key management API (the IBM CCA) and demonstrate a tool that identifies a previously known exploit.
[object-oriented programming, application program interfaces, program verification, Software algorithms, application programming interface, cryptography, bounded infinite-state model checking, Security, Application software, automatic API-level exploit discovery, Computer science, cryptographic-key management, software analysis, Failure analysis, Permission, Cryptography, Software tools, Contracts, Software engineering]
Sound methods and effective tools for model-based security engineering with UML
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Developing security-critical systems is difficult and there are many well-known examples of security weaknesses exploited in practice. Thus a sound methodology supporting secure systems development is urgently needed. We present an extensible verification framework for verifying UML models for security requirements. In particular, it includes various plugins performing different security analyses on models of the security extension UMLsec of UML. Here, we concentrate on an automated theorem prover binding to verify security properties of UMLsec models which make use of cryptography (such as cryptographic protocols). The paper aims to contribute towards usage of UML for secure systems development in practice by offering automated analysis routines connected to popular CASE tools. We present an example of such an application where our approach found and corrected several serious design flaws in an industrial biometric authentication system.
[Biometrics, Computer aided software engineering, Unified Modeling Language, program verification, model-based security engineering, Object oriented modeling, Unified modeling language, safety-critical software, cryptography, Cryptographic protocols, automated theorem prover, CASE tools, Acoustical engineering, secure systems development, Information security, Authentication, UML model verification, computer aided software engineering, theorem proving, software tools, security-critical systems, Cryptography, extensible verification framework, Software engineering]
Improving software security with a C pointer analysis
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
This paper presents a context-sensitive, inclusion-based, field-sensitive points-to analysis for C, which we use to detect and prevent program security vulnerabilities. In addition to a conservative points-to analysis, we propose an optimistic analysis that assumes a more restricted C semantics reflecting common C usage in order to increase the precision of the analysis. Using the proposed pointer alias analyses, we infer the types of variables in C programs and show that most C variables are used in a manner consistent with their declared types. We show that pointer analysis can be used to reduce the overhead of a dynamic string-buffer overflow detector by 30% to 100% among applications with significant overheads. Finally, using pointer analysis, we statically discover twelve actual format string vulnerabilities in three of the 12 programs we analyze.
[Algorithm design and analysis, type theory, C variables type inference, Software safety, C language, error detection, C pointer analysis, type safety, Operating systems, dynamic string-buffer overflow detection, program analysis, Permission, data structures, National security, Protection, optimistic analysis, buffer storage, object-oriented programming, data flow analysis, dynamic analysis, context-sensitive inclusion-based field-sensitive points-to analysis, Software debugging, Computer languages, security of data, Buffer overflow, software security, Software tools]
Locating causes of program failures
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Which is the defect that causes a software failure? By comparing the program states of a failing and a passing run, we can identify the state differences that cause the failure. However, these state differences can occur all over the program run. Therefore, we focus in space on those variables and values that are relevant for the failure, and in time on those moments where cause transitions occur - moments where new relevant variables begin being failure causes: "Initially, variable argc was 3; therefore, at shell-sort(), variable a[2] was 0, and therefore, the program failed." In our evaluation, cause transitions locate the failure-inducing defect twice as well as the best methods known so far.
[Software testing, Algorithm design and analysis, program debugging, program failure location, program diagnostics, Software algorithms, Debugging, automated debugging, Programming profession, Automatic testing, program analysis, Permission, Software engineering, adaptive testing]
An empirical study of fault localization for end-user programmers
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
End users develop more software than any other group of programmers, using software authoring devices such as e-mail filtering editors, by-demonstration macro builders, and spreadsheet environments. Despite this, there has been little research on finding ways to help these programmers with the dependability of their software. We have been addressing this problem in several ways, one of which includes supporting end-user debugging activities through fault localization techniques. This paper presents the results of an empirical study conducted in an end-user programming environment to examine the impact of two separate factors in fault localization techniques that affect technique effectiveness. Our results shed new insights into fault localization techniques for end-user programmers and the factors that affect them, with significant implications for the evaluation of those techniques.
[Software testing, System testing, program debugging, Filtering, program diagnostics, software reliability, end-user software engineering, Debugging, personal computing, Electronic mail, Programming profession, end-user programming, Programming environments, end-user debugging, Computer science, Automatic testing, software fault localization, software dependability, programming environments, Software engineering]
Goal-centric traceability for managing non-functional requirements
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
This paper describes a goal centric approach for effectively maintaining critical system qualities such as security, performance, and usability throughout the lifetime of a software system. In goal centric traceability (GCT) non-functional requirements and their interdependencies are modeled as softgoals in a softgoal interdependency graph (SIG). A probabilistic network model is then used to dynamically retrieve links between classes affected by a functional change and elements within the SIG. These links enable developers to identify potentially impacted goals; to analyze the level of impact on those goals; to make informed decisions concerning the implementation of the proposed change; and finally to develop appropriate risk mitigating strategies. This paper also reports experimental results for the link retrieval and illustrates the GCT process through an example of a change applied to a road management system.
[risk management, Roads, link retrieval, Maintenance engineering, softgoal interdependency graph, Risk analysis, Application software, software maintenance, nonfunctional requirements management, impact analysis, Engineering management, probabilistic network model, goal-centric traceability, systems analysis, Permission, Software systems, Temperature control, Usability, Quality management, system quality]
Real-time specification patterns
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Embedded systems are pervasive and frequently used for critical systems with time-dependent functionality. Dwyer et al. (1999) have developed qualitative specification patterns to facilitate the specification of critical properties, such as those that must be satisfied by embedded systems. Thus far, no analogous repository has been compiled for realtime specification patterns. This paper makes two main contributions: First, based on an analysis of timing-based requirements of several industrial embedded system applications, we created real-time specification patterns in terms of three commonly used real-time temporal logics. Second, as a means to further facilitate the understanding of the meaning of a specification, we offer a structured English grammar that includes support for real-time properties. We illustrate the use of the real-time specification patterns in the context of property specifications of a real-world automotive embedded system.
[Real time systems, Laboratories, structured English grammar, temporal logic, real-time temporal logics, Formal specifications, formal specification, Automotive engineering, Embedded software, Computer science, real-time specification patterns, Embedded system, embedded systems, Permission, pervasive system, Logic, Software engineering]
Monitoring and control in scenario-based requirements analysis
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Scenarios are an effective means for eliciting, validating and documenting requirements. At the requirements level, scenarios describe sequences of interactions between the software-to-be and agents in the environment. Interactions correspond to the occurrence of an event that is controlled by one agent and monitored by another. This paper presents a technique to analyse requirements-level scenarios for unforeseen, potentially harmful, consequences. Our aim is to perform analysis early in system development, where it is highly cost-effective. The approach recognises the importance of monitoring and control issues and extends existing work on implied scenarios accordingly. These so-called input-output implied scenarios expose problematic behaviours in scenario descriptions that cannot be detected using standard implied scenarios. Validation of these implied scenarios supports requirements elaboration. We demonstrate the relevance of input-output implied scenarios using a number of examples.
[message sequence charts, Object oriented modeling, Documentation, Educational institutions, Specification languages, formal verification, systems analysis, input-output implied scenarios, system development, Permission, Software systems, Performance analysis, scenario-based requirements analysis, Pattern analysis, Monitoring, Software engineering]
Model-based testing
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Model-based testing has become increasingly popular in recent years. Major reasons include: (1) the need for quality assurance for increasingly complex systems, (2) the emerging model-centric development paradigm, e.g., UML and MDA, with its seemingly direct connection to testing, and (3) the advent of test-centered development methodologies. Model-based testing relies on execution traces of behavior models. They are used as test cases for an implementation: input and expected output. This complements the ideas of model-driven testing. The latter uses static models to derive test drivers to automate test execution. This assumes the existence of test cases, and is, like the particular intricacies of OO testing, not in the focus of this tutorial. We cover major methodological and technological issues: the business case of model-based testing within model-based development, the need for abstraction and inverse concretization, test selection, and test case generation. We (1) discuss different scenarios of model-based testing, (2) present common abstractions when building models, and their consequences for testing, (3) explain how to use functional, structural, and stochastic test selection criteria, and (4) describe today's test generation technology. We provide both practical guidance and a discussion of the state-of-the-art. Potentials of model-based testing in practical applications and future research are highlighted.
[Software testing, static model, System testing, Costs, program testing, Automotive engineering, inverse concretization, model-centric development, behavior model, test case generation, Automatic control, Permission, model-driven testing, Automation, program diagnostics, test-centered development methodology, functional test selection criteria, stochastic test selection criteria, complex system, Automatic testing, Information security, quality assurance, structural test selection criteria, model-based testing, model-based development, Software engineering]
Is mutation an appropriate tool for testing experiments? [software testing]
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The empirical assessment of test techniques plays an important role in software testing research. One common practice is to instrument faults, either manually or by using mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. This paper investigates this important question based on a number of programs with comprehensive pools of test cases and known faults. It is concluded that, based on the data available thus far, the use of mutation operators is yielding trustworthy results (generated mutants are similar to real faults). Mutants appear however to be different from hand-seeded faults that seem to be harder to detect than real faults.
[Software testing, Performance evaluation, program testing, Instruments, software testing, Genetic mutations, Debugging, Computer science, Design engineering, Fault detection, mutation operators, Permission, Systems engineering and theory]
An empirical evaluation of test case filtering techniques based on exercising complex information flows
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Some software defects trigger failures only when certain complex information flows occur within the software. Profiling and analyzing such flows therefore provides a potentially important basis for filtering test cases. We report the results of an empirical evaluation of several test case filtering techniques that are based on exercising complex information flows. Both coverage-based and profile-distribution-based filtering techniques are considered. They are compared to filtering techniques based on exercising basic blocks, branches, function calls, and def-use pairs, with respect to their effectiveness for revealing defects.
[Software testing, System testing, program dependences, Computer aided software engineering, program testing, software testing, test case filtering, Debugging, data flow analysis, dynamic information flow analysis, Information filtering, observation-based testing, Computer science, coverage-based filtering, Automatic testing, Operating systems, profile-distribution-based filtering, Information filters, complex information flows, dynamic slicing, Software engineering]
Check 'n' crash: combining static checking and testing
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
We present an automatic error-detection approach that combines static checking and concrete test-case generation. Our approach consists of taking the abstract error conditions inferred using theorem proving techniques by a static checker (ESC/Java), deriving specific error conditions using a constraint solver, and producing concrete test cases (with the JCrasher tool) that are executed to determine whether an error truly exists. The combined technique has advantages over both static checking and automatic testing individually. Compared to ESC/Java, we eliminate spurious warnings and improve the ease-of-comprehension of error reports through the production of Java counterexamples. Compared to JCrasher, we eliminate the blind search of the input space, thus reducing the testing time and increasing the test quality.
[Software testing, Java, program testing, program diagnostics, software reliability, Vehicle crash testing, constraint solver, static analysis, Computer crashes, dynamic analysis, Information analysis, concrete test-case generation, Automatic testing, automatic error-detection, JCrasher tool, Permission, Concrete, Error correction, theorem proving, software tools, static checking, Software engineering]
Efficient and precise dynamic impact analysis using execute-after sequences
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
As software evolves, impact analysis estimates the potential effects of changes, before or after they are made, by identifying which parts of the software may be affected by such changes. Traditional impact-analysis techniques are based on static analysis and, due to their conservative assumptions, tend to identify most of the software as affected by the changes. Researchers have begun to investigate dynamic impact-analysis techniques, which rely on dynamic, rather than static, information about software behavior. Existing dynamic impact-analysis techniques are either very expensive in terms of execution overhead or amount of dynamic information collected - or imprecise. In this paper, we present a new technique for dynamic impact analysis that is almost as efficient as the most efficient existing technique and is as precise as the most precise existing technique. The technique is based on a novel algorithm that collects (and analyzes) only the essential dynamic information required for the analysis. We discuss our technique, prove its correctness, and present a set of empirical studies in which we compare our new technique with two existing techniques, in terms of performance and precision.
[Algorithm design and analysis, Software testing, dynamic impact analysis, Software maintenance, Costs, program diagnostics, Software algorithms, Educational institutions, Risk analysis, software maintenance, Information analysis, systems analysis, Permission, execute-after sequences, Software engineering]
DynAlloy: upgrading alloy with actions
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
We present DynAlloy, an extension to the Alloy specification language to describe dynamic properties of systems using actions. Actions allow us to appropriately specify dynamic properties, particularly, properties regarding execution traces, in the style of dynamic logic specifications. We extend Alloy's syntax with a notation for partial correctness assertions, whose semantics relies on an adaptation of Dijkstra's weakest liberal precondition. These assertions, defined in terms of actions, allow us to easily express properties regarding executions, favoring the separation of concerns between the static and dynamic aspects of a system specification. We also extend the Alloy tool in such a way that DynAlloy specifications are also automatically analyzable, as standard Alloy specifications. We present the foundations, two case-studies, and empirical results evidencing that the analysis of DynAlloy specifications can be performed efficiently.
[software specification, program verification, Reasoning about programs, Logic design, system specification, Specification languages, State-space methods, Alloy specification language, Formal specifications, formal specification, DynAlloy, Computer science, Software design, software validation, specification languages, Permission, Dijkstra weakest liberal precondition, Performance analysis, partial correctness assertions, Software engineering]
Beyond templates: a study of clones in the STL and some general implications
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Templates (or generics) help us write compact, generic code, which aids both reuse and maintenance. The STL is a powerful example of how templates help achieve these goals. Still, our study of the STL revealed substantial, and in our opinion, counter-productive repetitions (so-called clones) across groups of similar class or function templates. Clones occurred, as variations across these similar program structures were irregular and could not be unified by suitable template parameters in a natural way. We encountered similar problems in other class libraries as well as in application programs, written in a range of programming languages. In the paper, we present quantitative and qualitative results from our study. We argue that the difficulties we encountered affect programs in general. We present a solution that can treat such template-unfriendly cases of redundancies at the meta-level, complementing and extending the power of language features, such as templates, in areas of generic programming.
[Java, Software maintenance, meta-programming, Cloning, STL, software maintenance, software libraries, Computer science, Computer languages, Software libraries, Software design, Permission, software reusability, program clones, Software reusability, Software engineering]
The value of a usability-supporting architectural pattern in software architecture design: a controlled experiment
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Design patterns have been claimed to facilitate modification and improve understanding in software design. A controlled experiment was performed to assess the usefulness of portions of a usability-supporting architectural pattern (USAP) in modifying the design of software architectures to support a specific usability concern. Software engineering and information technology graduate students received different subsets of a USAP supporting cancellation functionality. They then studied a software architecture design and made modifications to add the ability to cancel commands. Results showed that participants who received a usability scenario, a list of general responsibilities, and a sample solution thought of significantly more key issues than participants who saw only the scenario. Implications for software development are that usability concerns can be included at architecture design time, and that USAPs can significantly help software architects to consider responsibilities inherent from usability concerns.
[System testing, software development, Programming, software architecture design, user interfaces, software architecture, Software design, Software architecture, systems analysis, Computer architecture, User interfaces, Permission, Software systems, Usability, usability-supporting architectural pattern, Software engineering]
Experimental context classification: incentives and experience of subjects
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
There is a need to identify factors that affect the result of empirical studies in software engineering research. It is still the case that seemingly identical replications of controlled experiments result in different conclusions due to the fact that all factors describing the experiment context are not clearly defined and hence controlled. In this article, a scheme for describing the participants of controlled experiments is proposed and evaluated. It consists of two main factors, the incentives for participants in the experiment and the experience of the participants. The scheme has been evaluated by classifying a set of previously conducted experiments from literature. It can be concluded that the scheme was easy to use and understand. It is also found that experiments that are classified in the same way to a large extent point at the same results, which indicates that the scheme addresses relevant factors.
[Context, Engineering management, software engineering research, Laboratories, Communication system control, experimental context classification, Permission, controlled experiment participant description, Software engineering, software metrics]
Introduction to the experience reports track
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[Software testing, Software architecture, Shape, Laboratories, Computer industry, Software tools, Organizing, Software engineering]
A cross-program investigation of students' perceptions of agile methods
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Research was conducted on using agile methods in software engineering education. This paper explores the perceptions of students from five different academic levels of agile practices. Information has been gathered through the collection of quantitative and qualitative data over three academic years, and analysis reveals student experiences, mainly positive but also some negative. Student opinions indicate the preference to continue to use agile practices at the workplace if allowed. A way these findings may potentially be extrapolated to the industrial settings is discussed. Finally, this report should encourage other academics considering adoption of agile methods in their computer science or software engineering curricula.
[Productivity, computer science education, Design methodology, Programming profession, Information analysis, agile methods, Computer science, extreme programming, Engineering management, Employment, Permission, student perceptions, software engineering education, software engineering, Software engineering, Testing]
Requirements interaction management in an eXtreme Programming environment: a case study
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
This paper reports experience applying the process of requirements interaction management (RIM) within the context of developing a commercial, Internet-based software application in an industrial venue, employing the eXtreme Programming (XP) methodology. We describe our means of managing the interactions - by extending the standard XP requirements process while maintaining consistency with the principles of XP itself. We outline our rationale for modifying the XP requirements process, and provide a temporal comparison showing that, for this project, modified process is essential to successful application of RIM.
[dependency analysis, Computer aided software engineering, Collaborative tools, extreme programming environment, system specification, Internet software development, user stories, Application software, Environmental management, interaction analysis, Programming environments, system analysis, requirements engineering, Web services, requirements interaction management, systems analysis, Permission, Streaming media, software engineering, Internet, Graphical user interfaces, Testing]
A multiple case study on the impact of pair programming on product quality
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Pair programming is a programming technique in which two programmers use one computer to work together on the same task. There is an ongoing debate over the value of pair programming in software development. The current body of knowledge in this area is scattered and unorganized. Review shows that most of the results have been obtained from experimental studies in university settings. Few, if any, empirical studies exist, where pair programming has been systematically under scrutiny in real software development projects. Thus, its proposed benefits remain currently without solid empirical evidence. This paper reports results from four software development projects where the impact of pair programming on software product quality was studied. Our empirical findings appear to offer contrasting results regarding some of the claimed benefits of pair programming. They indicate that pair programming may not necessarily provide as extensive quality benefits as suggested in literature, and on the other hand, does not result in consistently superior productivity when compared to solo programming.
[Productivity, Computer aided software engineering, Navigation, Scattering, Anthropometry, software quality, Programming profession, parallel programming, extreme programming, pair programming, Software quality, Permission, Solids, empirical software engineering, agile software development, Software engineering]
Re-engineering software architecture of home service robots: a case study
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
With the advances of robotics, computer science, and other related areas, home service robots attract much attention from both academia and industry. Home service robots present interesting technical challenges to the community in that they have a wide range of potential applications, such as home security, patient caring, cleaning, etc., and that the services provided by the robots in each application area are being defined as markets are formed and, therefore, they change constantly. Without architectural considerations to address these challenges, robot manufacturers often focus on developing technical components (e.g., vision recognizer, speech processor, and actuator) and then attempt to develop service robots by integrating these components. When prototypes are developed for a new application, or when services are added, modified, or removed from existing robots, unexpected, undesirable, and often dangerous side-effects, which are known as feature interaction problem, happen frequently. Reengineering of such robots can make a serious impact in delivery time and development cost. In this paper, we present our experience of re-engineering a prototype of a home service robot developed by Samsung Advanced Institute of Technology. First, we designed a modular and hierarchical software architecture that makes interaction among the components visible. With the visibility of interactions, we could assign functional responsibilities to each component clearly. Then, we re-engineered existing codes to conform to the new architecture using a reactive language Esterel. As a result, we could detect and solve feature interaction problems and alleviate the difficulty of adding or updating components.
[Robot vision systems, software reliability, reactive systems, software architecture reengineering, Cleaning, robot programming, Application software, Samsung Advanced Institute of Technology, Computer science, home automation, feature interaction problem, software architecture, systems re-engineering, Manufacturing processes, Software architecture, Service robots, home service robots, service robots, Prototypes, Speech recognition, Computer industry, Esterel reactive language]
Tool support for just-in-time architecture reconstruction and evaluation: an experience report
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The need for software architecture evaluation has drawn considerable attention. In practice, this is a challenging exercise for two main reasons. First, in deployed projects, software architecture documentation is often not readily available, and may not be a correct representation of the as built architecture. Second, large software systems have numerous potential views of the various architecturally significant structures in the system. In this paper, we assess the capabilities of software reverse engineering and architecture reconstruction tools to support just-in-time architecture reconstruction. If an application's architecture can be reconstructed efficiently, this could promote more effective architecture reviews and evaluations. We describe our experiences in leveraging multiple reconstruction tools and how these guided the choice of design artifacts to construct. We discovered that the tools complemented each other in identifying reconstruction scope, critical architectural elements, potential design irregularities and creating useful architectural views for different evaluation tasks. With the help of these tools, the reconstruction and evaluation effort was significantly streamlined and productive. Finally, we also report some potential improvements these tools could make.
[just-in-time architecture reconstruction, Reverse engineering, just-in-time architecture evaluation, Documentation, reverse engineering, Application software, Computer science, tool support, software architecture, CASE tools, Software design, Software architecture, software reverse engineering, Computer architecture, software architecture evaluation, computer aided software engineering, software tools, Australia, Software tools, Software engineering]
Global software development at Siemens: experience from nine projects
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
We report on the experiences of Siemens Corporation in nine globally-distributed software development projects. These projects represent a range of collaboration models, from co-development to outsourcing of components to outsourcing the software for an entire project. We report experience and lessons in issues of project management, division of labor, ongoing coordination of technical work, and communication. We include lessons learned, and conclude the paper with suggestions about important open research issues in this area.
[Productivity, Costs, Supply chain management, project management, Collaborative software, Project management, software development management, collaboration models, Programming, Siemens Corporation, Computer science, team working, global software development, outsourcing, Permission, multisite development, Collaborative work, Outsourcing, technical work coordination]
Five years of product line engineering in a small company
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
In 1999, a new team at MARKET MAKER Software AG began to develop a software product line for managing and displaying stock market data and financial market news. The basic idea was to use Web technology in all applications for delivering services to customers. It soon turned out that the company had to change both the processes and the organization. This report summarizes the changes made and the lessons learned over the past five years, when the product line idea was introduced into a small company which faced the pressure to quickly market the first product line instances.
[project management, stock market data software, financial market news software, Project management, Financial management, small-to-medium enterprises, Data engineering, World Wide Web, SME, Application software, Software development management, Databases, Engineering management, product development, Permission, software engineering, financial data processing, Internet, stock markets, Stock markets, product line engineering, Software engineering]
Introducing the PuLSE approach to an embedded system population at Testo AG
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Over the last few years, product line engineering has become a major theme in software engineering research, and is increasingly becoming a central topic of software engineering practice in the embedded domain. Migrating towards a product line approach is not an easy feat. It is even less so, if it is done under tight technology constraints in an embedded environment. It becomes even more difficult if the transition directly aims at integrating two product families into a single product population. In this paper, we discuss our experiences with a project where we successfully dealt with these difficulties and achieved a successful product line transition. In our paper, we strongly emphasize the role of technology transfer, as many facets of product line know-how had to be transferred to guarantee a complete transition to product line engineering. From the experiences of this project many lessons learned can be deduced, which can be transferred to different environments.
[Software testing, System testing, Testo AG, Pulse measurements, software development management, technology transfer, systematic software reuse, Software development management, embedded system population, software product line, Embedded system, Technology transfer, embedded systems, Permission, software reusability, Software systems, software engineering, Software reusability, PuLSE approach, product line engineering, product line introduction, Software engineering]
Requirements uncertainty: influencing factors and concrete improvements
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Practically all industry studies on software project results conclude that good requirements engineering plays a pivotal role for successful projects. A key reason for project failures is insufficient management of changing requirements during all stages of the project life cycle. This article investigates one of the root causes for changing requirements, namely requirements uncertainty. In an experimental field study we looked into four underlying drivers for requirements uncertainty. We found several techniques must be used simultaneously to see tangible success. Using only one such technique in isolation doesn't make a difference. The field study is supported by extensive data from well over 200 projects stemming from very different business areas of Alcatel over a period of two years. Results are presented with practical experiences to allow effective transfer.
[Uncertainty, project management, product life cycle management, Cement industry, requirements uncertainty, Project management, process improvement, software project, Delay, requirements engineering, Engineering management, product management, systems analysis, Permission, product life cycle, Computer industry, Concrete, Monitoring, Business]
Developing use cases and scenarios in the requirements process
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Scenarios are often used for discovering requirements using established techniques, but how such scenarios are initially developed is not so well understood. This experience paper reports the application of one scenario-based approach - RESCUE - to discover requirements for DMAN, an air traffic management system for the UK's National Air Traffic Services. A retrospective analysis of the DMAN use cases, scenarios and requirements artifacts revealed the importance of diverse information sources in the specification of use cases that enabled systematic requirements discovery. Results were used to explore 3 research questions that arose in previous studies. The paper reports lessons from this experience and offers guidelines that practitioners can apply in their requirements processes and academics can use to inform their research.
[Computer aided software engineering, requirements discovery, UK National Air Traffic Services, Human factors, traffic engineering computing, Guidelines, Information analysis, air traffic, Prototypes, systems analysis, air traffic management system, Permission, scenario development, Concrete, Concurrent engineering, Time factors, use case development, Software tools]
Observations and lessons learned from automated testing
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
This report addresses some of our observations made in a dozen of projects in the area of software testing, and more specifically, in automated testing. It documents, analyzes and consolidates what we consider to be of interest to the community. The major findings can be summarized in a number of lessons learned, covering test strategy, testability, daily integration, and best practices. The report starts with a brief description of five sample projects. Then, we discuss our observations and experiences and illustrate them with the sample projects. The report concludes with a synopsis of these experiences and with suggestions for future test automation endeavors.
[Software testing, Automation, program testing, software testing, Project management, automated testing, Asset management, Best practices, Automatic testing, Engineering management, Hardware, Software engineering, Quality management]
Static analysis tools as early indicators of pre-release defect density
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
During software development it is helpful to obtain early estimates of the defect density of software components. Such estimates identify fault-prone areas of code requiring further testing. We present an empirical approach for the early prediction of pre-release defect density based on the defects found using static analysis tools. The defects identified by two different static analysis tools are used to fit and predict the actual pre-release defect density for Windows Server 2003. We show that there exists a strong positive correlation between the static analysis defect density and the pre-release defect density determined by testing. Further, the predicted pre-release defect density and the actual pre-release defect density are strongly correlated at a high degree of statistical significance. Discriminant analysis shows that the results of static analysis tools can be used to separate high and low quality components with an overall classification rate of 82.91%.
[Software testing, object-oriented programming, program testing, software development, discriminant analysis, program diagnostics, Delay estimation, Debugging, Programming, static analysis tools, Computer science, Fault diagnosis, Reliability, software components, Windows Server 2003, Software tools, State estimation, Software engineering]
Validation methods for calibrating software effort models
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
COCONUT calibrates effort estimation models using an exhaustive search over the space of calibration parameters in a Cocomo I model. This technique is much simpler than other effort estimation method yet yields PRED levels comparable to those other methods. Also, it does so with less project data and fewer attributes (no scale factors). However, a comparison between COCONUT and other methods is complicated by differences in the experimental methods used for effort estimation. A review of those experimental methods concludes that software effort estimation models should be calibrated to local data using incremental holdout (not jack knife) studies, combined with randomization and hypothesis testing, repeated a statistically significant number of times.
[Costs, program verification, PRED, hypothesis testing, Laboratories, Cocomo I model, Calibration, Regression analysis, Yield estimation, Aerospace industry, Computer science, COCONUT, software effort estimation model calibration, Propulsion, Computer industry, software cost estimation, State estimation]
A case study on the automated verification of groupware protocols
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
We report on a fruitful combination of applying academic experience with formal modelling and verification techniques to an industrial case study. The goal of the case study was to investigate a priori, i.e. before implementation, the effects of adding a lightweight and easy-to-use publish/subscribe (event) notification service to thinkteam/spl reg/ - an asynchronous and dispersed groupware system which was developed by think3. Researchers from the Formal Methods and Tools (FM&&T) group of ISTI-CNR $with a longstanding experience in research on the development and application of formal methods, notations, and software tools for the specification, design, and verification of complex computer systems - therefore teamed up with think3 - a global provider of integrated product development solutions that provides mechanical design and Product Data Management (PDM) software catering the product management needs of design processes in the manufacturing industry. The technical details of this joint research effort have been documented elsewhere, here we report on the lessons learned from this experience.
[Protocols, Collaborative software, Computer aided manufacturing, publish-subscribe notification service, groupware protocols, Application software, Dispersion, Software development management, formal verification, formal modelling, thinkteam, groupware, formal methods, Collaborative work, Product development, Software tools, Research and development management, automated verification]
Introduction to education and training track
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[Industrial training, Computer science, Educational programs, Globalization, Educational institutions, Computer industry, Outsourcing, Open source software, Software engineering, Business]
Deciding what to design: closing a gap in software engineering education
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Software has jumped "out of the box" - it controls critical systems; it pervades business and commerce; it is embedded in myriad mechanisms; it infuses entertainment, communication, and other activities of everyday life. Designs for these applications are constrained not only by traditional considerations of capability and performance but also by economic, business, market, and policy issues and the context of intended use. The diversity of applications requires adaptability in responding to client needs, and the diversity of clients and contexts requires the ability to discriminate among criteria for success. As a result, software designers must also get out of their boxes: in addition to mastering traditional software development skills, they must understand the contextual issues that discriminate good solutions from merely competent ones. Current software engineering education, however, remains largely "in the box": it neglects the rich fabric of issues that lie between the client's problem and actual software development. At Carnegie Mellon we have addressed this major shortcoming with a course that teaches students to understand both the capabilities required by the client and the constraints imposed by the client's context. This paper presents our view of the engineering character of software engineering, describes the content and organization of our new course, reports on our experience from the first three offerings of our course, and suggests ways to adapt our course for other educational settings.
[Context, computer science education, software development skills, Communication system control, Programming, software designers, Control systems, Application software, Embedded software, Business communication, Communication system software, educational courses, Control engineering education, software engineering education, software engineering, Software engineering]
How to teach software modeling
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
To enhance motivation of students to study software engineering, some way of finding balance between the scientific aspect and the practical aspect of software engineering is required. In this paper, we claim that teaching multiple software modeling techniques from a unified viewpoint is a good way of obtaining the balance and attracting the students' interest as well.
[computer science education, Art, Educational products, Unified modeling language, Programming, formal specification, Information science, Software design, Software quality, Production, software modeling, software engineering education, Computer science education, Software engineering]
Software test program: a software residency experience
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The software test program (STP) is a cooperation between Motorola and the Center for Informatics of the Federal University of Pernambuco. It has been conceived with inspiration on the medical residency, adjusted to the software development practice. A software residency includes the formal teaching of the relevant concepts and deep practice, with specialization on some specific subject; here the focus is on software testing. The STP has been of great benefit to all parties involved.
[Software testing, computer science education, program testing, software development, Biomedical informatics, Human factors, medical residency, Programming, Information systems, Industrial training, software test program, Information science, software residency, software engineering, Computer science education, Systems engineering education, Motorola, Software engineering]
Enriching software engineering courses with service-learning projects and the open-source approach
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Real-world software engineers deal with complex problem. Yet many software engineering courses do not involve projects of enough complexity to give students such experience. We sense that service-learning projects, while difficult to manage and sustain, can serve a crucial role in this regard. Through trials in a senior-level software engineering course, we discovered that the open-source approach works well to enable students to work on large, multiple-term service-learning projects. We developed GROw, a cross-term, cross-team educational software process to meet the challenges of adopting complex, real-world projects in one-term courses, and to sustain service learning.
[Educational programs, computer science education, Costs, public domain software, real-world software engineers, Project management, Bifurcation, Open source software, Computer science, Information science, GROw, open-source approach, Computer industry, software engineering, Computer science education, service-learning projects, cross-team educational software, Software engineering]
Do students recognize ambiguity in software design? A multi-national, multi-institutional report
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Successful software engineering requires experience and acknowledgment of complexity, including that which leads designers to recognize ambiguity within the software design description itself. We report on a study of 21 post-secondary institutions from the USA, UK, Sweden, and New Zealand. First competency and graduating students as well as educators were asked to perform a software design task. We found that as students go from first competency to graduating seniors they tend to recognize ambiguities in under-specified problems. Additionally, participants who recognized ambiguity addressed more requirements of the design.
[Process design, computer science education, software complexity, software design ambiguity, Information systems, Computer science, Design engineering, Information science, Software design, Computer errors, software engineering, Computer science education, Engineering students, Software engineering]
The groupthink specification exercise
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Teaching students to read and write specifications is difficult. It is even more difficult to motivate specifications - to convince students of the value of specifications and make students eager to use them. This paper describes the group-think specification exercise. Groupthink is a fun group activity, in the style of a game show, that teaches students about specifications (the difficulty of writing them, techniques for getting them right, and criteria for evaluating them), teamwork, and communication. Specifications are not used as an end in themselves, but are motivated to students as a means to solving realistic problems that involve understanding system behavior. Students enjoy the activity, and it improves their ability to read and write specifications. The two-hour, low-prep activity is self-contained, scales from classes of ten to hundreds of students, and is freely available to other instructors.
[computer science education, groupthink specification, Documentation, formal specification, software system, Computer science, Information science, Engineering management, teamwork, Writing, Teamwork, Computer science education, Systems engineering education, Artificial intelligence, Software engineering]
Will earlier projects plus a disciplined process enforce SE principles throughout the CS curriculum?
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
This paper discusses two related challenges faced by software engineering instructors. First, assuming that projects are necessary to produce successful computer science majors, what should be the role of projects and how best do we integrate theory and application? Second, what life cycle models and associated processes should students have the opportunity to experience, and where in the curriculum should a disciplined process first appear? We review several curriculum plans that have been employed to address these problems. We also offer recommendations based on our experiences both with undergraduate computer science majors and with high school students in project Tri-P-LETS, where beginning programming students are taught to develop games and software simulations following a process.
[Educational programs, computer science education, Educational institutions, Application software, Programming profession, Software development management, Computer science, Tri-P-LETS, software engineering principles, Engineering management, educational courses, software engineering, Computer science education, Systems engineering education, computer science curriculum, Software engineering]
Some myths of software engineering education
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Based on many years of teaching software engineering, I present a number of lessons I have learned over the years. I do so in the form of a series of myths, the reverse of which can be considered challenges to educators. The overall message I wish to convey is that there's more to software engineering than engineering. The engineering metaphor gives us a lot of useful guidance in shaping our profession. But there's also a downside, in that this goes at the expense of the human, social dimension that is an essential element of our profession.
[computer science education, human factors, Human factors, Programming, Cultural differences, Organizing, Guidelines, Computer science, Information science, social dimension, Software quality, software engineering education, software engineering, Computer science education, human dimension, Software engineering]
Software engineering 2004: ACM/IEEE-CS guidelines for undergraduate programs in software engineering
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
This paper is an overview of Software Engineering 2004, the software engineering volume of the computing curricula 2001 project. We briefly describe the contents of the volume, the process used in developing the volume's guidelines, and how we expect the volume to be used in practice.
[Educational programs, computer science education, ACM guidelines, Educational products, IEEE-CS guidelines, computing curricula, Educational institutions, Information technology, Guidelines, Computer science, Feedback, educational courses, software engineering, Computer science education, IEEE standards, Standards development, Software engineering]
Towards increasing the compatibility of student pair programmers
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
As pair programming is used widely in software engineering education, instructors may wish to proactively form pairs to increase the likelihood of compatible pairs. A study involving 361 software engineering students was carried out at North Carolina State University to understand and predict pair compatibility. We have found that students are compatible with partners whom they perceive of similar skill, although instructors cannot proactively manage this perception. Pairing of two minority students is more likely and mixed gender pairs are less likely to be compatible. Additionally, pairing of students with similar actual skill level as measured by midterm grades in class, GPA, and SAT/GRE scores also likely results in compatible pairs. Our research addresses the following challenges faced by instructors in software engineering: 1) organizational concern in pairing of students; 2) increasing the retention rates of female and minority students in classes; and 3) proactively forming mutually-compatible pairs.
[Educational programs, computer science education, Demography, Human factors, Programming profession, Computer science, pair programming, Engineering management, SAT/GRE, software engineering education, GPA, software engineering, Computer science education, Springs, Object oriented programming, Software engineering]
Using peer reviews in teaching framework development
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Framework development is one of the most challenging software development tasks. Teaching framework development is even more challenging. In this paper, we propose a creative process to teach framework development. We propose using peer review in the process. We present the process and some examples and findings out of our experience.
[computer science education, software development, Object oriented modeling, Project management, Documentation, Programming, Application software, framework development teaching, Computer science, Engineering management, Education, software engineering, Standards development, Software engineering]
Process issues in course projects
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Defined software engineering process help teaching and guiding software engineering courses projects. However, using them raises several issues related to process and course features. Architecture issues relate to matching process and course lifecycle models. Size issues address project scope and extent. Support issues deal with student and instructor materials and tools.
[course projects, Educational programs, computer science education, Spirals, Job shop scheduling, Standardization, Programming, course lifecycle models, Information science, educational courses, Computer architecture, Management information systems, software engineering education, software engineering, Computer science education, Software engineering]
Towards an effective software engineering course project
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Software engineering instructors face many challenges. Among these challenges is the course project. Instructors are required to train their students on the professional skills to be ready for the real world business, which requires the students to work on real projects. However, because of the low quality of the students' work, not all of the professional organizations are cooperating to offer a chance for the software engineering students to work on a real project. Therefore, most of the software engineering courses' projects are in-class project, in which the instructors represent the clients. In this paper, I proposed a solution to this problem based on my experience in involving my software engineering students in real world projects.
[Computer science, Ethics, computer science education, Engineering management, Education, educational courses, Writing, Educational institutions, software engineering education, software engineering, software engineering course project, Software engineering]
Conducting empirical software engineering research in Nigeria: the posing problems
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Empirical software engineering research has advanced in many parts of the world especially the western nations, but little has been contributed in this research domain by the developing nations such as Nigeria, the well-acclaimed 'Giant of Africa'. The fast growing software industry in the country suggests that we need to incorporate solid software engineering studies into the various software process activities of the stakeholders in the industry, if at all quality software products must be turned out into the ever-competing global market. Recent survey of the Nigeria software industry shows that the industry is just coming into limelight, and that the industry is beset with 'software process compromise'. This short article takes a cursory look into the state of software engineering research in Nigeria with particular reference to the nature of the nation's software industry and the student/academic environment as well as their posing problems. The article concludes with some cogent recommendations.
[Project management, DP industry, Africa, global market, Management training, Computer science, Industrial training, software industry, Engineering management, Software quality, Computer industry, empirical software engineering, software engineering, globalisation, Software engineering, Business]
The making of a software engineer challenges for the educator
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Software engineering is foremost an engineering discipline. Engineering in general and software engineering specifically has to balance many factors to achieve viable tradeoffs - an understanding of the factors as well as the viability criteria is at the heart of the educational challenge. All engineering has one ultimate goal: the delivery of artifacts (products, commercial or not) that meet the needs of those using such artifacts. All engineering lives in the intersection of people, technology, domain, and opportunity aspects. Software engineering, however, is laden with its own specific difficulties. Software as an engineering medium fills a space between the fluidity of digital content, with which software shares the representation, and the nature of machines, with which software shares the flexible and repeatable application.
[Heart, computer science education, digital content, Supply chains, Documentation, Educational technology, Application software, educational challenge, Manufacturing industries, Information science, Space technology, educational courses, software engineering, Computer science education, Software engineering]
The challenges of software engineering education
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
We discuss the technical skills that a software engineer should possess. We take the viewpoint of a school of engineering and put the software engineer's education in the wider context of engineering education. We stress both the common aspects that crosscut all engineering fields and the specific issues that pertain to software engineering. We believe that even in a continuously evolving field like software, education should emphasize principles and recognize what are the stable and long-lasting design concepts. Even though the more mundane technological solutions cannot be ignored, the students should be equipped with skills that allow them to dominate the evolution of technology.
[Knowledge engineering, Continuing education, Courseware, computer science education, Educational institutions, Engineering education, Stress, Information science, software engineering education, software engineering, Computer science education, Books, technical skills, Software engineering]
Information systems development at the Virtual Global University: an experience report
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
In this paper, we present our experiences gained from the course "Information Systems Envelopment" which is entirely taught online at the Virtual Global University (VGU). We identify technical, economic, and pedagogical problems and challenges which are of general interest to anyone teaching software engineering online.
[computer science education, Costs, Virtual Global University, Least squares approximation, Information systems, Electronic learning, Education, information systems development, Management information systems, Streaming media, software engineering, information systems, Internet, Informatics, online teaching, Software engineering, Testing, Information Systems Envelopment]
A B.S. degree in informatics: contextualizing software engineering education
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Software engineering (SE) is very different in focus from traditional computer science: it is not just about computers and software, but as much about the context in which they are used. This means we must teach about software and information, development and design, technical and social issues, while creating solutions as well as understanding and analyzing them. In effect, we must teach a discipline broader than SE or CS alone for SE education to be effective. At UC Irvine, we designed and now offer a program doing just this - a four-year B.S. degree in informatics. The major brings topics in SE together with human-computer interaction, computer-supported collaborative work, social analysis, and management, along with other application disciplines. Here, we discuss the philosophy behind the major, its structure, and the questions concerning SE education that the new major raises.
[Educational programs, computer science education, teaching, social analysis, informatics, Application software, Information analysis, Guidelines, Computer science, Information science, human-computer interaction, social management, groupware, Collaborative work, software engineering education, software engineering, human computer interaction, contextual learning, Computer science education, Informatics, Software engineering, computer-supported collaborative work]
Software engineering education in the era of outsourcing, distributed development, and open source software: challenges and opportunities
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
As software development becomes increasingly globally distributed, and more software functions are delegated to common open source software (OSS) and commercial off-the-shelf (COTS) components, practicing software engineers face significant challenges for which current software engineering curricula may leave them inadequately prepared. A new multi-faceted distributed development model is emerging that effectively commoditizes many development activities once considered integral to software engineering, while simultaneously requiring practitioners to apply engineering principles in new and often unfamiliar contexts. We discuss the challenges that software engineers face as a direct result of outsourcing and other distributed development approaches that are increasingly being utilized by industry, and some of the key ways we need to evolve software engineering curricula to address these challenges.
[COTS component, Educational programs, computer science education, public domain software, open source software, Programming, distributed processing, engineering principle, distributed software development, Open source software, software engineering curricula, Design engineering, Engineering management, outsourcing, commercial off-the-shelf, Computer architecture, software packages, software engineering education, software engineering, Outsourcing, Computer science education, Software reusability, Software engineering]
The role of a project-based capstone course
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
A project-based capstone course aims at using software development skills while performing a project in the course domain. One of our main challenges is to simulate a real world environment so to provide our students with the experience they need. Planning this experience we should consider academic constraints as well as the students' schedule and skills. In this paper, we describe how we implement an agile software development method in a project-based capstone course in the domain of operating systems. We elaborate on how we simulate a real world environment and present a role scheme that is used by the students to manage the process. We suggest a discussion on how to use the role scheme as an assessment tool to measure the development process in general and students' contribution in particular. We expect to extend and refine the comprehension regarding process measurement in students' teams at the academia.
[Productivity, computer science education, role scheme, Computational modeling, project-based capstone course, operating system, Project management, academic constraint, student schedule, student skill, Environmental management, Programming profession, Computer science, assessment tool, Operating systems, educational courses, Particle measurements, software engineering, Computer science education, agile software development, project metrics, Testing]
Teaching human aspects of software engineering
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
This paper highlights the teaching of human aspects of software engineering, by presenting a course that deals with this topic. Specifically, this paper outlines the course's objective and structure, and, as the CfP asks, suggests two challenges an instructor of software engineering faces today.
[Educational programs, computer science education, Humans, human factors, Educational technology, Programming, teaching, History, human aspect, Ethics, Information science, educational courses, software engineering, course objective, Teamwork, Computer science education, course structure, Software engineering]
On the education of future software engineers
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The education of software engineers more and more addresses organizational and management issues, like for instance modeling the business structure and environment which receives a new software system. The teaching to software engineering students of modeling technologies based on standards like UML and the rational unified process, stressing the focus on designing business-oriented management of software services, raises novel questions than need to be addressed.
[computer science education, business-oriented management, Unified Modeling Language, Unified modeling language, teaching, Programming profession, Software design, modeling technology, Engineering management, Software systems, Computer industry, software engineering education, Software standards, software engineering, Internet, Computer science education, software service, Software engineering]
Agile software reuse recommender
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The demand for organisations to produce new or enhanced software implementations quickly in response to an ever changing environment has fuelled the use of agile processes. In such processes, the role of analysis, design models and documentation in the creation and evolution of the software is often marginalized. These challenges facing developers are the main motivation for our work. Reuse rewards and must be fostered. We must assist and encourage developers in making full use of large component repositories by complementing component retrieval with component recommendation, this allows developers to discover or locate components in a time efficient manner. The recommendation approach should be consistent with the principles of agile development; reusable components currently being developed should not need any additional documentation and reuse of such components should be appealing, straightforward and require little additional developer effort.
[component retrieval, Software maintenance, object-oriented programming, Filtering, Collaborative software, Documentation, component recommendation, Information retrieval, Educational institutions, agile development, Computer science, software reusability, component repository, Software reusability, Artificial intelligence, agile software reuse recommender, Software engineering]
Emergent process design
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The typical situation in software developing organizations can be characterized by two extremes: in some organizations or organizational units, software development processes are partly defined, whereas in organizations or organizational units where the processses are well defined, these processes are usually only valid at the project start. For design of a process that is able to fulfil the requirements, the author proposes an emergent process design (EPD) approach. The EPD approach guides the extraction and the adaptation of a project-specific process. This is done as follows. Initially, the project-specific process frame is extracted by tailoring one of the process variants provided in a process line. The first development is based on the tailored process frame. For further development iterations or projects that are to utilize this frame, the frame is adapted and refined to the project-specific process based on the logs of the performed tasks in the previous process iteration. In order to insure the desired quality of the final product, the project-specific process becomes more stringent over the course of the project.
[Process design, software development process, Costs, emergent process design, Project management, Programming, Software safety, Software development management, Technology management, Coordinate measuring machines, Engineering management, flexible plan driven process, project-specific process, software engineering, Research and development management]
Empirical validation of pair programming
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
This paper discusses an empirical assessment of pair programming. Several agile processes arose and have been adopted in industry with the promise of producing working software in the early phases of production process and to make the organization flexible and adaptable with respect to the changing environment. The outcomes of the experiments can be summarized as follows: (i) pair programming improves the productivity of developers with respect to solo programming; (ii) it also fosters knowledge transfer between the developers of the pair: the experiment suggests that this effect is emphasized when both the components own the same educational background; and, finally, (iii) distributing the components of a pair could seriously deteriorate expected benefits, if an appropriate support for communication and collaboration is not set up.
[Productivity, Art, Q factor, Design for experiments, Environmental economics, Programming profession, Knowledge transfer, agile process, knowledge transfer, pair programming, Engineering management, Computer industry, empirical software engineering, software engineering, programming, Software engineering]
Self-healing Web service compositions
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
SOAs (service oriented architectures) are becoming a new paradigm for flexible coordination between business partners within continuously evolving and heterogeneous environments. In these environments, the set of available services with which to do business changes constantly in number and type, hindering the degree of trustworthiness. In this research, the author focuses on how to augment this trust through the use of self-healing Web service compositions. There are three dimensions along which this research move to achieve the aforementioned goals: service composition and process description, monitoring and recovery strategies.
[Software testing, process description, recovery strategy, business change, Quality of service, quality of service, History, flexible coordination, process monitoring, Software debugging, system recovery, Engines, Web services, Software quality, Internet, Web service composition, service oriented architecture, Monitoring, Contracts, business data processing, Software engineering, functional requirement]
Test factoring: focusing test suites for the task at hand
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Frequent execution of a test suite during software maintenance can catch regression errors early, indicate whether progress is being made, and improve productivity. However, if the test suite takes a long time to produce feedback, the developer is slowed down, and the benefit of frequent testing is reduced. After a program is edited, ideally, only changed code would be tested. Any time spent executing previously tested, unchanged parts of the code is wasted. For a large test suite containing many small unit tests, test selection and prioritization can be effective. Test selection runs only those tests that are possibly affected by the most recent change, and test prioritization can run first the tests that are most likely to reveal a recently-introduced error.
[Software testing, Algorithm design and analysis, System testing, mock object, Costs, program testing, Instruments, regression error, Debugging, unit testing, test prioritization, software maintenance, Computer science, Automatic testing, Computer bugs, test factoring, test selection, Artificial intelligence]
Assurance patterns for distributed real-time embedded systems
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Recently, there has been a significant increase in demand for distributed real-time embedded systems (DREs). As the demand for DREs has increased, so has the complexity of these systems, thus prompting the need for more rigorous, repeatable, and cost-effective development techniques. We propose to make several contributions in the rigorous development of DREs: object analysis patterns, specification patterns, and an integrated modeling and analysis approach for the formal analysis of DRE system models. These contributions are presented collectively as assurance patterns that emphasize the analysis phase of DREs, with the intent of preventing and detecting errors in the early stages of development prior to coding and fabrication. Industrial projects play a major role in validating our techniques.
[Real time systems, Unified modeling language, object analysis pattern, software design, distributed processing, distributed real-time embedded system, error detection, formal specification, Information analysis, Embedded software, Embedded system, industrial project, embedded systems, specification pattern, formal analysis, Pattern analysis, integrated modeling, formal method, object-oriented programming, Natural languages, integrated analysis, Computer science, cost-effective development, requirements engineering, system complexity, software analysis, Timing, error prevention, Software engineering]
Knowledge-based architectural adaptation management for self-adaptive systems
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Self-adaptive systems continually evaluate and modify their own behavior to meet changing demands. An important element in the construction of architecture-based self-adaptive software is the specification of adaptation policy: this extended abstract presents an overview of work towards basing such specification on architecture-centric knowledge-based policies. This approach leverages techniques from the artificial intelligence field to explicitly represent adaptation policy at the architectural level, providing for strong decoupling between policy specification and architectural compositions, and supports dynamic runtime policy evolution promoting reuse potential and runtime flexibility.
[knowledge-based architectural adaptation management, Scalability, Decision making, dynamic runtime policy evolution, self-adjusting systems, Knowledge management, self-adaptive system, Application software, formal specification, artificial intelligence, architecture-based self-adaptive software, software architecture, runtime flexibility, Runtime, Software architecture, Engineering management, adaptation policy specification, knowledge based systems, software reusability, architecture-centric knowledge-based policy, Timing, Artificial intelligence, Software engineering]
Automatic generation of rule-based software configuration management systems
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
We propose a model-driven methodology and toolset for automatic SCM system repository creation and feature composition using code generation and rule engine technologies. Software configuration management (SCM) systems provide control over the evolution of complex software systems.
[Java, Software prototyping, rule engine technology, Control systems, program compilers, Engines, Computer science, configuration management, complex software system, Technology management, code generation, Prototypes, knowledge based systems, rule-based software configuration management system, feature composition, Software systems, model-driven methodology, Data models, automatic generation, Software engineering]
Design mentoring based on design evolution analysis
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The basic intuition underlying this work is that experienced designers are able to point out problematic patterns in the design structure of an artifact and questionable events and trends in its evolution. We believe that the very process of recognizing and reflecting upon specific interesting designs and design-evolution examples may help software developers acquire valuable design experience to complement their textbook knowledge. The objective of this work is to further develop JDEvAn, so that it becomes a software design-evolution mentor that can advise developers on a desired course of software maintenance and evolution, based on its assessment of the system's current design and the design rationale implicit in the system's evolution history. JDEvAn focuses on the logical view of object-oriented Java systems as the first design artifact to analyze.
[Java, Visualization, object-oriented programming, Object oriented modeling, Reverse engineering, Documentation, Relational databases, reverse engineering, software design evolution analysis, software design mentoring, History, Data mining, software maintenance, object-oriented Java system, structural evolution, Employee welfare, Software design, software design understanding, JDEvAn]
Demonstration of JIVE and JOVE: Java as it happens
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Dynamic software visualization is designed to provide programmers with insights as to what the program is doing. Most current visualizations either use program traces to show information about prior runs, slow the program down substantially, show only minimal information, or force the programmer to indicate when to turn visualizations on or off. We have developed a dynamic Java visualizer that provides a statement-level view of a Java program in action with low enough overhead so that it can be used almost all the time by programmers to understand what their program is doing while it is doing it.
[program comprehension, Java, Software performance, dynamic software visualization, reverse engineering, JIVE, Yarn, Programming profession, Computer science, Software design, Software libraries, program understanding, Data visualization, XML, JOVE, Modems, software engineering, program visualisation, dynamic Java visualizer, statement-level view]
Chianti: a change impact analysis tool for Java programs
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Chianti is a change impact analysis tool for Java that is implemented in the context of the eclipse environment. Chianti analyzes two versions of a Java program, decomposes their difference into a set of atomic changes, and a partial order inter-dependences of these changes is calculated. Change impact is then reported in terms of affected (regression or unit) tests whose execution behavior may have been modified by the applied changes. For each affected test, Chianti also determines a set of affecting changes that were responsible for the test's modified behavior. This latter step of isolating failure inducing changes for one specific test from irrelevant changes can be used as a debugging technique in situations where a test fails unexpectedly after a long editing session.
[Software testing, Algorithm design and analysis, Java, program debugging, Logic programming, Debugging, Logic testing, change impact analysis tool, regression test, Programming profession, partial order interdependence, object-oriented program, debugging technique, eclipse environment, Java program, Software systems, software tools, Object oriented programming, Software engineering]
The Concern Manipulation Environment
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The area of aspect-oriented software development (AOSD) has seen much progress in the past few years towards improving the quality of object-oriented, generative, and component-based software engineering, including some use in large-scale applications. Large-scale AOSD requires tools, paradigms, and methodologies that support multiple aspect models, multiple artifacts and formalisms, and multiple tasks and activities. The Concern Manipulation Environment (CME) is an Eclipse open source project that aims to provide a set of open, extensible components and a set of tools that promote aspect-oriented software development throughout the software lifecycle. This paper provides an overview of this programming environment. It also provides a general discussion of the available tools and the platforms where this environment can be integrated.
[Visualization, Protocols, object-oriented programming, open source project, Object oriented modeling, Collaborative software, public domain software, Eclipse, software lifecycle, aspect-oriented software development, Security, Concern Manipulation Environment, Open source software, programming environment, Large-scale systems, software tools, Software tools, Object oriented programming, programming environments, Software engineering]
Continuous testing in Eclipse
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Continuous testing uses excess cycles on a developer's workstation to continuously run regression tests in the background, providing rapid feedback about test failures as code is edited. It reduces the time and energy required to keep code well-tested, and it prevents regression errors from persisting uncaught for long periods of time.
[Software testing, Performance evaluation, Costs, program testing, regression error, Humans, Eclipse, Debugging, test failure, continuous testing, regression test, Computer science, development environment, Feedback, Workstations, Standards development, Artificial intelligence]
The Fujaba real-time tool suites: model-driven development of safety-critical, real-time systems
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
More and more complex functionality is today realized with complex, networked, real-time systems. The majority of the costs and time in development is required to design and verify the control software. As these systems are often used in a safety-critical environment, the Fujaba real-time tool suite aims at supporting the model-driven development of correct software for such safety-critical, networked, real-time systems. This paper presents an overview of this Unified Modeling Language (UML) tool, its framework and processes.
[Real time systems, Unified Modeling Language, software development, Unified modeling language, safety-critical software, UML tool, Software safety, model-driven development, networked system, Embedded software, Connectors, Software design, model checking, real-time systems, safety-critical system, embedded system, software tools, Software reusability, Software tools, real-time system, Fujaba real-time tool suite, Software engineering, Clocks]
CodeCrawler - an information visualization tool for program comprehension
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
CodeCrawler is a language independent, interactive, information visualization tool. It is mainly targeted at visualizing object-oriented software, and has been successfully validated in several industrial case studies over the past few years. CC adheres to lightweight principles: it implements and visualizes polymetric views, visualizations of software enriched with information such as software metrics and other source code semantics. CC is built on top of Moose, an extensible language independent reengineering environment that implements the FAMIX metamodel. In its last implementation, CC has become a general-purpose information visualization tool.
[program comprehension, extensible language independent reengineering environment, Visualization, Java, Object oriented modeling, Reverse engineering, object-oriented software, reverse engineering, Data mining, software visualization, Moose, Information analysis, Software metrics, FAMIX metamodel, Computer industry, CodeCrawler, program visualisation, Informatics, Software tools, information visualization tool]
Fluent-based Web animation: exploring goals for requirements validation
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
We present a tool that provides effective graphical animations as a means of validating both goals and software designs. Goals are objectives that a system is expected to meet. They are decomposed until they can be represented as fluents. Animations are specified in terms of fluents and driven by behaviour models.
[Visualization, program verification, software design, Human factors, Educational institutions, Control systems, graphical animation, formal specification, Software design, Web pages, behaviour model, Animation, Software systems, software tools, Logic, program visualisation, requirements validation, fluent-based Web animation, Software engineering]
Modeling and implementing software architecture with Acme and ArchJava
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
We demonstrate a tool to incrementally synchronize an Acme architectural model described in the Acme Architectural Description Language (ADL) with an implementation in ArchJava, an extension of the Java programming language that includes explicit architectural modeling constructs.
[Acme architectural model, Java, Navigation, Documentation, ArchJava, Java programming language, architectural modeling, Connectors, Computer languages, software architecture, Software architecture, Computer architecture, specification languages, Software systems, Architecture description languages, Software tools, Architectural Description Language]
Workshop on advances in model-based software testing
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[]
Software engineering for secure systems - SESS05 building trustworthy applications
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[Design engineering, Software maintenance, Maintenance engineering, Computer industry, Robustness, Application software, Security, Software tools, Software engineering, Testing]
Software engineering for large-scale multi-agent systems - SELMAS'05
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[Context-aware services, Multiagent systems, Information security, Software systems, Software agents, Large-scale systems, Application software, Artificial intelligence, Software engineering, Context modeling]
Second international workshop on software engineering for high performance computing system applications
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[]
7th international workshop on economics-driven software engineering research
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The thems and topics of this workshop are discussed.
[]
First international workshop on the modeling and analysis of concerns in software (MACS 2005)
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Presents the title page of the proceedings record.
[]
Human and social factors of software engineering
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[Computer science, Software maintenance, Collaborative software, Psychology, Software quality, Computer architecture, Human factors, Programming, Social factors, Software engineering]
Third workshop on software quality
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[]
MSR 2005 international workshop on mining software repositories
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[]
Workshop on architecting dependable systems (WADS 2005)
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[]
Predictor models in software engineering (PROMISE)
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[Machine learning algorithms, Machine learning, Software quality, Predictive models, Software measurement, Cost benefit analysis, Information technology, Artificial intelligence, Software engineering, Testing]
Third international workshop on dynamic analysis (WODA 2005)
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[]
Open source application spaces: the 5th workshop on open source software engineering
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[]
2nd Intl. Workshop on Software Engineering for Automotive Systems
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The workshop is intended to provide a discussion forum for researchers and practitioners working in or interested in the field of automotive software. The organizers and the program committee address both academia and industry, in order to transfer techniques and methods from other domains to that of embedded systems.
[]
Models and processes for the evaluation of off-the-shelf components - MPEC'05
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[Performance evaluation, Software maintenance, Costs, Software quality, Software systems, Proposals, Software reusability, Software tools, Open source software, Software engineering]
4th International Workshop on Scenarios and State Machines:, Models, Algorithms and Tools (SCESM'05)
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The thems and topics of this workshop are discussed.
[]
The first workshop on end-user software engineering
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[]
DEAS 2005: workshop on the design and evolution of autonomic application software
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Understanding software engineering issues for autonomic computing systems is critical for the software and information technology sectors, which are continually challenged to reduce the complexity of their systems. To be autonomic, a system must know itself as well as its boundaries and its environment, configure and reconfigure itself, continually optimize itself, recover or heal from malfunction, protect itself, and function in a heterogeneous world-while keeping its complexity hidden from the user. The goal of this workshop is to bring together researchers and practitioners, who investigate concepts, methodologies, techniques, technologies, and tools to design and evolve autonomic software.
[Autonomic computing, self-managed systems]
Rules of thumb for secure software engineering
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
The basis of this article is a tutorial that steps into software security breaches, by disseminating knowledge about the underlying principles of secure software. The main part is the presentation of 20 rules of secure software development.
[Java, software development, Thumb, Software performance, Programming, Application software, security of data, Information security, Computer errors, software engineering, software security, Computer security, Protection, Software engineering, secure software]
The software engineering of agent-based intelligent adaptive systems
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Future software systems will be intelligent and adaptive. They will have the ability to seamlessly integrate with smart applications that have not been explicitly designed to work together. Traditional software engineering approaches offer limited support for the development of intelligent systems. To handle the tremendous complexity and the new engineering challenges presented by intelligence, adaptiveness and seamless integration, developers need higher-level development constructs. Agent concepts are natural to describe intelligent adaptive systems. Agent-based technologies have been incorporating software engineering practices, and have matured to offer useful insights and concrete practices to mainstream software engineers. This tutorial presents the state of the art in agent development from a software engineering perspective, focusing on practices that are applicable today. We have walked the audience through analysis, design and verification of a portion of a real-world problem, a smart home network. We show how agent concepts more naturally match the engineering challenges of such systems like trust between adaptive components. The audience had a hands-on experience with analyzing, and designing parts of the smart home network and learn how to incorporate agent technologies into their current projects.
[knowledge engineering, Adaptive systems, Multiagent systems, Protocols, multi-agent systems, Smart homes, Application software, software agents, Intelligent agent, adaptive systems, multiagent system, smart home network, knowledge based systems, agent development, Concrete, software engineering, Intelligent systems, Artificial intelligence, agent-based intelligent adaptive system, ROADMAP, Software engineering]
Spiral development of software-intensive systems of systems
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Commercial, public service, and national security organizations are finding it increasingly attractive to integrate component system capabilities from many different best of breed sources to achieve their objectives. In doing so, they find that they are increasingly dependent on software to integrate the systems and to rapidly adapt them in response to competitive opportunities or threats, new technologies, or new organizational priorities. The resulting software-intensive systems of systems (SISOS) offer significant benefits, but come with significant new types of risks, such as simultaneous satisfaction of multiple stakeholders and quality attributes, and integration and rapid adaptation of multiple heterogeneous software products and COTS products. This tutorial provides software engineers, managers and researchers with an understanding of and emerging capabilities for practicing software engineering in the very large. It identifies the major opportunities and risks involved in software- intensive systems of systems, presents experience-based techniques for realizing the opportunities and mitigating the risks. It provides extensions of the risk-driven spiral model that are being used for SISOS development processes, and provides case study exercises to give participants experience in applying the techniques in representative situations.
[risk management, Spirals, ISO, experience-based technique, software-intensive systems of systems, spiral development, Software development management, Coordinate measuring machines, win-win spiral, Engineering management, systems analysis, Software quality, COTS product, Concurrent engineering, software engineering, multiple heterogeneous software products, Risk management, National security, Software engineering, software process]
The software engineer and the development, management and use of intellectual property
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[Technological innovation, Ethics, Engineering management, Intellectual property, Programming, Licenses, Protection, Open source software, Software development management, Business]
Financially informed requirements prioritization
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
This tutorial introduces a financially responsible approach to requirements prioritization that enhances the value creating potential of a software development project. The approach, known as the incremental funding method (IFM), is described in the book "Software by Numbers: Low-risk, High-Return Development". Tutorial attendees have learned how to group requirements into chunks of revenue-generating functionality known as minimal marketable features (MMFs), and how to carefully sequence those MMFs in order to maximize the overall value of the project, reduce initial funding investments, and manipulate other project metrics such as the time needed for a project to reach break-even status. A gentle introduction to financial analysis also equips participants to analyze and understand the impact of other requirements prioritization decisions upon the financial returns of a project. This process is applicable within any iterative development approach.
[financial analysis, Costs, project management, feature sequencing, incremental funding method, requirements prioritization, Project management, Time to market, software development management, minimal marketable features, Programming, Financial management, software development project, Investments, Character generation, Life estimation, net present value, financial project returns, Books, Iterative methods, project metrics, software metrics]
Component-based software engineering for embedded systems
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Although attractive, CBD has not been widely adopted in domains of embedded systems. The main reason is inability of these technologies to cope with the important concerns of embedded systems, such as resource constraints, real-time or dependability requirements. However, an increasing understanding of principles of CBD makes it possible to utilize these principles in implementation of different component-based models more appropriate for embedded systems. The aim of this tutorial is to point to the opportunity of applying this approach for development and maintenance of embedded systems. The tutorial gives insights into basic principles of CBD, the main concerns and characteristics of embedded systems and possible directions of adaptation of component-based approach for these systems. Different types of embedded systems and approaches for applying CBD are presented and illustrated by examples from research and practices. Also, challenges and research directions of CBD for embedded systems are discussed.
[Real time systems, Embedded computing, object-oriented programming, Object oriented modeling, component-based software engineering, Process control, Control systems, Reliability engineering, Application software, Design engineering, Embedded system, embedded systems, embedded system, software engineering, Software engineering, component-based development]
Story driven modeling a practical guide to model driven software development
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[Cyclic redundancy check, Java, System testing, Computer aided software engineering, Object oriented modeling, Unified modeling language, Programming, Application software, Guidelines, Software engineering]
Understanding metamodeling
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Metamodeling not only directly underpins the specification of modeling languages such as the UML, but is also the foundation for making the OMG's MDA vision come true. This paper starts by motivating metamodeling as an advanced way of creating software and then goes on to explore its fundamental principles. In particular, important new metamodeling concepts such as the distinction between ontological and linguistic instance-of relationships, the unification of class and object facets and deep instantiation are introduced. A metamodeling framework suitable for MDA is constructed step-by-step and then used to explain and critique the OMG's various metamodeling technologies. This information furnishes modelers with the heuristics they need to more effectively utilize OMG metamodeling technology and to know when metamodeling concepts are suitable and when they are not. The paper ends with some methodological advice on how to model in the presence of more than two modeling levels (objects and classes).
[ontological classification, Costs, type model, Unified modeling language, Metamodeling, Ontologies, metatype, Personnel, deep instantiation, model driven development, instance-facet, linguistic instance-of relationship, software engineering, Productivity, modeling language, object-oriented programming, token model, software creation, type-facet, Face detection, Middleware, Computer languages, metamodeling, ontologies (artificial intelligence), ontological instance-of relationship, linguistic classification, Software engineering]
Software visualization
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[Computer science, Software maintenance, Computer languages, Software design, Software algorithms, Data visualization, Computer graphics, Debugging, Animation, Software engineering]
Engineering safety-related requirements for software-intensive systems
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Many software-intensive systems have significant safety ramifications and need to have their associated safety-related requirements properly engineered. However, there is little effective interaction and collaboration between the requirements and safety teams on most projects. This paper is intended to improve such collaboration by providing clear definitions of the different kinds of safety-related requirements, examples of such requirements, and a generic process for producing them.
[Collaborative software, Documentation, Standardization, safety-critical software, Hazards, Software safety, software-intensive system, formal specification, Design engineering, Collaboration, safety-related requirement engineering, Product safety, Accidents, Software engineering]
Model-based testing
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
false
[Software testing, System testing, Quality assurance, Automatic testing, Unified modeling language, Buildings, Information security, Stochastic processes, Application software, Software engineering]
Reverse engineering of object oriented code
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
During software evolution, programmers devote most of their effort to the understanding of the structure and behavior of the system. For object oriented code, this might be particularly hard, when multiple, scattered objects contribute to the same function. Design views offer an invaluable help, but they are often not aligned with the code, when they are not missing at all. This tutorial describes some of the most advanced techniques that can be employed to reverse engineer several design views from the source code. The recovered diagrams, represented in UML (Unified Modeling Language), include class, object, interaction (collaboration and sequence), state and package diagrams. A unifying static code analysis framework used by most of the involved algorithms is presented at the beginning of the tutorial. A single running example is referred all over the presentation. Trade-offs (e.g., static vs. dynamic analysis), limitations and expected benefits are also discussed.
[Algorithm design and analysis, Software maintenance, object-oriented programming, program diagnostics, Reverse engineering, Unified modeling language, object oriented code, Scattering, reverse engineering, diagram recovery, Programming profession, static code analysis, Collaboration, Packaging, object oriented programming, Object oriented programming, Software engineering]
An architects' guide to enterprise application integration with J2EE and .NET
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Architects are faced with the problem of building enterprise scale information systems, with streamlined, automated internal business processes and Web-enabled business functions, all across multiple legacy applications. The underlying architectures for such systems are embodied in a range of diverse products known as enterprise application integration (EAI) technologies. We highlight some of the major problems, approaches and issues in designing EAI architectures and selecting appropriate supporting technology. An architect's perspective on designing large-scale integrated applications is taken, and we discuss requirements elicitation, architecture patterns, EAI technology and features, and risk mitigation. J2EE and .NET technologies are used to illustrate the capabilities of state-or-the-art integration technologies.
[Java, architecture pattern, Data security, Buildings, large-scale integrated application, Large scale integration, J2EE, Application software, Appropriate technology, risk mitigation, integration architecture, Information systems, Computer languages, software architecture, enterprise application integration, Software architecture, COTS, network operating systems, integrated software, Computer architecture, .NET, information systems, requirements elicitation, Australia]
Transformations of software models into performance models
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
It is widely recognized that in order to make performance validation an integrated activity along the software lifecycle, it is crucial to be supported from automated approaches. Easiness to annotate software models with performance parameters (e.g. the operational profile) and automated translations of the annotated models into ready-to-validate models are the key challenges in this direction. Several methodologies have been introduced in the last few years to address these challenges. The tutorial introduces the attendance to the main methodologies for annotating and transforming software models into performance models.
[ready-to-validate model, program verification, performance parameter, Software algorithms, software model, Software performance, Programming, performance model, annotated model, Embedded software, Software design, Software architecture, Software quality, Software systems, Hardware, software performance evaluation, Software engineering]
Aspect-oriented programming
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
Summary form only given on an article discussing Aspect-oriented programming.
[Computer languages, Software design, Computer architecture, Packaging, Resource management, Object oriented programming]
What you always wanted to know about agile methods but did not dare to ask
Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.
None
2005
A fleet of emerging agile methods is both gaining popularity and generating lots of controversy. Real-world examples argue for (e.g. Highsmith and Cockburn, 2001) and against (e.g. Rakitin, 2001) agile methods. Several leading software engineering experts suggest that synthesizing the two, agile with traditional, may provide developers with a comprehensive spectrum of methods (e.g. Boehm, 2002 and 2004 and Humphrey, 2005). This high-level overview tutorial provides background to understand how agile teams are trying to solve modern software development issues.
[Productivity, Software testing, System testing, Lean, software development, Amplitude shift keying, agile modeling, Inspection, Programming profession, agile method, extreme programming, Engineering management, Failure analysis, Modems, software engineering, Scrum, Software engineering, Crystal]
Message from the General Chair
29th International Conference on Software Engineering
None
2007
Presents the welcome message from the conference proceedings.
[]
Message from the Program Committee Co-Chairs
29th International Conference on Software Engineering
None
2007
Presents the welcome message from the conference proceedings.
[]
Conference organization
29th International Conference on Software Engineering
None
2007
Provides a listing of current committee members and society officers.
[]
ICSE 2007 Sponsors and Supporters
29th International Conference on Software Engineering
None
2007
Provides a listing of current committee members and society officers. The conference organizers greatly appreciate the support of the various corporate sponsors listed.
[]
Parallel Randomized State-Space Search
29th International Conference on Software Engineering
None
2007
Model checkers search the space of possible program behaviors to detect errors and to demonstrate their absence. Despite major advances in reduction and optimization techniques, state-space search can still become cost-prohibitive as program size and complexity increase. In this paper, we present a technique for dramatically improving the cost- effectiveness of state-space search techniques for error detection using parallelism. Our approach can be composed with all of the reduction and optimization techniques we are aware of to amplify their benefits. It was developed based on insights gained from performing a large empirical study of the cost-effectiveness of randomization techniques in state-space analysis. We explain those insights and our technique, and then show through a focused empirical study that our technique speeds up analysis by factors ranging from 2 to over 1000 as compared to traditional modes of state-space search, and does so with relatively small numbers of parallel processors.
[Java, Costs, program testing, Data structures, error detection, Concurrent computing, Computer science, optimisation, parallel randomized state-space search, model checkers, optimization techniques, Computer errors, Parallel processing, Error correction, Performance analysis, parallel processors, Logic]
A Sound Assertion Semantics for the Dependable Systems Evolution Verifying Compiler
29th International Conference on Software Engineering
None
2007
The verifying compiler (VC) project is a core component of the dependable systems evolution grand challenge. The VC offers the promise of automatically proving that a program or component is correct, where correctness is defined by program assertions. While several VC prototypes exist, all adopt a semantics for assertions that is unsound. This paper presents a consolidation of VC requirements analysis activities that, in particular, brought us to ask targeted VC customers what kind of semantics they wanted. Taking into account both practitioners' needs and current technological factors, we offer recovery of soundness through an adjusted definition of assertion validity that matches user expectations and can be implemented practically using current prover technology. We describe how support for the new semantics has been added to ESC/Java2. Preliminary results demonstrate the effectiveness of the new semantics at uncovering previously indiscernible specification errors.
[Software prototyping, Java, Virtual colonoscopy, Costs, program verification, dependable systems evolution, verifying compiler project, program compilers, requirements analysis, Runtime, Prototypes, Error correction, Performance analysis, Software tools, sound assertion semantics, Software engineering]
Behaviour Model Synthesis from Properties and Scenarios
29th International Conference on Software Engineering
None
2007
Synthesis of behaviour models from software development artifacts such as scenario-based descriptions or requirements specifications not only helps significantly reduce the effort of model construction, but also provides a bridge between approaches geared toward requirements analysis and those geared towards reasoning about system design at the architectural level. However, the models favoured by existing synthesis approaches are not sufficiently expressive to describe both universal constraints provided by requirements and existential statements provided by scenarios. In this paper, we propose a novel synthesis technique that constructs behaviour models in the form of modal transition systems (MTS) from a combination of safety properties and scenarios. MTSs distinguish required, possible and proscribed behaviour, and their elaboration not only guarantees the preservation of the properties and scenarios used for synthesis but also supports further elicitation of new requirements.
[software development artifacts, Buildings, Programming, Educational institutions, Software safety, behaviour model synthesis, System analysis and design, scenario-based descriptions, Bridges, requirements analysis, Analytical models, Upper bound, modal transition systems, Animation, software engineering, Software engineering]
Feature Oriented Model Driven Development: A Case Study for Portlets
29th International Conference on Software Engineering
None
2007
Model driven development (MDD) is an emerging paradigm for software construction that uses models to specify programs, and model transformations to synthesize executables. Feature oriented programming (FOP) is a paradigm for software product lines where programs are synthesized by composing features. feature oriented model driven development (FOMDD) is a blend of FOP and MDD that shows how products in a software product line can be synthesized in an MDD way by composing features to create models, and then transforming these models into executables. We present a case study of FOMDD on a product line of portlets, which are components of web portals. We reveal mathematical properties of portlet synthesis that helped us to validate the correctness of our abstractions, tools, and specifications, as well as optimize portlet synthesis.
[Automation, object-oriented programming, Documentation, portals, Programming, feature oriented model driven development, Web portals, software construction, Software standards, software engineering, DSL, Mathematical model, Standards development, software product, Portals, feature oriented programming]
Matching and Merging of Statecharts Specifications
29th International Conference on Software Engineering
None
2007
Model Management addresses the problem of managing an evolving collection of models, by capturing the relationships between models and providing well-defined operators to manipulate them. In this paper, we describe two such operators for manipulating hierarchical Statecharts: Match, for finding correspondences between models, and Merge, for combining models with respect to known correspondences between them. Our Match operator is heuristic, making use of both static and behavioural properties of the models to improve the accuracy of matching. Our Merge operator preserves the hierarchical structure of the input models, and handles differences in behaviour through parameterization. In this way, we automatically construct merges that preserve the semantics of Statecharts models. We illustrate and evaluate our work by applying our operators to AT&amp;T telecommunication features.
[Vocabulary, Merging, Laboratories, statecharts specifications, hierarchical structure, Performance gain, Visual databases, formal specification, match operator, Computer science, Machine learning, Software systems, Performance analysis, AT&amp;T telecommunication features, merge operator, hierarchical statecharts, Software engineering]
Regression Test Selection for AspectJ Software
29th International Conference on Software Engineering
None
2007
As aspect-oriented software development gains popularity, there is growing interest in using aspects to implement cross-cutting concerns in object-oriented systems. When aspect-oriented features are added to an object-oriented program, or when an existing aspect-oriented program is modified, the new program needs to be regression tested to validate these changes. To reduce the cost of regression testing, a regression-test-selection technique can be used to select only a necessary subset of test cases to rerun. Unfortunately, existing approaches for regression test selection for object-oriented software are not effective in the presence of aspectual information woven into the original code. This paper proposes a new regression-test-selection technique for AspectJ programs. At the core of our approach is a new control-flow representation for AspectJ software which captures precisely the semantic intricacies of aspect-related interactions. Based on this representation, we develop a novel graph comparison algorithm for test selection. Our experimental evaluation shows that, compared to existing approaches, the proposed technique is capable of achieving significantly more precise test selection.
[Software testing, Algorithm design and analysis, Java, System testing, Software maintenance, Costs, object-oriented programming, program testing, software development, Object oriented modeling, AspectJ software, object-oriented software, graph comparison algorithm, regression analysis, aspect-oriented program, Programming, regression test selection, object-oriented systems, Program processors, Weaving, software engineering]
Feedback-Directed Random Test Generation
29th International Conference on Software Engineering
None
2007
We present a technique that improves random test generation by incorporating feedback obtained from executing test inputs as they are created. Our technique builds inputs incrementally by randomly selecting a method call to apply and finding arguments from among previously-constructed inputs. As soon as an input is built, it is executed and checked against a set of contracts and filters. The result of the execution determines whether the input is redundant, illegal, contract-violating, or useful for generating more inputs. The technique outputs a test suite consisting of unit tests for the classes under test. Passing tests can be used to ensure that code contracts are preserved across program changes; failing tests (that violate one or more contract) point to potential errors that should be corrected. Our experimental results indicate that feedback-directed random test generation can outperform systematic and undirected random test generation, in terms of coverage and error detection. On four small but nontrivial data structures (used previously in the literature), our technique achieves higher or equal block and predicate coverage than model checking (with and without abstraction) and undirected random generation. On 14 large, widely-used libraries (comprising 780KLOC), feedback-directed random test generation finds many previously-unknown errors, not found by either model checking or undirected random generation.
[Software testing, System testing, Law, program testing, Object oriented modeling, failing tests, passing tests, error detection, Open source software, feedback-directed random test generation, Filters, Feedback, Error correction codes, Contracts, Legal factors]
Compatibility and Regression Testing of COTS-Component-Based Software
29th International Conference on Software Engineering
None
2007
Software engineers frequently update COTS components integrated in component-based systems, and can often chose among many candidates produced by different vendors. This paper tackles both the problem of quickly identifying components that are syntactically compatible with the interface specifications, but badly integrate in target systems, and the problem of automatically generating regression test suites. The technique proposed in this paper to automatically generate compatibility and prioritized test suites is based on behavioral models that represent component interactions, and are automatically generated while executing the original test suites on previous versions of target systems.
[Software testing, System testing, Java, object-oriented programming, program testing, Natural languages, regression testing, compatibility testing, commecial off-the-shelf, Control systems, interface specification, formal specification, COTS component-based software, Automatic testing, XML, Binary codes, software packages, Lead, Software systems, component interaction representation]
DECKARD: Scalable and Accurate Tree-Based Detection of Code Clones
29th International Conference on Software Engineering
None
2007
Detecting code clones has many software engineering applications. Existing approaches either do not scale to large code bases or are not robust against minor code modifications. In this paper, we present an efficient algorithm for identifying similar subtrees and apply it to tree representations of source code. Our algorithm is based on a novel characterization of subtrees with numerical vectors in the Euclidean space Rnmiddot and an efficient algorithm to cluster these vectors w.r.t. the Euclidean distance metric. Subtrees with vectors in one cluster are considered similar. We have implemented our tree similarity algorithm as a clone detection tool called DECKARD and evaluated it on large code bases written in C and Java including the Linux kernel and JDK. Our experiments show that DECKARD is both scalable and accurate. It is also language independent, applicable to any language with a formally specified grammar.
[Java, Euclidean distance metric, Cloning, trees (mathematics), subtrees, source code, Fingerprint recognition, tree representations, Application software, Deckard, Programming profession, code clones, Linux, Clustering algorithms, Euclidean distance, tree-based detection, Robustness, software engineering, Software engineering]
Very-Large Scale Code Clone Analysis and Visualization of Open Source Programs Using Distributed CCFinder: D-CCFinder
29th International Conference on Software Engineering
None
2007
The increasing performance-price ratio of computer hardware makes possible to explore a distributed approach at code clone analysis. This paper presents D-CCFinder, a distributed approach at large-scale code clone analysis. D-CCFinder has been implemented with 80 PC workstations in our student laboratory, and a vast collection of open source software with about 400 million lines in total has been analyzed with it in about 2 days. The result has been visualized as a scatter plot, which showed the presence of frequently used code as easy recognizable patterns. Also, D-CCFinder has been used to analyze a single software system against the whole collection in order to explore the presence of code imported from open source software.
[Visualization, public domain software, very-large scale code clone visualization, open source software, Laboratories, Cloning, Scattering, Distributed computing, Open source software, distributed CCFinder, recognizable patterns, very-large scale code clone analysis, systems analysis, Hardware, open source programs, code clone analysis, Performance analysis, Large-scale systems, Workstations]
Using Server Pages to Unify Clones in Web Applications: A Trade-Off Analysis
29th International Conference on Software Engineering
None
2007
Server page technique is commonly used for implementing Web application user interfaces. Server pages can represent many similar Web pages in a generic form. Yet our previous study revealed high rates of repetitions in Web applications, particularly in the user interfaces. Code duplication, commonly known as 'cloning', signals untapped opportunities to achieve simpler, smaller, more generic, and more maintainable Web applications. Using PHP server page technique, we conducted a case study to explore how far server page technique can be pushed to achieve clone-free Web applications. Our study suggests that clone unification using server pages affects system qualities (e.g., runtime performance) to an extent that may not be acceptable in many project situations. Our paper discusses the trade-offs we observed when applying server pages to unify clones in Web applications. We expect our findings to help in developing and validating complementary techniques that can unify clones without incurring such trade-offs.
[Computer interfaces, Productivity, Cloning, Application specific processors, PHP server page technique, HTML, user interfaces, Application software, code duplication, Computer science, Web application user interfaces, Web pages, User interfaces, Error correction, Internet, clone unification, trade-off analysis]
Automated Inference of Pointcuts in Aspect-Oriented Refactoring
29th International Conference on Software Engineering
None
2007
Software refactoring is the process of reorganizing the internal structure of code while preserving the external behavior. Aspect-Oriented Programming (AOP) provides new modularization of software systems by encapsulating crosscutting concerns. Based on these two techniques, aspect-oriented (AO) refactoring restructures crosscutting elements in code. AO refactoring includes two steps: aspect mining (identification of aspect candidates in code) and aspect refactoring (semantic-preserving transformation to migrate the aspect-candidate code to AO code). Aspect refactoring clusters similar join points together for the aspect candidates and encapsulates each cluster with an effective pointcut definition. With the increase in size of the code and crosscutting concerns, it is tedious to manually identify aspects and their corresponding join points, cluster the join points, and infer pointcut expressions. Therefore, there is a need to automate the process of AO refactoring. This paper proposes an automated approach that identifies aspect candidates in code and infers pointcut expressions for these aspects. Our approach mines for aspect candidates, identifies the join points for the aspect candidates, clusters the join points, and infers an effective pointcut expression for each cluster of join points. The approach also provides an additional testing mechanism to ensure that the inferred pointcut expressions are of correct strength. The empirical results show that our approach helps achieve a significant reduction in the total number of pointcut expressions to be used in the refactored code.
[Encapsulation, Software testing, Java, object-oriented programming, AOP, aspect mining, software systems, Scattering, data mining, Application software, inference mechanisms, software maintenance, aspect-oriented refactoring, Computer science, Computer languages, automated inference, pointcut expressions, Software systems, aspect-oriented programming, Functional programming, software refactoring, Software engineering]
A Formal Framework for Automated Round-Trip Software Engineering in Static Aspect Weaving and Transformations
29th International Conference on Software Engineering
None
2007
We present a formal framework for a recently introduced approach to automated round-trip software engineering (ARE) in source-level aspect weaving systems. Along with the formalization we improve the original method and suggest a new concept of weaving transactions in aspect-oriented programming (AOP). As the major contribution we formally show how, given a tree-shaped intermediate representation of a program and an ancillary transposition tree, manual edits in statically woven code can consistently be mapped back to their proper source of origin, which is either in the application core or in an element in the aspect space. The presented formalism is constructive. It frames AOP by generalizing static aspect weaving to classical tree transformations.
[codes, Automatic programming, static aspect weaving, Reverse engineering, trees (mathematics), statically woven code, tree-shaped intermediate representation, History, Programming environments, Information science, Tree graphs, Parallel programming, ancillary transposition tree, Automatic control, automated round-trip software engineering, aspect-oriented programming, Weaving, software engineering, formal framework, source-level aspect weaving systems, Software engineering]
Identifying Feature Interactions in Multi-Language Aspect-Oriented Frameworks
29th International Conference on Software Engineering
None
2007
The simultaneous use of multiple aspect languages has the potential of becoming a significant one, as new aspect- oriented frameworks are developed and existing ones expand to incorporate features of others. A key challenge in combining multiple aspect-oriented languages is identifying and resolving adverse feature interactions. These interactions occur due to the incompatible and inconsistent treatment of aspects, join points, and advice across different languages. In this paper, we analyze the root cause of this feature interaction problem. We classify common features of aspect languages, describe how these features may interact when using different aspect languages in tandem, and concretely illustrate how these interactions may be resolved. Our work allows AOP users and tool developers to reason about the occurrence of such adverse and unexpected feature interactions, and to apply several patterns for resolving these problems.
[Java, object-oriented programming, Shape, software design, Computer science, Software design, Computer displays, multi-language aspect-oriented frameworks, Concrete, software engineering, Performance analysis, feature interactions identification, Kernel, Software engineering, Testing]
Tracking Code Clones in Evolving Software
29th International Conference on Software Engineering
None
2007
Code clones are generally considered harmful in software development, and the predominant approach is to try to eliminate them through refactoring. However, recent research has provided evidence that it may not always be practical, feasible, or cost-effective to eliminate certain clone groups. We propose a technique for tracking clones in evolving software. Our technique relies on the concept of abstract clone region descriptors (CRD), which describe clone regions within methods in a robust way that is independent from the exact text of the clone region or its location in a file. We present our definition of CRDs, and describe a complete clone tracking system capable of producing CRDs from the output of a clone detection tool, notify developers of modifications to clone regions, and support the simultaneous editing of clone regions. We report on two experiments and a case study conducted to assess the performance and usefulness of our approach.
[Logic programming, source coding, software development, clone region descriptor, Q factor, Cloning, software maintenance, Programming profession, Computer science, Computer languages, clone detection tool, Writing, Software systems, Robustness, Monitoring, code clone tracking]
Do Maintainers Utilize Deployed Design Patterns Effectively?
29th International Conference on Software Engineering
None
2007
One claimed benefit of deploying design patterns is facilitating maintainers to perform anticipated changes. However, it is not at all obvious that the relevant design patterns deployed in software will invariably be utilized for the changes. Moreover, we observe that many well-known design patterns consist of three types of programming elements (called participants), and that performing an anticipated change typically entails multiple tasks related to different types of participants. This paper studies empirically whether maintainers utilize deployed design patterns, and when they do, which tasks they more commonly perform. Our experiments show that almost all subjects perform the task of adding new concrete participants, fewer perform the tasks involving clients, whereas even fewer perform the tasks involving abstract participants. Furthermore, utilizing deployed design patterns (by performing whichever of the corresponding tasks) is found to be statistically associated with the delivery of less faulty codes.
[Software maintenance, faulty code, object-oriented programming, faulty codes, Documentation, software maintenance, Open source software, software fault tolerance, design pattern, Computer science, Software design, Empirical study, design patterns, Software quality, Continuous production, Software systems, Concrete, Quality management]
OPIUM: Optimal Package Install/Uninstall Manager
29th International Conference on Software Engineering
None
2007
Linux distributions often include package management tools such as apt-get in Debian or yum in RedHat. Using information about package dependencies and conflicts, such tools can determine how to install a new package (and its dependencies) on a system of already installed packages. Using off-the-shelf SAT solvers, pseudo-boolean solvers, and Integer Linear Programming solvers, we have developed a new package-management tool, called Opium, that improves on current tools in two ways: (1) Opium is complete, in that if there is a solution, Opium is guaranteed to find it, and (2) Opium can optimize a user-provided objective function, which could for example state that smaller packages should be preferred over larger ones. We performed a comparative study of our tool against Debian's apt-get on 600 traces of real-world package installations. We show that Opium runs fast enough to be usable, and that its completeness and optimality guarantees provide concrete benefits to end users.
[package management tools, dynamic software linking, Linux distributions, pseudo-boolean solvers, Application software, Environmental economics, Opium, Packaging machines, configuration management, integer linear programming solvers, Software libraries, Software packages, Optimal Package Install/Uninstall Manager, Linux, Bandwidth, software packages, Integer linear programming, Economic forecasting, Joining processes, package dependencies]
Modeling Product Line Architectures through Change Sets and Relationships
29th International Conference on Software Engineering
None
2007
The essence of any modeling approach for product line architectures lies in its ability to express variability. Existing approaches do so by explicitly specifying variation points inside the architectural specification of the entire product line, usually with optional and alternative elements of some form. This, however, leads to a sizable mismatch between conceptual variability (i.e., the features through which architects logically view and interpret differences in product architectures) and actual variability (i.e., the modeling constructs through which the logical differences must be expressed). We contribute a new product line architecture modeling approach that unites the two. Our approach uses change sets to group related architectural differences and relationships to govern which change set combinations are valid when composed into a particular product architecture. The result lifts modeling of variability out of modeling architectural structure, consolidates related variation points, and explicitly and separately manages their compatibilities.
[Switches, change sets, architectural specification, formal specification, software architecture, Programmable logic arrays, Computer architecture, product development, software reusability, product line architectures, Informatics, change relationships, Software engineering]
On Accurate Automatic Verification of Publish-Subscribe Architectures
29th International Conference on Software Engineering
None
2007
The paper presents a novel approach based on Bogor for the accurate verification of applications based on Publish- Subscribe infrastructures. Previous efforts adopted standard model checking techniques to verify the application behavior, but they introduce strong simplifications on the underlying infrastructure to cope with the state space explosion problem and make automatic verification feasible. Instead of building on top of existing model checkers, our proposal embeds the asynchronous communication mechanisms of Publish-Subscribe infrastructures within Bogor. This way, Publish-Subscribe primitives become part of the specification language as additional, domain-specific, constructs. Accurate models become feasible without incurring in state space explosion problems, thus enabling the automated verification of applications on top of realistic communication infrastructures.
[Pervasive computing, asynchronous communication mechanisms, realistic communication infrastructures, program verification, Buildings, Explosions, State-space methods, Specification languages, Application software, Proposals, publish-subscribe architectures, Asynchronous communication, software architecture, Communication system software, publish-subscribe infrastructures, automatic verification, Publish-subscribe, specification languages, specification language, middleware]
Supporting Heterogeneous Architecture Descriptions in an Extensible Toolset
29th International Conference on Software Engineering
None
2007
Many architecture description languages (ADLs) have been proposed to model, analyze, configure, and deploy complex software systems. To face this diversity, extensible ADLs (or ADL interchange formats) have been proposed. These ADLs provide linguistic support for integrating various architectural aspects within the same description. Nevertheless, they do not support extensibility at the tool level, i.e. they do not provide an extensible toolset for processing ADL descriptions. In this paper, we present an extensible toolset for easing the development of architecture-based software systems. This toolset is not bound to a specific ADL, but rather uses a grammar description mechanism to accept various input languages, e.g. ADLs, interface definition languages (IDLs), domain specific languages (DSLs). Moreover, it can easily be extended to implement many different features, such as behavioral analysis, code generation, deployment, etc. Its extensibility is obtained by designing its core functionalities using fine-grained components that implement flexible design patterns. Experiments are presented to illustrate both the functionalities implemented by the toolset and the way it can be extended.
[ADL interchange formats, Java, architecture description languages, behavioral analysis, Fractals, Decoding, architecture-based software systems, Domain specific languages, software architecture, interface definition languages, Software architecture, extensible toolset, code generation, grammar description, Computer architecture, Software systems, complex software systems, domain specific languages, software tools, Architecture description languages, DSL, heterogeneous architecture descriptions, Graphical user interfaces]
Adaptive Online Program Analysis
29th International Conference on Software Engineering
None
2007
Analyzing a program run can provide important insights about its correctness. Dynamic analysis of complex correctness properties, however, usually results in significant run-time overhead and, consequently, it is rarely used in practice. In this paper, we present an approach for exploiting properties of stateful program specifications to reduce the cost of their dynamic analysis. With our approach, analysis results are guaranteed to be identical to those of a traditional expensive dynamic analyses, while analysis cost is very low - between 23% and 33% more than the un-instrumented program for the analyses we studied. We describe the principles behind our adaptive online program analysis technique, extentions to our Java run-time analysis framework that support such analyses, and report on the performance and capabilities of two different families of adaptive online program analyses.
[Performance evaluation, stateful program specifications, Java, Costs, Instruments, program diagnostics, Java run-time analysis, formal specification, Information analysis, adaptive online program analysis, Runtime, Failure analysis, Sampling methods, Performance analysis, Monitoring]
Exception-Chain Analysis: Revealing Exception Handling Architecture in Java Server Applications
29th International Conference on Software Engineering
None
2007
Although it is common in large Java programs to rethrow exceptions, existing exception-flow analyses find only single exception-flow links, thus are unable to identify multiple-link exception propagation paths. This paper presents a new static analysis that, when combined with previous exception-flow analyses, computes chains of semantically-related exception-flow links, and thus reports entire exception propagation paths, instead of just discrete segments of them. These chains can be used 1) to show the error handling architecture of a system, 2) to assess the vulnerability of a single component and the whole system, 3) to support better testing of error recovery code, and 4) to facilitate the tracing of the root cause of a logged problem. Empirical findings and a case history for Tomcat show that a significant portion of the chains found in our benchmarks span multiple components, and thus are hard to find manually.
[System testing, program testing, semantically-related exception-flow link, program tracing, static program analysis, History, program compilers, system recovery, exception-chain analysis, Information analysis, software architecture, file servers, Computer architecture, exception-flow analysis, Benchmark testing, Availability, Java, program diagnostics, Java server application, Application software, exception handling architecture, Programming profession, error recovery code testing, exception propagation path, program compiler, error handling, Software engineering]
Path-Sensitive Inference of Function Precedence Protocols
29th International Conference on Software Engineering
None
2007
Function precedence protocols define ordering relations among function calls in a program. In some instances, precedence protocols are well-understood (e.g., a call to pthread_mutex_init must always be present on all program paths before a call to pthread_mutex_lock). Oftentimes, however, these protocols are neither well- documented, nor easily derived. As a result, protocol violations can lead to subtle errors that are difficult to identify and correct. In this paper, we present CHRONICLER, a tool that applies scalable inter-procedural path-sensitive static analysis to automatically infer accurate function precedence protocols. Chronicler computes precedence relations based on a program's control-flow structure, integrates these relations into a repository, and analyzes them using sequence mining techniques to generate a collection of feasible precedence protocols. Deviations from these protocols found in the program are tagged as violations, and represent potential sources of bugs. We demonstrate CHRONICLER's effectiveness by deriving protocols for a collection of benchmarks ranging in size from 66 K to 2 M lines of code. Our results not only confirm the existence of bugs in these programs due to precedence protocol violations, but also highlight the importance of path sensitivity on accuracy and scalability.
[Protocols, Scalability, Programming, Data structures, interprocedural path-sensitive static analysis, path-sensitive inference, Computer science, Sockets, function precedence protocols, Computer bugs, systems analysis, control-flow structure, Automatic generation control, Libraries, Error correction, software tools, sequence mining techniques]
GoalDebug: A Spreadsheet Debugger for End Users
29th International Conference on Software Engineering
None
2007
We present a spreadsheet debugger targeted at end users. Whenever the computed output of a cell is incorrect, the user can supply an expected value for a cell, which is employed by the system to generate a list of change suggestions for formulas that, when applied, would result in the user-specified output. The change suggestions are ranked using a set of heuristics. In previous work, we had presented the system as a proof of concept. In this paper, we describe a systematic evaluation of the effectiveness of inferred change suggestions and the employed ranking heuristics. Based on the results of the evaluation we have extended both, the change inference process and the ranking of suggestions. An evaluation of the improved system shows that change inference process and the ranking heuristics have both been substantially improved and that the system performs effectively.
[Performance evaluation, Software testing, program debugging, spreadsheet debugger, Software debugging, Programming profession, Fault diagnosis, GoalDebug, end users, user-specified output, change inference process, Automatic testing, NIST, Error correction, ranking heuristics, user centred design]
A Technique for Enabling and Supporting Debugging of Field Failures
29th International Conference on Software Engineering
None
2007
It is difficult to fully assess the quality of software in- house, outside the actual time and context in which it will execute after deployment. As a result, it is common for software to manifest field failures, failures that occur on user machines due to untested behavior. Field failures are typically difficult to recreate and investigate on developer platforms, and existing techniques based on crash reporting provide only limited support for this task. In this paper, we present a technique for recording, reproducing, and minimizing failing executions that enables and supports in- house debugging of field failures. We also present a tool that implements our technique and an empirical study that evaluates the technique on a widely used e-mail client.
[Software testing, Software prototyping, program debugging, software development, field failures debugging, Educational institutions, Computer crashes, software quality, Application software, Software debugging, Quality assurance, Software quality, Pressing, software engineering, Software tools]
POLUS: A POwerful Live Updating System
29th International Conference on Software Engineering
None
2007
This paper presents POLUS, a software maintenance tool capable of iteratively evolving running software into newer versions. POLUS's primary goal is to increase the dependability of contemporary server software, which is frequently disrupted either by external attacks or by scheduled upgrades. To render POLUS both practical and powerful, we design and implement POLUS aiming to retain backward binary compatibility, support for multithreaded software and recover already tainted state of running software, yet with good usability and very low runtime overhead. To demonstrate the applicability of POLUS, we report our experience in using POLUS to dynamically update three prevalent server applications: vsftpd, sshd and apache HTTP server. Performance measurements show that POLUS incurs negligible runtime overhead: a less than 1% performance degradation (but 5% for one case). The time to apply an update is also minimal.
[Measurement, Software maintenance, iteratively evolving running software, backward binary compatibility, POLUS, contemporary server software, sshd, Application software, software maintenance, powerful live updating system, Degradation, Runtime, security of data, Computer bugs, apache HTTP server, system monitoring, software maintenance tool, Web server, Software tools, Usability, Software engineering, vsftpd]
Supporting Generic Sketching-Based Input of Diagrams in a Domain-Specific Visual Language Meta-Tool
29th International Conference on Software Engineering
None
2007
Software engineers often use hand-drawn diagrams as preliminary design artefacts and as annotations during reviews. We describe the addition of sketching support to a domain-specific visual language meta-tool enabling a wide range of diagram-based design tools to leverage this human-centric interaction support. Our approach allows visual design tools generated from high-level specifications to incorporate a range of sketching-based functionality including both eager and lazy recognition, moving from sketch to formalized content and back and using sketches for secondary annotation and collaborative design review. We illustrate the use of our sketching extension for an example domain-specific visual design tool and describe the architecture and implementation of the extension as a plug-in for our Eclipse-based meta-tool.
[Collaborative tools, Unified modeling language, domain-specific visual language meta-tool, Application software, generic sketching-based input, Design engineering, Software design, high-level specifications, diagram-based design tools, data visualisation, Eclipse-based meta-tool, Computer architecture, User interfaces, Collaborative work, software engineering, collaborative design, Software tools, Software engineering]
Fixing Inconsistencies in UML Design Models
29th International Conference on Software Engineering
None
2007
Changes are inevitable during software development and so are their unintentional side effects. The focus of this paper is on UML design models, where unintentional side effects lead to inconsistencies. We demonstrate that a tool can assist the designer in discovering unintentional side effects, locating choices for fixing inconsistencies, and then in changing the design model. Our techniques are "on-line, " applied as the designer works, and non-intrusive, without overwhelming the designer. This is a significant improvement over the state-of-the-art. Our tool is fully integrated with the design tool IBM Rational Rosetrade. It was empirically evaluated on 48 case studies.
[UML design models, Unified Modeling Language, software development, Scalability, Unified modeling language, Programming, Displays, Power system modeling, Best practices, Computer bugs, systems analysis, Streaming media, Motion pictures, software engineering, Software engineering, IBM Rational Rose]
The Factory Pattern in API Design: A Usability Evaluation
29th International Conference on Software Engineering
None
2007
The usability of software APIs is an important and infrequently researched topic. A user study comparing the usability of the factory pattern and constructors in API designs found highly significant results indicating that factories are detrimental to API usability in several varied situations. The results showed that users require significantly more time (p = 0.005) to construct an object with a factory than with a constructor while performing both context-sensitive and context- free tasks. These results suggest that the use of factories can and should be avoided in many cases where other techniques, such as constructors or class clusters, can be used instead.
[Encapsulation, Protocols, object-oriented programming, application program interfaces, Unified modeling language, Production facilities, API design, Application software, Programming profession, factory pattern, context-free task, Software libraries, Handheld computers, application program interface, Writing, context-sensitive task, software usability evaluation, Usability]
Overview and Evaluation of Constraint Validation Approaches in Java
29th International Conference on Software Engineering
None
2007
Integrity is a dependability attribute partially ensured through runtime validation of integrity constraints. A wide range of different constraint validation approaches exists-ranging from simple if conditions over explicit constraint validation methods and contract specifications to constraints as first class runtime entities of an application. However, increased support for explicitness and flexibility often comes at the price of increased performance costs. To address this issue, we contribute with an overview and evaluation of different constraint validation approaches for the Java programming language with respect to implementation, maintainability and performance. Our results show that the benefits of some of the more advanced approaches are certainly worth their costs by introducing a runtime overhead of only two to ten times the runtime of the fastest approach while other approaches introduce runtime overheads of more than 100, which might be simply too slow in certain applications.
[Java, Costs, program verification, Unified modeling language, Buildings, runtime validation, Java programming language, data integrity, System analysis and design, Information systems, Computer languages, integrity constraints, Runtime, constraint validation approaches, Logic, Contracts]
Ownership and Immutability Inference for UML-Based Object Access Control
29th International Conference on Software Engineering
None
2007
We propose a mechanism for object access control which is based on the UML. Specifically, we propose use of ownership and immutability constraints on UML associations and verification of these constraints through reverse engineering. These constraints inherently support software design principles, and impose requirements on the implementation that may help prevent serious program flaws. We propose implementation-level models for ownership and immutability that capture well the meaning of these concepts in design, and we develop novel static ownership and immutability inference analyses. We perform an empirical investigation on several small-to-large Java programs. The results indicate that the inference analyses are precise and practical. Therefore, the analyses can be integrated in reverse engineering tools and can help support effective reasoning about software quality and security.
[Access control, Java, Unified Modeling Language, Unified modeling language, Reverse engineering, reverse engineering, software quality, Security, Computer science, object access control, immutability constraints, software design principles, Software design, UML, Software quality, Marketing and sales, software engineering, immutability inference, software security, Software tools, Java programs]
Automatic Inference of Structural Changes for Matching across Program Versions
29th International Conference on Software Engineering
None
2007
Mapping code elements in one version of a program to corresponding code elements in another version is a fundamental building block for many software engineering tools. Existing tools that match code elements or identify structural changes - refactorings and API changes - between two versions of a program have two limitations that we overcome. First, existing tools cannot easily disambiguate among many potential matches or refactoring candidates. Second, it is difficult to use these tools' results for various software engineering tasks due to an unstructured representation of results. To overcome these limitations, our approach represents structural changes as a set of high-level change rules, automatically infers likely change rules and determines method-level matches based on the rules. By applying our tool to several open source projects, we show that our tool identifies matches that are difficult to find using other approaches and produces more concise results than other approaches. Our representation can serve as a better basis for other software engineering tools.
[codes, open source projects, Natural languages, automatic inference, structural changes, software engineering tools, Programming profession, Information analysis, Computer science, Software packages, Packaging, Rendering (computer graphics), program versions, code elements, software tools, Software tools, Software engineering, Testing]
Information Needs in Collocated Software Development Teams
29th International Conference on Software Engineering
None
2007
Previous research has documented the fragmented nature of software development work. To explain this in more detail, we analyzed software developers' day-to-day information needs. We observed seventeen developers at a large software company and transcribed their activities in go-minute sessions. We analyzed these logs for the information that developers sought, the sources that they used, and the situations th at prevented inform action from being acquired. We identified twenty-one information types and cataloged the outcome and source when each type of information was sought. The most frequently sought information included awareness about artifacts and coworkers. The most often deferred searches included knowledge about design and program behavior, such as why code was written a particular way, what a program was supposed to do, and the cause of a program state. Developers often had to defer tasks because the only source of knowledge was unavailable coworkers.
[Content management, program behavior, Cloning, Switches, Programming, File servers, Communication switching, Information analysis, systems analysis, Writing, collocated software development, software engineering, Books, information needs, Software engineering]
The Social Dynamics of Pair Programming
29th International Conference on Software Engineering
None
2007
This paper presents data from a four month ethnographic study of professional pair programmers from two software development teams. Contrary to the current conception of pair programmers, the pairs in this study did not hew to the separate roles of "driver" and "navigator". Instead, the observed programmers moved together through different phases of the task, considering and discussing issues at the same strategic "range " or level of abstraction and in largely the same role. This form of interaction was reinforced by frequent switches in keyboard control during pairing and the use of dual keyboards. The distribution of expertise among the members of a pair had a strong influence on the tenor of pair programming interaction. Keyboard control had a consistent secondary effect on decisionmaking within the pair. These findings have implications for software development managers and practitioners as well as for the design of software development tools.
[social aspects of automation, Navigation, software development, keyboard control, software development management, Data engineering, Programming profession, Software development management, Human computer interaction, Computer science, Technology management, pair programming, Engineering management, Keyboards, decision making, Dynamic programming, social dynamics]
Role Migration and Advancement Processes in OSSD Projects: A Comparative Case Study
29th International Conference on Software Engineering
None
2007
Socio-technical processes have come to the forefront of recent analysis of the open source software development (OSSD) world. Interest in making these processes explicit is mounting, from industry and the software process community, as well as among those who may become contributors to OSSD organization. This paper serves to close this gap by providing an empirical analysis of the role migration and project career advancement process, and role-sets within, that we have observed through comparative case studies within three large OSSD project organizations: Mozilla.org, Apache.org, and NetBeans.org.
[open source software development, Engineering profession, project career advancement process, public domain software, software development management, socio-technical process, Opportunistic software systems development, Programming, Nonhomogeneous media, Open source software, Information analysis, Recruitment, Engineering management, software process community, OSSD project organization, Computer industry, Software engineering]
The Role of Experience and Ability in Comprehension Tasks Supported by UML Stereotypes
29th International Conference on Software Engineering
None
2007
Proponents of design notations tailored for specific application domains or reference architectures, often available in the form of UML stereotypes, motivate them by improved understandability and modifiability. However, empirical studies that tested such claims report contradictory results, where the most intuitive notations are not always the best performing ones. This indicates the possible existence of relevant influencing factors, other than the design notation itself. In this work we report the results of a family of three experiments performed at different locations and with different subjects, in which we assessed the effectiveness of UML stereotypes for Web design in support to comprehension tasks. Replications with different subjects allowed us to investigate whether subjects' ability and experience play any role in the comprehension of stereotyped diagrams. We observed different behaviors of users with different degrees of ability and experience, which suggests alternative comprehension strategies of (and tool support for) different categories of users.
[Performance evaluation, program comprehension, Information resources, Navigation, program testing, Unified Modeling Language, understandability, developers' experience, Unified modeling language, reference architectures, empirical study, Code standards, software architecture, Web design, modifiability, Internet, UML stereotypes, Impedance, Testing, Software engineering]
Information Hiding and Visibility in Interface Specifications
29th International Conference on Software Engineering
None
2007
Information hiding controls which parts of a class are visible to non-privileged and privileged clients (e.g., subclasses). This affects detailed design specifications in two ways. First, specifications should not expose hidden class members. As noted in previous work, this is important because such hidden members are not meaningful to all clients. But it also allows changes to hidden implementation details without invalidating correctness proofs for client code, which is important for maintaining verified programs. Second, to enable sound modular reasoning, certain specifications must be visible to clients. We present rules for information hiding in specifications for Java-like languages, and demonstrate their application to the specification language JML. These rules restrict proof obligations to only mention visible class members, but retain soundness. This allows maintenance of implementations and their specifications without affecting client reasoning.
[Java, program verification, information hiding, Documentation, Specification languages, Formal specifications, software maintenance, interface specification, sound modular reasoning, formal specification, Java-like language, Upper bound, program maintenance, information visibility, Packaging, non privileged client, reasoning about programs, Protection, data encapsulation, Contracts, Testing, Software engineering]
Using GUI Run-Time State as Feedback to Generate Test Cases
29th International Conference on Software Engineering
None
2007
This paper presents a new automated model-driven technique to generate test cases by using feedback from the execution of a "seed test suite" on an application under test (AUT). The test cases in the seed suite are designed to be generated automatically and executed very quickly. During their execution, feedback obtained from the AUT's run-time state is used to generate new, "improved" test cases. The new test cases subsequently become part of the seed suite. This "anytime technique" continues iteratively, generating and executing additional test cases until resources are exhausted or testing goals have been met. The feedback-based technique is demonstrated for automated testing of graphical user interfaces (GUIs). An existing abstract model of the GUI is used to automatically generate the seed test suite. It is executed; during its execution, state changes in the GUI pinpoint important relationships between GUI events, which evolve the model and help to generate new test cases. Together with a reverse- engineering algorithm used to obtain the initial model and seed suite, the feedback-based technique yields a fully automatic, end-to-end GUI testing process. A feasibility study on four large fielded open-source software (OSS) applications demonstrates that this process is able to significantly improve existing techniques and help identify/report serious problems in the OSS. In response, these problems have been fixed by the developers of the OSS in subsequent versions.
[Software testing, feedback-based technique, State feedback, GUI run-time state, Costs, program testing, graphical user interfaces, public domain software, Educational institutions, reverse engineering, Application software, Open source software, Computer science, feedback, model-driven technique, Runtime, open-source software, Automatic testing, seed test suite, reverse-engineering algorithm, automated test case generation, Graphical user interfaces, application under test]
Automated Generation of Context-Aware Tests
29th International Conference on Software Engineering
None
2007
The incorporation of context-awareness capabilities into pervasive applications allows them to leverage contextual information to provide additional services while maintaining an acceptable quality of service. These added capabilities, however, introduce a distinct input space that can affect the behavior of these applications at any point during their execution, making their validation quite challenging. In this paper, we introduce an approach to improve the test suite of a context-aware application by identifying context-aware program points where context changes may affect the application's behavior, and by systematically manipulating the context data fed into the application to increase its exposure to potentially valuable context variations. Preliminary results indicate that the approach is more powerful than existing testing approaches used on this type of application.
[Context-aware services, Performance evaluation, System testing, program testing, program verification, Maintenance engineering, quality of service, Application software, contextual information, Middleware, Logic testing, Computer science, Automatic testing, valuable context variations, Feeds, context-aware tests, automated generation]
Hybrid Concolic Testing
29th International Conference on Software Engineering
None
2007
We present hybrid concolic testing, an algorithm that interleaves random testing with concolic execution to obtain both a deep and a wide exploration of program state space. Our algorithm generates test inputs automatically by interleaving random testing until saturation with bounded exhaustive symbolic exploration of program points. It thus combines the ability of random search to reach deep program states quickly together with the ability of concolic testing to explore states in a neighborhood exhaustively. We have implemented our algorithm on top of CUTE and applied it to obtain better branch coverage for an editor implementation (VIM 5.7, 150 K lines of code) as well as a data structure implementation in C. Our experiments suggest that hybrid concolic testing can handle large programs and provide, for the same testing budget, almost 4times the branch coverage than random testing and almost 2times that of concolic testing.
[Software testing, Performance evaluation, program debugging, hybrid concolic testing, program testing, Debugging, interleaving random testing, Data structures, concolic testing., State-space methods, directed random testing, random search, program state space, Automatic testing, Computer bugs, Interleaved codes, debugging, Concrete, CUTE, search problems, Software engineering]
Refactoring-Aware Configuration Management for Object-Oriented Programs
29th International Conference on Software Engineering
None
2007
Current text based software configuration management (SCM) systems have trouble with refactorings. Refactorings result in global changes and lead to merge conflicts. A refactoring-aware SCM system reduces merge conflicts, preserves program history better and makes it easier to understand program evolution. This paper describes MolhadoRef a refactoring-aware SCM system and the merge algorithm at its core. MolhadoRef records change operations (refactorings and edits) used to produce one version, and replays them when merging versions. Since refactorings are change operations with well defined semantics, MolhadoRef treats them intelligently. A case-study shows that MolhadoRef solves automatically more merge conflicts than CVS while resulting in fewer merge errors.
[MolhadoRef, Java, object-oriented programming, Merging, Software algorithms, Scattering, Manuals, History, Programming profession, software configuration management, refactoring-aware configuration management, Runtime, object-oriented programs, software engineering, Software engineering]
Refactoring for Parameterizing Java Classes
29th International Conference on Software Engineering
None
2007
Type safety and expressiveness of many existing Java libraries and their client applications would improve, if the libraries were upgraded to define generic classes. Efficient and accurate tools exist to assist client applications to use generic libraries, but so far the libraries themselves must be parameterized manually, which is a tedious, time-consuming, and error-prone task. We present a type- constraint-based algorithm for converting non-generic libraries to add type parameters. The algorithm handles the full Java language and preserves backward compatibility, thus making it safe for existing clients. Among other features, it is capable of inferring wildcard types and introducing type parameters for mutually-dependent classes. We have implemented the algorithm as a fully automatic refactoring in Eclipse. We evaluated our work in two ways. First, our tool parameterized code that was lacking type parameters. We contacted the developers of several of these applications, and in all cases they confirmed that the resulting parameterizations were correct and useful. Second, to better quantify its effectiveness, our tool parameterized classes from already-generic libraries, and we compared the results to those that were created by the libraries' authors. Our tool performed the refactoring accurately-in 87% of cases the results were as good as those created manually by a human expert, in 9% of cases the tool results were better, and in 4% of cases the tool results were worse.
[Java, generic libraries, parameterizing Java classes, Humans, constraint-based algorithm, Eclipse, type theory, Programming profession, software libraries, Java libraries, Computer languages, type safety, Java language, Libraries, Safety]
Supporting the Investigation and Planning of Pragmatic Reuse Tasks
29th International Conference on Software Engineering
None
2007
Software reuse has long been promoted as a means to increase developer productivity; however, reusing source code is difficult in practice and tends to be performed in an ad hoc manner. This is problematic because poor decisions can be made either to attempt an unwise, overly complex reuse task, or to avoid a reuse task that would have saved time and effort. This paper describes a lightweight tool that supports the investigation and planning of pragmatic reuse tasks. The tool helps developers to identify the dependencies from the source code they wish to reuse, and to decide how to deal with those dependencies. Questions about pragmatic reuse are evaluated through a survey of industrial developers. The tool is evaluated through the planning and execution of reuse tasks by industrial developers.
[Productivity, Performance evaluation, Navigation, software reuse, Laboratories, complex reuse task, Software performance, Displays, pragmatic reuse tasks, Global Positioning System, Computer science, Data visualization, software reusability, Computer industry, industrial developers]
Mining Security-Sensitive Operations in Legacy Code Using Concept Analysis
29th International Conference on Software Engineering
None
2007
This paper presents an approach to statically retrofit legacy servers with mechanisms for authorization policy enforcement. The approach is based upon the observation that security-sensitive operations performed by a server are characterized by idiomatic resource manipulations, called fingerprints. Candidate fingerprints are automatically mined by clustering resource manipulations using concept analysis. These fingerprints are then used to identify security-sensitive operations performed by the server. Case studies with three real-world servers show that the approach can be used to identify security-sensitive operations with a few hours of manual effort and modest domain knowledge.
[Access control, legacy code, program diagnostics, Lattices, data mining, Manuals, safety-critical software, Fingerprint recognition, static analysis, File servers, software maintenance, Authorization, candidate fingerprint, security-sensitive operation mining, Linux, pattern clustering, authorisation, Software systems, concept analysis, idiomatic resource manipulation, Resource management, Pattern analysis, authorization policy enforcement]
Managing Impacts of Security Protocol Changes in Service-Oriented Applications
29th International Conference on Software Engineering
None
2007
We present a software tool and a framework for security protocol change management. While we focus on trust negotiation protocols in this paper, many of the ideas are generally applicable to other types of protocols. Trust negotiation is a flexible approach to access control that is well suited to dynamic environments typical of service-oriented applications. However, managing the evolution of trust negotiation protocols is a difficult problem that has not been sufficiently addressed, especially in situations where there are ongoing negotiations. By using our framework, the consequences of changing the protocol that applies to ongoing trust negotiations can be automatically determined. We have also implemented a database-backed GUI tool to manage the change process as an extension of an existing system, and we have performed experiments to test the efficiency of our management software. Our experimental results show that the techniques proposed can scale to applications with tens of thousands of simultaneous users even on commodity PCs.
[Access control, Performance evaluation, Software testing, database-backed GUI tool, System testing, software development management, Access protocols, Security, Application software, security protocol change management, software architecture, software tool, Databases, security of data, management of change, software tools, protocols, trust negotiation protocols, Software tools, service-oriented applications, Graphical user interfaces]
When Role Models Have Flaws: Static Validation of Enterprise Security Policies
29th International Conference on Software Engineering
None
2007
Modern multiuser software systems have adopted role-based access control (RBAC) for authorization management. This paper presents a formal model for RBAC policy validation and a static-analysis model for RBAC systems that can be used to (i) identify the roles required by users to execute an enterprise application, (ii) detect potential inconsistencies caused by principal-delegation policies, which are used to override a user's role assignment, (Hi) report if the roles assigned to a user by a given policy are redundant or insufficient, and (iv) report vulnerabilities that can result from unchecked intra-component accesses. The algorithms described in this paper have been implemented as part of IBM's enterprise security policy evaluator (ESPE) tool. Experimental results show that the tool found numerous policy flaws, including ten previously unknown flaws from two production-level applications, with no false-positive reports.
[Access control, Java, program verification, enterprise security policies, program diagnostics, authorization management, Application software, multiuser software systems, Authorization, Runtime, adopted role-based access control, IBM Enterprise Security Policy Evaluator tool, static validation, Information security, authorisation, Permission, Software systems, Database systems, principal-delegation policies, Protection]
Predicting Faults from Cached History
29th International Conference on Software Engineering
None
2007
We analyze the version history of 7 software systems to predict the most fault prone entities and files. The basic assumption is that faults do not occur in isolation, but rather in bursts of several related faults. Therefore, we cache locations that are likely to have faults: starting from the location of a known (fixed) fault, we cache the location itself, any locations changed together with the fault, recently added locations, and recently changed locations. By consulting the cache at the moment a fault is fixed, a developer can detect likely fault-prone locations. This is useful for prioritizing verification and validation resources on the most fault prone files or entities. In our evaluation of seven open source projects with more than 200,000 revisions, the cache selects 10% of the source code files; these files account for 73%-95% of faults - a significant advance beyond the state of the art.
[Software algorithms, Switches, fault-prone location prediction, cache storage, History, resource validation, Open source software, software fault tolerance, Fault diagnosis, cache history, 7 software system version history, Fault detection, Software quality, resource verification, Isolation technology, Prediction algorithms, Software systems]
Detection of Duplicate Defect Reports Using Natural Language Processing
29th International Conference on Software Engineering
None
2007
Defect reports are generated from various testing and development activities in software engineering. Sometimes two reports are submitted that describe the same problem, leading to duplicate reports. These reports are mostly written in structured natural language, and as such, it is hard to compare two reports for similarity with formal methods. In order to identify duplicates, we investigate using natural language processing (NLP) techniques to support the identification. A prototype tool is developed and evaluated in a case study analyzing defect reports at Sony Ericsson mobile communications. The evaluation shows that about 2/3 of the duplicates can possibly be found using the NLP techniques. Different variants of the techniques provide only minor result differences, indicating a robust technology. User testing shows that the overall attitude towards the technique is positive and that it has a growth potential.
[Software testing, formal method, Vocabulary, program testing, natural language processing, software prototyping, software testing, Natural languages, user testing, prototype tool, Mobile communication, Relays, duplicate defect report detection, Prototypes, Failure analysis, Natural language processing, Robustness, software engineering, Sony Ericsson Mobile Communication, Software engineering]
An Empirical Study of the Evolution of an Agile-Developed Software System
29th International Conference on Software Engineering
None
2007
We have analyzed evolution patterns over two and a half years for a system developed using extreme programming. We find that the system shows a smooth pattern of growth overall, that (McCabe) code complexity is low, and that the relative amount of complexity control work (e.g. refactoring) is higher than in other systems we have studied. To interpret these results, we have drawn on qualitative data including the results of an observational study, records of progress and productivity, and comments on our findings from team members.
[Productivity, agile-developed software system, Documentation, Control systems, qualitative data, Personnel, code complexity, extreme programming, Genetic programming, Software systems, Collaborative work, software engineering, Large-scale systems, Software measurement, Pattern analysis]
Agility and Experimentation: Practical Techniques for Resolving Architectural Tradeoffs
29th International Conference on Software Engineering
None
2007
This paper outlines our experiences with making architectural tradeoffs between performance, availability, security, and usability, in light of stringent cost and time-to-market constraints, in an industrial web-conferencing system. We highlight the difficulties in anticipating future architectural requirements and tradeoffs and the value of using agility and experiments as a tool for mitigating architectural risks in situations when up front pen- and-paper analysis is simply impossible.
[Availability, Costs, Time to market, Control systems, architectural requirements, Security, time-to-market constraints, architectural risks, Delay, time to market, software architecture, Web-conferencing system, Space technology, architectural tradeoffs, Collaboration, groupware, Usability, Software engineering]
Usability Implications of Requiring Parameters in Objects' Constructors
29th International Conference on Software Engineering
None
2007
The usability of APIs is increasingly important to programmer productivity. Based on experience with usability studies of specific APIs, techniques were explored for studying the usability of design choices common to many APIs. A comparative study was performed to assess how professional programmers use APIs with required parameters in objects' constructors as opposed to parameterless "default" constructors. It was hypothesized that required parameters would create more usable and self- documenting APIs by guiding programmers toward the correct use of objects and preventing errors. However, in the study, it was found that, contrary to expectations, programmers strongly preferred and were more effective with APIs that did not require constructor parameters. Participants' behavior was analyzed using the cognitive dimensions framework, and revealing that required constructor parameters interfere with common learning strategies, causing undesirable premature commitment<sub>.</sub>
[Productivity, Software testing, Java, learning strategies, application program interfaces, Debugging, object constructors, inference mechanisms, Programming profession, Reactive power, Computer languages, API, Error correction, Usability, programmer productivity, error prevention, cognitive dimensions framework, Software engineering]
Performance Evaluation and Prediction for Legacy Information Systems
29th International Conference on Software Engineering
None
2007
Database-centric information systems are critical to the operations of large organisations. In particular, they often process a large amount of data with stringent performance requirements. Currently, however, there is a lack of systematic approaches to evaluating and predicting their performance when they are subject to an exorbitant growth of workload. In this paper, we introduce such a systematic approach that combines benchmarking, production system monitoring, and performance modelling (BMM) to address this issue. The approach helps the performance analyst to understand the system's operating environment and quantify its performance characteristics under varying load conditions via monitoring and benchmarking. Based on such realistic measurements, modelling techniques are used to predict the system performance. Our experience of applying BMM to a real-world system demonstrates the capability of BMM in predicting the performance of existing and enhanced software architectures in planning for its capacity growth.
[Production systems, enhanced software architectures, database-centric information systems, BMM, Software performance, Predictive models, performance evaluation, legacy information systems, database management systems, production system monitoring, Information systems, software architecture, Databases, System performance, Hardware, Performance analysis, Australia, performance modelling, Capacity planning]
Software Development Environments for Scientific and Engineering Software: A Series of Case Studies
29th International Conference on Software Engineering
None
2007
The need for high performance computing applications for computational science and engineering projects is growing rapidly, yet there have been few detailed studies of the software engineering process used for these applications. The DARPA High Productivity Computing Systems Program has sponsored a series of case studies of representative computational science and engineering projects to identify the steps involved in developing such applications (i.e. the life cycle, the workflows, technical challenges, and organizational challenges). Secondary goals were to characterize tool usage and identify enhancements that would increase the programmers' productivity. Finally, these studies were designed to develop a set of lessons learned that can be transferred to the general computational science and engineering community to improve the software engineering process used for their applications. Nine lessons learned from five representative projects are presented, along with their software engineering implications, to provide insight into the software development environments in this domain.
[Productivity, software development, Software performance, Programming, Supercomputers, Application software, Sun, Computer science, Acoustical engineering, High performance computing, software engineering, DARPA High Productivity Computing Systems Program, Software engineering]
Company-Wide Implementation of Metrics for Early Software Fault Detection
29th International Conference on Software Engineering
None
2007
To shorten time-to-market and improve customer satisfaction, software development companies commonly want to use metrics for assessing and improving the performance of their development projects. This paper describes a measurement concept for assessing how good an organization is at finding faults when most cost-effective, i. e. in most cases early. The paper provides results and lessons learned from applying the measurement concept widely at a large software development company. A major finding was that on average, 64 percent of all faults found would have been more cost effective to find during unit tests. An in-depth study of a few projects at a development unit also demonstrated how to use the measurement concept for identifying which parts in the fault detection process that needs to be improved to become more efficient (e.g. reduce the amount of time spent on rework).
[Costs, time-to-market, Time to market, DP industry, software development management, Software performance, Programming, company-wide implementation, software quality, software fault tolerance, Fault diagnosis, software development companies, Quality assurance, Fault detection, customer satisfaction, Customer satisfaction, early software fault detection, measurement concept, Software measurement, Testing, software metrics]
Applying Template Meta-Programming Techniques for a Domain-Specific Visual Language--An Industrial Experience Report
29th International Conference on Software Engineering
None
2007
Template meta-programming techniques can be used to increase efficiency in software development. These techniques have traditionally been used with textual programming languages, such as C++. In this paper, we discuss how corresponding techniques can be used with visual languages. The visual language under study in this paper is function block language (FBL). FBL is used in Metso Automation for writing automation control programs that are executed in realtime distributed environments. Efficient development of high quality programs and easy customizability of existing programs are key requirements in practical customer projects. These requirements have been one of the main motivations to develop template meta-programming support in FBL discussed. In this paper, we focus both on the technical aspects and on the lessons learnt from programmers' experiences and ways to work with templates. FBL and the programming techniques proposed have been used in hundreds of real-world projects at Metso Automation.
[automation control programs, Metso Automation, Automation, Automatic programming, function block language, Application software, visual languages, Programming profession, Computer languages, metacomputing, domain-specific visual language, Automatic control, Writing, Computer industry, Software systems, template meta-programming techniques, control engineering computing, visual programming, Software engineering]
Model-Based Security Engineering of Distributed Information Systems Using UMLsec
29th International Conference on Software Engineering
None
2007
Given the explosive growth of digitally stored information in modern enterprises, distributed information systems together with search engines are increasingly used in companies. By enabling the user to search all relevant information sources with one single query, however, crucial risks concerning information security arise. In order to make these applications secure, it is not sufficient to penetrate- and-patch past system development, but security analysis has to be an integral part of the system design process for such distributed information systems. This work presents the experiences and results of the security analysis of a search engine in the intranet of a German car manufacturer, by making use of an approach to model-based security engineering that is based on the UML extension UMLsec. The focus lies on the application's single-sign-on-mechanism, which was analyzed using the UMLsec method and tools. Main results of the paper include afield report on the employment of the UMLsec method in an industrial context as well as indications on its benefits and limitations.
[search engines, Unified Modeling Language, model-based security engineering, Unified modeling language, Distributed information systems, Companies, distributed processing, System analysis and design, Information analysis, security of data, Employment, Information security, Search engines, Explosives, UML extension UMLsec, Virtual manufacturing, distributed information systems]
Reconceptualizing a Family of Heterogeneous Embedded Systems via Explicit Architectural Support
29th International Conference on Software Engineering
None
2007
It has been widely advocated that software architecture provides an effective set of abstractions for engineering (families of) complex software systems. However, architectural concepts are seldom supported directly at the level of system implementation. In embedded environments in particular, developers are often forced to rely on low-level programming languages. While this is conducive to fine-grain control over the system, it does not lend itself to addressing larger issues such as ensuring architectural integrity or managing an application family. In this paper we describe our experience with fundamentally altering the manner in which a family of embedded applications is designed, analyzed, implemented, deployed, and evolved using explicit architectural constructs. We discuss our strategy, the challenges we faced in the course of our project, the lessons learned in the process, and several open issues that remain unresolved.
[fine-grain control, complex software system engineering, Peer to peer computing, Programming, low-level programming language, explicit software architectural support, Application software, programming languages, Middleware, heterogeneous embedded system reconceptualization, Connectors, Wireless sensor networks, software architecture, Software architecture, Embedded system, embedded systems, Software systems, Software engineering]
'Good' Organisational Reasons for 'Bad' Software Testing: An Ethnographic Study of Testing in a Small Software Company
29th International Conference on Software Engineering
None
2007
In this paper we report on an ethnographic study of a small software house to discuss the practical work of software testing. Through use of two rich descriptions, we discuss that 'rigour' in systems integration testing necessarily has to be organisationally defined. Getting requirements 'right', defining 'good' test scenarios and ensuring 'proper' test coverage are activities that need to be pragmatically achieved taking account of organisational realities and constraints such as: the dynamics of customer relationships; using limited effort in an effective way; timing software releases; and creating a market. We discuss how these organisational realities shape (1) requirements testing; (2) test coverage; (3) test automation; and (4) test scenario design.
[Software testing, System testing, requirement testing, Design automation, Shape, program testing, software testing, customer relationship, software company, test scenario design, Best practices, Computer science, Automatic testing, software house, software houses, ethnographic study, Computer industry, test coverage, Timing, system integration testing, Software engineering]
Enhancing Software Testing by Judicious Use of Code Coverage Information
29th International Conference on Software Engineering
None
2007
Recently, tools for the analysis and visualization of code coverage have become widely available. At first glance, their value in assessing and improving the quality of automated test suites seems to be obvious. Yet, experimental studies as well as experience from projects in industry indicate that their use is not without pitfalls. We found these tools in a number of recent projects quite beneficial. Therefore, we set out to gather code coverage information from one of these projects. In this experience report, first the system under scrutiny as well as our methodology is described. Then, four major questions concerning the impact and benefits of using these tools are discussed. Furthermore, a list of ten lessons learned is derived. The list may help developers judiciously use code coverage tools, in order to reap a maximum of benefits.
[Software testing, automated test quality, code coverage information, program testing, software testing, Data mining, Information analysis, Quality assurance, Automatic testing, Data visualization, systems analysis, software tools, code coverage tools, Software engineering]
Randomized Differential Testing as a Prelude to Formal Verification
29th International Conference on Software Engineering
None
2007
Most flight software testing at the Jet Propulsion Laboratory relies on the use of hand-produced test scenarios and is executed on systems as similar as possible to actual mission hardware. We report on a flight software development effort incorporating large-scale (biased) randomized testing on commodity desktop hardware. The results show that use of a reference implementation, hardware simulation with fault injection, a testable design, and test minimization enabled a high degree of automation in fault detection and correction. Our experience will be of particular interest to developers working in domains where on-time delivery of software is critical (a strong argument for randomized automated testing) but not at the expense of correctness and reliability (a strong argument for model checking, theorem proving, and other heavyweight techniques). The effort spent in randomized testing can prepare the way for generating more complete confidence using heavyweight techniques.
[Software testing, Jet Propulsion Laboratory, System testing, Design automation, program testing, Laboratories, Programming, flight software testing, formal verification, Automatic testing, randomized differential testing, Propulsion, aerospace computing, flight software development, Hardware, software engineering, Large-scale systems, Formal verification]
Can Requirements Be Creative? Experiences with an Enhanced Air Space Management System
29th International Conference on Software Engineering
None
2007
Requirements engineering is a creative process in which stakeholders work together to create ideas for new software systems that are eventually expressed as requirements. This paper reports a workshop that integrated creativity techniques with different types of use case and system context modeling to discover stakeholder requirements for EASM, a future air space management software system to enable the more effective, longer-term planning of UK and European airspace use. The workshop was successful in that it provided a range of outputs that were later assessed for their novelty and usefulness in the final specification of the EASM software. The paper describes the workshop structure, gives examples of outputs from it, and uses these results to answer 2 research questions about the utility of creativity techniques and workshops that had not been answered in previous research.
[Europe, Data processing, integrated creativity techniques, Product design, formal specification, Aerospace engineering, program specification, Human computer interaction, requirements engineering, Engineering management, aerospace computing, Software systems, Systems engineering and theory, Product development, software engineering, European airspace, air space management software system, Context modeling]
Applying ISO 9001:2000, MPS.BR and CMMI to Achieve Software Process Maturity: BL Informatica's Pathway
29th International Conference on Software Engineering
None
2007
Customer satisfaction, quality improvement and rework reduction are known to be the most important benefits obtained through deployment of software process maturity models and standards within an organization. Since 2003 BL Informatica has been motivated and has established and maintained its software processes based on international standards (like ISO 9001:2000) and maturity models (like MPS.BR and CMMI). In spite of the lack of financial and human resources, the organization achieved satisfactory results. This paper describes BL Infomatica's software processes improvement plan, lessons learned, difficulties and benefits that where collected during the execution of the improvement plan. It also presents quantitative results that demonstrate the return on investment during these years.
[Software maintenance, ISO standards, software quality, BL informatica's pathway, IEC standards, customer satisfaction, international standards, Standards organizations, Customer satisfaction, quality improvement, Software quality, software process improvement, ISO 9001:2000, Computer industry, CMMI, Software standards, software process maturity, MPS.BR, Capability maturity model, software standards, Software engineering]
Maturity Status within Front-End Support Organisations
29th International Conference on Software Engineering
None
2007
It may not be enough to develop mature processes at the back-end support level. Other strongly collaborating front-end support processes may substantially undermine them. For this reason, we have created CM3: front-end problem management - a detailed problem management process model to be utilised at the front-end support level. In this paper, we present the CM3 maturity levels at the front-end support and match them against the industrial state of practice within 15 software organisations. Our goal is to establish the current status of support maturity using CM3: front-end problem management. Our results show that the industrial processes studied suffice to provide basic problem management support at the front-end support level. However, only two out of 15 organisations studied have almost achieved the highest maturity level.
[maturity status, Software maintenance, Educational products, Virtual enterprises, software development management, front-end support organisations, Management training, Collaboration, Disaster management, Computer industry, problem management process model, Portfolios, front-end problem management, Testing, Software engineering]
A Constructivist Approach to Teaching Software Processes
29th International Conference on Software Engineering
None
2007
Recreating the context in which software processes are developed is difficult in the undergraduate classroom environment. As a result, traditional lecture-based teaching approaches do not necessarily translate into long-term understanding of software processes. To give students a deeper appreciation for the strengths and weaknesses of software process models, we designed the software process simulation game using constructivism as the underlying foundation. In this paper, we discuss the challenges associated with teaching software processes models, provide an overview of the game, detail its mechanics, and discuss the lessons learned from playing the game. Since the game does not involve actual programming or design activities, it can be used effectively for teaching both novice and experienced software engineers.
[Process design, Knowledge engineering, computer science education, Collaborative software, undergraduate classroom environment, software process simulation game, Programming, teaching, Design engineering, Software design, Education, constructivism, software processes teaching, software engineering, Problem-solving, Software tools, Software engineering]
Using Experiments in Software Engineering as an Auxiliary Tool for Teaching--A Qualitative Evaluation from the Perspective of Students' Learning Process
29th International Conference on Software Engineering
None
2007
In this paper we discuss issues of using students as subjects from the perspective of their benefits in terms of learning. We conduct interviews with students who participated in the experiments and present their perceptions of the experiments in order to validate the claims posed in the existing literature. Finally we show quantitative data on the rate of obtaining the distinctive pass grades for courses with and without experiments. The results show that integrating experiments with courses could lead to improvements of the performance of students on courses.
[Performance evaluation, Object oriented modeling, learning process, Unified modeling language, auxiliary tool, Humans, Programming, Best practices, Education, software engineering, computer aided instruction, software tools, students performance, Software engineering, Testing]
On the Impact of a Collaborative Pedagogy on African American Millennial Students in Software Engineering
29th International Conference on Software Engineering
None
2007
Millennial students (those born after 1982), particularly African Americans and women, have demonstrated a propensity toward collaborative activities. We conducted a collective case study at North Carolina State University and North Carolina A&amp;T to ascertain the role of collaboration and social interaction in attracting and retaining students in information technology. Responses from semi-structured interviews with 11 representative African American students in these classes were coded and analyzed. The responses from these minority students were used to evolve a social interaction model. The conjectures generated from the model suggest that pair programming and agile software methodologies effectively create a collaborative environment that is desirable to Millennial students, male and female, and, with the new evidence, minority and majority. Additionally, the African American Millennial students enjoy learning from their peers and believe that a collaborative environment better prepares them for the "real world".
[collaborative environment, computer science education, Engineering profession, Collaborative software, African American millennial students, Switches, social interaction model, agile software methodologies, Educational institutions, Information technology, Programming profession, Computer science, North Carolina State University, Collaboration, groupware, collaborative pedagogy, Collaborative work, software engineering, computer aided instruction, Software engineering]
Bug Hunt: Making Early Software Testing Lessons Engaging and Affordable
29th International Conference on Software Engineering
None
2007
Software testing efforts account for a large part of software development costs. However, as educators, we struggle to properly prepare students to perform software testing activities. This struggle is caused by multiple factors: (1) it is challenging to effectively incorporate software testing into an already over-packed curriculum, (2) ad-hoc efforts to teach testing generally happen too late in the students' career, after bad habits have already been developed, and (3) these efforts lack the necessary institutional consistency and support to be effective. To address these challenges we created Bug Hunt, a web-based tutorial to engage students in learning software testing strategies. In this paper we describe the most interesting aspects of the tutorial including the lessons and feedback mechanisms, and the facilities for instructors to configure the tutorial and obtain automatic student assessment. We also present the lessons learned after two years of deployment.
[Software testing, Performance evaluation, Courseware, computer science education, Costs, Engineering profession, program testing, software testing, Programming profession, Computer science, Bug Hunt, automatic student assessment, Feedback, software development costs, Web-based Tutorial., Computer science education, computer aided instruction, Web-based tutorial, Software engineering, Software Testing Education]
Good Practices for Educational Software Engineering Projects
29th International Conference on Software Engineering
None
2007
Recent publications indicate the importance of software engineering in the computer science curriculum. In this paper, we present the final part of software engineering education at University of Groningen in the Netherlands and Vaxjo University in Sweden, where student teams perform an industrial software development project. It furthermore presents the main educational problems encountered in such real-life projects and explains how this international course addresses these problems. The main contribution of this paper is a set of seven good practices for project based software engineering education.
[Software maintenance, computer science education, Uncertainty, University of Groningen, Project management, Programming, educational software engineering projects, Vaxjo University, Computer science, Sequential analysis, industrial software development project, Software quality, Computer industry, software engineering, Computer science education, computer science curriculum, Software engineering]
Top SE: Educating Superarchitects Who Can Apply Software Engineering Tools to Practical Development in Japan
29th International Conference on Software Engineering
None
2007
This paper discusses the Top SE program established to bridge the industry-academia gap. The program features extensive use of software engineering tools, not only to introduce students to the tools, but also as a conduit for learning the techniques and guidelines needed to apply the tools to practical software development situations. The curriculum is organized around practical problems mainly from the area of digital home appliances and focuses on upper stream software development processes. The Top SE program is developed and operated by a close collaboration between industry and academia. We illustrate our discussion with examples from one of the courses, verification of design models, which takes up model checking technologies, including three specific tools: SPIN, SMV, and LTSA.
[Educational programs, computer science education, Verification of Design Models, program verification, SPIN, Programming, practical software development, Educational institutions, software engineering tools, Guidelines, Bridges, Home appliances, software architecture, LTSA, Collaboration, educational courses, Computer industry, software tools, Top SE program, Informatics, SMV, Software engineering]
A Leveled Examination of Test-Driven Development Acceptance
29th International Conference on Software Engineering
None
2007
Test-driven development (TDD) has garnered considerable attention in professional settings and has made some inroads into software engineering and computer science education. A series of leveled experiments were conducted with students in beginning undergraduate programming courses through upper-level undergraduate, graduate, and professional training courses. This paper reports that mature programmers who try TDD are more likely to choose TDD over a similar test-last approach. Additionally this research reveals differences in programmer acceptance of TDD between beginning programmers who were reluctant to adopt TDD and more mature programmers who were more willing to adopt TDD. Attention is given to confounding factors, and future studies aimed at resolving these factors are identified. Finally proposals are made to improve early programmer acceptance of TDD.
[Software testing, Productivity, computer science education, undergraduate programming courses, Automatic programming, program testing, Optimized production technology, Programming profession, Industrial training, Computer languages, Automatic testing, Software quality, leveled examination, test-driven development acceptance, software engineering, Software engineering]
Using Soloman-Felder Learning Style Index to Evaluate Pedagogical Resources for Introductory Programming Classes
29th International Conference on Software Engineering
None
2007
Soloman-Felder learning style index has been applied extensively in programming classes to ascertain the learning styles of students. This paper presents an approach showing how learning styles of students can be used to evaluate pedagogical resources. In specific, learning style can be used to help determine an appropriate textbook and an appropriate mixture of additional pedagogical devices such as virtual labs or 'clickers'. An example from a first undergraduate programming course is used to illustrate the approach.
[computer science education, Instruments, Soloman-Felder learning style index, Cultural differences, Engineering education, Programming profession, programming classes, undergraduate programming course, Computer science, pedagogical resources, Internet, computer aided instruction, Engineering students, Software engineering]
Design and Evaluation of a Diagrammatic Notation to Aid in the Understanding of Concurrency Concepts
29th International Conference on Software Engineering
None
2007
It is generally accepted that concurrency can be difficult for students to reason about. While some studies provide insight into the nature of these difficulties [6], work remains to be done in understanding the aspects of learning about concurrency that are most difficult, and in developing approaches to dealing with this problem. We have conducted instructor interviews and an observational study of students, identified several key difficulties that students encounter, and developed a diagram that we believe will be an aid to understanding and problem-solving. We present the diagram and results of an initial user evaluation.
[computer science education, Unified Modeling Language, Unified modeling language, human factors, concurrent programming, student observational study, Cognition, diagrams, Yarn, concurrency concept learning, problem-solving, Concurrent computing, initial user evaluation, Education, concurrency control, UML 2.0 sequence diagram, Interleaved codes, operating systems (computers), diagrammatic notation, Problem-solving, OS-level synchronization primitive, Software engineering]
Creating a Computer Security Curriculum in a Software Engineering Program
29th International Conference on Software Engineering
None
2007
This paper discusses our experiences developing and delivering a computer security curriculum in a graduate software engineering program. It describes our program and student backgrounds, our approach to teaching computer security aimed towards a software engineering audience with disparate backgrounds, other challenges and solutions, course contents with emphasis on our core course, and additional venues and future plans.
[computer science education, Law, Data security, Forensics, Programming, Mathematics, teaching, course contents, Application software, graduate software engineering program, security of data, Operating systems, teaching computer security, educational courses, computer security curriculum, Public key cryptography, software engineering, software engineering audience, Computer security, Software engineering]
Introducing Accessibility Requirements through External Stakeholder Utilization in an Undergraduate Requirements Engineering Course
29th International Conference on Software Engineering
None
2007
Undergraduate software engineering courses aim to prepare students to deliver software in a variety of domains. The manner in which these courses are conducted varies, though team projects with real or imaginary stakeholders are common. While the key course concepts vary from the entire lifecycle to specific aspects of design, concepts like accessibility are rare. This paper will present a study of team projects in a requirements engineering course. One group of students conducted projects with accessibility requirements while another group of students delivered projects without accessibility requirements. The course content was the same, including discussion of accessibility. To support the understanding of accessibility, stakeholders with disabilities were included in the requirements engineering process. Both teams benefited from the experience as indirect knowledge acquisition occurred. Students from a previous offering of the course, with no external stakeholder interaction, demonstrated lower levels of accessibility understanding.
[Educational programs, computer science education, external stakeholder utilization, Knowledge acquisition, undergraduate requirements engineering course, knowledge acquisition, Programming, Design engineering, team projects, Prototypes, educational courses, undergraduate software engineering courses, requirements engineering process, Software systems, software engineering, Iterative methods, Time factors, Web sites, accessibility requirements, Software engineering]
Bringing the Systems Analysis and Design Course into 21^st Century: A Case Study in Implementing Modern Software Engineering Principles
29th International Conference on Software Engineering
None
2007
While today's IT students may have a theoretical knowledge of all the phases in the systems development lifecycle, they often have little exposure to software development core practices. Few are able to build a new system from scratch. This paper examines a pedagogical approach to modern software engineering based on "doing" systems development from day one. Course materials were produced by Agile methodology expert, Alistair Cockburn. Early results from an implementation of the experimental approach to the systems analysis and design course are examined.
[pedagogical approach, computer science education, software development, Programming, Educational institutions, teaching, systems analysis course, systems development lifecycle, Information technology, System analysis and design, Delay, Information systems, Guidelines, Computer science, educational courses, systems analysis, systems design course, software engineering, Computer science education, Software engineering]
A Template for Real World Team Projects for Highly Populated Software Engineering Classes
29th International Conference on Software Engineering
None
2007
Assigning projects of group work in the context of software engineering courses has become a commonly used practice in several educational institutions. Previously reported results examined different aspects of this approach. The problem is that most studies are based on relatively small group sizes. In this article a large scale project template for a class-wide project that is currently in use in the Department of Computer Engineering, Bogazici University, will be presented.
[computer science education, Project management, real world team projects, software engineering course, Programming, Educational institutions, Construction industry, large scale project template, Education, educational courses, Computer industry, software engineering, Large-scale systems, Metals industry, educational institutions, educational institution, Finishing, Software engineering]
Spotlight: A Prototype Tool for Software Plans
29th International Conference on Software Engineering
None
2007
Software evolution is made difficult by the need to integrate new features with all previously implemented features in the system. We present Spotlight, a prototype editor for software plans that seeks to address this problem by providing the programmer a principled way to separately develop and incrementally integrate independent features.
[Software prototyping, Software maintenance, Costs, prototype editor, Debugging, Spotlight, Displays, Educational institutions, Programming profession, software plans, software evolution, Computer science, Authentication, software tools, Software tools]
SoQueT: Query-Based Documentation of Crosscutting Concerns
29th International Conference on Software Engineering
None
2007
Understanding crosscutting concerns is difficult because their underlying relations remain hidden in a class-based decomposition of a system. Based on an extensive investigation of crosscutting concerns in existing systems and literature, we identified a number of typical implementation idioms and relations that allow us to group such concerns around so- called "sorts". In this paper, we present SoQueT, a tool that uses sorts to support the consistent description and documentation of crosscutting relations using pre-defined, sort- specific query templates.
[Marine technology, document handling, Java, Navigation, source coding, query-based documentation, Scattering, Documentation, source code, Mechanical factors, SoQueT, class-based decomposition, crosscutting concern documentation, Concrete]
SoftGUESS: Visualization and Exploration of Code Clones in Context
29th International Conference on Software Engineering
None
2007
We introduce SoftGUESS, a code clone exploration system. SoftGUESS is built on the more general GUESS system which provides users with a mechanism to interactively explore graph structures both through direct manipulation as well as a domain-specific language. We demonstrate SoftGUESS through a number of mini-applications to analyze evolutionary code-clone behavior in software systems. The mini-applications of SoftGUESS represent a novel way of looking at code-clones in the context of many system features. It is our hope that SoftGUESS will form the basis for other analysis tools in the software- engineering domain.
[codes, graph theory, Cloning, Switches, Domain specific languages, Computer science, evolutionary computation, code clone exploration system, Evolution (biology), domain-specific language, evolutionary code-clone behavior, Data visualization, Packaging, Biology computing, Software systems, graph structures, Libraries, SoftGUESS]
Kato: A Program Slicing Tool for Declarative Specifications
29th International Conference on Software Engineering
None
2007
This paper presents Kato, a tool that implements a novel class of optimizations that are inspired by program slicing for imperative languages but are applicable to analyzable declarative languages, such as Alloy. Kato implements a novel algorithm for slicing declarative models written in Alloy and leverages its relational engine KodKodfor analysis. Given an Alloy model, Kato identifies a slice representing the model's core: a satisfying instance for the core can systematically be extended into a satisfying instance for the entire model, while unsatisfiability of the core implies unsatisfiability of the entire model. The experimental results show that for a variety of subject models Kato's slicing algorithm enables an order of magnitude speed-up over Alloy's default translation to SAT.
[Algorithm design and analysis, Software testing, declarative languages, System testing, Optimizing compilers, imperative languages, Software algorithms, Buildings, declarative specifications, SAT, formal specification, Engines, Cost accounting, Kato tool, Automatic testing, KodKodfor analysis, Software systems, software tools, Alloy model, program slicing]
Korat: A Tool for Generating Structurally Complex Test Inputs
29th International Conference on Software Engineering
None
2007
This paper describes the Korat tool for constraint-based generation of structurally complex test inputs for Java programs. Korat takes: (1) an imperative predicate that specifies the desired structural integrity constraints and (2) a finitization that bounds the desired test input size. Korat generates all inputs (within the bounds) for which the predicate returns true. To do so, Korat performs a systematic search of the predicate's input space. The inputs that Korat generates enable bounded-exhaustive testing for programs ranging from library classes to stand-alone applications.
[Software testing, Java, System testing, program testing, constraint-based generation, Data structures, Korat tool, Application software, structurally complex test inputs, Software libraries, Tree graphs, Automatic testing, XML, bounded-exhaustive testing, Software systems, software tools, Java programs]
Crisp--A Fault Localization Tool for Java Programs
29th International Conference on Software Engineering
None
2007
Crisp is an Eclipse plug-in tool for constructing intermediate versions of a Java program that is being edited. After a long editing session, a programmer will run regression tests to make sure she has not invalidated previously tested functionality. If a test fails unexpectedly, Crisp allows the programmer to select parts of the edit that affected the failing test and to add them to the original program, creating an intermediate version guaranteed to compile. Then the programmer can re-execute the test in order to locate the exact reasons for the failure by concentrating on those affecting changes that were applied. Using Crisp, a programmer can it- eratively select, apply, and undo individual (or sets of) affecting changes and, thus effectively find a small set of failure-inducing changes. Crisp is an extension to our change impact analysis tool, Chianti, [6].
[Java, Debugging, regression analysis, Application software, Programming profession, software fault tolerance, fault localization tool, Crisp, Automatic testing, Prototypes, Eclipse plug-in tool, Java program, Performance analysis, Graphical user interfaces]
Suade: Topology-Based Searches for Software Investigation
29th International Conference on Software Engineering
None
2007
The investigation of a software system prior to a modification task often constitutes an important fraction of the overall effort associated with the task. We present Suade, an Eclipse plug-in to automatically generate suggestions for software investigation. The goal of Suade is to increase the efficiency with which developers explore the source code by recommending locations that are likely to be relevant to the task. Based on a context of software elements (fields and methods) explicitly specified by a developer, Suade automatically generates other elements that are likely to be relevant given the context, by analyzing the topology of structural dependencies in a software system.
[Java, source coding, source code, Suade, Eclipse plug-in, Spatial databases, Topology, software system, Open source software, Computer science, topology-based searches, software investigation, Software systems, Software standards, software engineering, Standards development, Pattern analysis, Software engineering]
SYNTHESIS: A Tool for Automatically Assembling Correct and Distributed Component-Based Systems
29th International Conference on Software Engineering
None
2007
SYNTHESIS is a tool for automatically assembling correct and distributed component-based systems. In our context, a system is correct when it is deadlock-free and performs only specified component interactions. In order to automatically synthesize the correct composition code, SYNTHESIS takes as input an high-level behavioural description for each component that must form the system to be built and a specification of the component interactions that must be enforced in the system. The automatically derived composition code is implemented as a set of distributed component wrappers that cooperatively interact with each other and with their wrapped components in order to prevent possible deadlocks and make the composed system exhibit only the specified interactions. The current version of SYNTHESIS supports two possible development platforms: Microsoft COM/DCOM, and EJB (Enterprise Java Beans).
[Java, Assembly systems, object-oriented programming, Cooling, enterprise JavaBeans, Displays, Microsoft DCOM, formal specification, distributed component wrapper, Computer science, component interaction specification, high-level behavioural description, automatic assembling component-based system, System recovery, Software systems, Collaborative work, SYNTHESIS tool, distributed component-based system, distributed object management, Software engineering, Business]
Presentations by Programmers for Programmers
29th International Conference on Software Engineering
None
2007
A common form of live technical presentation is that given by programmers for a programming audience during conferences, demonstrations, code reviews, and tutorials. Such presentations require manual switching between general presentation software and the integrated development environment (IDE), as well as reconfiguration of the IDE's UI to be readable by an audience. In this paper, we present a novel system that allows programmers to easily combine traditional slideware with seamless transitions to user-specified regions of the IDE along with special effects for live demonstration.
[Educational programs, general presentation software, Navigation, Switches, Programming profession, Computer science, programming audience, Feedback, integrated development environment, User interfaces, slideware, Motion pictures, live technical presentation, programming environments, technical presentation, Software engineering]
UML/Analyzer: A Tool for the Instant Consistency Checking of UML Models
29th International Conference on Software Engineering
None
2007
Large design models contain thousands of model elements. Designers easily get overwhelmed maintaining the consistency of such design models over time. Not only is it hard to detect new inconsistencies while the model changes but it also hard to keep track of known inconsistencies. The UML/analyzer tool identifies inconsistencies instantly with design changes and it keeps track of all inconsistencies over time. It does not require consistency rules with special annotations. Instead, it treats consistency rules as black-box entities and observes their behavior during their evaluation. The UML/Analyzer tool is integrated with the UML modeling tool IBM Rational RoseTM for broad applicability and usability. It is highly scalable and was evaluated on dozens of design models.
[Unified Modeling Language, instant consistency checking, Unified modeling language, Displays, Decoding, Best practices, analyzer tool, UML models, UML modeling tool, Feedback, Data visualization, Streaming media, Motion pictures, software engineering, software tools, Usability, Software engineering]
Revel8or: Model Driven Capacity Planning Tool Suite
29th International Conference on Software Engineering
None
2007
Designing complex multi-tier applications that must meet strict performance requirements is a challenging software engineering problem. Ideally, the application architect could derive accurate performance predictions early in the project life-cycle, leveraging initial application design-level models and a description of the target software and hardware platforms. To this end, we have developed a capacity planning tool suite for component-based applications, called Revel8tor. The tool adheres to the model driven development paradigm and supports benchmarking and performance prediction for J2EE, .Net and Web services platforms. The suite is composed of three different tools: MDAPerf MDABench and DSLBench. MDAPerf allows annotation of design diagrams and derives performance analysis models. MDABench allows a customized benchmark application to be modeled in the UML 2.0 Testing Profile and automatically generates a deployable application, with measurement automatically conducted. DSLBench allows the same benchmark modeling and generation to be conducted using a simple performance engineering Domain Specific Language (DSL) in Microsoft Visual Studio. DSLBench integrates with Visual Studio and reuses its load testing infrastructure. Together, the tool suite can assist capacity planning across platforms in an automated fashion.
[program testing, software performance requirement, Unified modeling language, Software performance, Web service, Predictive models, UML 2.0 testing profile, Microsoft Visual Studio, formal specification, domain specific language, software architecture, model driven development, load testing, network operating systems, Benchmark testing, design diagram annotation, .Net, Hardware, software engineering, Performance analysis, software performance evaluation, Java, object-oriented programming, project management, Unified Modeling Language, software reuse, J2EE, Application software, software project life-cycle, complex multitier application design, customized benchmark application, Web services, capacity planning tool suite, software reusability, software measurement, Capacity planning, Software engineering, software metrics, component-based application]
Tool Support for Developing Advanced Mechatronic Systems: Integrating the Fujaba Real-Time Tool Suite with CAMeL-View
29th International Conference on Software Engineering
None
2007
The next generation of advanced mechatronic systems is expected to use its software to exploit local and global networking capabilities to enhance their functionality and to adapt their local behavior when beneficial. Such systems will therefore include complex hard real-time coordination at the network level. This coordination is further reflected locally by complex reconfiguration in form of mode management and control algorithms. We present in this paper the integration of two tools which allow the integrated specification of real-time coordination and traditional control engineering specifically targeting the required complex reconfiguration of the local behavior.
[Real time systems, Mechatronics, Collaborative software, Software algorithms, Next generation networking, advanced mechatronic systems, Control engineering, real-time coordination, mechanical engineering computing, mechatronics, Model driven engineering, Software tools, Fujaba real-time tool suite, complex reconfiguration, Software engineering, Power engineering and energy]
Topes
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Programmers often omit input validation when inputs can appear in many different formats or when validation criteria cannot be precisely specified. To enable validation in these situations, we present a new technique that puts valid inputs into a consistent format and that identifies "questionable" inputs which might be valid or invalid, so that these values can be double-checked by a person or a program. Our technique relies on the concept of a "tope\
[Printing, reusable abstractions, data validation, data, spreadsheet data, abstract data types, abstraction, spreadsheet programs, Cleaning, Application software, application-independent abstraction, Programming profession, Computer science, Web services, topes, Web application, User interfaces, Performance analysis, Internet, Books, Software reusability, validation]
Tracking source locations
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Many programming tools require information to be associated with source locations. Current tools do this in different ways with different degrees of effectiveness. This paper is an investigation into the various approaches to maintaining source locations. It is based on an experiment that attempts to track a variety of locations over the evolution of a source file. The results demonstrate that relatively simple techniques can be very effective.
[Java, Visualization, Software algorithms, Maintenance engineering, programming tools, Programming profession, Radio access networks, software evolution, Computer science, tool support, source lines, Computer bugs, source file, Position measurement, Animation, software tools, source location tracking]
Answering conceptual queries with Ferret
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Programmers seek to answer questions as they investigate the functioning of a software system, such as "which execution path is being taken in this case?" Programmers attempt to answer these questions, which we call conceptual queries, using a variety of tools. Each type of tool typically highlights one kind of information about the system, such as static structural information or control-flow information. Unfortunately for the programmer, the tools seldom directly answer the programmer's conceptual queries. Instead, the programmer must piece together results from different tools to determine an answer to the initial query. At best, this process is time consuming and at worst, this process can lead to data overload and disorientation. In this paper, we present a model that supports the integration of different sources of information about a program. This model enables the results of concrete queries in separate tools to be brought together to directly answer many of a programmer's conceptual queries. In addition to presenting this model, we present a tool that implements the model, demonstrate the range of conceptual queries supported by this tool, and present the results of use of the conceptual queries in a small field study.
[tool integration, Information resources, static structural information, Software algorithms, data overload, Control systems, software system, Programming profession, Programming environments, Computer science, query processing, software representation models, conceptual queries answering, Permission, Software systems, Concrete, software engineering, programmer conceptual queries, Software engineering]
Specification patterns for probabilistic quality properties
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Probabilistic verification techniques are a powerful means to ensure that a software-intensive system fulfills its quality requirements. To apply these techniques an accurate specification of the required properties in a probabilistic temporal logic is necessary. To help practitioners formulate these properties correctly, this paper presents a specification pattern system of common probabilistic properties called ProProST. This pattern system has been a developed based on a survey of 152 properties from academic examples and 48 properties of real-word quality requirements from avionic, defence and automotive systems. Furthermore, a structured English grammar that can guide in the specification of probabilistic properties is given. Similar to previous specification patterns for traditional and real-time properties, the presented specification pattern system and the structured English grammar captures expert knowledge and helps practitioners to correctly apply formal verification techniques.
[Real time systems, Computer aided software engineering, probabilistic logic, pctl*, reliability, Aerospace electronics, software quality, software-intensive system, formal specification, Automotive engineering, security, formal verification, probabilistic verification, safety, Permission, specification patterns, pctl, probabilistic quality properties, structured English grammar, probabilistic quality patterns, Probabilistic logic, Hazards, ProProST, performance, grammars, quality requirements, probabilistic quality, Australia, csl, Formal verification, Software engineering]
Existential live sequence charts revisited
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Scenario-based specifications are a popular means for describing intended system behaviour. We aim to facilitate early analysis of system behaviour and the development of behaviour models in conjunction with scenarios. In this paper we define a novel scenario-based specification language with an existential semantics and that supports conditional specification of behaviour in the form of prechart and main chart. The language semantics is consistent with existing informal scenario-based and use-case based approaches to requirements engineering. The language provides a good fit with universal live sequence charts as standard existential live sequence charts do not adequately support conditional scenarios. In addition, we define a novel synthesis algorithm that, rather than building arbitrarily one of the many behaviour models that satisfy a scenario, constructs a modal transition system (MTS) which characterizes all behaviour models that conform to the scenario.
[Algorithm design and analysis, synthesis, use-case based approaches, Buildings, Software algorithms, synthesis algorithm, Educational institutions, Specification languages, scenarios, formal specification, mts, Analytical models, Multicast algorithms, requirements engineering, live sequence charts, modal transition system, specification languages, partial behaviour models, Permission, Animation, behaviour, conditional specification, scenario-based specification language, Software engineering]
Symbolic mining of temporal specifications
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Program specifications are important in many phases of the software development process, but they are often omitted or incomplete. An important class of specifications takes the form of temporal properties that prescribe proper usage of components of a software system. Recent work has focused on the automated inference of temporal specifications from the static or runtime behavior of programs. Many techniques match a specification pattern (represented by a finite state automaton) to all possible combinations of program components and enumerate the possible matches. Such approaches suffer from high space complexity and have not scaled beyond simple, two-letter alternating patterns (e.g. (ab)*). In this paper, we precisely define this form of specification mining and show that its general form is NP-complete. We observe a great deal of regularity in the representation and tracking of all possible combinations of system components. This motivates us to introduce a symbolic algorithm, based on binary decision diagrams (BDDs), that exploits this regularity. Our results show that this symbolic approach expands the tractability of this problem by orders of magnitude in both time and space. It enables us to mine more complex specifications, such as the common three-letter resource acquisition, usage, and release pattern ((ab+c)*). We have implemented our algorithm in a practical tool and used it to find significant specifications in real systems, including Apache Ant and Hibernate. We then used these specifications to find previously unknown bugs.
[Algorithm design and analysis, symbolic mining, software development process, release pattern, program verification, program components, data mining, temporal specification, Programming, symbolic algorithm, formal specification, software system, Boolean functions, Runtime, automated inference, specification pattern, finite state automaton, resource acquisition, runtime behavior, high space complexity, Learning automata, Data structures, dynamic analysis, specification mining, NP-complete problem, binary decision diagram, formal specifications, program specification, Computer science, usage pattern, Computer bugs, temporal properties, Software systems, Pattern matching]
Testing pervasive software in the presence of context inconsistency resolution services
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Pervasive computing software adapts its behavior according to the changing contexts. Nevertheless, contexts are often noisy. Context inconsistency resolution provides a cleaner pervasive computing environment to context-aware applications. A faulty context-aware application may, however, mistakenly mix up inconsistent contexts and resolved ones, causing incorrect results. This paper studies how such faulty context-aware applications may be affected by these services. We model how programs should handle contexts that are continually checked and resolved by context inconsistency resolution, develop novel sets of data flow equations to analyze the potential impacts, and thus formulate a new family of test adequacy criteria for testing these applications. Experimentation shows that our approach is promising.
[Software testing, Context-aware services, Pervasive computing, program testing, context inconsistency resolution services, data flow analysis, data flow equations, Application software, ubiquitous computing, pervasive computing, RFID tags, test adequacy, Radio frequency, context inconsistency resolution, context-aware applications, Permission, pervasive computing software, pervasive software, Radiofrequency identification, Software engineering, Context modeling]
ARTOO
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Intuition is often not a good guide to know which testing strategies will work best. There is no substitute for experimental analysis based on objective criteria: how many faults a strategy finds, and how fast. "Random" testing is an example of an idea that intuitively seems simplistic or even dumb, but when assessed through such criteria can yield better results than seemingly smarter strategies. The efficiency of random testing is improved if the generated inputs are evenly spread across the input domain. This is the idea of adaptive random testing (ART). ART was initially proposed for numerical inputs, on which a notion of distance is immediately available. To extend the ideas to the testing of object-oriented software, we have developed a notion of distance between objects and a new testing strategy called ARTOO, which selects as inputs objects that have the highest average distance to those already used as test inputs. ARTOO has been implemented as part of a tool for automated testing of object-oriented software. We present the ARTOO concepts, their implementation, and a set of experimental results of its application. Analysis of the results shows in particular that, compared to a directed random strategy, ARTOO reduces the number of tests generated until the first fault is found, in some cases by as much as two orders of magnitude. ARTOO also uncovers faults that the random strategy does not find in the time allotted, and its performance is more predictable.
[Software testing, System testing, object-oriented programming, program testing, Object oriented modeling, software testing, Subspace constraints, object-oriented software, random processes, Application software, object distance, Automatic testing, Software quality, ARTOO, adaptive random testing, Software tools, Random number generation, Software engineering]
Time will tell
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
We present an automatic fault localization technique which leverages time spectra as abstractions for program executions. Time spectra have been traditionally used for performance debugging. By contrast, we use them for functional correctness debugging by identifying pieces of program code that take a "suspicious" amount of time to execute. The approach can be summarized as follows: time spectra are collected from passing and failing runs, observed behavior models are created using the time spectra collected from passing runs, and deviations from these models in failing runs are identified and scored as potential causes of failures. Our empirical evaluations conducted on three real-life projects suggest that the proposed approach can effectively reduce the space of potential root causes for failures, which can in turn improve the turn around time for fixes.
[functional correctness debugging, program debugging, fault diagnosis, fault localization, automatic fault localization, Debugging, automated debugging, performance debugging, Information analysis, Automatic testing, Computer bugs, time spectra, Permission, Monitoring, program execution abstractions]
Towards reusable components with aspects
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
The potential of aspect-oriented programming to represent cross-cutting concerns as reusable components has yet to be fully realized. Indeed, authors have detailed significant challenges in creating reusable aspect component libraries. Proposed solutions include restricting the power of aspects upfront, inferring concern interaction, and shaping base code to conform to abstract design rules. Another proposed strategy is to reduce obliviousness in return for increased modularity by extending AspectJ with explicit join points (EJPs). This paper presents the results of an empirical case study that aides in the understanding of the tradeoffs between obliviousness and modularity. We present a refactoring of the exception handling concern for three real-life Java applications to use EJPs instead of oblivious aspects. The empirical differences between this version and an equivalent oblivious version are analyzed. Finally, we present guiding principles on how to strike a favorable balance between obliviousness and modularity.
[exception handling, Programming, Java applications, Security, software libraries, Design engineering, Software design, explicit join points, equivalent oblivious version, modularity, Permission, aspect-oriented programming, Libraries, AspectJ, Java, Multidimensional systems, object-oriented programming, Application software, software maintenance, abstract design rules, case study, configuration management, exception handling refactoring, Software quality, software reusability, reusable aspect component libraries]
Early prediction of software component reliability
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
The ability to predict the reliability of a software system early in its development, e.g., during architectural design, can help to improve the system's quality in a cost-effective manner. Existing architecture-level reliability prediction approaches focus on system-level reliability and assume that the reliabilities of individual components are known. In general, this assumption is unreasonable, making component reliability prediction an important missing ingredient in the current literature. Early prediction of component reliability is a challenging problem because of many uncertainties associated with components under development. In this paper we address these challenges in developing a software component reliability prediction framework. We do this by exploiting architectural models and associated analysis techniques, stochastic modeling approaches, and information sources available early in the development lifecycle. We extensively evaluate our framework to illustrate its utility as an early reliability prediction approach.
[Uncertainty, Information resources, system-level reliability, modeling, Buildings, software reliability, software component reliability, Stochastic processes, Predictive models, reliability prediction, Software reliability, software system, Information analysis, software architecture, component reliability prediction, Software architecture, architecture-level reliability prediction, Software systems, Software engineering]
Executable misuse cases for modeling security concerns
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Misuse cases are a way of modeling negative requirements, that is, behaviors that should not occur in a system. In particular, they can be used to model attacks on a system as well as the security mechanisms needed to avoid them. However, like use cases, misuse cases describe requirements in a high-level and informal manner. This means that, whilst they are easy to understand, they do not lend themselves to testing or analysis. In this paper, we present an executable misuse case modeling language which allows modelers to specify misuse case scenarios in a formal yet intuitive way and to execute the misuse case model in tandem with a corresponding use case model. Misuse scenarios are given in executable form and mitigations are captured using aspect-oriented modeling. The technique is useful for brainstorming potential attacks and their mitigations. Furthermore, the use of aspects allows mitigations to be maintained separately from the core system model. The paper, supported by a UML-based modeling tool, describes an application to two case studies, providing evidence that the technique can support red-teaming of security requirements for realistic systems.
[executable misuse case modeling language, aspect scenarios, object-oriented programming, aspect-oriented modeling, security of data, Unified Modeling Language, UML-based modeling tool, security mechanisms, software tools, Security, misuse cases]
Mining library specifications using inductive logic programming
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Software libraries organize useful functionalities in order to promote modularity and code reuse. A typical library is used by client programs through an application programming interface (API) that hides its internals from the client. Typically, the rules governing the correct usage of the API are documented informally. In many cases, libraries may have complex API usage rules and unclear documentation. As a result, the behaviour of the library under some corner cases may not be well understood by the programmer. Formal specifications provide a precise understanding of the API behaviour. We propose a methodology for learning interface specifications using Inductive Logic Programming (ILP). Our technique runs several unit tests on the library in order to generate relations describing the operation of the library. The data collected from these tests are used by an inductive learner to obtain rich Datalog/Prolog specifications. Such specifications capture essential properties of interest to the user. They may be used for applications such as reverse engineering the library internals or constructing checks on the application code to enforce proper API usage along with other properties of interest.
[software specification, Logic programming, application program interfaces, application programming interface, data mining, Datalog-Prolog specification, formal specification, software libraries, software library specification, machine learning., Libraries, verification, inductive logic programming, datalog]
Temporal dependency based checkpoint selection for dynamic verification of fixed-time constraints in grid workflow systems
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
In grid workflow systems, temporal correctness is critical to assure the timely completion of grid workflow execution. To monitor and control the temporal correctness, fixed-time constraints are often assigned to a grid workflow and then verified. A checkpoint selection strategy is used to select checkpoints along grid workflow execution for verifying fixed-time constraints. The problem of existing representative strategies is that they do not differentiate fixed-time constraints as once a checkpoint is selected, they verify all fixed-time constraints. However, these checkpoints do not need to be taken for those constraints whose consistency can be deduced from others. The corresponding verification of such constraints is consequently unnecessary and can severely impact the efficiency of overall temporal verification. To address the problem, in this paper, we develop a new temporal dependency based checkpoint selection strategy which can select checkpoints according to different fixed-time constraints. With our strategy, the corresponding unnecessary verification can be avoided. The comparison and experimental simulation further demonstrate that our new strategy can improve the efficiency of overall temporal verification significantly over the existing representative strategies.
[checkpointing, program verification, temporal dependency, grid computing, fixed-time constraints, grid workflow systems, dynamic verification, temporal dependency based checkpoint selection, grid workflows, overall temporal verification, checkpoint selection, checkpoint selection strategy, grid workflow execution, temporal correctness, Monitoring]
Precise memory leak detection for java software using container profiling
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
A memory leak in a Java program occurs when object references that are no longer needed are unnecessarily maintained. Such leaks are difficult to understand because static analyses typically cannot precisely identify these redundant references, and existing dynamic analyses for leak detection track and report fine-grained information about individual objects, producing results that are usually hard to interpret and lack precision. We introduce a novel container-based heap-tracking technique, based on the observation that many memory leaks in Java programs occur due to containers that keep references to unused data entries. The novelty of the described work is two-fold: (1) instead of tracking arbitrary objects and finding leaks by analyzing references to unused objects, the technique tracks only containers and directly identifies the source of the leak, and (2) the approach computes a confidence value for each container based on a combination of its memory consumption and its elements' staleness (time since last retrieval), while previous approaches do not consider such combined metrics. Our experimental results show that the reports generated by the proposed technique can be very precise: for two bugs reported by Sun and for a known bug in SPECjbb, the top containers in the reports include the containers that leak memory.
[Java, Software maintenance, precise memory leak detection, leaking confidence, Containers, Data structures, Leak detection, Programming profession, Information analysis, Runtime, memory leaks, Computer bugs, container profiling, software engineering, Java software, Java programs, Software engineering]
The effect of program and model structure on mc/dc test adequacy coverage
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
In avionics and other critical systems domains, adequacy of test suites is currently measured using the MC/DC metric on source code (or on a model in model-based development). We believe that the rigor of the MC/DC metric is highly sensitive to the structure of the implementation and can therefore be misleading as a test adequacy criterion. We investigate this hypothesis by empirically studying the effect of program structure on MC/DC coverage. To perform this investigation, we use six realistic systems from the civil avionics domain and two toy examples. For each of these systems, we use two versions of their implementation-with and without expression folding (i.e., inlining). To assess the sensitivity of MC/DC to program structure, we first generate test suites that satisfy MC/DC over a non-inlined implementation. We then run the generated test suites over the inlined implementation and measure MC/DC achieved. For our realistic examples, the test suites yield an average reduction of 29.5% in MC/DC achieved over the inlined implementations at 5% statistical significance level.
[Software testing, System testing, program testing, source coding, NASA, source code, Aerospace electronics, avionics, Logic testing, Certification, program structure, expression folding, Current measurement, modified condition and decision coverage criterion, structural coverage metrics, Permission, Computer industry, model-based development, DC generators, civil avionics, MC-DC test adequacy coverage, software metrics]
Static detection of cross-site scripting vulnerabilities
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Web applications support many of our daily activities, but they often have security problems, and their accessibility makes them easy to exploit. In cross-site scripting (XSS), an attacker exploits the trust a Web client (browser) has for a trusted server and executes injected script on the browser with the server's privileges. In 2006, XSS constituted the largest class of newly reported vulnerabilities making it the most prevalent class of attacks today. Web applications have XSS vulnerabilities because the validation they perform on untrusted input does not suffice to prevent that input from invoking a browser's JavaScript interpreter, and this validation is particularly difficult to get right if it must admit some HTML mark-up. Most existing approaches to finding XSS vulnerabilities are taint-based and assume input validation functions to be adequate, so they either miss real vulnerabilities or report many false positives. This paper presents a static analysis for finding XSS vulnerabilities that directly addresses weak or absent input validation. Our approach combines work on tainted information flow with string analysis. Proper input validation is difficult largely because of the many ways to invoke the JavaScript interpreter; we face the same obstacle checking for vulnerabilities statically, and we address it by formalizing a policy based on the W3C recommendation, the Firefox source code, and online tutorials about closed-source browsers. We provide effective checking algorithms based on our policy. We implement our approach and provide an extensive evaluation that finds both known and unknown vulnerabilities in real-world web applications.
[cross-site scripting, program verification, trusted server, W3C recommendation, Web applications, Displays, HTML, Information filtering, input validation, Information analysis, obstacle checking, closed-source browsers, Information filters, MySpace, string analysis, Java, Data analysis, Firefox source code, program diagnostics, JavaScript interpreter, static analysis, Application software, static detection, web applications, security of data, Web client, HTML mark-up, Internet, cross-site scripting vulnerabilities]
A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, naive Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: &gt;75% percentage of correctly classified files, a recall of &gt;80%, and a false positive rate &lt;30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.
[Costs, process related software metrics, regression analysis, Predictive models, software quality, comparative analysis, decision tree, Software metrics, Java file classification, defect prediction, naive Bayes method, Permission, learning (artificial intelligence), logistic regression, Testing, Classification tree analysis, Java, pattern classification, program diagnostics, cost-sensitive classification, product related software metrics, machine learning, change metrics, Eclipse project, decision trees, Bayes methods, static code attribute, Resource management, Logistics, Software engineering, software metrics]
On the difficulty of replicating human subjects studies in software engineering
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Replications play an important role in verifying empirical results. In this paper, we discuss our experiences performing a literal replication of a human subjects experiment that examined the relationship between a simple test for consistent use of mental models, and success in an introductory programming course. We encountered many difficulties in achieving comparability with the original experiment, due to a series of apparently minor differences in context. Based on this experience, we discuss the relative merits of replication, and suggest that, for some human subjects studies, literal replication may not be the the most effective strategy for validating the results of previous studies.
[Performance evaluation, replication, computer science education, Data analysis, Humans, empirical, experience report, mental models, Lungs, Collaboration, Permission, Packaging, literal replication, human subjects, software engineering, human subjects experiment, introductory programming course, Cognitive science, programming, Software engineering, Testing]
An empirical study of the effects of test-suite reduction on fault localization
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Fault-localization techniques that utilize information about all test cases in a test suite have been presented. These techniques use various approaches to identify the likely faulty part(s) of a program, based on information about the execution of the program with the test suite. Researchers have begun to investigate the impact that the composition of the test suite has on the effectiveness of these fault-localization techniques. In this paper, we present the first experiment on one aspect of test-suite composition--test-suite reduction. Our experiment studies the impact of the test-suite reduction on the effectiveness of fault-localization techniques. In our experiment, we apply 10 test-suite reduction strategies to test suites for eight subject programs. We then measure the differences between the effectiveness of four existing fault-localization techniques on the unreduced and reduced test suites. We also measure the reduction in test-suite size of the 10 test-suite reduction strategies. Our experiment shows that fault-localization effectiveness varies depending on the test-suite reduction strategy used, and it demonstrates the trade-offs between test-suite reduction and fault-localization effectiveness.
[Software testing, program testing, fault localization, test-suite reduction, program execution, Educational institutions, Size measurement, empirical study, fault-localization techniques, Software debugging, software fault tolerance, test-suite reduction strategy, Fault diagnosis, Permission, Software engineering]
Calysto
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Automatically detecting bugs in programs has been a long-held goal in software engineering. Many techniques exist, trading-off varying levels of automation, thoroughness of coverage of program behavior, precision of analysis, and scalability to large code bases. This paper presents the Calysto static checker, which achieves an unprecedented combination of precision and scalability in a completely automatic extended static checker. Calysto is interprocedurally path-sensitive, fully context-sensitive, and bit-accurate in modeling data operations - comparable coverage and precision to very expensive formal analyses - yet scales comparably to the leading, less precise, static-analysis-based tool for similar properties. Using Calysto, we have discovered dozens of bugs, completely automatically, in hundreds of thousands of lines of production, open-source applications, with a very low rate of false error reports. This paper presents the design decisions, algorithms, and optimizations behind Calysto's performance.
[Software testing, program debugging, program verification, Scalability, scalable extended static checking, formal verification, Permission, formal analysis, software engineering, automatic extended static checker, Calysto static checker, static checking, precise extended static checking, Java, data operations, Automation, program behavior, program diagnostics, static analysis, Computer science, bugs detection, analysis precision, open source application, Computer bugs, Formal verification, Software engineering, Context modeling]
jPredictor
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
jPredictor is a tool for detecting concurrency errors in Java programs. The Java program is instrumented to emit property-relevant events at runtime and then executed. The resulting execution trace is collected and analyzed by Predictor, which extracts a causality relation sliced using static analysis and refined with lock-atomicity information. The resulting abstract model, a hybrid of a partial order and atomic blocks, is then exhaustively analyzed against the property and errors with counter-examples are reported to the user. Thus, jPredictor can "predict" errors that did not happen in the observed execution, but which could have happened under a different thread scheduling. The analysis technique employed in jPredictor is fully automatic, generic (works for any trace property), sound (produces no false alarms) but it is incomplete (may miss errors). Two common types of errors are investigated in this paper: dataraces and atomicity violations. Experiments show that jPredictor is precise (in its predictions), effective and efficient. After the code producing them was executed only once, jPredictor found all the errors reported by other tools. It also found errors missed by other tools, including static race detectors, as well as unknown errors in popular systems like Tomcat and the Apache FTP server.
[Algorithm design and analysis, sliced causality, dataraces violation, Information analysis, Concurrent computing, Runtime, predictive runtime analysis, abstract model, Permission, runtime verification, Java programs, causality relation, Testing, Java, Instruments, program diagnostics, static analysis, Software debugging, concurrency errors, atomicity violation, execution trace, jPredictor, concurrency control, thread scheduling, property-relevant events, lock atomicity information, Error correction]
Dynamic detection of atomic-set-serializability violations
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Previously we presented atomic sets, memory locations that share some consistency property, and units of work, code fragments that preserve consistency of atomic sets on which they are declared. We also proposed atomic-set serializability as a correctness criterion for concurrent programs, stating that units of work must be serializable for each atomic set. We showed that a set of problematic data access patterns characterize executions that are not atomic-set serializable. Our criterion subsumes data races (single-location atomic sets) and serializability (all locations in one set). In this paper, we present a dynamic analysis for detecting violations of atomic-set serializability. The analysis can be implemented efficiently, and does not depend on any specific synchronization mechanism. We implemented the analysis and evaluated it on a suite of real programs and benchmarks. We found a number of known errors as well as several problems not previously reported.
[Algorithm design and analysis, Atomic measurements, object-oriented programming, Logic programming, atomicity, dynamic analysis, Yarn, Logic testing, concurrent object-oriented programming, code fragment, Computer bugs, concurrency control, data access pattern, dynamic atomic-set-serializability violation detection, serializability, system monitoring, Dynamic programming, Error correction, memory location, Object oriented programming, concurrent object-oriented program, data races, Software engineering, consistency property]
An empirical study of software developers' management of dependencies and changes
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Different approaches and tools have been proposed to support change impact analysis, i.e., the identification of the potential consequences of a change, or the estimation of what needs to be modified to accomplish a change. However, just a few empirical studies of software developers' actual change impact analysis approaches have been reported in the literature. To minimize this gap, this paper describes an empirical study of two software development teams. It describes, through the presentation of ethnographic data, the strategies used by software developers to handle the effect of software dependencies and changes in their work. The concept of impact management is proposed as an analytical framework to present these practices and is used to suggest avenues for future research in change impact analysis techniques.
[Software testing, ethnographic data, software developer management, Project management, software development management, Software performance, Programming, socio-technical aspects, software dependency, software maintenance, empirical studies, Software development management, software evolution, change impact analysis technique, collaborative software development, Software systems, Performance analysis, Software tools, Informatics, change impact analysis, Software engineering]
TODO or to bug
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Software development is a highly collaborative activity that requires teams of developers to continually manage and coordinate their programming tasks. In this paper, we describe an empirical study that explored how task annotations embedded within the source code play a role in how software developers manage personal and team tasks. We present findings gathered by combining results from a survey of professional software developers, an analysis of code from open source projects, and interviews with software developers. Our findings help us describe how task annotations can be used to support a variety of activities fundamental to articulation work within software development. We describe how task management is negotiated between the more formal issue tracking systems and the informal annotations that programmers write within their source code. We report that annotations have different meanings and are dependent on individual, team and community use. We also present a number of issues related to managing annotations, which may have negative implications for maintenance. We conclude with insights into how these findings could be used to improve tool support and software process.
[task management, program debugging, TODO, open source projects, Protocols, Collaborative software, Collaborative tools, public domain software, Project management, software developers, task annotations, task analysis, collaborative activity, To Bug, Programming profession, Open source software, Software development management, Embedded software, work practices, groupware, Collaborative work, Software tools, source code comments]
Evolving software product lines with aspects
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Software product lines (SPLs) enable modular, large-scale reuse through a software architecture addressing multiple core and varying features. To reap the benefits of SPLs, their designs need to be stable. Design stability encompasses the sustenance of the product line's modularity properties in the presence of changes to both the core and varying features. It is usually assumed that aspect-oriented programming promotes better modularity and changeability of product lines than conventional variability mechanisms, such as conditional compilation. However, there is no empirical evidence on its efficacy to prolong design stability of SPLs through realistic development scenarios. This paper reports a quantitative study that evolves two SPLs to assess various design stability facets of their aspect-oriented implementations. Our investigation focused upon a multi-perspective analysis of the evolving product lines in terms of modularity, change propagation, and feature dependency. We have identified a number of scenarios which positively or negatively affect the architecture stability of aspectual SPLs.
[Encapsulation, realistic development scenario, software product lines, design stability assessment, Stability, software product line evolution, Product design, software maintenance, Computer science, empirical evaluation, Computer languages, software architecture, Software architecture, systems analysis, Computer architecture, product development, Permission, software reusability, aspect-oriented programming, Large-scale systems, Software engineering]
A verification system for timed interval calculus
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Timed Interval Calculus (TIC) is a highly expressive set-based notation for specifying and reasoning about embedded real-time systems. However, it lacks mechanical proving support, as its verification usually involves infinite time intervals and continuous dynamics. In this paper, we develop a system based on a generic theorem prover, Prototype Verification System (PVS), to assist formal verification of TIC at a high grade of automation. TIC semantics has been constructed by the PVS typed higher-order logic. Based on the encoding, we have checked all TIC reasoning rules and discovered subtle flaws. A translator has been implemented in Java to automatically transform TIC models into PVS specifications. A collection of supplementary rules and PVS strategies has been defined to facilitate the rigorous reasoning of TIC models with functional and non-functional (for example, real-time) requirements at the interval level. Our approach is generic and can be applied further to support other real-time notations.
[Real time systems, higher-order logic, prototype verification system, set-based notation, embedded real-time system, program translator, Calculus, calculus, formal specification, formal logic, formal verification, Mathematical analysis, Prototypes, embedded systems, theorem proving, Logic, specification language, Embedded computing, Java, Encoding, State-space methods, Formal specifications, reasoning about program, encoding, pvs, Power system modeling, program interpreters, generic theorem prover, real-time systems, reasoning about programs, timed interval calculus semantics]
DySy
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Dynamically discovering likely program invariants from concrete test executions has emerged as a highly promising software engineering technique. Dynamic invariant inference has the advantage of succinctly summarizing both "expected" program inputs and the subset of program behaviors that is normal under those inputs. In this paper, we introduce a technique that can drastically increase the relevance of inferred invariants, or reduce the size of the test suite required to obtain good invariants. Instead of falsifying invariants produced by pre-set patterns, we determine likely program invariants by combining the concrete execution of actual test cases with a simultaneous symbolic execution of the same tests. The symbolic execution produces abstract conditions over program variables that the concrete tests satisfy during their execution. In this way, we obtain the benefits of dynamic inference tools like Daikon: the inferred invariants correspond to the observed program behaviors. At the same time, however, our inferred invariants are much more suited to the program at hand than Daikon's hard-coded invariant patterns. The symbolic invariants are literally derived from the program text itself, with appropriate value substitutions as dictated by symbolic execution. We implemented our technique in the DySy tool, which utilizes a powerful symbolic execution and simplification engine. The results confirm the benefits of our approach. In Daikon's prime example benchmark, we infer the majority of the interesting Daikon invariants, while eliminating invariants that a human user is likely to consider irrelevant.
[Software testing, program testing, software engineering technique, symbolic reasoning, Humans, Engines, abstract program condition, daikon, dynamic invariant inference, dysy, Permission, Benchmark testing, pex, program behavior, concrete test execution, Educational institutions, DySy tool, Application software, Computer science, dynamic program invariant inference, system monitoring, Concrete, dynamic symbolic execution, reasoning about programs, Software engineering]
Incremental state-space exploration for programs with dynamically allocated data
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
We present a novel technique that speeds up state-space exploration (SSE) for evolving programs with dynamically allocated data. SSE is the essence of explicit-state model checking and an increasingly popular method for automating test generation. Traditional, non-incremental SSE takes one version of a program and systematically explores the states reachable during the program's executions to find property violations. Incremental SSE considers several versions that arise during program evolution: reusing the results of SSE for one version can speed up SSE for the next version, since state spaces of consecutive program versions can have significant similarities. We have implemented our technique in two model checkers: Java PathFinder and the J-Sim state-space explorer. The experimental results on 24 program evolutions and exploration changes show that for non-initial runs our technique speeds up SSE in 22 cases from 6.43% to 68.62% (with median of 42.29%) and slows down SSE in only two cases for -4.71% and -4.81%.
[Software testing, J-Sim state-space explorer, System testing, Java, incremental state-space exploration, jpf, Object oriented modeling, SSE, dynamically allocated data, incremental computation, Java PathFinder, State-space methods, Software safety, automating test generation, Computer science, resource allocation, model checking, Automatic testing, Permission, state-space exploration, explicit-state model checking, java pathfinder, Software engineering, j-sim]
Debugging reinvented
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
When software developers want to understand the reason for a program's behavior, they must translate their questions about the behavior into a series of questions about code, speculating about the causes in the process. The Whyline is a new kind of debugging tool that avoids such speculation by instead enabling developers to select a question about program output from a set of why did and why didn't questions derived from the program's code and execution. The tool then finds one or more possible explanations for the output in question, using a combination of static and dynamic slicing, precise call graphs, and new algorithms for determining potential sources of values and explanations for why a line of code was not reached. Evaluations of the tool on one task showed that novice programmers with the Whyline were twice as fast as expert programmers without it. The tool has the potential to simplify debugging in many software development contexts.
[Java, Visualization, program debugging, Navigation, program behavior, software development, whyline, Reasoning about programs, software developers, Software debugging, program's code, Programming profession, Computer science, precise call graphs, Prototypes, software engineering, program's execution, Output feedback, Testing, debugging tool]
Granularity in software product lines
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Building software product lines (SPLs) with features is a challenging task. Many SPL implementations support features with coarse granularity - e.g., the ability to add and wrap entire methods. However, fine-grained extensions, like adding a statement in the middle of a method, either require intricate workarounds or obfuscate the base code with annotations. Though many SPLs can and have been implemented with the coarse granularity of existing approaches, fine-grained extensions are essential when extracting features from legacy applications. Furthermore, also some existing SPLs could benefit from fine-grained extensions to reduce code replication or improve readability. In this paper, we analyze the effects of feature granularity in SPLs and present a tool, called Colored IDE (CIDE), that allows features to implement coarse-grained and fine-grained extensions in a concise way. In two case studies, we show how CIDE simplifies SPL development compared to traditional approaches.
[software product lines, Mathematics, Spatial databases, Transaction databases, ide, Computer science, Design engineering, coarse granularity, Software design, virtual separation of concerns, colored IDE, Permission, Feature extraction, code replication, software engineering, feature refactoring, Software tools, Informatics]
Scalable detection of semantic clones
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Several techniques have been developed for identifying similar code fragments in programs. These similar fragments, referred to as code clones, can be used to identify redundant code, locate bugs, or gain insight into program design. Existing scalable approaches to clone detection are limited to finding program fragments that are similar only in their contiguous syntax. Other, semantics-based approaches are more resilient to differences in syntax, such as reordered statements, related statements interleaved with other unrelated statements, or the use of semantically equivalent control structures. However, none of these techniques have scaled to real world code bases. These approaches capture semantic information from Program Dependence Graphs (PDGs), program representations that encode data and control dependencies between statements and predicates. Our definition of a code clone is also based on this representation: we consider program fragments with isomorphic PDGs to be clones. In this paper, we present the first scalable clone detection algorithm based on this definition of semantic clones. Our insight is the reduction of the difficult graph similarity problem to a simpler tree similarity problem by mapping carefully selected PDG subgraphs to their related structured syntax. We efficiently solve the tree similarity problem to create a scalable analysis. We have implemented this algorithm in a practical tool and performed evaluations on several million-line open source projects, including the Linux kernel. Compared with previous approaches, our tool locates significantly more clones, which are often more semantically interesting than simple copied and pasted code fragments.
[Performance evaluation, Software maintenance, program debugging, program design, Linux kernel, data flow graphs, redundant code, program compilers, scalable clone detection algorithm, bugs location, real world code bases, program fragments, Tree graphs, refactoring, program dependence graphs, semantically equivalent control structures, program representations, program code fragments, Kernel, contiguous syntax, program control structures, tree similarity problem, clone detection, Software algorithms, Cloning, trees (mathematics), PDG subgraphs, structured syntax, semantic clones, million-line open source projects, software maintenance, graph similarity problem, Computer science, code clones, reordered statements, Linux, Computer bugs, isomorphic PDG, scalable detection, semantic information, Detection algorithms, program dependence graph]
The effect of the number of inspectors on the defect estimates produced by capture-recapture models
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Inspections can be made more cost-effective by using capture-recapture methods to estimate post-inspection defects. Previous capture-recapture studies of inspections used relatively small data sets compared with those used in biology and wildlife research (the origin of the models). A common belief is that capture-recapture models underestimate the number of defects but their performance can be improved with data from more inspectors. This increase has not been evaluated in detail. This paper evaluates new estimators from biology not been previously applied to inspections. Using a data from seventy-three inspectors, we analyze the effect of the number of inspectors on the quality of estimates. Contrary to previous findings indicating that Jackknife is the best estimator, our results show that the SC estimators are better suited to software inspections. Our results also provide a detailed analysis of the number of inspectors necessary to obtain estimates within 5% to 20% of the actual.
[requirements, program testing, program verification, Biological system modeling, Project management, Inspection, inspections, capture-recapture models, Information analysis, Animals, Wildlife, Software quality, software inspections, post-inspection defects, validation and verification, State estimation, Computational biology, Software engineering, inspection]
Predicting accurate and actionable static analysis warnings
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Static analysis tools report software defects that may or may not be detected by other verification methods. Two challenges complicating the adoption of these tools are spurious false positive warnings and legitimate warnings that are not acted on. This paper reports automated support to help address these challenges using logistic regression models that predict the foregoing types of warnings from signals in the warnings and implicated code. Because examining many potential signaling factors in large software development settings can be expensive, we use a screening methodology to quickly discard factors with low predictive power and cost-effectively build predictive models. Our empirical evaluation indicates that these models can achieve high accuracy in predicting accurate and actionable static analysis warnings, and suggests that the models are competitive with alternative models built without screening.
[static analysis warnings, logistic regression analysis, screening, regression analysis, Predictive models, Programming, software quality, verification methods, spurious false positive warnings, static analysis tools, Accuracy, formal verification, potential signaling factors, Permission, software tools, experimental program analysis, Statistical analysis, program diagnostics, Regression analysis, software development settings, legitimate warnings, Software quality, software defects, screening methodology, logistic regression models, Power system reliability, Software tools, Logistics]
Sufficient mutation operators for measuring test effectiveness
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Mutants are automatically-generated, possibly faulty variants of programs. The mutation adequacy ratio of a test suite is the ratio of non-equivalent mutants it is able to identify to the total number of non-equivalent mutants. This ratio can be used as a measure of test effectiveness. However, it can be expensive to calculate, due to the large number of different mutation operators that have been proposed for generating the mutants. In this paper, we address the problem of finding a small set of mutation operators which is still sufficient for measuring test effectiveness. We do this by defining a statistical analysis procedure that allows us to identify such a set, together with an associated linear model that predicts mutation adequacy with high accuracy. We confirm the validity of our procedure through cross-validation and the application of other, alternative statistical analyses.
[Software testing, nonequivalent mutants, Costs, Statistical analysis, program testing, Genetic mutations, Predictive models, testing effectiveness, Time measurement, mathematical operators, Application software, Computer science, Automatic testing, mutation operators, mutation analysis, test effectiveness, Software measurement, statistical analysis]
Are fit tables really talking?
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Test-driven software development tackles the problem of operationally defining the features to be implemented by means of test cases. This approach was recently ported to the early development phase, when requirements are gathered and clarified. Among the existing proposals, Fit (Framework for Integrated Testing) supports the precise specification of requirements by means of so called Fit tables, which express relevant usage scenarios in a tabular format, easily understood also by the customer. Fit tables can be turned into executable test cases through the creation of pieces of glue code, called fixtures. In this paper, we test the claimed benefits of Fit through a series of three controlled experiments in which Fit tables and related fixtures are used to clarify a set of change requirements, in a software evolution scenario. Results indicate improved correctness achieved with no significant impact on time, however benefits of Fit vary in a substantial way depending on the developers' experience. Preliminary results on the usage of Fit in combination with pair programming revealed another relevant source of variation.
[Software testing, System testing, framework for integrated testing, program testing, executable test cases, Programming, HTML, Proposals, formal specification, evolution tasks, fixtures, fit tables, software evolution, table lookup, requirements specification, Information resources, Natural languages, software maintenance, empirical studies, Employee welfare, acceptance test, glue code, Automatic testing, test-driven software development, Fixtures]
Data flow testing of service-oriented workflow applications
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
WS-BPEL applications are a kind of service-oriented application. They use XPath extensively to integrate loosely-coupled workflow steps. However, XPath may extract wrong data from the XML messages received, resulting in erroneous results in the integrated process. Surprisingly, although XPath plays a key role in workflow integration, inadequate researches have been conducted to address the important issues in software testing. This paper tackles the problem. It also demonstrates a novel transformation strategy to construct artifacts. We use the mathematical definitions of XPath constructs as rewriting rules, and propose a data structure called XPath rewriting graph (XRG), which not only models how an XPath is conceptually rewritten but also tracks individual rewritings progressively. We treat the mathematical variables in the applied rewriting rules as if they were program variables, and use them to analyze how information may be rewritten in an XPath conceptually. We thus develop an algorithm to construct XRGs and a novel family of data flow testing criteria to test WS-BPEL applications. Experiment results show that our testing approach is promising.
[Software testing, WS-BPEL application testing, program testing, soa, data flow graphs, XML message, query languages, service-oriented workflow application, Data mining, XRG, Engines, Information analysis, xpath, loosely-coupled workflow step integration, xml, specification languages, data structures, Mathematical model, workflow management software, service-orientation, software testing, data flow testing criteria, testing, Data structures, Application software, query rewriting rule, ws-bpel, XPath rewriting graph data structure, workflow testing, xml document model, Web services, XML, transformation strategy, Concrete, rewriting rules]
A tale of four kernels
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
The FreeBSD, GNU/Linux, Solaris, and Windows operating systems have kernels that provide comparable facilities. Interestingly, their code bases share almost no common parts, while their development processes vary dramatically. We analyze the source code of the four systems by collecting metrics in the areas of file organization, code structure, code style, the use of the C preprocessor, and data organization. The aggregate results indicate that across various areas and many different metrics, four systems developed using wildly different processes score comparably. This allows us to posit that the structure and internal quality attributes of a working, non-trivial software artifact will represent first and foremost the engineering requirements of its construction, with the influence of process being marginal, if any.
[freebsd, proprietary software, software quality, Open source software, nontrivial software artifact, Solaris operating systems, Technology management, data organization, open source, Windows operating systems, Operating systems, linux, Permission, Kernel, operating system kernels, FreeBSD operating systems, development processes, comparison, opensolaris, source code, file organization, Construction industry, wrk, Linux operating systems, Linux, Aggregates, Software quality, file organisation, code style, Software engineering, software metrics, code structure]
Defining and continuous checking of structural program dependencies
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Dependencies between program elements need to be modeled from different perspectives reflecting architectural, design, and implementation level decisions. To avoid erosion of the intended structure of the code, it is necessary to explicitly codify these different perspectives on the permitted dependencies and to detect violations continuously and incrementally as software evolves. We propose an approach that uses declarative queries to group source elements - across programming language module boundaries - into overlapping ensembles. The dependencies between these ensembles are also specified as logic queries. The approach has been integrated into the incremental build process of Eclipse to ensure continuous checking, using an engine for tabled and incremental evaluation of logic queries. Our evaluation shows that our approach is fast enough for day-to-day use along the incremental build process of modern IDEs.
[programming language module, program verification, design level decisions, Design methodology, Production facilities, Gettering, Engines, continuous checking, incremental evaluation, Permission, datalog, Java, Data analysis, Logic programming, program diagnostics, structural program dependencies, static analysis, declarative queries, program elements, Programming profession, controlling program dependencies, Computer languages, implementation level decisions, architectural level decisions, logic queries]
A case study evaluation of maintainability and performance of persistency techniques
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Efforts for software evolution supersede any other part of the software life cycle. Technological decisions have a major impact on the maintainability, but are not well reflected by existing code or architecture based metrics. The way the persistency of object structures with relational databases is solved affects the maintainability of the overall system. Besides maintainability other quality attributes of the software are of interest, in particular performance metrics. However, a systematic evaluation of the benefits and drawback of different persistency frameworks is lacking. In this paper we systematically evaluate the maintainability and performance of different technological approaches for this mapping. The paper presents a testbed and an evaluation process with specifically designed metrics to evaluate persistency techniques regarding their maintainability and performance. In the second part we present and discuss the results of the case study.
[Software maintenance, gqm, Relational databases, Software performance, architecture based metrics, persistency technique, software evolution, software architecture, Software design, Computer architecture, software life cycle, Software measurement, software performance evaluation, Testing, Application software, software maintenance, relational databases, persistency techniques, persistency framework, performance, performance metrics, Software quality, maintainability, Software engineering, software metrics]
Automatic modularity conformance checking
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
According to Parnas's information hiding principle and Baldwin and Clark's design rule theory, the key step to decomposing a system into modules is to determine the design rules (or in Parnas's terms, interfaces) that decouple otherwise coupled design decisions and to hide decisions that are likely to change in independent modules. Given a modular design, it is often difficult to determine whether and how its implementation realizes the designed modularity. Manually comparing code with abstract design is tedious and error-prone. We present an automated approach to check the conformance of implemented modularity to designed modularity, using design structure matrices as a uniform representation for both. Our experiments suggest that our approach has the potential to manifest the decoupling effects of design rules in code, and to detect modularity deviation caused by implementation faults. We also show that design and implementation models together provide a comprehensive view of modular structure that makes certain implicit dependencies within code explicit.
[Algorithm design and analysis, conformance checking, Software maintenance, program verification, Data structures, formal specification, Computer science, automatic modularity conformance checking, Software architecture, modularity, Fault detection, Clustering algorithms, Permission, Software systems, design structure matrices, Software engineering]
Breaking the barriers to successful refactoring
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Refactoring is the process of changing the structure of code without changing its behavior. Refactoring can be semi-automated with tools, which should make it easier for programmers to refactor quickly and correctly. However, we have observed that many tools do a poor job of communicating errors triggered by the refactoring process and that programmers using them sometimes refactor slowly, conservatively, and incorrectly. In this paper we characterize problems with current refactoring tools, demonstrate three new tools to assist in refactoring, and report on a user study that compares these new tools against existing tools. The results of the study show that speed, accuracy, and user satisfaction can be significantly increased. From the new tools we induce a set of usability recommendations that we hope will help inspire a new generation of programmer-friendly refactoring tools.
[extract method, refactoring process, usability, refactoring, environments, software tools, Usability, tools, Programming profession, programmer-friendly refactoring tools, code structure]
Systematically refactoring inheritance to delegation in java
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Because of the strong coupling of classes and the proliferation of unneeded class members induced by inheritance, the suggestion to use composition and delegation instead has become common place. The presentation of a corresponding refactoring in the literature may lead one to believe that such a transformation is a straightforward undertaking. However, closer analysis reveals that this refactoring is neither always possible, nor does it necessarily achieve its desired effect. We have therefore identified the necessary preconditions and realizable postconditions of the refactoring, and built a tool that can perform it completely automatically. By applying this tool to all subclasses of several open-source projects, we have collected evidence of the applicability of the refactoring and of its capability to deliver on its promises. The refactoring builds on constraint graphs originally developed for type inference to check the preconditions and to compute the necessary delegation as well as the subtype relationships that must be maintained.
[Java, constraint graphs, object-oriented programming, graph theory, inheritance, Reflection, forwarding, software maintenance, Open source software, open recursion, evaluation, Computer languages, refactoring, design, refactoring inheritance, Frequency, Concrete, Performance analysis, delegation, Object oriented programming, Software engineering]
ReBA
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Although in theory the APIs of software libraries and frameworks should be stable, they change in practice. This forces clients of the library API to change as well, making software maintenance expensive. Changing a client might not even be an option if its source code is missing or certain policies forbid its change. By giving a library both the old and the new API, clients can be shielded from API changes and can run with the new version of the library. This paper presents our solution and a tool, ReBA, that automatically generates compatibility layers between new library APIs and old clients. In the first stage, ReBA generates another version of the library, called adapted-library, that supports both the old and the new APIs. In the second stage, ReBA shrinks the adapted-library into a minimal, client-specific compatibility layer containing only classes truly required by the client. Evaluations on controlled experiments and case studies using Eclipse core libraries shows that our approach effectively adapts clients to new library versions, and is efficient.
[Java, new library version, application program interfaces, source code, libraries, Production facilities, Data mining, software maintenance, software libraries, configuration management, evolving software library API, component reuse, refactoring, Loading, application program interface, Probability density function, software reusability, refactoring-aware binary adaptation, api compatibility, Libraries, Software]
Impact analysis of database schema changes
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
We propose static program analysis techniques for identifying the impact of relational database schema changes upon object-oriented applications. We use dataflow analysis to extract all possible database interactions that an application may make. We then use this information to predict the effects of schema change. We evaluate our approach with a case-study of a commercially available content management system, where we investigated 62 versions of between 70 k-127 k LoC and a schema size of up to 101 tables and 568 stored procedures. We demonstrate that the program analysis must be more precise, in terms of context-sensitivity than related work. However, increasing the precision of this analysis increases the computational cost. We use program slicing to reduce the size of the program that needs to be analyzed. Using this approach, we are able to analyse the case study in under 2 minutes on a standard desktop machine, with no false negatives and a low level of false positives.
[Software maintenance, Data analysis, Content management, object-oriented programming, dataflow analysis, Object oriented databases, object-oriented application, database interaction, Relational databases, data flow analysis, Application software, relational databases, impact analysis, static program analysis technique, Permission, Software systems, Computational efficiency, program slicing, relational database schema change, Software engineering]
An approach to detecting duplicate bug reports using natural language and execution information
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
An open source project typically maintains an open bug repository so that bug reports from all over the world can be gathered. When a new bug report is submitted to the repository, a person, called a triager, examines whether it is a duplicate of an existing bug report. If it is, the triager marks it as duplicate and the bug report is removed from consideration for further work. In the literature, there are approaches exploiting only natural language information to detect duplicate bug reports. In this paper we present a new approach that further involves execution information. In our approach, when a new bug report arrives, its natural language information and execution information are compared with those of the existing bug reports. Then, a small number of existing bug reports are suggested to the triager as the most similar bug reports to the new bug report. Finally, the triager examines the suggested bug reports to determine whether the new bug report duplicates an existing bug report. We calibrated our approach on a subset of the Eclipse bug repository and evaluated our approach on a subset of the Firefox bug repository. The experimental results show that our approach can detect 67%-93% of duplicate bug reports in the Firefox bug repository, compared to 43%-72% using natural language information alone.
[Software maintenance, Costs, natural language processing, public domain software, Natural languages, Laboratories, natural language information, Educational technology, information retrieval, Open source software, triager, execution information, duplicate bug report, Computer bugs, Software quality, Eclipse bug repository, Computer science education, Testing]
Mining framework usage changes from instantiation code
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Framework evolution may break existing users, which need to be migrated to the new framework version. This is a tedious and error-prone process that benefits from automation. Existing approaches compare two versions of the framework code in order to find changes caused by refactorings. However, other kinds of changes exist, which are relevant for the migration. In this paper, we propose to mine framework usage change rules from already ported instantiations, the latter being applications build on top of the framework, or test cases maintained by the framework developers. Our evaluation shows that our approach finds usage changes not only caused by refactorings, but also by conceptual changes within the framework. Further, it copes well with some issues that plague tools focusing on finding refactorings such as deprecated program elements or multiple changes applied to a single program element.
[Software maintenance, Automation, refactorings, data mining, framework usage change mining, framework comprehension, evolution, Data mining, software maintenance, Guidelines, error-prone process, Software design, instantiation code, Permission, software reusability, migration, Software systems, Software reusability, Testing, Software engineering]
Recommending adaptive changes for framework evolution
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
In the course of a framework's evolution, changes ranging from a simple refactoring to a complete rearchitecture can break client programs. Finding suitable replacements for framework elements that were accessed by a client program and deleted as part of the framework's evolution can be a challenging task. We present a recommendation system, SemDiff, that suggests adaptations to client programs by analyzing how a framework adapts to its own changes. In a study of the evolution of the Eclipse JDT framework and three client programs, our approach recommended relevant adaptive changes with a high level of precision, and detected non-trivial changes typically undiscovered by current refactoring detection techniques.
[Eclipse JDT framework, Software maintenance, adaptive change recommendation, Humans, mining software repositories, Manuals, client programs, partial program analysis, recommendation system, software evolution, Large-scale systems, refactoring detection techniques, Java, framework evolution, Documentation, Inspection, Application software, software maintenance, origin analysis, Computer science, framework, adaptive changes, historical study, Software engineering, SemDiff]
Four enhancements to automateddistributed system experimentation methods
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Experimentation is an essential tool employed by the developers of software systems, especially distributed systems. In prior work we developed a model-driven framework for automating various experimentation tasks, such as workload generation, and demonstrated that it gives the engineer a cost-effective means to conduct large-scale experiments on distributed testbeds. We have enhanced the methods underlying the framework in four significant ways: (1) increasing the expressiveness of workloads by allowing for conditional and reactive behaviors; (2) supporting the repeatability of experiments through the creation of environment workloads that can control the operational context; (3) enabling the composability of application and environment workloads to obtain a broader class of experiments; and (4) extending the scope of experiment management to include control over multiple runs. We use the enhancements to conduct a series of interesting new experiments. Specifically, the enhancements allow us to manipulate a fixed-wired testbed so that it simulates a mobile-wireless environment, and to selectively and maliciously inject faults into a system.
[experiment automation, System testing, Educational institutions, Distributed computing, Environmental management, Computer science, software system development, emulab, automated distributed system experimentation methods, planetlab, Automatic testing, Permission, model-driven framework, Software systems, distributed systems, software engineering, Large-scale systems, Informatics]
Automatic generation of software behavioral models
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Dynamic analysis of software systems produces behavioral models that are useful for analysis, verification and testing. The main techniques for extracting models of functional behavior generate either models of constraints on data, usually in the form of Boolean expressions, or models of interactions between components, usually in the form of finite state machines. Both data and interaction models are useful for analyzing and verifying different aspects of software behavior, but none of them captures the complex interplay between data values and components interactions. Thus related analysis and testing techniques can miss important information. In this paper, we focus on the generation of models of relations between data values and component interactions, and we present GK-tail, a technique to automatically generate extended finite state machines (EFSMs) from interaction traces. EFSMs model the interplay between data values and component interactions by annotating FSM edges with conditions on data values. We show that EFSMs include details that are not captured by either Boolean expressions or (classic) FSM alone, and allow for more accurate analysis and verification than separate models, even if considered jointly.
[Software testing, System testing, Law, program testing, program verification, software verification, Data mining, finite state machines, software system, component interaction, Information analysis, EFSM model, Permission, Informatics, GK-tail, software behavioral model, software testing, model synthesis, dynamic analysis, extended finite state machine, gk-tail, data values, software analysis, Automata, Software systems, Legal factors]
Detecting model inconsistency through operation-based model construction
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Nowadays, large-scale industrial software systems may involve hundreds of developers working on hundreds of different but related models representing parts of the same system specification. Detecting and resolving structural inconsistencies between these models is then critical. In this article we propose to represent models by sequences of elementary construction operations, rather than by the set of model elements they contain. Structural and methodological consistency rules can then be expressed uniformly as logical constraints on such sequences. Our approach is meta-model independent, allowing us to deal with consistency between different models whatever their kind. We have validated our approach by building a Prolog engine that detects violations of structural and methodological constraints specified on UML 2.1 models and requirement models. This engine has been integrated into two contemporary UML-based modelling environments, Eclipse EMF and rational software architect (RSA).
[operation-based model construction, Unified modeling language, rational software architect, system specification, formal specification, consistency, Engines, requirement models, Mars, UML 2.1 models, formal verification, industrial software system, Permission, model, Large-scale systems, PROLOG, UML-based modelling environment, Unified Modeling Language, Buildings, meta-model, Stress, meta-model independent, Computer industry, Software systems, Inference algorithms, model inconsistency, logic, Prolog engine]
The influence of organizational structure on software quality
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Often software systems are developed by organizations consisting of many teams of individuals working together. Brooks states in the Mythical Man Month book that product quality is strongly affected by organization structure. Unfortunately there has been little empirical evidence to date to substantiate this assertion. In this paper we present a metric scheme to quantify organizational complexity, in relation to the product development process to identify if the metrics impact failure-proneness. In our case study, the organizational metrics when applied to data from Windows Vista were statistically significant predictors of failure-proneness. The precision and recall measures for identifying failure-prone binaries, using the organizational metrics, was significantly higher than using traditional metrics like churn, complexity, coverage, dependencies, and pre-release bug measures that have been used to date to predict failure-proneness. Our results provide empirical evidence that the organizational metrics are related to, and are effective predictors of failure-proneness.
[organizational complexity, pre-release bug measures, software systems, product development process, developers, Programming, Reliability engineering, software quality, organizational structure, software mining, Software metrics, Engineering management, Windows Vista, Software measurement, Mythical Man Month book, software performance evaluation, Quality management, failures, code churn, software development management, Educational institutions, empirical studies, Software quality, Software systems, failure-prone binaries, organisational aspects, Software engineering]
Predicting defects using network analysis on dependency graphs
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
In software development, resources for quality assurance are limited by time and by cost. In order to allocate resources effectively, managers need to rely on their experience backed by code complexity metrics. But often dependencies exist between various pieces of code over which managers may have little knowledge. These dependencies can be construed as a low level graph of the entire system. In this paper, we propose to use network analysis on these dependency graphs. This allows managers to identify central program units that are more likely to face defects. In our evaluation on Windows Server 2003, we found that the recall for models built from network measures is by 10% points higher than for models built from complexity metrics. In addition, network measures could identify 60% of the binaries that the Windows developers considered as critical-twice as many as identified by complexity metrics.
[Costs, Windows developers, defects prediction, software development, Predictive models, dependency graph, software quality, resources allocation, Network servers, Quality assurance, central program units, Engineering management, defect prediction, quality assurance, Permission, network analysis, code complexity metrics, Resource management, Software measurement, dependency graphs, windows server 2003, Quality management, Software engineering, software metrics]
Open source software peer review practices
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Peer review is seen as an important quality assurance mechanism in both industrial development and the open source software (OSS) community. The techniques for performing inspections have been well studied in industry; in OSS development, peer reviews are less well understood. We examine the two peer review techniques used by the successful, mature Apache server project: review-then-commit and commit-then-review. Using archival records of email discussion and version control repositories, we construct a series of metrics that produces measures similar to those used in traditional inspection experiments. Specifically, we measure the frequency of review, the level of participation in reviews, the size of the artifact under review, the calendar time to perform a review, and the number of reviews that find defects. We provide a comparison of the two Apache review techniques as well as a comparison of Apache review to inspection in an industrial project. We conclude that Apache reviews can be described as (1) early, frequent reviews (2) of small, independent, complete contributions (3) conducted asynchronously by a potentially large, but actually small, group of self-selected experts (4) leading to an efficient and effective peer review technique.
[Performance evaluation, quality assurance mechanism, public domain software, email discussion, Frequency measurement, software quality, Open source software, Quality assurance, frequent reviews, industrial project, commit-then-review, mining software repositories (email), inspection, open source software, Calendars, Inspection, industrial development, Size measurement, Time measurement, Apache server project, peer review, peer review practices, Computer industry, Particle measurements, review-then-commit, version control repositories, software metrics]
Ahaa --agile, hybrid assessment method for automotive, safety critical smes
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
The need for software is increasingly growing in the automotive industry. Software development projects are, however, often troubled by time and budget overruns, resulting in systems that do not fulfill customer requirements. Both research and industry lack strategies to combine reducing the long software development lifecycles (as required by time-to-market demands) with increasing the quality of the software developed. Software process improvement (SPI) provides the first step in the move towards software quality, and assessments are a vital part of this process. Unfortunately, software process assessments are often expensive and time consuming. Additionally, they often provide companies with a long list of issues without providing realistic suggestions. The goal of this paper is to describe a new low-overhead assessment method that has been designed specifically for small-to-medium-sized (SMEs) organisations wishing to be automotive software suppliers. This assessment method integrates the structured-ness of the plan-driven SPI models of Capability Maturity Model Integration (CMMI) and Automotive SPICEtrade with the flexibleness of agile practices.
[Capability Maturity Model Integration, ISO standards, production engineering computing, Programming, small-to-medium-sized organisations, Software safety, software quality, Automotive engineering, software process assessments, software development lifecycles, software process improvement, Permission, CMMI, agile practices, Capability maturity model, Capability Maturity Model, automotive industry, automotive software suppliers, safety-critical, software development management, automotive spice, assessment methods, small-to-medium enterprises, cmmi, safety-critical SME, Ice, automobile industry, automotive SPICE, software development projects, agile hybrid assessment method, Software quality, Computer industry, Software engineering, customer requirements]
Models for model's sake
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
In automotive software and system design, explicit system and especially software models have found their way into the development process. This paper try to give an overview for what such models have so-far been used and which advantages they brought to vehicle manufacturers and suppliers. Another focus of this paper is the comparison to functional models which are already used in the automotive industry to define control algorithms and function implementation. In many cases too strong analogies have been seen between the existing functional control algorithm models and the new system models - leading to suboptimal development processes and tools. This paper therefore try to outline differences between these model types. Finally, a synthesis between functional, system, and software models was sketched.
[tooling, Industrial control, production engineering computing, Programming, functional control algorithm, Automotive engineering, Vehicles, software architecture, Software architecture, uml, Permission, Mathematical model, vehicle manufacturers, suboptimal development processes, automotive software engineering, system engineering, automotive industry, explicit system, automotive software, system design, automobile industry, vehicle suppliers, autosar, Differential equations, Software systems, automobile manufacture, model-based development, Software engineering]
Time-bounded adaptation for automotive system software
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Software is increasingly deployed in vehicles as demand for new functionality increases and cheaper and more powerful hardware becomes available. Likewise, emerging wireless communication protocols allow the integration of new software into vehicles, thereby enabling time-bounded adaptive response to changes that occur in mobile environments. Examples of time-bounded adaptation include adaptive cruise control and the dynamic integration of location-aware services within fixed time bounds. This paper provides three contributions to the study of time-bounded adaptation for automotive system software. First, we categorise automotive systems with respect to requirements for dynamic software adaptation. Second, we define a taxonomy that captures various dimensions of dynamic adaptation in emerging automotive system software. Third, we use this taxonomy to analyse existing research projects in the automotive domain. Our analysis shows that although time-bounded synchronisation of applications and data is a key requirement for next-generation automotive systems, it is not adequately covered by existing work.
[time-bounded adaptive response, Wireless application protocol, Taxonomy, automotive system software, dynamic integration, taxonomy, Vehicle dynamics, Adaptive control, time-bounded adaptation, adaptive cruise control, Automotive engineering, Vehicles, Wireless communication, Programmable control, software architecture, mobile computing, automotive software systems, dynamic adaptation, wireless communication protocols, Hardware, System software, location-aware services]
Mulit-level system integration based on AUTOSAR
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
The design of distributed embedded real-time system is a challenging task. Besides solving the control-engineering issues, one has to consider real-time scheduling, reliability and production requirements w.r.t. production cost of the electronic control unit (ECU). This has a considerable impact on the employed software design techniques. These design techniques are well known in the automotive software industry, but are applied with different flavors at each vehicle manufacturer and their suppliers. This situation has changed considerably with the results of the AUTOSAR development partnership, which unifies the flavors of automotive software design. Automotive software design is embedded in the so-called V-Cycle of embedded automotive system development[1]. It starts with the requirements analysis which results later on in a model of the control algorithm. The control algorithm is tested against a vehicle model and establishes the topmost level in system integration. The second system integration level is the adaptation of the control algorithm to be run on a rapid-prototyping system. The rapid-prototyping system is integrated into an existing E/Earchitecture. The E/E-architecture consists of the ECUs connected by networks like CAN or FlexRay and gateways. Sensors- and actuators being used by several control algorithms are coupled to an ECU, which might propagate signals to other ECUs via a vehicle network. From the software point of view, the controlalgorithm has now to respect real-time scheduling and the quantization of the sensor- and actuator signals, no matter whether these signals are generated on the rapid-prototyping system or exchanged via the bus with other ECUs. Further development steps in the V-cycle are the software implementation, the ECU- and the network integration. The integrated ECUs and networks are tested against vehicle models running on Hardware-in-the-loop (HiL) systems. If this works fine, the ECUs are integrated in the real vehicle for calibration. The software implementation and the ECU integration are deeply influenced by AUTOSAR. AUTOSAR is a development partnership of all stakeholders in the automotive software development (e.g. vehicle manufacturers and their suppliers) which unifies several software implementation techniques[2]. It describes a common ECU software architecture[3] consisting of configurable basic software modules (BSW), a runtime environment (RTE) and a software component description[4]. The software component description describes the interfaces for dataexchange as well as the access points for the RTE. The basic idea of consisting of interconnected software components which are later mapped to an E/E-architecture. Currently, the VFB structure of AUTOSAR software architectures is mainly driven by the next generation E/E-architectures. The VFB structure forms the third system integration level. The next integration step into an AUTOSAR environment is the integration of the rapid-prototyping tested control algorithm to AUTOSAR software components. Since most VFB descriptions use fixed-point interfaces, the control algorithm has to be transformed to fixed-point arithmetic. If the control algorithm is modelled in tools like ASCET, this conversion can be achieved by code-generation. The same holds true for control algorithms already being used in E/E-architectures without an AUTOSAR software architecture. The VFB-description with the control-algorithms forms the fourth system integration level, which can be simulated with plant-models on a PC by tools like INTECRIO-VP. The fifth system integration level is given by the mapping process as defined in the AUTOSAR methodology[5] and requires the configuration of the RTE and the BSW modules for a single ECU. At this integration level, one can perform HiL testing and calibration in the same way as for non-AUTOSAR systems. Several evaluation projects, e.g. [6] and [7], have shown that the multi-level integration approach is feasible to guide the configuration capabilities of AUTOSAR software architectures.
[Real time systems, System testing, automotive software design, virtual functional bus, Automotive engineering, Vehicles, Embedded software, electronic control unit, Software design, Software architecture, software component description, automatic code-generation, embedded systems, hardware-in-the-loop systems, Production, runtime environment, multi-level system integration, Job shop scheduling, rapid prototyping, automobile industry, AUTOSAR, embedded software, mechanical engineering computing, configurable basic software modules, systems analysis, Channel bank filters, automotive, distributed embedded real-time system]
Asam odx
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
In this paper, we outline a possible formalization of the semantics of the XML-based ODX language. ODX is a substantial part of the emerging automotive ASAM MCD standard, which describes a middleware layer between offboard diagnosis applications and onboard diagnosis services of electronic control units (ECUs) of cars. The contribution of our work is threefold: Firstly, a consequent application of our results can contribute to a well-structured development process in the automotive diagnosis domain. We are currently showing this in practice as part of our ongoing cooperation with the diagnosis department of AUDI AG. Secondly, our proposition is the first step towards guaranteed standard-conformity of implementations of the MCD run-time system, also specified by the standard and strongly depending on ODX and its semantics. Last but not least, the paper can serve as an encouraging example of the application of formal methods in practice.
[Performance evaluation, electronic control units, Protocols, Standardization, Software performance, AUDI AG, automotive ASAM MCD standard, Automotive engineering, Vehicles, formal semantics, asam odx, onboard diagnosis services, automotive diagnosis, Permission, middleware, XML-based ODX language, offboard diagnosis applications, electronic engineering computing, Application software, programming language semantics, Middleware, XML, automotive electronics, formal methods, middleware layer, automotive, Artificial intelligence]
A language for advanced protocol analysis in automotive networks
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
The increased use and interconnection of electronic components in automobiles has made communication behavior in automotive networks drastically more complex. Both communication designs at application level and complex communication scenarios are often under-specified or out of scope of existing analysis techniques. We extend traditional protocol analyzers in order to capture communication at the level of abstraction that reflects application design and show that the same technique can be used to specify, monitor and test complex scenarios. We present CFR (channel filter rule) models, a novel approach for the specification of analyzers and a domain-specific language that implements this approach. From CFR models, we can fully generate powerful analyzers that extract design intentions, abstract protocol layers and even complex scenarios from low level communication data. We show that three basic concepts (channels, filters and rules) are sufficient to build such powerful analyzers and identify possible areas of application.
[Protocols, telecommunication channels, Electronic components, communication data, Automotive engineering, automobile, Filters, automotive network, electronic component, automotive systems engineering, automotive engineering, protocols, Monitoring, Power generation, Testing, automotive components, communication design, complex communication, computer networks, filtering theory, LAN interconnection, protocol analysis, Automobiles, Domain specific languages, communication behavior, domain-specific language, channel filter rule, protocol specifications]
Clone detection in automotive model-based development
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Model-based development is becoming an increasingly common development methodology. In important domains like embedded systems already major parts of the code are generated from models specified with domain-specific modelling languages. Hence, such models are nowadays an integral part of the software development and maintenance process and therefore have a major economic and strategic value for the software-developing organisations. Nevertheless almost no work has been done on a quality defect that is known to seriously hamper maintenance productivity in classic code-based development. This paper presents an approach for the automatic detection of clones in large models as they are used in model-based development of control systems. The approach is based on graph theory and hence can be applied to most graphical data-flow languages. An industrial case study demonstrates the applicability of our approach for the detection of clones in Matlab/Simulink models that are widely used in model-based development of embedded systems in the automotive domain.
[Software maintenance, software maintenance productivity, graph theory, Programming, model clone, software quality, matlab/simulink, Automotive engineering, automatic clone detection, Matlab/Simulink model, Embedded system, embedded systems, automotive engineering, Control system synthesis, Automatic control, embedded system, Mathematical model, control system, graphical data-flow language, Productivity, data-flow, clone detection, software development, program diagnostics, Cloning, Graph theory, domain-specific modelling language, software maintenance, automotive model-based development, classic code-based development, software reusability, software-development organisation, quality defect]
Formal verification of an automotive scenario in service-oriented computing
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
We report on the successful application of academic experience with formal modelling and verification techniques to an automotive scenario from the service-oriented computing domain. The aim of this industrial case study is to verify a priori, thus before implementation, certain design issues. The specific scenario is a simplified version of one of possible new services for car drivers to be provided by the in-vehicle computers.
[Costs, Roads, Vehicle driving, Unified modeling language, car drivers, Service oriented architecture, automotive scenario, in-vehicle computers, Automotive engineering, software architecture, service-oriented computing, mobile computing, formal verification, model checking, formal modelling, automotive engineering, Permission, automotive systems, Internet telephony, Formal verification, Software engineering]
Analyzing medical processes
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
This paper shows how software engineering technologies used to define and analyze complex software systems can also be effective in detecting defects in human-intensive processes used to administer healthcare. The work described here builds upon earlier work demonstrating that healthcare processes can be defined precisely. This paper describes how finite-state verification can be used to help find defects in such processes as well as find errors in the process definitions and property specifications. The paper includes a detailed example, based upon a real-world process for transfusing blood, where the process defects that were found led to improvements in the process.
[blood transfusion, Natural languages, Medical services, Human factors, human-intensive processes, finite-state verification, Blood, Computer science, Hospitals, formal verification, model checking, Engineering management, Permission, Software systems, software engineering, software engineering technologies, healthcare processes, medical computing, health care, property specifications, Software engineering, medical processes]
Rational quality requirements for medical software
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
In this paper we discuss the challenges of software quality for medical software and present some ideas for improving medical software quality requirements through software engineering methods. We apply the quality requirements engineering method MOQARE to elicit specific quality requirements for an imaginary drug advisory system and report our lessons learned.
[Drugs, medical software, drugs, Medical treatment, Medical services, software quality, Application software, formal specification, rational quality requirement engineering, medical software quality challenges, Diseases, risk analysis, imaginary drug advisory system, medical business processes, formal verification, systems analysis, Software quality, quality requirements, software engineering, medical computing, Medical diagnostic imaging, Biomedical imaging, Software engineering, Biomedical engineering]
Supporting requirements engineering for medical products
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
The usability and, more generally, the overall user-perceived quality of medical devices is an important aspect, which is often insufficiently addressed in the corresponding system development activities. Fortunately, the development of new standards like IEC/DIN EN 60601-1-6 is strengthening the focus on usability/user acceptance issues. This paper argues for the need to consider usability and user acceptance issues in early system development phases like the requirements engineering phase. In this paper, an empirically validated new quality model for user satisfaction is described first. The importance of the quality aspects included in this quality model for the medical domain is outlined. Then, the new quality model is used to develop a systematic methodology called appraisal and measurement of user satisfaction (AMUSE), which allows gathering user acceptance information early in system development. The key activities of the AMUSE methodology and typical application scenarios are shown. Further on, the application of AMUSE, which was developed in close cooperation with Siemens Corporate Technology, is demonstrated in a real-world scenario at Siemens Audiologische Technik, a line of business of Siemens Medical Solutions. At the end, the first lessons learned from the application of the AMUSE methodology in this medical domain are discussed.
[quality measurement, early system development, Siemens Corporate Technology, requirements prioritization, user-perceived quality, Appraisal, appraisal and measurement of user satisfaction, medical products, Siemens Audiologische Technik, requirements engineering, IEC standards, Engineering management, product management, Disaster management, Innovation management, product innovation, Standards development, Software measurement, Usability, medical computing, Biomedical engineering, Quality management, user satisfaction]
Experiences with Mirth
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Integration engines are a crucial piece of the health care information technology puzzle. Health care organizations like hospitals and clinics are faced with vast amounts of data and a slew of interchange standards and protocols when addressing the issue of exchanging data between information systems. This paper describes our experience in developing Mirth, a popular open source health care messaging integration engine. Based on a unique client-server and enterprise service bus hybrid architecture, Mirth supports the development of interfaces for moving data between two or more systems. We describe the Mirth architecture in detail and discuss our experiences through several case studies that demonstrate its use. We also provide our insights gained in designing the Mirth architecture through several lessons learned.
[enterprise service bus hybrid architecture, client-server systems, Protocols, information technology, public domain software, Biomedical informatics, Medical services, Engines, Information systems, open source health care messaging integration engine, software architecture, Software architecture, Hospitals, health care integration, Standards organizations, health care information technology, Computer architecture, information systems, Mirth architecture, health care, client-server system, Software engineering, middleware]
Developing an architecture of a knowledge-based electronic patient record
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Medicine as knowledge-intensive domain has been the subject of various approaches of computer-based knowledge management. Most of them concentrated on the design and implementation of expert systems for clinical decision support. Today, medical knowledge bases are implemented for various purposes, including encyclopedic sources of information for clinicians. We present a prototypical development of architecture for an electronic patient record which structurally depends on such an encyclopedic representation and is therefore knowledge-based. Using the KADS approach for knowledge engineering, three modeling steps and architectural parts could be identified, definition of basic concepts, the structural knowledge base model, and the interactive process of knowledge instantiation which constitutes clinical documentation. Furthermore, we present an analysis of possible benefits of a knowledge-based electronic patient record in health care as well as in adjacent fields.
[Knowledge engineering, benefits, knowledge engineering, expert systems, Medical services, knowledge-intensive domain, knowledge management, software architecture, electronic patient record, Computer architecture, kads, Permission, health care, Paramagnetic resonance, Navigation, clinical documentation, encyclopedic sources, Medical treatment, Documentation, Knowledge management, medical information systems, Medical expert systems, decision support systems, encyclopedic representation, clinical decision support, computer-based knowledge management, knowledge-based electronic patient record, medical knowledge, structural knowledge base model, knowledge instantiation]
Design and implementation of the software architecture for a 3-D reconstruction system in medical imaging
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
The design and implementation of the reconstruction system in medical X-ray imaging is a challenging issue due to its immense computational demands. In order to ensure an efficient clinical workflow it is inevitable to meet high performance requirements. Hence, the usage of hardware acceleration is mandatory. The software architecture of the reconstruction system is required to be modular in a sense that different accelerator hardware platforms are supported and it must be possible to implement different parts of the algorithm using different acceleration architectures and techniques. This paper introduces and discusses the design of a software architecture for an image reconstruction system that meets the aforementioned requirements. We implemented a multi-threaded software framework that combines two software design patterns: the pipeline and the master/worker pattern. This enables us to take advantage of the parallelism in off-the-shelf accelerator hardware such as multi-core systems, the Cell processor, and graphics accelerators in a very flexible and reusable way.
[Software algorithms, patterns, image reconstruction, 3-d reconstruction, hardware acceleration, Image reconstruction, X-ray imaging, parallel programming, clinical workflow, software architecture, hardware abstraction layer, Software design, Software architecture, 3D reconstruction system, software design and architecture, Computer architecture, Three dimensional displays, Hardware, Acceleration, medical image processing, medical imaging, Biomedical imaging, medical X-ray imaging]
Applying model-based testing to healthcare products
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Healthcare software systems are becoming more and more complex since they will be highly integrated to support a wide variety of healthcare workflows (e.g., financial, administration, diagnosis, and treatment). In addition, healthcare software systems will be more safety-critical as they will provide medical decision support for doctors. All of those require systematic and thorough testing of the systems before they are put into use or upgraded. Model-Based Testing (MBT) is an approach to apply formal, explicit system use-models to generate the test cases for testing the system behaviors under different input data sets. This paper describes our experiences in applying this approach to testing some Siemens healthcare software systems. We will report the benefits and challenges in using MBT for testing healthcare software systems.
[Software testing, System testing, Thyristors, program debugging, program testing, Unified modeling language, Medical services, Aerospace industry, Automatic testing, model-based testing, Software systems, Aerospace testing, software engineering, medical decision support, medical computing, Medical diagnostic imaging, healthcare software systems]
Developing a security protocol for a distributed decision support system in a healthcare environment
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
In this paper, we describe the unique security issues involved in healthcare domains. These have been addressed to the needs of the HealthAgents project. In the proposed approach, several levels of security have been provided in accordance with Software Engineering principles, ethical regulations for healthcare data, as well as the security requirements usually raised from the distributed clinical settings. The result is the production of a secure and maintainable Multi-Agent System that enables secure communication, uniform home site authentication, and customised resource access authorisation. A security policy rule scheme has been designed for agent interaction modelling. This separates the functional and non-functional (security) requirements but let security policy constraints integrate into the running of the agents via a unified role notion. Each user/agent can play a function role only when its assigned social rights roles permit the access to resources of various types and geographical locations, as specified in the function role behaviour. The approach is illustrated using a comprehensive secure access case.
[Decision support systems, security protocol, Production systems, ethical regulations, Protocols, Medical services, distributed processing, distributed decision support system, geographical locations, Communication system security, healthcare, Authorization, software engineering, security requirements, health care, Multiagent systems, Data security, HealthAgents project, decision support systems, healthcare data, security model, security of data, healthcare environment, Authentication, Software engineering]
Model-based security analysis for mobile communications
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Mobile communication systems are increasingly used in companies. In order to make these applications secure, the security analysis has to be an integral part of the system design and IT management process for such mobile communication systems. This work presents the experiences and results from the security analysis of a mobile system architecture at a large German telecommunications company, by making use of an approach to model-based security engineering that is based on the UML extension UMLsec. The focus lies on the security mechanisms and security policies of the mobile applications which were analyzed using the UMLsec method and tools. Main results of the paper include a field report on the employment of the UMLsec method in an industrial telecommunications context as well as indications of its benefits and limitations.
[telecommunication security, Unified modeling language, mobile telecommunication systems, Mobile communication, model-based software engineering, Communication system security, System analysis and design, Information analysis, UML extension, mobile communications, uml, Permission, mobile system architecture, model-based security engineering, umlsec, industrial telecommunications context, Application software, UMLsec, mobile communication, Communication industry, Information security, IT management process, German telecommunications company, model-based security analysis, Software engineering]
Experience applying the SPIN model checker to an industrial telecommunications system
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Model checking has for years been advertised as a way of ensuring the correctness of complex software systems. However, there exist surprisingly few critical studies of the application of model checking to industrial-scale software systems by people other than the model checker's own authors. In this paper we report our experience in applying the Spin model checker to the validation of the failover protocols of a commercial telecommunications system. While we conclude that model checking is not yet ready for such applications, we find that current research in the model checking community is working to address the difficulties we encountered.
[Algorithm design and analysis, failover protocol validation, experience report, Application software, Toy industry, teleconferencing, industrial telecommunications system, Concurrent computing, SPIN model checker, complex software system, formal verification, model checking, Communication industry, Failure analysis, formal methods, electronic conferencing system, Computer industry, Software systems, Mathematical model, protocols, Testing]
WS-AMUSE - web service architecture for multimedia services
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Recently, a move from traditional, network specific multimedia services to IP-based solutions could be observed. Although many of these applications have similar requirements and address the same issues, individual solutions based on specialized protocols are commonly used. This specialization prohibits the extraction and reuse of common services and hinders the interoperability between services and the integration with external components. A promising approach to overcome these disadvantages is the adoption of the service-oriented paradigm in communication protocols and a modularization into cooperating services. In this paper, we present a generic framework for multimedia applications consisting of a set of reusable Web service components, a modeling language based on finite state automata and a compiler. The results of a BPEL based prototypical implementation of a Voice-over-IP application show that the service oriented approach and the automaton based modeling language can satisfy the above mentioned criteria and ease application development through a higher level of abstraction. On the other hand our benchmarks indicate that current Web service technologies can lead to an insufficient performance, depending on the application scenario. Possible solutions to circumvent these deficiencies are presented at the end of the paper.
[Protocols, soa, web services, automaton based modeling language, vod, Multimedia communication, multimedia computing, multimedia application, reusable Web service component, Web and internet services, Prototypes, voip, Permission, modularization, bpel, Informatics, Web service technology, Service oriented architecture, finite state automata, communication protocols, voice-over-IP application, interoperability, network specific multimedia services, Web services, transport protocols, Automata, specialized protocols, Web service architecture, generic framework, IP-based solution, Internet telephony]
An Industrial Case Study of Customizing Operational Profiles Using Log Compression
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Large customers commonly request on-site capacity testing before upgrading to a new version of a mission critical telecom application. Customers fear that the new version cannot handle their current workload. These on-site engagements are costly and time consuming. These engagements prolong the upgrade cycle for products and reduce the revenue stream of rapidly growing companies. We present an industrial case study for a lightweight simple approach for customizing the operational profile for a particular deployment. The approach flags sequences of repeated events out of millions of events in execution logs. A performance engineer can identify noteworthy usage scenarios using these flagged sequences. The identified scenarios are used to customize the operational profile. Using a customized profile for performance testing alleviates customer's concerns about the performance of a new version of an application, and results in more realistic performance and reliability estimates. The simplicity of our approach ensures that customers can easily grasp the results of our analysis over other more complex analysis approaches. We demonstrate the feasibility and applicability of our approach by customizing the operational profile of an enterprise telecom application.
[Software testing, program testing, software development, Mission critical systems, software development management, customized operational profile, mission critical telecom application, Reliability engineering, Telecommunication computing, log compression, product upgrade cycle, capacity management (computers), Communication industry, enterprise telecom application, software performance testing, Permission, Benchmark testing, Computer industry, performance engineer, software performance evaluation, Software engineering, Capacity planning, on-site capacity testing]
Interval Quality: Relating Customer-Perceived Quality to Process Quality
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
We investigate relationships among software quality measures commonly used to assess the value of a technology, and several aspects of customer perceived quality measured by interval quality (IQ): a novel measure of the probability that a customer will observe a failure within a certain interval after software release. We integrate information from development and customer support systems to compare defect density measures and IQ for six releases of a major telecommunications system. We find a surprising negative relationship between the traditional defect density and IQ. The four years of use in several large telecommunication products demonstrates how a software organization can control customer perceived quality not just during development and verification, but also during deployment by changing the release rate strategy and by increasing the resources to correct field problems rapidly. Such adaptive behavior can compensate for the variations in defect density between major and minor releases.
[Density measurement, software development management, Programming, Telecommunication control, software quality, Customer satisfaction, Software quality, Lead, Permission, customer-perceived quality, software organization, Software measurement, Business, Quality management, interval quality, process quality, software metrics]
Improving the handsets network test process via DMAIC concepts
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
The wireless network evolution has allowed that the handset technology provides a broad and new set of resources and facilities to their users. However, this evolution is also increasing the number and complexity of testing prior to handsets deployment, so that there is a need to apply methodologies to ensure the quality of the test process while it evolves. This paper relates how the DMAIC framework could be used as an option to ensure and improve the quality of handsets network test processes. DMAIC is a six sigma framework based on measures and statistical analysis, which has commonly been applied during several stages of software development. Our focus, in this paper, is on the definition of the DMAIC phases and how this framework could be useful to stress problems and lead the effort of tests corrections and improvements.
[wireless network evolution, Statistical analysis, software development, Quality of service, six sigma framework, Programming, Stress, DMAIC, Wireless networks, six sigma (quality), handsets network test process, Permission, Telephone sets, handsets network, metrics, Software measurement, statistical analysis, Testing, Six sigma, mobile handsets, process quality]
3-step knowledge transition
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Software engineering is developing very fast. To keep up with the changes, software companies need effective methods of knowledge transfer. In the paper a 3-step approach to knowledge transfer, called Technical Drama, is presented. The paper is focused on transferring knowledge concerning architecture evaluation, but the approach could also be applied to transferring knowledge concerning inspections, testing etc. It is claimed in the paper that the Technical Drama can be useful in the industrial context (two case studies are described) as well as at university (then a kind of software studio is required).
[computer science education, industrial context, Inspection, Programming, architecture evaluation, Knowledge transfer, software architecture, atam, knowledge transfer, Software architecture, Education, Technical Drama, Computer architecture, university education, Permission, Computer industry, knowledge transition, software engineering, Software engineering, Testing]
From programming to modeling
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Distributed Software Engineering (DSE) concepts in Computer Science (or Engineering) Degrees are commonly introduced using a hands-on approach mainly consisting of teaching a particular distributed and component-based technology platform (such as Java Enterprise Edition or Microsoft .NET) and proposing the students to develop a small distributed software application with it. Though this approach provides the students with some relevant practical knowledge, we believe that it is not the most appropriate way of teaching all the concepts and particularities of DSE. Thus, in this paper we report on our experience of redesigning an initial DSE course following a model-based approach. By raising the level of abstraction we gained modularity, separation of concerns and technology independence, while making the course evolve according to the latest trends in software development methods.
[component-based technology platform, education, Unified modeling language, model-based approach, Programming, computer science degree, teaching, hands-on approach, Software architecture, Education, uml, Computer architecture, software engineering, virtual university, Educational programs, computer science education, object-oriented programming, distributed software engineering, Object oriented modeling, component-based software development, Application software, Computer science, educational courses, distributed software engineering course, odp, Software engineering]
A study of student strategies for the corrective maintenance of concurrent software
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Graduates of computer science degree programs are increasingly being asked to maintain large, multi-threaded software systems; however, the maintenance of such systems is typically not well-covered by software engineering texts or curricula. We conducted a think-aloud study with 15 students in a graduate-level computer science class to discover the strategies that students apply, and to what effect, in performing corrective maintenance on concurrent software. We collected think-aloud and action protocols, and annotated the protocols for a number of behavioral attributes and maintenance strategies. We divided the protocols into groups based on the success of the participant in both diagnosing and correcting the failure. We evaluated these groups for statistically significant differences in these attributes and strategies. In this paper, we report a number of interesting observations that came from this study. All participants performed diagnostic executions of the program to aid program comprehension; however, the participants that used this as their predominant strategy for diagnosing the fault were all unsuccessful. Among the participants that successfully diagnosed the fault and displayed high confidence in their diagnosis, we found two commonalities. They all recognized that the fault involved the violation of a concurrent-programming idiom. And, they all constructed detailed behavioral models (similar to UML sequence diagrams) of execution scenarios. We present detailed analyses to explain the attributes that correlated with success or lack of success. Based on these analyses, we make recommendations for improving software engineering curriculums by better training students how to apply these strategies effectively.
[Software maintenance, corrective maintenance, Protocols, fault diagnosis, Unified modeling language, concurrent programming, Software performance, think-aloud method, training, Fault diagnosis, think-aloud protocols, multithreaded software systems, Permission, computer science degree programs, program comprehension, computer science education, student training, multi-threading, program diagnostics, Maintenance engineering, diagnostic executions, software maintenance, action protocols, student strategies, Computer science, concurrent software, concurrent-programming idiom, concurrency control, Software systems, software engineering curriculums, Software engineering]
Best practices in extreme programming course design
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Teaching (and therefore learning) extreme programming (XP) in a university setting is difficult because of course time limitations and the soft nature of XP that requires first-hand experience in order to see and really learn the methods. For example, iterations are either shorter or fewer than appropriate. In this paper we present the properties to tune when designing an eXtreme programming course. These are the properties we gathered by conducting three XP labs as part of our software engineering teaching. Within this paper we describe our set-up as well as the important properties. Lecturers and teachers can use this property system and combine it with their own constraints in order to derive a better XP lab for their curriculum.
[Educational programs, computer science education, Computational modeling, software engineering teaching, Laboratories, extreme programming course design, agile development, Best practices, Programming profession, extreme programming, Information science, Software design, Management information systems, software engineering, Computer science education, Software engineering, lab design]
Using the inverted classroom to teach software engineering
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
An inverted classroom is a teaching environment that mixes the use of technology with hands-on activities. In an inverted classroom, typical in-class lecture time is replaced with laboratory and in-class activities. Outside class time, lectures are delivered over some other medium such as video on-demand. In a three credit hour course for instance, contact hours are spent having students actively engaged in learning activities. Outside of class, students are focused on viewing 3-6 hours of lectures per week. Additional time outside of class is spent completing learning activities. In this paper we present the inverted classroom model in the context of a software engineering curriculum. The paper motivates the use of the inverted classroom and suggests how different courses from the Software Engineering 2004 Model Curriculum Volume can incorporate the use of the inverted classroom. In addition, we present the results of a pilot course that utilized the inverted classroom model at Miami University and describe courses that are currently in process of piloting its use.
[computer science education, podcasting, Collaborative software, Laboratories, Digital audio broadcasting, software engineering curriculum, Computer science, Portable media players, Production, Permission, software engineering, Computer science education, computer aided instruction, technology in education, inverted classroom, Software engineering, Context modeling, inverted classroom model]
A teamwork-based approach to programming fundamentals with scheme, smalltalk &#x00026; java
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
In October 2004 the University of Lugano in southern Switzerland established a new faculty of informatics. Its founding principles are innovation in teaching and faculty participation in the research community. With respect to teaching, students spend mornings attending lectures and afternoons in an Atelier designed to support interaction both among students and with the instructors. In teaching the first year "Programming Fundamentals" courses, we took advantage of the clean slate nature of the faculty to introduce innovative teaching elements. The novel aspects include our use of Scheme, Smalltalk, and Java, our combination of individual, pair and group projects and the integration of expert lectures to introduce useful, but slightly orthogonal elements at key points in the semester. Our very positive experience is reported along with a discussion of aspects to improve in the future.
[Java, Technological innovation, computer science education, Portable computers, faculty participation, Natural languages, informatics. faculty, teaching, Switzerland, Programming profession, Computer languages, innovative teaching elements, Java programming, Scheme programming, educational courses, Permission, Libraries, Computer science education, Smalltalk programming, Informatics, teaching innovation, programming fundamentals courses, programming]
Power through brokering
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Many software engineering projects use open source software tools or components. The project team's active participation in the open source community may be necessary for the team to use the technology. Based on an in-depth field study of industry software engineering project students interacting with an open source community, we find that participation in the community may affect the team's work and learning by strengthening the power of the broker between the team and the community. We outline pitfalls and benefits of having student teams acquire development-related knowledge from open source communities. The findings are relevant to the organization and supervision of software engineering student projects interacting with open source communities.
[computer science education, project management, communities of practice, public domain software, Project management, Human factors, Open source software, Programming profession, Information science, open source, FLOSS, Computer industry, Particle measurements, software engineering education, floss, software engineering, Computer science education, Springs, software engineering student projects, open source community, Software engineering]
Design patterns
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
In computer science curricula the two areas programming and software engineering are usually separated. In programming students learn an object oriented language and then deepen their knowledge in other languages, algorithms and data structures. On the other hand software engineering starts with discussing processes and then addresses topics like requirements engineering, software design and software architectures. Design patterns are on the border of these two areas and can be covered from both sides: either as an advanced programming course or as an application of software design and micro architectures. In this paper we present courses on design patterns and on software design which try to bridge this gap.
[computer science education, object-oriented programming, software design, software architectures, Data structures, object oriented language, Application software, software engineering curriculum, Computer science, Bridges, Design engineering, software architecture, Software design, requirements engineering, Software architecture, systems analysis, design patterns, Computer architecture, object-oriented languages, software engineering, computer science curricula, Object oriented programming, programming, Software engineering]
Change management
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
sd&amp;m AG, software design &amp; management, has the development and integration of custom built information systems for business critical processes as its area of business. IT consulting with engineering and implementation competence makes it complete. Our clients are major companies and organizations. They want to achieve a competitive edge by implementing custom built solutions. The core competence of the 1.600 person company is the design of IT architectures and the realization of complex projects in a cooperative way with clients. The overall key success factor is the qualification of the team. Continuously to manage is the growth including small acquisitions, the dynamic of technology and the increasing expectations of our clients. For our clients we are innovation partner. Our understanding is not to be the earliest possible adapter for the last hype but being able to differentiate between hype and future value. We use two sources for improvement: project experience and innovation management. Project experience: We have implemented continuous cycles of learning and improvement, project experiences and best practices are collected and evaluated by communities and sd&amp;m Research, enriched by scientific methods together with universities. Best practices certified by our best software architects will be finished to excellent solutions and become part of our organized knowledge, can be distributed by our development platform as a solution pattern or component, influence our training schools, flow into publications and community work and sometimes result in a book. Announcement and distribution is one of the key success factors. Innovation management: Results of our innovation management are in a first phase knowledge with small experience about a new programming language or environment, a new integration product suite or a new way to design application systems and their successful integration into complex application landscapes. In the following phase we go to our market and challenge our clients view on concrete innovation examples. If we find their interest we identify together real small pilot projects. After project end together with the client we evaluate the results compared to what we expected. If it was valuable for one client we go back with our first capabilities and make to steps: (a) building a new community and (b) designing qualification methods. The community works on the topic of interest with more prototypes, cookbooks and infrastructure - we invest. The qualification methods have at least to outputs: how to integrate the content into the standard qualification activities and how to ramp up the experienced part of the team. For really important content in the domain of our core competence we implement so called schools. In every school we educate 20 - 30 software engineers in 4 to 5 days distributed over the whole company. The trainers come from the community. The life cycle of this school type is 1 - 2 years, then the content is established in the standard education. Parallel with the first projects starts the above describe improvement process by systematic evaluation of project experiences in the new arenas. On the other side we work on the design of the new service offering and bring it to the market. This works only with qualified colleagues. It's always the same: fighting for the first reference clients and projects. Effects where we were successful are for example .net, SAP NetWeaver Technology and business intelligence solutions. First class qualified software engineers and architects are necessary to keep this continuous change management alive with the objective to transfer knowledge into capabilities.
[Technological innovation, software engineers, project management, Companies, Educational institutions, project experience, Best practices, change management, Software development management, Software design, Technology management, custom built information systems, Management information systems, management of change, Innovation management, business critical processes, software engineering education, innovation management, software engineering, industrial SE projects, Qualifications]
Metamodel-based tool integration with moflon
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Nowadays, a typical software development process involves many developers which participate in the development process by using a wide variety of development tools. As a consequence, the data representing the project as a whole is distributed over different development tools. For the purpose of consistency, maintainability, and traceability it is an essential task to be aware of the relationships between semantic equivalent data in different tool repositories. The real-time systems Lab at the Technische Universitat Darmstadt performs research in the area of tool and metamodel integration to provide solutions to overcome this gap. In this demonstration we present the metamodeling framework MOFLON that addresses these issues by bringing together the latest OMG standards with graph transformations and triple graph grammars. Using MOFLON, developers can generate code for specific tools needed to perform analysis and transformation on one development tool or to incrementally integrate data of different modeling tools.
[Real time systems, software development process, tool integration, data representation, Unified modeling language, model transformation, Metamodeling, Programming, Guidelines, graph grammars, model-driven software development, Embedded system, MOFLON, Hardware, data structures, software engineering, development tool, software tools, triple graph grammar, triple graph grammars, metacomputing, metamodel-based tool integration, real-time systems, Packaging, Computer industry, Software engineering]
Genie
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Engineering adaptive software is an increasingly complex task. Here, we demonstrate Genie, a tool that supports the modelling, generation, and operation of highly reconfigurable, component-based systems. We showcase how Genie is used in two case-studies: i) the development and operation of an adaptive flood warning system, and ii) a service discovery application. In this context, adaptation is enabled by the Gridkit reflective middleware platform.
[Adaptive systems, reflective middleware, component-based adaptive systems, grid computing, Floods, Design engineering, model driven development, Runtime, Software design, Genie, software tools, service discovery application, middleware, object-oriented programming, Gridkit reflective middleware platform, software generation, dynamic variability, Application software, Middleware, adaptive flood warning system, model-driven engineering, Alarm systems, Model driven engineering, engineering adaptive software, Software engineering]
Global consistency checking of distributed models with TReMer+
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
We present TReMer+, a tool for consistency checking of distributed models (i.e., models developed by distributed teams). TReMer+ works by first constructing a merged model before checking consistency. This enables a flexible way of verifying global consistency properties that is not possible with other existing tools.
[Vocabulary, Automation, distributed model, Merging, global consistency checking, model merging, Containers, distributed development, software maintenance, consistency checking, Computer science, formal verification, TReMer+, Collaboration, Large-scale systems, Software engineering, Graphical user interfaces]
Marama
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
We describe the Marama suite of meta-tools. This Eclipse-based toolset permits rapid specification of notational elements, meta-models, view editors and view-model mappings. It has a novel set of behavioural specification tools for both visual and model level behaviours. An integrated mapping tool provides model transformation and code generation support. The toolset has been applied to several significant application development tasks and has undergone a variety of evaluations.
[view editor, Computer aided software engineering, code transformation, Shape, Marama Eclipse meta-toolset, visual languages, formal specification, program compilers, Computer architecture, domain-specific visual languages, software tools, DSL, meta-model, view-model mapping, Connectors, Computer science, behavioural visual language specification tool, code generation, Character generation, model-driven engineering, meta-tools, Model driven engineering, multiview environment generation, Software tools, programming environments, notational element specification, Context modeling]
Tool support for the navigation in graphical models
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Graphical models are omnipresent in the software engineering field, but most current graphical modeling languages do not scale with the increasing size and complexity of today's systems. The navigation in the diagrams becomes a major problem especially if different aspects of the system are scattered over multiple, only loosely coupled diagrams. In this paper we present the hierarchical navigation capabilities of the Adora modeling tool. The user of this tool can freely control the level of detail in different parts of the model to reduce the size and complexity of the diagrams being displayed. Our fisheye visualization technique makes it possible to integrate all modeling aspects (structure, data, behavior, etc.) in one coherent model while keeping the size and complexity of the diagrams within reasonable limits.
[graphical model navigation, hierarchical network, Unified modeling language, Switches, information visualization, visual languages, computerised navigation, graphical modeling languages, fisheye view, Graphical models, Software design, navigation, complexity reduction, software engineering, software tools, Size control, Informatics, Adora modeling tool, Navigation, Scattering, graphical user interface, visualization technique, Data visualization, graphical models, focus+context, simulation languages, program visualisation, Software engineering]
Using JULE to generate a compliance test suite for the UML standard
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
The Java-UML Lightweight Enumerator (JULE) tool implements a vitally important aspect of the framework for software tool certification - test suite generation. The framework uses UML models as the test inputs for the bounded exhaustive-testing approach. Within a size bound for the metamodel types, JULE enumerates only the set of non-isomorphic models in the form of relational structures. These models are classified into two sets - demonstration and counterexample - using binary decision diagrams (BDDs). The power of JULE lies in its model enumeration and its use of a high-performance grid infrastructure. Hence, JULE efficiently generates a very small test suite while increasing the bound on the input size to the extent that is practical for certification purpose.
[Software testing, bounded exhaustive-testing approach, metamodel, program testing, certification purpose, Design methodology, Unified modeling language, binary decision diagrams, Boolean functions, uml, ocl, Software standards, software tools, test suite generation, Java, Unified Modeling Language, JULE, relational structures, Data structures, Educational institutions, binary decision diagram, Certification, certification, Computer science, test generation, Java-UML lightweight enumerator, Software tools, software tool certification]
Analyzing model evolution
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Model-driven development leads to development processes in which a large number of different versions of models are produced. We present FAME, a tool environment which enables fine-grained analysis of the version history of a model. The tool is generic in the sense that it can work with various model types including UML and domain-specific languages.
[Software maintenance, Unified modeling language, fine-grained analysis, tool environment, History, formal specification, model-driven development, model evolution, Software metrics, software tools, Mathematical model, domain-specific languages, tracing, development process, Domain specific languages, Software development management, configuration management, version history, FAME, UML, Packaging, Software systems, metrics, history analysis, Software engineering]
SEURAT
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
A completed software product is the end result of many decisions that must be made throughout the development lifecycle. Unfortunately, the rationale for these decisions is usually not captured and is therefore lost. The Software Using RATionale (SEURAT) system integrates with the Eclipse Interactive Development Environment to support rationale capture and use. In addition to presenting the rationale to the developer/maintainer as needed, SEURAT also supports requirements traceability and impact assessment.
[software maintainer, inference, Programming, software-using rationale, Displays, Environmental management, Design engineering, software development lifecycle, Prototypes, eclipse interactive development environment, Software prototyping, argumentation, Decision making, software development management, Documentation, software maintenance, traceability, rationale, Software development management, requirement traceability, integrated rationale management, decision making, software product, programming environments, Software engineering]
Adams re-trace
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
In this demonstration we present the traceability recovery tool developed in ADAMS, a fine-grained artefact management system. The tool is based on an information retrieval technique, namely latent semantic indexing, and aims at supporting the software engineer in the identification of traceability links between artefacts of different types. The tool has also been integrated in the Eclipse-based client of ADAMS.
[Software maintenance, program diagnostics, latent semantic indexing, Eclipse-based client, Documentation, information retrieval, Information retrieval, Mathematics, software maintenance, system recovery, Software development management, traceability link recovery tool, fine-grained software artefact management system, information retrieval technique, ir-based traceability recovery, Engineering management, ADAMS re-trace, software engineer, software tools, Software tools, Informatics, programming environments, Indexing, Software engineering]
Clonetracker
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Code clones are generally considered to be an obstacle to software maintenance. Research has provided evidence that it may not always be practical, feasible, or cost-effective to eliminate certain clone groups through refactoring. This paper describes CloneTracker, an Eclipse plug-in that provides support for tracking code clones in evolving software. With CloneTracker, developers can specify clone groups they wish to track, and the tool will automatically generate a clone model that is robust to changes to the source code, and can be shared with other collaborators of the project. When future modifications intersect with tracked clones, CloneTracker will notify the developer, provide support to consistently apply changes to a corresponding clone region, and provide support for updating the clone model. CloneTracker complements existing techniques by providing support for reusing knowledge about the location of clones in source code, and support for keeping track of clones when refactoring is not desirable.
[Software maintenance, Logic programming, software reuse, Collaborative tools, program diagnostics, Cloning, code clone management, History, software maintenance, software evolution, Computer science, tool support, Computer languages, CloneTracker, code clone, refactoring, Writing, software reusability, Software systems, Robustness, software tools, software refactoring, source code analysis, simultaneous editing]
SpyWare
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Our research is driven by the motivation that change must be put in the center, if one wants to understand the complex processes of software evolution. We built a toolset named SpyWare which, using a monitoring plug-in for integrated development environments (IDEs), tracks the changes that a developer performs on a program as they happen. SpyWare stores these first-class changes in a change repository and offers a plethora of productivity-enhancing IDE extensions to exploit the recorded information.
[Visualization, invasive software, visualization, Reverse engineering, change, Time measurement, History, modelling, Programming environments, SpyWare, software evolution, software process improvement, integrated development environment, Packaging, Software systems, change-aware development toolset, program transformation, Informatics, Software tools, Monitoring]
Dynamic round-trip GUI maintenance
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
One difficulty in software maintenance is that the relationship between observed program behavior and source code is not always clear. This is true for the maintenance of graphical user interfaces (GUIs), because user interface code can be scattered across the decomposition of applications. A popular approach to develop and maintain GUIs is to use "What you see is what you get" editors. They allow developers to work directly with a graphical design view instead of scattered source elements. Unfortunately GUI editors are limited by their ability to statically reconstruct dynamic collaborations between objects. In our research we investigate the combination of a hybrid dynamic and static approach to allow for round-trip maintenance of GUIs. Dynamic analysis reconstructs object relationships, providing a concrete context in which maintenance can be performed. Static checking guides the reconciliation between the GUI editors' design view and source. We implemented a prototype IDE plugin and evaluate our approach by applying it to five open source projects.
[Software maintenance, Software prototyping, open source projects, graphical user interfaces, Scattering, source code, Application software, software maintenance, observed program behavior, dynamic round-trip GUI maintenance, User interfaces, Collaborative work, Rendering (computer graphics), Concrete, gui, Performance analysis, maintenance, Graphical user interfaces]
Juzi
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
This paper describes Juzi, a tool for automatic repair of complex data structures. Juzi takes a Java class representing the data structure as well as a predicate method that specifies the structural integrity constraints as inputs. Juzi instruments its inputs and generates a new Java class which behaves similarly to the original class, yet automatically repairs itself when the structural integrity constraints are violated. Juzi implements a novel repair algorithm. Given a structure that violates its integrity constraints, Juzi performs a systematic search based on symbolic execution to repair the structure, i.e., mutate it such that the resulting structure satisfies the given constraints. Experiments on structures ranging from library classes to standalone applications, show that Juzi repairs complex structures while enabling programs to recover from erroneous executions caused by data structure corruptions.
[Java, Instruments, Maintenance engineering, Data structures, Java class, Data mining, software maintenance, systematic search, error recovery, complex data structures, Binary trees, symbolic execution, Juzi, data structure repair, data structures, repair algorithm, assertions, Testing]
Deryaft
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Deryaft is a tool for generating likely representation invariants of structurally complex data. Given a small set of concrete structures, Deryaft analyzes their key characteristics to formulate local and global properties that the structures exhibit. For effective formulation of structural invariants, Deryaft focuses on graph properties, including reachability, and views the program heap as an edge-labeled graph. Deryaft outputs a Java predicate that represents the invariants; the predicate takes an input structure and returns true if and only if it satisfies the invariants.
[Software testing, Java, automatic programming, Deryaft tool, reachability analysis, Object oriented modeling, invariant detection, Data structures, structurally complex data representation invariant, Software debugging, program heap, Runtime, Software libraries, directed graphs, Writing, edge-labeled directed graph, Java predicate, Concrete, data structures, reachability property, representation invariants, automatic generation, Software engineering]
State extensions for java pathfinder
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Java PathFinder (JPF) is an explicit-state model checker for Java programs. JPF implements a backtrackable Java Virtual Machine (JVM) that provides non-deterministic choices and control over thread scheduling. JPF is itself implemented in Java and runs on top of a host JVM. JPF represents the JVM state of the program being checked and performs three main operations on this state representation: bytecode execution, state backtracking, and state comparison. This paper summarizes four extensions that we have developed to the JPF state representation and operations. One extension provides a new functionality to JPF, and three extensions improve performance of JPF in various scenarios. Some of our code has already been included in publicly available JPF.
[Software testing, Java, jpf, program verification, Java pathfinder, backtrackable Java virtual machine, Debugging, Virtual machining, bytecode execution, Yarn, Open source software, explicit-state model, mixed execution, Computer bugs, state backtracking, virtual machines, thread scheduling, scheduling, Hardware, backtracking, delta execution, Software tools, java pathfinder, Software engineering]
Tool support for data validation by end-user programmers
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
End-user programming tools for creating spreadsheets and webforms offer no data types except "string" for storing many kinds of data, such as person names and street addresses. Consequently, these tools cannot automatically validate these data. To address this problem, we have developed a new userextensible model for string-like data. Each "tope" in this model is a user-defined abstraction that guides the interpretation of strings as a particular kind of data, such as a mailing address. Specifically, each tope implementation contains software functions for recognizing and reformatting that tope's kind of data. With our tools, end-user programmers define new topes and associate them with fields in spreadsheets, webforms, and other programs. This makes it possible at runtime to distinguish between invalid data, valid data, and questionable data that could be valid or invalid. Once identified, questionable and/or invalid data can be double-checked and possibly corrected, thereby increasing the overall reliability of the data.
[program verification, data validation, data, end-user programming tools, spreadsheet programs, Electronic mail, Application software, Programming profession, end-user programming, Computer science, spreadsheets, user-defined abstraction, Runtime, software functions, Robustness, software tools, Books, Software tools, Software reusability, validation, Software engineering]
A business process explorer
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
A business process is composed of a set of interrelated tasks which are joined together by control flow elements. E-commerce systems implement business processes to automate the daily operations of an organization. Organizations must continuously modify their e-commerce systems to accommodate changes to business processes. However, modifying e-commerce systems is a time consuming and error prone task. To correctly perform this task, developers require an in-depth understanding of multi-tiered e-commerce systems and the business processes that they implement. In this paper, we present a business process explorer tool which automatically recovers business processes from three tier e-commerce systems. Developers can explore the recovered business processes and browse the corresponding source code. We integrate our tool with IBM WebSphere Business Modeler (WBM), a leading commercial tool for business process management and modeling. Business analysts could then visualize and analyze the recovered processes using WBM. The business process explorer eases the co-evolution of business processes and their e-commerce system implementation.
[program comprehension, Visualization, business process modeling, IBM WebSphere Business Modeler, business process explorer tool, Merging, Documentation, source code, e-commerce business processes, Electronic commerce, Business communication, Databases, business process, control flow elements, business process management, Automatic control, User interfaces, Cost function, e-commerce, Books, business data processing, electronic commerce, multitiered e-commerce systems, process recovery]
Rubacon
2008 ACM/IEEE 30th International Conference on Software Engineering
None
2008
Compliance frameworks, laws and regulations such as Sarbanes Oxley, Basel II, Solvency II, HIPAA etc. demand from companies in a more and more rigorous way to demonstrate that their organisation, processes and supporting IT landscape implement and follow a set of guidelines at differing levels of abstraction. The work presented in this paper aims to contribute to a software engineering process which is driven by security, risk and compliance management considerations. We concentrate on a part of this approach that focusses on the question how one can use software engineering methods and tools to enforce that the configuration of a system enforces the security policies that arise from business compliance regulations. We present tool support for model-based compliance engineering, i.e. for the model-based development and analysis of software configurations that ensures compliance with security policies. It allows one to check UML models of business applications and their configuration data for adherence to security policies and compliance requirements. The tool is based on standardized data formats, such as UML and XML, which makes its integration into existing business architectures as efficient as possible.
[Unified modeling language, Basel II, Guidelines, business process re-engineering, Engineering management, business architectures, Computer architecture, HIPAA, Rubacon, access control, software tools, business applications, security analysis, Unified Modeling Language, Data security, Solvency II, Application software, software engineering tools, Sarbanes Oxley, UML models, security of data, user permissions, XML, security policies, Risk management, model-based compliance engineering, Software tools, Software engineering, software engineering methods]
Foreword
2009 IEEE 31st International Conference on Software Engineering
None
2009
Presents the welcome message from the conference proceedings.
[Educational programs, Meeting planning, Continents, Organizing, Software engineering]
ICSE 2009 conference organization
2009 IEEE 31st International Conference on Software Engineering
None
2009
Provides a listing of current committee members and society officers.
[]
Predicting build failures using social network analysis on developer communication
2009 IEEE 31st International Conference on Software Engineering
None
2009
A critical factor in work group coordination, communication has been studied extensively. Yet, we are missing objective evidence of the relationship between successful coordination outcome and communication structures. Using data from IBM's Jazztrade project, we study communication structures of development teams with high coordination needs. We conceptualize coordination outcome by the result of their code integration build processes (successful or failed) and study team communication structures with social network measures. Our results indicate that developer communication plays an important role in the quality of software integrations. Although we found that no individual measure could indicate whether a build will fail or succeed, we leveraged the combination of communication structure measures into a predictive model that indicates whether an integration will fail. When used for five project teams, our predictive model yielded recall values between 55% and 75%, and precision values between 50% to 76%.
[Density measurement, Social network services, software development management, build failure, coordination outcome, IBM Jazz project, integration build process, Data mining, software integration, workgroup coordination, Collaboration, developer communication, social network measure, social network analysis, Probability density function, social networking (online), Software, Communication networks, team communication structure, development teams]
How tagging helps bridge the gap between social and technical aspects in software development
2009 IEEE 31st International Conference on Software Engineering
None
2009
Empirical research on collaborative software development practices indicates that technical and social aspects of software development are often intertwined. The processes followed are tacit and constantly evolving, thus not all of them are amenable to formal tool support. In this paper, we explore how ldquotaggingrdquo, a lightweight social computing mechanism, is used to bridge the gap between technical and social aspects of managing work items. We present the results from an empirical study on how tagging has been adopted and adapted over the past two years of a large project with 175 developers. Our research shows that the tagging mechanism was eagerly adopted by the team, and that it has become a significant part of many informal processes. Our findings indicate that lightweight informal tool support, prevalent in the social computing domain, may play an important role in improving team-based software development practices.
[tagging, Collaborative software, Social network services, Humans, identification technology, technical aspects, Programming, lightweight social computing, Software development management, Bridges, Computer science, collaborative software development, social aspects, groupware, Tagging, Collaborative work, social sciences computing, software engineering, Software tools]
Tesseract: Interactive visual exploration of socio-technical relationships in software development
2009 IEEE 31st International Conference on Software Engineering
None
2009
Software developers have long known that project success requires a robust understanding of both technical and social linkages. However, research has largely considered these independently. Research on networks of technical artifacts focuses on techniques like code analysis or mining project archives. Social network analysis has been used to capture information about relations among people. Yet, each type of information is often far more useful when combined, as when the ldquogoodnessrdquo of social networks is judged by the patterns of dependencies in the technical artifacts. To bring such information together, we have developed Tesseract, an interactive exploratory environment that utilizes cross-linked displays to visualize the myriad relationships between artifacts, developers, bugs, and communications. We evaluated Tesseract by (1) demonstrating its feasibility with GNOME project data (2) assessing its usability via informal user evaluations, and (3) verifying its suitability for the open source community via semi-structured interviews.
[software development, Social network services, Collaborative software, data mining, technical artifacts, Programming, Displays, socio-technical relationships, project archive mining, Tesseract, Information analysis, Couplings, code analysis, GNOME project data, Computer bugs, Data visualization, social network analysis, Robustness, software engineering, Usability, interactive visual exploration]
HOLMES: Effective statistical debugging via efficient path profiling
2009 IEEE 31st International Conference on Software Engineering
None
2009
Statistical debugging aims to automate the process of isolating bugs by profiling several runs of the program and using statistical analysis to pinpoint the likely causes of failure. In this paper, we investigate the impact of using richer program profiles such as path profiles on the effectiveness of bug isolation. We describe a statistical debugging tool called HOLMES that isolates bugs by finding paths that correlate with failure. We also present an adaptive version of HOLMES that uses iterative, bug-directed profiling to lower execution time and space overheads. We evaluate HOLMES using programs from the SIR benchmark suite and some large, real-world applications. Our results indicate that path profiles can help isolate bugs more precisely by providing more information about the context in which bugs occur. Moreover, bug-directed profiling can efficiently isolate bugs with low overheads, providing a scalable and accurate alternative to sparse random sampling.
[Software testing, program debugging, Statistical analysis, Instruments, Debugging, program profiles, Computer crashes, effective statistical debugging, Programming profession, Optimization, HOLMES, efficient path profiling, path profiles, bug isolation, iterative bug-directed profiling, Computer bugs, Benchmark testing, Sampling methods, statistical analysis]
Taming coincidental correctness: Coverage refinement with context patterns to improve fault localization
2009 IEEE 31st International Conference on Software Engineering
None
2009
Recent techniques for fault localization leverage code coverage to address the high cost problem of debugging. These techniques exploit the correlations between program failures and the coverage of program entities as the clue in locating faults. Experimental evidence shows that the effectiveness of these techniques can be affected adversely by coincidental correctness, which occurs when a fault is executed but no failure is detected. In this paper, we propose an approach to address this problem. We refine code coverage of test runs using control- and data-flow patterns prescribed by different fault types. We conjecture that this extra information, which we call context patterns, can strengthen the correlations between program failures and the coverage of faulty program entities, making it easier for fault localization techniques to locate the faults. To evaluate the proposed approach, we have conducted a mutation analysis on three real world programs and cross-validated the results with real faults. The experimental results consistently show that coverage refinement is effective in easing the coincidental correctness problem in fault localization techniques.
[Software maintenance, program debugging, Costs, program test, program testing, Genetic mutations, fault localization, Debugging, data flow analysis, context pattern, data-flow pattern, program entity, system recovery, software fault tolerance, Fault diagnosis, coverage refinement, program failure, Fault detection, Councils, control-flow pattern, mutation analysis, coincidental correctness taming, Cities and towns, Testing]
Lightweight fault-localization using multiple coverage types
2009 IEEE 31st International Conference on Software Engineering
None
2009
Lightweight fault-localization techniques use program coverage to isolate the parts of the code that are most suspicious of being faulty. In this paper, we present the results of a study of three types of program coverage-statements, branches, and data dependencies-to compare their effectiveness in localizing faults. The study shows that no single coverage type performs best for all faults-different kinds of faults are best localized by different coverage types. Based on these results, we present a new coverage-based approach to fault localization that leverages the unique qualities of each coverage type by combining them. Because data dependencies are noticeably more expensive to monitor than branches, we also investigate the effects of replacing data-dependence coverage with an approximation inferred from branch coverage. Our empirical results show that (1) the cost of fault localization using combinations of coverage is less than using any individual coverage type and closer to the best case (without knowing in advance which kinds of faults are present), and (2) using inferred data-dependence coverage retains most of the benefits of combinations.
[Performance evaluation, Java, program debugging, Costs, data dependencies, program coverage, Instruments, lightweight fault localization, Educational institutions, Software debugging, Condition monitoring, Runtime, Isolation technology, branch coverage, Informatics]
Succession: Measuring transfer of code and developer productivity
2009 IEEE 31st International Conference on Software Engineering
None
2009
Code ownership transfer or succession is a crucial ingredient in open source code reuse and in offshoring projects. Measuring succession can help understand factors that affect the success of such transfers and suggest ways to make them more efficient. We propose and evaluate several methods to measure succession based on the chronology and traces of developer activities. Using ten instances of offshoring succession identified through interviews, we find that the best succession measure can accurately pinpoint the most likely mentors. We model the productivity ratio of more than 1000 developer pairs involved in the succession to test conjectures formulated using the organizational socialization theory and find the ratio to decrease for instances of offshoring and for mentors who have worked primarily on a single project or have transferred ownership for their non-primary project code, thus supporting a theory-based conjectures and providing practical suggestions on how to improve succession.
[Productivity, offshoring succession, codes, Costs, open source code reuse, Programming, code ownership transfer, Open source software, developer productivity, Manufacturing processes, nonprimary project code, Virtual groups, software engineering, organizational socialization theory, Business, Testing]
A case-study on using an Automated In-process Software Engineering Measurement and Analysis system in an industrial environment
2009 IEEE 31st International Conference on Software Engineering
None
2009
Automated systems for measurement and analysis are not adopted on a large scale in companies, despite the opportunities they offer. The fear of the ldquobig brotherrdquo and the lack of reports giving insights into the real adoption process and concrete usages in industry are barriers to this adoption. We report on a case-study on the adoption and long-term usage (2 years of running system) of such a system in a company focusing on the adoption process and the related challenges we encountered.
[Automation, Cement industry, Project management, Switches, industrial environment, Electrical resistance measurement, systems analysis, Computer industry, Concrete, software engineering, Large-scale systems, Software measurement, automated inprocess software engineering measurement, Software engineering]
Using quantitative analysis to implement autonomic IT systems
2009 IEEE 31st International Conference on Software Engineering
None
2009
The software underpinning today's IT systems needs to adapt dynamically and predictably to rapid changes in system workload, environment and objectives. We describe a software framework that achieves such adaptiveness for IT systems whose components can be modelled as Markov chains. The framework comprises (i) an autonomic architecture that uses Markov-chain quantitative analysis to dynamically adjust the parameters of an IT system in line with its state, environment and objectives; and (ii) a method for developing instances of this architecture for real-world systems. Two case studies are presented that use the framework successfully for the dynamic power management of disk drives, and for the adaptive management of cluster availability within data centres, respectively.
[Energy consumption, program verification, autonomic legacy IT system, software framework, Markov chain, PRISM probabilistic model checker, software architecture, Runtime, Computer architecture, Logic devices, Pervasive computing, quantitative analysis tool, Power system management, program diagnostics, probability, adaptive cluster availability management, Probabilistic logic, data centre, Knowledge management, software maintenance, computer centres, software fault tolerance, dynamic power management, Disk drives, disk drive, Web services, Markov processes, Energy management, autonomic software architecture]
Model evolution by run-time parameter adaptation
2009 IEEE 31st International Conference on Software Engineering
None
2009
Models can help software engineers to reason about design-time decisions before implementing a system. This paper focuses on models that deal with non-functional properties, such as reliability and performance. To build such models, one must rely on numerical estimates of various parameters provided by domain experts or extracted by other similar systems. Unfortunately, estimates are seldom correct. In addition, in dynamic environments, the value of parameters may change over time. We discuss an approach that addresses these issues by keeping models alive at run time and feeding a Bayesian estimator with data collected from the running system, which produces updated parameters. The updated model provides an increasingly better representation of the system. By analyzing the updated model at run time, it is possible to detect or predict if a desired property is, or will be, violated by the running implementation. Requirement violations may trigger automatic reconfigurations or recovery actions aimed at guaranteeing the desired goals. We illustrate a working framework supporting our methodology and apply it to an example in which a Web service orchestrated composition is modeled through a discrete time Markov chain. Numerical simulations show the effectiveness of the approach.
[Parameter estimation, discrete time Markov chain, Adaptation model, software reliability, Predictive models, Reliability engineering, Web service orchestrated composition, Data mining, Design engineering, Runtime, Web services, Bayesian methods, Markov processes, Numerical simulation, Bayes methods, Bayesian estimator, run-time parameter adaptation]
Taming Dynamically Adaptive Systems using models and aspects
2009 IEEE 31st International Conference on Software Engineering
None
2009
Since software systems need to be continuously available under varying conditions, their ability to evolve at runtime is increasingly seen as one key issue. Modern programming frameworks already provide support for dynamic adaptations. However the high-variability of features in Dynamic Adaptive Systems (DAS) introduces an explosion of possible runtime system configurations (often called modes) and mode transitions. Designing these configurations and their transitions is tedious and error-prone, making the system feature evolution difficult. While Aspect-Oriented Modeling (AOM) was introduced to improve the modularity of software, this paper presents how an AOM approach can be used to tame the combinatorial explosion of DAS modes. Using AOM techniques, we derive a wide range of modes by weaving aspects into an explicit model reflecting the runtime system. We use these generated modes to automatically adapt the system. We validate our approach on an adaptive middleware for home-automation currently deployed in Rennes metropolis.
[dynamic adaptations, adaptive middleware, home-automation, Adaptive systems, aspect-oriented modeling, dynamically adaptive systems taming, software systems, runtime system configurations, Explosions, Middleware, Runtime, Software systems, Model driven engineering, Weaving, Robustness, software engineering, Dynamic programming, object-oriented methods, Time factors]
Accurate Interprocedural Null-Dereference Analysis for Java
2009 IEEE 31st International Conference on Software Engineering
None
2009
Null dereference is a commonly occurring defect in Java programs, and many static-analysis tools identify such defects. However, most of the existing tools perform a limited interprocedural analysis. In this paper, we present an interprocedural path-sensitive and context-sensitive analysis for identifying null dereferences. Starting at a dereference statement, our approach performs a backward demand-driven analysis to identify precisely paths along which null values may flow to the dereference. The demand-driven analysis avoids an exhaustive program exploration, which lets it scale to large programs. We present the results of empirical studies conducted using large open-source and commercial products. Our results show that: (1) our approach detects fewer false positives, and significantly more interprocedural true positives, than other commonly used tools; (2) the analysis scales to large subjects; and (3) the identified defects are often deleted in subsequent releases, which indicates that the reported defects are important.
[Java, program diagnostics, context-sensitive analysis, Open source software, static-analysis tool, Information analysis, path-sensitive analysis, backward demand-driven analysis, Computer bugs, null-dereference analysis, Performance analysis, Safety, Arithmetic]
The road not taken: Estimating path execution frequency statically
2009 IEEE 31st International Conference on Software Engineering
None
2009
A variety of compilers, static analyses, and testing frameworks rely heavily on path frequency information. Uses for such information range from optimizing transformations to bug finding. Path frequencies are typically obtained through profiling, but that approach is severely restricted: it requires running programs in an indicative environment, and on indicative test inputs. We present a descriptive statistical model of path frequency based on features that can be readily obtained from a program's source code. Our model is over 90% accurate with respect to several benchmarks, and is sufficient for selecting the 5% of paths that account for over half of a program's total runtime. We demonstrate our technique's robustness by measuring its performance as a static branch predictor, finding it to be more accurate than previous approaches on average. Finally, our qualitative analysis of the model provides insight into which source-level features indicate ldquohot pathsrdquo.
[program debugging, program testing, Roads, Scalability, Frequency estimation, program compilers, compilers, Information analysis, Runtime, testing frameworks, Benchmark testing, Robustness, Performance analysis, descriptive statistical model, Data analysis, program diagnostics, bug finding, hot paths, performance measurement, path execution frequency estimation, static analyses, static branch predictor, Particle measurements, statistical analysis, program source code]
Automatic dimension inference and checking for object-oriented programs
2009 IEEE 31st International Conference on Software Engineering
None
2009
This paper introduces UniFi, a tool that attempts to automatically detect dimension errors in Java programs. UniFi infers dimensional relationships across primitive type and string variables in a program, using an inter-procedural, context-sensitive analysis. It then monitors these dimensional relationships as the program evolves, flagging inconsistencies that may be errors. UniFi requires no programmer annotations, and supports arbitrary program-specific dimensions, thus providing fine-grained dimensional consistency checking. UniFi exploits features of object-oriented languages, but can be used for other languages as well. We have run UniFi on real-life Java code and found that it is useful in exposing dimension errors. We present a case study of using UniFi on nightly builds of a 19,000 line code base as it evolved over 10 months.
[Java, program debugging, arbitrary program-specific dimension, software debugging, object-oriented programming, context-sensitive analysis, object-oriented language, Software debugging, Programming profession, Physics, Equations, object-oriented program, Computer science, Computer languages, Computer displays, UniFi-automatic dimension error detection tool, fine-grained dimensional consistency checking, Intrusion detection, Computer errors, Java code, programmer annotation]
In-field healing of integration problems with COTS components
2009 IEEE 31st International Conference on Software Engineering
None
2009
Developers frequently integrate complex COTS frameworks and components in software applications. COTS products are often only partially documented, and developers may misuse technologies and introduce integration faults, as witnessed by the many entries in fault repositories. Once identified, common integration problems and their fixes are usually documented in forums and fault repositories on the Web, but this does not prevent them to occur in the field when COTS products are reused. In this paper, we propose a methodology and a self- healing technology that can reduce the occurrence of infield failures caused by common integration problems that are identified and documented by COTS developers. Our methodology supports COTS developers in producing healing connectors for common misuses of COTS products. Our technology produces information that facilitate debugging and patching of applications that use COTS products. Application developers inject healing connectors into their systems to automatically repair problems caused by misuses of COTS products. Healing takes place at run-time, on-the-fly and in-the-field. The activity of healing connectors is traced in log files, to facilitate debugging and patching of integration problems. Empirical experiences with several applications and COTS products show the feasibility of the approach and the efficiency of the technology.
[Software testing, Java, program debugging, application debugging, Costs, object-oriented programming, application patching, Debugging, Application software, integration fault repository, system recovery, software fault tolerance, in-field integration problem healing, infield failure, Connectors, Runtime, software packages, COTS product, COTS software component, Springs, Informatics, Web server]
Modular string-sensitive permission analysis with demand-driven precision
2009 IEEE 31st International Conference on Software Engineering
None
2009
In modern software systems, programs are obtained by dynamically assembling components. This has made it necessary to subject component providers to access-control restrictions. What permissions should be granted to each component? Too few permissions may cause run-time authorization failures, too many constitute a security hole. We have designed and implemented a composite algorithm for precise static permission analysis for Java and the CLR. Unlike previous work, the analysis is modular and fully integrated with a novel slicing-based string analysis that is used to statically compute the string values defining a permission and disambiguate permission propagation paths. The results of our research prototype on production-level Java code support the effectiveness, practicality, and precision of our techniques, and show outstanding improvement over previous work.
[Algorithm design and analysis, modern software systems, Java, Runtime environment, object-oriented programming, Laboratories, component assembling, run-time authorization failures, Inspection, CLR, security hole, Security, modular string-sensitive permission analysis, static permission analysis, Authorization, composite algorithm, slicing-based string analysis, Prototypes, demand-driven precision, authorisation, Permission, access control restrictions, program slicing, Testing]
License integration patterns: Addressing license mismatches in component-based development
2009 IEEE 31st International Conference on Software Engineering
None
2009
In this paper we address the problem of combining software components with different and possibly incompatible legal licenses to create a software application that does not violate any of these licenses while potentially having its own. We call this problem the license mismatch problem. The rapid growth and availability of open source software (OSS) components with varying licenses, and the existence of more than 70 OSS licenses increases the complexity of this problem. Based on a study of 124 OSS software packages, we developed a model which describes the interconnection of components in these packages from a legal point of view. We used our model to document integration patterns that are commonly used to solve the license mismatch problem in practice when creating both proprietary and OSS applications. Software engineers with little legal expertise could use these documented patterns to understand and address the legal issues involved in reusing components with different and possibly conflicting licenses.
[object-oriented programming, Law, public domain software, license integration patterns, license mismatch problem, industrial property, Licenses, Programming, documented patterns, Application software, Open source software, open source software components, Software packages, legal licenses, Computer architecture, software packages, Packaging, software reusability, license mismatches, software engineering, reusing components, Software tools, Legal factors, component-based development]
Automatic creation of SQL Injection and cross-site scripting attacks
2009 IEEE 31st International Conference on Software Engineering
None
2009
We present a technique for finding security vulnerabilities in Web applications. SQL injection (SQLI) and cross-site scripting (XSS) attacks are widespread forms of attack in which the attacker crafts the input to the application to access or modify user data and execute malicious code. In the most serious attacks (called second-order, or persistent, XSS), an attacker can corrupt a database so as to cause subsequent users to execute malicious code. This paper presents an automatic technique for creating inputs that expose SQLI and XSS vulnerabilities. The technique generates sample inputs, symbolically tracks taints through execution (including through database accesses), and mutates the inputs to produce concrete exploits. Ours is the first analysis of which we are aware that precisely addresses second-order XSS attacks. Our technique creates real attack vectors, has few false positives, incurs no runtime overhead for the deployed application, works without requiring modification of application code, and handles dynamic programming-language constructs. We implemented the technique for PHP, in a tool ARDILLA. We evaluated ARDILLA on five PHP applications and found 68 previously unknown vulnerabilities (23 SQLI, 33 first-order XSS, and 12 second-order XSS).
[security vulnerabilities, malicious code, Data security, SQL injection, Web applications, HTML, Application software, SQL, Privacy, Runtime, Databases, security of data, cross-site scripting attacks, Concrete, Internet, Monitoring, Testing]
Invariant-based automatic testing of AJAX user interfaces
2009 IEEE 31st International Conference on Software Engineering
None
2009
AJAX-based Web 2.0 applications rely on stateful asynchronous client/server communication, and client-side runtime manipulation of the DOM tree. This not only makes them fundamentally different from traditional web applications, but also more error-prone and harder to test. We propose a method for testing AJAX applications automatically, based on a crawler to infer a flow graph for all (client-side) user interface states. We identify AJAX-specific faults that can occur in such states (related to DOM validity, error messages, discoverability, back-button compatibility, etc.) as well as DOM-tree invariants that can serve as oracle to detect such faults. We implemented our approach in ATUSA, a tool offering generic invariant checking components, a plugin-mechanism to add application-specific state validators, and generation of a test suite covering the paths obtained during crawling. We describe two case studies evaluating the fault revealing capabilities, scalability, required manual effort and level of automation of our approach.
[DOM validity, application-specific state validator, program testing, Scalability, asynchronous client/server communication, Crawlers, AJAX user interfaces, client-side runtime manipulation, user interfaces, invariant checking components, Fault diagnosis, Runtime, Tree graphs, AJAX-specific faults, discoverability, back-button compatibility, DOM-tree invariants, flow graph, AJAX-based Web 2.0 application, client-server systems, Java, Automation, flow graphs, DOM tree, Flow graphs, invariant-based automatic testing, error messages, Automatic testing, Fault detection, User interfaces, AJAX application, Internet]
FEATUREHOUSE: Language-independent, automated software composition
2009 IEEE 31st International Conference on Software Engineering
None
2009
Superimposition is a composition technique that has been applied successfully in many areas of software development. Although superimposition is a general-purpose concept, it has been (re)invented and implemented individually for various kinds of software artifacts. We unify languages and tools that rely on superimposition by using the language-independent model of feature structure trees (FSTs). On the basis of the FST model, we propose a general approach to the composition of software artifacts written in different languages, Furthermore, we offer a supporting framework and tool chain, called FEATUREHOUSE. We use attribute grammars to automate the integration of additional languages, in particular, we have integrated Java, C#, C, Haskell, JavaCC, and XML. Several case studies demonstrate the practicality and scalability of our approach and reveal insights into the properties a language must have in order to be ready for superimposition.
[superimposition technique, Scalability, FEATUREHOUSE, Programming, Mathematics, C language, feature structure tree, Haskell language, tree data structures, Informatics, Java, software development, Collaborative software, FST, Documentation, attribute grammar, software maintenance, automated software composition, attribute grammars, software merging, configuration management, language-independent model, XML, Software systems, Software tools, C# language, functional languages]
Automatically capturing source code context of NL-queries for software maintenance and reuse
2009 IEEE 31st International Conference on Software Engineering
None
2009
As software systems continue to grow and evolve, locating code for maintenance and reuse tasks becomes increasingly difficult. Existing static code search techniques using natural language queries provide little support to help developers determine whether search results are relevant, and few recommend alternative words to help developers reformulate poor queries. In this paper, we present a novel approach that automatically extracts natural language phrases from source code identifiers and categorizes the phrases and search results in a hierarchy. Our contextual search approach allows developers to explore the word usage in a piece of software, helping them to quickly identify relevant program elements for investigation or to quickly recognize alternative words for query reformulation. An empirical evaluation of 22 developers reveals that our contextual search approach significantly outperforms the most closely related technique in terms of effort and effectiveness.
[Software maintenance, Vocabulary, Automation, Costs, natural language processing, Natural languages, Humans, software maintenance, contextual search approach, Search methods, NL-query reformulation, natural language phrase extraction, Search engines, software reusability, automatic capturing source code context, Software systems, Frequency, query formulation]
Semantics-based code search
2009 IEEE 31st International Conference on Software Engineering
None
2009
Our goal is to use the vast repositories of available open source code to generate specific functions or classes that meet a user's specifications. The key words here are specifications and generate. We let users specify what they are looking for as precisely as possible using keywords, class or method signatures, test cases, contracts, and security constraints. Our system then uses an open set of program transformations to map retrieved code into what the user asked for. This approach is implemented in a prototype system for Java with a Web interface.
[Web interface, Security, formal specification, program compilers, Open source software, user specification, security constraint, class signature, Prototypes, method signature, Search engines, open source code repository, keyword, Contracts, Testing, Java, software reuse, contract, programming language semantics, Programming profession, Computer science, Writing, software reusability, semantics-based code search, program transformation]
Reasoning about edits to feature models
2009 IEEE 31st International Conference on Software Engineering
None
2009
Features express the variabilities and commonalities among programs in a software product line (SPL). A feature model defines the valid combinations of features, where each combination corresponds to a program in an SPL. SPLs and their feature models evolve over time. We classify the evolution of a feature model via modifications as refactorings, specializations, generalizations, or arbitrary edits. We present an algorithm to reason about feature model edits to help designers determine how the program membership of an SPL has changed. Our algorithm takes two feature models as input (before and after edit versions), where the set of features in both models are not necessarily the same, and it automatically computes the change classification. Our algorithm is able to give examples of added or deleted products and efficiently classifies edits to even large models that have thousands of features.
[Algorithm design and analysis, Law, Software algorithms, program feature model, Classification algorithms, software maintenance, Sun, Computer science, software product line, Feedback, program change classification, product development, software reusability, software edit reasoning, reasoning about programs, Logic, software refactoring, Assembly, Legal factors]
Learning operational requirements from goal models
2009 IEEE 31st International Conference on Software Engineering
None
2009
Goal-oriented methods have increasingly been recognised as an effective means for eliciting, elaborating, analysing and specifying software requirements. A key activity in these approaches is the elaboration of a correct and complete set of opertional requirements, in the form of pre- and trigger-conditions, that guarantee the system goals. Few existing approaches provide support for this crucial task and mainly rely on significant effort and expertise of the engineer. In this paper we propose a tool-based framework that combines model checking, inductive learning and scenarios for elaborating operational requirements from goal models. This is an iterative process that requires the engineer to identify positive and negative scenarios from counterexamples to the goals, generated using model checking, and to select operational requirements from suggestions computed by inductive learning.
[opertional requirement elaboration, Laboratories, Documentation, Programming, Educational institutions, iterative process, scenarios, tool-based framework, formal specification, operational requirement learning, software requirements, Phase detection, Learning systems, inductive learning, model checking, Goal-oriented requirements engineering, Safety, software tools, Logic, learning by example, goal-oriented methods]
Complete and accurate clone detection in graph-based models
2009 IEEE 31st International Conference on Software Engineering
None
2009
Model-Driven Engineering (MDE) has become an important development framework for many large-scale software. Previous research has reported that as in traditional code-based development, cloning also occurs in MDE. However, there has been little work on clone detection in models with the limitations on detection precision and completeness. This paper presents ModelCD, a novel clone detection tool for Matlab/Simulink models, that is able to efficiently and accurately detect both exactly matched and approximate model clones. The core of ModelCD is two novel graph-based clone detection algorithms that are able to systematically and incrementally discover clones with a high degree of completeness, accuracy, and scalability. We have conducted an empirical evaluation with various experimental studies on many real-world systems to demonstrate the usefulness of our approach and to compare the performance of ModelCD with existing tools.
[Costs, clone detection, Scalability, graph theory, mathematics computing, Cloning, Aerospace electronics, Matlab-Simulink models, ModelCD, Software design, model-driven engineering, code-based development, Model driven engineering, software engineering, Large-scale systems, software tools, Mathematical model, Detection algorithms, Software tools, graph-based models]
How we refactor, and how we know it
2009 IEEE 31st International Conference on Software Engineering
None
2009
Much of what we know about how programmers refactor in the wild is based on studies that examine just a few software projects. Researchers have rarely taken the time to replicate these studies in other contexts or to examine the assumptions on which they are based. To help put refactoring research on a sound scientific basis, we draw conclusions using four data sets spanning more than 13 000 developers, 240 000 tool-assisted refactorings, 2500 developer hours, and 3400 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. For example, we find that programmers frequently do not indicate refactoring activity in commit logs, which contradicts assumptions made by several previous researchers. In contrast, we were able to confirm the assumption that programmers do frequently intersperse refactoring with other program changes. By confirming assumptions and replicating studies made by other researchers, we can have greater confidence that those researchers' conclusions are generalizable.
[Performance evaluation, program change, History, software maintenance, software project, Software debugging, Programming profession, Computer bugs, Frequency, Books, software refactoring, Catalogs, Testing]
The secret life of bugs: Going past the errors and omissions in software repositories
2009 IEEE 31st International Conference on Software Engineering
None
2009
Every bug has a story behind it. The people that discover and resolve it need to coordinate, to get information from documents, tools, or other people, and to navigate through issues of accountability, ownership, and organizational structure. This paper reports on a field study of coordination activities around bug fixing that used a combination of case study research and a survey of software professionals. Results show that the histories of even simple bugs are strongly dependent on social, organizational, and technical knowledge that cannot be solely extracted through automation of electronic repositories, and that such automation provides incomplete and often erroneous accounts of coordination. The paper uses rich bug histories and survey results to identify common bug fixing coordination patterns and to provide implications for tool designers and researchers of coordination in software development.
[Productivity, program debugging, Automation, bug histories, Navigation, software bugs, software development, electronic repositories, Programming, Spatial databases, History, Data mining, Software debugging, Software development management, software repositories, bug fixing coordination patterns, Computer bugs, software engineering, coordination activities]
Discovering and representing systematic code changes
2009 IEEE 31st International Conference on Software Engineering
None
2009
Software engineers often inspect program differences when reviewing others' code changes, when writing check-in comments, or when determining why a program behaves differently from expected behavior after modification. Program differencing tools that support these tasks are limited in their ability to group related code changes or to detect potential inconsistencies in those changes. To overcome these limitations and to complement existing approaches, we built Logical Structural Diff (LSdiff), a tool that infers systematic structural differences as logic rules. LSdiff notes anomalies from systematic changes as exceptions to the logic rules. We conducted a focus group study with professional software engineers in a large E-commerce company; we also compared LSdiff's results with textual differences and with structural differences without rules. Our evaluation suggests that LSdiff complements existing differencing tools by grouping code changes that form systematic change patterns regardless of their distribution throughout the code, and its ability to discover anomalies shows promise in detecting inconsistent changes.
[Pervasive computing, systematic code change discovery, program differencing tool, logical structural diff tool, logic rule, Flow graphs, software maintenance, Programming profession, Computer science, Tree graphs, Computer bugs, check-in comment, Packaging, Writing, software engineering, Logic, Software tools]
Improving API documentation usability with knowledge pushing
2009 IEEE 31st International Conference on Software Engineering
None
2009
The documentation of API functions typically conveys detailed specifications for the benefit of interested readers. In some cases, however, it also contains usage directives, such as rules or caveats, of which authors of invoking code must be made aware to prevent errors and inefficiencies. There is a risk that these directives may be ldquolostrdquo within the verbose text, or that the text would not be read because there are so many invoked functions. To address these concerns for Java, an Eclipse plug-in named eMoose decorates method invocations whose targets have associated directives. Our goal is to lead readers to investigate further, which we aid by highlighting the tagged directives in the JavaDoc hover. We present a lab study that demonstrates the directive awareness problem in traditional documentation use and the potential benefits of our approach.
[usage directive, Java, application program interfaces, system documentation, Documentation, Inspection, Eclipse plug-in, Application software, Guidelines, Computer science, Software libraries, Runtime, knowledge pushing, Software systems, API documentation usability, Usability, eMoose decorates method]
Listening to programmers &#x2014; Taxonomies and characteristics of comments in operating system code
2009 IEEE 31st International Conference on Software Engineering
None
2009
Innovations from multiple directions have been proposed to improve software reliability. Unfortunately, many of the innovations are not fully exploited by programmers. To bridge the gap, this paper proposes a new approach to ldquolistenrdquo to thousands of programmers: studying their programming comments. Since comments express programmers' assumptions and intentions, comments can reveal programmers' needs, which can provide guidance (1) for language/-tool designers on where they should develop new techniques or enhance the usability of existing ones, and (2) for programmers on what problems are most pervasive and important so that they should take initiatives to adopt some existing tools or language extensions. We studied 1050 comments randomly sampled from the latest versions of Linux, FreeBSD, and OpenSolaris. We found that 52.6% of these comments could be leveraged by existing or to-be-proposed tools for improving reliability. Our findings include: (1) many comments describe code relationships, code evolutions, or the usage and meaning of integers and integer macros, (2) a significant amount of comments could be expressed by existing annotation languages, and (3) many comments express synchronization related concerns but are not well supported by annotation languages.
[Technological innovation, operating system code, Taxonomy, software reliability, OpenSolaris, Software reliability, Programming profession, Bridges, Computer languages, Operating systems, Linux, Computer bugs, operating systems (computers), Usability, FreeBSD]
Equality and hashing for (almost) free: Generating implementations from abstraction functions
2009 IEEE 31st International Conference on Software Engineering
None
2009
In an object-oriented language such as Java, every class requires implementations of two special methods, one for determining equality and one for computing hash codes. Although the specification of these methods is usually straightforward, they can be hard to code (due to subclassing, delegation, cyclic references, and other factors) and often harbor subtle faults. A technique is presented that simplifies this task. Instead of writing code for the methods, the programmer gives, as a brief annotation, an abstraction function that defines an abstract view of an object's representation, and sometimes an additional observer in the form of an iterator method. Equality and hash codes are then computed in library code that uses reflection to read the annotations. Experiments on a variety of programs suggest that, in comparison to writing the methods by hand, our technique requires less text from the programmer and results in methods that are more often correct.
[Java, object-oriented programming, object-oriented language, library code, abstraction function, Reflection, Programming profession, cyclic references, subclassing, Computer science, object representation, Writing, Cost function, Libraries, Artificial intelligence, Contracts, Testing]
Locating need-to-translate constant strings for software internationalization
2009 IEEE 31st International Conference on Software Engineering
None
2009
Modern software applications require internationalization to be distributed to different regions of the world. In various situations, many software applications are not internationalized at early stages of development. To internationalize such an existing application, developers need to externalize some hard-coded constant strings to resource files, so that translators can easily translate the application into a local language without modifying its source code. Since not all the constant strings require externalization, locating those need-to-translate constant strings is a necessary task that developers must complete for internationalization. In this paper, we present an approach to automatically locating need-to-translate constant strings. Our approach first collects a list of API methods related to the graphical user interface (GUI), and then searches for need-to-translate strings from the invocations of these API methods based on string-taint analysis. We evaluated our approach on four real-world open source applications: RText, Risk, ArtOfIllusion, and Megamek. The results show that our approach effectively locates most of the need-to-translate constant strings in all the four applications.
[ArtOfIllusion, graphical user interfaces, public domain software, Laboratories, Risk, Open source software, Databases, need-to-translate constant strings, software engineering, string-taint analysis, Graphical user interfaces, Software prototyping, Java, open source applications, hard-coded constant strings, graphical user interface, Educational technology, Application software, API methods, Computer science, software applications, Megamek, RText, software internationalization, Software tools]
Automatically finding patches using genetic programming
2009 IEEE 31st International Conference on Software Engineering
None
2009
Automatic program repair has been a longstanding goal in software engineering, yet debugging remains a largely manual process. We introduce a fully automated method for locating and repairing bugs in software. The approach works on off-the-shelf legacy applications and does not require formal specifications, program annotations or special coding practices. Once a program fault is discovered, an extended form of genetic programming is used to evolve program variants until one is found that both retains required functionality and also avoids the defect in question. Standard test cases are used to exercise the fault and to encode program requirements. After a successful repair has been discovered, it is minimized using structural differencing algorithms and delta debugging. We describe the proposed method and report experimental results demonstrating that it can successfully repair ten different C programs totaling 63,000 lines in under 200 seconds, on average.
[Software maintenance, program debugging, Costs, genetic programming, Genetic mutations, Debugging, patches, genetic algorithms, off-the-shelf legacy applications, Formal specifications, Biological information theory, structural differencing algorithms, delta debugging, program fault discovery, Computer bugs, Genetic programming, C programs, Biology computing, software engineering, automatic program repair, Testing]
FlexSync: An aspect-oriented approach to Java synchronization
2009 IEEE 31st International Conference on Software Engineering
None
2009
Designers of concurrent programs are faced with many choices of synchronization mechanisms, among which clear functional trade-offs exist. Making synchronization customizable is highly desirable as different deployment scenarios of the same program often prioritize synchronization choices differently. Unfortunately, such customizations cannot be accomplished in the conventional non-modular implementation of synchronization. To enable customizability, we present FlexSync, an aspect oriented synchronization library, to enable the modular reasoning and the declarative specification of synchronization. Complex Java systems can simultaneously work with multiple synchronization mechanisms without any code changes. The FlexSync load-time weaver performs deployment time optimizations and ensures these synchronization mechanisms consistently interact with each other and with the core system. We evaluated FlexSync on commercially used complex Java systems and observed significant speedups as a result of the deployment-specific customization.
[Java, Fluctuations, object-oriented programming, aspect-oriented approach, nonmodular implementation, Buffer storage, Buildings, Data structures, Information retrieval, Java synchronization, Yarn, Delay, Concurrent computing, concurrent program, Software libraries, optimisation, optimization]
Effective static deadlock detection
2009 IEEE 31st International Conference on Software Engineering
None
2009
We present an effective static deadlock detection algorithm for Java. Our algorithm uses a novel combination of static analyses each of which approximates a different necessary condition for a deadlock. We have implemented the algorithm and report upon our experience applying it to a suite of multi-threaded Java programs. While neither sound nor complete, our approach is effective in practice, finding all known deadlocks as well as discovering previously unknown ones in our benchmarks with few false alarms.
[Algorithm design and analysis, Java, Data analysis, Multicore processing, multi-threading, static deadlock detection, multi-threaded program, static analysis, Yarn, Sun, system recovery, Concurrent computing, Databases, concurrency control, System recovery, Detection algorithms]
Refactoring sequential Java code for concurrency via concurrent libraries
2009 IEEE 31st International Conference on Software Engineering
None
2009
Parallelizing existing sequential programs to run efficiently on multicores is hard. The Java 5 package java.util.concurrent (j.u.c.) supports writing concurrent programs: much of the complexity of writing thread-safe and scalable programs is hidden in the library. To use this package, programmers still need to reengineer existing code. This is tedious because it requires changing many lines of code, is error-prone because programmers can use the wrong APIs, and is omission-prone because programmers can miss opportunities to use the enhanced APIs. This paper presents our tool, Concurrencer, that enables programmers to refactor sequential code into parallel code that uses three j.u.c. concurrent utilities. Concurrencer does not require any program annotations. Its transformations span multiple, non-adjacent, program statements. A find-and-replace tool can not perform such transformations, which require program analysis. Empirical evaluation shows that concurrencer refactors code effectively: concurrencer correctly identifies and applies transformations that some open-source developers overlooked, and the converted code exhibits good speedup.
[Java, concurrencer refactors code, java.util.concurrent, refactoring sequential Java code, software maintenance, Yarn, Programming profession, Open source software, software libraries, Concurrent computing, concurrent libraries, Ash, program analysis, Packaging, Writing, Parallel processing, parallel code, Libraries]
Maintaining and evolving GUI-directed test scripts
2009 IEEE 31st International Conference on Software Engineering
None
2009
Since manual black-box testing of GUI-based applications (GAPs) is tedious and laborious, test engineers create test scripts to automate the testing process. These test scripts interact with GAPs by performing actions on their GUI objects. An extra effort that test engineers put in writing test scripts is paid off when these scripts are run repeatedly. Unfortunately, releasing new versions of GAPs with modified GUIs breaks their corresponding test scripts thereby obliterating benefits of test automation. We offer a novel approach for maintaining and evolving test scripts so that they can test new versions of their respective GAPs. We built a tool to implement our approach, and we conducted a case study with forty five professional programmers and test engineers to evaluate this tool. The results show with strong statistical significance that users find more failures and report fewer false positives (p &lt; 0.02) in test scripts with our tool than with a flagship industry product and a baseline manual approach. Our tool is lightweight and it takes less than eight seconds to analyze approximately 1KLOC of test scripts.
[Performance evaluation, Automation, Costs, GUI-directed test script evolution, statistical significance, program testing, graphical user interfaces, GUI-directed test script maintenance, Manuals, Maintenance engineering, software maintenance, Logic testing, professional programmer, flagship industry product, Runtime, Automatic testing, test engineer, GUI-based application, test automation tool, Writing, software tools, manual black-box testing, Graphical user interfaces]
MINTS: A general framework and tool for supporting test-suite minimization
2009 IEEE 31st International Conference on Software Engineering
None
2009
Test-suite minimization techniques aim to eliminate redundant test cases from a test-suite based on some criteria, such as coverage or fault-detection capability. Most existing test-suite minimization techniques have two main limitations: they perform minimization based on a single criterion and produce suboptimal solutions. In this paper, we propose a test-suite minimization framework that overcomes these limitations by allowing testers to (1) easily encode a wide spectrum of test-suite minimization problems, (2) handle problems that involve any number of criteria, and (3) compute optimal solutions by leveraging modern integer linear programming solvers. We implemented our framework in a tool, called MINTS, that is freely-available and can be interfaced with a number of different state-of-the-art solvers. Our empirical evaluation shows that MINTS can be used to instantiate a number of different test-suite minimization problems and efficiently find an optimal solution for such problems using different solvers.
[Software testing, Performance evaluation, System testing, Software maintenance, Minimization methods, program testing, integer programming, software reliability, integer linear programming solver, Educational institutions, Encoding, linear programming, general framework, MINTS, test-suite minimization framework, Automatic testing, Fault detection, Integer linear programming, fault-detection capability, redundancy, redundant test cases]
Synthesizing intensional behavior models by graph transformation
2009 IEEE 31st International Conference on Software Engineering
None
2009
This paper describes an approach (SPY) to recovering the specification of a software component from the observation of its run-time behavior. It focuses on components that behave as data abstractions. Components are assumed to be black boxes that do not allow any implementation inspection. The inferred description may help understand what the component does when no formal specification is available. SPY works in two main stages. First, it builds a deterministic finite-state machine that models the partial behavior of instances of the data abstraction. This is then generalized via graph transformation rules. The rules can generate a possibly infinite number of behavior models, which generalize the description of the data abstraction under an assumption of ldquoregularityrdquo with respect to the observed behavior. The rules can be viewed as a likely specification of the data abstraction. We illustrate how SPY works on relevant examples and we compare it with competing methods.
[Java, Service oriented architecture, Documentation, Inspection, Containers, data abstraction, software component specification, Formal specifications, formal specification, graph grammars, Runtime, graph transformation, Web services, intensional behavior models, Writing, data structures, Contracts]
Analyzing critical process models through behavior model synthesis
2009 IEEE 31st International Conference on Software Engineering
None
2009
Process models capture tasks performed by agents together with their control flow. Building and analyzing such models is important but difficult in certain areas such as safety-critical healthcare processes. Tool-supported techniques are needed to find and correct flaws in such processes. On another hand, event-based formalisms such as Labeled Transition Systems (LTS) prove effective for analyzing agent behaviors. The paper describes a blend of state-based and event-based techniques for analyzing task models involving decisions. The input models are specified as guarded high-level message sequence charts, a language allowing us to integrate material provided by stakeholders such as multi-agent scenarios, decision trees, and flowchart fragments. The input models are compiled into guarded LTS, where transition guards on fluents support the integration of state-based and event-based analysis. The techniques supported by our tool include model checking against process-specific properties, invariant generation, and the detection of incompleteness, unreachability, and undesirable non-determinism in process decisions. They are based on a trace semantics of process models, defined in terms of guarded LTS, which are in turn defined in terms of pure LTS. The techniques complement our previous palette for synthesizing behavior models from scenarios and goals. The paper also describes our preliminary experience in analyzing cancer treatment processes using these techniques.
[unreachability detection, multi-agent systems, program verification, Unified modeling language, behavior model synthesis, Medical services, undesirable non determinism, task analysis, formal specification, Flowcharts, state-based technique, control flow, critical process model analysis, Performance analysis, Decision trees, incompleteness detection, Radio control, Assembly, program diagnostics, Medical treatment, cancer treatment process, task model analysis, high-level message sequence chart, programming language semantics, process model semantics trace, invariant generation, model checking, process-specific property, System recovery, cancer, medical computing, event-based technique, Cancer]
Validation of contracts using enabledness preserving finite state abstractions
2009 IEEE 31st International Conference on Software Engineering
None
2009
Pre/post condition-based specifications are common-place in a variety of software engineering activities that range from requirements through to design and implementation. The fragmented nature of these specifications can hinder validation as it is difficult to understand if the specifications for the various operations fit together well. In this paper we propose a novel technique for automatically constructing abstractions in the form of behaviour models from pre/post condition-based specifications. The level of abstraction at which such models are constructed preserves enabledness of sets of operations, resulting in a finite model that is intuitive to validate and which facilitates tracing back to the specification for debugging. The paper also reports on the application of the approach to an industrial strength protocol specification in which concerns were identified.
[program debugging, Protocols, post condition-based specification, software engineering activities, Debugging, enabledness preservation, Educational institutions, State-space methods, Application software, formal specification, Runtime, industrial strength protocol specification, Automata, Writing, debugging, finite state abstractions, contracts validation, Contracts, Software engineering, precondition-based specification]
WISE: Automated test generation for worst-case complexity
2009 IEEE 31st International Conference on Software Engineering
None
2009
Program analysis and automated test generation have primarily been used to find correctness bugs. We present complexity testing, a novel automated test generation technique to find performance bugs. Our complexity testing algorithm, which we call WISE (worst-case inputs from symbolic execution), operates on a program accepting inputs of arbitrary size. For each input size, WISE attempts to construct an input which exhibits the worst-case computational complexity of the program. WISE uses exhaustive test generation for small input sizes and generalizes the result of executing the program on those inputs into an ldquoinput generator.rdquo The generator is subsequently used to efficiently generate worst-case inputs for larger input sizes. We have performed experiments to demonstrate the utility of our approach on a set of standard data structures and algorithms. Our results show that WISE can effectively generate worst-case inputs for several of these benchmarks.
[Algorithm design and analysis, Performance evaluation, program debugging, program testing, Data structures, WISE, Computational complexity, automated test generation, Programming profession, correctness bugs, performance bugs, Jacobian matrices, worst-case computational complexity, worst-case inputs, complexity testing, Automatic testing, Computer bugs, program analysis, Benchmark testing, symbolic execution, Performance analysis, computational complexity]
Taint-based directed whitebox fuzzing
2009 IEEE 31st International Conference on Software Engineering
None
2009
We present a new automated white box fuzzing technique and a tool, BuzzFuzz, that implements this technique. Unlike standard fuzzing techniques, which randomly change parts of the input file with little or no information about the underlying syntactic structure of the file, BuzzFuzz uses dynamic taint tracing to automatically locate regions of original seed input files that influence values used at key program attack points (points where the program may contain an error). BuzzFuzz then automatically generates new fuzzed test input files by fuzzing these identified regions of the original seed input files. Because these new test files typically preserve the underlying syntactic structure of the original seed input files, they tend to make it past the initial input parsing components to exercise code deep within the semantic core of the computation. We have used BuzzFuzz to automatically find errors in two open-source applications: Swfdec (an Adobe Flash player) and MuPDF (a PDF viewer). Our results indicate that our new directed fuzzing technique can effectively expose errors located deep within large programs. Because the directed fuzzing technique uses taint to automatically discover and exploit information about the input file format, it is especially appropriate for testing programs that have complex, highly structured input file formats.
[program debugging, Law, program testing, Instruments, Laboratories, dynamic taint tracing, BuzzFuzz, Open source software, taint-based directed whitebox fuzzing, Computer science, Automatic testing, Character generation, MuPDF, Libraries, Artificial intelligence, Legal factors, Swfdec]
Do code clones matter?
2009 IEEE 31st International Conference on Software Engineering
None
2009
Code cloning is not only assumed to inflate maintenance costs but also considered defect-prone as inconsistent changes to code duplicates can lead to unexpected behavior. Consequently, the identification of duplicated code, clone detection, has been a very active area of research in recent years. Up to now, however, no substantial investigation of the consequences of code cloning on program correctness has been carried out. To remedy this shortcoming, this paper presents the results of a large-scale case study that was undertaken to find out if inconsistent changes to cloned code can indicate faults. For the analyzed commercial and open source systems we not only found that inconsistent changes to clones are very frequent but also identified a significant number of faults induced by such changes. The clone detection tool used in the case study implements a novel algorithm for the detection of inconsistent clones. It is available as open source to enable other researchers to use it as basis for further investigations.
[Software maintenance, Java, Costs, clone detection, program verification, public domain software, Cloning, software maintenance, program compilers, code duplication, Open source software, software fault tolerance, Fault diagnosis, code cloning, open source system, maintenance cost, Software quality, Large-scale systems, defect-prone, system faults, commercial system, Detection algorithms, Software engineering, program correctness]
Mining exception-handling rules as sequence association rules
2009 IEEE 31st International Conference on Software Engineering
None
2009
Programming languages such as Java and C++ provide exception-handling constructs to handle exception conditions. Applications are expected to handle these exception conditions and take necessary recovery actions such as releasing opened database connections. However, exception-handling rules that describe these necessary recovery actions are often not available in practice. To address this issue, we develop a novel approach that mines exception-handling rules as sequence association rules of the form ldquo(FC<sub>c</sub> 1...FC<sub>c</sub> n) nland FC<sub>a</sub> rArr (FC<sub>e</sub> 1...FC<sub>e</sub> m)rdquo. This rule describes that function call FCa should be followed by a sequence of function calls (FC<sub>e</sub> 1...FC<sub>e</sub> m) when FC<sub>a</sub> is preceded by a sequence of function calls (FC<sub>e</sub> 1...FC<sub>c</sub> n). Such form of rules is required to characterize common exception-handling rules. We show the usefulness of these mined rules by applying them on five real-world applications (including 285 KLOC) to detect violations in our evaluation. Our empirical results show that our approach mines 294 real exception-handling rules in these five applications and also detects 160 defects, where 87 defects are new defects that are not found by a previous related approach.
[Java, C++, data mining, function calls, exception handling, mining exception-handling rules, opened database connections, C++ language, Data mining, Association rules, Application software, programming languages, Computer science, Degradation, Computer languages, Databases, Lead, sequence association rules]
Safe-commit analysis to facilitate team software development
2009 IEEE 31st International Conference on Software Engineering
None
2009
Software development teams exchange source code in shared repositories. These repositories are kept consistent by having developers follow a commit policy, such as ldquoProgram edits can be committed only if all available tests succeed.rdquo Such policies may result in long intervals between commits, increasing the likelihood of duplicative development and merge conflicts. Furthermore, commit policies are generally not automatically enforceable. We present a program analysis to identify committable changes that can be released early, without causing failures of existing tests, even in the presence of failing tests in a developer's local workspace. The algorithm can support relaxed commit policies that allow early release of changes, reducing the potential for merge conflicts. In experiments using several versions of a non-trivial software system with failing tests, 3 newly enabled commit policies were shown to allow a significant percentage of changes to be committed.
[Software testing, System testing, software development, program diagnostics, Project management, Programming, Control systems, safe-commit analysis, Guidelines, Computer science, Degradation, Failure analysis, program analysis, Software systems, software engineering]
Does distributed development affect software quality? An empirical case study of Windows Vista
2009 IEEE 31st International Conference on Software Engineering
None
2009
It is widely believed that distributed software development is riskier and more challenging than collocated development. Prior literature on distributed development in software engineering and other fields discuss various challenges, including cultural barriers, expertise transfer difficulties, and communication and coordination overhead. We evaluate this conventional belief by examining the overall development of Windows Vista and comparing the post-release failures of components that were developed in a distributed fashion with those that were developed by collocated teams. We found a negligible difference in failures. This difference becomes even less significant when controlling for the number of developers working on a binary. We also examine component characteristics such as code churn, complexity, dependency information, and test code coverage and find very little difference between distributed and collocated components to investigate if less complex components are more distributed. Further, we examine the software process and phenomena that occurred during the Vista development cycle and present ways in which the development process utilized may be insensitive to geography by mitigating the difficulties introduced in prior work in this area.
[Geography, Availability, Costs, Government, Programming, distributed processing, distributed software development, software quality, Cultural differences, post-release failures, Windows Vista, Software quality, operating systems (computers), software engineering, Global communication, Software engineering, Testing]
The impact of process choice in high maturity environments: An empirical analysis
2009 IEEE 31st International Conference on Software Engineering
None
2009
We present the results of a three year field study of the software development process choices made by project teams at two leading offshore vendors. In particular, we focus on the performance implications of project teams that chose to augment structured, plan-driven processes to implement the CMM level-5 key process areas (KPAs) with agile methods. Our analysis of 112 software projects reveals that the decision to augment the firm-recommended, plan-driven approach with improvised, agile methods was significantly affected by the extent of client knowledge and involvement, the newness of technology, and the project size. Furthermore this decision had a significant and mostly positive impact on project performance indicators such as reuse, rework, defect density, and productivity.
[Productivity, Software testing, CMM level-5 key process areas, offshore vendors, software development process choices, Project management, Standardization, Programming, Personnel, Certification, agile methods, Coordinate measuring machines, software process improvement, Software standards, project teams, Standards development]
How to avoid drastic software process change (using stochastic stability)
2009 IEEE 31st International Conference on Software Engineering
None
2009
Before performing drastic changes to a project, it is worthwhile to thoroughly explore the available options within the current structure of a project. An alternative to drastic change are internal changes that adjust current options within a software project. In this paper, we show that the effects of numerous internal changes can out-weigh the effects of drastic changes. That is, the benefits of drastic change can often be achieved without disrupting a project. The key to our technique is SEESAW, a novel stochastic stability tool that (a) considers a very large set of minor changes using stochastic sampling; and (b) carefully selects the right combination of effective minor changes. Our results show, using SEESAW, project managers have more project improvement options than they currently realize. This result should be welcome news to managers struggling to maintain control and continuity over their project in the face of multiple demands for drastic change.
[stochastic sampling, Uncertainty, project management, sampling methods, Stability, software process change, Laboratories, Stochastic processes, Project management, Process control, Personnel, software maintenance, software project, Computer science, Propulsion, Sampling methods, stochastic processes, stochastic stability tool]
UEMan: A tool to manage user evaluation in development environments
2009 IEEE 31st International Conference on Software Engineering
None
2009
One of the challenges in software development is to collect and analyze the end users' feedback in an effective and efficient manner. In this paper we present a tool to manage user evaluation alongside the process of software development. The tool is based on the idea that user evaluation should be managed iteratively from within the integrated development environment (IDE) in order to provide high quality user interface. The main capabilities include creating the experiment object as part of the software project; deriving development tasks from the analysis of evaluation data; and tracing these tasks to and from the code. Further, we provide a library to enable development of Java aspects for creation of automatic measures to increase the body of the evaluation data. Using this tool, development teams can manage user-centered design (UCD) activities at the IDE level, hence developing software products with an adequate level of usability.
[Java, Data analysis, software development, UEMan, User centered design, Programming, user interfaces, user evaluation management, software project, Environmental management, software libraries, user interface, tool, Software development management, Software libraries, library, Feedback, integrated development environment, User interfaces, user-centered design activities, software tools, programming environments, Quality management, user centred design]
TranStrL: An automatic need-to-translate string locator for software internationalization
2009 IEEE 31st International Conference on Software Engineering
None
2009
Software internationalization is often necessary when distributing software applications to different regions around the world. In many cases, developers often do not internationalize a software application at the beginning of the development stage. To internationalize such an existing application, developers need to externalize some hard-coded constant strings to resource files, so that translators can easily translate the application to be in a local language without modifying its source code. Since not all the constant strings require externalization, locating those need-to-translate constant strings is a basic task that the developers must conduct. In this paper, we present TranStrL, an Eclipse plug-in tool that automatically locates need-to-translate constant strings in Java code. Our tool maintains a pre-collected list of API methods related to the Graphical User Interface (GUI), and then searches for need-to-translate strings in the source code starting from the invocations of these API methods using string-taint analysis.
[TranStrL, application program interfaces, graphical user interfaces, Laboratories, Open source software, Databases, need-to-translate constant strings, software development stage, string locator, software engineering, Computer science education, string-taint analysis, Graphical user interfaces, Java, Software prototyping, hard-coded constant strings, graphical user interface, Educational technology, source code, Application software, API methods, Computer science, Eclipse plug-in tool, software internationalization, Java code, software application]
SmartTutor: Creating IDE-based interactive tutorials via editable replay
2009 IEEE 31st International Conference on Software Engineering
None
2009
Interactive tutorials, like Eclipse's cheat sheets, are good for novice programmers to learn how to perform tasks (e.g., checking out a CVS project) in an integrated development environment (IDE). Creating these tutorials often requires programming effort that is time-consuming and difficult. In this paper, we propose an approach using editable replay of user actions to help authors create interactive tutorials with little programming effort. User actions of performing a task can be recorded, edited, and presented as a tutorial. The tutorial can be replayed interactively for mentoring. We present our SmartTutor implementation in the Eclipse IDE and conduct a preliminary evaluation on it, which demonstrates efficiency gains for the tutorial authors.
[Tutorial, Educational programs, Navigation, IDE-based interactive tutorials, intelligent tutoring systems, Software performance, Educational technology, Documentation, Eclipse IDE, Programming profession, Employee welfare, Computer science, novice programmers, integrated development environment, interactive systems, cheat sheets, SmartTutor, Computer science education, editable replay, programming]
A toolset for automated failure analysis
2009 IEEE 31st International Conference on Software Engineering
None
2009
Classic fault localization techniques can automatically provide information about the suspicious code blocks that are likely responsible for observed failures. This information is useful, but not sufficient to completely understand the causes of failing executions, which still require further (time-consuming) investigations to be exactly identified. A useful and comprehensive source of information is frequently given by the set of unexpected events that have been observed during failures. Sequences of unexpected events are usually simple to be interpret, and testers can guess the expected correct sequences of events from the faulty sequences. In this paper, we present a tool that automatically identifies anomalous events that likely caused failures, filters the possible false positives, and presents the resulting data by building views that show chains of cause-effect relations, i.e., views that show when anomalous events are caused by other anomalous events. The use of the technique to investigate a fault in the Tomcat application server is also presented in the paper.
[Information resources, Law, Event detection, fault localization technique, Phase detection, automated failure analysis, Fault diagnosis, Filters, Failure analysis, fault tolerant computing, Tomcat application server, Web server, Legal factors, Testing]
JUnitMX - A change-aware unit testing tool
2009 IEEE 31st International Conference on Software Engineering
None
2009
Developers use unit testing to improve the quality of software systems. Current development tools for unit testing help with automating test execution, with reporting results, and with generating test stubs. However, they offer no aid for designing tests aimed specifically at exercising the effects of changes to a program. This paper describes a unit testing tool that leverages a change model to assist developers in the creation of new unit tests. The tool provides developers with quantitative feedback and detailed information about change effects, which not only facilitate the writing of more effective tests, but also motivate developers with an achievable coverage goal.
[Software testing, program change, System testing, JUnitMX-change-aware unit testing tool, program testing, software quality, software maintenance, test execution automation, Computer science, test stub generation, Automatic testing, software system quality improvement, Feedback, Writing, Software systems, Solids, software tools, Software tools, Protection]
CocoViz with ambient audio software exploration
2009 IEEE 31st International Conference on Software Engineering
None
2009
For ages we used our ears side by side with our ophthalmic stimuli to gather additional information, leading and supporting us in our visualization. Nowadays numerous software visualization techniques exist that aim to facilitate program comprehension. In this paper we discuss how we can support such software comprehension visualization with environmental audio and lead users to identify relevant aspects. We use cognitive visualization techniques and audio concepts described in our previous work to create an ambient audio software exploration (AASE) out of program entities (packages, classes ...) and their mapped properties. The concepts where implemented in a extended version of our tool called CocoViz. Our first results with the prototype shows that with this combination of visual and aural means we can provide additional information to lead users during program comprehension tasks.
[program comprehension, Visualization, Software prototyping, Software maintenance, Navigation, Documentation, Maintenance engineering, ophthalmic stimuli, software maintenance, software visualization, cognitive visualization techniques, Software packages, data visualisation, audio concepts, Ear, Packaging, ambient audio software exploration, Informatics, CocoViz]
ConcernLines: A timeline view of co-occurring concerns
2009 IEEE 31st International Conference on Software Engineering
None
2009
Understanding the evolution of a software system requires understanding how information about the release history, non-functional requirements and project milestones relates to functional requirements on the software components. This short paper describes a new tool, called CONCERNLINES, that supports this cognitive process by visualizing co-occurring concerns over time.
[Java, CONCERNLINES tool, Programming, software release history, Displays, reverse engineering, History, software component, Computer science, co-occurring concern visualization, non functional requirement, cognitive process, timeline view, Feedback, Data visualization, software evolution system understanding, User interfaces, Software systems, Frequency, program visualisation]
Alitheia Core: An extensible software quality monitoring platform
2009 IEEE 31st International Conference on Software Engineering
None
2009
Research in the fields of software quality and maintainability requires the analysis of large quantities of data, which often originate from open source software projects. Pre-processing data, calculating metrics, and synthesizing composite results from a large corpus of project artefacts is a tedious and error prone task lacking direct scientific value. The Alitheia Core tool is an extensible platform for software quality analysis that is designed specifically to facilitate software engineering research on large and diverse data sources, by integrating data collection and preprocessing phases with an array of analysis services, and presenting the researcher with an easy to use extension mechanism. The system has been used to process several projects successfully, forming the basis of an emerging ecosystem of quality analysis tools.
[Performance evaluation, Project management, software maintainability, Control systems, software quality, Open source software, Phased arrays, Alitheia Core, Databases, Software quality, software engineering, Monitoring, software quality monitoring, Quality management, Software engineering]
VIDA: Visual interactive debugging
2009 IEEE 31st International Conference on Software Engineering
None
2009
Software debugging is time-consuming and effort-consuming. Although software debugging, especially fault-localization, has been studied for long, few practical debugging tools have been developed and used by the industry. In this paper we present VIDA, a visual interactive debugging tool, which has been integrated with the Eclipse Integrated Development Environment to support a programmer's debugging process. During the programmer's conventional debugging process, VIDA continuously recommends break-points for the programmer based on the analysis of execution information and the gathered feedback from the programmer. Moreover, VIDA provides a program outline to help the programmer choose breakpoints and visualizes the static dependency relation to help the programmer make estimation at breakpoints.
[Visualization, Java, program debugging, software debugging, fault localization, program outline, History, Software debugging, Sun, Programming profession, Information analysis, Feedback, programmer debugging process, VIDA, Computer industry, visual programming, visual interactive debugging tool, Testing]
Feedback-driven requirements engineering: The Heuristic Requirements Assistant
2009 IEEE 31st International Conference on Software Engineering
None
2009
The complexity of today's software systems is constantly increasing. As a result, requirements for these systems become more comprehensive and complicated. In this setting, requirements engineers struggle to capture consistent and complete requirements of high quality. We propose a feedback-centric requirements editor to help analysts controlling the information overload. Our HeRA tool provides analysts with important data from various feedback facilities. The feedback is directly given based on the input to the editor. On the one hand, it is based on heuristic rules, on the other hand, on automatically derived models. Thus, when new requirements are added, the analyst gets important information on how consistent these requirements are with the existing ones.
[heuristic requirements assistant, Data analysis, Terminology, Unified modeling language, software systems, Documentation, formal specification, Delay, Information analysis, formal verification, feedback-driven requirements engineering, Feedback, HeRA tool, systems analysis, Automatic control, Software systems, Software engineering, feedback-centric requirements editor]
&#x00C6;vol: A tool for defining and planning architecture evolution
2009 IEEE 31st International Conference on Software Engineering
None
2009
Architecture evolution is a key feature of most software systems. There are few tools that help architects plan and execute these evolutionary paths. We demonstrate a tool to enable architects to describe evolution paths, associate properties with elements of the paths, and perform tradeoff analysis over these paths.
[associate properties, Software maintenance, software systems, Project management, Programming, architecture evolution, evolutionary paths, Computer science, AEligvol, software architecture, Software architecture, Computer architecture, Software systems, Large-scale systems, Performance analysis, tradeoff analysis, Software engineering]
Ldiff: An enhanced line differencing tool
2009 IEEE 31st International Conference on Software Engineering
None
2009
Differencing tools are highly relevant for a series of software engineering tasks, including analyzing developers' activities, assessing the changeability of software artifacts, and monitoring the maintenance of critical assets such as source clones and vulnerable instructions. This tool demonstration shows the features of ldiff, an enhanced, language-independent line differencing tool. L-diff builds upon the Unix diff and overcomes its limitations in determining whether an artifact line has been changed or is the result of additions and removals, and in tracking artifact fragments that have been moved upward or downward within the file. The paper describes the tool and shows its capability of analyzing changes on different kinds of software artifacts, including use cases, code developed with different programming languages, and test cases.
[Software testing, Algorithm design and analysis, Unix, line differencing tool, Software maintenance, software monitoring, Software algorithms, Cloning, mining software repositories, monitoring, software maintenance, task analysis, Ldiff, software evolution, differencing algorithm, Computer languages, software artifacts, software engineering tasks, Performance analysis, Software tools, Monitoring, Software engineering]
SemDiff: Analysis and recommendation support for API evolution
2009 IEEE 31st International Conference on Software Engineering
None
2009
As a framework evolves, changes in its application programming interface (API) can break client programs that extend the framework. Repairing a client program can be a challenging task because developers need to understand the context surrounding the API change. This paper describes SemDiff, a tool that recommends replacements for framework methods that were accessed by a client program and deleted during the evolution of the framework. SemDiff recommends replacements for non-trivial changes undiscovered by other change-detection techniques and also enables developers to look at the context of the changes that led to the deletion of a framework method.
[Software prototyping, Java, application program interfaces, application programming interface, Fingerprint recognition, Electronic mail, API evolution, Application software, change-detection technique, Computer science, Genetic programming, Prototypes, User interfaces, Writing, SemDiff]
CloneDetective - A workbench for clone detection research
2009 IEEE 31st International Conference on Software Engineering
None
2009
The area of clone detection has considerably evolved over the last decade, leading to approaches with better results, but at the same time using more elaborate algorithms and tool chains. In our opinion a level has been reached, where the initial investment required to setup a clone detection tool chain and the code infrastructure required for experimenting with new heuristics and algorithms seriously hampers the exploration of novel solutions or specific case studies. As a solution, this paper presents CloneDetective, an open source framework and tool chain for clone detection, which is especially geared towards configurability and extendability and thus supports the preparation and conduction of clone detection research.
[Software maintenance, clone detection tool chain, open source framework, Filtering, program verification, Heuristic algorithms, Cloning, CloneDetective workbench, software maintenance, Phase detection, software evolution, Plagiarism, Investments, Detectors, code infrastructure, Detection algorithms, Recommender systems, program correctness]
Save-IDE - A tool for design, analysis and implementation of component-based embedded systems
2009 IEEE 31st International Conference on Software Engineering
None
2009
The paper presents Save-IDE, an integrated development environment for the development of component-based embedded systems. Save-IDE supports efficient development of dependable embedded systems by providing tools for design of embedded software systems using a dedicated component model, formal specification and analysis of component and system behaviors already in early development phases, and a fully automated transformation of the system of components into an executable image.
[Real time systems, software component analysis, object-oriented programming, Save-IDE, Formal specifications, formal specification, System analysis and design, Embedded software, Analytical models, Software design, Image analysis, software tool, Embedded system, embedded systems, systems analysis, integrated development environment, Software systems, Timing, software tools, component-based embedded system design, programming environments, dedicated component model]
FeatureIDE: A tool framework for feature-oriented software development
2009 IEEE 31st International Conference on Software Engineering
None
2009
Tools support is crucial for the acceptance of a new programming language. However, providing such tool support is a huge investment that can usually not be provided for a research language. With FeatureIDE, we have built an IDE for AHEAD that integrates all phases of feature-oriented software development. To reuse this investment for other tools and languages, we refactored FeatureIDE into an open source framework that encapsulates the common ideas of feature-oriented software development and that can be reused and extended beyond AHEAD. Among others, we implemented extensions for FeatureC++ and FeatureHouse, but in general, FeatureIDE is open for everybody to showcase new research results and make them usable to a wide audience of students, researchers, and practitioners.
[Availability, Java, FeatureIDE, Programming, Mathematics, C++ language, software maintenance, Open source software, Computer science, Computer languages, feature-oriented software development, FeatureHouse, Investments, Education, FeatureC++, programming language, software tools, Informatics]
Synthesis of timed behavior from scenarios in the Fujaba Real-Time Tool Suite
2009 IEEE 31st International Conference on Software Engineering
None
2009
Based on a well-defined component architecture the tool supports the synthesis of so-called real-time statecharts from timed sequence diagrams. The two step synthesis process addresses the existing scalability problems by a proper decomposition and allows the user to define particular restrictions on the resulting statecharts.
[Real time systems, Mechatronics, real-time statecharts, Scalability, Unified modeling language, timed sequence diagrams, Communication system control, component architecture, Next generation networking, Connectors, Component architectures, software architecture, software tools, timed behavior, Intelligent systems, Fujaba real-time tool suite, Software engineering]
ContextServ: A platform for rapid and flexible development of context-aware Web services
2009 IEEE 31st International Conference on Software Engineering
None
2009
Context-aware Web services are currently emerging as an important technology for building innovative context-aware applications. Unfortunately, context-aware Web services are still difficult to build. This paper describes ContextServ, a platform for rapid development of context-aware Web services. ContextServ adopts model-driven development where context-aware Web services are specified using ContextUML, a UML based modeling language. The platform also offers a set of automated tools for generating and deploying executable implementations of context-aware Web services. This paper presents the motivation, system design, implementation, and usage of ContextServ.
[Context-aware services, Unified Modeling Language, Context awareness, Application software, Simple object access protocol, model-driven development, Intelligent sensors, Computer science, context-aware Web service, Web services, Food technology, ContextServ platform, Computer networks, ContextUML language, Context modeling, UML-based modeling language]
ReMan: A pro-active reputation management infrastructure for composite Web services
2009 IEEE 31st International Conference on Software Engineering
None
2009
REMAN is a reputation management infrastructure for composite Web services. It supports the aggregation of client feedback on the perceived QoS of external services, using reputation mechanisms to build service rankings. Changes in rankings are pro-actively notified to composite service clients to enable self-tuning properties in their execution.
[composite Web service, client feedback aggregation, Quality of service, Maintenance, quality of service, Engines, Web services, QoS, Feedback, self-tuning property, Computer architecture, ReMan pro-active reputation management infrastructure, service ranking, Standards development, Informatics, Monitoring, quality-of-service, Quality management]
ITACA: An integrated toolbox for the automatic composition and adaptation of Web services
2009 IEEE 31st International Conference on Software Engineering
None
2009
Adaptation is of utmost importance in systems developed by assembling reusable software services accessed through their public interfaces. This process aims at solving, as automatically as possible, mismatch cases which may be given at the different interoperability levels among interfaces by synthesizing a mediating adaptor. In this paper, we present a toolbox that fully supports the adaptation process, including: (i) different methods to construct adaptation contracts involving several services; (ii) simulation and verification techniques which help to identify and correct erroneous behaviours or deadlocking executions; and (iii) techniques for the generation of centralized or distributed adaptor protocols based on the aforementioned contracts. Our toolbox relates our models with implementation platforms, starting with the automatic extraction of behavioural models from existing interface descriptions, until the final adaptor implementation is generated for the target platform.
[Irrigation, Assembly systems, open systems, program verification, Quality of service, Web service, deadlock execution, verification technique, centralized adaptor protocol, protocols, Software reusability, Contracts, erroneous behaviour correction, simulation technique, Access protocols, software service reusability, interoperability, adaptation process, distributed adaptor protocol, Computer science, mediating adaptor synthesis, Web services, System recovery, software reusability, public interface, Software tools]
Performance modeling in industry: a case study on storage virtualization
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In software engineering, performance and the integration of performance analysis methodologies gain increasing importance, especially for complex systems. Well-developed methods and tools can predict non-functional performance properties like response time or resource utilization in early design stages, thus promising time and cost savings. However, as performance modeling and performance prediction is still a young research area, the methods are not yet well-established and in wide-spread industrial use. This work is a case study of the applicability of the Palladio Component Model as a performance prediction method in an industrial environment. We model and analyze different design alternatives for storage virtualization on an IBM* system. The model calibration, validation and evaluation is based on data measured on a System z9* as a proof of concept. The results show that performance predictions can identify performance bottlenecks and evaluate design alternatives in early stages of system development. The experiences gained were that performance modeling helps to understand and analyze a system. Hence, this case study substantiates that performance modeling is applicable in industry and a valuable method for evaluating design decisions.
[Unified modeling language, industrial environment, Throughput, virtualisation, Phase change materials, performance prediction, Analytical models, storage management, nonfunctional performance properties, Palladio component model, performance modeling, response time, Hardware, Software, storage virtualization, software engineering, Time factors, resource utilization, software performance evaluation, performance analysis]
Improving throughput via slowdowns
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Many service-oriented systems are not well equipped to guarantee that service time is optimized. We have specifically examined two industrial systems which implement service-oriented architectures in real, field environments. We discovered that both were not engineered to properly address surges in service request rate. In the absence of an integral solution, it is difficult and costly to (re-) engineer such a solution in the field. The challenge faced by this study was to deliver a low cost solution, without re-engineering the target systems. This paper introduces such a generic solution. The solution slows-down some components to deliver improvement in request service time. It was implemented, tested, and successfully applied to two industrial systems with no need to modify their logic or architecture. Experiments with those systems exhibited significant improvement in performance. These results have validated our solution and its industrial applicability across systems and environments.
[Actuators, object-oriented programming, service-oriented systems, Noise, software reliability, Throughput, real environments, automated performance management, Servers, service request rate, systems re-engineering, Runtime, field environments, self-managing systems, industrial systems, Computer architecture, service-oriented architectures, integral solution, Monitoring, service-oriented architecture]
A role-based qualification and certification program for software architects: an experience report from Siemens
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In this experience report, we describe the motivation, experience, lessons learned, and future directions of a software engineering curriculum used at a large international company. The "Curriculum for Software Engineers" project, which developed the content and a role-based qualification and certification program, was started at Siemens in 2006. This paper includes an overview of various kinds of certification in the software engineering area and why we chose the knowledge- and experience-based type of certification. The experience report part focuses mainly on the "certified senior software architect" role, as this role has the longest history and participants from many different business units and countries.
[education, Conferences, training, knowledge- experience-based certification, software architecture, software architect, knowledge based systems, Computer architecture, software engineering, Testing, Business, computer science education, role-based certification program, testing, curriculum, software engineering curriculum, international company, Certification, certification, Siemens report, business unit, educational courses, role-based qualification, Software, Qualifications]
Assessments in global software development: a tailorable framework for industrial projects
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Assessments are an effective technique for software quality assurance. As global software development (GSD) becomes the standard, an assessment framework must be flexible to support different sourcing and shoring models. Although much work exists on inspections and reviews, an assessment framework which addresses these challenges is missing. We present a systematic yet flexible assessment framework. The paper contributes: i) The description of our assessment framework which addresses four challenges: Appropriateness of a software requirements specification (SRS), viability of software architectures and SRS, wholeness of work packages, and compliance of results with predefined quality objectives. ii) A detailed explanation how the assessment framework can be tailored to support offshore and outsourcing scenarios. This paper describes the result of a two years research initiative at Capgemini SD&amp;M and serves the practitioner to implement assessment frameworks according to his needs. We also discuss open research questions of high relevance for the software industry.
[software architecture viability, DP industry, Programming, software quality, formal specification, assessment, Software architecture, global software development, assessment framework, software industry, SRS, outsourcing, quality assurance, compliance, Computer architecture, software packages, Logic gates, Software, software quality assurance, Outsourcing, work packages, software requirement specification]
Penalty policies in professional software development practice: a multi-method field study
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Organizational Punishment/Penalty is a pervasive phenomenon in many professional organizations. In some software development organizations, punishment measures have been adopted in an attempt to improve software developers' performance, reduce the software defects, and hence ensure software quality. It is unclear whether these measures are effective. This article presents the results of a multi-method field study that analyzes software engineers' perception towards penalty policies in relation to software quality in a software development process. The results were generated via both qualitative and quantitative methods. Through interviews, we collected the individuals' perception towards the penalty policy. By extracting data in a software configuration management system, we identified several patterns of defects change. We found that while a penalty mechanism does help to reduce software defects in daily coding activity, it fails in achieving programmers' maximum work potential. Meanwhile, experienced software programmers require less time to adapt to penalty policies and benefit from exist of less experienced developers. Some additional findings and implications are also discussed.
[software development process, penalty policies, perception and performance of software developers, Programming, human resource management, software quality, Quality assurance, professional software development practice, software developer performance improvement, Interviews, programmer maximum work potential, software defect reduction, multimethod field study, qualitative method, software development management, quantitative method, software engineer perception, software development organization, configuration management, organizational penalty, professional aspects, software configuration management system, Games, Organizations, organizational punishment, software defects, Software systems, personnel, organisational aspects, penalty policy]
A modeling language's evolution driven by tight interaction between academia and industry
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Domain specific languages play an important role in model-driven engineering of software-intensive industrial systems. A rich body of knowledge exists on the development of languages, modeling environments, and transformation systems. The understanding of architectural choices for combining these parts into a feasible solution, however, is not particularly deep. We report on an endeavor in the realm of a technology transfer process from academia to industry, where we encountered unexpected influences of the architecture on the modeling language. By examining the evolution of our language and its programming interface, we show that these influences mainly stemmed from practical considerations; for identifying these early on, tight interaction between our research lab and the industrial partner was key. In addition, we share insights into the practice of cooperating with industry by presenting essential lessons we learned.
[modeling language, Test facilities, Automation, model driven engineering architecture, domains specific language, Unified modeling language, Cloning, modeling environment, technology transfer, academic-industry cooperation, transformation systems, Engines, software-intensive industrial systems, technology transfer process, Prototypes, model-driven engineering, specification languages, domain specific languages, Clabjects, DSL, programming interface]
Staying afloat in an expanding sea of choices: emerging best practices for eclipse rich client platform development
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The Eclipse Rich Client Platform attracts considerable attention for being a promising candidate for providing the component model Java never had. This is even truer since the incorporation of OSGi for providing services within the framework. However, the rapid sequence of new versions and the continuous growth of features lead to a discussion that almost exclusively focused on technological aspects while leaving application developers in the midst of a sea of sometimes conflicting choices of how to implement their business-oriented applications. This lack of guidance leads to systems with vastly different architectures (or lack thereof) which often force complete rewrites when further development steps are to be taken. The best practices and architectural blueprints that provide this guidance in the field of object- or service-orientation haven't emerged yet. In this experience report, we render our observations made in several projects over the last years about the challenges that cooperating teams of application developers face when using RCP. We provide a first business-oriented architectural blue-print and best practices that have helped us greatly to overcome these challenges.
[Java, plug-in, OSGi, service-orientation, Eclipse, Switches, component model, Servers, object-orientation, Best practices, business-oriented architectural blue-print, component-based architecture, rich client platform, bundles, object-oriented languages, business-oriented applications, Libraries, Software, software engineering, Eclipse rich client platform development, architectural blueprints, RCP]
Integrating legacy systems with MDE
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Integrating several legacy software systems together is commonly performed with multiple applications of the Adapter Design Pattern in OO languages such as Java. The integration is based on specifying bi-directional translations between pairs of APIs from different systems. Yet, manual development of wrappers to implement these translations is tedious, expensive and error-prone. In this paper, we explore how models, aspects and generative techniques can be used in conjunction to alleviate the implementation of multiple wrappers. Briefly the steps are, (1) the automatic reverse engineering of relevant concepts in APIs to high-level models; (2) the manual definition of mapping relationships between concepts in different models of APIs using an ad-hoc DSL; (3) the automatic generation of wrappers from these mapping specifications using AOP. This approach is weighted against manual development of wrappers using an industrial case study. Criteria are the relative code length and the increase of automation.
[Adaptation models, Java, models, application program interfaces, Biological system modeling, mapping specification, Reverse engineering, bidirectional translation, Manuals, OO languages, aspects, software maintenance, MDE, Analytical models, legacy software system, adapter design pattern, Semantics, model-driven engineering, wrappers, legacy systems, API, automatic reverse engineering, automatic generation, relative code length]
Can clone detection support quality assessments of requirements specifications?
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Due to their pivotal role in software engineering, considerable effort is spent on the quality assurance of software requirements specifications. As they are mainly described in natural language, relatively few means of automated quality assessment exist. However, we found that clone detection, a technique widely applied to source code, is promising to assess one important quality aspect in an automated way, namely redundancy that stems from copy &amp; paste operations. This paper describes a large-scale case study that applied clone detection to 28 requirements specifications with a total of 8,667 pages. We report on the amount of redundancy found in real-world specifications, discuss its nature as well as its consequences and evaluate in how far existing code clone detection approaches can be applied to assess the quality of requirements specifications in practice.
[clone detection, Redundancy, Cloning, software requirements specification, source code, Inspection, Programming, software quality, formal specification, copy operation, quality assurance, quality assessment, requirements specification, Software, software engineering, Quality assessment, redundancy, paste operation]
Comprehending module dependencies and sharing
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software often lives in a complex software eco-system with complex interactions and dependencies between different modules or components. In Windows, this problem is exacerbated both by the overall system complexity and its closed source nature. Even when source is available, there are still interactions with modules which are only in binary form. This paper proposes two visualizations for investigating the dependencies between programs and other binaries, such as, dynamically linked libraries on Windows. Our visualizations are based on run-time traces obtained either from the Windows kernel or through binary instrumentation. Thus, our techniques do not need to rely on source code. We use the following scenarios to explain how our visualizations can be used to investigate various aspects of software dependencies: (i) visualizing whole system software dependencies; (ii) visualizing the interactions between selected modules of some software; (iii) discovering unexpected module interactions; and (iv) understanding the source of the modules being used.
[Context, Visualization, visualization, Windows kernel, Instruments, software ecosystem, shared library, source code, dependency graph, Databases, system complexity, module dependencies, Libraries, software engineering, software dependencies, Kernel, computational complexity]
Making defect-finding tools work for you
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Given the high costs of software testing and fixing bugs after release, early detection of bugs using static analysis can result in significant savings. However, despite their many benefits, recent availability of many such tools, and evidence of a positive return-on-investment, static-analysis tools are not used widely because of various usability and usefulness problems. The usability inhibitors include the lack of features, such as capabilities to merge reports from multiple tools and view warning deltas between two builds of a system. The usefulness problems are related primarily to the accuracy of the tools: identification of false positives (or, spurious bugs) and uninteresting bugs among the true positives. In this paper, we present the details of an online portal, developed at IBM Research, to address these problems and promote the adoption of static-analysis tools. We report our experience with the deployment of the portal within the IBM developer community. We also highlight the problems that we have learned are important to address, and present our approach toward solving some of those problems.
[Java, usability problems, program diagnostics, software testing, static analysis portal, portals, bug fixing, defect views, Servers, Open source software, online portal, IBM Research, Computer bugs, Null value, defect differencing, IBM developer community, defect merging, defect-finding tools, static-analysis tools, Usability, Portals, usefulness problems, defect prioritization]
Formalization and validation of a subset of the European Train Control System
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The European Train Control System (ETCS) is a control system for the interoperability of the railways across Europe. In this paper, we report on the activities of the EuRailCheck project, promoted by the European Railway Agency, for the development of a methodology and tools for the formalization and validation of the ETCS specifications. Within the project, we achieved three main results. First, we developed a methodology for the formalization and validation of the ETCS specifications. The methodology is based on a three-phases approach that goes from the informal analysis of the requirements, to their formalization and validation. Second, we developed a set of support tools, covering the various phases of the methodology. Third, we formalized a realistic subset of the specification in an industrial setting. The results of the project were positively evaluated by domain experts from different manufacturing and railway companies.
[EuRailCheck project, ETCS specifications, Unified modeling language, informal analysis, Europe, Companies, Control systems, interoperability, railway industry, railways, European train control system, formal methods, Syntactics, European railway agency, railway companies, Rail transportation, Joining processes, requirements validation, methodology]
From scripts to specifications: the evolution of a flight software testing effort
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
This paper describes the evolution of a software testing effort during a critical period for the flagship Mars Science Laboratory rover project at the Jet Propulsion Laboratory. Formal specification for post-run analysis of log files, using a domain-specific language, LogScope, replaced scripted real-time analysis. Log analysis addresses the key problems of on-the-fly approaches and cleanly separates specification and execution. Mining the test repository suggested the inadequacy of the scripted approach, and encouraged a partly engineer-driven development. LogScope development should hold insights for others facing the tight deadlines and reactionary nature of testing for critical projects. LogScope received a JPL Mariner Award for "improving productivity and quality of the MSL Flight Software" and has been discussed as an approach for other flight missions. We note LogScope features that most contributed to ease of adoption and effectiveness. LogScope is general and can be applied to any software producing logs.
[Jet Propulsion Laboratory, program testing, software testing effort, Laboratories, data mining, post-run analysis, temporal logic, formal specification, domain specific language, Space vehicles, flight missions, Semantics, MSL flight software, log files, aerospace computing, Libraries, runtime verification, log analysis, logs, test infrastructure, Python, LogScope, development practices, on-the-fly approach, program diagnostics, testing, Telemetry, space flight software, flagship Mars Science Laboratory rover project, LogScope development, scripted real-time analysis, JPL Mariner Award, Software, test repository, engineer driven development]
Experiences in initiating concurrency software research efforts
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Multi-core CPUs are now common in modern computers. To get access to effectively an unlimited supply of compute resources, software programs that have been highly optimized to use a single CPU need to be converted where possible to use concurrency. We have initiated our concurrency software research for performance enhancement on a large-scale system with high throughput and low latency transactions. In this paper, we report our experience, experiments, and results in various aspects of concurrency design and programming, including multi-threaded prototypes, static and dynamic concurrency analysis, future techniques and trends, concurrency experiments, and concurrency design patterns. Based on the concurrency experiments, we achieved at least 80 percent overall performance increases as measured by transaction throughput. As a result, capital expenditures for large scale deployments can be significantly reduced.
[multi-core, static concurrency analysis, parallelism, Programming, Servers, parallel programming, large-scale system, Concurrent computing, Runtime, Parallel processing, research and development, concurrency software research, software engineering, multicore CPU, Message systems, concurrency design, multiprocessing systems, multiprocessing programs, performance enhancement, multithreaded prototypes, dynamic concurrency analysis, large-scale systems, concurrency, Software, concurrency programming]
A cost-benefit framework for making architectural decisions in a business context
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In any IT-intensive organization, it is useful to have a model to associate a value with software and system architecture decisions. More generally, any effort-a project undertaken by a team-needs to have an associated value to offset its labor and capital costs. Unfortunately, it is extremely difficult to precisely evaluate the benefit of "architecture projects"-those that aim to improve one or more quality attributes of a system via a structural transformation without (generally) changing its behavior. We often resort to anecdotal and informal "hand-waving" arguments of risk reduction or increased developer productivity. These arguments are typically unsatisfying to the management of organizations accustomed to decision-making based on concrete metrics. This paper will discuss research done to address this long-standing dilemma. Specifically, we will present a model derived from analyzing actual projects undertaken at Vistaprint Corporation. The model presented is derived from an analysis of effort tracked against modifications to specific software components before and after a significant architectural transformation to the subsystem housing those components. In this paper, we will discuss the development, implementation, and iteration of the model and the results that we have obtained.
[Measurement, risk reduction, IT-intensive organization, structural transformation, commerce, developer productivity, dependency structure matrix, Training, software architecture, cost-benefit framework, cost-benefit analysis, Computer architecture, software components, architecture, design structure matrix, architectural decisions, system architecture decisions, Estimation, business context, Couplings, decision-making, quality attributes, Organizations, decision making, Vistaprint Corporation, software metrics]
Social computing networks: a new paradigm for engineering self-adaptive pervasive software systems
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software systems are increasingly permeating a variety of domains including medical, industrial automation, and emergency response. The advances in portable and embedded computing devices and the recent advances in wireless network connectivity have paved the way for the proliferation of smart spaces in such domains. At the same time, the emergence of service-oriented technology (e.g., web services [10]) and interoperability standards (e.g., WSDL [11], UDDI [5]) has made it possible to develop pervasive software systems intended for execution in smart spaces that were not even conceivable a few years back.
[portable computing devices, interoperability standards, open systems, Social network services, wireless network connectivity, self-adaptive pervasive software systems engineering, Ontologies, ubiquitous computing, Synthetic aperture sonar, social computing, service-oriented technology, Software systems, software engineering, embedded computing devices, smart spaces, Biomedical monitoring, Monitoring, service-oriented architecture]
An eclectic approach for change impact analysis
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Change impact analysis aims at identifying software artifacts being affected by a change. In the past, this problem has been addressed by approaches relying on static, dynamic, and textual analysis. Recently, techniques based on historical analysis and association rules have been explored. This paper proposes a novel change impact analysis method based on the idea that the mutual relationships between software objects can be inferred with a statistical learning approach. We use the bivariate Granger causality test, a multivariate time series forecasting approach used to verify whether past values of a time series are useful for predicting future values of another time series. Results of a preliminary study performed on the Samba daemon show that change impact relationships inferred with the Granger causality test are complementary to those inferred with association rules. This opens the road towards the development of an eclectic impact analysis approach conceived by combining different techniques.
[association rule, statistical learning, Time series analysis, data mining, mining software repositories, time series, Association rules, History, software maintenance, causality, software object, bivariate Granger causality test, software artifact, Software, Samba daemon, eclectic impact analysis, multivariate time series forecasting, learning (artificial intelligence), Bioinformatics, change impact analysis, Software engineering]
Domain-specific tailoring of code smells: an empirical study
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Code smells refer to commonly occurring patterns in source code that indicate poor programming practices or code decay. Detecting code smells helps developers find design problems that can cause trouble in future maintenance. Detection rules for code smells, based on software metrics, have been proposed, but they do not take domain-specific characteristics into consideration. In this study we investigate whether such generic heuristics can be tailored to include domain-specific factors. Input into these domain-specific heuristics comes from an iterative empirical field study in a software maintenance project. The results yield valuable insight into code smell detection.
[Measurement, domain-specific, source code, Programming, empirical study, Encoding, software maintenance, code decay, Couplings, software maintenance project, code smells, Semantics, domain-specific heuristics, Software, poor programming practices, domain-specific tailoring, Graphical user interfaces, software metrics]
CUTA4UML: bridging the gap between informal and formal requirements for dynamic system aspects
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In this paper, we describe our integrated approach to improve requirements elicitation and system specification by supporting a strong and direct involvement of all stakeholders, including non technical personnel, in the early phases of a project. This so-called CUTA4UML approach comprises a framework of methods, tools and feedback cycles that is based on the concept of Participatory Design (PD) and instantiated via an extended version of a user-driven "card game" (CUTA). This card game can be executed on paper or via electronic means. Our focus will be on the system's dynamic aspects which are, always, especially tricky to cover. We will also take a closer look at the inherently involved problem to match the rather informal results of such an card-based approach to a quite formal (UML -- Activity Diagrams) modeling technique -- a step necessary to make the results useful in a structured development process and, so far, one of the weak points in all of these approaches. All our concepts and conclusions are based on a sound experimental basis and have been evaluated and discussed with industrial partners, mostly in a context of small and medium sized companies.
[CUTA, formal requirement, Unified modeling language, system specification, formal specification, feedback cycles, card games, computer games, UML activity, software tools, informal requirement, Business, CUTA4UML approach, Unified Modeling Language, formal modeling technique, Object oriented modeling, UML activity diagrams, Educational institutions, participatory design, user-driven card game, dynamic system, software tool, Collaboration, systems analysis, Games, nontechnical personnel, requirements elicitation, Software engineering]
Can we certify systems for freedom from malware
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Malicious code is any code that has been modified with the intention of harming its usage or the user. Typical categories of malicious code include Trojan Horses, viruses, worms etc. With the growth in complexity of computing systems, detection of malicious code is becoming horrendously complex. For security of embedded devices it is important to ensure the integrity of software running in it. The general virus detection is undecidable. However, in the case of embedded systems or personal systems, the software and hardware configurations are known a priori. We are experimenting to see whether we can certify such systems for malware freedom. Most of the current efforts on malware detection rely heavily on detection of syntactic patterns. Malware writers are resorting to simple syntactic transformations (which preserve the program semantics) such as various compiler optimizations and program obfuscation techniques to evade detection. Our work is based on semantic behaviour of programs. We are working towards developing a model of the behaviour of a program executing in an environment. Our approach to detect tampering is based on benchmarking the behaviour of a program executing in an environment, and then matching the observed behaviour of the program in a similar environment with the benchmark (a la translation validation in a sense or bisimulation that is widely used in model checking). Since execution behaviour remains the same in majority of obfuscations, our approach is resilient to such exploits. We have performed several experiments in this direction and obtained encouraging results. Differences between the benchmarked behaviour and the observed behaviour quantifies the damage due to a virus. This enables us to arrive at refined notions of "harm" done by a virus and appropriate measures for protection.
[invasive software, worms, viruses, malware, syntactic patterns, malicious code detection, security, personal systems, program obfuscation, USA Councils, virus detection, Benchmark testing, Syntactics, embedded devices, computing systems, Malware, Software, Hardware, compiler optimization, Trojan horses]
Using dynamic execution traces and program invariants to enhance behavioral model inference
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software behavioral models have proven useful for design, validation, verification, and maintenance. However, existing approaches for deriving such models sometimes overgeneralize what behavior is legal. We outline a novel approach that utilizes inferred likely program invariants and method invocation sequences to obtain an object-level model that describes legal execution sequences. The key insight is using program invariants to identify similar states in the sequences. We exemplify how our approach improves upon certain aspects of the state-of-the-art FSA-inference techniques.
[TV, Law, program testing, legal execution sequence, Heuristic algorithms, Computational modeling, software behavioral model, FSA-inference technique, dynamic analysis, specification mining, program invariant, dynamic execution traces, behavioral model inference, system monitoring, Software, object-level model, likely invariants, Testing]
Synthesized essence: what game jams teach about prototyping of new software products
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The development of video games comprises engineering teams within various disciplines, e.g., software engineering, game production, and creative arts. Game jams are a promising approach for (software+) development projects to foster on new product development. This paper evaluates the concept of game jam, a community design/development activity, and its positive effects on new software product development with tight schedules in time-oriented, competitive environments. Game jams have received more public attention in recent times, but the concept itself has not been formally discussed so far. A game jam is a composition of design and development strategies: new product development, participatory design, lightweight construction, rapid experience prototyping, product-value focusing, aesthetics and technology, concurrent development and multidisciplinarity. Although game jams are normally used for rapid prototyping of small computer games, the constellation of the mentioned elements provides a powerful technique for rapidly prototyping new product ideas and disruptive innovations.
[software prototyping, prototyping, community design-development activity, new software product development, concurrent engineering, new product development, game development, lightweight construction, computer games, product-value focusing, product development, concurrent development, game jam, multidisciplinary strategy, Testing, innovation, Technological innovation, video game, software development management, software development project, participatory design, aesthetics, rapid experience prototyping, Collaboration, Games, Software, Product development, agile, Software engineering]
An analysis of the effects of company culture, education and experience on confirmation bias levels of software developers and testers
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In this paper, we present a preliminary analysis of factors such as company culture, education and experience, on confirmation bias levels of software developers and testers. Confirmation bias is defined as the tendency of people to verify their hypotheses rather than refuting them and thus it has an effect on all software testing.
[Measurement, program testing, company culture effects, software testing, Psychology, cognitive biases, Programming, Educational institutions, Cognition, education effect analysis, confirmation bias levels, professional aspects, software tester, Software, software engineering, software developer, confirmation bias, Testing]
Compose &#x0026; conquer: modularity for end-users
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Users have vast amounts of information at their disposal and access to many tools that can compute on that data. Often, no particular program can fulfill a user's needs; or, when such a program exists, it may be too obscure for the user to find. When users encounter this problem, they resort to ad-hoc approaches, such as importing and exporting data to different file formats, as a way to piece together disparate features from several different programs. As a result, users end up manually entering and manipulating data, defeating the purpose of automatic computing machines. In this paper, we introduce the concept of a Creativity Engine, which is like a search engine over the possible links between software libraries. A Creativity Engine can be used to bring together libraries that were never intended to work together, in way that can bring within users' reach the almost infinite flexibility their machines have to transform data.
[file format, architectural mismatch, data manipulation, personal computing, end-user modularity, Engines, Programming profession, software fault tolerance, software libraries, end-user programming, modularity, USA Councils, Animation, file organisation, Libraries, Software, software tools, data transformation, ad hoc approach, automatic computing machines, creativity engine]
Slicing and dicing bugs in concurrent programs
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
A lack of scalable verification tools for concurrent programs has not allowed concurrent software development to keep abreast with hardware trends in multi-core technologies. The growing complexity of modern concurrent systems necessitates the use of abstractions in order to verify all the expected behaviors of the system. Current abstraction refinement techniques are restricted to verifying mostly sequential and simpler concurrent programs. In this work, we present a novel incremental underapproximation technique that uses program slicing. Based on a reachability property, an initial backward slice for a single thread is generated. The information in the program slice is coupled with a concrete execution to drive the lone thread; generating an underapproximation of the program behavior space. If the target location is reached in the underapproximation, then we have an actual concrete trace. Otherwise, the initial single-thread slice is refined to include another thread that affects the reachability of the target location. In this case, the concrete execution only considers the two threads in the slice and preemption points between the threads only occur at locations in the slice. This refinement process is repeated until the target location is reached or is shown to be unreachable. Initial results indicate that the incremental technique can potentially allow the discovery of errors in larger systems using fewer resources and produce a better reduction in systems that are correct.
[program debugging, verification tool, program verification, multicore technology, bugs dicing, bugs slicing, concurrent engineering, incremental underapproximation technique, underapproximation, USA Councils, program slicing, incremental compilers, Testing, Runtime environment, Java, Limiting, reachability analysis, concurrent software development, abstraction refinement technique, concurrency, concurrent program, abstraction-refinement, Concrete, Software, reachability property, computational complexity]
Requirements reflection: requirements as runtime entities
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Computational reflection is a well-established technique that gives a program the ability to dynamically observe and possibly modify its behaviour. To date, however, reflection is mainly applied either to the software architecture or its implementation. We know of no approach that fully supports requirements reflection- that is, making requirements available as runtime objects. Although there is a body of literature on requirements monitoring, such work typically generates runtime artefacts from requirements and so the requirements themselves are not directly accessible at runtime. In this paper, we define requirements reflection and a set of research challenges. Requirements reflection is important because software systems of the future will be self-managing and will need to adapt continuously to changing environmental conditions. We argue requirements reflection can support such self-adaptive systems by making requirements first-class runtime entities, thus endowing software systems with the ability to reason about, understand, explain and modify requirements at runtime.
[Uncertainty, requirements, Biological system modeling, reflection, runtime, Cognition, requirement monitoring, self-adaptive system, formal specification, software system, software architecture, Runtime, requirement reflection, systems analysis, Computer architecture, Software, computational reflection, self-adaptive systems, Monitoring]
Adinda: a knowledgeable, browser-based IDE
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In practice, many people have to work together to develop and maintain a software system. However, the programmer's key tool, the Integrated Development Environment (IDE), is a solo-tool, serving to help individual programmers understand and modify the system. Such an IDE does not leverage the knowledge other team members may have of the design and implementation of the system. We propose to resolve this problem by exploring, experimentally, new ways of inferring knowledge from past IDE-interactions, and of maximizing collaboration among developers. Our approach, called Adinda, revolves around transforming the IDE into a set of integrated services, accessible via a web browser, and enriched with Web 2.0 technologies. Such services will not only help developers perform traditional IDE tasks, but also facilitate the required informal communication and collaboration needs of software development projects. In this paper, we report on our vision, approach and challenges for building Adinda, and initial results.
[Web 2.0, data mining, Programming, Web 2.0 technologies, IDE, Servers, Data mining, interaction mining, software engineering, project management, Browsers, software development projects, team members, Web browser, Collaboration, integrated software, collaboration, online front-ends, knowledgeable browser-based IDE, integrated development environment, Adinda, integrated services, Software, Internet, programming environments]
Code canvas: zooming towards better development environments
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The user interfaces of today's development environments have a "bento box" design that partitions information into separate areas. This design makes it difficult to stay oriented in the open documents and to synthesize information shown in different areas. Code Canvas takes a new approach by providing an infinite zoomable surface for software development. A canvas both houses editable forms of all of a project's documents and allows multiple layers of visualization over those documents. By uniting the content of a project and information about it onto a single surface, Code Canvas is designed to leverage spatial memory to keep developers oriented and to make it easy to synthesize information.
[document handling, Visualization, Navigation, software development, zoomable user interfaces, spatial memory, Programming, project document, integrated development environments, open document, user interfaces, software visualization, Engines, user interface, infinite zoomable surface, Layout, bento box design, data visualisation, code canvas, User interfaces, information synthesis, Software, data visualization]
Knowledge transfer in global software development: leveraging acceptance test case specifications
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Effective knowledge transfer (KT) is always important in software development projects, but crucial in global software development (GSD). Two challenges arise: First, reviews of the software requirements specification (SRS) are indispensable, but not always effective. Second, using knowledge representations that support KT from customers to developers is paramount. However, 'classical' SRS often don't support SRS comprehension of all stakeholders. We address these two challenges with a new approach that exploits the multi-fold power of a acceptance test case specifications (ATC-Specs): 1) A specific two-stage test-based review technique is used. We argue that these two-stage reviews of ATC-Specs increase the quality of the ATC-Specs and the SRS. 2) Additionally to the SRS, ATC-Specs are delivered to the offshore team, bridging the mental models of different stakeholders, and thus effectively transferring knowledge. We provide preliminary evidence of the validity of our approach based on a commercial GSD project at Capgemini SD&amp;M.
[project management, program testing, Conferences, software requirements specification, Knowledge representation, Programming, software development project, two-stage test-based review technique, knowledge management, formal specification, Knowledge transfer, ATC-Specs, knowledge transfer, global software development, knowledge representation, review, test case specification, acceptance test case specification, Software, Testing]
Dynamic symbolic data structure repair
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Generic repair of complex data structures is a new and exciting area of research. Existing approaches can integrate with good software engineering practices such as program assertions. But in practice there is a wide variety of assertions and not all of them satisfy the style rules imposed by existing repair techniques. I.e., a "badly" written assertion may render generic repair inefficient or ineffective. In this paper we build on the state of the art in generic repair and discuss how generic repair can work effectively with a wider range of correctness conditions. We motivate how dynamic symbolic techniques enable generic repair to support a wider range of correctness conditions and present DSDSR, a novel repair algorithm based on dynamic symbolic execution. We implement the algorithm for Java and report initial empirical results to demonstrate the promise of our approach for generic repair.
[Java, program assertions, Heuristic algorithms, Instruments, Maintenance engineering, Data structures, data structure invariants, software maintenance, Engines, dynamic symbolic data structure repair, complex data structure, data structure repair, data structures, software engineering, dynamic symbolic execution, generic repair, Software engineering]
Towards better support for the evolution of safety requirements via the model monitoring approach
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The research is motivated by the challenge from the evolution of safety requirements, which leads to revision of system designs at design-time or post-implementation at a high cost. This paper proposes a complementary methodology, namely the model monitoring approach, to better support the evolution throughout the life-cycle at a lower cost.
[safety requirements, Computational modeling, requirements evolution, Microwave ovens, safety-critical software, Control systems, formal specification, B&#x0FC;chi automata, formal verification, Automata, safety, complementary methodology, model monitoring approach, Safety, Monitoring]
Supporting program comprehension with source code summarization
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
One of the main challenges faced by today's developers is keeping up with the staggering amount of source code that needs to be read and understood. In order to help developers with this problem and reduce the costs associated with it, one solution is to use simple textual descriptions of source code entities that developers can grasp easily, while capturing the code semantics precisely. We propose an approach to automatically determine such descriptions, based on automated text summarization technology.
[program comprehension, cost reduction, summary, automated text summarization, Natural languages, source code summarization, reverse engineering, Large scale integration, text summarization, software maintenance, code semantics, textual description, Semantics, Tagging, Software systems, software cost estimation, Software engineering]
Detecting recurring and similar software vulnerabilities
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
New software security vulnerabilities are discovered on almost daily basis and it is vital to be able to identify and resolve them as early as possible. Fortunately, many software vulnerabilities are recurring or very similar, thus, one could effectively detect and fix a vulnerability in a system by consulting the similar vulnerabilities and fixes from other systems. In this paper, we propose, SecureSync, an automatic approach to detect and provide suggested resolutions for recurring software vulnerabilities on multiple systems sharing/using similar code or API libraries. The core of SecureSync includes a usage model and a mapping algorithm for matching vulnerable code across different systems, a model for the comparison of vulnerability reports, and a tracing technique from a report to corresponding source code. Our preliminary evaluation with case studies showed the potential usefulness of SecureSync.
[tracing technique, Protocols, software security vulnerability, API library, Computational modeling, safety-critical software, mapping algorithm, Vectors, Security, Databases, application program interface, Software, Libraries, recurring software vulnerability, SecureSync approach, vulnerable code matching]
Bridging lightweight and heavyweight task organization: the role of tags in adopting new task categories
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In collaborative software development projects, tasks are often used as a mechanism to coordinate and track shared development work. Modern development environments provide explicit support for task management where tasks are typically organized and managed through predefined categories. Although there have been many studies that analyze data available from task management systems, there has been relatively little work on the design of task management tools. In this paper we explore how tagging with freely assigned keywords provides developers with a lightweight mechanism to further categorize and annotate development tasks. We investigate how tags that are frequently used over a long period of time reveal the need for additional predefined categories of keywords in task management tool support. Finally, we suggest future work to explore how integrated lightweight tool features in a development environment may improve software development practices.
[project management, software development, heavyweight task organization, Programming, software management, task management systems, annotations, collaborative software development projects, tags, USA Councils, collaboration, Organizations, groupware, Tagging, task categories, Software, software engineering, Interviews, lightweight task organization, Software engineering]
Syde: a tool for collaborative software development
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Team collaboration is essential for the success of multi-developer projects. When team members are spread across different locations, individual awareness of the activity of others drops due to communication barriers. We built Syde, a tool infrastructure to reestablish team awareness by sharing change and conflict information across developer's workspaces. Our main challenge is to balance the tradeoff between offering relevant information about the activity of the team and avoiding information overload. The novelty of our approach is that we model source code changes as first-class entities to record the detailed evolution of a multi-developer project. Hence, Syde delivers precise change information to interested developers.
[Visualization, team awareness, visualization, Conferences, Servers, change information, team working, Presses, multideveloper project, awareness, conflict information, groupware, software engineering, software tools, Syde, source coding, Object oriented modeling, information management, change, source code changes, information overload, collaborative software development, collaboration, Software, team collaboration]
StakeSource: harnessing the power of crowdsourcing and social networks in stakeholder analysis
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Projects often fail because they overlook stakeholders. Unfortunately, existing stakeholder analysis tools only capture stakeholders' information, relying on experts to manually identify them. StakeSource is a web-based tool that automates stakeholder analysis. It "crowdsources" the stakeholders themselves for recommendations about other stakeholders and aggregates their answers using social network analysis.
[crowdsourcing, Social network services, social networks, Educational institutions, Electronic mail, Computer science, stakeholder analysis tool, recommender systems, recommender system, stakeholder information, outsourcing, Web-based tool, StakeSource, stakeholder analysis, social network analysis, User interfaces, social networking (online), Libraries, Software, software engineering, Internet, software tools]
CoDesign: a highly extensible collaborative software modeling framework
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Large, multinational software development organizations face a number of issues in supporting software design and modeling by geographically distributed architects. To address these issues, we present CoDesign, an extensible, collaborative, event-based software modeling framework developed in a distributed, collaborative setting by our two organizations. CoDesign's core capabilities include real-time model synchronization between geographically distributed architects, as well as detection and resolution of a range of modeling conflicts via several off-the-shelf conflict detection engines.
[extensible collaborative software modeling framework, event-based software modeling framework, real-time model synchronization, Unified modeling language, multinational software development organization, software design, off-the-shelf conflict detection engines, Synchronization, Servers, Engines, software architecture, Detectors, Computer architecture, design, groupware, Software, CoDesign]
Flexible architecture conformance assessment with ConQAT
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The architecture of software systems is known to decay if no counter-measures are taken. In order to prevent this architectural erosion, the conformance of the actual system architecture to its intended architecture needs to be assessed and controlled; ideally in a continuous manner. To support this, we present the architecture conformance assessment capabilities of our quality analysis framework ConQAT. In contrast to other tools, ConQAT is not limited to the assessment of use-dependencies between software components. Its generic architectural model allows the assessment of various types of dependencies found between different kinds of artifacts. It thereby provides the necessary tool-support for flexible architecture conformance assessment in diverse contexts.
[Java, software architecture, Databases, Cloning, software system architecture, Computer architecture, Companies, flexible architecture conformance assessment, Software systems, ConQAT quality analysis framework, software quality, software component]
Visualizing the Java heap
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Many of the problems that occur in long-running systems involve the way that the system uses memory. We have developed a framework for extracting and building a model of the heap from a running Java system. Such a model is only useful if programmers can extract from it the information they need to understand, find, and eventually fix memory-related problems in their system. We demonstrate the tool in action, showing how it works dynamically on running processes and how it is designed to address a variety of specific memory issues.
[Java, Visualization, dynamic program understanding, long-running system, Debugging, Programming, object ownership, Sun, programming tools, storage management, memory management, Memory management, memory-related problem, Java heap, Software]
FLAT3: feature location and textual tracing tool
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Feature location is the process of finding the source code that implements a functional requirement of a software system. It plays an important role in software maintenance activities, but when it is performed manually, it can be challenging and time-consuming, especially for large, long-lived systems. This paper describes a tool called FLAT3 that integrates textual and dynamic feature location techniques along with feature annotation capabilities and a useful visualization technique, providing a complete suite of tools that allows developers to quickly and easily locate the code that implements a feature and then save these annotations for future use.
[FLAT3 tool, concept location, program comprehension, Visualization, Software maintenance, source code, information retrieval, Information retrieval, dynamic analysis, software maintenance, formal specification, textual tracing tool, software system, Computer science, visualization technique, feature annotation capability, Libraries, software evolution and maintenance, software tools, Software engineering, functional requirement, dynamic feature location]
Exemplar: EXEcutable exaMPLes ARchive
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Searching for applications that are highly relevant to development tasks is challenging because the high-level intent reflected in the descriptions of these tasks doesn't usually match the low-level implementation details of applications. In this demo we show a novel code search engine called Exemplar (EXEcutable exaMPLes ARchive) to bridge this mismatch. Exemplar takes natural-language query that contains high-level concepts (e.g., MIME, data sets) as input, then uses information retrieval and program analysis techniques to retrieve applications that implement these concepts.
[Java, search engines, Exemplar, natural language processing, program diagnostics, Documentation, natural language query, information retrieval, executable example archive, Engines, query processing, code search engine, program analysis techniques, Search engines, Software, Cryptography]
LSdiff: a program differencing tool to identify systematic structural differences
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Program differencing tools such as GNU diff identify individual differences but do not determine how those differences are related to each other. For example, an extract superclass refactoring on several subclasses will be represented by diff as a scattered collection of line additions and deletions which must be manually pieced together. In our previous work, we developed LSdiff, a novel program differencing technique that automatically identifies systematic structural differences as logic rules. This paper presents an LSdiff Eclipse plug-in that provides a summary of systematic structural differences along with textual differences within an Eclipse integrated development environment. This plugin provides several additional features to allow developers to interpret LSdiff rules easily, to select the abstraction level of program differencing analysis, and to reduce its running time through incremental program analysis.
[program differencing tool, program differencing analysis, structured programming, automatic systematic structural difference identification, program differencing, extract superclass, incremental program analysis, software evolution, textual differences, Systematics, Accuracy, LSdiff rules, USA Councils, logic programming, software tools, Eclipse integrated development environment, Navigation, program diagnostics, LSdiff Eclipse plug-in, inference mechanisms, logic rules, line deletion, Syntactics, Feature extraction, Software, code change, line addition]
Legacy component integration by the Fujaba real-time tool suite
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
We present a Tool Suite which supports the (re-)construction of a behavioral model of a legacy component based on a learning approach by exploiting knowledge of known models of the existing component environment. This in turn enables to check whether the legacy component can be integrated correctly into its environment.
[Real time systems, Context, learning approach, object-oriented programming, legacy component, Learning automata, Unified modeling language, safety-critical systems, formal verification, Automata, model-driven engineering, Fujaba realtime tool suite, integration, software tools, legacy system, Mathematical model, learning (artificial intelligence), Context modeling]
BPGen: an automated breakpoint generator for debugging
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
During debugging processes, breakpoints are frequently used to inspect and understand runtime behaviors of programs. Although most development environments offer convenient breakpoint facilities, the use of these environments usually requires considerable human efforts in order to generate useful breakpoints. Before setting breakpoints or typing breakpoint conditions, developers usually have to make some judgements and hypotheses on the basis of their observations and experience. To reduce this kind of efforts we present a tool, named BPGen, to automatically generate breakpoints for debugging. BPGen uses three well-known dynamic fault localization techniques in tandem to identify suspicious program statements and states, through which both conditional and unconditional breakpoints are generated. BPGen is implemented as an Eclipse plugin for supplementing the existing Eclipse JDT debugger.
[Java, program debugging, conditional breakpoints, Conferences, Eclipse JDT debugger, BPGen, Debugging, unconditional breakpoints, software fault tolerance, Nearest neighbor searches, breakpoint, Runtime, automated breakpoint generator, dynamic fault localization technique, debugging process, debugging, Software, Eclipse plugin, software tools, inspection, runtime behaviors inspection]
Runtime repair of software faults using event-driven monitoring
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In software with emergent properties, despite the best efforts to remove faults before execution, there is a high likelihood that faults will occur during runtime. These faults can lead to unacceptable program behavior during execution, even leading to the program terminating unexpectedly. Using a distributed event-driven runtime software-fault monitor to repair faulty states creates an enforceable runtime specification. Using such an architecture can help ensure that emergent systems operate within specification, increasing the reliability of such software.
[software reliability, Maintenance engineering, distributed processing, message broker, formal specification, event-driven monitoring, specifications, Engines, software fault tolerance, event-driven systems, software architecture, Runtime, temporal invariants, Games, Computer architecture, distributed event-driven runtime software-fault monitor, Software, runtime software-fault monitoring, runtime specification, rule engine, unacceptable program behavior, runtime repair, Monitoring, video games]
TestFul: automatic unit-test generation for Java classes
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
This paper presents TestFul, an Eclipse plugin for the generation of tests for Java classes. It is based on the idea of search-based testing, working both at class and method level. The former puts objects in useful states, used by the latter to exercise the uncovered parts of the class.
[Software testing, Java, automatic test pattern generation, program testing, Evolutionary computation, Search problems, Java class, automatic unit test generation, test generation, search-based testing, Java classes, Software, Eclipse plugin, Graphical user interfaces]
End-user requirements blogging with iRequire
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
End-user involvement in software engineering is an ambivalent topic. However, novel paradigms such as service-oriented computing suggest more active end-user involvement to gather individual needs for software personalization. In this paper, we present a mobile requirements elicitation tool which enables end-users to blog needs in situ without analysts' facilitation.
[Context, end-user involvement, Blogs, personal computing, Mobile communication, software personalization, Mobile handsets, requirements blogging, formal specification, mobile requirement elicitation tool, service-oriented computing, context detection, mobile computing, Databases, formal verification, systems analysis, Software, end-user requirement blogging, software engineering, requirements elicitation, Web sites, Software engineering, iRequire]
The small project observatory: a tool for reverse engineering software ecosystems
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software evolution researchers have focused mostly on analyzing single software systems. However, often projects are developed and co-exist within software ecosystems, i.e., the larger contexts of companies, research groups or open-source communities. We present The Small Project Observatory, a web-based analysis platform for ecosystem reverse engineering through interactive visualization and exploration.
[Context, Visualization, Ecosystems, Reverse engineering, software ecosystem, mining software repositories, ecosystem reverse engineering, software ecosystems, reverse engineering, software maintenance, software visualization, software evolution, Web-based analysis platform, Small Project Observatory, Observatories, software tool, Collaboration, Software, Internet, software tools, interactive visualization]
A research demonstration of code bubbles
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Today's integrated development environments (IDEs) are hampered by their dependence on files and file-based editing. We propose a novel user interface that is based on collections of lightweight editable fragments, called bubbles, which when grouped together form concurrently visible working sets. We describe the design of a prototype IDE user interface for Java based on working sets.
[Visualization, human factors, integrated development environments, user interfaces, navigation, debugging, concurrent views, Context, prototype IDE user interface, Java, file based editing, Navigation, source coding, text editing, code bubble, Debugging, Documentation, source code, Data structures, working set, lightweight editable fragments, integrated development environment, User interfaces, file organisation, bubbles]
A flexible tool suite for change-aware test-driven development of web applications
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Though Web Applications development fits well with Test-Driven Development, there are some problems that hinder its success. In this demo we present a tool suite to improve TDD; the suite supports the representation of web requirements using a domain-specific language and the automatic generation of interaction tests among others.
[automatic test software, automatic interaction test generation, Navigation, program testing, web requirements, Browsers, flexible tool suite, change management, change-aware test-driven development, TDD improvement, Publishing, domain-specific language, web engineering, TDD, Web application, specification languages, User interfaces, Software, Internet, software tools, DSL, Web requirements representation, Testing]
Reverse engineering with the reclipse tool suite
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Design pattern detection is a reverse engineering methodology that helps software engineers to analyze and understand legacy software by recovering its design and thereby aiding in the preparation of re-engineering activities. We present Reclipse, a reverse engineering tool suite for static and dynamic design pattern detection in combination with a pattern candidate rating used to assess the detection results' reliability.
[Reverse engineering, Educational institutions, reverse engineering, reclipse tool suite, software architecture, Runtime, static design pattern detection, dynamic design pattern detection, Software systems, software tools, legacy software engineer, reengineering activity, Pattern matching]
SOABench: performance evaluation of service-oriented middleware made easy
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
SOABench is a framework for the automatic generation, execution and analysis of testbeds for evaluating the performance of service-oriented middleware. Testbeds can be characterized in terms of the composite services to execute, the workload to generate, the deployment configuration to use, the performance metrics to gather, the data analyses to perform on them, and the reports to produce.
[Measurement, experiment automation, SOABench, program testing, Service oriented architecture, performance evaluation, Engines, Analytical models, testbed automatic generation, performance metrics, testbed execution, testbed generation, testbed analysis, service-oriented architectures, service-oriented middleware, composite services, web service compositions, DSL, service-oriented architecture, software performance evaluation, Testing, middleware, software metrics]
SM@RT: representing run-time system data as MOF-compliant models
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Runtime models represent the dynamic data of running systems, and enable developers to manipulate the data in an abstract, model-based way. This paper presents SM@RT, a tool that help realize runtime models on a wide class of systems. Receiving a meta-model specifying the target system's data type and an API description specifying how to manipulate the data, SM@RT automatically generates the synchronizer to maintain the runtime model for this system.
[run-time system data representation, application program interfaces, Computational modeling, MOF, Humanoid robots, meta-model, runtime model, Synchronization, Runtime, code generation, MOF-compliant model, Data models, API description, Androids, SM@RT, Context modeling]
Eliminating dead-code from XQuery programs
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
One of the challenges in web software development is to help achieving a good level of quality in terms of code size and runtime performance, for increasingly popular domain specific languages such as XQuery. We present an IDE equipped with static analysis features for assisting the programmer. These features are capable of identifying and eliminating dead code automatically. The tool is based on newly developed formal programming language verification techniques [4, 3], which are now mature enough to be introduced in the process of software development.
[Navigation, Web software development, formal verification technique, Switches, Programming, query languages, XQuery program, Runtime, formal verification, dead-code elimination, XML, programming language verification technique, specification languages, Syntactics, Books, functional languages]
EMFStore: a model repository for EMF models
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Models need to be put under version control to facilitate collaboration and to control change. EMFStore is a Software Configuration Management system tailored to the specific requirements for versioning models. It employs operation-based change tracking, conflict detection and merging.
[model repository, operation-based change tracking, merging, Object oriented modeling, Computational modeling, Unified modeling language, Merging, version control, EMF model, Servers, formal specification, EMFStore, configuration management, USA Councils, conflict detection, software configuration management system, versioning model, Software, software requirement]
CUTS: a system execution modeling tool for realizing continuous system integration testing
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
This paper presents the Component Workload Emulator (CoWork-Er) Utilization Test Suite (CUTS), which is a system execution modeling tool for validating quality-of-service (QoS) properties, e.g., end-to-end response time, throughput, and scalability, on the target architecture continuously throughout the software lifecycle.
[program testing, component workload emulator utilization test suite, end-to-end response time, software lifecycle, Quality of service, continuous system integration testing, system execution modeling tool, quality of service, CUTS, scalability, software architecture, Runtime, Computer architecture, Software, throughput, Mathematical model, quality-of-service, Load modeling, Testing]
SSG: a model-based development environment for smart, security-aware GUIs
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
We present a development environment for automatically building smart, security-aware GUIs following a model-based approach. Our environment consists of a number of plugins that have been developed using the Eclipse framework and includes three model editors, a model-transformation tool, and a code generator.
[Access control, user interface management systems, graphical user interfaces, Unified modeling language, model-based approach, Generators, formal specification, program compilers, model-based development environment, security of data, model editors, Eclipse framework, model-transformation tool, Data models, Software, smart security-aware GUI, code generator, programming environments, Graphical user interfaces]
Managing iterations with UNICASE
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Planning iterations in software projects requires considering artifacts from different aspects such as requirements, specifications, tasks or even bug reports. UNICASE is a unified CASE tool integrating these relevant artifacts into one model. We demonstrate how the tool supports planning and executing iterations.
[program debugging, project management, requirements, Conferences, iteration management, Unified modeling language, software development management, iteration planning, specification, software project, unified CASE tool, formal specification, System analysis and design, UNICASE, bug report, Analytical models, task, Software, computer aided software engineering, Planning, Software engineering]
JDF: detecting duplicate bug reports in Jazz
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Both developers and users submit bug reports to a bug repository. These reports can help reveal defects and improve software quality. As the number of bug reports in a bug repository increases, the number of the potential duplicate bug reports increases. Detecting duplicate bug reports helps reduce development efforts in fixing defects. However, it is challenging to manually detect all potential duplicates because of the large number of existing bug reports. This paper presents JDF (representing Jazz Duplicate Finder), a tool that helps users to find potential duplicates of bug reports on Jazz, which is a team collaboration platform for software development and process management. JDF finds potential duplicates for a given bug report using natural language and execution information.
[Measurement, Calculators, software development, natural language processing, Natural languages, software development management, information retrieval, Educational institutions, bug repository, software quality, Servers, JDF, team working, execution information, bug report, process management, Computer architecture, groupware, Software, bug reports, natural language, team collaboration]
Using invariant functions and invariant relations to compute loop functions
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In this short paper we discuss the design, implementation and operation of an automated tool that computes the function of while loops written in C-like programming languages.
[program control structures, program diagnostics, Lattices, high level languages, automated tool implementation, Programming, automated tool design, Function approximation, invariant relation, Equations, C-like programming languages, loop function computation, loop semantic, while loop, Computer languages, Semantics, invariant function, program analysis, software tools]
LM: a miner for scenario-based specifications
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
We present LM, a tool for mining scenario-based specifications in the form of Live Sequence Charts, a visual language that extends sequence diagrams with modalities. LM comes with a project management component, a wizard-like interface to the mining algorithm, a set of pre- and post-processing extensions, and a visualization module.
[Measurement, wizard-like interface, Visualization, project management, postprocessing extension, project management component, Software algorithms, data mining, scenario-based specification mining, software management, Data mining, visual languages, specification mining, formal specification, preprocessing extension, tool, visual language, live sequence charts, Semantics, Feature extraction, live sequence chart, visualization module, Software engineering]
RAW: runtime automatic workarounds
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Faults in Web APIs may escape the testing process, and therefore affect thousands of Web applications. As a consequence, users of these applications might suffer from related failures for a long time until proper fixes are released by the Web API developers. In this paper we present RAW, a tool that tries to find workarounds automatically and at runtime, thereby reducing the negative impact of faults in Web applications. Runtime and automatically deployed workarounds serve as a temporary relief for application users while proper fixes are developed and released.
[Google, testing process, application program interfaces, program testing, Web API developer, Educational institutions, RAW, runtime automatic workaround, Browsers, Servers, software fault tolerance, application user, Runtime, Web API fault, Web application, Pressing, Internet, Web sites, Testing]
Software engineering in South Africa
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The tourist slogan used to market South Africa A World in One Country cuts across many more dimensions than just those of interest to tourists. Everywhere in the country, there is evidence of both a highly advanced and sophisticated economy and lifestyle, as well as of poverty and underdevelopment. The purpose of this session is to reflect on whether and how this peculiar positioning of the country impacts on the IT industry in general, and on software engineering in particular. Based on their experience of South African IT in general, and on the practice, teaching and research of software engineering in particular, session panelists will give their perspectives on what is being done and on what should be done. Are the challenges and opportunities significantly different from elsewhere? Are there the opportunities, threats and challenges for disseminating IT skills into the underdeveloped contexts in South Africa and Africa? What does South Africa need to do to become the outsourcing point of choice for North Atlantic IT? How do companies that operate both in South Africa and elsewhere spread the development load, and what are the challenges in doing this?
[information technology, software engineering teaching, software engineering research, Programming, software engineering practice, Educational institutions, information industry, Information systems, outsourcing, IT skills, Software, software engineering, Software engineering, South African IT industry]
Portable secure identity management for software engineering
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Identity management refers to authentication, sharing of personally identifiable information (PII) and provision of mechanisms protecting the privacy thereof. The most commonly implemented is federated authentication, permitting users to maintain a single set of credentials to access many services. Specifications exist for profile exchange between a service provider (SP) and the identity provider (IdP), but are rarely used. Most frequently, local storage of profile data is utilised due to security and privacy concerns. Key work in this area includes that of the PRIME project, which provides privacy enhancing identity management. Their work utilises local data stores and/or trusted third parties.
[Access control, federated authentication, Radiation detectors, profile exchange, service provider, Mobile communication, privacy, personally identifiable information, Privacy, portable secure identity management, security, Authentication, message authentication, PRIME project, data privacy, software engineering, identity provider, Software engineering]
Dynamic service quality and resource negotiation for high-availability service-oriented systems
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The principle goal of our research project is to improve the availability of service-oriented systems. This is especially important in systems that cross organizational boundaries. In this paper we outline the main research questions, our approach, which is based on appropriate redundancy strategies, and the progress achieved to date.
[Availability, Redundancy, Service oriented architecture, high-availability service-oriented systems, redundancy strategy, software quality, availability, cross-organizational boundary, resource allocation, resource negotiation, service-oriented, Fault tolerant systems, dynamic service quality, redundancy, service-oriented architecture]
Zenet: generating and enforcing real-time temporal invariants
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Generating correct specifications for real-time event-driven software systems is difficult and time-consuming. Even when such specifications have been created, they are often used to guide development rather than state properties guaranteed by the actual system. We propose a specification generator that reads execution traces and can generate invariants with real-time constraints. That specification can also offer programmers the ability to repair violated invariants at runtime. Creating fault-tolerant systems in this manner would provide software engineers guarantees about the software's high-level operation and its ability to recover from errors.
[Real time systems, software high-level operation, real-time constraints, Maintenance engineering, real-time temporal invariant, runtime softwarefault monitoring, formal specification, Engines, software fault tolerance, Zenet, Runtime, specification generator, temporal invariants, Games, Software, software engineering, real-time event driven software system, rule engine, fault tolerant system, Monitoring, video games]
Balancing collaboration and discipline in software development processes
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Neither traditional, agile or free/open software development models can be effective to all projects contexts. We claim that collaboration and discipline can be the driver to tailor software development processes to meet projects and organizations needs. This work proposes that process tailoring can be conducted through a context management approach.
[agile software development model, software development process, public domain software, software prototyping, Programming, agile methods, free/open source software, groupware, software development processes, free software development model, Context, project management, Social network services, context management approach, software development management, process tailoring, open software development model, software development projects, Collaboration, collaboration, context management, Organizations, Software, organisational aspects, Context modeling]
A proposal for consistency checking in dynamic software product line models using OCL
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Ubiquitous applications use context information to provide services and relevant information for their users. On the other hand, in Software Product Line approaches, commonality and variability of a system family should be identified and documented through variability modeling. Thus, one of the challenges to build Context-Aware Product Lines, called Dynamic Software Product Lines, is the consistent representation of context information that influences the variability model. This work proposes the use of UML profiles and OCL to formalize and represent variability and context concepts in a consistent manner.
[Context, Adaptation models, dynamic software product line models, program verification, Unified Modeling Language, Computational modeling, system commonality, Unified modeling language, ubiquitous computing, formal specification, consistency checking, context-aware product line, dynamic software product line model, Prototypes, UML profile, context information, object-oriented languages, variability modeling, context concept, OCL, Software, ubiquitous applications, system variability, Context modeling]
Behavioural validation of software engineering artefacts
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software engineering artefacts that define behaviour tend to be of a fragmented nature in order to facilitate their construction, modification, and modular reasoning (e.g. modular code, pre/post-conditions specifications). However, fragmentation makes the validation of global behaviour difficult. Typically synthesis techniques that yield global representations of large and potentially infinite states are used in combination with simulation, animation or partial explorations, techniques which necesarily loose the global view of system behaviour. I aim to develop abstraction-for-validation techniques that automatically produce finite state abstractions that are sufficiently small to support validating the emergent behaviour of a fragmented description "at a glance".
[approximation theory, finite state abstraction, code understanding, behaviour models, contract conformance, abstraction-for-validation technique, Complexity theory, Approximation methods, software engineering artefact, Analytical models, behavioural validation, finite state approximation, Software, software engineering, Cognitive science, Contracts, Software engineering]
Umple: a model-oriented programming language
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Our research tool, Umple, has the objective of raising the abstraction level of programming languages by including modeling abstractions such as UML associations, attributes and state machines. My research focuses on the syntax and semantics of state machines in Umple and the empirical validation of Umple as a whole.
[program verification, Unified modeling language, UML association, Programming, abstraction level, Encoding, finite state machines, programming languages, Umple, modeling abstraction, Semantics, Syntactics, state machine, Software, programming language, model-oriented programming language]
An incremental methodology for quantitative software architecture evaluation with probabilistic models
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Probabilistic models are crucial in the quantification of non-functional attributes in safety-and mission-critical software systems. These models are often re-evaluated in assessing the design decisions. Evaluation of such models is computationally expensive and exhibits exponential complexity with the problem size. This research aims at constructing an incremental quality evaluation framework and delta evaluation scheme to address this issue. The proposed technique will provide a computational advantage for the probabilistic quality evaluations enabling their use in automated design space exploration by architecture optimization algorithms. The expected research outcomes are to be validated with a range of realistic architectures and case studies from automotive industry.
[incremental methodology, automated design space exploration, safety-critical software, software quality, architecture evaluation, Optimization, probabilistic model, software architecture, architecture optimization algorithm, optimisation, incremental evaluation models, Computer architecture, probabilistic quality evaluation, probabilistic properties, Mathematical model, automotive industry, mission-critical software system, Computational modeling, probability, delta evaluation scheme, Probabilistic logic, delta evaluation, realistic architecture, automobile industry, safety-critical software system, exponential complexity, quantitative software architecture evaluation, Markov processes, design decision, nonfunctional attribute quantification, Reliability, incremental quality evaluation framework, computational advantage]
Synthesize software product line
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In the development of a Software Product Line (SPL), it is useful to compare various products in order to identify reusable assets and synthesize them in an optimized way. Current differencing approaches provide the difference on a low level thus still leaves the SPL practitioner considerable manual synthesis work. This paper presents a comparison approach based on Common Variability Language (CVL), which is able to identify the difference on a higher conceptual level. We believe that our CVL Compare approach will offer better model comparison support in the context of identifying and synthesizing SPLs.
[Context, CVL Compare approach, common variability language, Unified modeling language, software product line synthesis, SPL development, Signal resolution, Domain specific languages, USA Councils, software reusability, Software, software cost estimation, reusable assets identification, DSL]
Choreography of intelligent e-services
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Electronic Services (e-Services), referred to as a set of automated enterprise services using ICT to achieve a business goal, have significantly contributed to the growth of e-commerce, science, and telecommunications. However, applications that use e-Services seldom interoperate effectively, and this restricts the benefits they offer. The main purpose of e-Services is to have an anthology of network-resident software services accessed via standardised protocols whose meaning can be regularly discovered and integrated into applications. Our study supplements the definition provided for e-Service by complementing it with an intelligent capability for the purpose of effective and efficient choreography of processes, hence the term "intelligent e-Services". The aim of the study is to propose the composition of intelligent e-Services in a manner which encourages the interoperability of a range of services pertaining to various autonomous virtual enterprises (VEs). It is expected that a framework that defines and supports the composition of intelligent e-Services will be formed. It is also anticipated that the study will play an important role in contributing to the formation of dynamic virtual enterprises (DVEs) as an application business scenario.
[intelligent e-service, open systems, Service oriented architecture, interoperability, Microstrip, dynamic virtual enterprise, telecommunications, Semantic Web, electronic service, e-services, choreography, composition, Prototypes, Collaboration, automated enterprise service, science, autonomous virtual enterprise, virtual enterprises, e-commerce, network-resident software service, standardised protocol, Business, electronic commerce]
A framework for handling variants of software models
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The United Nations Centre for Trade Facilitation and Electronic Business (UN/CEFACT) envisions seamless information exchange between business partners in electronic commerce. Therefore, UN/CEFACT provides the UML Profile for Core Components for the definition of document models based on UML class diagrams. Having used this approach for three years in practice, it became evident that managing document model versions is a prerequisite for successfully utilizing Core Components. While managing software versions in the area of Software Engineering is well understood and successfully applied in industrial projects, the direct application of the same techniques for versioning models is conditionally appropriate. In this research abstract we propose to combine techniques from traditional Software Configuration Management with the concepts of reference modeling, where similar problems are addressed in a different context.
[Adaptation models, program verification, Unified modeling language, United Nations Centre for Trade Facilitation and Electronic Business, reference modeling, industrial projects, business partners, core component utilization, UN-CEFACT, software process improvement, software engineering, Business, electronic commerce, software model variant handling framework, object-oriented programming, Unified Modeling Language, software version management, Computational modeling, UML class diagrams, software configuration management, seamless information exchange, UML profile, model-driven engineering, Software systems, version management, document model version management, Software engineering]
Improving wide-area distributed system availability
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The Software-as-a-Service (SaaS) paradigm and corresponding service-oriented technologies have simplified the development of larger, more complex software systems that routinely span administrative and organisational boundaries. These systems inhabit a complex operating environment with numerous threats to the dependability of service compositions. These threats include many system-level failures whose causes are difficult and time-consuming to determine. It is difficult to detect vulnerabilities to these failures prior to deployment of an application into production and applications are currently not well-equipped to handle them effectively. This results in lengthy downtimes of production systems and hence low availability. The goal of this PhD is to increase the availability of such systems by eliminating as many failures as possible before deployment and by assisting administrators to diagnose their causes more efficiently. We propose a novel monitoring technique and apply failure injection techniques that target these difficult failures and enable separate administrative domains to cooperate in handling them. Furthermore, we investigate the extent to which we can equip these systems to be self-diagnosing.
[Availability, Protocols, program diagnostics, wide area distributed system availability, software as a service paradigm, self-diagnosing, system recovery, system level failure, failure injection techniques, Web services, Machine learning, service oriented technologies, complex software systems, monitoring technique, cloud computing, Monitoring, service-oriented architecture, Testing]
Risk assessment on distributed software projects
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Risk Assessment is a growing discipline on the context of distributed software projects. The complexity to develop software and the related knowledge require automated support for project managers in order to analyze actions to reduce the project's risks and measure the impact of such actions. My thesis aims to investigate an approach for the assessment of risks in globally distributed software projects. This research proposes to apply stochastic simulation technique to analyze project data and identify factors that are likely to impact team productivity and that could affect the team's ability to meet its schedule objective. We aim this thesis to be applied by project management groups to perform risk assessment early in the software development process and to help decision-making process.
[Schedules, risk assessment, software development process, project data analysis, distributed software projects, Project management, Programming, distributed processing, digital simulation, team productivity, global software development, stochastic models, decision-making process, stochastic processes, team ability, risk management, project management, data analysis, Computational modeling, software development management, project management group, stochastic simulation technique, decision making, Software, Risk management, Software engineering]
VisAr3D: an approach to software architecture teaching based on virtual and augmented reality
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
This paper aims to present an approach entitled VisAr3D to support software architecture teaching by means of virtual and augmented reality. Thus, it intends to define a 3D visualization environment which includes exploration, interaction and simulation resources to establish a practical and attractive learning, focusing on large scale systems.
[computer science education, 3D visualization environment, virtual reality, software architecture teaching, attractive learning, augmented reality, teaching, Training, software architecture, Software architecture, data visualisation, Computer architecture, large scale system, software engineering education, Three dimensional displays, Software, VisAr3D, computer aided instruction]
Change impact analysis from business rules
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Impact analysis is the identification of the potential consequences of a change, or estimating what needs to be modified to accomplish a change, including related costs and schedule estimates. In this work, we distinguish between two kinds of concerns related to impact analysis: (1) business-specific concerns, those related to stakeholders interested in checking if other business rules are impacted by the change and also need to be modified; and (2) software-specific concerns, those related to stakeholders interested in the impacted software artifacts that need to be modified. Several traceability techniques have been studied and none of them supported impact analysis that dealt with business-specific concerns with reasonable values of precision and recall for the discovered impacts. Our research work aims to support business-specific concerns during impact analysis, by proposing and evaluating a traceability technique that resorts on a new traceability model defined over business rules, with expected precision and recall values of 100%.
[program diagnostics, traceability techniques, business rule, business rules, traceability technique, business-specific concern, impact analysis, Analytical models, software-specific concern, USA Councils, Semantics, management of change, business concerns, Finite impulse response filter, software artifact, Software, software engineering, change impact analysis, business data processing, Business, Software engineering]
Software architecture for systems of software intensive systems (S3): the concepts and detection of inter-system relationships
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Key to software architecture is the description of relationships between software components [10] supported by commonly understood semantic definitions [9][8]. However, the definitions do not adequately capture the inter-system level software relationships. This leaves software architects either unaware of critical relationships or, to 'roll their own' based on aggregations of code-level call/use structures. This leads to critical gaps in the architectural description and communication problems within distributed development environments - as poorly understood relationships can inadvertently propagate changes and break system interoperability [2]. The solution requires a description of new system level relationships and a new systematic, repeatable technique to detect both immediate and linked system level relationships. The solution will be developed through the mining of existing software ecosystems and industry systems of software intensive systems (S3) architectures. Validation will be performed through case studies from industry collaborations.
[open systems, systems-of-systems, data mining, software intensive systems, distributed development environment, Twitter, software architecture, code-level call structure, architectural description, software ecosystem mining, Software architecture, system interoperability, linked system level relationship, Computer architecture, software components, distributed object management, program control structures, object-oriented programming, Computational modeling, communication problem, semantic definition, Couplings, code-level use structure, Software, intersystem level software relationship]
Formal methods for web services: a taxonomic approach
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Formal methods can be used to verify different perspective of a Web service. An ensemble of specific techniques is not supported by a general approach to the problem. To understand which formal method should be combined and used is a challenge. This paper outlines our approach to address this problem.
[Context, service development life-cycle, service verification, Web services, Algebra, formal verification, web application, Taxonomy, formal methods, taxonomic approach, Software reliability, web service]
Exploratory study of a UML metric for fault prediction
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
This paper describes the use of a UML metric, an approximation of the CK-RFC metric, for predicting faulty classes before their implementation. We built a code-based prediction model of faulty classes using Logistic Regression. Then, we tested it in different projects, using on the one hand their UML metrics, and on the other hand their code metrics. To decrease the difference of values between UML and code measures, we normalized them using Linear Scaling to Unit Variance. Our results indicate that the proposed UML RFC metric can predict faulty code as well as its corresponding code metric does. Moreover, the normalization procedure used was of great utility, not just for enabling our UML metric to predict faulty code, using a code-based prediction model, but also for improving the prediction results across different packages and projects, using the same model.
[Measurement, UML metric, Unified modeling language, regression analysis, Predictive models, code-based prediction model, unit variance, faulty code prediction, Mathematical model, logistic regression, Unified Modeling Language, Object oriented modeling, fault-proneness prediction, CK metrics, Chidamber and Kemerer-response for class metric, software fault tolerance, faulty classes prediction, UML, Collaboration, fault-prone code, Software, code metrics, linear scaling, fault prediction, normalization procedure, software metrics]
Feature-oriented requirements modelling
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Our goal is to develop a method for creating models of functional software requirements in which features are explicit, with a focus on the automotive software domain. In the current state of our proposed method, feature requirements are modelled using state machines that describe changes to a shared domain model.
[requirements, Object oriented modeling, Unified modeling language, feature-oriented requirements modelling, automotive software domain, Programming, feature interaction, finite state machines, formal specification, modelling, Automotive engineering, features, functional software requirement, state machine, Feature extraction, Software, Software engineering]
Automatic enforcement of architectural design rules
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Current techniques for modeling software architecture lacks support for the modeling of architectural design rules, i.e. rules defined by the architect that have to be followed in the detailed design. This is a problem in the context of Model-Driven Development in which it is assumed that major design artifacts are represented as formal or semi-formal models. The PhD project presented in this paper addresses this problem by the definition of a method for modeling architectural design rules in a form that is easily interpreted by developers. A tool for automatic validation of the design model against the architectural rules has also been developed. The method is designed to be easy to learn and use for both architects and developers. As a part of the PhD project the method is also currently validated in a case study on an industrial development project.
[architectural design rule modeling, program verification, Object oriented modeling, model-driven development (MDD), Unified modeling language, architectural rules, model-driven engineering (MDE), semiformal model, embedded software development, ubiquitous computing, automatic enforcement, Information systems, automatic validation, industrial development project, software architecture, Software architecture, PhD project, software architecture modeling, Computer architecture, Software, model-driven development context]
SMT-based bounded model checking for multi-threaded software in embedded systems
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The transition from single-core to multi-core processors has made multi-threaded software an important subject over the last years in computer-aided verification. Model checkers have been successfully applied to discover subtle errors, but they suffer from combinatorial state space explosion when verifying multi-threaded software. In our previous work, we have extended the encodings from SMT-based bounded model checking (BMC) to provide more accurate support for program verification and to use different background theories and solvers in order to improve scalability and precision in a completely automatic way. We now focus on extending this work to support an SMT-based BMC formulation of multithreaded software which allows the state space to be reduced by abstracting the number of state variables and interleavings from the proof of unsatisfiability generated by the SMT solvers. The core idea of our approach aims to extract the proof objects produced by the SMT solvers in order to control the number of interleavings and to remove logic that is not relevant to a given property. This work aims to develop a new algorithmic method and corresponding tools based on SMT to verify embedded software in multi-core systems.
[SAT modulo theories, combinatorial mathematics, program verification, Instruction sets, computer-aided verification, Embedded software, SMT-based BMC formulation, proof objects, embedded systems, state variable, embedded system, multithreaded software, multicore processor, combinatorial state space explosion, multiprocessing systems, Multicore processing, multi-threading, Computational modeling, Software algorithms, SMT-based bounded model checking, single-core processor, Encoding, multicore systems, computer aided verification, computer aided software engineering, formal software verification]
Enhancing collaboration of multi-developer projects with synchronous changes
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In a multi-developer project, team collaboration is essential for the success of the project. When team members are spread across different locations, informal interactions are lost, having an impact on individual awareness of the activity of others. In this scenario, collaboration becomes a challenge. A number of works have tried to reestablish team awareness by sharing change information across developers' workspaces. The main challenge of these approaches is to balance the tradeoff between offering useful information about the activity of others and avoiding information overload. In this work, we address the challenge of enhancing group awareness and stimulating collaboration by providing relevant information to developers in a non-intrusive manner. The novelty of our approach is that we model changes as first-class entities to deliver precise change information to targeted developers.
[Visualization, project management, team awareness, visualization, Object oriented modeling, software development management, change, Servers, assistance, change information, team member, team working, Presses, multideveloper projects, awareness, Collaboration, collaboration, management of change, Software, synchronous change, group awareness, team collaboration]
SOFAS: software analysis services
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
We propose a distributed and collaborative software analysis platform to enable seamless interoperability of software analysis tools across platform, geographical and organizational boundaries. In particular, we devise software analysis tools as services that can be accessed and composed over the Internet. These distributed services shall be widely accessible through a software analysis broker where organizations and research groups can register and share their tools. To enable (semi)-automatic use and composition of these tools, they will be classified and mapped into a software analysis taxonomy and adhere to specific meta-models and ontologies for their category of analysis. We claim that moving software analysis "outside the lab and into the Web" is highly beneficial from many point of views. Simple, common analyses can be effortlessly combined together into much meaningful, complex and novel ones. Analyses can be run everywhere and anytime without the need to install several tools and to cope with many output formats. Empirical studies can be easily replicated. At last, we claim that this will greatly help in the maturing of the field and boost its role in supporting software development practices.
[software analysis tools, tool composition, open systems, Ontologies, History, seamless interoperability, platform boundaries, Semantics, Computer architecture, groupware, analysis category, organizational boundaries, software tools, ontologies, program diagnostics, geographical boundaries, collaborative software analysis platform, SOFAS, Web services, software development practice, ontologies (artificial intelligence), distributed software analysis platform, metamodels, Internet, software analysis broker, software analysis services, distributed services]
Informal software design knowledge reuse
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In this paper, I describe a novel approach for developing and evaluating an infrastructure for supporting informal knowledge capture, representation, and reuse during software design meetings. It is the goal of this work to address the challenges that exist in reusing knowledge while at the whiteboard. The research centers on the design and evaluation of DesignMinders, a tool that augments electronic whiteboards to support the capture and exploration of knowledge using electronic note cards, which can be used over time to preserve knowledge generated during past sessions. The tool will be analyzed in an evaluation that will compare designs produced by two teams, one using DesignMinders and one not.
[note cards, electronic note cards, Conferences, Object oriented modeling, DesignMinders, informal knowledge capture, Programming, electronic whiteboards, whiteboard, Software design, Software architecture, informal software design knowledge reuse, knowledge representation, systems analysis, software tools]
The role of emergent knowledge structures in collaborative software development
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Many collaboration features in software development tools draw on lightweight technologies such as tagging and wikis. We propose to study the role of emergent knowledge structures created through these features. Using a mixed-methods approach, we investigate which processes emergent knowledge structures support and how tool support can leverage them.
[Web 2.0, software development, Collaborative software, mixed-methods approach, lightweight tool support, Programming, software development tool, collaborative software development, collaboration, groupware, emergent, Tagging, emergent knowledge structure support, Software, Internet, software tools, web 2.0, Feeds, Software engineering]
Capturing the long-term impact of changes
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Developers change source code to add new functionality, fix bugs, or refactor their code. Many of these changes have immediate impact on quality or stability. However, some impact of changes may become evident only in the long term. The goal of this thesis is to explore the long-term impact of changes by detecting dependencies between code changes and by measuring their influence on software quality, software maintainability, and development effort. Being able to identify the changes with the greatest long-term impact will strengthen our understanding of a project's history and thus shape future code changes and decisions.
[Conferences, source code, software maintainability, software quality, History, Data mining, software maintenance, long-term impact, development effort, Software, software engineering, change dependencies, metrics, Software measurement, stability, long-term impact of changes, Software engineering]
Failure preventing recommendations
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software becomes more and more integral to our lives thus software failures affect more people than ever. Failures are not only responsible for billions of dollars lost to industry but can cause lethal accidents. Although there has been much research into predicting such failures, those predictions usually concentrate either on the technical or the social level of software development. With the ever growing size of software teams we think that coordination among developers is becoming increasingly more important. Therefore, we propose to leverage the combination of both social and technical dimensions to create recommendation upon which developers can act to prevent software failures.
[software development, failures, Social network services, Conferences, software reliability, social networks, software failure, Computer crashes, Complexity theory, Data mining, software fault tolerance, recommender systems, software team, software failure preventing recommendation, lethal accident, Software, Software engineering]
Impact analysis for event-based components and systems
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In my dissertation, I aim to develop a dependence-based impact analysis technique for event-based systems and event-based components that communicate via messages. This paper motivates the problem, summarizes the open challenges and outlines proposed solution and evaluation strategies.
[message-oriented middleware platforms, software maintenance, Middleware, Equations, event-based software architectural style, Connectors, event-based components, Computer languages, software architecture, event-based systems, dependence-based impact analysis technique, middleware, event-based middleware platforms]
Empirical evaluation of effort on composing design models
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The importance of model composition in model-centric software development is recognized by researchers and practitioners. However, the lack of empirical evidence about the impact of model composition techniques on developers' effort is a key impairment for their adoption in real-world design settings. Software engineers are left without any guidance on how to properly use certain model techniques in a way that effectively reduces their development effort. This work aims to address this problem by: (1) providing empirical evidence on model composition effort through a family of experimental studies; (2) defining quantitative indicators to objectively assess key attributes of model composition effort; (3) deriving a method to support the systematic application of composition techniques; and (4) conceiving a new model composition technique to overcome the problems identified throughout the experimental evaluations.
[Object oriented modeling, Biological system modeling, Unified modeling language, model composition effort, Stability analysis, empirical evidence, development effort, empirical studies, empirical evaluation, model composition, design models, Semantics, UML, model-centric software development, Software, software engineering, Mathematical model]
Analysis of execution log files
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Log analysis can be used to find problems, define operational profiles, and even pro-actively prevent issues. The goal of my dissertation research is to investigate log management and analysis techniques suited for very large and very complex logs, such as those we might expect in a computational cloud system.
[Algorithm design and analysis, proactively prevent issues, Heuristic algorithms, diagnosis, Data mining, Security, execution log file analysis, analysis of textual data, fault isolation, Fault diagnosis, dissertation research, operational profiles, systems analysis, log files, log management, system monitoring, Software, computational cloud system, Arrays, cloud computing]
Towards end-user enabled web service consumption for Mashups
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Services Oriented Architecture (SOA) and Web 2.0 are two complementary trends towards a programmable Web. In this respect, the WS-* stack became pervasive for Enterprise Application Integration and Mashups attracted interest as novel Web 2.0 applications combining heterogeneous Web sources. While most Mashup tools focus on the integration of lightweight Web Services (i.e. RSS/Atom, REST), evolving Enterprise Mashups particularly need to integrate WS-* for enterprise-class data and logic. Hence, it requires experienced developers to prepare rather complex WS-* (e.g. as widgets) to enable casual developers and business users to create Mashups. Therefore, we postulate the hypothesis that this requires an a-priori simplification and cognitive support. In this abstract, we propose a research agenda on how to address such a simplification approach for Enterprise Services to ultimately empower business users to create real enterprise-class Mashups.
[Web 2.0, Mashups, end-user, SOA, Service oriented architecture, folksonomy, Complexity theory, Web sources, Simple object access protocol, business users, enterprise application integration, WS-*, Web services, Marketing and sales, Enterprise Mashup, service oriented architecture, business data processing, service-oriented architecture, Business, Web service consumption, enterprise mashup tool, enterprise-class data]
Constraint solving techniques for software testing and analysis
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software testing and analysis are very important research topics in software engineering. We are interested in improving the accuracy of analysis, as well as automation of test generation. In particular, we have been working on the automatic generation of small Orthogonal Arrays which can be used for combinatorial testing, and the computation of path execution frequency for a program path. The basic idea is to reduce the original problems to constraint satisfaction problems and develop effective constraint solving techniques for solving the problems.
[Software testing, program testing, Input variables, software testing, computability, constraint satisfaction problem, Frequency estimation, Vectors, path execution frequency, combinatorial testing, software analysis, orthogonal array generation, Software, software engineering, Software engineering, constraint solving technique]
A methodology to support load test analysis
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Performance analysts rely heavily on load testing to measure the performance of their applications under a given load. During the load test, analyst strictly monitor and record thousands of performance counters to measure the run time system properties such as CPU utilization, Disk I/O, memory consumption, network traffic etc. The most frustrating problem faced by analysts is the time spent and complexity involved in analysing these huge counter logs and finding relevant information distributed across thousands of counters. We present our methodology to help analysts by automatically identifying important performance counters for load test and comparing them across tests to find performance gain/loss. Further, our methodology help analysts to understand the root cause of a load test failure by finding previously solved problems in test repositories. A case study on load test data of a large enterprise application shows that our methodology can effectively guide performance analysts to identify and compare top performance counters across tests in limited time thereby archiving 88% counter data reduction.
[load test analysis support, program testing, Conferences, test repositories, Electronic mail, large enterprise application, Servers, run time system properties, performance analysts, automation, performance loss, load test, load test failure, Monitoring, software performance evaluation, Testing, automatic performance counter identification, Radiation detectors, counters, relevant information search, performance gain, performance counters, business data processing, principal component analysis, Principal component analysis]
Emerging Faculty Symposium 2010
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The challenge and prospect of becoming a new teaching faculty member at a research university is one that most people accept with enthusiasm and energy, but also with some trepidation: &#x02022; How do I get a position? &#x02022; How do I get to develop and publish strong research results? &#x02022; How will I be able to balance the many aspects of work as well as my personal life? &#x02022; What is my academic path to tenure and beyond?
[Industries, Collaboration, Materials, Educational institutions, Software systems, Software engineering]
SUITE 2010: 2nd International Workshop on Search-Driven Development - Users, Infrastructure, Tools &#x0026; Evaluation
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
SUITE is a workshop that focuses on exploring the notion of search as a fundamental activity during software development. The first edition of SUITE (SUITE 2009 [4]) was held at ICSE 2009. SUITE 2010, like its predecessor, devotes its attention to various research topics pertaining to the information needs of software developers. In SUITE 2010, we plan to emphasize open issues identified in SUITE 2009. We aim to continue building an active network of people interested in the research area that SUITE addresses.
[Industries, Conferences, USA Councils, Programming, Search engines, search driven development, software information retrieval, Software, information needs, Software engineering]
2010 ICSE 2nd International Workshop on Principles of Engineering Service-Oriented Systems (PESOS 2010)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Service-oriented systems have attracted great interest from industry and research communities worldwide. Service integrators, developers, and providers are collaborating to address the various challenges in the field. PESOS 2010 is a forum for all these communities to present and discuss a wide range of topics related to service-oriented systems. The goal of PESOS was to bring together researchers from academia and industry, as well as practitioners working in the areas of software engineering and service-oriented systems to discuss research challenges, recent developments, novel applications, as well as methods, techniques, experiences, and tools to support the engineering and use of service-oriented systems.
[Industries, service oriented systems, user centric, Conferences, Communities, Taxonomy, Educational institutions, Software, Software engineering]
New Horizons in Multicore Software Engineering
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
This paper provides a summary of the Third International Workshop on Multicore Software Engineering (IWMSE 2010). Motivated by multicore and manycore processors that are available on every desktop, software engineers need to exploit parallelism to make applications run faster. At the same time, programmers are facing many challenges due to the complexity of parallel programming. The workshop brought together researchers and practitioners to advance the state-of-the-art in software engineering for multicore and manycore systems. The contributions covered topics ranging from programming models, performance engineering, parallel patterns, fault-tolerance, to testing.
[Fault tolerance, Multicore processing, Conferences, Fault tolerant systems, Programming, Software, Software engineering]
The 6th International Workshop on Software Engineering for Secure Systems (SESS'10)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The 6th edition of the SESS workshop aims at providing a venue for software engineers and security researchers to exchange ideas and techniques. In fact, software is at core of most of the business transactions and its smart integration in an industrial setting may be the competitive advantage even when the core competence is outside the ICT field. As a result, the revenues of a firm depend directly on several complex software-based systems. Thus, stakeholders and users should be able to trust these systems to provide data and elaborations with a degree of confidentiality, integrity, and availability compatible with their needs. Moreover, the pervasiveness of software products in the creation of critical infrastructures has raised the value of trustworthiness and new efforts should be dedicated to achieve it. However, nowadays almost every application has some kind of security requirement even if its use is not to be considered critical.
[Conferences, Cities and towns, information security, Educational institutions, Software, software engineering, Security, secure systems, Software engineering, Testing]
Cooperative and Human Aspects of Software Engineering (CHASE 2010)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software is created by people---software engineers---working in varied environments, under various conditions. Thus understanding cooperative and human aspect of software development is crucial to comprehend how methods and tools are used, and thereby improving the creation and maintenance of software. Inspired by the hosting country's concept of co-responsibility -- ubuntu -- we especially invited contributions that address community-based development like open source development and sustainability of ICT eco-systems. The goal of this workshop is to provide a forum for discussing high quality research on human and cooperative aspects of software engineering. We aim at providing both a meeting place for the growing community and the possibility for researchers interested in joining the field to present their work in progress and get an overview over the field.
[Conferences, Communities, Humans, Programming, Educational institutions, Software, cooperative and human aspects of software engineering, Software engineering]
Fifth International Workshop on Sharing and Reusing Architectural Knowledge (SHARK 2010)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Architectural Knowledge (AK) is defined as the integrated representation of the software architecture of a software-intensive system or family of systems along with architectural decisions and their rationale, external influence and the development environment. The SHARK workshop series focuses on current methods, languages, and tools that can be used to extract, represent, share, apply, and reuse AK, and the experimentation and/or exploitation thereof. This fifth edition of SHARK will discuss, among other topics, the contributions of this community to a Body of Knowledge on software architecture.
[Presses, software architecture, Software architecture, Conferences, Communities, Computer architecture, Reliability, knowledge management, Business]
First International Workshop on Product Line Approaches in Software Engineering (PLEASE 2010)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
PLEASE is a new workshop series that focuses on exploring the present and the future of Software Product Line Engineering (SPLE) techniques. The goal of the workshop is to bring together researchers and practitioners with special interest in SPLE in order to discuss ongoing research and new ideas for advancing the field. The workshop's main theme, Beyond Product Lines, focuses on the adaptation of SPLE to dynamic settings in which neither the goal nor the organizational structure is stable. We seek to foster exchange of ideas, techniques, and approaches with the broader software engineering community. In a special session of this year's edition, we examine how to leverage existing research by discussing synergy opportunities with members of the Software Clones community. The first edition of PLEASE is held in conjunction with the 32st International Conference in Software Engineering (May 2--8, 2010. Cape Town, South Africa).
[software product lines, Conferences, Ecosystems, Cloning, variability modeling, Educational institutions, Software, product line engineering, Portfolios, Software engineering]
Flexible Modeling Tools (FlexiTools2010)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Modeling tools are often not used for tasks during the software lifecycle for which they should be helpful; more free-from approaches, such as office tools and white boards, are frequently used instead. Why is this? What might be done to make modeling tools more suitable? What key research challenges must be overcome to achieve this? The goal of this workshop is to bring together people who understand the activities and needs of developers and other stakeholders throughout the software lifecycle, user interface design and tool infrastructure to explore these questions.
[USA Councils, Conferences, Organizations, Educational institutions, Software, Concrete, Software engineering]
SESENA 2010: Workshop on Software Engineering for Sensor Network Applications
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
This editorial preface describes the aims and motivation as well as some details of the reviewing process of SESENA 2010, the First International Workshop on Software Engineering for Sensor Network Applications, which took place under the umbrella of ICSE 2010, the 32rd ACM/IEEE International Conference on Software Engineering, in Cape Town, South Africa, May 2010. See also our workshop's website at http://www.sesena.info/
[Computer aided software engineering, Conferences, Standardization, testing, Programming, Educational institutions, sensor networks, model-driven development, tools, Wireless communication, software engineering, methodology, deployment]
Bridging the Gap Between the Theory and Practice of Software Test Automation
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In software development practice, testing often accounts for as much as 50&#x025; of the total development effort. It is therefore imperative to reduce the cost and improve the effectiveness of software testing by automating the testing process. In the past decades, a substantial amount of research effort has been invested into the development and study of automatic test case generation, automatic test oracles, and other (semi-)automated testing techniques. As the theory and practice of software testing becomes more mature, a deeper and more meaningful automation of the testing process is possible. Therefore, the automation of various testing activities is now becoming an integral part of industrial practice. In response to and in support of these exciting developments, the 5th Workshop on the Automation of Software Test provides a publication forum that bridges the gap between the theory and practice of automated testing.
[Computer science, Automation, Conferences, Educational institutions, Software, Testing, Software engineering]
Fifth Workshop on Software Engineering for Adaptive and Self-Managing Systems (SEAMS 2010)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The Software Engineering for Adaptive and Self-managing Systems (SEAMS) workshop has consolidated the interest in the software engineering community on self-adaptive and self-managing systems. SEAMS provides a forum for researchers and practitioners to share new results, discuss challenging issues, raise awareness, and promote collaboration within the community. The SEAMS 2010 workshop aims to continue the success of previous ICSE SEAMS workshops: in Shanghai in 2006, in Minneapolis in 2007, in Leipzig in 2008, and in Vancouver in 2009.
[Adaptive systems, USA Councils, Conferences, Communities, adaptive software, Educational institutions, Software, autonomic computing, Software engineering]
Second International Workshop on Software Research and Climate Change
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
This workshop will explore the contributions that software research can make to the challenge of tackling climate change. Software is a critical enabling technology in nearly all aspects of climate change, from the computational models used by climate scientists to improve our understanding of the impact of human activities on earth systems, through to the information and control systems needed to build an effective carbon-neutral society. The intent of the workshop is to explore how software research can contribute to this challenge, to build a community of researchers interested in responding to the challenge, and to map out a research agenda.
[Earth, computational thinking, green IT, global warming, Conferences, Communities, inter-disciplinary research, Educational institutions, Control systems, Software, Meteorology, climate change]
First International Workshop on Quantitative Stochastic Models in the Verification and Design of Software Systems (QUOVADIS 2010)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Nowadays requirements related to quality attributes such as performance, reliability, safety and security are often considered the most important requirements for software development projects. To reason about these quality attributes different stochastic models can be used. These models enable probabilistic verification as well as quantitative prediction at design time. On the other hand, these models could be also used to perform runtime adaptation in order to achieve certain quality goals. This workshop aims to provide a forum for researchers in these areas that should help with the adoption of quantitative stochastic models into general software development processes.
[Adaptation models, Computational modeling, Conferences, Predictive models, Educational institutions, Probabilistic logic, Software engineering]
2nd International Workshop on Software Engineering in Health Care (SEHC 2010)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Society faces increasing reliance on software-intensive systems to manage health services, from scheduling, billing, and patient records to the control of life-critical devices and procedures. There are important concerns about software quality, security, and privacy, user interfaces, system interoperability, process automation and improvement, and many other issues of current concern to software engineering practitioners and researchers. It is widely recognized that information and communication technologies (ICTs) will transform healthcare of the future, being a driving force for improving care and access, while reducing the overall cost when broadly calculated based on overall productivity and quality of life.
[Privacy, USA Councils, Conferences, Medical services, Educational institutions, Security, Software engineering]
RSSE 2010: Second International Workshop on Recommendation Systems for Software Engineering
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The goal of this one-day workshop is to bring together researchers and practitioners with interest and experience in the theory, elaboration, and evaluation of concepts, techniques, and tools for providing recommendations to developers, managers, and other stakeholders involved in software engineering tasks.
[development tasks, Conferences, inference, data mining, Debugging, Educational institutions, Data mining, feedback, usability, infrastructure, Usability, recommendation systems for software engineering, Software engineering]
Web2SE: First Workshop on Web 2.0 for Software Engineering
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Social software is built around an "architecture of participation" where user data is aggregated as a side-effect of using Web 2.0 applications. Web 2.0 implies that processes and tools are socially open, and that content can be used in several different contexts. Web 2.0 tools and technologies support interactive information sharing, data interoperability and user centered design. For instance, wikis, blogs, tags and feeds help us organize, manage and categorize content in an informal and collaborative way. One goal of this workshop is to investigate how these technologies can improve software development practices. Some of these technologies have made their way into collaborative software development processes such as Agile and Scrum, and in development platforms such as Rational Team Concert which draw their inspiration from Web 2.0. These processes and environments are just scratching the surface of what can be done by incorporating Web 2.0 approaches and technologies into collaborative software development. This workshop aims to improve our understanding of how Web 2.0, manifested in technologies such as mashups or dashboards, can change the culture of collaborative software development.
[process, Web 2.0, Conferences, USA Councils, collaboration, Programming, Educational institutions, Software, Internet, tools, Software engineering]
Workshop on Emerging Trends in Software Metrics (WETSoM 2010)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The Workshop on Emerging Trends in Software Metrics aims at bringing together researchers and practitioners to discuss the progress of software metrics. The motivation for this workshop is the low impact that software metrics has on current software development. The goals of this workshop are to critically examine the evidence for the effectiveness of existing metrics and to identify new directions for development of software metrics.
[complexity, Software metrics, Conferences, Educational institutions, Software, Complexity theory, software quality, software metrics]
1st International Workshop on Replication in Empirical Software Engineering Research (RESER)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The RESER 2010 workshop provides a venue in which empirical Software Engineering researchers may present and discuss theoretical foundations and methods of replication, as well as the results of replicated studies.
[replication, Conferences, Laboratories, methods, Educational institutions, research, Computer science, USA Councils, computer science, Software, software engineering, validity, reporting, validation, Software engineering]
Software Development Governance (SDG) Workshop
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
This is the introduction of the 3rd workshop on Software Development Governance (SDG), which will take place as part of ICSE 2010. This year we have combined two successful workshops (SDG - software development governance and LMSA -- leadership and management in software architecture) since both workshops deal with decisions that are part of the development process e.g., business and organizational decisions that impact the technical decisions concerned with the product architecture and the product quality.
[Software architecture, Conferences, Organizations, decision making, Programming, Lead, Software, roles and responsibilities, software development governance]
Fourth International Workshop on Software Clones (IWSC)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software clones are identical or similar pieces of code. They are often the result of copy--and--paste activities as ad-hoc code reuse by programmers. Software clones research is of high relevance for the industry. Many researchers have reported high rates of code cloning in both industrial and open-source systems. In this workshop we will explore lines of research that evaluate code clone detection methods, reason about ways to remove clones, assess the effect of clones on maintainablity, track clones' evolution, and investigate the root causes of clones.
[Conferences, Biological system modeling, Software algorithms, Cloning, Educational institutions, Software, code clone detection, software clone, software maintenance, Software engineering]
2010 ICSE International Workshop on Advances and Applications of Problem Orientation (WAAPO-2010)
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software problems originate from real world problems. A software solution must address its real world problem in a satisfactory way. A software engineer must therefore understand the real world problem that their software intends to address. To be able to do this, the software engineer must understand the problem context and how it is to be affected by the proposed software, expressed as the requirements. Without this knowledge the engineer can only hope to chance upon the right solution for the problem. Application of problem-oriented approaches may well be a way of meeting this challenge.
[Knowledge engineering, Presses, requirements, Conferences, design, Educational institutions, Software, software engineering, Problem-solving, problem orientation, specifications, Software engineering]
Analysing "people" problems in requirements engineering
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The aim of this tutorial is to explain in an accessible manner the psychology of people in the context of misunderstandings, politics and social issues that affect software development. It focuses on user-stakeholder interaction techniques for analysis and interpretation of human behaviour, and how psychological knowledge can be used to improve the requirements engineering (RE) process as well as interpreting the implications of human motivations and values for requirements and software systems architecture. Soft issues, such as politics and people's feelings, are often cited as problems in the RE process and as key causes of system failure. It is clear from the RE literature that understanding user beliefs and values is vital for the success of software development. The London Ambulance service is a canonical example of system failure caused, in part, by inadequate understanding of ambulance crews' motivations, values of self esteem and autonomy, and the emotional reaction to lack of involvement in the requirements process, leading to technology failure .
[London ambulance service, software development, Psychology, Humans, Tutorials, Programming, Educational institutions, formal specification, Guidelines, people problems, software architecture, requirements engineering, social issues, Computer architecture, user-stakeholder interaction, software systems architecture, psychological knowledge]
Software architecture: foundations, theory, and practice
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software architecture has become a centerpiece subject for software engineers, both researchers and practitioners alike. At the heart of every software system is its software architecture, i.e., "the set of principal design decisions about the system". Architecture permeates all major facets of a software system, for principal design decisions may potentially be made at any time during a system's lifetime, and potentially by any stakeholder. Such decisions encompass structural concerns, such as the system's high-level building blocks -components, connectors, and configurations; the system's deployment; the system's non-functional properties; and the system's evolution patterns, including runtime adaptation. Software architectures found particularly useful for families of systems - product lines - are often codified into architectural patterns, architectural styles, and reusable, parameterized reference architectures. This tutorial affords the participant an extensive treatment of the field of software architecture, its foundation, principles, and elements, including those mentioned above. Additionally, the tutorial introduces the participants to the state-of-the-art as well as the state-of-the-practice in software architecture, and looks at emerging and likely future trends in this field. The discussion is illustrated with numerous real-world examples. One example given prominent treatment is the architecture of the World Wide Web and its underlying architectural style, REpresentational State Transfer (REST).
[system deployment, World Wide Web, software system, architectural pattern, software architecture, architectural style, Software architecture, system lifetime, Computer architecture, system connectors, design, reusable parameterized reference architecture, software engineering, runtime adaptation, object-oriented programming, Architecture, Buildings, REpresentational State Transfer, Tutorials, system configuration, non-functional property, system high-level building blocks, structural concern, component, system evolution pattern, connector, system components, Software systems, design decision, Internet]
Generative software development
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Generation of software from modeling languages such as UML and domain specific languages (DSLs) has become an important paradigm in software engineering. In this contribution, we present some positions on software development in a model based, generative manner based on home grown DSLs as well as the UML. This includes development of DSLs as well as development of models in these languages in order to generate executable code, test cases or models in different languages. Development of formal DSLs contains concepts of metamodels or grammars (syntax), context conditions (static analysis and quality assurance) as well as possibilities to define the semantics of a language. The growing number and complexity of DSLs is addressed by concepts for the modular and compositional development of languages and their tools. Moreover, we introduce approaches to code generation and model transformation. Finally, we give an overview of the relevance of DSLs for various steps of software development processes.
[metamodel, Unified modeling language, model transformation, Programming, ubiquitous computing, context condition, program compilers, language semantics, domain specific language, Quality assurance, specification languages, test case, software engineering, software tools, DSL, modeling language, formal languages, Unified Modeling Language, software development, Object oriented modeling, Grammar, formal DSL, language modular development, code generation, grammars, UML, Syntactics, simulation languages, code execution]
Cloud service engineering
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Building on compute and storage virtualization, Cloud Computing provides scalable, network-centric, abstracted IT infrastructure, platforms, and applications as on-demand services that are billed by consumption. Cloud Service Engineering is the application of a systematic approach to leverage Cloud Computing in the context of the Internet in its combined role as a platform for technical, economic, organizational and social networks. This tutorial introduces concepts and technology of Cloud Computing and Cloud Service Engineering, providing an overview of state-of-the-art in research and practice. We show how to set up a private Cloud that delivers Infrastructure-as-a-Service (IaaS). Eucalyptus and OpenNebula are popular open source software frameworks for creating on-premise Clouds. Promises, challenges and solutions for integrating services of a private Cloud with public Cloud services such as Amazon EC2 and SQS are discussed. We show how the best of both worlds - private and public Clouds - can be combined to build scalable and secure systems.
[Economics, Cloud computing, Biological system modeling, Computational modeling, public domain software, Tutorials, Programming, infrastructure-as-a-service, storage management, cloud service engineering, open source software frameworks, software engineering, storage virtualization, Internet, cloud computing, Business]
Bayesian methods for data analysis in software engineering
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software engineering researchers analyze programs by applying a range of test cases, measuring relevant statistics and reasoning about the observed phenomena. Though the traditional statistical methods provide a rigorous analysis of the data obtained during program analysis, they lack the flexibility to build a unique representation for each program. Bayesian methods for data analysis, on the other hand, allow for flexible updates of the knowledge acquired through observations. Despite their strong mathematical basis and obvious suitability to software analysis, Bayesian methods are still largely under-utilized in the software engineering community, primarily because many software engineers are unfamiliar with the use of Bayesian methods to formulate their research problems. This tutorial will provide a broad introduction of Bayesian methods for data analysis, with a specific focus on problems of interest to software engineering researchers. In addition, the tutorial will provide an in-depth understanding of a subset of popular topics such as Bayesian inference, probabilistic prediction techniques, Markov models, information theory and sampling. The core concepts will be explained using case studies and the application of prominent statistical tools on examples drawn from software engineering research. At the end of the tutorial, the participants will acquire the necessary skills and background knowledge to formulate their research problems using Bayesian methods, and analyze their formulation using appropriate software tools.
[Data analysis, Statistical analysis, data analysis, program diagnostics, Tutorials, reasoning, Bayesian methods, program analysis, Markov processes, Software, software engineering, Bayes methods, reasoning about programs, software tools, Software engineering, statistics]
Cost effectiveness analysis in software engineering
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The tutorial presented an approach that leverages well-known economic and financial concepts for evaluating the cost effectiveness of software development processes and techniques. Software engineering studies often report separately on the costs and benefits of a phenomenon of interest, and rarely adequately address the combined bottom line implications. In particular, tensions between quality and productivity are hard to reconcile, making objective, high-level insights elusive. To address this need, the tutorial focused on quantitative methods for synthesizing co-dependent cost-benefit effects and analyzing the resulting behaviors.
[Productivity, Economics, Measurement, Biological system modeling, Tutorials, software quality, software economics, cost-benefit effects, quality, productivity, cost-benefit analysis, cost effectiveness, Software, software engineering, cost effectiveness analysis, Software engineering]
Coaching agile software projects: tutorial proposal - ICSE 2010
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Our tutorial for ICSE 2010 focuses on coaching agile software teams. Based on eight years of experience guiding agile software projects in the academia and industry, we focus in the tutorial on a coaching framework for agile software projects. The tutorial participants become familiar with coaching practices and gain experience with some of the practices. The tutorial has two main parts. In the first part, we present the coaching framework, including the goals, structure, and guiding principles. In the second part, we focus on the following central themes in agile development processes which, we suggest, are appropriate to be included in such a coaching framework: Teamwork and collaboration, time and measures, learning and reflection, and change and leadership. Since 2003, we have facilitated this tutorial and similar ones in different settings (industry, academia, conferences). The coaching framework, as well as the themes, case studies, and analysis approach presented in the tutorial, are summarized in our book Agile Software Engineering published by Springer in 2008 (Hazzan and Dubinsky, 2008).
[learning processes, computer science education, ICSE 2010, project management, software prototyping, Tutorials, agile software projects tutorial proposal, Programming, software management, retrospective, Reflection, agile development process, Education, groupware, Software, coaching, software engineering, Teamwork, coaching framework, agile software development, software project management, Software engineering]
Parameterized unit testing: theory and practice
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Unit testing has been widely recognized as an important and valuable means of improving software reliability, as it exposes bugs early in the software development life cycle. However, manual unit testing is often tedious and insufficient. Testing tools can be used to enable economical use of resources by reducing manual effort. Recently parameterized unit testing has emerged as a very promising and effective methodology to allow the separation of two testing concerns or tasks: the specification of external, black-box behavior (i.e., assertions or specifications) by developers and the generation and selection of internal, white-box test inputs (i.e., high-code-covering test inputs) by tools. A parameterized unit test (PUT) is simply a test method that takes parameters, calls the code under test, and states assertions. PUTs have been supported by various testing frameworks. Various open source and industrial testing tools also exist to generate test inputs for PUTs. This tutorial presents latest research on principles and techniques, as well as practical considerations to apply parameterized unit testing on real-world programs, highlighting success stories, research and education achievements, and future research directions in developer testing. The tutorial will help improve developer skills and knowledge for writing PUTs and give overview of tool automation in supporting PUTs. Attendees will acquire the skills and knowledge needed to perform research or conduct practice in the field of developer testing and to integrate developer testing techniques in their own research, practice, and education.
[program debugging, Pex, program testing, education, public domain software, open source testing tool, software reliability, developer skills improvement, unit testing, Programming, developer testing, formal specification, manual unit testing, software development life cycle, USA Councils, testing tools, Education, industrial testing tool, software tools, external behavior specification, Testing, parameterized unit testing, Tutorials, testing, tool automation, black-box behavior specification, Writing, symbolic execution, theories, mock objects, Software]
The "physics" of notations: a scientific approach to designing visual notations in software engineering
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Visual notations form an integral part of the language of software engineering (SE). Yet historically, SE researchers and notation designers have ignored or undervalued issues of visual representation. In evaluating and comparing notations, details of visual syntax are rarely discussed. In designing notations, the majority of effort is spent on semantics, with graphical conventions often an afterthought. Typically no design rationale, scientific or otherwise, is provided for visual representation choices. While SE has developed mature methods for evaluating and designing semantics, it lacks equivalent methods for visual syntax. This tutorial defines a set of principles for designing cognitively effective visual notations: ones that are optimised for human communication and problem solving. Together these form a design theory, called the Physics of Notations as it focuses on the physical (perceptual) properties of notations rather than their logical (semantic) properties. The principles were synthesised from theory and empirical evidence from a wide range of fields and rest on an explicit theory of how visual notations communicate. They can be used to evaluate, compare and improve existing visual notations as well as to construct new ones. The tutorial identifies serious design flaws in some of the leading SE notations together with practical suggestions for improving them. It also showcases some examples of visual notation design excellence from SE and other fields.
[Visualization, visual representation, visualization, Unified modeling language, Tutorials, problem solving, diagrams, semantics, human communication, programming language semantics, analysis, modelling, Physics, visual notation designing, concrete syntax, optimisation, Semantics, data visualisation, Syntactics, software engineering, communication, mature methods, visual syntax, Software engineering]
Multicore software engineering: the next challenge in software engineering
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Due to stagnating clock rates, future increases in processor performance will have to come from parallelism. Inexpensive multicore processors with several cores on a chip have become standard in PCs, laptops, servers, and embedded devices will follow; manycore chips with hundreds of processors on a single chip are predicted. Software engineers are now asked to write parallel applications of all sorts, and need to quickly grasp the relevant aspects of general-purpose parallel programming. This tutorial at ICSE 2010 prepares them for this challenge.
[multiprocessing systems, Multicore processing, Tutorials, parallelism, manycore chips, parallel applications, multicore software engineering, parallel programming, general-purpose parallel programming, Parallel programming, Parallel processing, software engineering, multicore processors, stagnating clock rates, processor performance, Software tools, Software engineering]
Engineering safety- and security-related requirements for software-intensive systems: tutorial summary
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
This full-day tutorial introduces the attendee to the engineering of safety- and security-related requirements for software-intensive systems. It provides a consistent, effective, and efficient method for identifying, analyzing, specifying, verifying, and validating the four different types of safety- and security-related requirements.
[safety-related requirements engineering, Tutorials, safety-critical software, safety engineering, Security, software-intensive system, formal specification, software-intensive systems, requirements engineering, formal verification, security of data, security engineering, Fires, Software, security-related requirements engineering, Software engineering]
Using ethnographic methods in software engineering research
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
This tutorial provides an overview of the role of ethnography in Software Engineering research. It describes the use of ethnographic methods as a means to provide an in-depth understanding of the socio-technological realities surrounding everyday software development practice. The knowledge gained can be used to improve processes, methods and tools as well as develop observed industrial practices. The tutorial begins with a brief historical account of ethnography in the fields of Software Engineering, CSCW, Information Systems and other related areas. This sets the stage for a more in-depth discussion of methods for data collection and analysis used in ethnographic studies. It then describes how these methods can be and have been used by software engineering researchers to understand developers' work practices, to inform the development of processes, methods and tools and to evaluate the applicability of current processes, methods and tools. Finally, some practical issues concerning the selection and use of ethnographic methods by software engineers are discussed. Throughout the tutorial, examples from the presenters' experience illustrate the points made.
[Context, social aspects of automation, software engineering research, Tutorials, Programming, industrial practices, Educational institutions, Information systems, ethnographic studies, ethnographic method, software development practice, socio-technological realities, qualitative methods, empirical software engineering, Software, software engineering, CSCW, information systems, qualitative research, ethnography, Software engineering]
Design science methodology: principles and practice
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Design scientists have to balance the demands of methodological rigor that they share with purely curiosity-driven scientists, with the demands of practical utility that they share with utility-driven engineers. Balancing these conflicting demands can be conceptually complex and may lead to methodological mistakes. For example, treating a design question as an empirical research question may lead to re searcher to omit the identification of the practical problem to solve, to omit the identification of stakeholder-motivated evaluation criteria, or to omit trade-off and sensitivity analysis. This tutorial aims to clear up this methodological mist in the case of software engineering (SE) research. The core distinction is that between practical problems and knowledge questions. A practical problem is a difference between stakeholder goals and experiences, that they wish to reduce, and a knowledge question is a lack in knowledge, that they wish to reduce [14]. For example, to reduce the number of build failures in distributed SE projects is a practical problem; to ask for the relation between team communication structure and code integration build failures is a knowledge question.
[IEEE Computer Society, omit trade-off, Conferences, utility driven engineers, stakeholder motivated evaluation criteria, sensitivity analysis, Psychology, Programming, utility programs, curiosity driven scientists, USA Councils, design science methodology, Software, software engineering, Software engineering]
Combinatorial test design in practice
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Combinatorial testing is a specification based sampling technique that provides a systematic way to select combinations of program inputs or features for testing. It has been applied over the years to test input data, configurations, web forms, protocols, graphical user interfaces and for testing software product lines. This tutorial introduces the fundamentals of combinatorial testing, including both practical and theoretical foundations, to provide a comprehensive introduction that is relevant to both test practitioners and software engineering researchers. The tutorial will present an overview of Combinatorial Test Design (CTD) and describe some state of the art research advances and domains where CTD has been applied. It will present the theoretical underpinnings of CTD and explain a few algorithmic techniques used to generate CTD samples, as well as describe recent work on practical extensions to these algorithms that allow for a broader use of CTD. A session devoted to modeling test problems using CTD will follow, with attendees obtaining hands-on experience using several realistic problems.
[Algorithm design and analysis, program inputs, combinatorial mathematics, program testing, Software algorithms, Tutorials, combinatorial test design, specification based sampling technique, functional testing, program features, combinatorial interaction testing, Software, software engineering, Mathematical model, Testing, Graphical user interfaces]
Software architecture and agile software development: a clash of two cultures?
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software architecture is taking a bad rap with the agilists---proponents of agile and lean software development approaches: "BUFD big up-front design\
[lean software development, software prototyping, architectural issue, system documentation, software development management, Tutorials, Documentation, BUFD big up-front design, Programming, project architecture, agile software architecture, software architecture, architectural effort, agile process, Software architecture, Computer architecture, Software, agile software development]
Code clone detection in practice
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Due to the negative impact of code cloning on software maintenance efforts as well as on program correctness [4-6], the duplication of code is generally viewed as problematic. However, the techniques and tools developed by the research community in the last decade have not found broad acceptance in software engineering practice yet. This tutorial contributes to a more widespread application of existing approaches by illustrating where cloning comes from, what its consequences are, and how it can be detected.
[clone detection, Communities, Redundancy, Cloning, Tutorials, reproduction (copying), software maintenance, code duplication, Manifolds, Software, software engineering, redundancy, code clone detection, Software engineering]
New processes for new horizons: the incremental commitment model
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
The wide variety of software-intensive systems needed to support the new horizons of evolving technology, system and software complexity, high dependability, global interoperability, emergent requirements, and adaptability to rapid change make traditional and current one-size-fits-all process models infeasible. This tutorial presents the process framework, principles, practices, and case studies for a new model developed and being used to address these challenges. It has a series of risk-driven decision points that enable projects to converge on whatever combination of agile, plan-driven, formal, legacy-oriented, reuse-oriented, or adaptive processes that best fit a project's situation. The tutorial discusses the decision table for common special cases; exit ramps for terminating non-viable projects; support of concurrent engineering of requirements, solutions and plans; and evidence-based commitment milestones for synchronizing the concurrent engineering. The tutorial will include case studies and exercises for participants' practice and discussion.
[risk management, Spirals, software development management, feasibility evidence, Tutorials, incremental commitment model, process framework, software complexity, evidence-based commitment, one-size-fits-all process models, Modeling, software-intensive systems, concurrent engineering, hardware-software-human factors integration, decision table, emergent requirements, Software, Concurrent engineering, global interoperability, risk-driven decision points, Software engineering]
Mining software engineering data
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software engineering data (such as code bases, execution traces, historical code changes, mailing lists, and bug databases) contains a wealth of information about a project's status, progress, and evolution. Using well-established data mining techniques, practitioners and researchers have started exploring the potential of this valuable data in order to better manage their projects and to produce higher quality software systems that are delivered on time and within budget. This tutorial presents the latest research in mining software engineering data, discusses challenges associated with mining software engineering data, highlights success stories of mining software engineering data, and outlines future research directions. Attendees will acquire the knowledge and skills needed to integrate the mining of software engineering data in their own research or practice. This tutorial builds on several successful offerings at ICSE since 2007.
[Software maintenance, project management, data mining, knowledge acquisition, mining software repositories, Tutorials, Data mining, History, software engineering data mining, mining software engineering data, project progress, project evolution, software engineering, Reliability, project status, Software engineering]
Behavioural validation of software engineering artefacts
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software engineering artefacts that define behaviour tend to be of a fragmented nature in order to facilitate their construction, modification, and modular reasoning (e.g. modular code, pre/post-conditions specifications). However, fragmentation makes the validation of global behaviour difficult. Typically synthesis techniques that yield global representations of large or infinite states are used in combination with simulation or partial explorations, techniques which necessarily lose the global view of system behaviour. I am working on the development of abstraction-for-validation techniques that automatically produce finite state abstractions that are sufficiently small to support validating the emergent behaviour of a fragmented description "at a glance".
[Analytical models, Protocols, code understanding, behaviour models, contract conformance, Software, Cognitive science, Complexity theory, Contracts, Software engineering]
QED: a proof system based on reduction and abstraction for the static verification of concurrent software
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
We present a proof system and supporting tool, QED, for the static verification of concurrent software. Our key idea is to simplify the verification of a program by rewriting it with larger atomic actions. We demonstrated the simplicity and effectiveness of our approach on benchmarks with intricate synchronization.
[rewriting systems, QED supporting tool, program verification, atomicity, Instruction sets, proof system, Interference, abstraction, Data structures, static verification, Synchronization, concurrent programs, Concurrent computing, concurrent software, rewriting, concurrency control, reduction, Software tools]
Software engineering abstractions for the multi-touch revolution
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Multi-touch interfaces allow users to use multiple fingers to provide input to a graphical user interface. The idea of allowing users to touch and manipulate digital information with their hands has been subject of research for more than 25 years. Recently several of these research artifacts have found their way to industry, with examples like the iPhone and the Microsoft Surface. Mainstream programming languages do not offer support to deal with the complexity of these new devices. Unlike the evolution in the hardware technology, the complexity of these new devices has not yet been addressed by adequate software engineering abstractions. Current multi-touch frameworks provide a narrow range of hardcoded functionality like pinch, rotate and move known as multi-touch gestures. There is however a substantial need to develop new and more gestures for domain specific applications. Multi-touch devices are inherently concurrent and provide a continuous stream of events. In many of these frameworks capturing these events to extract gestures is done by means of event handlers. Programming multi-touch devices with event handlers is cumbersome for a number of reasons.
[software engineering abstractions, iPhone, graphical user interfaces, graphical user interface, Microsoft Surface, Programming, multitouch revolution, multitouch interfaces, multitouch devices, Cognition, Complexity theory, Engines, digital information, multitouch gestures, Libraries, Software, software engineering, event handlers, Graphical user interfaces, mobile handsets]
Predicting build outcome with developer interaction in Jazz
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Investigating the human aspect of software development is becoming prominent in current research. Studies found that the misalignment between the social and technical dimensions of software work leads to losses in developer productivity and defects. We use the technical and social dependencies among pairs of developers to predict the success of a software build. Using the IBM Jazz&#x2122; data we found information about developers and their social and technical relation can build a powerful predictor for the success of a software build. Investigating human aspects of software development is becoming prominent in current research. High misalignment between the social and technical dimensions of software work lowers productivity and quality.
[social dependency, software development, Social network services, Humans, Programming, technical dependency, software quality, software build, Data mining, developer interaction, build outcome, developer productivity, human aspect, IBM Jazz data, software productivity, developer defects, Software, technical dimensions, Software measurement, Software engineering, software metrics, social dimensions]
Improved social trustability of code search results
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Search is a fundamental activity in software development. However, to search source code efficiently, it is not sufficient to implement a traditional full text search over a base of source code, human factors have to be taken into account as well. We looked into ways of increasing the search results code trustability by providing and analysing a range of meta data alongside the actual search results.
[Java, social aspects of automation, social trustability, source code search, full text search, meta data, software development, Humans, human factors, Licenses, Search problems, code search results, Databases, Search engines, Software, software engineering]
Test-driven roles for pair programming
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
We propose a new model of two-person programming with a new structure based on test-driven development. In test driven development, developers follow two rules: "write new code only if an automated test has failed," and "eliminate duplication". The result is short, rapid development cycles in which an initially failing test is written to specify new functionality, code is written to make the test pass, and the code is then refactored to eliminate newly-introduced duplication. In our new model, the process of test-driven development is parallelized, with one member of the pair working primarily on tests, while the other works primarily on implementation. While developers can and will swap roles as it suits them, authoring tests and implementation gives structure to the developers' collaboration.
[Visualization, test-driven roles, program testing, visualizations, rapid development cycles, two-person programming, Programming profession, pair programming, test-driven development, Data visualization, Software, software engineering, Testing]
Providing support for creating next generation software architecture languages
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Many languages for software architectures have been proposed, each dealing with different stakeholder concerns, operating at different levels of abstraction and with different degrees of formality. It is known that a universal architectural language cannot exist since the various concerns, requirements, and domains may change. Moreover, stakeholder concerns and needs are various and ever evolving even while designing a single system. Model-driven techniques may be used to answer the need for supporting the creation of extensible, customizable and stakeholder-oriented architectural languages (i.e., next generation architectural languages). Part of this approach is developed in a framework called byADL. In this paper I present the big picture behind the approach, the research aspects considered in order to get byADL closer to an ideal architectural framework and future research issues.
[next generation software architecture languages, model driven technique, stakeholder oriented architectural languages, modeling, ISO standards, Next generation networking, software architecture, Software architecture, Semantics, Computer architecture, specification languages, BYADL, Software]
STORM: static unit checking of concurrent programs
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Concurrency is inherent in today's software. Unexpected interactions between concurrently executing threads often cause subtle bugs in concurrent programs. Such bugs are hard to discover using traditional testing techniques since they require executing a program on a particular unit test (i.e. input) through a particular thread interleaving. A promising solution to this problem is static program analysis since it can simultaneously check a concurrent program on all inputs as well as through all possible thread interleavings. This paper describes a scalable, automatic, and precise approach to static unit checking of concurrent programs implemented in a tool called Storm. Storm has been applied on a number of real-world Windows device drivers, and the tool found a previously undiscovered concurrency bug in a driver from Microsoft's Driver Development Kit.
[Context, Microsoft driver development kit, program debugging, Storm, unit checking, multi-threading, program testing, program verification, program diagnostics, static unit checking, device drivers, Windows device drivers, testing techniques, thread interleavings, static analysis, Complexity theory, static program analysis, concurrent programs, Concurrent computing, Storms, Computer bugs, Software, undiscovered concurrency bug, Testing]
Making program refactoring safer
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Automated refactorings may change the program behavior. We propose an approach and its implementation called SafeRefactor for making program refactoring safer. We applied 10 Eclipse refactorings in a number of automatically generated programs, and used SafeRefactor to identify 50 bugs that lead to behavioral changes or compilation errors.
[Java, program debugging, Eclipse refactorings, automated refactorings, program behavior, SafeRefactor, Metals, testing, compilation errors, program compilers, program refactoring, refactoring, Computer bugs, Safety, Reliability, Catalogs, Testing]
Staying aware of relevant feeds in context
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
To stay aware of relevant information and avoid productivity loss, a developer has to continuously read through new incoming information. Our approach supports the integration of dynamic and static information in a development environment that allows the developer to continuously monitor the relevant information in context of his work.
[Context, Productivity, human-centric software engineering, human factors, Servers, development environment, dynamic information, Computer bugs, Software systems, feeds, software engineering, context awareness, static information, Feeds]
Developing and evaluating the code bubbles metaphor
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Today's integrated development environments (IDEs) are hampered by their dependence on files and file-based editing. A novel user interface that is based on collections of lightweight editable fragments, called bubbles, which when grouped together form concurrently visible working sets is proposed. An overview of this interface, as well as a summary of the results of a quantitative and a qualitative evaluation of the interface is presented.
[Visualization, Software maintenance, Navigation, Buildings, qualitative evaluation, Debugging, visible working sets, code bubbles metaphor, user interfaces, user interface, quantitative evaluation, file-based editing, Layout, integrated development environment, User interfaces, lightweight editable fragment]
Summarizing software concerns
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
While working on software concerns to perform evolution tasks, developers often encounter a lack of abstraction. They have to work with all the details of large subsets of code that constitute a concern at a low level of abstraction. In this paper we propose a framework to summarize software concerns in order to raise the level of abstraction and to subsequently improve the productivity of software developers. We use a combination of static analysis, information retrieval, and natural language processing techniques to extract and deduct knowledge about different parts of the concern and its interactions with other concerns. Then we produce a description of the concern using natural language generation and summarization techniques.
[natural language processing, program diagnostics, software evolution tasks, human-centric software engineering, system documentation, software abstraction, information retrieval, natural language generation, static analysis, Educational institutions, software developers, Servers, software maintenance, knowledge deduction, Computer science, productivity, knowledge extraction, Cities and towns, Software, software concern summarization]
Commit 2.0: enriching commit comments with visualization
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
Software developers use commit comments to document changes and as a mean of communication in their team. However, the support given by IDEs is restricted with this respect, as they limit the users to use only text to document changes. In this paper we propose and implement an approach to enrich commit comments with software visualization: Commit 2.0 generates visualizations of the performed changes at different granularity levels, and let the user enrich them with annotations.
[Visualization, Commit 2.0, software development, Blogs, system documentation, software development management, Documentation, Programming, Data mining, software visualization, commit comments, Data visualization, Software, document changes, program visualisation]
Helios: impact analysis for event-based components and systems
2010 ACM/IEEE 32nd International Conference on Software Engineering
None
2010
In this paper, the author proposes Helios, a novel approach to determine a dependence graph that captures message dependencies by sequentially analyzing the source code of each event-based component. A technique that produces such a message dependence graph does not currently exist.
[Helios, object-oriented programming, Servers, Middleware, message dependencies, Connectors, impact analysis, event-based components, dependence graph, Publishing, Software systems, software engineering, source code analysis]
A practical guide for using statistical tests to assess randomized algorithms in software engineering
2011 33rd International Conference on Software Engineering
None
2011
Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difficult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot of current practice in software engineering research. We show that randomized algorithms are used in a significant percentage of papers but that, in most cases, randomness is not properly accounted for. This casts doubts on the validity of most empirical results assessing randomized algorithms. There are numerous statistical tests, based on different assumptions, and it is not always clear when and how to use these tests. We hence provide practical guidelines to support empirical research on randomized algorithms in software engineering.
[Algorithm design and analysis, Context, non-parametric test, bonferroni adjustment, Statistical analysis, Software algorithms, Search problems, parametric test, systematic review, statistical tests, practical guide, snapshot representation, randomized algorithms, statistical difference, survey, software engineering, confidence interval, statistical analysis, effect size, Software engineering, Testing]
aComment: mining annotations from comments and code to detect interrupt related concurrency bugs
2011 33rd International Conference on Software Engineering
None
2011
Concurrency bugs in an operating system (OS) are detrimental as they can cause the OS to fail and affect all applications running on top of the OS. Detecting OS concurrency bugs is challenging due to the complexity of the OS synchronization, particularly with the presence of the OS specific interrupt context. Existing dynamic concurrency bug detection techniques are designed for user level applications and cannot be applied to operating systems. To detect OS concurrency bugs, we proposed a new type of annotations - interrupt related annotations - and generated 96,821 such annotations for the Linux kernel with little manual effort. These annotations have been used to automatically detect 9 real OS concurrency bugs (7 of which were previously unknown). Two of the key techniques that make the above contributions possible are: (1) using a hybrid approach to extract annotations from both code and comments written in natural language to achieve better coverage and accuracy in annotation extraction and bug detection; and (2) automatically propagating annotations to caller functions to improve annotating and bug detection. These two techniques are general and can be applied to non-OS code, code written in other programming languages such as Java, and for extracting other types of specifications.
[program debugging, data mining, Linux kernel, aComment, Concurrent computing, interrupt related annotation, interrupt related concurrency bugs, dynamic concurrency bug detection technique, interrupts, user level application, concurrency bug detection, Kernel, Context, operating system kernels, operating system, annotation extraction, operating systems, static analysis, Linux, Computer bugs, concurrency control, annotation mining, OS concurrency bugs, annotation languages, natural language]
Camouflage: automated anonymization of field data
2011 33rd International Conference on Software Engineering
None
2011
Privacy and security concerns have adversely affected the usefulness of many types of techniques that leverage information gathered from deployed applications. To address this issue, we present an approach for automatically anonymizing failure-inducing inputs that builds on a previously developed technique. Given an input I that causes a failure f, our approach generates an anonymized input I' that is different from I but still causes f. I' can thus be sent to developers to enable them to debug f without having to know I. We implemented our approach in a prototype tool, camouflage, and performed an extensive empirical evaluation where we applied camouflage to a large set of failure-inducing inputs for several real applications. The results of the evaluation are promising, as they show that camouflage is both practical and effective at generating anonymized inputs; for the inputs that we considered, I and I' shared no sensitive information. The results also show that our approach can outperform the general technique it extends.
[input anonymization, Switches, Credit cards, anonymized input, Security, CAMOUFLAGE, Optimization, Privacy, field data, security of data, failure-inducing inputs, automated anonymization, Prototypes, privacy concern, symbolic execution, data privacy, software engineering, Arrays, security concerns]
A lightweight code analysis and its role in evaluation of a dependability case
2011 33rd International Conference on Software Engineering
None
2011
A dependability case is an explicit, end-to-end argument, based on concrete evidence, that a system satisfies a critical property. We report on a case study constructing a dependability case for the control software of a medical device. The key novelty of our approach is a lightweight code analysis that generates a list of side conditions that correspond to assumptions to be discharged about the code and the environment in which it executes. This represents an unconventional trade-off between, at one extreme, more ambitious analyses that attempt to discharge all conditions automatically (but which cannot even in principle handle environmental assumptions), and at the other, flow- or context-insensitive analyses that require more user involvement. The results of the analysis suggested a variety of ways in which the dependability of the system might be improved.
[Protons, dependability case, biomedical equipment, side conditions, user interfaces, Servers, side conditions list, property-part diagram, code analysis, patient treatment, medical control systems, user involvement, Hardware, software engineering, Safety, Structural beams, control engineering computing, medical device, problem frames, end-to-end argument, control software, flow-insensitive analysis, context-insensitive analysis, lightweight code analysis, Software, Concrete]
Towards quantitative software reliability assessment in incremental development processes
2011 33rd International Conference on Software Engineering
None
2011
The iterative and incremental development is becoming a major development process model in industry, and allows us for a good deal of parallelism between development and testing. In this paper we develop a quantitative software reliability assessment method in incremental development processes, based on the familiar non-homogeneous Poisson processes. More specifically, we utilize the software metrics observed in each incremental development and testing, and estimate the associated software reliability measures. In a numerical example with a real incremental developmental project data, it is shown that the estimate of software reliability with a specific model can take a realistic value, and that the reliability growth phenomenon can be observed even in the incremental development scheme.
[iterative methods, software reliability, incremental development processes, reliability growth phenomenon, maximum likelihood estimation, associated software reliability measures, realistic value, Software metrics, stochastic processes, incremental developmental project data, Testing, quantitative software reliability assessment method, data analysis, incremental testing, Software reliability, incremental development, iterative development, Software, Data models, familiar nonhomogeneous Poisson processes, incremental development scheme, development process model, software reliability estimation, non-homogeneous poisson processes, software metrics]
The impact of fault models on software robustness evaluations
2011 33rd International Conference on Software Engineering
None
2011
Following the design and in-lab testing of software, the evaluation of its resilience to actual operational perturbations in the field is a key validation need. Software-implemented fault injection (SWIFI) is a widely used approach for evaluating the robustness of software components. Recent research [24, 18] indicates that the selection of the applied fault model has considerable influence on the results of SWIFI-based evaluations, thereby raising the question how to select appropriate fault models (i.e. that provide justified robustness evidence). This paper proposes several metrics for comparatively evaluating fault models's abilities to reveal robustness vulnerabilities. It demonstrates their application in the context of OS device drivers by investigating the influence (and relative utility) of four commonly used fault models, i.e. bit flips (in function parameters and in binaries), data type dependent parameter corruptions, and parameter fuzzing. We assess the efficiency of these models at detecting robustness vulnerabilities during the SWIFI evaluation of a real embedded operating system kernel and discuss application guidelines for our metrics alongside.
[Measurement, Context, operating system kernels, program testing, software robustness evaluation, Complexity theory, Servers, data type dependent parameter corruption, software fault tolerance, fault model, software-implemented fault injection, SWIFI, fault models, OS device driver, embedded operating system kernel, bit flips, Robustness, Software, fault injection, Transient analysis, parameter fuzzing, robustness testing, software metrics]
Transformation for class immutability
2011 33rd International Conference on Software Engineering
None
2011
It is common for object-oriented programs to have both mutable and immutable classes. Immutable classes simplify programing because the programmer does not have to reason about side-effects. Sometimes programmers write immutable classes from scratch, other times they transform mutable into immutable classes. To transform a mutable class, programmers must find all methods that mutate its transitive state and all objects that can enter or escape the state of the class. The analyses are non-trivial and the rewriting is tedious. Fortunately, this can be automated. We present an algorithm and a tool, Immutator, that enables the programmer to safely transform a mutable class into an immutable class. Two case studies and one controlled experiment show that Immutator is useful. It (i) reduces the burden of making classes immutable, (ii) is fast enough to be used interactively, and (iii) is much safer than manual transformations.
[Algorithm design and analysis, immutability, Java, rewriting systems, object-oriented programming, Cloning, immutable classes, Transforms, Data structures, Production facilities, immutator, rewriting, class immutability transformation, object-oriented programs, Libraries, program transformation]
Refactoring Java programs for flexible locking
2011 33rd International Conference on Software Engineering
None
2011
Recent versions of the Java standard library offer flexible locking constructs that go beyond the language's built-in monitor locks in terms of features, and that can be fine-tuned to suit specific application scenarios. Under certain conditions, the use of these constructs can improve performance significantly, by reducing lock contention. However, the code transformations needed to convert between locking constructs are non-trivial, and great care must be taken to update lock usage throughout the program consistently. We present Relocker, an automated tool that assists programmers with refactoring synchronized blocks into ReentrantLocks and ReadWriteLocks, to make exploring the performance tradeoffs among these constructs easier. In experiments on a collection of real-world Java applications, Relocker was able to refactor over 80% of built-in monitors into ReentrantLocks. Additionally, in most cases the tool could automatically infer the same ReadWriteLock usage that programmers had previously introduced manually.
[Java, language built-in monitor locks, program diagnostics, ReadWriteLocks, Switches, code transformations, Throughput, synchronized block refactoration, Synchronization, software maintenance, Sun, Java standard library, refactoring, flexible locking, ReentrantLocks, Benchmark testing, Java program refactoring, relocker, read-write locks, Monitoring, monitors]
Refactoring pipe-like mashups for end-user programmers
2011 33rd International Conference on Software Engineering
None
2011
Mashups are becoming increasingly popular as end users are able to easily access, manipulate, and compose data from many web sources. We have observed, however, that mashups tend to suffer from deficiencies that propagate as mashups are reused. To address these deficiencies, we would like to bring some of the benefits of software engineering techniques to the end users creating these programs. In this work, we focus on identifying code smells indicative of the deficiencies we observed in web mashups programmed in the popular Yahoo! Pipes environment. Through an empirical study, we explore the impact of those smells on end-user programmers and observe that users generally prefer mashups without smells. We then introduce refactorings targeting those smells, reducing the complexity of the mashup programs, increasing their abstraction, updating broken data sources and dated components, and standardizing their structures to fit the community development patterns. Our assessment of a large sample of mashups shows that smells are present in 81% of them and that the proposed refactorings can reduce the number of smelly mashups to 16%, illustrating the potential of refactoring to support the thousands of end users programming mashups.
[Computer aided software engineering, Mashups, Communities, Redundancy, end user software engineering, end users programming mashups, Generators, Web sources, software maintenance, Yahoo! Pipes environment, code smells identification, web mashups, mashup program complexity, pipelike mashups refactoring, refactoring, Wires, end user programmers, software reusability, software engineering techniques, Internet, Web mashups, computational complexity]
Mining message sequence graphs
2011 33rd International Conference on Software Engineering
None
2011
Dynamic specification mining involves discovering software behavior from traces for the purpose of program comprehension and bug detection. However, mining program behavior from execution traces is difficult for concurrent/distributed programs. Specifically, the inherent partial order relationships among events occurring across processes pose a big challenge to specification mining. In this paper, we propose a framework for mining partial orders so as to understand concurrent program behavior. Our miner takes in a set of concurrent program traces, and produces a message sequence graph (MSG) to represent the concurrent program behavior. An MSG represents a graph where the nodes of the graph are partial orders, represented as Message Sequence Charts. Mining an MSG allows us to understand concurrent program behaviors since the nodes of the MSG depict important "phases" or "interaction snippets" involving several concurrently executing processes. To demonstrate the power of this technique, we conducted experiments on mining behaviors of several fairly complex distributed systems. We show that our miner can produce the corresponding MSGs with both high precision and recall.
[program comprehension, program debugging, message sequence charts, Learning automata, program diagnostics, Unified modeling language, Merging, message sequence graph mining, Banking, Maintenance engineering, distributed processing, partial order relationships, bug detection, Data mining, concurrent program behavior understanding, specification mining, dynamic specification mining, software behavior, directed graphs, distributed systems, Software, concurrent program traces, partial order mining framework]
Automatically detecting and describing high level actions within methods
2011 33rd International Conference on Software Engineering
None
2011
One approach to easing program comprehension is to reduce the amount of code that a developer has to read. Describing the high level abstract algorithmic actions associated with code fragments using succinct natural language phrases potentially enables a newcomer to focus on fewer and more abstract concepts when trying to understand a given method. Unfortunately, such descriptions are typically missing because it is tedious to create them manually. We present an automatic technique for identifying code fragments that implement high level abstractions of actions and expressing them as a natural language description. Our studies of 1000 Java programs indicate that our heuristics for identifying code fragments implementing high level actions are widely applicable. Judgements of our generated descriptions by 15 experienced Java programmers strongly suggest that indeed they view the fragments that we identify as representing high level actions and our synthesized descriptions accurately express the abstraction.
[program comprehension, Java, high level actions, high level abstract algorithmic actions, natural language processing, program diagnostics, succinct natural language phrases, Natural languages, automatic technique, Documentation, documentation, code fragments, program compilers, Pragmatics, Semantics, natural language description, Java programmers, Syntactics, Software, abstract concepts, Java programs]
Portfolio: finding relevant functions and their usage
2011 33rd International Conference on Software Engineering
None
2011
Different studies show that programmers are more interested in finding definitions of functions and their uses than variables, statements, or arbitrary code fragments [30, 29, 31]. Therefore, programmers require support in finding relevant functions and determining how those functions are used. Unfortunately, existing code search engines do not provide enough of this support to developers, thus reducing the effectiveness of code reuse. We provide this support to programmers in a code search system called Portfolio that retrieves and visualizes relevant functions and their usages. We have built Portfolio using a combination of models that address surfing behavior of programmer and sharing Related concepts among functions. We conducted an experiment with 49 professional programmers to compare Portfolio to Google Code Search and Koders using a standard methodology. The results show with strong statistical significance that users find more relevant functions with higher precision with Portfolio than with Google Code Search and Koders.
[Koders, Visualization, search engines, statistical significance, Google code search, function retrieval, Portfolio function, function call graph, portfolio, data visualisation, function visualization, Search engines, Portfolios, Google, Navigation, code search, information retrieval, code reuse, Portfolio code search system, Storage area networks, code search engine, Portfolio usage, ranking, statistical analysis, pagerank]
Angelic debugging
2011 33rd International Conference on Software Engineering
None
2011
Software ships with known bugs because it is expensive to pinpoint and fix the bug exposed by a failing test. To reduce the cost of bug identification, we locate expressions that are likely causes of bugs and thus candidates for repair. Our symbolic method approximates an ideal approach to fixing bugs mechanically, which is to search the space of all edits to the program for one that repairs the failing test without breaking any passing test. We approximate the expensive ideal of exploring syntactic edits by instead computing the set of values whose substitution for the expression corrects the execution. We observe that an expression is a repair candidate if it can be replaced with a value that fixes a failing test and in each passing test, its value can be changed to another value without breaking the test. The latter condition makes the expression flexible in that it permits multiple values. The key observation is that the repair of a flexible expression is less likely to break a passing test. The method is called angelic debugging because the values are computed by angelically nondeterministic statements. We implemented the method on top of the Java PathFinder model checker. Our experiments with this technique show promise of its applicability in speeding up program debugging.
[Computers, angelic non-determinism, Java, program debugging, program testing, angelic debugging, nondeterministic statements, Debugging, symbolic method, Maintenance engineering, bug identification, formal verification, tests, Computer bugs, symbolic execution, debugging, Concrete, search space, failing test, Java PathFinder model checker, Testing]
An empirical study of build maintenance effort
2011 33rd International Conference on Software Engineering
None
2011
The build system of a software project is responsible for transforming source code and other development artifacts into executable programs and deliverables. Similar to source code, build system specifications require maintenance to cope with newly implemented features, changes to imported Application Program Interfaces (APIs), and source code restructuring. In this paper, we mine the version histories of one proprietary and nine open source projects of different sizes and domain to analyze the overhead that build maintenance imposes on developers. We split our analysis into two dimensions: (1) Build Coupling, i.e., how frequently source code changes require build changes, and (2) Build Ownership, i.e., the proportion of developers responsible for build maintenance. Our results indicate that, despite the difference in scale, the build system churn rate is comparable to that of the source code, and build changes induce more relative churn on the build system than source code changes induce on the source code. Furthermore, build maintenance yields up to a 27% overhead on source code development and a 44% overhead on test development. Up to 79% of source code developers and 89% of test code developers are significantly impacted by build maintenance, yet investment in build experts can reduce the proportion of impacted developers to 22% of source code developers and 24% of test code developers.
[Java, application program interfaces, build coupling, build systems, mining software repositories, Maintenance engineering, software management, software maintenance, software project, Couplings, Linux, source code development, build ownership, empirical software engineering, Software, Libraries, API, build maintenance effort, source code restructuring]
An empirical investigation into the role of API-level refactorings during software evolution
2011 33rd International Conference on Software Engineering
None
2011
It is widely believed that refactoring improves software quality and programmer productivity by making it easier to maintain and understand software systems. However, the role of refactorings has not been systematically investigated using fine-grained evolution history. We quantitatively and qualitatively studied API-level refactorings and bug fixes in three large open source projects, totaling 26523 revisions of evolution. The study found several surprising results: One, there is an increase in the number of bug fixes after API-level refactorings. Two, the time taken to fix bugs is shorter after API-level refactorings than before. Three, a large number of refactoring revisions include bug fixes at the same time or are related to later bug fix revisions. Four, API-level refactorings occur more frequently before than after major software releases. These results call for re-thinking refactoring's true benefits. Furthermore, frequent floss refactoring mistakes observed in this study call for new software engineering tools to support safe application of refactoring and behavior modifying edits together.
[Productivity, program debugging, open source projects, application program interfaces, Manuals, empirical study, fine-grained evolution history, software quality, History, Data mining, software maintenance, software engineering tools, software evolution, frequent floss refactoring mistakes, bug fixes, Accuracy, refactoring, Computer bugs, defects, release cycle, Software, software tools, API-level refactorings]
Factors leading to integration failures in global feature-oriented development: an empirical analysis
2011 33rd International Conference on Software Engineering
None
2011
Feature-driven software development is a novel approach that has grown in popularity over the past decade. Researchers and practitioners alike have argued that numerous benefits could be garnered from adopting a feature-driven development approach. However, those persuasive arguments have not been matched with supporting empirical evidence. Moreover, developing software systems around features involves new technical and organizational elements that could have significant implications for outcomes such as software quality. This paper presents an empirical analysis of a large-scale project that implemented 1195 features in a software system. We examined the impact that technical attributes of product features, attributes of the feature teams and cross-feature interactions have on software integration failures. Our results show that technical factors such as the nature of component dependencies and organizational factors such as the geographic dispersion of the feature teams and the role of the feature owners had complementary impact suggesting their independent and important role in terms of software quality. Furthermore, our analyses revealed that cross-feature interactions, measured as the number of architectural dependencies between two product features, are a major driver of integration failures. The research and practical implications of our results are discussed.
[technical elements, cross-feature interaction, software development management, Programming, software quality, product features, Dispersion, feature team attributes, Couplings, large scale project, software integration failures, organizational elements, global software development, global feature oriented development, Software quality, feature-oriented development, Software systems, feature driven software development, Software measurement, cross feature interactions, organisational aspects]
Assessing programming language impact on development and maintenance: a study on c and c++
2011 33rd International Conference on Software Engineering
None
2011
Billions of dollars are spent every year for building and maintaining software. To reduce these costs we must identify the key factors that lead to better software and more productive development. One such key factor, and the focus of our paper, is the choice of programming language. Existing studies that analyze the impact of choice of programming language suffer from several deficiencies with respect to methodology and the applications they consider. For example, they consider applications built by different teams in different languages, hence fail to control for developer competence, or they consider small-sized, infrequently-used, short-lived projects. We propose a novel methodology which controls for development process and developer competence, and quantifies how the choice of programming language impacts software quality and developer productivity. We conduct a study and statistical analysis on a set of long-lived, widely-used, open source projects - Firefox, Blender, VLC, and MySQL. The key novelties of our study are: (1) we only consider projects which have considerable portions of development in two languages, C and C++, and (2) a majority of developers in these projects contribute to both C and C++ code bases. We found that using C++ instead of C results in improved software quality and reduced maintenance effort, and that code bases are shifting from C to C++. Our methodology lays a solid foundation for future studies on comparative advantages of particular programming languages.
[Measurement, public domain software, Complexity theory, software quality, VLC, developer productivity, software evolution, productivity, Fires, MySQL, programming language, Testing, open source projects, software development, Maintenance engineering, C++ language, software maintenance, productive development, empirical studies, Firefox, Computer bugs, Blender, Software, high-level languages, statistical analysis]
On-demand feature recommendations derived from mining public product descriptions
2011 33rd International Conference on Software Engineering
None
2011
We present a recommender system that models and recommends product features for a given domain. Our approach mines product descriptions from publicly available online specifications, utilizes text mining and a novel incremental diffusive clustering algorithm to discover domain-specific features, generates a probabilistic feature model that represents commonalities, variants, and cross-category features, and then uses association rule mining and the k-Nearest-Neighbor machine learning strategy to generate product specific feature recommendations. Our recommender system supports the relatively labor-intensive task of domain analysis, potentially increasing opportunities for re-use, reducing time-to-market, and delivering more competitive software products. The approach is empirically validated against 20 different product categories using thousands of product descriptions mined from a repository of free software applications.
[Algorithm design and analysis, ondemand feature recommendation, data mining, probability, information retrieval, public product description mining, domain analysis, Association rules, online specification, association rule mining, incremental diffusive clustering algorithm, recommender systems, recommender system, k-nearest-neighbor machine learning, Clustering algorithms, domain-specific feature, probabilistic feature model, Feature extraction, Software, text mining, clustering, learning (artificial intelligence), Recommender systems, Viruses (medical)]
Inferring better contracts
2011 33rd International Conference on Software Engineering
None
2011
Considerable progress has been made towards automatic support for one of the principal techniques available to enhance program reliability: equipping programs with extensive contracts. The results of current contract inference tools are still often unsatisfactory in practice, especially for programmers who already apply some kind of basic Design by Contract discipline, since the inferred contracts tend to be simple assertions - the very ones that programmers find easy to write. We present new, completely automatic inference techniques and a supporting tool, which take advantage of the presence of simple programmer-written contracts in the code to infer sophisticated assertions, involving for example implication and universal quantification. Applied to a production library of classes covering standard data structures such as linked lists, arrays, stacks, queues and hash tables, the tool is able, entirely automatically, to infer 75% of the complete contracts - contracts yielding the full formal specification of the classes - with very few redundant or irrelevant clauses.
[contract inference tool, linked lists, design-by-contract discipline, hash tables, software reliability, stacks, data mining, Data structures, inference technique, Software reliability, Indexes, Data mining, inference mechanisms, formal specification, program reliability, queues, Software, arrays, Decision trees, Contracts, contract inference, invariants, random testing]
LIME: a framework for debugging load imbalance in multi-threaded execution
2011 33rd International Conference on Software Engineering
None
2011
With the ubiquity of multi-core processors, software must make effective use of multiple cores to obtain good performance on modern hardware. One of the biggest roadblocks to this is load imbalance, or the uneven distribution of work across cores. We propose LIME, a framework for analyzing parallel programs and reporting the cause of load imbalance in application source code. This framework uses statistical techniques to pinpoint load imbalance problems stemming from both control flow issues (e.g., unequal iteration counts) and interactions between the application and hardware (e.g., unequal cache miss counts). We evaluate LIME on applications from widely used parallel benchmark suites, and show that LIME accurately reports the causes of load imbalance, their nature and origin in the code, and their relative importance.
[program debugging, Correlation, multiprocessing systems, application source code, multi-threading, Instruction sets, statistical techniques, Debugging, performance debugging, Dynamic scheduling, Regression analysis, parallel programs, control flow issues, load imbalance, Accuracy, multithreaded execution, parallel section, LIME, load imbalance debugging, Hardware, multicore processor ubiquity, statistical analysis]
Synthesis of live behaviour models for fallible domains
2011 33rd International Conference on Software Engineering
None
2011
We revisit synthesis of live controllers for event-based operational models. We remove one aspect of an idealised problem domain by allowing to integrate failures of controller actions in the environment model. Classical treatment of failures through strong fairness leads to a very high computational complexity and may be insufficient for many interesting cases. We identify a realistic stronger fairness condition on the behaviour of failures. We show how to construct controllers satisfying liveness specifications under these fairness conditions. The resulting controllers exhibit the only possible behaviour in face of the given topology of failures: they keep retrying and never give up. We then identify some well-structure conditions on the environment. These conditions ensure that the resulting controller will be eager to satisfy its goals. Furthermore, for environments that satisfy these conditions and have an underlying probabilistic behaviour, the measure of traces that satisfy our fairness condition is 1, giving a characterisation of the kind of domains in which the approach is applicable.
[Context, Adaptation models, Protocols, event-based operational models, live behaviour models, Ceramics, behavioural modelling, probabilistic behaviour, controller synthesis, controller synthesis technique, fallible domains, Belts, Polynomials, software engineering, Safety, computational complexity]
Coverage guided systematic concurrency testing
2011 33rd International Conference on Software Engineering
None
2011
Shared-memory multi-threaded programs are notoriously difficult to test, and because of the often astronomically large number of thread schedules, testing all possible interleavings is practically infeasible. In this paper we propose a coverage-guided systematic testing framework, where we use dynamically learned ordering constraints over shared object accesses to select only high-risk interleavings for test execution. An interleaving is of high-risk if it has not been covered by the ordering constraints, meaning that it has concurrency scenarios that have not been tested. Our method consists of two components. First, we utilize dynamic information collected from good test runs to learn ordering constraints over the memory-accessing and synchronization statements. These ordering constraints are treated as likely invariants since they are respected by all the tested runs. Second, during the process of systematic testing, we use the learned ordering constraints to guide the selection of interleavings for future test execution. Our experiments on public domain multithreaded C/C++ programs show that, by focusing on only the high-risk interleavings rather than enumerating all possible interleavings, our method can increase the coverage of important concurrency scenarios with a reasonable cost and detect most of the concurrency bugs in practice.
[Schedules, program testing, C, Instruction sets, public domain software, test execution, memory access, Concurrent computing, Reactive power, Systematics, learned ordering constraints, shared memory program, interleaved storage, shared memory systems, synchronization statements, multi-threaded programs, coverage-guided systematic testing, constraint handling, shared object accesses, Testing, coverage, C++, partial order reduction, multi-threading, concurrency testing, dynamic information collection, concurrency theory, interleavings, C++ language, public domain programs, concurrency, Computer bugs]
Inference of field initialization
2011 33rd International Conference on Software Engineering
None
2011
A raw object is partially initialized, with only some fields set to legal values. It may violate its object invariants, such as that a given field is non-null. Programs often manipulate partially-initialized objects, but they must do so with care. Furthermore, analyses must be aware of field initialization. For instance, proving the absence of null pointer dereferences or of division by zero, or proving that object invariants are satisfied, requires information about initialization. We present a static analysis that infers a safe over-approximation of the program variables, fields, and array elements that, at run time, might hold raw objects. Our formalization is flow-sensitive and interprocedural, and it considers the exception flow in the analyzed program. We have proved the analysis sound and implemented it in a tool called Julia that computes initialization and nullness information. We have evaluated Julia on over 160K lines of code. We have compared its output to manually-written initialization and nullness information, and to an independently-written type-checking tool that checks initialization and nullness. Julia's output is accurate and useful both to programmers and to static analyses.
[independently-written type-checking tool, Java, initialization computation, program verification, program diagnostics, inference, abstract interpretation, Receivers, Manuals, static analysis, program variable over-approximation, flow-sensitive formalization, Approximation methods, inference mechanisms, partially-initialized objects, nullness information, field initialization, Semantics, initialization, Software, Arrays, interprocedural formalization]
Taming reflection: Aiding static analysis in the presence of reflection and custom class loaders
2011 33rd International Conference on Software Engineering
None
2011
Static program analyses and transformations for Java face many problems when analyzing programs that use reflection or custom class loaders: How can a static analysis know which reflective calls the program will execute? How can it get hold of classes that the program loads from remote locations or even generates on the fly? And if the analysis transforms classes, how can these classes be re-inserted into a program that uses custom class loaders? In this paper, we present TamiFlex, a tool chain that offers a partial but often effective solution to these problems. With TamiFlex, programmers can use existing static-analysis tools to produce results that are sound at least with respect to a set of recorded program runs. TamiFlex inserts runtime checks into the program that warn the user in case the program executes reflective calls that the analysis did not take into account. TamiFlex further allows programmers to re-insert offline-transformed classes into a program. We evaluate TamiFlex in two scenarios: benchmarking with the DaCapo benchmark suite and analysing large-scale interactive applications. For the latter, TamiFlex significantly improves code coverage of the static analyses, while for the former our approach even appears complete: the inserted runtime checks issue no warning. Hence, for the first time, TamiFlex enables sound static whole-program analyses on DaCapo. During this process, TamiFlex usually incurs less than 10% runtime overhead.
[Heuristic algorithms, Transforms, taming reflection, TamiFlex, static program analysis, tool chain, offline-transformed classes, Runtime, DaCapo benchmark suite, reflective calls, Benchmark testing, software tools, runtime check, dynamic class loaders, dynamic class loading, Java, tracing, code coverage, Instruments, program diagnostics, reflection, static analysis, Virtual machining, static-analysis tool, large-scale interactive application analysis, native code, custom class loaders]
Patching vulnerabilities with sanitization synthesis
2011 33rd International Conference on Software Engineering
None
2011
We present automata-based static string analysis techniques that automatically generate sanitization statements for patching vulnerable web applications. Our approach consists of three phases: Given an attack pattern we first conduct a vulnerability analysis to identify if strings that match the attack pattern can reach the security-sensitive functions. Next, we compute vulnerability signatures that characterize all input strings that can exploit the discovered vulnerability. Given the vulnerability signatures, we then construct sanitization statements that 1) check if a given input matches the vulnerability signature and 2) modify the input in a minimal way so that the modified input does not match the vulnerability signature. Our approach is capable of generating relational vulnerability signatures (and corresponding sanitization statements) for vulnerabilities that are due to more than one input.
[sanitization synthesis, Input variables, automata theory, program diagnostics, security-sensitive functions, Doped fiber amplifiers, Security, Approximation methods, Reachability analysis, vulnerable Web application patching, security of data, automata-based static string analysis techniques, vulnerability signatures, Impedance matching, Automata, automatic sanitization statement generation, Internet, automata, string analysis]
Configuring global software teams: a multi-company analysis of project productivity, quality, and profits
2011 33rd International Conference on Software Engineering
None
2011
In this paper, we examined the impact of project-level configurational choices of globally distributed software teams on project productivity, quality, and profits. Our analysis used data from 362 projects of four different firms. These projects spanned a wide range of programming languages, application domain, process choices, and development sites spread over 15 countries and 5 continents. Our analysis revealed fundamental tradeoffs in choosing configurational choices that are optimized for productivity, quality, and/or profits. In particular, achieving higher levels of productivity and quality require diametrically opposed configurational choices. In addition, creating imbalances in the expertise and personnel distribution of project teams significantly helps increase profit margins. However, a profit-oriented imbalance could also significantly affect productivity and/or quality outcomes. Analyzing these complex tradeoffs, we provide actionable managerial insights that can help software firms and their clients choose configurations that achieve desired project outcomes in globally distributed software development.
[application domain, empirical analysis, project quality, software engineering economics, Companies, development sites, Programming, distributed processing, global distributed software teams, DP management, Personnel, programming languages, multicompany analysis, globally distributed software development, project profits, global distributed software development, software engineering, Productivity, Biological system modeling, project-level configurational choices, project productivity, Dispersion, process choices, quality management, Software]
Does the initial environment impact the future of developers
2011 33rd International Conference on Software Engineering
None
2011
Software developers need to develop technical and social skills to be successful in large projects. We model the relative sociality of developer as a ratio between the size of her communication network and the number of tasks she participates in. We obtain both measures from the problem tracking systems. We use her workflow peer network to represent her social learning, and the issues she has worked on to represent her technical learning. Using three open source and three traditional projects we investigate how the project environment reflected by the sociality measure at the time a developer joins, affects her future participation. We find: a) the probability that a new developer will become one of long-term and productive developers is highest when the project sociality is low; b) times of high sociality are associated with a higher intensity of new contributors joining the project; c) there are significant differences between the social learning trajectories of the developers who join in low and in high sociality environments; d) the open source and commercial projects exhibit different nature in the relationship between developer's tenure and the project's environment at the time she joins. These findings point out the importance of the initial environment in determining the future of the developers and may lead to better training and learning strategies in software organizations.
[open source project, relative sociality, software developers, project sociality, History, initial environment, learning trajectory, initial environment importance, workflow peer network, project environment, commercial project, Trajectory, social aspects of automation, Social network services, probability, software development management, Time measurement, technical learning, Atmospheric measurements, social learning, Particle measurements, problem tracking systems, Software, software organization, socio-technical balance, developer sociality model]
Socio-technical developer networks: should we trust our measurements?
2011 33rd International Conference on Software Engineering
None
2011
Software development teams must be properly structured to provide effectiv collaboration to produce quality software. Over the last several years, social network analysis (SNA) has emerged as a popular method for studying the collaboration and organization of people working in large software development teams. Researchers have been modeling networks of developers based on socio-technical connections found in software development artifacts. Using these developer networks, researchers have proposed several SNA metrics that can predict software quality factors and describe the team structure. But do SNA metrics measure what they purport to measure? The objective of this research is to investigate if SNA metrics represent socio-technical relationships by examining if developer networks can be corroborated with developer perceptions. To measure developer perceptions, we developed an online survey that is personalized to each developer of a development team based on that developer's SNA metrics. Developers answered questions about other members of the team, such as identifying their collaborators and the project experts. A total of 124 developers responded to our survey from three popular open source projects: the Linux kernel, the PHP programming language, and the Wireshark network protocol analyzer. Our results indicate that connections in the developer network are statistically associated with the collaborators whom the developers named. Our results substantiate that SNA metrics represent socio-technical relationships in open source development projects, while also clarifying how the developer network can be interpreted by researchers and practitioners.
[Measurement, operating system kernels, software development teams, open source development projects, public domain software, developers, Linux kernel, Programming, Electronic mail, software quality, SNA metrics, Wireshark network protocol analyzer, Linux, developer networks, Collaboration, network analysers, social network analysis, developer network, social networking (online), PHP programming language, sociotechnical developer network, Kernel, software metrics]
Model projection: simplifying models in response to restricting the environment
2011 33rd International Conference on Software Engineering
None
2011
This paper introduces Model Projection. Finite state models such as Extended Finite State Machines are being used in an ever increasing number of software engineering activities. Model projection facilitates model development by specializing models for a specific operating environment. A projection is useful in many design-level applications including specification reuse and property verification. The applicability of model projection rests upon three critical concerns: correctness, effectiveness, and efficiency, all of which are addressed in this paper. We introduce four related algorithms for model projection and prove each correct. We also present an empirical study of effectiveness and efficiency using ten models, including widely studied benchmarks as well as industrial models. Results show that a typical projection includes about half of the states and a third of the transitions from the original model.
[slicing, program verification, model simplification, Atmospheric modeling, software engineering activities, Unified modeling language, Merging, design level applications, model projection, finite state machines, industrial models, specification reuse, finite state models, Analytical models, Semantics, property verification, environment restriction, software reusability, Silicon, Sugar, extended finite state machines]
MeCC: memory comparison-based clone detector
2011 33rd International Conference on Software Engineering
None
2011
In this paper, we propose a new semantic clone detection technique by comparing programs' abstract memory states, which are computed by a semantic-based static analyzer. Our experimental study using three large-scale open source projects shows that our technique can detect semantic clones that existing syntactic- or semantic-based clone detectors miss. Our technique can help developers identify inconsistent clone changes, find refactoring candidates, and understand software evolution related to semantic clones.
[refactoring candidate finding, clone detection, abstract interpretation, Cloning, static analysis, Educational institutions, semantic clone detection technique, software maintenance, memory comparison-based clone detector, Reactive power, software evolution understanding, Semantics, Detectors, Syntactics, program abstract memory state, Software, large-scale open source projects, clone change identification, MeCC, semantic-based static analyzer]
Frequency and risks of changes to clones
2011 33rd International Conference on Software Engineering
None
2011
Code Clones - duplicated source fragments - are said to increase maintenance effort and to facilitate problems caused by inconsistent changes to identical parts. While this is certainly true for some clones and certainly not true for others, it is unclear how many clones are real threats to the system's quality and need to be taken care of. Our analysis of clone evolution in mature software projects shows that most clones are rarely changed and the number of unintentional inconsistent changes to clones is small. We thus have to carefully select the clones to be managed to avoid unnecessary effort managing clones with no risk potential.
[Java, Software maintenance, clone detection, Terminology, duplicated source fragments, Merging, Cloning, data flow analysis, Maintenance engineering, software quality, software maintenance, code clones, clone evolution, mature software projects, system quality, clone evolution analysis]
Symbolic model checking of software product lines
2011 33rd International Conference on Software Engineering
None
2011
We study the problem of model checking software product line (SPL) behaviours against temporal properties. This is more difficult than for single systems because an SPL with n features yields up to 2n individual systems to verify. As each individual verification suffers from state explosion, it is crucial to propose efficient formalisms and heuristics. We recently proposed featured transition systems (FTS), a compact representation for SPL behaviour, and defined algorithms for model checking FTS against linear temporal properties. Although they showed to outperform individual system verifications, they still face a state explosion problem as they enumerate and visit system states one by one. In this paper, we tackle this latter problem by using symbolic representations of the state space. This lead us to consider computation tree logic (CTL) which is supported by the industry-strength symbolic model checker NuSMV. We first lay the foundations for symbolic SPL model checking by defining a feature-oriented version of CTL and its dedicated algorithms. We then describe an implementation that adapts the NuSMV language and tool infrastructure. Finally, we propose theoretical and empirical evaluations of our results. The benchmarks show that for certain properties, our algorithm is over a hundred times faster than model checking each system with the standard algorithm.
[program testing, program verification, tool infrastructure, symbolic representation, NuSMV language, formal logic, features, Boolean functions, Rain, Semantics, featured transition system, product development, symbolic model checking, software product lines, Computational modeling, computation tree logic, system verification, specification, Encoding, Explosions, software product line, SPL behaviour, temporal property, software reusability, Software]
Verifying multi-threaded software using smt-based context-bounded model checking
2011 33rd International Conference on Software Engineering
None
2011
We describe and evaluate three approaches to model check multi-threaded software with shared variables and locks using bounded model checking based on Satisfiability Modulo Theories (SMT) and our modelling of the synchronization primitives of the Pthread library. In the lazy approach, we generate all possible interleavings and call the SMT solver on each of them individually, until we either find a bug, or have systematically explored all interleavings. In the schedule recording approach, we encode all possible interleavings into one single formula and then exploit the high speed of the SMT solvers. In the underapproximation and widening approach, we reduce the state space by abstracting the number of interleavings from the proofs of unsatisfiability generated by the SMT solvers. In all three approaches, we bound the number of context switches allowed among threads in order to reduce the number of interleavings explored. We implemented these approaches in ESBMC, our SMT-based bounded model checker for ANSI-C programs. Our experiments show that ESBMC can analyze larger problems and substantially reduce the verification time compared to state-of-the-art techniques that use iterative context-bounding algorithms or counter-example guided abstraction refinement.
[Context, Schedules, SMT solver, multi-threading, Instruction sets, state space, sat modulo theories, Software algorithms, computability, multi-threaded systems, ANSI-C programs, symbolic and explicit model checking, iterative context-bounding algorithm, Reactive power, formal verification, SMT-based context-bounded model checking, Satisfiability Modulo Theories, counter-example guided abstraction refinement, multithreaded software, formal software verification, SMT-based bounded model checker, Context modeling]
Run-time efficient probabilistic model checking
2011 33rd International Conference on Software Engineering
None
2011
Unpredictable changes continuously affect software systems and may have a severe impact on their quality of service, potentially jeopardizing the system's ability to meet the desired requirements. Changes may occur in critical components of the system, clients' operational profiles, requirements, or deployment environments. The adoption of software models and model checking techniques at run time may support automatic reasoning about such changes, detect harmful configurations, and potentially enable appropriate (self-)reactions. However, traditional model checking techniques and tools may not be simply applied as they are at run time, since they hardly meet the constraints imposed by on-the-fly analysis, in terms of execution time and memory occupation. This paper precisely addresses this issue and focuses on reliability models, given in terms of Discrete Time Markov Chains, and probabilistic model checking. It develops a mathematical framework for run-time probabilistic model checking that, given a reliability model and a set of requirements, statically generates a set of expressions, which can be efficiently used at run-time to verify system requirements. An experimental comparison of our approach with existing probabilistic model checkers shows its practical applicability in run-time verification.
[run-time verification, discrete time Markov chain, software reliability, software system, Analytical models, formal verification, run-time model checking, run-time efficient probabilistic model checking technique, Mathematical model, Transient analysis, Computational modeling, probability, client operational profile, software model, on-the-ffy analysis, reliability model, Probabilistic logic, mathematical framework, quality of service, inference mechanisms, memory occupation, automatic reasoning, Markov processes, Reliability, discrete time markov chains]
Non-essential changes in version histories
2011 33rd International Conference on Software Engineering
None
2011
Numerous techniques involve mining change data captured in software archives to assist engineering efforts, for example to identify components that tend to evolve together. We observed that important changes to software artifacts are sometimes accompanied by numerous non-essential modifications, such as local variable refactorings, or textual differences induced as part of a rename refactoring. We developed a tool-supported technique for detecting non-essential code differences in the revision histories of software systems. We used our technique to investigate code changes in over 24,000 change sets gathered from the change histories of seven long-lived open-source systems. We found that up to 15.5% of a system's method updates were due solely to non-essential differences. We also report on numerous observations on the distribution of non-essential differences in change history and their potential impact on change-based analyses.
[Java, public domain software, change based analyses, software change analysis, data mining, mining software repositories, Programming, software archives, change data mining, rename refactoring, History, Data mining, software maintenance, nonessential code difference detection, Open source software, tool supported technique, software artifacts, local variable refactorings, long lived open source systems, nonessential version history changes, differencing algorithms, software tools, Catalogs]
Aspect recommendation for evolving software
2011 33rd International Conference on Software Engineering
None
2011
Cross-cutting concerns are unavoidable and create difficulties in the development and maintenance of large-scale systems. In this paper, we present a novel approach that identifies certain groups of code units that potentially share some cross-cutting concerns and recommends them for creating and updating aspects. Those code units, called concern peers, are detected based on their similar interactions (similar calling relations in similar contexts, either internally or externally). The recommendation is applicable to both the aspectization of non-aspect-oriented programs (i.e. for aspect creation), and the evolution of aspect-oriented programs (i.e. for aspect updating). The empirical evaluation on several real-world software systems shows that our approach is scalable and provides useful recommendations.
[Context, Algorithm design and analysis, code units, cross-cutting concern, Peer to peer computing, similar contexts, aspect mining, Containers, Maintenance engineering, aspectization, aspect recommendation, cross-cutting concerns, concern peer, software maintenance, similar calling relations, nonaspect-oriented programs, large-scale systems, real-world software systems, Databases, evolving software, concern peers, aspect-oriented programming, Software]
Identifying program, test, and environmental changes that affect behaviour
2011 33rd International Conference on Software Engineering
None
2011
Developers evolve a software system by changing the program source code, by modifying its context by updating libraries or changing its configuration, and by improving its test suite. Any of these changes can cause differences in program behaviour. In general, program paths may appear or disappear between executions of two subsequent versions of a system. Some of these behavioural differences are expected by a developer; for example, executing new program paths is often precisely what is intended when adding a new test. Other behavioural differences may or may not be expected or benign. For example, changing an XML configuration file may cause a previously-executed path to disappear, which may or may not be expected and could be problematic. Furthermore, the degree to which a behavioural change might be problematic may only become apparent over time as the new behaviour interacts with other changes. We present an approach to identify specific program call dependencies where the programmer's changes to the program source code, its tests, or its environment are not apparent in the system's behaviour, or vice versa. Using a static and a dynamic call graph from each of two program versions, we partition dependencies based on their presence in each of the four graphs. Particular partitions contain dependencies that help a programmer develop insights about often subtle behavioural changes.
[dynamic impact analysis, software behaviour, library updating, program behaviour, program diagnostics, environmental change, configuration changing, test identification, static analysis, Educational institutions, Approximation methods, dynamic call graph, Runtime, directed graphs, Prototypes, Focusing, Data visualization, program call dependencies, program identification, Libraries, context modification, program source code, comparative analyses, static call graph]
Program abstractions for behaviour validation
2011 33rd International Conference on Software Engineering
None
2011
Code artefacts that have non-trivial requirements with respect to the ordering in which their methods or procedures ought to be called are common and appear, for instance, in the form of API implementations and objects. This work addresses the problem of validating if API implementations provide their intended behaviour when descriptions of this behaviour are informal, partial or non-existent. The proposed approach addresses this problem by generating abstract behaviour models which resemble typestates. These models are statically computed and encode all admissible sequences of method calls. The level of abstraction at which such models are constructed has shown to be useful for validating code artefacts and identifying findings which led to the discovery of bugs, adjustment of the requirements expected by the engineer to the requirements implicit in the code, and the improvement of available documentation.
[Algorithm design and analysis, program debugging, API implementation, Computational modeling, program diagnostics, Software algorithms, source code validation, Approximation methods, behaviour model synthesis, formal specification, nontrivial requirement, behaviour validation, Semantics, bug discovery, code artefact, Concrete, Safety, abstract behaviour model, automated abstraction, program abstraction]
Programs, tests, and oracles: the foundations of testing revisited
2011 33rd International Conference on Software Engineering
None
2011
In previous decades, researchers have explored the formal foundations of program testing. By exploring the foundations of testing largely separate from any specific method of testing, these researchers provided a general discussion of the testing process, including the goals, the underlying problems, and the limitations of testing. Unfortunately, a common, rigorous foundation has not been widely adopted in empirical software testing research, making it difficult to generalize and compare empirical research. We continue this foundational work, providing a framework intended to serve as a guide for future discussions and empirical studies concerning software testing. Specifically, we extend Gourlay's functional description of testing with the notion of a test oracle, an aspect of testing largely overlooked in previous foundational work and only lightly explored in general. We argue additional work exploring the interrelationship between programs, tests, and oracles should be performed, and use our extension to clarify concepts presented in previous work, present new concepts related to test oracles, and demonstrate that oracle selection must be considered when discussing the efficacy of a testing process.
[Software testing, Reactive power, program testing, software testing, testing formalism, NASA, theory of testing, Syntactics, Software, test oracle, formal foundations, Software engineering]
RACEZ: a lightweight and non-invasive race detection tool for production applications
2011 33rd International Conference on Software Engineering
None
2011
Concurrency bugs, particularly data races, are notoriously difficult to debug and are a significant source of unreliability in multithreaded applications. Many tools to catch data races rely on program instrumentation to obtain memory instruction traces. Unfortunately, this instrumentation introduces significant runtime overhead, is extremely invasive, or has a limited domain of applicability making these tools unsuitable for many production systems. Consequently, these tools are typically used during application testing where many data races go undetected. This paper proposes RACEZ , a novel race detection mechanism which uses a sampled memory trace collected by the hardware performance monitoring unit rather than invasive instrumentation. The approach introduces only a modest overhead making it usable in production environments. We validate RACEZ using two open source server applications and the PARSEC benchmarks. Our experiments show that RACEZ catches a set of known bugs with reasonable probability while introducing only 2.8% runtime slow down on average.
[program debugging, multiprocessing programs, multi-threading, Instruments, Instruction sets, production systems, production engineering computing, production applications, sampling, program instrumentation, RACEZ, Synchronization, Servers, probability analysis, noninvasive race detection tool, lightweight race detection tool, Computer bugs, Phasor measurement units, concurrency bugs, multithreaded applications, performance monitoring unit, Monitoring, data races]
Detecting software modularity violations
2011 33rd International Conference on Software Engineering
None
2011
This paper presents Clio, an approach that detects modularity violations, which can cause software defects, modularity decay, or expensive refactorings. Clio computes the discrepancies between how components should change together based on the modular structure, and how components actually change together as revealed in version history. We evaluated Clio using 15 releases of Hadoop Common and 10 releases of Eclipse JDT. The results show that hundreds of violations identified using Clio were indeed recognized as design problems or refactored by the developers in later versions. The identified violations exhibit multiple symptoms of poor design, some of which are not easily detectable using existing approaches.
[modular structure, design structure matrix, Unified modeling language, Cloning, software defect, expensive refactorings, History, software maintenance, Clio, Couplings, USA Councils, refactoring, modularity decay, Games, Software, bad code smells, modularity violation detection, software modularity violation]
Feature cohesion in software product lines: an exploratory study
2011 33rd International Conference on Software Engineering
None
2011
Software product lines gain momentum in research and industry. Many product-line approaches use features as a central abstraction mechanism. Feature-oriented software development aims at encapsulating features in cohesive units to support program comprehension, variability, and reuse. Surprisingly, not much is known about the characteristics of cohesion in feature-oriented product lines, although proper cohesion is of special interest in product-line engineering due to its focus on variability and reuse. To fill this gap, we conduct an exploratory study on forty software product lines of different sizes and domains. A distinguishing property of our approach is that we use both classic software measures and novel measures that are based on distances in clustering layouts, which can be used also for visual exploration of product-line architectures. This way, we can draw a holistic picture of feature cohesion. In our exploratory study, we found several interesting correlations (e.g., between development process and feature cohesion) and we discuss insights and perspectives of investigating feature cohesion (e.g., regarding feature interfaces and programming style).
[program comprehension, product-line engineering, software product lines, Visualization, feature cohesion, visual clustering, Programming, Couplings, program reuse, clustering layout, software product line, feature-oriented software development, pattern clustering, Layout, Clustering algorithms, program variability, software measurement, Software, software engineering, Software measurement, featurevisu]
Leveraging software architectures to guide and verify the development of sense/compute/control applications
2011 33rd International Conference on Software Engineering
None
2011
A software architecture describes the structure of a computing system by specifying software components and their interactions. Mapping a software architecture to an implementation is a well known challenge. A key element of this mapping is the architecture's description of the data and control-flow interactions between components. The characterization of these interactions can be rather abstract or very concrete, providing more or less implementation guidance, programming support, and static verification. In this paper, we explore one point in the design space between abstract and concrete component interaction specifications. We introduce a notion of interaction contract that expresses allowed interactions between components, describing both data and control-flow constraints. This declaration is part of the architecture description, allows generation of extensive programming support, and enables various verifications. We instantiate our approach in an architecture description language for Sense/Compute/Control applications, and describe associated compilation and verification strategies.
[Context, Actuators, software architectures, Programming, software component specification, concrete component interaction specifications, Web servers, formal specification, software architecture, development verification, formal verification, sense/compute/control applications, Computer architecture, architectural conformance, Sensors, generative programming, Contracts, abstract component interaction specifications]
Refactoring to role objects
2011 33rd International Conference on Software Engineering
None
2011
Role objects are a widely recognized design pattern for representing objects that expose different properties in different contexts. By developing a tool that automatically refactors legacy code towards this pattern and by applying this tool to several programs, we have found not only that refactoring to role objects as currently defined produces code that is hard to read and to maintain, but also that the refactoring has preconditions so strong that it is rarely applicable in practice. We have therefore taken a fresh look at role objects and devised an alternative form that solves the exact same design problems, yet is much simpler to introduce and to maintain. We describe refactoring to this new, lightweight form of role objects in informal terms and report on the implementation of our refactoring tool for the JAVA programming language, presenting evidence of the refactoring's increased applicability in several sample programs.
[Context, Java, object-oriented programming, Receivers, Java programming language, roles, object schizophrenia, software maintenance, ubiquitous computing, design problems, design pattern, Couplings, refactors legacy code, refactoring to patterns, role object refactoring tool, object representation, Organizations, Concrete, object-oriented methods, delegation, Object oriented programming]
Supporting professional spreadsheet users by generating leveled dataflow diagrams
2011 33rd International Conference on Software Engineering
None
2011
Thanks to their flexibility and intuitive programming model, spreadsheets are widely used in industry, often for business-critical applications. Similar to software developers, professional spreadsheet users demand support for maintaining and transferring their spreadsheets. In this paper, we first study the problems and information needs of professional spreadsheet users by means of a survey conducted at a large financial company. Based on these needs, we then present an approach that extracts this information from spreadsheets and presents it in a compact and easy to understand way, with leveled dataflow diagrams. Our approach comes with three different views on the dataflow that allow the user to analyze the dataflow diagrams in a top-down fashion. To evaluate the usefulness of the proposed approach, we conducted a series of interviews as well as nine case studies in an industrial setting. The results of the evaluation clearly indicate the demand for and usefulness of our approach in ease the understanding of spreadsheets.
[Industries, top-down fashion, leveled dataflow diagrams, visualization, intuitive programming model, financial company, data flow diagrams, Companies, data flow analysis, Educational institutions, spreadsheet programs, software developers, diagrams, Programming profession, end-user programming, professional spreadsheet users, spreadsheets, financial management, Software, software engineering, information needs, business-critical applications, Interviews, industrial setting]
Reverse engineering feature models
2011 33rd International Conference on Software Engineering
None
2011
Feature models describe the common and variable characteristics of a product line. Their advantages are well recognized in product line methods. Unfortunately, creating a feature model for an existing project is time-consuming and requires substantial effort from a modeler. We present procedures for reverse engineering feature models based on a crucial heuristic for identifying parents - the major challenge of this task. We also automatically recover constructs such as feature groups, mandatory features, and implies/excludes edges. We evaluate the technique on two large-scale software product lines with existing reference feature models--the Linux and eCos kernels--and FreeBSD, a project without a feature model. Our heuristic is effective across all three projects by ranking the correct parent among the top results for a vast majority of features. The procedures effectively reduce the information a modeler has to consider from thousands of choices to typically five or less.
[software product lines, feature models, feature similarity, operating system kernels, reverse engineering feature model, feature group, Biological system modeling, Image edge detection, Buildings, Reverse engineering, Linux kernel, implies-excludes edge, mandatory feature, eCos kernel, Linux, Semantics, product line method, variability modeling, Feature extraction, software engineering, FreeBSD]
Empirical assessment of MDE in industry
2011 33rd International Conference on Software Engineering
None
2011
This paper presents some initial results from a twelve-month empirical research study of model driven engineering (MDE). Using largely qualitative questionnaire and interview methods we investigate and document a range of technical, organizational and social factors that apparently influence organizational responses to MDE: specifically, its perception as a successful or unsuccessful organizational intervention. We then outline a range of lessons learned. Whilst, as with all qualitative research, these lessons should be interpreted with care, they should also be seen as providing a greater understanding of MDE practice in industry, as well as shedding light on the varied, and occasionally surprising, social, technical and organizational factors that affect success and failure. We conclude by suggesting how the next phase of the research will attempt to investigate some of these issues from a different angle and in greater depth.
[Productivity, Industries, social factors, Unified modeling language, Companies, production engineering computing, model driven engineering, research study, organizational intervention, Training, organizational factors, technical factors, industries, research and development, empirical software engineering, Software, software engineering, Interviews, organisational aspects]
Dealing with noise in defect prediction
2011 33rd International Conference on Software Engineering
None
2011
Many software defect prediction models have been built using historical defect data obtained by mining software repositories (MSR). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises. This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding FP (false positive) or FN (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20%-35% of both FP and FN noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved.
[program debugging, program testing, Noise, data mining, mining software repositories, buggy files, Predictive models, historical defect data, Noise measurement, Electrical resistance measurement, MSR, software defect prediction models, bug report links, Training, Resistance, data quality, defect prediction, false positive noise, noise resistance, Prediction algorithms, buggy changes, optional bug fix keywords, false negative noise]
Ownership, experience and defects: a fine-grained study of authorship
2011 33rd International Conference on Software Engineering
None
2011
Recent research indicates that "people" factors such as ownership, experience, organizational structure, and geographic distribution have a big impact on software quality. Understanding these factors, and properly deploying people resources can help managers improve quality outcomes. This paper considers the impact of code ownership and developer experience on software quality. In a large project, a file might be entirely owned by a single developer, or worked on by many. Some previous research indicates that more developers working on a file might lead to more defects. Prior research considered this phenomenon at the level of modules or files, and thus does not tease apart and study the effect of contributions of different developers to each module or file. We exploit a modern version control system to examine this issue at a fine-grained level. Using version history, we examine contributions to code fragments that are actually repaired to fix bugs. Are these code fragments "implicated" in bugs the result of contributions from many? or from one? Does experience matter? What type of experience? We find that implicated code is more strongly associated with a single developer's contribution; our findings also indicate that an author's specialized experience in the target file is more important than general experience. Our findings suggest that quality control efforts could be profitably targeted at changes made by single developers with limited prior experience on that file.
[Industries, Productivity, quality control, code developer, Control systems, software quality, code fragments, History, experience, ownership, Computer bugs, modern version control system, collaboration, Software quality, software developer]
Interface decomposition for service compositions
2011 33rd International Conference on Software Engineering
None
2011
Service-based applications can be realized by composing existing services into new, added-value composite services. The external services with which a service composition interacts are usually known by means of their syntactical interface. However, an interface providing more information, such as a behavioral specification, could be more useful to a service integrator for assessing that a certain external service can contribute to fulfill the functional requirements of the composite application. Given the requirements specification of a composite service, we present a technique for obtaining the behavioral interfaces - in the form of labeled transition systems - of the external services, by decomposing the global interface specification that characterizes the environment of the service composition. The generated interfaces guarantee that the service composition fulfills its requirements during the execution. Our approach has been implemented in the LTSA tool and has been applied to two case studies.
[Availability, Thyristors, composite service, Computational modeling, Government, labeled transition system, interface decomposition, services, syntactical interface, behavioral interface, formal specification, behavioral specification, servicecompositions, Silicon carbide, service integrator, service-based application, requirements specification, Software, Safety, service composition, functional requirement, LTSA tool]
Unifying execution of imperative and declarative code
2011 33rd International Conference on Software Engineering
None
2011
We present a unified environment for running declarative specifications in the context of an imperative object-Oriented programming language. Specifications are Alloy-like, written in first-order relational logic with transitive closure, and the imperative language is Java. By being able to mix imperative code with executable declarative specifications, the user can easily express constraint problems in place, i.e., in terms of the existing data structures and objects on the heap. After a solution is found, the heap is updated to reflect the solution, so the user can continue to manipulate the program heap in the usual imperative way. We show that this approach is not only convenient, but, for certain problems can also outperform a standard imperative implementation. We also present an optimization technique that allowed us to run our tool on heaps with almost 2000 objects.
[Context, Java, object-oriented programming, Encoding, formal specification, imperative code execution, declarative programming, Upper bound, Algebra, first-order relational logic, executable declarative specification, imperative language, formal methods, object-oriented languages, data structures, declarative code execution, Arrays, imperative object-oriented programming language, transitive closure, constraint-based languages, executable specifications]
Always-available static and dynamic feedback
2011 33rd International Conference on Software Engineering
None
2011
Developers who write code in a statically typed language are denied the ability to obtain dynamic feedback by executing their code during periods when it fails the static type checker. They are further confined to the static typing discipline during times in the development process where it does not yield the highest productivity. If they opt instead to use a dynamic language, they forgo the many benefits of static typing, including machine-checked documentation, improved correctness and reliability, tool support (such as for refactoring), and better runtime performance. We present a novel approach to giving developers the benefits of both static and dynamic typing, throughout the development process, and without the burden of manually separating their program into statically and dynamically-typed parts. Our approach, which is intended for temporary use during the development process, relaxes the static type system and provides a semantics for many type-incorrect programs. It defers type errors to run time, or suppresses them if they do not affect runtime semantics. We implemented our approach in a publicly available tool, DuctileJ, for the Java language. In case studies, DuctileJ conferred benefits both during prototyping and during the evolution of existing code.
[dynamic language, software reliability, prototyping, DuctileJ, gradual typing, hybrid typing, Runtime, productivity, refactoring, Semantics, Prototypes, Libraries, static type checker, dynamic feedback, many type-incorrect program semantics, Testing, Java, runtime semantics, software development management, static typing discipline, data flow analysis, always-available static feedback, programming language semantics, development process, static typing, machine-checked documentation, tool support, dynamic typing, Java language, statically typed language, Software, type error, code execution]
Improving requirements quality using essential use case interaction patterns
2011 33rd International Conference on Software Engineering
None
2011
Requirements specifications need to be checked against the 3C's - Consistency, Completeness and Correctness - in order to achieve high quality. This is especially difficult when working with both natural language requirements and associated semi-formal modelling representations. We describe a technique and support tool that allows us to perform semi-automated checking of natural language and semi-formal requirements models, supporting both consistency management between representations but also correctness and completeness analysis. We use a concept of essential use case interaction patterns to perform the correctness and completeness analysis on the semi-formal representation. We highlight potential inconsistencies, incompleteness and incorrectness using visual differencing in our support tool. We have evaluated our approach via an end user study which focused on the tool's usefulness, ease of use, ease of learning and user satisfaction and provided data for cognitive dimensions of notations analysis of the tool.
[Visualization, essential use cases, program verification, completeness analysis, requirements patterns, computational linguistics, semiformal representation, correctness analysis, essential use case interaction patterns, software quality, formal specification, Analytical models, requirements quality, requirements specifications, consistency completeness and correctness, Libraries, Mathematical model, Online banking, Natural languages, consistency management, semiformal modelling representations, support tool, tool support, requirements engineering, natural language requirements, semiautomated checking, Pattern matching]
Understanding broadcast based peer review on open source software projects
2011 33rd International Conference on Software Engineering
None
2011
Software peer review has proven to be a successful technique in open source software (OSS) development. In contrast to industry, where reviews are typically assigned to specific individuals, changes are broadcast to hundreds of potentially interested stakeholders. Despite concerns that reviews may be ignored, or that discussions will deadlock because too many uninformed stakeholders are involved, we find that this approach works well in practice. In this paper, we describe an empirical study to investigate the mechanisms and behaviours that developers use to find code changes they are competent to review. We also explore how stakeholders interact with one another during the review process. We manually examine hundreds of reviews across five high profile OSS projects. Our findings provide insights into the simple, community-wide techniques that developers use to effectively manage large quantities of reviews. The themes that emerge from our study are enriched and validated by interviewing long-serving core developers.
[OSS project, project management, peer-to-peer computing, public domain software, open source software, Communities, broadcast based peer review, long-serving core developer, grounded theory, software management, Electronic mail, case studies, History, open source software project, Linux, peer review, Software, software engineering, Interviews]
Software systems as cities: a controlled experiment
2011 33rd International Conference on Software Engineering
None
2011
Software visualization is a popular program comprehension technique used in the context of software maintenance, reverse engineering, and software evolution analysis. While there is a broad range of software visualization approaches, only few have been empirically evaluated. This is detrimental to the acceptance of software visualization in both the academic and the industrial world. We present a controlled experiment for the empirical evaluation of a 3D software visualization approach based on a city metaphor and implemented in a tool called CodeCity. The goal is to provide experimental evidence of the viability of our approach in the context of program comprehension by having subjects perform tasks related to program comprehension. We designed our experiment based on lessons extracted from the current body of research. We conducted the experiment in four locations across three countries, involving 41 participants from both academia and industry. The experiment shows that CodeCity leads to a statistically significant increase in terms of task correctness and decrease in task completion time. We detail the experiment we performed, discuss its results and reflect on the many lessons learned.
[Industries, Measurement, Visualization, software systems, program comprehension technique, Maintenance engineering, reverse engineering, software maintenance, software visualization, CodeCity, empirical validation, software evolution analysis, city metaphor, task completion time, Data visualization, data visualisation, 3D software visualization approach, Cities and towns, Software, task correctness]
Automated cross-browser compatibility testing
2011 33rd International Conference on Software Engineering
None
2011
With the advent of Web 2.0 applications and new browsers, the cross-browser compatibility issue is becoming increasingly important. Although the problem is widely recognized among web developers, no systematic approach to tackle it exists today. None of the current tools, which provide screenshots or emulation environments, specifies any notion of cross-browser compatibility, much less check it automatically. In this paper, we pose the problem of cross-browser compatibility testing of modern web applications as a 'functional consistency' check of web application behavior across different web browsers and present an automated solution for it. Our approach consists of (1) automatically analyzing the given web application under different browser environments and capturing the behavior as a finite-state machine; (2) formally comparing the generated models for equivalence on a pairwise-basis and exposing any observed discrepancies. We validate our approach on several open-source and industrial case studies to demonstrate its effectiveness and real-world relevance.
[Navigation, program testing, Humans, HTML, dynamic analysis, Browsers, finite state machines, Standards, cross-browser compatibility testing, Web 2.0 application, cross-browser compatibility notion, Fires, online front-ends, finite state machine, web testing, cross-browser compatibility, pairwise-basis equivalence, Testing]
A framework for automated testing of javascript web applications
2011 33rd International Conference on Software Engineering
None
2011
Current practice in testing JavaScript web applications requires manual construction of test cases, which is difficult and tedious. We present a framework for feedback-directed automated test generation for JavaScript in which execution is monitored to collect information that directs the test generator towards inputs that yield increased coverage. We implemented several instantiations of the framework, corresponding to variations on feedback-directed random testing, in a tool called Artemis. Experiments on a suite of JavaScript applications demonstrate that a simple instantiation of the framework that uses event handler registrations as feedback information produces surprisingly good coverage if enough tests are generated. By also using coverage information and read-write sets as feedback information, a slightly better level of coverage can be achieved, and sometimes with many fewer tests. The generated tests can be used for detecting HTML validity problems and other programming errors.
[Java, program testing, event driven, HTML, Generators, automated testing, Browsers, Servers, web applications, ajax, javascript, feedback-directed automated test generation, Reactive power, event handler registration, Web pages, Javascript Web application, read-write sets, debugging, Artemis, Internet, coverage information, Testing, random testing]
Coalescing executions for fast uncertainty analysis
2011 33rd International Conference on Software Engineering
None
2011
Uncertain data processing is critical in a wide range of applications such as scientific computation handling data with inevitable errors and financial decision making relying on human provided parameters. While increasingly studied in the area of databases, uncertain data processing is often carried out by software, and thus software based solutions are attractive. In particular, Monte Carlo (MC) methods execute software with many samples from the uncertain inputs and observe the statistical behavior of the output. In this paper, we propose a technique to improve the cost-effectiveness of MC methods. Assuming only part of the input is uncertain, the certain part of the input always leads to the same execution across multiple sample runs. We remove such redundancy by coalescing multiple sample runs in a single run. In the coalesced run, the program operates on a vector of values if uncertainty is present and a single value otherwise. We handle cases where control flow and pointers are uncertain. Our results show that we can speed up the execution time of 30 sample runs by an average factor of 2.3 without precision lost or by up to 3.4 with negligible precision lost.
[Uncertainty, coalescing, uncertainty handling, human provided parameters, financial decision making, uncertainty, monte carlo, Proteins, Monte Carlo methods, coalescing executions, redundancy, Mathematical model, Kernel, uncertainty analysis, scientific computation data handling, data flow analysis, Data processing, statistical behavior, MC methods, software based solutions, decision making, scientific information systems, data handling, sensitivity, statistical analysis, uncertain data processing]
Mining parametric specifications
2011 33rd International Conference on Software Engineering
None
2011
Specifications carrying formal parameters that are bound to concrete data at runtime can effectively and elegantly capture multi-object behaviors or protocols. Unfortunately, parametric specifications are not easy to formulate by nonexperts and, consequently, are rarely available. This paper presents a general approach for mining parametric specifications from program executions, based on a strict separation of concerns: (1) a trace slicer first extracts sets of independent interactions from parametric execution traces; and (2) the resulting non-parametric trace slices are then passed to any conventional non-parametric property learner. The presented technique has been implemented in jMiner, which has been used to automatically mine many meaningful and non-trivial parametric properties of OpenJDK 6.
[Java, formal parameter specification, Protocols, OpenJDK 6, program verification, Noise, data mining, JMlNER, program execution, dynamic analysis, Complexity theory, Data mining, parametric specification mining, formal specification, multiobject behavior, Runtime, parametric execution traces, nonparametric trace slice, nontrivial parametric properties, protocols, Monitoring, nonparametric property learner, parametric specifications]
Estimating footprints of model operations
2011 33rd International Conference on Software Engineering
None
2011
When performed on a model, a set of operations (e.g., queries or model transformations) rarely uses all the information present in the model. Unintended underuse of a model can indicate various problems: the model may contain more detail than necessary or the operations may be immature or erroneous. Analyzing the footprints of the operations - i.e., the part of a model actually used by an operation - is a simple technique to diagnose and analyze such problems. However, precisely calculating the footprint of an operation is expensive, because it requires analyzing the operation's execution trace. In this paper, we present an automated technique to estimate the footprint of an operation without executing it. We evaluate our approach by applying it to 75 models and five operations. Our technique provides software engineers with an efficient, yet precise, evaluation of the usage of their models.
[Computational modeling, Object oriented modeling, Heuristic algorithms, program diagnostics, Unified modeling language, Petri nets, static analysis, software enginering, model operation, Analytical models, model operations, model footprint, software engineering, footprint estimation, Context modeling]
Precise identification of problems for structural test generation
2011 33rd International Conference on Software Engineering
None
2011
An important goal of software testing is to achieve at least high structural coverage. To reduce the manual efforts of producing such high-covering test inputs, testers or developers can employ tools built based on automated structural test-generation approaches. Although these tools can easily achieve high structural coverage for simple programs, when they are applied on complex programs in practice, these tools face various problems, such as (1) the external-method-call problem (EMCP), where tools cannot deal with method calls to external libraries; (2) the object-creation problem (OCP), where tools fails to generate method-call sequences to produce desirable object states. Since these tools currently could not be powerful enough to deal with these problems in testing complex programs in practice, we propose cooperative developer testing, where developers provide guidance to help tools achieve higher structural coverage. To reduce the efforts of developers in providing guidance to tools, in this paper, we propose a novel approach, called Covana, which precisely identifies and reports problems that prevent the tools from achieving high structural coverage primarily by determining whether branch statements containing notcovered branches have data dependencies on problem candidates. We provide two techniques to instantiate Covana to identify EMCPs and OCPs. Finally, we conduct evaluations on two open source projects to show the effectiveness of Covana in identifying EMCPs and OCPs.
[Software testing, Covana, program testing, Instruments, precise identification, software testing, Engines, Runtime, object-creation problem, data dependency, EMCP, problem identification, external-method-call problem, Libraries, automated structural test-generation, dynamic symbolic execution, structural test generation, OCP, Monitoring]
Interactivity, continuity, sketching, and experience: (keynote abstract)
2011 33rd International Conference on Software Engineering
None
2011
The design and development of a software system deals with both the world of making and that of using. The world of making is concerned with molding, constructing and building. The world of using is concerned with engaging, experiencing, and interacting-with. The two worlds are structurally, semantically, and temporally intertwined through software. Designing the world of using requires painstaking effort to envision all the possible situations of use for all the possible types of users in all the possible contexts, in various temporal and situational levels of granularity, to create a coherent and convivial user experience. To frame this world of using, it is not sufficient to identify typical use scenarios or depict snapshots of crucial usage situations. Thorough analyses of the possible flows of interactions over a long period of time through the dynamism of user engagement and experience are essential in framing the world of using. Through the delineation of our work on designing and developing research prototype tools for creative knowledge activities, observing and analyzing interaction design processes, and directing user experience design teams for consumer products, this talk will address the expression, representation, communication, and assessment of the design of the world of using from the perspectives of interactivity, continuity, sketching and experience.
[Industries, Engineering profession, Educational institutions, consumer products, Computer science, software system development, Software systems, software engineering, user experience design teams, Cognitive science, software system design, verification, user centred design]
Exciting new trends in design thinking
2011 33rd International Conference on Software Engineering
None
2011
Summary form only given. Design and design thinking are becoming the hot topics and new business processes around the world - yes, business processes! Business schools are adding design thinking courses to their curricula and business professors are writing books on design thinking. Countries like Korea and Singapore are vying to be the leading Asian Design Nations. New, so-called Convergent courses, programs and schools are emerging globally that combine engineering, business and design disciplines and departments into integrated efforts. The Do-It-Yourself (DIY) Design Movement is gaining momentum and the personal discipline of Making things is coming back. DIY Prototyping and Manufacturing are gaining ground and opportunities with new technologies and innovations. User-Generated Design is becoming a common corporate process. Design process and design thinking are being applied cross-functionally to such global issues as clean water and alternative energy. And the old traditional view of design as art and decoration and styling is giving way to a broader and more comprehensive way of thinking and solving human-centered problems by other than just a few elite professionals. In light of all this and more, Bill is excited about the ideas of ubiquitous design education for everyone and DIY design as a universal human experience. He is passionate about an idea in what Victor Papanek said 40 years ago in his seminal book, Design for the Real World, "All that we do, almost all the time, is design, for design is basic to all human activity". Just as all humans are inherently businesspeople in many ways at many times, we are also all designers in many ways at many times - it is time to believe this and make the best of it.
[Computers, Art, convergent courses, Humans, management science, business schools, ubiquitous design education, business processes, Educational institutions, Product design, DIY manufacturing, DIY prototyping, human centered problems, user generated design, educational courses, do-it-yourself design movement, Asian design nations, verification, Business, management education, design thinking courses, universal human experience]
A case study of measuring process risk for early insights into software safety
2011 33rd International Conference on Software Engineering
None
2011
In this case study, we examine software safety risk in three flight hardware systems in NASA's Constellation spaceflight program. We applied our Technical and Process Risk Measurement (TPRM) methodology to the Constellation hazard analysis process to quantify the technical and process risks involving software safety in the early design phase of these projects. We analyzed 154 hazard reports and collected metrics to measure the prevalence of software in hazards and the specificity of descriptions of software causes of hazardous conditions. We found that 49-70% of 154 hazardous conditions could be caused by software or software was involved in the prevention of the hazardous condition. We also found that 12-17% of the 2013 hazard causes involved software, and that 23-29% of all causes had a software control. The application of the TRPM methodology identified process risks in the application of the hazard analysis process itself that may lead to software safety risk.
[NASA, software safety, flight hardware systems, technical and process risk measurement, constellation program, Hazards, Software safety, Personnel, software maintenance, measurement, risk analysis, TPRM, hazard reports, process risk measurement, safety, software control, aerospace computing, NASA constellation spaceflight program, empirical software engineering, Software measurement]
Model-driven engineering practices in industry
2011 33rd International Conference on Software Engineering
None
2011
In this paper, we attempt to address the relative absence of empirical studies of model driven engineering through describing the practices of three commercial organizations as they adopted a model driven engineering approach to their software development. Using in-depth semi-structured interviewing we invited practitioners to reflect on their experiences and selected three to use as exemplars or case studies. In documenting some details of attempts to deploy model driven practices, we identify some 'lessons learned', in particular the importance of complex organizational, managerial and social factors - as opposed to simple technical factors - in the relative success, or failure, of the endeavour. As an example of organizational change management the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus.
[model-driven engineering practice, software development, organizational change management, Computational modeling, Unified modeling language, Companies, model driven engineering, organizational motivation, organizational commitment, Printers, manufacturing data processing, organizational factor, empirical evaluation, management of change, organizational process, social factor, Software, software engineering, organisational aspects, managerial factor, Software engineering]
SORASCS: a case study in soa-based platform design for socio-cultural analysis
2011 33rd International Conference on Software Engineering
None
2011
An increasingly important class of software-based systems is platforms that permit integration of third-party components, services, and tools. Service-Oriented Architecture (SOA) is one such platform that has been successful in providing integration and distribution in the business domain, and could be effective in other domains (e.g., scientific computing, healthcare, and complex decision making). In this paper, we discuss our application of SOA to provide an integration platform for socio-cultural analysis, a domain that, through models, tries to understand, analyze and predict relationships in large complex social systems. In developing this platform, called SORASCS, we had to overcome issues we believe are generally applicable to any application of SOA within a domain that involves technically nai&#x0308;ve users and seeks to establish a sustainable software ecosystem based on a common integration platform. We discuss these issues, the lessons learned about the kinds of problems that occur, and pathways toward a solution.
[socio-cultural analysis, Communities, Service oriented architecture, SORASCS, service oriented architectures, Analytical models, SOA based platform design, complex social systems, socio cultural analysis, business domain, User interfaces, Data models, social sciences computing, service oriented architecture, software based systems, service-oriented architecture, sustainable software ecosystem, platform design, Business]
A method for selecting SOA pilot projects including a pilot metrics framework
2011 33rd International Conference on Software Engineering
None
2011
Many organizations are introducing Service Oriented Architecture (SOA) as part of their business transformation projects to take advantage of the proposed benefits associated with using SOA. However, in many cases organizations don't necessarily know on which projects introducing SOA would be of value and show real benefits to the organization. In this paper we outline a method and pilot metrics framework (PMF) to help organization's select from a set of candidate projects those which would be most suitable for piloting SOA. The PMF is used as part of a method based on identifying a set of benefit and risk criteria, investigating each of the candidate projects, mapping them to the criteria and then selecting the most suitable project(s). The paper outlines a case study where the PMF was applied in a large government organization to help them select pilot projects and develop an overall strategy for introducing SOA into their organization.
[project management, government organization, Service oriented architecture, Semiconductor optical amplifiers, pilot metrics framework, business transformation project, Current measurement, SOA pilot project, Organizations, Radar, project selection criteria, service oriented architecture, Interviews, service-oriented architecture]
Architecture evaluation without an architecture: experience with the smart grid
2011 33rd International Conference on Software Engineering
None
2011
This paper describes an analysis of some of the challenges facing one portion of the Electrical Smart Grid in the United States - residential Demand Response (DR) systems. The purposes of this paper are twofold: 1) to discover risks to residential DR systems and 2) to illustrate an architecture-based analysis approach to uncovering risks that span a collection of technical and social concerns. The results presented here are specific to residential DR but the approach is general and it could be applied to other systems within the Smart Grid and to other critical infrastructure domains. Our architecture-based analysis is different from most other approaches to analyzing complex systems in that it addresses multiple quality attributes simultaneously (e.g., performance, reliability, security, modifiability, usability, etc.) and it considers the architecture of a complex system from a socio-technical perspective where the actions of the people in the system are as important, from an analysis perspective, as the physical and computational elements of the system. This analysis can be done early in a system's lifetime, before substantial resources have been committed to its construction or procurement, and so it provides extremely cost-effective risk analysis.
[smart grid, risk discovery, ultra-large-scale systems, smart power grids, residential DR system, cost-effective risk analysis, architecture evaluation, Security, complex system, architecture analysis, large-scale systems, risk analysis, socio-technical systems, software architecture, socio-technical concern, Electricity, quality attributes, Computer architecture, Pricing, Load management, Smart grids, demand side management, Reliability, United States residential demand response system]
Bringing domain-specific languages to digital forensics
2011 33rd International Conference on Software Engineering
None
2011
Digital forensics investigations often consist of analyzing large quantities of data. The software tools used for analyzing such data are constantly evolving to cope with a multiplicity of versions and variants of data formats. This process of customization is time consuming and error prone. To improve this situation we present DERRIC, a domain-specific language (DSL) for declaratively specifying data structures. This way, the specification of structure is separated from data processing. The resulting architecture encourages customization and facilitates reuse. It enables faster development through a division of labour between investigators and software engineers. We have performed an initial evaluation of DERRIC by constructing a data recovery tool. This so-called carver has been automatically derived from a declarative description of the structure of JPEG files. We compare it to existing carvers, and show it to be in the same league both with respect to recovered evidence, and runtime performance.
[Algorithm design and analysis, digital forensics, domain-specific languages, Data analysis, data analysis, Digital forensics, data description languages, declarative description, Documentation, DERRIC, Encoding, Generators, formal specification, JPEG files, data recovery tool, carver, data structure specification, Transform coding, model-driven engineering, data structures, software tools, computer forensics]
Building and using pluggable type-checkers
2011 33rd International Conference on Software Engineering
None
2011
This paper describes practical experience building and using pluggable type-checkers. A pluggable type-checker refines (strengthens) the built-in type system of a programming language. This permits programmers to detect and prevent, at compile time, defects that would otherwise have been manifested as run-time errors. The prevented defects may be generally applicable to all programs, such as null pointer dereferences. Or, an application-specific pluggable type system may be designed for a single application. We built a series of pluggable type checkers using the Checker Framework, and evaluated them on 2 million lines of code, finding hundreds of bugs in the process. We also observed 28 first-year computer science students use a checker to eliminate null pointer errors in their course projects. Along with describing the checkers and characterizing the bugs we found, we report the insights we had throughout the process. Overall, we found that the type checkers were easy to write, easy for novices to productively use, and effective in finding real bugs and verifying program properties, even for widely tested and used open source projects.
[program debugging, program verification, programming languages, nonnull, field descriptor, Runtime, java, fully qualified name, binary name, type system, programming language, enumeration, Testing, annotation, Java, pluggable type, application-specific pluggable type system, intern, checker framework, Documentation, program properties verification, type qualifier, run-time errors, bug finding, null pointer dereferences, case study, enum, Computer bugs, pluggable type-checkers, compile time, Arrays]
Deploying CogTool: integrating quantitative usability assessment into real-world software development
2011 33rd International Conference on Software Engineering
None
2011
Usability concerns are often difficult to integrate into real-world software development processes. To remedy this situation, IBM research and development, partnering with Carnegie Mellon University, has begun to employ a repeatable and quantifiable usability analysis method, embodied in CogTool, in its development practice. CogTool analyzes tasks performed on an interactive system from a storyboard and a demonstration of tasks on that storyboard, and predicts the time a skilled user will take to perform those tasks. We discuss how IBM designers and UX professionals used CogTool in their existing practice for contract compliance, communication within a product team and between a product team and its customer, assigning appropriate personnel to fix customer complaints, and quantitatively assessing design ideas before a line of code is written. We then reflect on the lessons learned by both the development organizations and the researchers attempting this technology transfer from academic research to integration into real-world practice, and we point to future research to even better serve the needs of practice.
[Productivity, quantitative usability assessment, Carnegie Mellon University, Computational modeling, Humans, customer complaints, cogtool, Programming, user interfaces, storyboard, usability engineering, contract compliance, user interface, real-world software development, software development practice, cognitive modeling, CogToo, software engineering, Usability, Contracts]
Experiences with text mining large collections of unstructured systems development artifacts at jpl
2011 33rd International Conference on Software Engineering
None
2011
Often repositories of systems engineering artifacts at NASA's Jet Propulsion Laboratory (JPL) are so large and poorly structured that they have outgrown our capability to effectively manually process their contents to extract useful information. Sophisticated text mining methods and tools seem a quick, low-effort approach to automating our limited manual efforts. Our experiences of exploring such methods mainly in three areas including historical risk analysis, defect identification based on requirements analysis, and over-time analysis of system anomalies at JPL, have shown that obtaining useful results requires substantial unanticipated efforts - from preprocessing the data to transforming the output for practical applications. We have not observed any quick 'wins' or realized benefit from short-term effort avoidance through automation in this area. Surprisingly we have realized a number of unexpected long-term benefits from the process of applying text mining to our repositories. This paper elaborates some of these benefits and our important lessons learned from the process of preparing and applying text mining to large unstructured system artifacts at JPL aiming to benefit future TM applications in similar problem domains and also in hope for being extended to broader areas of applications.
[Jet Propulsion Laboratory, text analysis, defect identification, data mining, Manuals, JPL, experience, formal verification, system anomaly over-time analysis, systems development artifact, text mining, Text mining, unstructured systems development artifacts, NASA, requirements assurance, information retrieval, historical risk analysis, systems engineering artifacts, system repository mining, Association rules, risk assurance, aerospace industry, requirements analysis, assurance, information extraction, systems analysis, large collections, sophisticated text mining methods, Software, risk, Risk management]
An evaluation of the internal quality of business applications: does size matter?
2011 33rd International Conference on Software Engineering
None
2011
This study summarizes results of a study of the internal, structural quality of 288 business applications comprising 108 million lines of code collected from 75 companies in 8 industry segments. These applications were submitted to a static analysis that evaluates quality within and across application components that may be coded in different languages. The analysis consists of evaluating the application against a repository of over 900 rules of good architectural and coding practice. Results are presented for measures of security, performance, and changeability. The effect of size on quality is evaluated, and the ability of modularity to reduce the impact of size is suggested by the results.
[Industries, quality size, benchmarking, Government, ISO standards, static analysis, maintainability performance efficiency, Encoding, internal quality, Complexity theory, software quality, business application, Security, architectural practice, software architecture, coding practice, business data processing, software metrics]
Characterizing the differences between pre- and post- release versions of software
2011 33rd International Conference on Software Engineering
None
2011
Many software producers utilize beta programs to predict post-release quality and to ensure that their products meet quality expectations of users. Prior work indicates that software producers need to adjust predictions to account for usage environments and usage scenarios differences between beta populations and post-release populations. However, little is known about how usage characteristics relate to field quality and how usage characteristics differ between beta and post-release. In this study, we examine application crash, application hang, system crash, and usage information from millions of Windows users to 1) examine the effects of usage characteristics differences on field quality (e.g. which usage characteristics impact quality), 2) examine usage characteristics differences between beta and post-release (e.g. do impactful usage characteristics differ), and 3) report experiences adjusting field quality predictions for Windows. Among the 18 usage characteristics that we examined, the five most important were: the number of application executed, whether the machines was pre-installed by the original equipment manufacturer, two sub-populations (two language/geographic locales), and whether Windows was 64-bit (not 32-bit). We found each of these usage characteristics to differ between beta and post-release, and by adjusting for the differences, accuracy of field quality predictions for Windows improved by ~59%.
[program verification, usage, software quality, user interfaces, Windows, field quality prediction, application hang, windows error reporting (wer), Runtime, software producer, Hardware, beta population, beta, customer experience improvement program, usage information, Computer crashes, Software reliability, Telemetry, windows, software version, reliability analysis component (rac), beta program, system crash, Software, equipment manufacturer]
Why software quality improvement fails: (and how to succeed nevertheless)
2011 33rd International Conference on Software Engineering
None
2011
Quality improvement is the key to enormous cost reduction in the IT business. However, improvement projects often fail in practice. In many cases, stakeholders fearing, e.g., a loss of power or not recognizing the benefits inhibit the improvement. Systematic change management and an economic perspective help to overcome these issues, but are little known and seldom applied. This industrial experience report presents the main challenges in software quality improvement projects as well as practices for tackling them. The authors have performed over 50 quality analyses and quality improvement projects in mission-critical software systems of European banking, insurance and automotive companies.
[Economics, Measurement, systematic change management, information technology, software quality, change management, Quality assurance, software quality improvement, economic perspective, quality improvement, quality assurance, Organizations, Software quality, IT business, industrial experience, business data processing]
Code coverage analysis in practice for large systems
2011 33rd International Conference on Software Engineering
None
2011
Large systems generate immense quantities of code coverage data. A user faced with the task of analyzing this data, for example, to decide on test areas to improve, faces a 'needle in a haystack' problem. In earlier studies we introduced substring hole analysis, a technique for presenting large quantities of coverage data in a succinct way. Here we demonstrate the successful use of substring hole analysis on large scale data from industrial software systems. For this end we augment substring hole analysis by introducing a work flow and tool support for practical code coverage analysis. We conduct real data experiments indicating that augmented substring hole analysis enables code coverage analysis where it was previously impractical, correctly identifies functionality that is missing from existing tests, and can increase the probability of finding bugs. These facilitate cost-effective code coverage analysis.
[Algorithm design and analysis, program debugging, data analysis, program diagnostics, augmented substring hole analysis, system test, code coverage analysis, substring hole analysis, needle in a haystack problem, industrial software system, Computer bugs, Semantics, Writing, debugging, Software, Risk management, Testing]
Practical change impact analysis based on static program slicing for industrial software systems
2011 33rd International Conference on Software Engineering
None
2011
Change impact analysis, i.e., knowing the potential consequences of a software change, is critical for the risk analysis, developer effort estimation, and regression testing of evolving software. Static program slicing is an attractive option for enabling routine change impact analysis for newly committed changesets during daily software build. For small programs with a few thousand lines of code, static program slicing scales well and can assist precise change impact analysis. However, as we demonstrate in this paper, static program slicing faces unique challenges when applied routinely on large and evolving industrial software systems. Despite recent advances in static program slicing, to our knowledge, there have been no studies of static change impact analysis applied on large and evolving industrial software systems. In this paper, we share our experiences in designing a static change impact analysis framework for such software systems. We have implemented our framework as a tool called Imp and have applied Imp on an industrial codebase with over a million lines of C/ C++ code with promising empirical results.
[Algorithm design and analysis, program testing, software testing, regression testing, C++ code, regression analysis, Data structures, software change, empirical study, Explosions, C++ language, C code, risk analysis, practical change impact analysis, Accuracy, industrial codebase, industrial software system, static program slicing, Software systems, Imp tool, program slicing, change impact analysis, Software engineering, developer effort estimation]
Value-based program characterization and its application to software plagiarism detection
2011 33rd International Conference on Software Engineering
None
2011
Identifying similar or identical code fragments becomes much more challenging in code theft cases where plagiarizers can use various automated code transformation techniques to hide stolen code from being detected. Previous works in this field are largely limited in that (1) most of them cannot handle advanced obfuscation techniques; (2) the methods based on source code analysis are less practical since the source code of suspicious programs is typically not available until strong evidences are collected; and (3) those depending on the features of specific operating systems or programming languages have limited applicability. Based on an observation that some critical runtime values are hard to be replaced or eliminated by semantics-preserving transformation techniques, we introduce a novel approach to dynamic characterization of executable programs. Leveraging such invariant values, our technique is resilient to various control and data obfuscation techniques. We show how the values can be extracted and refined to expose the critical values and how we can apply this runtime property to help solve problems in software plagiarism detection. We have implemented a prototype with a dynamic taint analyzer atop a generic processor emulator. Our experimental results show that the value-based method successfully discriminates 34 plagiarisms obfuscated by SandMark, plagiarisms heavily obfuscated by KlassMaster, programs obfuscated by Thicket, and executables obfuscated by Loco/Diablo.
[Java, source coding, program diagnostics, automated code transformation techniques, operating systems, SandMark, Registers, dynamic code identification, programming languages, Optimization, software plagiarism detection, data obfuscation techniques, Runtime, Program processors, KlassMaster, security of data, value-based program characterization, Loco/Diablo, Plagiarism, Thicket, operating systems (computers), software engineering, source code analysis, semantics-preserving transformation techniques]
A comparison of model-based and judgment-based release planning in incremental software projects
2011 33rd International Conference on Software Engineering
None
2011
Numerous factors are involved when deciding when to implement which features in incremental software development. To facilitate a rational and efficient planning process, release planning models make such factors explicit and compute release plan alternatives according to optimization principles. However, experience suggests that industrial use of such models is limited. To investigate the feasibility of model and tool support, we compared input factors assumed by release planning models with factors considered by expert planners. The former factors were cataloged by systematically surveying release planning models, while the latter were elicited through repertory grid interviews in three software organizations. The findings indicate a substantial overlap between the two approaches. However, a detailed analysis reveals that models focus on only select parts of a possibly larger space of relevant planning factors. Three concrete areas of mismatch were identified: (1) continuously evolving requirements and specifications, (2) continuously changing prioritization criteria, and (3) authority-based decision processes. With these results in mind, models, tools and guidelines can be adjusted to address better real-life development processes.
[surveying release planning models, grid computing, incremental software development, Programming, software management, substantial overlap, model-based release planning, industrial use, practitioners' mental models, planning (artificial intelligence), Systematics, optimisation, relevant planning factors, continuously changing prioritization criteria, software engineering, large development projects, real-life development processes, Interviews, rational planning process, expert planners, repertory grid interviews, Biological system modeling, case study, repertory grid, Couplings, tool support, continuously evolving specifications, incremental software projects, authority-based decision processes, optimization principles, continuously evolving requirements, scrum, Software, judgment-based release planning, Planning, agile, software organizations]
An industrial case study on quality impact prediction for evolving service-oriented software
2011 33rd International Conference on Software Engineering
None
2011
Systematic decision support for architectural design decisions is a major concern for software architects of evolving service-oriented systems. In practice, architects often analyse the expected performance and reliability of design alternatives based on prototypes or former experience. Model-driven prediction methods claim to uncover the tradeoffs between different alternatives quantitatively while being more cost-effective and less error-prone. However, they often suffer from weak tool support and focus on single quality attributes. Furthermore, there is limited evidence on their effectiveness based on documented industrial case studies. Thus, we have applied a novel, model-driven prediction method called Q-ImPrESS on a large-scale process control system consisting of several million lines of code from the automation domain to evaluate its evolution scenarios. This paper reports our experiences with the method and lessons learned. Benefits of Q-ImPrESS are the good architectural decision support and comprehensive tool framework, while one drawback is the time-consuming data collection.
[program testing, Unified modeling language, Quality of service, Predictive models, lqn, dtmc, reliability prediction, reverse engineering, case study, systematic decision support, palladio, performance prediction, Analytical models, quality impact prediction, model-driven prediction methods, service-oriented software, Computer architecture, Software, Reliability, architectural design decisions, Q-ImPrESS, service-oriented architecture, industrial software, trade-off analysis]
Enabling the runtime assertion checking of concurrent contracts for the Java modeling language
2011 33rd International Conference on Software Engineering
None
2011
Though there exists ample support for Design by Contract (DbC) for sequential programs, applying DbC to concurrent programs presents several challenges. In previous work, we extended the Java Modeling Language (JML) with constructs to specify concurrent contracts for Java programs. We present a runtime assertion checker (RAC) for the expanded JML capable of verifying assertions for concurrent Java programs. We systematically evaluate the validity of system testing results obtained via runtime assertion checking using actual concurrent and functional faults on a highly concurrent industrial system from the telecommunications domain.
[Java, Head, object-oriented programming, Instruments, Interference, jml, runtime assertion checking, concurrent contract, telecommunications domain, concurrency, design by contract, design-by-contract, Runtime, formal verification, java, concurrency control, simulation languages, Java programs, Contracts, Java modeling language, Message systems]
Perspectives of delegation in team-based distributed software development over the GENI infrastructure: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Team-based distributed software development (TBDSD) is one of the single biggest challenges facing software companies. The need to manage development efforts and resources in different locations increase the complexity and cost of modern day software development. Current software development environments do not provide suitable support to delegate task among teams with appropriate directives. TBDSD is also limited to the current internet capabilities. One of the resulting problems is the difficulty to delegate and control tasks assigned among remote teams. This paper proposes (1) a new framework for delegation in TBDSD, and (2) perspectives for deploying Process-centered Software Engineering Environments (PSEE) over the Global Environment for Network Innovations (GENI) infrastructure. GENI, the 'future Internet' that is taking shape in prototypes across the US, will allow, in the context of our study, to securely access and share software artifacts, resources, and tools as never before seen over the current Internet.
[Context, process, control task, GENI infrastructure, Process control, software company, Programming, geni, Internet capability, distributed software development, formal specification, software artifacts, NIER track, Prototypes, process-centered software engineering environments, global environment for network innovation infrastructure, Computer architecture, team-based distributed software development, Software, Internet, delegation]
The hidden experts in software-engineering communication: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Sharing knowledge in a timely fashion is important in distributed software development. However, because experts are difficult to locate, developers tend to broadcast information to find the right people, which leads to overload and to communication breakdowns. We study the context in which experts are included in an email discussion so that team members can identify experts sooner. In this paper, we conduct a case study examining why people emerge in discussions by examining email within a distributed team. We find that people emerge in the following four situations: when a crisis occurs, when they respond to explicit requests, when they are forwarded in announcements, and when discussants follow up on a previous event such as a meeting. We observe that emergent people respond not only to situations where developers are seeking expertise, but also to execute routine tasks. Our findings have implications for expertise seeking and knowledge management processes.
[Context, software-engineering communication, knowledge sharing, Programming, expertise seeking, distributed software development, Electronic mail, Servers, knowledge management, human factors in software engineering, NIER track, Organizations, distributed team, software engineering, collaborative software engineering, distributed programming, email]
How do programmers ask and answer questions on the web?: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Question and Answer (Q&amp;A) websites, such as Stack Overflow, use social media to facilitate knowledge exchange between programmers and fill archives with millions of entries that contribute to the body of knowledge in software development. Understanding the role of Q&amp;A websites in the documentation landscape will enable us to make recommendations on how individuals and companies can leverage this knowledge effectively. In this paper, we analyze data from Stack Overflow to categorize the kinds of questions that are asked, and to explore which questions are answered well and which ones remain unanswered. Our preliminary findings indicate that Q&amp;A websites are particularly effective at code reviews and conceptual questions. We pose research questions and suggest future work to explore the motivations of programmers that contribute to Q&amp;A websites, and to understand the implications of turning Q&amp;A exchanges into technical mini-blogs through the editing of questions and answers.
[Knowledge engineering, software development, Documentation, Programming, Media, questions, stack overflow, Encoding, Q&amp;A websites, question and answer, USA Councils, NIER track, q&#x0026;a, Software, software engineering, Internet, Web sites, social media]
Sketching tools for ideation: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Sketching facilitates design in the exploration of ideas about concrete objects and abstractions. In fact, throughout the software engineering process when grappling with new ideas, people reach for a pen and start sketching. While pen and paper work well, digital media can provide additional features to benefit the sketcher. Digital support will only be successful, however, if it does not detract from the core sketching experience. Based on research that defines characteristics of sketches and sketching, this paper offers three preliminary tool examples. Each example is intended to enable sketching while maintaining its characteristic experience.
[Visualization, Shape, software engineering process, hand-drawn computing, digital support, early design, tools, sketching, sketching tools, USA Councils, NIER track, Prototypes, software engineering, ideation, object-oriented methods, core sketching experience, Usability, Software engineering]
Digitally annexing desk space for software development: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Software engineering is a team activity yet the programmer's key tool, the IDE, is still largely that of a soloist. This paper describes the vision, implementation and initial evaluation of CoffeeTable - a fully featured research prototype resulting from our reflections on the software design process. CoffeeTable exchanges the traditional IDE for one built around a shared interactive desk. The proposed solution encourages smooth transitions between agile and traditional modes of working whilst helping to create a shared vision and common reference frame - key to sustaining a good design. This paper also presents early results from the evaluation of CoffeeTable and offers some insights from the lessons learned. In particular, it highlights the role of developer tools and the software constructions that are shaped by them.
[digitally annexing desk space, Visualization, collocation, Navigation, software development, collaboration and collaborative construction, software visualisation, Programming, interactive desk, CoffeeTable, cscw, IDE, ide, tabletop user interfaces, NIER track, Collaboration, data visualisation, groupware, Software, software engineering, Interviews, Software engineering, software design process]
Information foraging as a foundation for code navigation: NIER track
2011 33rd International Conference on Software Engineering
None
2011
A major software engineering challenge is to understand the fundamental mechanisms that underlie the developer's code navigation behavior. We propose a novel and unified theory based on the premise that we can study developer's information seeking strategies in light of the foraging principles that evolved to help our animal ancestors to find food. Our preliminary study on code navigation graphs suggests that the tenets of information foraging provide valuable insight into software maintenance. Our research opens the avenue towards the development of ecologically valid tool support to augment developers' code search skills.
[program comprehension, Software maintenance, information foraging, code navigation, Navigation, Profitability, Biological system modeling, Debugging, Maintenance engineering, foundation, software maintenance, NIER track, foraging theory, information seeking, software engineering, Software engineering]
Identifying method friendships to remove the feature envy bad smell: NIER track
2011 33rd International Conference on Software Engineering
None
2011
We propose a novel approach to identify Move Method refactoring opportunities and remove the Feature Envy bad smell from source code. The proposed approach analyzes both structural and conceptual relationships between methods and uses Relational Topic Models to identify sets of methods that share several responsabilities, i.e., 'friend methods'. The analysis of method friendships of a given method can be used to pinpoint the target class (envied class) where the method should be moved in. The results of a preliminary empirical evaluation indicate that the proposed approach provides meaningful refactoring opportunities.
[Measurement, source code, Educational institutions, move method refactoring opportunities, software maintenance, program compilers, method friendships identification, Couplings, source code quality, Accuracy, NIER track, refactoring, relational topic model, feature envy bad smell, Software, Reliability, Facebook, relational topic models]
The code orb: supporting contextualized coding via at-a-glance views (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
While code is typically presented as a flat file to a developer who must change it, this flat file exists within a context that can drastically influence how a developer approaches changing it. While the developer clearly must be careful changing any code, they probably should be yet more careful in changing code that recently saw major changes, is barely covered by test cases, and was the source of a number of bugs. Contextualized coding refers to the ability of the developer to effectively use such contextual information while they work on some changes. In this paper, we introduce the Code Orb, a contextualized coding tool that builds upon existing mining and analysis techniques to warn developers on a line-by-line basis of the volatility of the code they are working on. The key insight underneath the Code Orb is that it is neither desired nor possible to always present a code's context in its entirety; instead, it is necessary to provide an abstracted view of the context that informs the developer of which parts of the code they need to pay more attention to. This paper discusses the principles of and rationale behind contextualized coding, introduces the Code Orb, and illustrates its function with example code and context drawn from the Mylyn [11] project.
[Context, code orb, Visualization, data analysis, contextualized coding, mining technique, data mining, analysis technique, Encoding, Code Orb tool, History, encoding, software visualization, recommendation systems, line-by-line development, Computer bugs, Software, software engineering, code change, Software engineering]
Permission-based programming languages: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Linear permissions have been proposed as a lightweight way to specify how an object may be aliased, and whether those aliases allow mutation. Prior work has demonstrated the value of permissions for addressing many software engineering concerns, including information hiding, protocol checking, concurrency, security, and memory management. We propose the concept of a permission-based programming language - a language whose object model, type system, and runtime are all co-designed with permissions in mind. This approach supports an object model in which the structure of an object can change over time, a type system that tracks changing structure in addition to addressing the other concerns above, and a runtime system that can dynamically check permission assertions and leverage permissions to parallelize code. We sketch the design of the permission-based programming language Plaid, and argue that the approach may provide significant software engineering benefits.
[parallelize code, Protocols, Programming, Security, programming languages, formal specification, parallel programming, storage management, memory management, Runtime, security, permissions, type system, permission assertion, software engineering, protocol checking, permission-based programming languages, Java, object-oriented programming, types, information hiding, linear permission, object specification, runtime system, concurrency, security of data, concurrency control, object-oriented languages, Plaid, object model, data encapsulation]
Toward a better understanding of tool usage: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Developers use tools to develop software systems and always alleged better tools are being produced and purchased. Still there have been only limited studies on how people really use tools; these studies have used limited data, and the interactions between tools have not been properly elaborated. The advent of the AISEMA (Automated In-Process Software Engineering Measurement and Analysis) systems [3] has enabled a more detailed collection of tools data. Our "new idea" is to take advantage of such data to build a simple model based on an oriented graph that enables a good understanding on how tools are used individually and collectively. We have empirically validated the model analyzing an industrial team of 19 developers for a period of 10 months.
[tool usage, graph theory, software development management, automated in-process software engineering measurement and analysis, Switches, Programming, AISEMA systems, Browsers, History, software system development, NIER track, Software, software tools, Paints, Software engineering]
Characterizing process variation: NIER track
2011 33rd International Conference on Software Engineering
None
2011
A process model, namely a formal definition of the coordination of agents performing activities using resources and artifacts, can aid understanding of the real-world process it models. Moreover, analysis of the model can suggest improvements to the real-world process. Complex real-world processes, however, exhibit considerable amounts of variation that can be difficult or impossible to represent with a single process model. Such processes can often be modeled better, within the restrictions of a given modeling notation, by a family of models. This paper presents an approach to the formal characterization of some of these process families. A variety of needs for process variation are identified, and suggestions are made about how to meet some of these needs using different approaches. Some mappings of different needs for variability to approaches for meeting them are presented as case studies.
[software product lines, Navigation, Computational modeling, Unified modeling language, Nominations and elections, software development management, Programming, software agents, formal characterization, real-world process understanding, software process improvement, NIER Track, agents coordination, process model, Software, Robustness, process families, process variation characterization, system variation]
Blending freeform and managed information in tables: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Tables are an important tool used by business analysts engaged in early requirements activities (in fact it is safe to say that tables appeal to many other types of user, in a variety of activities and domains). Business analysts typically use the tables provided by office tools. These tables offer great flexibility, but no underlying model, and hence no consistency management, multiple views or other advantages familiar to the users of modeling tools. Modeling tools, however, are usually too rigid for business analysts. In this paper we present a flexible modeling approach to tables, which combines the advantages of both office and modeling tools. Freeform information can co-exist with information managed by an underlying model, and an incremental formalization approach allows each item of information to transition fluidly between freeform and managed. As the model evolves, it is used to guide the user in the process of formalizing any remaining freeform information. The model therefore helps users without restricting them. Early feedback is described, and the approach is analyzed briefly in terms of cognitive dimensions.
[Visualization, office tools, Computational modeling, Object oriented modeling, business analysts, incremental formalization approach, business analysis, formal specification, Analytical models, tables, NIER track, modeling tools, requirements activities, flexible modeling, Organizations, Data models, freeform information, business data processing]
Design and implementation of a data analytics infrastructure in support of crisis informatics research: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Crisis informatics is an emerging research area that studies how information and communication technology (ICT) is used in emergency response. An important branch of this area includes investigations of how members of the public make use of ICT to aid them during mass emergencies. Data collection and analytics during crisis events is a critical pre-requisite for performing such research, as the data generated during these events on social media networks are ephemeral and easily lost. We report on the current state of a crisis informatics data analytics infrastructure that we are developing in support of a broader, interdisciplinary research program. We also comment on the role that software engineering research plays in these increasingly common, highly-interdisciplinary research efforts.
[Java, software frameworks, emergency response, data analytics, information technology, Twitter, data collection, scalability, data analytics infrastructure, crisis informatics, Databases, NIER track, information and communication technology, social media networks, Software, software engineering, ICT, Reliability, informatics research crisis, Informatics, emergency services, Software engineering]
A domain specific requirements model for scientific computing: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Requirements engineering is a core activity in software engineering. However, formal requirements engineering methodologies and documented requirements are often missing in scientific computing projects. We claim that there is a need for methodologies, which capture requirements for scientific computing projects, because traditional requirements engineering methodologies are difficult to apply in this domain. We propose a novel domain specific requirements model to meet this need. We conducted an exploratory experiment to evaluate the usage of this model in scientific computing projects. The results indicate that the proposed model facilitates the communication across the domain boundary, which is between the scientific computing domain and the software engineering domain. It supports requirements elicitation for the projects efficiently.
[Computational modeling, Object oriented modeling, scientific computing, Unified modeling language, formal requirements engineering, domain specific, formal specification, requirements modeling, NIER track, scientific computing projects, domain specific requirements model, Software, software engineering, Numerical models, natural sciences computing, Mathematical model, Software engineering]
CREWW: collaborative requirements engineering with wii-remotes (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
In this paper, we present CREWW, a tool for co-located, collaborative CRC modeling and use case analysis. In CRC sessions role play is used to involve all stakeholders when determining whether the current software model completely and consistently captures the modeled use case. In this activity it quickly becomes difficult to keep track of which class is currently active or along which path the current state was reached. CREWW was designed to alleviate these and other weaknesses of the traditional approach.
[Computers, class responsibility collaboration, remote consoles, Object oriented modeling, Unified modeling language, CRC modeling, software model, Wii remotes, Educational institutions, History, Servers, use case analysis, requirements engineering, collaborative requirements engineering, Collaboration, groupware, software engineering, collaborative work]
Learning to adapt requirements specifications of evolving systems: (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
We propose a novel framework for adapting and evolving software requirements models. The framework uses model checking and machine learning techniques for verifying properties and evolving model descriptions. The paper offers two novel contributions and a preliminary evaluation and application of the ideas presented. First, the framework is capable of coping with errors in the specification process so that performance degrades gracefully. Second, the framework can also be used to re-engineer a model from examples only, when an initial model is not available. We provide a preliminary evaluation of our framework by applying it to a Pump System case study, and integrate our prototype tool with the NuSMV model checker. We show how the tool integrates verification and evolution of abstract models, and also how it is capable of re-engineering partial models given examples from an existing system.
[Knowledge engineering, software requirement model, Adaptation models, Computational modeling, NuSMV model checker, adaptation, machine learning, formal specification, machine learning in software engineering, evolving system, pump system, requirement specification process, formal verification, model checking, NIER track, Machine learning, Software, Robustness, Numerical models]
Towards overcoming human analyst fallibility in the requirements tracing process: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Our research group recently discovered that human analysts, when asked to validate candidate traceability matrices, produce predictably imperfect results, in some cases less accurate than the starting candidate matrices. This discovery radically changes our understanding of how to design a fast, accurate and certifiable tracing process that can be implemented as part of software assurance activities. We present our vision for the new approach to achieving this goal. Further, we posit that human fallibility may impact other software engineering activities involving decision support tools.
[Industries, tracing, program diagnostics, Communities, Humans, software assurance activity, accuracy, traceability, formal specification, software assurance, tools, software engineering activity, human analyst fallibility, certifiable tracing process, Accuracy, decision support tool, formal verification, USA Councils, NIER track, requirement tracing process, systems analysis, Software, validation, Software engineering]
Positive effects of utilizing relationships between inconsistencies for more effective inconsistency resolution: NIER track
2011 33rd International Conference on Software Engineering
None
2011
State-of-the-art modeling tools can help detect inconsistencies in software models. Some can even generate fixing actions for these inconsistencies. However such approaches handle inconsistencies individually, assuming that each single inconsistency is a manifestation of an individual defect. We believe that inconsistencies are merely expressions of defects. That is, inconsistencies highlight situations under which defects are observable. However, a single defect in a software model may result in many inconsistencies and a single inconsistency may be the result of multiple defects. Inconsistencies may thus be related to other inconsistencies and we believe that during fixing, one should consider clusters of such related inconsistencies. This paper provides first evidence and emerging results that several inconsistencies can be linked to a single defect and show that with such knowledge only a subset of fixes need to be considered during inconsistency resolution.
[program verification, Unified modeling language, Maintenance engineering, Educational institutions, inconsistencies, Analytical models, relationship utilization, USA Councils, user guidance, software models, state-of-the-art modeling tools, Software, clustering, fixing action generation, effective inconsistency resolution, Bars]
Matching logic: a new program verification approach (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
Matching logic is a new program verification logic, which builds upon operational semantics. Matching logic specifications are constrained symbolic program configurations, called patterns, which can be matched by concrete configurations. By building upon an operational semantics of the language and allowing specifications to directly refer to the structure of the configuration, matching logic has at least three benefits: (1) One's familiarity with the formalism reduces to one's familiarity with the operational semantics of the language, that is, with the language itself; (2) The verification process proceeds the same way as the program execution, making debugging failed proof attempts manageable because one can always see the "current configuration" and "what went wrong', same like in a debugger; and (3) Nothing is lost in translation, that is, there is no gap between the language itself and its verifier. Moreover, direct access to the structure of the configuration facilitates defining subpatterns that one may reason about, such as disjoint lists or trees in the heap, as well as supporting framing in various components of the configuration at no additional costs.
[program debugging, program verification, rewriting logic, operational semantics, constrained symbolic program configuration, Programming, Cognition, programming language semantics, formal specification, maude, formal logic, program verification logic, Computer languages, matching logic, Semantics, Syntactics, Concrete, Pattern matching, matching logic specification]
Model-based performance testing: NIER track
2011 33rd International Conference on Software Engineering
None
2011
In this paper, we present a method for performance testing of transactional systems. The methods models the system under test, finds the software and hardware bottlenecks and generate the workloads that saturate them. The framework is adaptive, the model and workloads are determined during the performance test execution by measuring the system performance, fitting a performance model and by analytically computing the number and mix of users that will saturate the bottlenecks. We model the software system using a two layers queuing model and use analytical techniques to find the workload mixes that change the bottlenecks in the system. Those workload mixes become stress vectors and initial starting points for the stress test cases. The rest of test cases are generated based on a feedback loop that drives the software system towards the worst case behaviour.
[Adaptation models, program control structures, queueing theory, queuing model, program testing, Computational modeling, performance testing, system under test, stress testing, performance models, Stress, feedback loop, adaptive system, model-based performance testing NIER track, transactional systems, performance test execution, stress test cases, Software, Hardware, Time factors, stress vectors, Testing]
Tuple density: a new metric for combinatorial test suites (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
We propose tuple density to be a new metric for combinatorial test suites. It can be used to distinguish one test suite from another even if they have the same size and strength. Moreover, it is also illustrated how a given test suite can be optimized based on this metric. The initial experimental results are encouraging.
[Measurement, combinatorial test suites, combinatorial mathematics, program testing, Optimization, test suites, Computer science, combinatorial testing, NIER track, Software, tuple density, metrics, Arrays, Testing, Software engineering, software metrics]
Search-enhanced testing: NIER track
2011 33rd International Conference on Software Engineering
None
2011
The prime obstacle to automated defect testing has always been the generation of "correct" results against which to judge the behavior of the system under test - the "oracle problem". So called "back-to-back" testing techniques that exploit the availability of multiple versions of a system to solve the oracle problem have mainly been restricted to very special, safety critical domains such as military and space applications since it is so expensive to manually develop the additional versions. However, a new generation of software search engines that can find multiple copies of software components at virtually zero cost promise to change this situation. They make it economically feasible to use the knowledge locked in reusable software components to dramatically improve the efficiency of the software testing process. In this paper we outline the basic ingredients of such an approach.
[search engines, program testing, back-to-back testing technique, Manuals, Programming, automated testing, search-enhanced testing, Indexes, oracle problem, software search engines, software testing process, multi-version programming, software component, search-driven development, back-to-back testing, Search engines, Software, Testing, Software engineering]
Fuzzy set-based automatic bug triaging: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Assigning a bug to the right developer is a key in reducing the cost, time, and efforts for developers in a bug fixing process. This assignment process is often referred to as bug triaging. In this paper, we propose Bugzie, a novel approach for automatic bug triaging based on fuzzy set-based modeling of bug-fixing expertise of developers. Bugzie considers a system to have multiple technical aspects, each is associated with technical terms. Then, it uses a fuzzy set to represent the developers who are capable/competent of fixing the bugs relevant to each term. The membership function of a developer in a fuzzy set is calculated via the terms extracted from the bug reports that (s)he has fixed, and the function is updated as new fixed reports are available. For a new bug report, its terms are extracted and corresponding fuzzy sets are union'ed. Potential fixers will be recommended based on their membership scores in the union'ed fuzzy set. Our preliminary results show that Bugzie achieves higher accuracy and efficiency than other state-of-the-art approaches.
[program debugging, fuzzy set, Bugzie, fuzzy set theory, Training, Support vector machines, Fuzzy sets, fuzzy set-based automatic bug triaging, Accuracy, Bayesian methods, NIER track, Computer bugs, bug fixing process, bug triaging, Software]
Exploiting hardware advances for software testing and debugging: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Despite the emerging ubiquity of hardware monitoring mechanisms and prior research work in other fields, the applicability and usefulness of hardware monitoring mechanisms have not been fully scrutinized for software engineering. In this work, we identify several recently developed hardware mechanisms that lend themselves well to structural test overage analysis and automated fault localization and explore their potential. We discuss key factors impacting the applicability of hardware monitoring mechanism for these software engineering tasks, present novel online analyses leveraging these mechanisms, and provide preliminary results demonstrating the promise of this emerging hardware.
[program debugging, software debugging, program testing, hardware monitoring mechanisms, Instruments, software testing, performance monitoring, fault localization, hardware advance exploitation, Program processors, branch testing, Hardware, software engineering, structural test overage analysis, Kernel, Monitoring, Testing]
Better testing through oracle selection: (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
In software testing, the test oracle determines if the application under test has performed an execution correctly. In current testing practice and research, significant effort and thought is placed on selecting test inputs, with the selection of test oracles largely neglected. Here, we argue that improvements to the testing process can be made by considering the problem of oracle selection. In particular, we argue that selecting the test oracle and test inputs together to complement one another may yield improvements testing effectiveness. We illustrate this using an example and present selected results from an ongoing study demonstrating the relationship between test suite selection, oracle selection, and fault finding.
[Software testing, Measurement, program testing, software selection, test oracles, Aerospace electronics, software testing process, software fault tolerance, empirical studies, NIER track, fault finding, Oracle selection testing, Joints, Monitoring, Software engineering]
Tracking data structures for postmortem analysis: (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
Analyzing the runtime behaviors of the data structures is important because they usually relate to the obscured program performance and understanding issues. The runtime evolution history of data structures creates the possibility of building a lightweight and non-checkpointing based solution for the backward analysis for validating and mining both the temporal and stationary properties of the data structure. We design and implement TAEDS, a framework that focuses on gathering the data evolution history of a program at the runtime and provides a virtual machine for programmers to examine the behavior of data structures back in time. We show that our approach facilitates many programming tasks such as diagnosing memory problems and improving the design of the data structures themselves.
[tracing, Instruments, Debugging, data structure, data evolution, History, postmortem analysis, backward analysis, Runtime, virtual machine, NIER track, virtual machines, program analysis, temporal properties, TAEDS, debugging, data structures, stationary properties, Arrays, Monitoring]
Iterative context-aware feature location: (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
Locating the program element(s) relevant to a particular feature is an important step in efficient maintenance of a software system. The existing feature location techniques analyze each feature independently and perform a one-time analysis after being provided an initial input. As a result, these techniques are sensitive to the quality of the input, and they tend to miss the nonlocal interactions among features. In this paper, we propose to address the proceeding two issues in feature location using an iterative context-aware approach. The underlying intuition is that the features are not independent of each other, and the structure of source code resembles the structure of features. The distinguishing characteristics of the proposed approach are: 1) it takes into account the structural similarity between a feature and a program element to determine their relevance; 2) it employs an iterative process to propagate the relevance of the established mappings between a feature and a program element to the neighboring features and program elements. Our initial evaluation suggests the proposed approach is more robust and can significantly increase the recall of feature location with a slight decrease in precision.
[iterative methods, program diagnostics, information retrieval, source code, structural similarity, feature location, software maintenance, ubiquitous computing, iterative context aware feature location, software system maintenance, feature extraction, program element location, one-time analysis]
A study of ripple effects in software ecosystems: (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
When the Application Programming Interface (API) of a framework or library changes, its clients must be adapted. This change propagation - known as a ripple effect - is a problem that has garnered interest: several approaches have been proposed in the literature to react to these changes. Although studies of ripple effects exist at the single system level, no study has been performed on the actual extent and impact of these API changes in practice, on an entire software ecosystem associated with a community of developers. This paper reports on early results of such an empirical study of API changes that led to ripple effects across an entire ecosystem. Our case study subject is the development community gravitating aroung the Squeak and Pharo software ecosystems: six years of evolution, nearly 3,000 contributors, and close to 2,500 distinct systems.
[Adaptation models, Squeak software ecosystem, application program interfaces, Pharo software ecosystem, Ecosystems, Communities, application programming interface, mining software repositories, software ecosystems, ripple effects, Data mining, empirical studies, NIER track, Libraries, Kernel]
Tracing architectural concerns in high assurance systems: (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
Software architecture is shaped by a diverse set of interacting and competing quality concerns, each of which may have broad-reaching impacts across multiple architectural views. Without traceability support, it is easy for developers to inadvertently change critical architectural elements during ongoing system maintenance and evolution, leading to architectural erosion. Unfortunately, existing traceability practices, tend to result in the proliferation of traceability links, which can be difficult to create, maintain, and understand. We therefore present a decision-centric approach that focuses traceability links around the architectural decisions that have shaped the delivered system. Our approach, which is informed through an extensive investigation of architectural decisions made in real-world safety-critical and performance-critical applications, provides enhanced support for advanced software engineering tasks.
[architectural concern tracing, Redundancy, software traceability, traceability links, design rationale, Aerospace electronics, Maintenance engineering, architectural preservation, high assurance systems, software architecture, system maintenance, Software architecture, interacting quality concerns, Software, software engineering tasks, system evolution, decision-centric approach, Discrete cosine transforms, competing quality concerns, architecturally significant requirements]
A combination approach for enhancing automated traceability: (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
Tracking a variety of traceability links between artifacts assists software developers in comprehension, efficient development, and effective management of a system. Traceability systems to date based on various Information Retrieval (IR) techniques have been faced with a major open research challenge: how to extract these links with both high precision and high recall. In this paper we describe an experimental approach that combines Regular Expression, Key Phrases, and Clustering with IR techniques to enhance the performance of IR for traceability link recovery between documents and source code. Our preliminary experimental results show that our combination technique improves the performance of IR, increases the precision of retrieved links, and recovers more true links than IR alone.
[program diagnostics, Unified modeling language, document clustering, Documentation, information retrieval, Information retrieval, Large scale integration, Thesauri, regular expression, traceability, key phrases, traceability link recovery, Clustering algorithms, automated traceability system, Software, software engineering, clustering]
Capturing tacit architectural knowledge using the repertory grid technique (NIER track): (nier track)
2011 33rd International Conference on Software Engineering
None
2011
Knowledge about the architecture of a software-intensive system tends to vaporize easily. This leads to increased maintenance costs. We explore a new idea: utilizing the repertory grid technique to capture tacit architectural knowledge. Particularly, we investigate the elicitation of design decision alternatives and their characteristics. To study the applicability of this idea, we performed an exploratory study. Seven independent subjects applied the repertory grid technique to document a design decision they had to take in previous projects. Then, we interviewed each subject to understand their perception about the technique. We identified advantages and disadvantages of using the technique. The main advantage is the reasoning support it provides; the main disadvantage is the additional effort it requires. Also, applying the technique depends on the context of the project. Using the repertory grid technique is a promising approach for fighting architectural knowledge vaporization.
[Knowledge engineering, document handling, tacit architectural knowledge, Psychology, design decision alternatives, Educational institutions, Encoding, software maintenance, repertory grid, software intensive system architecture, design decision documentation, software architecture, Software architecture, tacit architecture knowledge, Computer architecture, maintenance costs, repertory grid technique, architectural knowledge vaporization]
Flexible generators for software reuse and evolution: NIER Track
2011 33rd International Conference on Software Engineering
None
2011
Developers tend to use models and generators during initial development, but often abandon them later in software evolution and reuse. One reason for that is that code generated from models (e.g., UML) is often manually modified, and changes cannot be easily propagated back to models. Once models become out of sync with code, any future re-generation of code overrides manual modifications. We propose a flexible generator solution that alleviates the above problem. The idea is to let developers weave arbitrary manual modifications into the generation process, rather than modify already generated code. A flexible generator stores specifications of manual modifications in executable form, so that weaving can be automatically re-done any time code is regenerated from modified models. In that way, models and manual modification can evolve independently but in sync with each other, and the generated code never gets directly changed. As a proof of concept, we have already built a flexible generator prototype by a merger of conventional generation system and variability technique to handle manual modifications. We believe a flexible generator approach alleviates an important problem that hinders wide spread adoption of MDD in software practice.
[Productivity, software product lines, domain-specific languages, Unified Modeling Language, software reuse, Unified modeling language, Manuals, Programming, Generators, software evolution, flexible generators, code generation, NIER track, UML, software reusability, generators, Software, DSL]
The lazy initialization multilayered modeling framework: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Lazy Initialization Multilayer Modeling (LIMM) is an object oriented modeling language targeted to the declarative definition of Domain Specific Languages (DSLs) for Model Driven Engineering. It focuses on the precise definition of modeling frameworks spanning over multiple layers. In particular, it follows a two dimensional architecture instead of the linear architecture followed by many other modeling frameworks. The novelty of our approach is to use lazy initialization for the definition of mapping between different modeling abstractions, within and across multiple layers, hence providing the basis for exploiting the potential of metamodeling.
[Object oriented modeling, Unified modeling language, lazy initialization multilayered modeling framework, Metamodeling, model driven engineering, Nonhomogeneous media, limm, 2D architecture, strict metamodeling, software architecture, metamodeling, NIER track, Semantics, specification languages, object-oriented languages, domain specific languages, Data models, object oriented modeling language, linear architecture, DSL, instantiation]
Towards architectural information in implementation: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Agile development methods favor speed and feature producing iterations. Software architecture, on the other hand, is ripe with techniques that are slow and not oriented directly towards implementation of costumers' needs. Thus, there is a major challenge in retaining architectural information in a fast-faced agile project. We propose to embed as much architectural information as possible in the central artefact of the agile universe, the code. We argue that thereby valuable architectural information is retained for (automatic) documentation, validation, and further analysis, based on a relatively small investment of effort. We outline some preliminary examples of architectural annotations in Java and Python and their applicability in practice.
[architectural information, Java, architectural annotations, program verification, costumer needs, software prototyping, Unified modeling language, system documentation, Documentation, Programming, agile methods, software architecture, automatic documentation, Software architecture, NIER track, software validation, Computer architecture, agile development methods, fast-faced agile project, Software, architectural reconstruction, Python]
Topic-based defect prediction: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Defects are unavoidable in software development and fixing them is costly and resource-intensive. To build defect prediction models, researchers have investigated a number of factors related to the defect-proneness of source code, such as code complexity, change complexity, or socio-technical factors. In this paper, we propose a new approach that emphasizes on technical concerns/functionality of a system. In our approach, a software system is viewed as a collection of software artifacts that describe different technical concerns/-aspects. Those concerns are assumed to have different levels of defect-proneness, thus, cause different levels of defectproneness to the relevant software artifacts. We use topic modeling to measure the concerns in source code, and use them as the input for machine learning-based defect prediction models. Preliminary result on Eclipse JDT shows that the topic-based metrics have high correlation to the number of bugs (defect-proneness), and our topic-based defect prediction has better predictive performance than existing state-of-the-art approaches.
[Measurement, topic-based metrics, Correlation, software reliability, Predictive models, topic modeling, Complexity theory, communication complexity, code complexity, software artifacts, Semantics, defect prediction, learning (artificial intelligence), machine learning-based defect prediction model, source coding, software development, program diagnostics, socio-technical factor, source code defect-proneness, change complexity, topic-based defect prediction model, Computer bugs, NIER Track, Software, Eclipse JDT, software cost estimation, software metrics]
Automated usability evaluation of parallel programming constructs: nier track
2011 33rd International Conference on Software Engineering
None
2011
Multicore computers are ubiquitous, and proposals to extend existing languages with parallel constructs mushroom. While everyone claims to make parallel programming easier and less error-prone, empirical language usability evaluations are rarely done in-the-field with many users and real programs. Key obstacles are costs and a lack of appropriate environments to gather enough data for representative conclusions. This paper discusses the idea of automating the usability evaluation of parallel language constructs by gathering subjective and objective data directly in every software engineer's IDE. The paper presents an Eclipse prototype suite that can aggregate such data from potentially hundreds of thousands of programmers. Mismatch detection in subjective and objective feedback as well as construct usage mining can improve language design at an early stage, thus reducing the risk of developing and maintaining inappropriate constructs. New research directions arising from this idea are outlined for software repository mining, debugging, and software economics.
[Parallel languages, program debugging, software debugging, subjective feedback, data mining, multicore computers, automated usability evaluation, IDE, objective feedback, Data mining, Servers, ubiquitous computing, software economics, ususability, parallel programming, Prototypes, empirical software engineering, parallel languages, tools and environments, multiprocessing systems, mismatch detection, software repository mining, Eclipse prototype suite, parallel language constructs, construct usage mining, Usability]
Data analytics for game development: NIER track
2011 33rd International Conference on Software Engineering
None
2011
The software engineering community has had seminal papers on data analysis for software productivity, quality, reliability, performance etc. Analyses have involved software systems ranging from desktop software to telecommunication switching systems. Little work has been done on the emerging digital game industry. In this paper we explore how data can drive game design and production decisions in game development. We define a mixture of qualitative and quantitative data sources, broken down into three broad categories: internal testing, external testing, and subjective evaluations. We present preliminary results of a case study of how data collected from users of a released game can inform subsequent development.
[Industries, data analysis, Communities, software reliability, desktop software, digital game industry, software quality, game production decision, subjective evaluation, game design, Vehicles, game development, internal testing, game metrics, software productivity, USA Councils, computer games, Games, software engineering, external testing, Usability, software performance, telecommunication switching system, Testing]
Mining service abstractions: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Several lines of research rely on the concept of service abstractions to enable the organization, the composition and the adaptation of services. However, what is still missing, is a systematic approach for extracting service abstractions out of the vast amount of services that are available all over the Web. To deal with this issue, we propose an approach for mining service abstractions, based on an agglomerative clustering algorithm. Our experimental findings suggest that the approach is promising and can serve as a basis for future research.
[Algorithm design and analysis, agglomerative clustering, service abstraction mining, abstraction recovery, data mining, services, Data mining, agglomerative clustering algorithm, Web services, pattern clustering, NIER track, Clustering algorithms, XML, Internet, service-oriented architecture]
A software behaviour analysis framework based on the human perception systems: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Understanding software behaviour can help in a variety of software engineering tasks if one can develop effective techniques for analyzing the information generated from a system's run. These techniques often rely on tracing. Traces, however, can be considerably large and complex to process. In this paper, we present an innovative approach for trace analysis inspired by the way the human brain and perception systems operate. The idea is to mimic the psychological processes that have been developed over the years to explain how our perception system deals with huge volume of visual data. We show how similar mechanisms can be applied to the abstraction and simplification of large traces. Some preliminary results are also presented.
[Visualization, Unified modeling language, Humans, Psychology, trace analysis, visual databases, human perception, human brain, psychology, NIER track, software engineering, psychological processes, visual data, program comprehension, data analysis, visual perception, software behaviour analysis, human perception system, dynamic analysis, brain, Phase detection, Software, phase detection, psychological process, Software engineering]
Dynamic shape analysis of program heap using graph spectra: nier track
2011 33rd International Conference on Software Engineering
None
2011
Programs written in languages such as Java and C# maintain most of their state on the heap. The size and complexity of these programs pose a challenge in understanding and maintaining them; Heap analysis by summarizing the state of heap graphs assists programmers in these tasks. In this paper we present a novel dynamic heap analysis technique that uses spectra of the heap graphs to summarize them. These summaries capture the shape of recursive data structures as dynamic invariants or likely properties of these structures that must be preserved after any destructive update. Initial experiments show that this approach can generate meaningful summaries for a range of subject structures.
[C#, deryaft, Java, Symmetric matrices, Laplace equations, Shape, program diagnostics, Data structures, shape analysis, Matrix decomposition, C language, software maintenance, graph spectra, dynamic shape analysis, program heap, program understanding, program maintainance, structural invariant generation, Eigenvalues and eigenfunctions]
Program analysis: from qualitative analysis to quantitative analysis (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
We propose to combine symbolic execution with volume computation to compute the exact execution frequency of program paths and branches. Given a path, we use symbolic execution to obtain the path condition which is a set of constraints; then we use volume computation to obtain the size of the solution space for the constraints. With such a methodology and supporting tools, we can decide which paths in a program are executed more often than the others. We can also generate certain test cases that are related to the execution frequency, e.g., those covering cold paths.
[program diagnostics, Computer science, Semantics, volume computation, program analysis, qualitative analysis, Syntactics, symbolic execution, Software, Concrete, Numerical models, quantitative analysis, execution probability, Testing]
Diagnosing new faults using mutants and prior faults (NIER track)
2011 33rd International Conference on Software Engineering
None
2011
Literature indicates that 20% of a program's code is responsible for 80% of the faults, and 50-90% of the field failures are rediscoveries of previous faults. Despite this, identification of faulty code can consume 30-40% time of error correction. Previous fault-discovery techniques focusing on field failures either require many pass-fail traces, discover only crashing failures, or identify faulty "files" (which are of large granularity) as origin of the source code. In our earlier work (the F007 approach), we identify faulty "functions" (which are of small granularity) in a field trace by using earlier resolved traces of the same release, which limits it to the known faulty functions. This paper overcomes this limitation by proposing a new "strategy" to identify new and old faulty functions using F007. This strategy uses failed traces of mutants (artificial faults) and failed traces of prior releases to identify faulty functions in the traces of succeeding release. Our results on two UNIX utilities (i.e., Flex and Gzip) show that faulty functions in the traces of the majority (60-85%) of failures of a new software release can be identified by reviewing only 20% of the code. If compared against prior techniques then this is a notable improvement in terms of contextual knowledge required and accuracy in the discovery of finer-grain fault origin.
[Measurement, Unix, faulty code, Flexible printed circuits, fault diagnosis, program diagnostics, crashing failures, mutants, source code, execution traces, program code, Fault diagnosis, decision tree, USA Councils, NIER track, UNIX utilities, Software, Decision trees, Testing, faulty function]
Empirical results on the study of software vulnerabilities: NIER track
2011 33rd International Conference on Software Engineering
None
2011
While the software development community has put a significant effort to capture the artifacts related to a discovered vulnerability in organized repositories, much of this information is not amenable to meaningful analysis and requires a deep and manual inspection. In the software assurance community a body of knowledge that provides an enumeration of common weaknesses has been developed, but it is not readily usable for the study of vulnerabilities in specific projects and user environments. We propose organizing the information in project repositories around semantic templates. In this paper, we present preliminary results of an experiment conducted to evaluate the effectiveness of using semantic templates as an aid to studying software vulnerabilities.
[software development, buffer overflow, Taxonomy, Programming, Security, repository, software assurance, semantic templates, software vulnerabilities, Accuracy, NIER track, experiment, Semantics, manual inspection, Software, software engineering, Buffer overflow, software vulnerability]
Multifractal aspects of software development: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Software development is difficult to model, particularly the noisy, non-stationary signals of changes per time unit, extracted from version control systems (VCSs). Currently researchers are utilizing timeseries analysis tools such as ARIMA to model these signals extracted from a project's VCS. Unfortunately current approaches are not very amenable to the underlying power-law distributions of this kind of signal. We propose modeling changes per time unit using multifractal analysis. This analysis can be used when a signal exhibits multi-scale self-similarity, as in the case of complex data drawn from power-law distributions. Specifically we utilize multifractal analysis to demonstrate that software development is multifractal, that is the signal is a fractal composed of multiple fractal dimensions along a range of Hurst exponents. Thus we show that software development has multi-scale self-similarity, that software development is multifractal. We also pose questions that we hope multifractal analysis can answer.
[Heart, VCS, timeseries analysis tools, Programming, Wavelet analysis, Fractals, multifractal analysis, fractals, nonstationary signals, multiscale self-similarity, NIER track, multifractal, power-law, software engineering, multiple fractal dimensions, version control systems, Wavelet transforms, software development, version control, multifractal aspects, time series, ARIMA, wavelets, fractal, Software, underlying power-law distributions, Hurst exponents, Software engineering]
The American law institute's principles on software contracts and their ramifications for software engineering research: NIER track
2011 33rd International Conference on Software Engineering
None
2011
The American Law Institute has recently published principles of software contracts that may have profound impact on changing the software industry. One of the principles implies a non-disclaimable liability of software vendors for any hidden material defects. In this paper, we describe the new principle, first from a legal and then from a software engineering point of view. We point out potential ramifications and research directions for the software engineering community.
[Warranties, software vendors, software contracts, american law institute, hidden defects, software engineering research, information technology law, contract law, Materials, contracts, legal interpretation, Certification, software industry, American law, software houses, ramifications, Software, software engineering, Contracts, Software engineering, Testing]
Toward sustainable software engineering: NIER track
2011 33rd International Conference on Software Engineering
None
2011
Current software engineering practices have significant effects on the environment. Examples include e-waste from computers made obsolete due to software upgrades, and changes in the power demands of new versions of software. Sustainable software engineering aims to create reliable, long-lasting software that meets the needs of users while reducing environmental impacts. We conducted three related research efforts to explore this area. First, we investigated the extent to which users thought about the environmental impact of their software usage. Second, we created a tool called GreenTracker, which measures the energy consumption of software in order to raise awareness about the environmental impact of software usage. Finally, we explored the indirect environmental effects of software in order to understand how software affects sustainability beyond its own power consumption. The relationship between environmental sustainability and software engineering is complex; understanding both direct and indirect effects is critical to helping humans live more sustainably.
[Computers, software usage, Energy consumption, sustainable software engineering, software, environmental factors, GreenTracker, environmental impacts, sustainability, power consumption, sustainable development, power aware computing, environmental sustainability, green it, Green products, Air pollution, Software systems, software engineering, energy consumption, Software engineering]
MT-Scribe: an end-user approach to automate software model evolution
2011 33rd International Conference on Software Engineering
None
2011
Model evolution is an essential activity in software system modeling, which is traditionally supported by manual editing or writing model transformation rules. However, the current state of practice for model evolution presents challenges to those who are unfamiliar with model transformation languages or metamodel definitions. This demonstration presents a demonstration-based approach that assists end-users through automation of model evolution tasks (e.g., refactoring, model scaling, and aspect weaving).
[Adaptation models, Computational modeling, software system modeling, MT-scribe, Programming, software model evolution, software maintenance, demonstration-based approach, model evolution, metamodel definitions, Layout, Syntactics, mt-scribe, Software, Weaving, demonstration, model transformation rules, model transformation languages, end-user approach]
Inconsistent path detection for XML IDEs
2011 33rd International Conference on Software Engineering
None
2011
We present the first IDE augmented with static detection of inconsistent paths for simplifying the development and debugging of any application involving XPath expressions.
[Context, Java, XPath expression, unsatisfiable, Navigation, static, Debugging, inconsistent path static detection, XML IDE, error-checking, ide, xpath, XML, User interfaces, Syntactics]
Automated security hardening for evolving UML models
2011 33rd International Conference on Software Engineering
None
2011
Developing security-critical software correctly and securely is difficult. To address this problem, there has been a significant amount of work over the last 10 years on providing model-based development approaches based on the Unified Modeling Language which aim to raise the trustworthiness of security-critical systems, some of them including tools allowing the user to check whether a UML model satisfies the relevant security requirements. However, when the requirements are not satisfied by a given model, it can be challenging for the user to determine which changes to do to the model so that it will indeed satisfy the security requirements. Also, the fact that software continues to evolve on an ongoing basis, even after the implementation has been shipped to the customer, increases the challenge since in principle, the software has to be re-verified after each modification, requiring significant efforts. We present work on automated tool-support that exploits recent work on secure software evolution in the Secure Change project in order to support the security hardening of evolving UML models (within the context of the UML security extension UMLsec).
[Context, Unified Modeling Language, Computational modeling, umlsec, Unified modeling language, secure change project, safety-critical software, UMLsec security extension, Security, software evolution, Analytical models, evolving UML model, security of data, security-critical software, security requirement, model-based development, Software, Context modeling, security hardening]
JavAdaptor: unrestricted dynamic software updates for Java
2011 33rd International Conference on Software Engineering
None
2011
Dynamic software updates (DSU) are one of the top-most features requested by developers and users. As a result, DSU is already standard in many dynamic programming languages. But, it is not standard in statically typed languages such as Java. Even if at place number three of Oracle's current request for enhancement (RFE) list, DSU support in Java is very limited. Therefore, over the years many different DSU approaches for Java have been proposed. Nevertheless, DSU for Java is still an active field of research, because most of the existing approaches are too restrictive. Some of the approaches have shortcomings either in terms of flexibility or performance, whereas others are platform dependent or dictate the program's architecture. With JavAdaptor, we present the first DSU approach which comes without those restrictions. We will demonstrate JavAdaptor based on the well-known arcade game Snake which we will update stepwise at runtime.
[Java, arcade game, Snake, Programming, Educational institutions, software maintenance, statically typed languages, tool support, dynamic programming languages, request for enhancement, Runtime, Computer architecture, Games, Software, software engineering, Oracle, JavAdaptor, unrestricted dynamic software updates, dynamic software updates]
DyTa: dynamic symbolic execution guided with static verification results
2011 33rd International Conference on Software Engineering
None
2011
Software-defect detection is an increasingly important research topic in software engineering. To detect defects in a program, static verification and dynamic test generation are two important proposed techniques. However, both of these techniques face their respective issues. Static verification produces false positives, and on the other hand, dynamic test generation is often time consuming. To address the limitations of static verification and dynamic test generation, we present an automated defect-detection tool, called DyTa, that combines both static verification and dynamic test generation. DyTa consists of a static phase and a dynamic phase. The static phase detects potential defects with a static checker; the dynamic phase generates test inputs through dynamic symbolic execution to confirm these potential defects. DyTa reduces the number of false positives compared to static verification and performs more efficiently compared to dynamic test generation.
[automatic test software, dynamic test generation, program debugging, DyTa, program testing, program verification, Instruments, program diagnostics, testing, static checker, static analysis, static verification, Computer crashes, software-defect detection, defect detection, dynamic symbolic execution, software engineering, Space exploration, automated defect-detection tool, Contracts, IEEE Potentials, Testing, Software engineering]
Identifying opaque behavioural changes
2011 33rd International Conference on Software Engineering
None
2011
Developers modify their systems by changing source code, updating test suites, and altering their system's execution context. When they make these modifications, they have an understanding of the behavioural changes they expect to happen when the system is executed; when the system does not conform to their expectations, developers try to ensure their modification did not introduce some unexpected or undesirable behavioural change. We present an approach that integrates with existing continuous integration systems to help developers identify situations whereby their changes may have introduced unexpected behavioural consequences. In this research demonstration, we show how our approach can help developers identify and investigate unanticipated behavioural changes.
[program testing, opaque behavioural change identification, source code, Programming, static analysis, Educational institutions, continuous integration systems, dynamic analysis, test suites updating, unexpected behavioural change, impact analysis, Computer languages, Concrete, research demonstration, Software engineering, Periodic structures]
FireDetective: understanding ajax client/server interactions
2011 33rd International Conference on Software Engineering
None
2011
Ajax-enabled web applications are a new breed of highly interactive, highly dynamic web applications. Although Ajax allows developers to create rich web applications, Ajax applications can be difficult to comprehend and thus to maintain. FireDetective aims to facilitate the understanding of Ajax applications. It uses dynamic analysis at both the client (browser) and server side and subsequently connects both traces for further analysis.
[program comprehension, Java, Visualization, client-server systems, FireDetective, dynamic analysis, Browsers, Servers, web applications, Ajax client-server interaction, ajax, Fires, Data visualization, Ajax-enabled Web application, system monitoring, Libraries, Internet]
BQL: capturing and reusing debugging knowledge
2011 33rd International Conference on Software Engineering
None
2011
When fixing a bug, a programmer tends to search for similar bugs that have been resolved in the past. A fix for a similar bug may help him fix his bug or at least understand his bug. We designed and implemented the Bug Query Language (BQL) and its accompanying tools to help users search for similar bugs to aid debugging. This paper demonstrates the main features of the BQL infrastructure. We populated BQL with bugs collected from open-source projects and show that BQL could have helped users to fix real-world bugs.
[program debugging, open source project, public domain software, Debugging, query languages, Database languages, BQL infrastructure, Databases, reusing debugging knowledge, Computer bugs, Semantics, Search engines, bug query language, debugging knowledge, bql]
Covana: precise identification of problems in pex
2011 33rd International Conference on Software Engineering
None
2011
Achieving high structural coverage is an important goal of software testing. Instead of manually producing test inputs that achieve high structural coverage, testers or developers can employ tools built based on automated test-generation approaches, such as Pex, to automatically generate such test inputs. Although these tools can easily generate test inputs that achieve high structural coverage for simple programs, when applied on complex programs in practice, these tools face various problems, such as the problems of dealing with method calls to external libraries or generating method-call sequences to produce desired object states. Since these tools are currently not powerful enough to deal with these various problems in testing complex programs, we propose cooperative developer testing, where developers provide guidance to help tools achieve higher structural coverage. In this demo, we present Covana, a tool that precisely identifies and reports problems that prevent Pex from achieving high structural coverage. Covana identifies problems primarily by determining whether branch statements containing not-covered branches have data dependencies on problem candidates.
[Covana, Pex, program testing, cooperative developer testing, software testing, Object recognition, Engines, structural coverage, Runtime, data dependency, problem identification, Computer architecture, groupware, Libraries, automated test-generation approach, dynamic symbolic execution, structural test generation, Graphical user interfaces, Testing]
The quamoco tool chain for quality modeling and assessment
2011 33rd International Conference on Software Engineering
None
2011
Continuous quality assessment is crucial for the long-term success of evolving software. On the one hand, code analysis tools automatically supply quality indicators, but do not provide a complete overview of software quality. On the other hand, quality models define abstract characteristics that influence quality, but are not operationalized. Currently, no tool chain exists that integrates code analysis tools with quality models. To alleviate this, the Quamoco project provides a tool chain to both define and assess software quality. The tool chain consists of a quality model editor and an integration with the quality assessment toolkit ConQAT. Using the editor, we can define quality models ranging from abstract characteristics down to operationalized measures. From the quality model, a ConQAT configuration can be generated that can be used to automatically assess the quality of a software system.
[Object oriented modeling, Inspection, software quality, tool chain, quality modeling, quamoco tool chain, software evolution, code analysis tool, Analytical models, Software quality, quality assessment, Software systems, Quality assessment]
ReAssert: a tool for repairing broken unit tests
2011 33rd International Conference on Software Engineering
None
2011
Successful software systems continuously change their requirements and thus code. When this happens, some existing tests get broken because they no longer reflect the intended behavior, and thus they need to be updated. Repairing broken tests can be time-consuming and difficult. We present ReAssert, a tool that can automatically suggest repairs for broken unit tests. Examples include replacing literal values in tests, changing assertion methods, or replacing one assertion with several. Our experiments show that ReAssert can repair many common test failures and that its suggested repairs match developers' expectations.
[Productivity, program testing, Instruments, software reliability, assertion methods, unit testing, Maintenance engineering, software maintenance, Open source software, test repair, ReAssert, reassert, formal verification, testing tools, broken unit tests repair, common test failures, Software systems, Libraries, software systems requirements, Testing]
AutoBlackTest: a tool for automatic black-box testing
2011 33rd International Conference on Software Engineering
None
2011
In this paper we present AutoBlackTest, a tool for the automatic generation of test cases for interactive applications. AutoBlackTest interacts with the application though its GUI, and uses reinforcement learning techniques to understand the interaction modalities and to generate relevant testing scenarios. Early results show that the tool has the potential of automatically discovering bugs and generating useful system and regression test suites.
[Adaptation models, program testing, graphical user interfaces, test automation, AutoBlackTest tool, graphical user interface, black-box testing, Twitter, Learning, reinforcement learning technique, bug discovery, Prototypes, automatic black-box testing, regression test suite, Software, GUI, learning (artificial intelligence), q-learning, Graphical user interfaces, Testing]
Using MATCON to generate CASE tools that guide deployment of pre-packaged applications
2011 33rd International Conference on Software Engineering
None
2011
The complex process of adapting pre-packaged applications, such as Oracle or SAP, to an organization's needs is full of challenges. Although detailed, structured, and well-documented methods govern this process, the consulting team implementing the method must spend a huge amount of manual effort to make sure the guidelines of the method are followed as intended by the method author. MATCON breaks down the method content, documents, templates, and work products into reusable objects, and enables them to be cataloged and indexed so these objects can be easily found and reused on subsequent projects. By using models and meta-modeling the reusable methods, we automatically produce a CASE tool to apply these methods, thereby guiding consultants through this complex process. The resulting tool helps consultants create the method deliverables for the initial phases of large customization projects. Our MATCON output, referred to as Consultant Assistant, has shown significant savings in training costs, a 20 - 30% improvement in productivity, and positive results in large Oracle and SAP implementations.
[Computer aided software engineering, MATCON, SAP, Biological system modeling, Metamodeling, computer-aided software engineering, reusable methods, information reuse, Guidelines, CASE tools, model-driven software development, software reusability, Software, computer aided software engineering, pre-packaged applications, Oracle, packaged applications, method authoring, Business]
SEREBRO: facilitating student project team collaboration
2011 33rd International Conference on Software Engineering
None
2011
In this demonstration, we show SEREBRO, a lightweight courseware developed for student team collaboration in a software engineering class. SEREBRO couples an idea forum with software project management tools to maintain cohesive interaction between team discussion and resulting work products, such as tasking, documentation, and version control. SEREBRO has been used consecutively for two years of software engineering classes. Student input and experiments on student use in these classes has directed SERBRO to its current functionality.
[computer supported cooperative work, Electronic publishing, lightweight courseware, project management, student project, Documentation, software management, social networking, Information services, Collaboration, groupware, collaborative software tools, Software, software engineering, Internet, SEREBRO, software project management, courseware, Software engineering, team collaboration]
StakeSource2.0: using social networks of stakeholders to identify and prioritise requirements
2011 33rd International Conference on Software Engineering
None
2011
Software projects typically rely on system analysts to conduct requirements elicitation, an approach potentially costly for large projects with many stakeholders and requirements. This paper describes StakeSource2.0, a web-based tool that uses social networks and collaborative filtering, a "crowdsourcing" approach, to identify and prioritise stakeholders and their requirements.
[collaborative filtering, StakeSource2.0, Filtering, Social network services, requirement elicitation, social networks, Educational institutions, Electronic mail, software project, formal specification, Computer science, social network, formal verification, Web-based tool, Collaboration, systems analysis, groupware, social networking (online), Software, Internet, software tools, requirements elicitation]
Miler: a toolset for exploring email data
2011 33rd International Conference on Software Engineering
None
2011
Source code is the target and final outcome of software development. By focusing our research and analysis on source code only, we risk forgetting that software is the product of human efforts, where communication plays a pivotal role. One of the most used communications means are emails, which have become vital for any distributed development project. Analyzing email archives is non-trivial, due to the noisy and unstructured nature of emails, the vast amounts of information, the unstandardized storage systems, and the gap with development tools. We present Miler, a toolset that allows the exploration of this form of communication, in the context of software maintenance and evolution. With Miler we can retrieve data from mailing list repositories in different formats, model emails as first-class entities, and transparently store them in databases. Miler offers tools and support for navigating the content, manually labelling emails with discussed source code entities, automatically linking emails to source code, measuring code entities' popularity in mailing lists, exposing structured content in the unstructured content, and integrating email communication in an IDE.
[Measurement, miler toolset, toolset, first-class entities, software development, mailing list repository, electronic mail, email data exploration, content navigation, email communication, source code entity, code entity popularity measurement, Electronic mail, IDE, software maintenance, Engines, software evolution, email communication integration, Data models, unstructured data, manual email labelling, Joining processes, Kernel, distributed development project]
A demonstration of a distributed software design sketching tool
2011 33rd International Conference on Software Engineering
None
2011
Software designers frequently sketch when they design, particularly during the early phases of exploration of a design problem and its solution. In so doing, they shun formal design tools, the reason being that such tools impose conformity and precision prematurely. Sketching on the other hand is a highly fluid and flexible way of expressing oneself. In this paper, we present Calico, a sketch-based distributed software design tool that supports software designers with a variety of features that improve over the use of just pen-and-paper or a regular whiteboard, and are tailored specifically for software design. Calico is meant to be used on electronic whiteboards or tablets, and provides for rapid creation and manipulation of design content by sets of developers who can collaborate distributedly.
[Unified modeling language, software design, Human factors, distributed processing, formal design tools, creative exploration, electronic whiteboards, sketching, Software design, design content creation, distributed software design sketching tool, Collaboration, electronic whiteboard, informal drawing, Computer architecture, design, Calico, computer aided software engineering, tablets, design content manipulation, calico, Software engineering]
View infinity: a zoomable interface for feature-oriented software development
2011 33rd International Conference on Software Engineering
None
2011
Software product line engineering provides efficient means to develop variable software. To support program comprehension of software product lines (SPLs), we developed View Infinity, a tool that provides seamless and semantic zooming of different abstraction layers of an SPL. First results of a qualitative study with experienced SPL developers are promising and indicate that View Infinity is useful and intuitive to use.
[program comprehension, software product lines, Visualization, software product line engineering, view infinity, Programming, reverse engineering, variability, seamless zooming, abstraction layers, Histograms, feature-oriented software development, Image color analysis, Semantics, product development, software reusability, Software, semantic zooming, zoomable interface, Software engineering]
CodeTopics: which topic am I coding now?
2011 33rd International Conference on Software Engineering
None
2011
Recent studies indicated that showing the similarity between the source code being developed and related high-level artifacts (HLAs), such as requirements, helps developers improve the quality of source code identifiers. In this paper, we present CodeTopics, an Eclipse plug-in that in addition to showing the similarity between source code and HLAs also highlights to what extent the code under development covers topics described in HLAs. Such views complement information derived by showing only the similarity between source code and HLAs helping (i) developers to identify functionality that are not implemented yet or (ii) newcomers to comprehend source code artifacts by showing them the topics that these artifacts relate to.
[program comprehension, Java, Object oriented modeling, source code identifier quality, CodeTopics, Eclipse plug-in, Educational institutions, Information retrieval, reverse engineering, software quality, traceability, high-level artifacts, Games, Software, Monopoly, software tools, source code lexicon, source code artifacts comprehension]
JDeodorant: identification and application of extract class refactorings
2011 33rd International Conference on Software Engineering
None
2011
Evolutionary changes in object-oriented systems can result in large, complex classes, known as "God Classes". In this paper, we present a tool, developed as part of the JDeodorant Eclipse plugin, that can recognize opportunities for extracting cohesive classes from "God Classes" and automatically apply the refactoring chosen by the developer.
[Measurement, object-oriented programming, God classes, Educational institutions, object-oriented systems, software maintenance, class refactoring extraction, software reengineering, Couplings, JDeodorant Eclipse plugin, refactoring, Clustering algorithms, Syntactics, Software, clustering, object-oriented methods, Informatics]
Evolve: tool support for architecture evolution
2011 33rd International Conference on Software Engineering
None
2011
Incremental change is intrinsic to both the initial development and subsequent evolution of large complex software systems. Evolve is a graphical design tool that captures this incremental change in the definition of software architecture. It supports a principled and manageable way of dealing with unplanned change and extension. In addition, Evolve supports decentralized evolution in which software is extended and evolved by multiple independent developers. Evolve supports a model-driven approach in that architecture definition is used to directly construct both initial implementations and extensions to these implementations. The tool implements Backbone - an architectural description language (ADL), which has both a textual and a UML2, based graphical representation. The demonstration focuses on the graphical representation.
[Runtime environment, graphical design tool, Unified Modeling Language, incremental software change, Backbone language, architectural description language, complex software system evolution, Servers, visual languages, model driven approach, software architecture, systems re-engineering, Databases, Computer architecture, Organizations, textual representation, Software, software tools, Evolve tool support, UML2 based graphical representation, decentralized evolution, software architecture evolution]
Portfolio: a search engine for finding functions and their usages
2011 33rd International Conference on Software Engineering
None
2011
In this demonstration, we present a code search system called Portfolio that retrieves and visualizes relevant functions and their usages. We will show how chains of relevant functions and their usages can be visualized to users in response to their queries.
[Context, Java, search engines, Navigation, source coding, functions, search engine, relevant functions visualization, information retrieval, Data mining, code search system, queries, query processing, relevant functions retrieval, portfolio, Search engines, Software, source code search engines, Portfolios]
Impact of process simulation on software practice: an initial report
2011 33rd International Conference on Software Engineering
None
2011
Process simulation has become a powerful technology in support of software project management and process improvement over the past decades. This research, inspired by the Impact Project, intends to investigate the technology transfer of software process simulation to the use in industrial settings, and further identify the best practices to release its full potential in software practice. We collected the reported applications of process simulation in software industry, and identified its wide adoption in the organizations delivering various software intensive systems. This paper, as an initial report of the research, briefs a historical perspective of the impact upon practice based on the documented evidence, and also elaborates the research-practice transition by examining one detailed case study. It is shown that research has a significant impact on practice in this area. The analysis of impact trace also reveals that the success of software process simulation in practice highly relies on the association with other software process techniques or practices and the close collaboration between researchers and practitioners.
[Industries, Impact Project, project management, Computational modeling, DP industry, software intensive systems, Educational institutions, technology transfer, process simulation, impact analysis, Analytical models, software industry, software process improvement, software process simulation, Software, Mathematical model, software project management, Software engineering, software process]
Impact of software resource estimation research on practice: a preliminary report on achievements, synergies, and challenges
2011 33rd International Conference on Software Engineering
None
2011
This paper is a contribution to the Impact Project in the area of software resource estimation. The objective of the Impact Project has been to analyze the impact of software engineering research investments on software engineering practice. The paper begins by summarizing the motivation and context for analyzing software resource estimation; and by summarizing the study's purpose, scope, and approach. The approach includes analyses of the literature; interviews of leading software resource estimation researchers, practitioners, and users; and value/impact surveys of estimators and users. The study concludes that research in software resource estimation has had a significant impact on the practice of software engineering, but also faces significant challenges in addressing likely future software trends.
[Industries, Impact Project, Schedules, software resource estimation, Government, Estimation, software engineering practice, Educational institutions, Modeling, software engineering research investment, Software, software cost estimation, software trend, impact project]
Symbolic execution for software testing in practice: preliminary assessment
2011 33rd International Conference on Software Engineering
None
2011
We present results for the "Impact Project Focus Area" on the topic of symbolic execution as used in software testing. Symbolic execution is a program analysis technique introduced in the 70s that has received renewed interest in recent years, due to algorithmic advances and increased availability of computational power and constraint solving technology. We review classical symbolic execution and some modern extensions such as generalized symbolic execution and dynamic test generation. We also give a preliminary assessment of the use in academia, research labs, and industry.
[dynamic test generation, Java, program testing, software testing, Unified modeling language, Security, preliminary assessment, Computer bugs, program analysis, symbolic execution, generalized symbolic execution, computational power, Concrete, Software, Testing]
ICSE 2011 technical briefings
2011 33rd International Conference on Software Engineering
None
2011
The better we meet the interest of our community, the better we can help bringing ourselves up-to-date with the latest and greatest in and around software engineering. To this purpose, ICSE 2011 for the first time featured technical briefings, an all-day venue for communicating the state of topics related to software engineering, thus providing an exchange of ideas as well as an introduction to the main conference itself.
[Software testing, Patents, Visualization, Communities, Educational institutions, Software, verification, Software engineering]
Exploring, exposing, and exploiting emails to include human factors in software engineering
2011 33rd International Conference on Software Engineering
None
2011
Researchers mine software repositories to support software maintenance and evolution. The analysis of the structured data, mainly source code and changes, has several benefits and offers precise results. This data, however, leaves communication in the background, and does not permit a deep investigation of the human factor, which is crucial in software engineering. Software repositories also archive documents, such as emails or comments, that are used to exchange knowledge among people - we call it "people-centric information." By covering this data, we include the human factor in our analysis, yet its unstructured nature makes it currently sub-exploited. Our work, by focusing on email communication and by implementing the necessary tools, investigates methods for exploring, exposing, and exploiting unstructured data. We believe it is possible to close the gap between development and communication, extract opinions, habits, and views of developers, and link implementation to its rationale; we see in a future where software analysis and development is routinely augmented with people-centric information.
[Measurement, toolset, software development, data mining, electronic mail, human factors, Human factors, email communication, Electronic mail, Data mining, History, software maintenance, software repositories, software evolution, emails, structured data, software analysis, people-centric information, Software, software engineering, unstructured data, Software engineering]
GATE: game-based testing environment
2011 33rd International Conference on Software Engineering
None
2011
In this paper, we propose a game-based public testing mechanism called GATE. The purpose of GATE is to make use of the rich human resource on the Internet to help increase effectiveness in software testing and improve test adequacy. GATE facilitates public testing in three main steps: 1) decompose the test criterion satisfaction problem into many smaller sub-model satisfaction problems; 2) construct games for each individual sub-models and presenting the games to the public through web servers; 3) collect and convert public users' action sequence data into real test cases which guarantee to cover not adequately tested elements. A preliminary study on apache-commons-math library shows that 44% of the branches have not been adequately tested by state of the art automatic test generation techniques. Among these branches, at least 42% are decomposable by GATE into smaller sub-problems. These elements naturally become the potential targets of GATE for public game-based testing.
[Computers, program testing, public testing, Humans, test cases, Servers, program compilers, test adequacy, game based public testing mechanism, computer games, file servers, human computation, Testing, test criterion satisfaction problem, code coverage, software testing, testing, game based testing environment, GATE, Web servers, automatic test generation techniques, human resource, apache commons math library, Games, Logic gates, Software, Internet, submodel satisfaction problems, public users action sequence data]
Reuse vs. maintainability: revealing the impact of composition code properties
2011 33rd International Conference on Software Engineering
None
2011
Over the last years, several composition mechanisms have emerged to improve program modularity. Even though these mechanisms widely vary in their notation and semantics, they all promote a shift in the way programs are structured. They promote expressive means to define the composition of two or more reusable modules. However, given the complexity of the composition code, its actual effects on software quality are not well understood. This PhD research aims at investigating the impact of emerging composition mechanisms on the simultaneous satisfaction of software reuse and maintainability. In order to perform this analysis, we intend to define a set of composition driven metrics and compare their efficacy with traditional modularity metrics. Finally, we plan to derive guidelines on how to use new composition mechanisms to maximize reuse and stability of software modules.
[Measurement, software reuse, modularity metrics, software stability, composition driven metrics, Programming, software maintainability, Complexity theory, software quality, software maintenance, Guidelines, composition code properties, Stability criteria, software reusability, program modularity, Software, advanced composition mechanisms]
Specification mining in concurrent and distributed systems
2011 33rd International Conference on Software Engineering
None
2011
Distributed systems contain several interacting components that perform complex computational tasks. Formal specification of the interaction protocols are crucial to the understanding of these systems. Dynamic specification mining from traces containing information about actual interactions during execution of distributed systems can play a useful role in verification and comprehension when formal specification is not available. A framework for behavioral specification mining in distributed systems is proposed. Concurrency and complexity in the distributed models raise special challenges to specification mining in such systems.
[Protocols, Accuracy, Learning automata, Unified modeling language, Automata, distributed systems, Software, Data mining, specification mining]
Detecting architecturally-relevant code smells in evolving software systems
2011 33rd International Conference on Software Engineering
None
2011
Refactoring tends to avoid the early deviation of a program from its intended architecture design. However, there is little knowledge about whether the manifestation of code smells in evolving software is indicator of architectural deviations. A fundamental difficulty in this process is that developers are only equipped with static analysis techniques for the source code, which do not exploit traceable architectural information. This work addresses this problem by: (1) identifying a family of architecturally-relevant code smells; (2) providing empirical evidence about the correlation of code smell patterns and architectural degeneration; (3) proposing a set of metrics and detection strategies and that exploit traceable architectural information in smell detection; and (4) conceiving a technique to support the early identification of architecture degeneration symptoms by reasoning about code smell patterns.
[Measurement, Correlation, software systems, detection strategies, architecture degeneration symptoms, software evolution, software architecture, Accuracy, refactoring, code smells, Computer architecture, DSL, Graphical user interfaces, program diagnostics, metrics strategies, design rule, source code, architecturally-relevant code smell detection, code smell pattern correlation, software maintenance, static analysis techniques, Software, reasoning about programs, architectural degeneration, software metrics]
Pragmatic reuse in web application development
2011 33rd International Conference on Software Engineering
None
2011
Highly interactive web applications that offer user experience and responsiveness of desktop applications are becoming increasingly popular. They are often composed out of visually distinctive user-interface (UI) elements that encapsulate a certain behavior - the so called UI controls. Similar controls are often used in a large number of web pages, and facilitating their reuse would offer considerable benefits. Unfortunately, because of a very short time-to-market, and a fast pace of technology development, preparing controls for reuse is usually not a primary concern. The focus of my research will be to circumvent this limitation by developing a method, and the accompanying tool for supporting web UI control reuse.
[Web application development, reuse, Web UI control reuse, web application, HTML, user interfaces, code analysis, user responsiveness, interactive Web applications, Fires, interactive systems, technology development, desktop applications, Cascading style sheets, UI controls, time-to-market, ui controls, pragmatic reuse, user experience, UI elements, Pragmatics, time to market, Web pages, software reusability, Software, Internet, Web sites, visually distinctive user-interface elements, Software engineering]
Inconsistency management framework for model-based development
2011 33rd International Conference on Software Engineering
None
2011
In contrast to programming environments, model-based development tools lack in an efficient support in detecting and repairing design errors. However, inconsistencies must be resolved eventually and the detection of inconsistencies is of little use if it is not known how to use this information. Quite many approaches exist for detecting inconsistencies. Only some of them provide solutions for repairing individual inconsistencies but none of them are able to investigate the repair problem comprehensively - in particular considering the side effects that might occur when applying a repair. My PhD thesis focuses on resolving inconsistencies, the different strategies one can follow and how to deal with the critical problem of side effects. My approach is based on an incremental approach for detecting inconsistencies and combines runtime analysis of the design rules' evaluation behavior with static analysis on the design rules' structure. The main contribution is an efficient and effective inconsistency management framework that can be applied generically to (most) modeling and design rule languages.
[Computational modeling, program diagnostics, design rule evaluation behavior, Unified modeling language, Maintenance engineering, Programming, static analysis, model-based development tools, incremental approach, inconsistencies, resolving inconsistencies, Analytical models, modeling languages, inconsistency management framework, user guidance, design rule languages, logic programming, side effects, Concrete, simulation languages, programming environments]
Mental models and parallel program maintenance
2011 33rd International Conference on Software Engineering
None
2011
Parallel programs are difficult to write, test, and debug. This thesis explores how programmers build mental models about parallel programs, and demonstrates, through user evaluations, that maintenance activities can be improved by incorporating theories based on such models. By doing so, this work aims to increase the reliability and performance of today's information technology infrastructure by improving the practice of maintaining and testing parallel software.
[program debugging, mental model, program test, program testing, parallel program maintenance, information technology, software reliability, Debugging, parallel software maintenance, parallelism, Maintenance engineering, user evaluation, software maintenance, parallel software testing, information technology infrastructure, parallel programming, concurrency, Concurrent computing, Parallel programming, program debug, Cognitive science, Testing]
Pragmatic prioritization of software quality assurance efforts
2011 33rd International Conference on Software Engineering
None
2011
A plethora of recent work leverages historical data to help practitioners better prioritize their software quality assurance efforts. However, the adoption of this prior work in practice remains low. In our work, we identify a set of challenges that need to be addressed to make previous work on quality assurance prioritization more pragmatic. We outline four guidelines that address these challenges to make prior work on software quality assurance more pragmatic: 1) Focused Granularity (i.e., small prioritization units), 2) Timely Feedback (i.e., results can be acted on in a timely fashion), 3) Estimate Effort (i.e., estimate the time it will take to complete tasks), and 4) Evaluate Generality (i.e., evaluate findings across multiple projects and multiple domains). We present two approaches, at the code and change level, that demonstrate how prior approaches can be more pragmatic.
[Measurement, computational linguistics, unit testing, software quality, Data mining, History, Pragmatics, Guidelines, pragmatic prioritization, change risk, Software, software quality assurance, Testing, software metrics]
Directed test suite augmentation
2011 33rd International Conference on Software Engineering
None
2011
Test suite augmentation techniques are used in regression testing to identify code elements affected by changes and to generate test cases to cover those elements. Whereas methods and techniques to find affected elements have been extensively researched in regression testing, how to generate new test cases to cover these elements cost-effectively has rarely been studied. It is known that generating test cases is very expensive, so we want to focus on this second step. We believe that reusing existing test cases will help us achieve this task. This research intends to provide a framework for test suite augmentation techniques that will reuse existing test cases to automatically generate new test cases to cover as many affected elements as possible cost-effectively.
[program testing, Heuristic algorithms, code element identification, Software algorithms, regression testing, regression analysis, Minimization, directed test suite augmentation technique, empirical studies, Genetic algorithms, test suite augmentation, Software systems, Testing]
Reengineering legacy software products into software product line based on automatic variability analysis
2011 33rd International Conference on Software Engineering
None
2011
In order to deliver the various and short time-to-market software products to customers, the paradigm of Software Product Line (SPL) represents a new endeavor to the software development. To migrate a family of legacy software products into SPL for effective reuse, one has to understand commonality and variability among existing products variants. The existing techniques rely on manual identification and modeling of variability, and the analysis based on those techniques is performed at several mutually independent levels of abstraction. We propose a sandwich approach that consolidates feature knowledge from top-down domain analysis with bottom-up analysis of code similarities in subject software products. Our proposed method integrates model differencing, clone detection, and information retrieval techniques, which can provide a systematic means to reengineer the legacy software products into SPL based on automatic variability analysis.
[Java, software development, software reuse, bottom-up domain analysis, Cloning, legacy software product reengineering, Information retrieval, Cognition, variability analysis, automatic variability analysis, spl, legacy software, sandwich approach, information retrieval technique, systems re-engineering, software product line, Semantics, model differencing technique, time-to-market software product, software reusability, Feature extraction, Software, top-down domain analysis, clone detection technique]
1.x-Way architecture-implementation mapping
2011 33rd International Conference on Software Engineering
None
2011
A new architecture-implementation mapping approach, 1.x-way mapping, is presented to address architecture-implementation conformance. It targets maintaining conformance of structure and behavior, providing a solution to architecture changes, and protecting architecture-prescribed code from being manually changed. Technologies developed in this work include deep separation of generated and non-generated code, an architecture change model, architecture-based code regeneration, and architecture change notification.
[1.x-way mapping, Unified modeling language, architecture implementation mapping approach, Manuals, Programming, architecture change notification, architecture prescribed code, program compilers, generated code, architecture based code regeneration, software architecture, architecture-implementation mapping, Software architecture, Computer architecture, Software, nongenerated code]
Using software evolution history to facilitate development and maintenance
2011 33rd International Conference on Software Engineering
None
2011
Much research in software engineering have been focused on improving software quality and automating the maintenance process to reduce software costs and mitigating complications associated with the evolution process. Despite all these efforts, there are still high cost and effort associated with software bugs and software maintenance, software still continues to be unreliable, and software bugs can wreak havoc on software producers and consumers alike. My dissertation aims to advance the state-of-art in software evolution research by designing tools that can measure and predict software quality and to create integrated frameworks that helps in improving software maintenance and research that involves mining software repositories.
[Measurement, Software maintenance, software evolution history, software designing tool, software development, software bugs, data mining, Maintenance engineering, software repository mining, software quality, software maintenance, empirical studies, developer productivity, software evolution, Computer languages, Computer bugs, Software quality, software process improvement, software producer, software consumer, software engineering, software cost estimation, software tools, software cost reduction]
Searching, selecting, and synthesizing source code
2011 33rd International Conference on Software Engineering
None
2011
As a programmer writes new software, he or she may instinctively sense that certain functionality is generally or widely-enough applicable to have been implemented before. Unfortunately, programmers face major challenges when attempting to reuse this functionality: First, developers must search for source code relevant to the high-level task at hand. Second, they must select specific components from the relevant code to reuse. Third, they synthesize these components into their own software projects. Techniques exist to address specific instances of these three challenges, but these techniques do not support programmers throughout the reuse process. The goal of this research is to create a unified approach to searching, selecting, and synthesizing source code. We believe that this approach will provide programmers with crucial insight on how high-level functionality present in existing software can be reused.
[Java, search engines, Software algorithms, source code, information retrieval, Programming, high-level functionality, Search engines, software reusability, Software, Libraries, source code search engines, Portfolios]
Tracing architecturally significant requirements: a decision-centric approach
2011 33rd International Conference on Software Engineering
None
2011
This thesis describes a Decision-Centric traceability framework that supports software engineering activities such as architectural preservation, impact analysis, and visualization of design intent. We present a set of traceability patterns, derived from studying real-world architectural designs in high-assurance and high-performance systems. We further present a trace-retrieval approach that reverse engineers design decisions and their associated traceability links by training a classifier to recognize fragments of design decisions and then using the traceability patterns to reconstitute the decisions from their individual parts.
[real-world architectural designs, high-assurance systems, software engineering activities, software traceability, information retrieval, traceability links, design rationale, trace-retrieval approach, architectural preservation, Software reliability, impact analysis, software architecture, Software architecture, high-performance systems, Software systems, Discrete cosine transforms, design intent, architecturally significant requirements, decision-centric traceability framework]
Predictable dynamic deployment of components in embedded systems
2011 33rd International Conference on Software Engineering
None
2011
Dynamic reconfiguration - the ability to hot swap a component or to introduce a new component into a system - is essential to supporting evolutionary change in long-live and highly available systems. A major issue related to this process is to ensure application consistency and performance after reconfiguration. This task is especially challenging for embedded systems which run with limited resources and have specific dependability requirements. We focus on checking resources constraints and propose for a component compliance checking to be performed during its deployment. Our main objective is preserving system integrity during and after reconfiguration by developing a resource efficient dynamic deployment mechanism that will include component validation in respect to available system resources and performance requirements.
[Availability, Computational modeling, application consistency, dependability requirements, Vehicle dynamics, program compilers, resources constraints checking, Embedded systems, component compliance checking, formal verification, USA Councils, embedded systems, dynamic deployment mechanism, Computer architecture, dynamic reconfiguration, component testing, Testing]
A declarative approach to enable flexible and dynamic service compositions
2011 33rd International Conference on Software Engineering
None
2011
Service Oriented Architecture (SOA) is considered the best solution to develop distributed applications that compose existing services to provide new added-value services for their users. However, we claim that the fully potential of SOA is still to be reached and it is our belief that this is caused by the same tools used to build service orchestrations. To overcome this situation we are developing a new declarative language for service orchestration, enhanced with an innovative runtime system which simplifies the development of flexible and robust service compositions.
[innovative runtime system, SOA, Service oriented architecture, declarative language, Programming, dynamic service compositions, Engines, Runtime, specification languages, added-value services, service orchestration, Concrete, service oriented architecture, service-oriented architecture, specification language, Business]
A framework for the integration of user centered design and agile software development processes
2011 33rd International Conference on Software Engineering
None
2011
Agile and user centered design integration (AUCDI) is of significant interest to researchers who want to achieve synergy and eliminate limitations of each. Currently, there are no clear principles or guidelines for practitioners to achieve successful integration. In addition, substantial differences exist between agile and UCD approaches which pose challenges to integration attempts. As a result, practitioners developed individual integration strategies. However, success evaluation of current AUCDI attempts has been anecdotal. Moreover, AUCDI attempts cannot be generalized to provide guidance and assistance to other projects or organizations with different needs. My thesis aims to provide a Software Process Improvement (SPI) framework for AUCDI by providing generic guidelines and practices for organizations aspiring to achieve AUCDI in order to address AUCDI challenges including: introducing systematicity and structure into AUCDI, assessing AUCDI processes, and accommodating project and organizational characteristics.
[synergy, software prototyping, User centered design, Programming, organizational characteristics, Guidelines, AUCDI, framework, software process improvement framework, generic guidelines, project characteristics, Organizations, software process improvement, organizations, agile and user centered design integration, agile software development processes, individual integration strategy, SPI framework, agile, Usability, Capability maturity model, organisational aspects, user centered design, user centred design]
Improving open source software patch contribution process: methods and tools
2011 33rd International Conference on Software Engineering
None
2011
The patch contribution process (PCP) is very important to the sustainability of OSS projects. Nevertheless, there are several issues on patch contribution in mature OSS projects, which include time consuming process, lost and ignored patches, slow review process. These issues are recognized by researchers and OSS projects, but have not been addressed. In this dissertation, I apply Kanban method to guide process improvement and tools development to reduce PCP cycle time.
[Measurement, open source software patch contribution process, Conferences, public domain software, Communities, OSS projects, Project management, tool development, Open source software, coordination, productivity, free/open source software, software process improvement, kanban, lost patch, software tools, Kanban method, time consuming process, project management, process improvement, PCP cycle time, lean methods, ignored patch, slow review process, Software engineering]
Systematizing security test case planning using functional requirements phrases
2011 33rd International Conference on Software Engineering
None
2011
Security experts use their knowledge to attempt attacks on an application in an exploratory and opportunistic way in a process known as penetration testing. However, building security into a product is the responsibility of the whole team, not just the security experts who are often only involved in the final phases of testing. Through the development of a black box security test plan, software testers who are not necessarily security experts can work proactively with the developers early in the software development lifecycle. The team can then establish how security will be evaluated such that the product can be designed and implemented with security in mind. The goal of this research is to improve the security of applications by introducing a methodology that uses the software system's requirements specification statements to systematically generate a set of black box security tests. We used our methodology on a public requirements specification to create 137 tests and executed these tests on five electronic health record systems. The tests revealed 253 successful attacks on these five systems, which are used to manage the clinical records for approximately 59 million patients, collectively. If non-expert testers can surface the more common vulnerabilities present in an application, security experts can attempt more devious, novel attacks.
[requirements, program testing, software testing, Buildings, Medical services, testing, Programming, Security, functional requirements phrase, public requirements specification, formal specification, requirements specification statement, software development lifecycle, security, security of data, security test case planning, black box security test plan, vulnerabilities, Software, penetration testing, verification, Testing, Software engineering]
Mining software repositories using topic models
2011 33rd International Conference on Software Engineering
None
2011
Software repositories, such as source code, email archives, and bug databases, contain unstructured and unlabeled text that is difficult to analyze with traditional techniques. We propose the use of statistical topic models to automatically discover structure in these textual repositories. This discovered structure has the potential to be used in software engineering tasks, such as bug prediction and traceability link recovery. Our research goal is to address the challenges of applying topic models to software repositories.
[Adaptation models, program debugging, Object oriented modeling, Computational modeling, program diagnostics, data mining, mining software repositories, source code, software repository mining, bug databases, lda, textual repository, bug prediction, Data mining, statistical topic model, email archives, traceability link recovery, topic models, Software, software engineering, Resource management, statistical analysis, Software engineering]
Test blueprint: an effective visual support for test coverage
2011 33rd International Conference on Software Engineering
None
2011
Test coverage is about assessing the relevance of unit tests against the tested application. It is widely acknowledged that a software with a "good" test coverage is more robust against unanticipated execution, thus lowering the maintenance cost. However, insuring a coverage of a good quality is challenging, especially since most of the available test coverage tools do not discriminate software components that require a "strong" coverage from the components that require less attention from the unit tests. HAPAO is an innovative test covepage tool, implemented in the Pharo Smalltalk programming language. It employs an effective and intuitive graphical representation to visually assess the quality of the coverage. A combination of appropriate metrics and relations visually shapes methods and classes, which indicates to the programmer whether more effort on testing is required. This paper presents the essence of HAPAO using a real world case study.
[Measurement, Visualization, visualization, program testing, Complexity theory, software quality, software maintenance cost, software component, unit tests, Pharo Smalltalk programming language, graphical representation, HAPAO test coverage tool, software tools, Testing, Context, coverage, object-oriented programming, test coverage visual support, testing, software maintenance, software test coverage, Computer languages, test blueprint, object-oriented languages, Software, software cost estimation, program visualisation, pharo, software metrics]
A formal approach to software synthesis for architectural platforms
2011 33rd International Conference on Software Engineering
None
2011
Software-intensive systems today often rely on middleware platforms as major building blocks. As such, the architectural choices of such systems are being driven to a significant extent by such platforms. However, the diversity and rapid evolution of these platforms lead to architectural choices quickly becoming obsolete. Yet architectural choices are among the most difficult to change. This paper presents a novel and formal approach to end-to-end transformation of application models into architecturally correct code, averting the problem of mapping application models to such architectural platforms.
[Actuators, Metals, software intensive systems, software synthesis, Servers, Middleware, architectural platforms, Connectors, formal approach, software architecture, architectural styles, Sensors, middleware platforms, architectural maps, middleware]
Detecting cross-browser issues in web applications
2011 33rd International Conference on Software Engineering
None
2011
Cross-browser issues are prevalent in web applications. However, existing tools require considerable manual effort from developers to detect such issues. Our technique and prototype tool - WEBDIFF detects such issues automatically and reports them to the developer. Along with each issue reported, the tool also provides details about the affected HTML element, thereby helping the developer to fix the issue. WEBDIFF is the first technique to apply concepts from computer vision and graph theory to identify cross-browser issues in web applications. Our results show that WEBDIFF is practical and can find issues in real world web applications.
[Computer vision, Visualization, program testing, graph theory, Manuals, WEBDIFF, prototype tool, cross-browser issues, dynamic analysis, Browsers, tree matching, real world Web applications, Web pages, HTML element, computer vision, online front-ends, Internet, web testing, Testing, Software engineering, hypermedia markup languages]
Measuring subversions: security and legal risk in reused software artifacts
2011 33rd International Conference on Software Engineering
None
2011
A software system often includes a set of library dependencies and other software artifacts necessary for the system's proper operation. However, long-term maintenance problems related to reused software can gradually emerge over the lifetime of the deployed system. In our exploratory study we propose a manual technique to locate documented security and legal problems in a set of reused software artifacts. We evaluate our technique with a case study of 81 Java libraries found in a proprietary e-commerce web application. Using our approach we discovered both a potential legal problem with one library, and a second library that was affected by a known security vulnerability. These results support our larger thesis: software reuse entails long-term maintenance costs. In future work we strive to develop automated techniques by which developers, managers, and other software stakeholders can measure, address, and minimize these costs over the lifetimes of their software assets.
[legal risk, reuse, Law, software reliability, long-term maintenance problems, Licenses, documented security, Security, software system, Java libraries, software stakeholders, security, software artifact reusability, proprietary e-commerce Web application, Libraries, Software measurement, licensing, electronic commerce, potential legal problem, software assets, subversions measurement, software maintenance, library dependencies, security of data, manual technique, Software, Internet, maintenance]
Building domain specific software architectures from software architectural design patterns
2011 33rd International Conference on Software Engineering
None
2011
Software design patterns are best practice solutions to common software problems. However, applying design patterns in practice can be difficult since design pattern descriptions are general and can be applied at multiple levels of abstraction. In order to address the aforementioned issue, this research focuses on creating a systematic approach to designing domain specific distributed, real-time and embedded (DRE) software from software architectural design patterns. To address variability across a DRE domain, software product line concepts are used to categorize and organize the features and design patterns. The software architectures produced are also validated through design time simulation. This research is applied and validated using the space flight software (FSW) domain.
[Real time systems, domain specific software architecture, object-oriented programming, distributed real-timeand embedded software, Unified modeling language, software architectures, software architectural design pattern, design time simulation, space flight software, Space vehicles, software architecture, Software architecture, embedded software, uml, distributed software, design patterns, Computer architecture, software product line concept, FSW domain, Kernel, DRE software, real-time software]
Using impact analysis in industry
2011 33rd International Conference on Software Engineering
None
2011
Software is subjected to continuous change, and with increasing size and complexity performing changes becomes more critical. Impact analysis assists in estimating the consequences of a change, and is an important research topic. Nevertheless, until now researchers have not applied and evaluated those techniques in industry. This paper contributes an approach suitable for an industrial setting, and an evaluation of its application in a large software system.
[Visualization, IEEE Computer Society, regression testing, software system, impact analysis, static impact analysis, industrial engineering, Software systems, software engineering, industrial case study, Testing, Software engineering, Business, industrial setting]
A decision support system for the classification of software coding faults: a research abstract
2011 33rd International Conference on Software Engineering
None
2011
A decision support system for fault classification is presented. The fault classification scheme is developed to provide guidance in process improvement and fault-based testing. The research integrates results in fault classification, source code analysis, and fault-based testing research. Initial results indicate that existing change type and fault classification schemes are insufficient for this purpose. Development of sufficient schemes and their evaluation are discussed.
[Decision support systems, Java, program testing, program diagnostics, object-oriented software, process improvement, software fault taxonomy, Classification algorithms, decision support systems, software fault tolerance, software evolution, fault model, software process improvement, decision support system, Syntactics, User interfaces, Software, software coding fault classification, fault-based testing, source code analysis, Testing]
Specification mining in concurrent and distributed systems
2011 33rd International Conference on Software Engineering
None
2011
Dynamic specification mining involves discovering software behavior from traces for the purpose of program comprehension and bug detection. However, in concurrent/distributed programs, the inherent partial order relationships among events occurring across processes pose a big challenge to specification mining. A framework for mining partial orders that takes in a set of concurrent program traces, and produces a message sequence graph (MSG) is proposed. Mining an MSG allows one to understand concurrent behaviors since the nodes of the MSG depict important "phases" or "interaction snippets" involving several concurrently executing processes. Experiments on mining behaviors of fairly complex distributed systems show that the proposed miner can produce the corresponding MSGs with both high precision and high recall.
[program comprehension, message sequence graph, Learning automata, Atmospheric modeling, Unified modeling language, data mining, concurrent system, bug detection, Data mining, specification mining, concurrent engineering, formal specification, partial order mining, dynamic specification mining, Accuracy, graphs, software discovery, complex distributed system, Automata, distributed systems, Software, concurrent program traces, distributed programming, interaction snippet]
A case study on refactoring in Haskell programs
2011 33rd International Conference on Software Engineering
None
2011
Programmers use refactoring to improve the design of existing code without changing external behavior. Current research does not empirically answer the question, "Why and how do programmers refactor functional programs?" In order to answer the question, I conducted a case study on three open source projects in Haskell. I investigated changed portions of code in 55 successive versions of a given project to classify how programmers refactor. I found a total of 143 refactorings classified by 12 refactoring types. I also found 5 new refactoring types and propose two new refactoring tools that would be useful for developers.
[haskell, functional programming, Humans, Programming, Servers, software maintenance, Computer languages, refactoring, Games, functional language, Haskell program refactoring tool, Software, programmer refactor, code design, functional program]
Build system maintenance
2011 33rd International Conference on Software Engineering
None
2011
The build system, i.e., the infrastructure that converts source code into deliverables, plays a critical role in the development of a software project. For example, developers rely upon the build system to test and run their source code changes. Without a working build system, development progress grinds to a halt, as the source code is rendered useless. Based on experiences reported by developers, we conjecture that build maintenance for large software systems is considerable, yet this maintenance is not well understood. A firm understanding of build maintenance is essential for project managers to allocate personnel and resources to build maintenance tasks effectively, and reduce the build maintenance overhead on regular development tasks, such as fixing defects and adding new features. In our work, we empirically study build maintenance in one proprietary and nine open source projects of different sizes and domain. Our case studies thus far show that: (1) similar to Lehman's first law of software evolution, build system specifications tend to grow unless effort is invested into restructuring them, (2) the build system accounts for up to 31% of the code files in a project, and (3) up to 27% of development tasks that change the source code also require build maintenance. Currently, we are working on identifying concrete measures that projects can take to reduce the build maintenance overhead.
[Java, IEEE Computer Society, open source projects, build system specifications, software project development, large software systems, build systems, mining software repositories, Maintenance engineering, build system maintenance, software management, software maintenance, Couplings, Linux, empirical software engineering, Software, Software engineering]
Finding relevant functions in millions of lines of code
2011 33rd International Conference on Software Engineering
None
2011
Source code search engines locate and display fragments of code relevant to user queries. These fragments are often isolated and detached from one another. Programmers need to see how source code interacts in order to understand the concepts implemented in that code, however. In this paper, we present Portfolio, a source code search engine that retrieves and visualizes relevant functions as chains of function invocations. We evaluated Portfolio against Google Code Search and Koders in a case study with 49 professional programmers. Portfolio outperforms both of these engines in terms of relevance and visualization of the returned results.
[user queries, Visualization, Google, search engines, source coding, information retrieval, code fragments, Google code search, Engines, professional programmer, query processing, Storage area networks, invocation, source code search engine, Search engines, Software, source code search engines, Portfolios]
Requirements tracing: discovering related documents through artificial pheromones and term proximity
2011 33rd International Conference on Software Engineering
None
2011
Requirements traceability is an important undertaking as part of ensuring the quality of software in the early stages of the Software Development Life Cycle. This paper demonstrates the applicability of swarm intelligence to the requirements tracing problem using pheromone communication and a focus on the common text around linking terms or words in order to find related textual documents. Through the actions and contributions of each individual member of the swarm, the swarm as a whole exposes relationships between documents in a collective manner. Two techniques have been examined, simple swarm and pheromone swarm. The techniques have been validated using two real-world datasets from two problem domains.
[document handling, Vocabulary, pheromone communication, simple swarm, swarms, requirements traceability, pheromone swarm, information retrieval, Educational institutions, artificial pheromones, Thesauri, software quality, term proximity, Presses, software development life cycle, requirements tracing problem, document discovery, Software, software engineering, Joining processes, textual documents, swarm intelligence]
An end-user demonstration approach to support aspect-oriented modeling
2011 33rd International Conference on Software Engineering
None
2011
Aspect-oriented modeling (AOM) is a technique to separate concerns that crosscut the modularity boundaries of a modeling hierarchy. AOM is traditionally supported by manual editing or writing model transformation rules that refine a base model. However, traditional model weaving approaches present challenges to those who are unfamiliar with a model transformation language or metamodel definitions. This poster describes an approach to weave aspect models by recording and analyzing demonstrated operations by end-users.
[aspect-oriented modeling, Computational modeling, Object oriented modeling, Programming, end-user demonstration approach, Engines, model transformation language, metamodel definitions, modeling hierarchy, Writing, aspect-oriented programming, Weaving, Data models, software engineering, demonstration, model transformation rules]
Problem identification for structural test generation: first step towards cooperative developer testing
2011 33rd International Conference on Software Engineering
None
2011
Achieving high structural coverage is an important goal of software testing. Instead of manually producing high-covering test inputs that achieve high structural coverage, testers or developers can employ tools built based on automated test-generation approaches to automatically generate such test inputs. Although these tools can easily generate high-covering test inputs for simple programs, when applied on complex programs in practice, these tools face various problems, such as the problems of dealing with method calls to external libraries, generating method-call sequences to produce desired object states, and exceeding defined boundaries of resources due to loops. Since these tools currently are not powerful enough to deal with these various problems in testing complex programs, we propose cooperative developer testing, where developers provide guidance to help tools achieve higher structural coverage. To reduce the efforts of developers in providing guidance to the tools, we propose a novel approach, called Covana. Covana precisely identifies and reports problems that prevent the tools from achieving high structural coverage primarily by determining whether branch statements containing not-covered branches have data dependencies on problem candidates.
[Software testing, automatic test software, Java, automatic test pattern generation, program testing, cooperative developer testing, software testing, Covana approach, software libraries, structural coverage, software tool, data dependency, problem identification, automated test generation approach, method call sequence, Software, Concrete, dynamic symbolic execution, software tools, Face, structural test generation, Monitoring, complex program testing]
Palus: a hybrid automated test generation tool for java
2011 33rd International Conference on Software Engineering
None
2011
In object-oriented programs, a unit test often consists of a sequence of method calls that create and mutate objects. It is challenging to automatically generate sequences that are legal and behaviorally-diverse, that is, reaching as many different program states as possible. This paper proposes a combined static and dynamic test generation approach to address these problems, for code without a formal specification. Our approach first uses dynamic analysis to infer a call sequence model from a sample execution, then uses static analysis to identify method dependence relations based on the fields they may read or write. Finally, both the dynamically-inferred model (which tends to be accurate but incomplete) and the statically-identified dependence information (which tends to be conservative) guide a random test generator to create legal and behaviorally-diverse tests. Our Palus tool implements this approach. We compared it with a pure random approach, a dynamic-random approach (without a static phase), and a static-random approach (without a dynamic phase) on six popular open-source Java programs. Tests generated by Palus achieved 35% higher structural coverage on average. Palus is also internally used in Google, and has found 22 new bugs in four well-tested products.
[dynamic test generation, Java, Law, hybrid automated test generation tool, dynamic analysis, Palus, formal specification, automated test generation, object-oriented program, Analytical models, Computer bugs, static and dynamic analyses, Software, Testing]
Scalable automatic linearizability checking
2011 33rd International Conference on Software Engineering
None
2011
Concurrent data structures are widely used but notoriously difficult to implement correctly. Linearizability is one main correctness criterion of concurrent data structure algorithms. It guarantees that a concurrent data structure appears as a sequential one to users. Unfortunately, linearizability is challenging to verify since a subtle bug may only manifest in a small portion of numerous thread interleavings. Model checking is therefore a potential primary candidate. However, current approaches of model checking linearizability suffer from severe state space explosion problem and are thus restricted in handling few threads and/or operations. This paper describes a scalable, fully automatic and general linearizability checking method based on [16] by incorporating symmetry and partial order reduction techniques. Our insights emerged from the observation that the similarity of threads using concurrent data structures causes model checking to generate large redundant equivalent portions of the state space, and the loose coupling of threads causes it to explore lots of unnecessary transition execution orders. We prove that symmetry reduction and partial order reduction can be combined in our approach and integrate them into the model checking algorithm. We demonstrate its efficiency using a number of real-world concurrent data structure algorithms.
[Computers, partial order reduction techniques, Computational modeling, Instruction sets, Conferences, symmetry reduction, Software algorithms, Data structures, linearizability checking, state space explosion problem, concurrent data structure algorithms, Computer science, formal verification, model checking, unnecessary transition execution orders, scalable automatic linearizability checking method, data structures]
Workshop on cooperative and human aspects of software engineering: (CHASE 2011)
2011 33rd International Conference on Software Engineering
None
2011
Software is created by people for people working in varied environments, under various conditions. Thus understanding cooperative and human aspects of software development is crucial to comprehend how methods and tools are used, and thereby improve the creation and maintenance of software. Over the years, both researchers and practitioners have recognized the need to study and understand these aspects. Despite recognizing this, researchers in cooperative and human aspects have no clear place to meet and are dispersed in different research conferences and areas. The goal of this workshop is to provide a forum for discussing high quality research on human and cooperative aspects of software engineering. We aim at providing both a meeting place for the growing community and the possibility for researchers interested in joining the field to present their work in progress and get an overview over the field.
[cooperative and human factors, Conferences, Communities, Humans, Programming, Educational institutions, Software, Software engineering]
Fourth international workshop on multicore software engineering: (IWMSE 2011)
2011 33rd International Conference on Software Engineering
None
2011
This paper summarizes the highlights of the Fourth International Workshop on Multicore Software Engineering (IWMSE 2011). The workshop addresses the software engineering and parallel programming challenges that come with the wide availability of multicore processors. Researchers and practitioners have come together to present and discuss new work on programming techniques, refactoring, performance engineering, and applications.
[Program processors, Multicore processing, Conferences, Programming, Educational institutions, multicore, parallel programming, Software engineering]
Workshop on flexible modeling tools: (FlexiTools 2011)
2011 33rd International Conference on Software Engineering
None
2011
Modeling tools are often not used for tasks during the software lifecycle for which they should be more helpful; instead free-from approaches, such as office tools and white boards, are frequently used. Prior workshops explored why this is the case and what might be done about it. The goal of this workshop is to continue those discussions and also to form an initial set of challenge problems and research challenges that researchers and developers of flexible modeling tools should address.
[modeling, Conferences, USA Councils, Computational modeling, Communities, flexible modeling tools, Educational institutions, Software, software development tools, Software engineering]
Workshop on games and software engineering: (GAS 2011)
2011 33rd International Conference on Software Engineering
None
2011
At the core of video games are complex interactions leading to emergent behaviors. This complexity creates difficulties architecting components, predicting their behaviors and testing the results. The Workshop on Games and Software Engineering (GAS 2011) provides an opportunity for software engineering researchers and practitioners who work with games to come together and discuss how these two areas can be intertwined.
[Conferences, Communities, Games, Educational institutions, software engineering, Software engineering, Testing, video games]
Workshop on software engineering for cloud computing: (SECLOUD 2011)
2011 33rd International Conference on Software Engineering
None
2011
Cloud computing is emerging as more than simply a technology platform but a software engineering paradigm for the future. Hordes of cloud computing technologies, techniques, and integration approaches are widely being adopted, taught at the university level, and expected as key skills in the job market. The principles and practices of the software engineering and software architecture community can serve to help guide this emerging domain. The fundamental goal of the ICSE 2011 Software Engineering for Cloud Workshop is to bring together the diverse communities of cloud computing and of software engineering and architecture research with the hopes of sharing and disseminating key tribal knowledge between these rich areas. We expect as the workshop output a set of identified key software engineering challenges and important issues in the domain of cloud computing, specifically focused on how software engineering practice and research can play a role in shaping the next five years of research and practice for clouds. Furthermore, we expect to share "war stories\
[Cloud computing, Conferences, USA Councils, Communities, NASA, secloud, software engineering, cloud computing]
Second international workshop on software engineering for sensor network applications: (SESENA 2011)
2011 33rd International Conference on Software Engineering
None
2011
We describe the motivation, focus, and organization of SESENA11, the 2nd International Workshop on Software Engineering for Sensor Network Applications. The workshop took place under the umbrella of ICSE 2011, the 33rd ACM/IEEE International Conference on Software Engineering, in Honolulu, Hawaii, on May 22, 2011. The aim was to attract researchers belonging to the Software Engineering (SE) and Wireless Sensor Network (WSN) communities, not only to exchange their recent research results on the topic, but also to stimulate discussion on the core open problems and define a shared research agenda. More information can be found at the workshop website: http://www.sesena.info.
[Conferences, Communities, development tools, testing, Educational institutions, sensor networks, model-driven development, Wireless sensor networks, USA Councils, Software, software engineering, methodology, verification, Software engineering, deployment]
Seventh international workshop on software engineering for secure systems: (SESS 2011)
2011 33rd International Conference on Software Engineering
None
2011
The 7th edition of the SESS workshop aims at providing a venue for software engineers and security researchers to exchange ideas and techniques. In fact, software is at core of most of the business transactions and its smart integration in an industrial setting may be the competitive advantage even when the core competence is outside the ICT field. As a result, the revenues of a firm depend directly on several complex software-based systems. Thus, stakeholders and users should be able to trust these systems to provide data and elaborations with a degree of confidentiality, integrity, and availability compatible with their needs. Moreover, the pervasiveness of software products in the creation of critical infrastructures has raised the value of trustworthiness and new efforts should be dedicated to achieve it. However, nowadays almost every application has some kind of security requirement even if its use is not to be considered critical.
[Context, Computer science, Conferences, information security, Educational institutions, Software, software engineering, Security, secure systems, Software engineering]
Fourth workshop on refactoring tools: (WRT 2011)
2011 33rd International Conference on Software Engineering
None
2011
Refactoring is the process of applying behavior-preserving transformations to a program with the objective of improving the program's design. A specific refactoring is identified by a name (e.g., Extract Method), a set of preconditions, and a set of transformations that need to be performed. Tool support for refactoring is essential because checking the preconditions of refactoring often requires nontrivial program analysis, and applying transformations may affect many locations throughout a program. In recent years, the emergence of light-weight programming methodologies such as Extreme Programming has generated a great amount of interest in refactoring, and refactoring support has become a required feature in today's IDEs. This workshop is a continuation of a series of previous workshops (ECOOP 2007, OOPSLA 2008 and 2009 - see http://refactoring.info/WRT) where researchers and developers of refactoring tools can meet and discuss recent ideas and work, and view tool demonstrations.
[USA Councils, Conferences, refactoring, Semantics, program manipulation, program analysis, Programming, Educational institutions, Servers, transformation, Engines]
Second international workshop on product line approaches in software engineering: (PLEASE 2011)
2011 33rd International Conference on Software Engineering
None
2011
PLEASE workshop series focuses on exploring the present and the future of Software Product Line Engineering techniques. The main goal of PLEASE 2011 is to bring together industrial practitioner and software product line researchers in order to couple real-life industrial problems with concrete solutions developed by the community. We plan for an interactive workshop, where participants can apply their expertise to current industrial problems, while those who face challenges in the area of product line engineering can benefit from the suggested solutions. We also intend to establish ongoing, long-lasting relationships between industrial and research participants to the mutual benefits of both. The second edition of PLEASE is held in conjunction with the 33rd International Conference in Software Engineering (May 21-28, 2011, Honolulu, Hawaii).
[software product lines, variability management, Publishing, Conferences, USA Councils, Educational institutions, Software, Concrete, product line engineering, Software engineering]
Third international workshop on software engineering in healthcare: (SEHC 2011)
2011 33rd International Conference on Software Engineering
None
2011
This paper briefly describes the 3rd International Workshop on Software Engineering in Healthcare, held at the 2011 International Conference on Software Engineering.
[Economics, e-health, Conferences, Communities, Medical services, health, medicine, electronic health records, healthcare, medical informatics, mobile health, Software, Medical diagnostic imaging, Software engineering]
Collaborative teaching of globally distributed software development: community building workshop (CTGDSD 2011)
2011 33rd International Conference on Software Engineering
None
2011
Software engineering project courses where student teams are geographically distributed can effectively simulate the problems of globally distributed software development (DSD). However, this pedagogical model has proven difficult to adopt or sustain. It requires significant pedagogical resources and collaboration infrastructure. Institutionalizing such courses also requires compatible and reliable teaching partners. The purpose of this workshop is to foster a community of international faculty and institutions committed to developing, supporting, and teaching DSD. Foundational materials presented will include pedagogical materials and infrastructure developed and used in teaching DSD courses along with results and lessons learned. Long-range goals include: lowering adoption barriers by providing common pedagogical materials, validated collaboration infrastructure, and a pool of potential teaching partners from around the globe.
[computational thinking, Conferences, Communities, Collaboration, Programming, Educational institutions, distributed software development, Software engineering]
Fifth international workshop on software clones: (IWSC 2011)
2011 33rd International Conference on Software Engineering
None
2011
Software clones are identical or similar pieces of code, design or other artifacts. Clones are known to be closely related to various issues in software engineering, such as software quality, complexity, architecture, refactoring, evolution, licensing, plagiarism, and so on. Various characteristics of software systems can be uncovered through clone analysis, and system restructuring can be performed by merging clones. The goals of this workshop are to bring together researchers and practitioners from around the world to evaluate the current state of research and applications, discuss common problems, discover new opportunities for collaboration, exchange ideas, envision new areas of research and applications, and explore synergies with similarity analysis in other areas and disciplines.
[Measurement, software clones, clone detection, Conferences, Plagiarism, Cloning, Educational institutions, reverse engineering, Software, software maintenance, Software engineering]
Second international workshop on managing technical debt: (MTD 2011)
2011 33rd International Conference on Software Engineering
None
2011
The technical debt metaphor is gaining significant traction in the software development community as a way to understand and communicate issues of intrinsic quality, value, and cost. The idea is that developers sometimes accept compromises in a system in one dimension (e.g., modularity) to meet an urgent demand in some other dimension (e.g., a deadline), and that such compromises incur a "debt": on which "interest" has to be paid and which should be repaid at some point for the long-term health of the project. Little is known about technical debt, beyond feelings and opinions. The software engineering research community has an opportunity to study this phenomenon and improve the way it is handled. We can offer software engineers a foundation for managing such trade-offs based on models of their economic impacts. The goal of this second workshop is to discuss managing technical debt as a part of the research agenda for the software engineering field.
[Economics, USA Councils, Conferences, Programming, Educational institutions, Software, software quality, software economics, technical debt, Software engineering]
Sixth international workshop on traceability in emerging forms of software engineering: (TEFSE 2011)
2011 33rd International Conference on Software Engineering
None
2011
The Sixth International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE 2011) will bring together researchers and practitioners to examine the challenges of recovering and maintaining traceability for the myriad forms of software engineering artifacts, ranging from user needs to models to source code. The objective of the 6th edition of TEFSE is to build on the work the traceability research community has completed in identifying the open traceability challenges. In particular, it is intended to be a working event focused on discussing the main problems related to software artifact traceability and propose possible solutions for such problems. Moreover, the workshop also aims at identifying key issues concerning the importance of maintaining the traceability information during software development, to further improve the cooperation between academia and industry and to facilitate technology transfer.
[Productivity, Industries, Conferences, software traceability, Documentation, Educational institutions, Software, Software engineering, software evolution]
Sixth international workshop on automation of software test: (AST 2011)
2011 33rd International Conference on Software Engineering
None
2011
The Sixth International Workshop on Automation of Software Test (AST 2011) is associated with the 33rd International Conference on Software Engineering (ICSE 2011). This edition of AST was focused on the special theme of Software Design and the Automation of Software Test and authors were encouraged to submit work in this area. The workshop covers two days with presentations of regular research papers, industrial case studies and experience reports. The workshop also aims to have extensive discussions on collaborative solutions in the form of charette sessions. This paper summarizes the organization of the workshop, the special theme, as well as the sessions.
[Software testing, Analytical models, Automation, Software design, automation of software test, Conferences, software testing, software design, software tools]
Third international workshop on principles of engineering service-oriented systems: (PESOS 2011)
2011 33rd International Conference on Software Engineering
None
2011
Service-oriented systems have attracted great interest from industry and research communities worldwide. Service integrators, developers, and providers are collaborating to address the various challenges in the field. PESOS 2011 is a forum for all these communities to present and discuss a wide range of topics related to service-oriented systems. The goal of PESOS is to bring together researchers from academia and industry, as well as practitioners working in the areas of software engineering and service-oriented systems to discuss research challenges, recent developments, novel applications, as well as methods, techniques, experiences, and tools to support the engineering of service-oriented systems.
[Industries, engineering principles, service-oriented systems, Conferences, USA Councils, Service oriented architecture, Educational institutions, Australia, service-oriented architecture]
Workshop on SHAring and Reusing architectural Knowledge: (SHARK 2011)
2011 33rd International Conference on Software Engineering
None
2011
Architectural Knowledge (AK) is defined as the integrated representation of the software architecture of a software-intensive system or family of systems along with architectural decisions and their rationale, external influence and the development environment. The SHARK workshop series focuses on current methods, languages, and tools that can be used to extract, represent, share, apply, and reuse AK, and the experimentation and/or exploitation thereof. This sixth edition of SHARK will discuss, among other topics, the approaches for AK personalization, where knowledge is not codified through templates or annotations, but it is exchanged through the discussion between the different stakeholders.
[Presses, software architecture, Software architecture, Conferences, Communities, Documentation, Educational institutions, Knowledge management, knowledge management]
Second international workshop on web 2.0 for software engineering: (Web2SE 2011)
2011 33rd International Conference on Software Engineering
None
2011
Social software is built around an "architecture of participation" where user data is aggregated as a side-effect of using Web 2.0 applications. Web 2.0 implies that processes and tools are socially open, and that content can be used in several different contexts. Web 2.0 tools and technologies support interactive information sharing, data interoperability and user centered design. For instance, wikis, blogs, tags and feeds help us organize, manage and categorize content in an informal and collaborative way. Some of these technologies have made their way into collaborative software development processes and development platforms. These processes and environments are just scratching the surface of what can be done by incorporating Web 2.0 approaches and technologies into collaborative software development. Web 2.0 opens up new opportunities for developers to form teams and collaborate, but it also comes with challenges for developers and researchers. Web2SE aims to improve our understanding of how Web 2.0, manifested in technologies such as mashups or dashboards, can change the culture of collaborative software development.
[process, USA Councils, Conferences, collaboration, Programming, Educational institutions, Software, Internet, web 2.0, tools, Software engineering]
Workshop on emerging trends in software metrics: (WETSoM 2011)
2011 33rd International Conference on Software Engineering
None
2011
The Workshop on Emerging Trends in Software Metrics aims at bringing together researchers and practitioners to discuss the progress of software metrics. The motivation for this workshop is the low impact that software metrics has on current software development. The goals of this workshop are to critically examine the evidence for the effectiveness of existing metrics and to identify new directions for development of software metrics.
[complexity, Software metrics, Conferences, Object oriented modeling, Educational institutions, Software, software quality, Software engineering, software metrics]
Fourth international workshop on software engineering for computational science and engineering: (SE-CSE2011)
2011 33rd International Conference on Software Engineering
None
2011
Computational Science and Engineering (CSE) software supports a wide variety of domains including nuclear physics, crash simulation, satellite data processing, fluid dynamics, climate modeling, bioinformatics, and vehicle development. The increase in the importance of CSE software motivates the need to identify and understand appropriate software engineering (SE) practices for CSE. Because of the uniqueness of CSE software development, existing SE tools and techniques developed for the business/IT community are often not efficient or effective. Appropriate SE solutions must account for the salient characteristics of the CSE development environment. This situation creates an opportunity for members of the SE community to interact with members of the CSE community to address this need. This workshop facilitates that collaboration by bringing together members of the SE community and the CSE community to share perspectives and present findings from research and practice relevant to CSE software. A significant portion of the workshop is devoted to focused interaction among the participants with the goal of generating a research agenda to improve tools, techniques, and experimental methods for studying CSE software engineering.
[computational science, Scientific computing, Conferences, Computational modeling, Communities, computational engineering, Programming, Software, Software engineering]
Third international workshop on search-driven development: users, infrastructure, tools, and evaluation (SUITE 2011)
2011 33rd International Conference on Software Engineering
None
2011
SUITE is a workshop that focuses on exploring the notion of search as a fundamental activity during software development. The first two editions of SUITE were held at ICSE 2009/2010 [1, 2], and they have focused on the building of a research community that brings researchers and practioners who are interested in the research areas that SUITE addresses. While this thrid workshop continues the effort of community building, it puts more focus on addressing directly some of the urgent issues identified by previous two workshops, encouraging researchers to contribute to and take advantage of common datasets that we have started assembling for SUITE research.
[Human computer interaction, Conferences, Buildings, Programming, Search engines, search driven development, software information retrieval, Software, information needs, Software engineering]
First workshop on developing tools as plug-ins: (TOPI 2011)
2011 33rd International Conference on Software Engineering
None
2011
Our knowledge as to how to solve software engineering problems is increasingly being encapsulated in tools. These tools are at their strongest when they operate in a pre-existing development environment that can provide integration with existing elements such as compilers, debuggers, profilers and visualizers. The first Workshop on Developing Tools as Plug-ins is a new forum in which to addresses research, ongoing work, ideas, concepts, and critical questions related to the engineering of software tools and plug-ins.
[Knowledge engineering, Visualization, Runtime, plug-ins, software development, Conferences, USA Councils, ides, Encoding, tools]
SCORE 2011: the second student contest on software engineering
2011 33rd International Conference on Software Engineering
None
2011
SCORE 2011 is the second iteration of a team-oriented software engineering contest that attracts student teams from around the world, culminating in a final round of competition and awards at ICSE. Each team has responded to one of the project proposals provided by the SCORE program committee, usually in the context of a software engineering project course. In this second iteration we have built on the success of SCORE 2009, greatly expanding the number and geographical distribution of student teams, including many of very high quality.
[Computers, Context, contests, project courses, Awards activities, USA Councils, student projects, Educational institutions, Proposals, Software engineering]
Message from the Chairs
2012 34th International Conference on Software Engineering
None
2012
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
2012 34th International Conference on Software Engineering
None
2012
Provides a listing of current committee members.
[]
A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each
2012 34th International Conference on Software Engineering
None
2012
There are more bugs in real-world programs than human programmers can realistically address. This paper evaluates two research questions: &#x201C;What fraction of bugs can be repaired automatically?&#x201D; and &#x201C;How much does it cost to repair a bug automatically?&#x201D; In previous work, we presented GenProg, which uses genetic programming to repair defects in off-the-shelf C programs. To answer these questions, we: (1) propose novel algorithmic improvements to GenProg that allow it to scale to large programs and find repairs 68% more often, (2) exploit GenProg's inherent parallelism using cloud computing resources to provide grounded, human-competitive cost measurements, and (3) generate a large, indicative benchmark set to use for systematic evaluations. We evaluate GenProg on 105 defects from 8 open-source programs totaling 5.1 million lines of code and involving 10,193 test cases. GenProg automatically repairs 55 of those 105 defects. To our knowledge, this evaluation is the largest available of its kind, and is often two orders of magnitude larger than previous work in terms of code or test suite size or defect count. Public cloud computing prices allow our 105 runs to be reproduced for $403; a successful repair completes in 96 minutes and costs $7.32, on average.
[Cloud computing, program debugging, GenProg, public domain software, C language, Open source software, Systematics, Genetic programming, Benchmark testing, cloud computing, genetic programming, algorithmic improvement, Maintenance engineering, open-source program, genetic algorithms, software maintenance, defect repair, systematic evaluation, automated program repair, Computer bugs, grounded human-competitive cost measurement, real-world program, software cost estimation, cloud computing resource, program bug, repair cost, off-the-shelf C program]
Where should the bugs be fixed? More accurate information retrieval-based bug localization based on bug reports
2012 34th International Conference on Software Engineering
None
2012
For a large and evolving software system, the project team could receive a large number of bug reports. Locating the source code files that need to be changed in order to fix the bugs is a challenging task. Once a bug report is received, it is desirable to automatically point out to the files that developers should change in order to fix the bug. In this paper, we propose BugLocator, an information retrieval based method for locating the relevant files for fixing a bug. BugLocator ranks all files based on the textual similarity between the initial bug report and the source code using a revised Vector Space Model (rVSM), taking into consideration information about similar bugs that have been fixed before. We perform large-scale experiments on four open source projects to localize more than 3,000 bugs. The results show that BugLocator can effectively locate the files where the bugs should be fixed. For example, relevant buggy files for 62.60% Eclipse 3.1 bugs are ranked in the top ten among 12,863 files. Our experiments also show that BugLocator outperforms existing state-of-the-art bug localization methods.
[program debugging, open source projects, Computational modeling, public domain software, information retrieval, Information retrieval, feature location, Vectors, software quality, rVSM, software system, Equations, revised vector space model, information retrieval-based bug localization, bug localization, source code files, BugLocator, information retrieval based method, Computer bugs, bug reports, textual similarity, Mathematical model, Indexing]
Developer prioritization in bug repositories
2012 34th International Conference on Software Engineering
None
2012
Developers build all the software artifacts in development. Existing work has studied the social behavior in software repositories. In one of the most important software repositories, a bug repository, developers create and update bug reports to support software development and maintenance. However, no prior work has considered the priorities of developers in bug repositories. In this paper, we address the problem of the developer prioritization, which aims to rank the contributions of developers. We mainly explore two aspects, namely modeling the developer prioritization in a bug repository and assisting predictive tasks with our model. First, we model how to assign the priorities of developers based on a social network technique. Three problems are investigated, including the developer rankings in products, the evolution over time, and the tolerance of noisy comments. Second, we consider leveraging the developer prioritization to improve three predicted tasks in bug repositories, i.e., bug triage, severity identification, and reopened bug prediction. We empirically investigate the performance of our model and its applications in bug repositories of Eclipse and Mozilla. The results indicate that the developer prioritization can provide the knowledge of developer priorities to assist software tasks, especially the task of bug triage.
[program debugging, Mozilla, Noise, Communities, software repository, Eclipse, Programming, database management systems, developer prioritization, bug triage, software evolution, bug reports, social network technique, severity identification, reopened bug prediction, software development, Social network services, social behavior, bug repository, developer ranking, Noise measurement, software maintenance, Computer bugs, Software, predictive task, Internet]
WhoseFault: Automatic developer-to-fault assignment through fault localization
2012 34th International Conference on Software Engineering
None
2012
This paper describes a new technique, which automatically selects the most appropriate developers for fixing the fault represented by a failing test case, and provides a diagnosis of where to look for the fault. This technique works by incorporating three key components: (1) fault localization to inform locations whose execution correlate with failure, (2) history mining to inform which developers edited each line of code and when, and (3) expertise assignment to map locations to developers. To our knowledge, the technique is the first to assign developers to execution failures, without the need for textual bug reports. We implement this technique in our tool, WHOSEFAULT, and describe an experiment where we utilize a large, open-source project to determine the frequency in which our tool suggests an assignment to the actual developer who fixed the fault. Our results show that 81% of the time, WHOSEFAULT produced the same developer that actually fixed the fault within the top three suggestions. We also show that our technique improved by a difference between 4% and 40% the results of a baseline technique. Finally, we explore the influence of each of the three components of our technique over its results, and compare our expertise algorithm against an existing expertise assessment technique and find that our algorithm provides greater accuracy, by up to 37%.
[Measurement, Correlation, fault diagnosis, program testing, public domain software, automatic developer-to-fault assignment, data mining, mining software repositories, execution failure, History, Data mining, failing test case, history mining, developer assignment, fault fixing, location mapping, WHOSEFAULT, Informatics, program diagnostics, Software algorithms, fault localization, expertise assignment, software fault tolerance, Software, open-source project]
Generating range fixes for software configuration
2012 34th International Conference on Software Engineering
None
2012
To prevent ill-formed configurations, highly configurable software often allows defining constraints over the available options. As these constraints can be complex, fixing a configuration that violates one or more constraints can be challenging. Although several fix-generation approaches exist, their applicability is limited because (1) they typically generate only one fix, failing to cover the solution that the user wants; and (2) they do not fully support non-Boolean constraints, which contain arithmetic, inequality, and string operators. This paper proposes a novel concept, range fix, for software configuration. A range fix specifies the options to change and the ranges of values for these options. We also design an algorithm that automatically generates range fixes for a violated constraint. We have evaluated our approach with three different strategies for handling constraint interactions, on data from five open source projects. Our evaluation shows that, even with the most complex strategy, our approach generates complete fix lists that are mostly short and concise, in a fraction of a second.
[Algorithm design and analysis, arithmetic operator, Navigation, constraint interaction, nonBoolean constraint, software configuration, range fixes, configuration management, Reactive power, Linux, Semantics, fix-generation approach, inequality operator, Software, Concrete, string operator]
Graph-based pattern-oriented, context-sensitive source code completion
2012 34th International Conference on Software Engineering
None
2012
Code completion helps improve developers' programming productivity. However, the current support for code completion is limited to context-free code templates or a single method call of the variable on focus. Using software libraries for development, developers often repeat API usages for certain tasks. Thus, a code completion tool could make use of API usage patterns. In this paper, we introduce GraPacc, a graph-based, pattern-oriented, context-sensitive code completion approach that is based on a database of such patterns. GraPacc represents and manages the API usage patterns of multiple variables, methods, and control structures via graph-based models. It extracts the context-sensitive features from the code under editing, e.g. the API elements on focus and their relations to other code elements. Those features are used to search and rank the patterns that are most fitted with the current code. When a pattern is selected, the current code will be completed via a novel graph-based code completion algorithm. Empirical evaluation on several real-world systems shows that GraPacc has a high level of accuracy in code completion.
[Context, application program interfaces, source coding, single method call, context-sensitive feature extraction, graph theory, API usage pattern, developer programming productivity improvement, software libraries, API elements, Databases, Layout, feature extraction, API usage patterns, graph-based pattern-oriented context-sensitive source code completion, GraPacc, Feature extraction, Libraries, context-free code templates, code elements, Pattern matching, Graphical user interfaces, pattern-based code completion]
Automatic input rectification
2012 34th International Conference on Software Engineering
None
2012
We present a novel technique, automatic input rectification, and a prototype implementation, SOAP. SOAP learns a set of constraints characterizing typical inputs that an application is highly likely to process correctly. When given an atypical input that does not satisfy these constraints, SOAP automatically rectifies the input (i.e., changes the input so that it satisfies the learned constraints). The goal is to automatically convert potentially dangerous inputs into typical inputs that the program is highly likely to process correctly. Our experimental results show that, for a set of benchmark applications (Google Picasa, ImageMagick, VLC, Swfdec, and Dillo), this approach effectively converts malicious inputs (which successfully exploit vulnerabilities in the application) into benign inputs that the application processes correctly. Moreover, a manual code analysis shows that, if an input does satisfy the learned constraints, it is incapable of exploiting these vulnerabilities. We also present the results of a user study designed to evaluate the subjective perceptual quality of outputs from benign but atypical inputs that have been automatically rectified by SOAP to conform to the learned constraints. Specifically, we obtained benign inputs that violate learned constraints, used our input rectifier to obtain rectified inputs, then paid Amazon Mechanical Turk users to provide their subjective qualitative perception of the difference between the outputs from the original and rectified inputs. The results indicate that rectification can often preserve much, and in many cases all, of the desirable data in the original input.
[image processing, automatic input rectification, Amazon mechanical turk users, manual code analysis, benchmark application, Dillo, Google Picasa, Simple object access protocol, Security, VLC, SOAP, Engines, Videos, Training, security of data, subjective qualitative perception, Safety, Internet, ImageMagick, Monitoring, Swfdec]
Overcoming the challenges in cost estimation for distributed software projects
2012 34th International Conference on Software Engineering
None
2012
We describe how we studied, in-situ, the operational processes of three large high process maturity distributed software development companies and discovered three common problems they faced with respect to early stage project cost estimation. We found that project managers faced significant challenges to accurately estimate project costs because the standard metrics-based estimation tools they used (a) did not effectively incorporate diverse distributed project configurations and characteristics, (b) required comprehensive data that was not fully available for all starting projects, and (c) required significant domain experience to derive accurate estimates. To address these challenges, we collaborated with practitioners at the three firms and developed a new learning-oriented and semi-automated early-stage cost estimation solution that was specifically designed for globally distributed software projects. The key idea of our solution was to augment the existing metrics-driven estimation methods with a case repository that stratified past incidents related to project effort estimation issues from the historical project databases at the firms into several generalizable categories. This repository allowed project managers to quickly and effectively &#x201C;benchmark&#x201D; their new projects to all past projects across the firms, and thereby learn from them. We deployed our solution at each of our three research sites for real-world field-testing over a period of six months. Project managers of 219 new large globally distributed projects used both our method to estimate the cost of their projects as well as the established metrics-based estimation approaches they were used to. Our approach achieved significantly reduced estimation errors (of up to 60%). This resulted in more than 20% net cost savings, on average, per project - a massive total cost savings across all projects at the three firms!
[Measurement, learning-oriented early-stage cost estimation, project management, software engineering economics, analogies, globally distributed software projects, Estimation, Companies, Cognition, learning, early stage project cost estimation, project effort estimation, cost estimation, Databases, Globally distributed software development, historical project databases, Benchmark testing, standard metrics-based estimation tools, case-based reasoning, Software, software cost estimation, distributed software development companies, software metrics, semiautomated early-stage cost estimation]
Characterizing logging practices in open-source software
2012 34th International Conference on Software Engineering
None
2012
Software logging is a conventional programming practice. While its efficacy is often important for users and developers to understand what have happened in the production run, yet software logging is often done in an arbitrary manner. So far, there have been little study for understanding logging practices in real world software. This paper makes the first attempt (to the best of our knowledge) to provide a quantitative characteristic study of the current log messages within four pieces of large open-source software. First, we quantitatively show that software logging is pervasive. By examining developers' own modifications to the logging code in the revision history, we find that they often do not make the log messages right in their first attempts, and thus need to spend a significant amount of efforts to modify the log messages as after-thoughts. Our study further provides several interesting findings on where developers spend most of their efforts in modifying the log messages, which can give insights for programmers, tool developers, and language and compiler designers to improve the current logging practice. To demonstrate the benefit of our study, we built a simple checker based on one of our findings and effectively detected 138 pieces of new problematic logging code from studied software (24 of them are already confirmed and fixed by developers).
[Printing, public domain software, software logging, log message, Programming, empirical study, checker, Servers, History, program compilers, Open source software, programming practice, problematic logging code, log quality, open-source software, tool developer, Production, logging practice characterization, system monitoring, failure diagnosis, software tools, language designer, compiler designer]
The impacts of software process improvement on developers: A systematic review
2012 34th International Conference on Software Engineering
None
2012
This paper presents the results of a systematic review on the impacts of Software Process Improvement (SPI) on developers. This review selected 26 studies from the highest quality journals, conferences, and workshop in the field. The results were compiled and organized following the grounded theory approach. Results from the grounded theory were further categorized using the Ishikawa (or fishbone) diagram. The Ishikawa Diagram models all the factors potentially impacting software developers, and shows both the positive and negative impacts. Positive impacts include a reduction in the number of crises, and an increase in team communications and morale, as well as better requirements and documentation. Negative impacts include increased overhead on developers through the need to collect data and compile documentation, an undue focus on technical approaches, and the fact that SPI is oriented toward management and process quality, and not towards developers and product quality. This systematic review should support future practice through the identification of important obstacles and opportunities for achieving SPI success. Future research should also benefit from the problems and advantages of SPI identified by developers.
[Conferences, Cause effect analysis, software developers, software process improvement impact, SPI, crisis number reduction, Systematics, software process improvement, Ishikawa diagram, cause-effect analysis, grounded theory approach, requirements, positive impacts, Documentation, impact, systematic review, team morale, data collection, Standards, negative impacts, technical approach, team communications, documentation compiling, Software, developer, software process]
Combining functional and imperative programming for multicore software: An empirical study evaluating Scala and Java
2012 34th International Conference on Software Engineering
None
2012
Recent multi-paradigm programming languages combine functional and imperative programming styles to make software development easier. Given today's proliferation of multicore processors, parallel programmers are supposed to benefit from this combination, as many difficult problems can be expressed more easily in a functional style while others match an imperative style. Due to a lack of empirical evidence from controlled studies, however, important software engineering questions are largely unanswered. Our paper is the first to provide thorough empirical results by using Scala and Java as a vehicle in a controlled comparative study on multicore software development. Scala combines functional and imperative programming while Java focuses on imperative shared-memory programming. We study thirteen programmers who worked on three projects, including an industrial application, in both Scala and Java. In addition to the resulting 39 Scala programs and 39 Java programs, we obtain data from an industry software engineer who worked on the same project in Scala. We analyze key issues such as effort, code, language usage, performance, and programmer satisfaction. Contrary to popular belief, the functional style does not lead to bad performance. Average Scala run-times are comparable to Java, lowest run-times are sometimes better, but Java scales better on parallel hardware. We confirm with statistical significance Scala's claim that Scala code is more compact than Java code, but clearly refute other claims of Scala on lower programming effort and lower debugging effort. Our study also provides explanations for these observations and shows directions on how to improve multi-paradigm languages in the future.
[Java, Multicore processing, multiprocessing programs, multicore software development, functional programming, imperative shared-memory programming, Scala, Debugging, language usage, parallel programmers, programmer satisfaction, parallel programming, multiparadigm programming languages, Parallel programming, imperative programming, Software, software engineering, Testing]
Uncovering performance problems in Java applications with reference propagation profiling
2012 34th International Conference on Software Engineering
None
2012
Many applications suffer from run-time bloat: excessive memory usage and work to accomplish simple tasks. Bloat significantly affects scalability and performance, and exposing it requires good diagnostic tools. We present a novel analysis that profiles the run-time execution to help programmers uncover potential performance problems. The key idea of the proposed approach is to track object references, starting from object creation statements, through assignment statements, and eventually statements that perform useful operations. This propagation is abstracted by a representation we refer to as a reference propagation graph. This graph provides path information specific to reference producers and their run-time contexts. Several client analyses demonstrate the use of reference propagation profiling to uncover runtime inefficiencies. We also present a study of the properties of reference propagation graphs produced by profiling 36 Java programs. Several cases studies discuss the inefficiencies identified in some of the analyzed programs, as well as the significant improvements obtained after code optimizations.
[Context, Java, excessive memory usage, code optimizations, program diagnostics, graph theory, object creation statements, reference propagation profiling, Receivers, run-time execution profiling, Java applications, run-time bloat, Data structures, reference propagation graph, Vectors, object reference tracking, Optimization, performance problem uncovering, Resource management, assignment statements]
Performance debugging in the large via mining millions of stack traces
2012 34th International Conference on Software Engineering
None
2012
Given limited resource and time before software release, development-site testing and debugging become more and more insufficient to ensure satisfactory software performance. As a counterpart for debugging in the large pioneered by the Microsoft Windows Error Reporting (WER) system focusing on crashing/hanging bugs, performance debugging in the large has emerged thanks to available infrastructure support to collect execution traces with performance issues from a huge number of users at the deployment sites. However, performance debugging against these numerous and complex traces remains a significant challenge for performance analysts. In this paper, to enable performance debugging in the large in practice, we propose a novel approach, called StackMine, that mines callstack traces to help performance analysts effectively discover highly impactful performance bugs (e.g., bugs impacting many users with long response delay). As a successful technology-transfer effort, since December 2010, StackMine has been applied in performance-debugging activities at a Microsoft team for performance analysis, especially for a large number of execution traces. Based on real-adoption experiences of StackMine in practice, we conducted an evaluation of StackMine on performance debugging in the large for Microsoft Windows 7. We also conducted another evaluation on a third-party application. The results highlight substantial benefits offered by StackMine in performance debugging in the large for large-scale software systems.
[program debugging, Microsoft Windows 7, callstack trace mining, program testing, data mining, performance debugging, hanging bugs, Data mining, Delay, large-scale software system, crashing bugs, development-site testing, Performance analysis, Microsoft Windows error reporting system, software performance evaluation, program diagnostics, Debugging, performance bugs, third-party application, software release, Computer bugs, technology-transfer effort, StackMine, Software systems, operating systems (computers), performance analysis]
Automatically finding performance problems with feedback-directed learning software testing
2012 34th International Conference on Software Engineering
None
2012
A goal of performance testing is to find situations when applications unexpectedly exhibit worsened characteristics for certain combinations of input values. A fundamental question of performance testing is how to select a manageable subset of the input data faster to find performance problems in applications automatically. We offer a novel solution for finding performance problems in applications automatically using black-box software testing. Our solution is an adaptive, feedback-directed learning testing system that learns rules from execution traces of applications and then uses these rules to select test input data automatically for these applications to find more performance problems when compared with exploratory random testing. We have implemented our solution and applied it to a medium-size application at a major insurance company and to an open-source application. Performance problems were found automatically and confirmed by experienced testers and developers.
[Software testing, program testing, performance testing, Companies, Matrix decomposition, exploratory random testing, learning systems, execution trace, Databases, adaptive feedback-directed learning software testing system, performance problem, Insurance, manageable subset, open-source application, Graphical user interfaces, black-box software testing, major insurance company]
Predicting performance via automated feature-interaction detection
2012 34th International Conference on Software Engineering
None
2012
Customizable programs and program families provide user-selectable features to allow users to tailor a program to an application scenario. Knowing in advance which feature selection yields the best performance is difficult because a direct measurement of all possible feature combinations is infeasible. Our work aims at predicting program performance based on selected features. However, when features interact, accurate predictions are challenging. An interaction occurs when a particular feature combination has an unexpected influence on performance. We present a method that automatically detects performance-relevant feature interactions to improve prediction accuracy. To this end, we propose three heuristics to reduce the number of measurements required to detect interactions. Our evaluation consists of six real-world case studies from varying domains (e.g., databases, encoding libraries, and web servers) using different configuration techniques (e.g., configuration files and preprocessor flags). Results show an average prediction accuracy of 95%.
[program performance prediction, customizable programs, Educational institutions, Generators, user-selectable features, Encryption, Indexes, configuration management, configuration techniques, Accuracy, program families, automated performance-relevant feature interaction detection, Feature extraction, feature selection]
Sound empirical evidence in software testing
2012 34th International Conference on Software Engineering
None
2012
Several promising techniques have been proposed to automate different tasks in software testing, such as test data generation for object-oriented software. However, reported studies in the literature only show the feasibility of the proposed techniques, because the choice of the employed artifacts in the case studies (e.g., software applications) is usually done in a non-systematic way. The chosen case study might be biased, and so it might not be a valid representative of the addressed type of software (e.g., internet applications and embedded systems). The common trend seems to be to accept this fact and get over it by simply discussing it in a threats to validity section. In this paper, we evaluate search-based software testing (in particular the EvoSuite tool) when applied to test data generation for open source projects. To achieve sound empirical results, we randomly selected 100 Java projects from SourceForge, which is the most popular open source repository (more than 300,000 projects with more than two million registered users). The resulting case study not only is very large (8,784 public classes for a total of 291,639 bytecode level branches), but more importantly it is statistically sound and representative for open source projects. Results show that while high coverage on commonly used types of classes is achievable, in practice environmental dependencies prohibit such high coverage, which clearly points out essential future research directions. To support this future research, our SF100 case study can serve as a much needed corpus of classes for test generation.
[Software testing, Context, class corpus, Java, open source projects, program testing, public domain software, object-oriented software, test data generation, unit testing, Containers, Security, Java projects, SF100 case study, open source repository, environment, test case generation, search-based software testing, SourceForge, search-based software engineering, Software, object-oriented methods, security exception]
Privacy and utility for defect prediction: Experiments with MORPH
2012 34th International Conference on Software Engineering
None
2012
Ideally, we can learn lessons from software projects across multiple organizations. However, a major impediment to such knowledge sharing are the privacy concerns of software development organizations. This paper aims to provide defect data-set owners with an effective means of privatizing their data prior to release. We explore MORPH which understands how to maintain class boundaries in a data-set. MORPH is a data mutator that moves the data a random distance, taking care not to cross class boundaries. The value of training on this MORPHed data is tested via a 10-way within learning study and a cross learning study using Random Forests, Naive Bayes, and Logistic Regression for ten object-oriented defect datasets from the PROMISE data repository. Measured in terms of exposure of sensitive attributes, the MORPHed data was four times more private than the unMORPHed data. Also, in terms of the f-measures, there was little difference between the MORPHed and unMORPHed data (original data and data privatized by data-swapping) for both the cross and within study. We conclude that at least for the kinds of OO defect data studied in this project, data can be privatized without concerns for inference efficacy.
[Data privacy, privacy concerns, knowledge sharing, data mining, Companies, regression analysis, Predictive models, data mutator, random forests, privacy, MORPH, f-measures, Privacy, defect prediction, software engineering, software projects, logistic regression, object-oriented programming, class boundaries, trees (mathematics), object-oriented defect datasets, software development organizations, random distance, PROMISE data repository, Privatization, naive Bayes, Software, data privacy, Bayes methods]
Bug prediction based on fine-grained module histories
2012 34th International Conference on Software Engineering
None
2012
There have been many bug prediction models built with historical metrics, which are mined from version histories of software modules. Many studies have reported the effectiveness of these historical metrics. For prediction levels, most studies have targeted package and file levels. Prediction on a fine-grained level, which represents the method level, is required because there may be interesting results compared to coarse-grained (package and file levels) prediction. These results include good performance when considering quality assurance efforts, and new findings about the correlations between bugs and histories. However, fine-grained prediction has been a challenge because obtaining method histories from existing version control systems is a difficult problem. To tackle this problem, we have developed a fine-grained version control system for Java, Historage. With this system, we target Java software and conduct fine-grained prediction with well-known historical metrics. The results indicate that fine-grained (method-level) prediction outperforms coarse-grained (package and file levels) prediction when taking the efforts necessary to find bugs into account. Using a correlation analysis, we show that past bug information does not contribute to method-level bug prediction.
[Measurement, program debugging, fine-grained module histories, Predictive models, Complexity theory, software quality, effort-based evaluation, History, finegrained histories, correlation analysis, prediction levels, bug prediction models, Java software, version control systems, fine-grained version control system, file levels, bug information, fine-grained level, Java, fine-grained prediction, quality assurance efforts, historical metrics, software modules, bug prediction, version history, Historage, method-level bug prediction, coarse-grained prediction, Computer bugs, quality assurance, method-level prediction, Software, correlation methods]
Reconciling manual and automatic refactoring
2012 34th International Conference on Software Engineering
None
2012
Although useful and widely available, refactoring tools are underused. One cause of this underuse is that a developer sometimes fails to recognize that she is going to refactor before she begins manually refactoring. To address this issue, we conducted a formative study of developers' manual refactoring process, suggesting that developers' reliance on &#x201C;chasing error messages&#x201D; when manually refactoring is an error-prone manual refactoring strategy. Additionally, our study distilled a set of manual refactoring workflow patterns. Using these patterns, we designed a novel refactoring tool called BeneFactor. BeneFactor detects a developer's manual refactoring, reminds her that automatic refactoring is available, and can complete her refactoring automatically. By alleviating the burden of recognizing manual refactoring, BeneFactor is designed to help solve the refactoring tool underuse problem.
[Java, chasing error message, refactoring tool underuse problem, Manuals, error-prone manual refactoring strategy, Software reliability, software maintenance, manual refactoring workflow pattern, Videos, BeneFactor, automatic refactoring, Software, software tools]
WitchDoctor: IDE support for real-time auto-completion of refactorings
2012 34th International Conference on Software Engineering
None
2012
Integrated Development Environments (IDEs) have come to perform a wide variety of tasks on behalf of the programmer, refactoring being a classic example. These operations have undeniable benefits, yet their large (and growing) number poses a cognitive scalability problem. Our main contribution is WitchDoctor - a system that can detect, on the fly, when a programmer is hand-coding a refactoring. The system can then complete the refactoring in the background and propose it to the user long before the user can complete it. This implies a number of technical challenges. The algorithm must be 1) highly efficient, 2) handle unparseable programs, 3) tolerate the variety of ways programmers may perform a given refactoring, 4) use the IDE's proven and familiar refactoring engine to perform the refactoring, even though the the refactoring has already begun, and 5) support the wide range of refactorings present in modern IDEs. Our techniques for overcoming these challenges are the technical contributions of this paper. We evaluate WitchDoctor's design and implementation by simulating over 5,000 refactoring operations across three open-source projects. The simulated user is faster and more efficient than an average human user, yet WitchDoctor can detect more than 90% of refactoring operations as they are being performed - and can complete over a third of refactorings before the simulated user does. All the while, WitchDoctor remains robust in the face of non-parseable programs and unpredictable refactoring scenarios. We also show that WitchDoctor is efficient enough to perform computation on a keystroke-by-keystroke basis, adding an average overhead of only 15 milliseconds per keystroke.
[Real time systems, Context, unparseable program handling, public domain software, cognitive scalability problem, integrated development environments, refactoring real-time autocompletion, IDE, History, Data mining, software maintenance, familiar refactoring engine, open-source projects, WitchDoctor, Engines, repository mining, keystroke-by-keystroke basis, refactoring, Syntactics, IDE proven, programmer way toleration, change detection, Software, IDE support]
Use, disuse, and misuse of automated refactorings
2012 34th International Conference on Software Engineering
None
2012
Though refactoring tools have been available for more than a decade, research has shown that programmers underutilize such tools. However, little is known about why programmers do not take advantage of these tools. We have conducted a field study on programmers in their natural settings working on their code. As a result, we collected a set of interaction data from about 1268 hours of programming using our minimally intrusive data collectors. Our quantitative data show that programmers prefer lightweight methods of invoking refactorings, usually perform small changes using the refactoring tool, proceed with an automated refactoring even when it may change the behavior of the program, and rarely preview the automated refactorings. We also interviewed nine of our participants to provide deeper insight about the patterns that we observed in the behavioral data. We found that programmers use predictable automated refactorings even if they have rare bugs or change the behavior of the program. This paper reports some of the factors that affect the use of automated refactorings such as invocation method, awareness, naming, trust, and predictability and the major mismatches between programmers' expectations and automated refactorings. The results of this work contribute to producing more effective tools for refactoring complex software.
[Context, trust, Software maintenance, Human factors, Programming, Educational institutions, predictable automated refactorings, software maintenance, predictability, invocation method, refactoring tools, Programming environments, Human computer interaction, behavioral data, awareness, interaction data, naming, User interfaces, Software, lightweight methods, software tools, Reliability, Interviews, Software engineering]
Test confessions: A study of testing practices for plug-in systems
2012 34th International Conference on Software Engineering
None
2012
Testing plug-in-based systems is challenging due to complex interactions among many different plug-ins, and variations in version and configuration. The objective of this paper is to increase our understanding of what testers and developers think and do when it comes to testing plug-in-based systems. To that end, we conduct a qualitative (grounded theory) study, in which we interview 25 senior practitioners about how they test plug-in applications based on the Eclipse plug-in architecture. The outcome is an overview of the testing practices currently used, a set of identified barriers limiting test adoption, and an explanation of how limited testing is compensated by self-hosting of projects and by involving the community. These results are supported by a structured survey of more than 150 professionals. The study reveals that unit testing plays a key role, whereas plug-in specific integration problems are identified and resolved by the community. Based on our findings, we propose a series of recommendations and areas for future research.
[practice testing, open source software development, program testing, public domain software, test confession, Communities, Eclipse, Manuals, Companies, grounded theory, unit testing, plug-in application testing, limited testing, plug-in architectures, Computer architecture, software engineering, plug-in-based system testing, Interviews, Eclipse plug-in architecture, plug-in specific integration problem, Testing, Graphical user interfaces]
How do professional developers comprehend software?
2012 34th International Conference on Software Engineering
None
2012
Research in program comprehension has considerably evolved over the past two decades. However, only little is known about how developers practice program comprehension under time and project pressure, and which methods and tools proposed by researchers are used in industry. This paper reports on an observational study of 28 professional developers from seven companies, investigating how developers comprehend software. In particular we focus on the strategies followed, information needed, and tools used. We found that developers put themselves in the role of end users by inspecting user interfaces. They try to avoid program comprehension, and employ recurring, structured comprehension strategies depending on work context. Further, we found that standards and experience facilitate comprehension. Program comprehension was considered a subtask of other maintenance tasks rather than a task by itself. We also found that face-to-face communication is preferred to documentation. Overall, our results show a gap between program comprehension research and practice as we did not observe any use of state of the art comprehension tools and developers seem to be unaware of them. Our findings call for further careful analysis and for reconsidering research agendas.
[program comprehension, Java, Visualization, Content management, Companies, Documentation, work context, reverse engineering, structured comprehension strategies, user interfaces, software maintenance, face-to-face communication, empirical studies, maintenance tasks, project pressure, professional developers, comprehension tools, Software, context awareness, Interviews, maintenance, software documentation]
Asking and answering questions about unfamiliar APIs: An exploratory study
2012 34th International Conference on Software Engineering
None
2012
The increasing size of APIs and the increase in the number of APIs available imply developers must frequently learn how to use unfamiliar APIs. To identify the types of questions developers want answered when working with unfamiliar APIs and to understand the difficulty they may encounter answering those questions, we conducted a study involving twenty programmers working on different programming tasks, using unfamiliar APIs. Based on the screen captured videos and the verbalization of the participants, we identified twenty different types of questions programmers ask when working with unfamiliar APIs, and provide new insights to the cause of the difficulties programmers encounter when answering questions about the use of APIs. The questions we have identified and the difficulties we observed can be used for evaluating tools aimed at improving API learning, and in identifying areas of the API learning process where tool support is missing, or could be improved.
[unfamiliar API, Navigation, application program interfaces, question answering, Documentation, API learning process, Programming, Production facilities, Videos, question asking, programming tasks, XML, Usability, tool evaluation]
Automated repair of HTML generation errors in PHP applications using string constraint solving
2012 34th International Conference on Software Engineering
None
2012
PHP web applications routinely generate invalid HTML. Modern browsers silently correct HTML errors, but sometimes malformed pages render inconsistently, cause browser crashes, or expose security vulnerabilities. Fixing errors in generated pages is usually straightforward, but repairing the generating PHP program can be much harder. We observe that malformed HTML is often produced by incorrect constant prints, i.e., statements that print string literals, and present two tools for automatically repairing such HTML generation errors. PHPQuickFix repairs simple bugs by statically analyzing individual prints. PHPRepair handles more general repairs using a dynamic approach. Based on a test suite, the property that all tests should produce their expected output is encoded as a string constraint over variables representing constant prints. Solving this constraint describes how constant prints must be modified to make all tests pass. Both tools were implemented as an Eclipse plugin and evaluated on PHP programs containing hundreds of HTML generation errors, most of which our tools were able to repair automatically.
[program debugging, program testing, constant prints, dynamic approach, HTML, automated repair, string constraints, PHP Web applications, bug repairing, Databases, authoring languages, USA Councils, PHP, constraint handling, string constraint solving, Cascading style sheets, hypermedia markup languages, PHPRepair, program diagnostics, HTML generation error automated repair, Maintenance engineering, individual print static analysis, Browsers, PHPQuickFix, Computer bugs, Eclipse plugin, Internet]
Leveraging test generation and specification mining for automated bug detection without false positives
2012 34th International Conference on Software Engineering
None
2012
Mining specifications and using them for bug detection is a promising way to reveal bugs in programs. Existing approaches suffer from two problems. First, dynamic specification miners require input that drives a program to generate common usage patterns. Second, existing approaches report false positives, that is, spurious warnings that mislead developers and reduce the practicability of the approach. We present a novel technique for dynamically mining and checking specifications without relying on existing input to drive a program and without reporting false positives. Our technique leverages automatically generated tests in two ways: Passing tests drive the program during specification mining, and failing test executions are checked against the mined specifications. The output are warnings that show with concrete test cases how the program violates commonly accepted specifications. Our implementation reports no false positives and 54 true positives in ten well-tested Java programs.
[Java, program debugging, Protocols, program testing, data mining, Receivers, dynamic specification miner, Generators, checking specification, Bug detection, Specification mining, specification mining, formal specification, false positives, usage pattern, Runtime, formal verification, automated bug detection, Computer bugs, Java program, False positives, Concrete, leveraging test generation]
Axis: Automatically fixing atomicity violations through solving control constraints
2012 34th International Conference on Software Engineering
None
2012
Atomicity, a general correctness criterion in concurrency programs, is often violated in real-world applications. The violations are difficult for developers to fix, making automatic bug fixing techniques attractive. The state of the art approach aims at automating the manual fixing process but cannot provide any theoretical reasoning and guarantees. We provide an automatic approach that applies well-studied discrete control theory to guarantee deadlocks are not introduced and maximal preservation of the concurrency of the original code. Under the hood, we reduce the problem of violation fixing to a constraint solving problem using the Petri net model. Our evaluation on 13 subjects shows that the slowdown incurred by our patches is only 40% of that of the state of the art. With the deadlock-free guarantee, our patches incur moderate overhead (around 10%), which is a worthwhile cost for safety.
[program debugging, automatic bug fixing techniques, Petri nets, control constraints solving, Vectors, Cognition, atomicity violation automatic fixing, general correctness criterion, Equations, Concurrent computing, deadlock-free guarantee, Petri net model, Computer bugs, concurrency control, concurrency programs, System recovery, Mathematical model, discrete control theory, Axis]
CBCD: Cloned buggy code detector
2012 34th International Conference on Software Engineering
None
2012
Developers often copy, or clone, code in order to reuse or modify functionality. When they do so, they also clone any bugs in the original code. Or, different developers may independently make the same mistake. As one example of a bug, multiple products in a product line may use a component in a similar wrong way. This paper makes two contributions. First, it presents an empirical study of cloned buggy code. In a large industrial product line, about 4% of the bugs are duplicated across more than one product or file. In three open source projects (the Linux kernel, the Git version control system, and the PostgreSQL database) we found 282, 33, and 33 duplicated bugs, respectively. Second, this paper presents a tool, CBCD, that searches for code that is semantically identical to given buggy code. CBCD tests graph isomorphism over the Program Dependency Graph (PDG) representation and uses four optimizations. We evaluated CBCD by searching for known clones of buggy code segments in the three projects and compared the results with text-based, token-based, and AST-based code clone detectors, namely Simian, CCFinder, Deckard, and CloneDR. The evaluation shows that CBCD is fast when searching for possible clones of the buggy code in a large system, and it is more precise for this purpose than the other code clone detectors.
[program debugging, public domain software, graph theory, Linux kernel, AST-based code clone detectors, CBCD tests graph isomorphism, Complexity theory, Deckard, Optimization, PDG, text-based code clone detectors, Detectors, CloneDR, token-based code clone detectors, PostgreSQL database, Kernel, CBCD, Validation, operating system kernels, open source projects, buggy code segments, Simian, Cloning, cloned buggy code detector, SQL, configuration management, Git version control system, Linux, Computer bugs, program dependency graph representation, industrial product line, CCFinder, Debugging aids]
Crosscutting revision control system
2012 34th International Conference on Software Engineering
None
2012
Large and medium scale software projects often require a source code revision control (RC) system. Unfortunately, RC systems do not perform well with obliviousness and quantification found in aspect-oriented code. When classes are oblivious to aspects, so is the RC system, and the crosscutting effect of aspects is not tracked. In this work, we study this problem in the context of using AspectJ (a standard AOP language) with Subversion (a standard RC system). We describe scenarios where the crosscutting effect of aspects combined with the concurrent changes that RC supports can lead to inconsistent states of the code. The work contributes a mechanism that checks-in with the source code versions of crosscutting metadata for tracking the effect of aspects. Another contribution of this work is the implementation of a supporting Eclipse plug-in (named XRC) that extends the JDT, AJDT, and SVN plug-ins for Eclipse to provide crosscutting revision control (XRC) for aspect-oriented programming.
[RC system, Visualization, source code revision control, Subversion, Programming, XRC, aspects, Control systems, large and medium scale software project, History, Engines, source code version, aspect-oriented programming, SVN plug-in, crosscutting revision control system, AspectJ, Java, concurrent change, AJDT plug-in, version control, aspect-oriented code, Eclipse plug-in, software maintenance, revision control, crosscutting effect, configuration management, crosscutting metadata, Weaving, Software, standard AOP language]
Where does this code come from and where does it go? &#x2014; Integrated code history tracker for open source systems
2012 34th International Conference on Software Engineering
None
2012
When we reuse a code fragment in an open source system, it is very important to know the history of the code, such as the code origin and evolution. In this paper, we propose an integrated approach to code history tracking for open source repositories. This approach takes a query code fragment as its input, and returns the code fragments containing the code clones with the query code. It utilizes publicly available code search engines as external resources. Based on this model, we have designed and implemented a prototype system named Ichi Tracker. Using Ichi Tracker, we have conducted three case studies. These case studies show the ancestors and descendents of the code, and we can recognize their evolution history.
[Software Evolution, search engines, publicly available code search engines, public domain software, open source repositories, Licenses, History, Ichi Tracker, Engines, Strontium, Code Search, Search engines, Google, Cloning, Open Source System, software maintenance, code descendents, code clones, code ancestors, integrated code history tracker, query code fragment reusability, software reusability, code evolution, code origin, open source systems]
Improving early detection of software merge conflicts
2012 34th International Conference on Software Engineering
None
2012
Merge conflicts cause software defects which if detected late may require expensive resolution. This is especially true when developers work too long without integrating concurrent changes, which in practice is common as integration generally occurs at check-in. Awareness of others' activities was proposed to help developers detect conflicts earlier. However, it requires developers to detect conflicts by themselves and may overload them with notifications, thus making detection harder. This paper presents a novel solution that continuously merges uncommitted and committed changes to create a background system that is analyzed, compiled, and tested to precisely and accurately detect conflicts on behalf of developers, before check-in. An empirical study confirms that our solution avoids overloading developers and improves early detection of conflicts over existing approaches. Similarly to what happened with continuous compilation, this introduces the case for continuous merging inside the IDE.
[Merging, background system, version control, Programming, other activity awareness, IDE, Servers, software maintenance, early detection improvement, software merge conflicts, configuration management, Computer languages, continuous merging, Animals, conflict detection, awareness, Semantics, integrated development environment, software defects, uncommitted change merging, merge conflicts, Software]
A history-based matching approach to identification of framework evolution
2012 34th International Conference on Software Engineering
None
2012
In practice, it is common that a framework and its client programs evolve simultaneously. Thus, developers of client programs may need to migrate their programs to the new release of the framework when the framework evolves. As framework developers can hardly always guarantee backward compatibility during the evolution of a framework, migration of its client program is often time-consuming and error-prone. To facilitate this migration, researchers have proposed two categories of approaches to identification of framework evolution: operation-based approaches and matching-based approaches. To overcome the main limitations of the two categories of approaches, we propose a novel approach named HiMa, which is based on matching each pair of consecutive revisions recorded in the evolution history of the framework and aggregating revision-level rules to obtain framework-evolution rules. We implemented our HiMa approach as an Eclipse plug-in targeting at frameworks written in Java using SVN as the version-control system. We further performed an experimental study on HiMa together with a state-of-art approach named AURA using six tasks based on three subject Java frameworks. Our experimental results demonstrate that HiMa achieves higher precision and higher recall than AURA in most circumstances and is never inferior to AURA in terms of precision and recall in any circumstances, although HiMa is computationally more costly than AURA.
[version-control system, Control systems, SVN, History, framework-evolution rules, HiMa approach, client program developer, Natural language processing, program migration, Java, operation-based approach, backward compatibility, Java frameworks, mining version history, natural language processing, framework evolution, software migration, Eclipse plug-in, history-based matching approach, software maintenance, framework evolution identification, consecutive revision pair matching, revision-level rules aggregation, Aggregates, Software, AURA]
Detecting similar software applications
2012 34th International Conference on Software Engineering
None
2012
Although popular text search engines allow users to retrieve similar web pages, source code search engines do not have this feature. Detecting similar applications is a notoriously difficult problem, since it implies that similar highlevel requirements and their low-level implementations can be detected and matched automatically for different applications. We created a novel approach for automatically detecting Closely reLated ApplicatioNs (CLAN) that helps users detect similar applications for a given Java application. Our main contributions are an extension to a framework of relevance and a novel algorithm that computes a similarity index between Java applications using the notion of semantic layers that correspond to packages and class hierarchies. We have built CLAN and we conducted an experiment with 33 participants to evaluate CLAN and compare it with the closest competitive approach, MUDABlue. The results show with strong statistical significance that CLAN automatically detects similar applications from a large repository of 8,310 Java applications with a higher precision than MUDABlue.
[Web page retrieval, closely related application detection, packages hierarchy, Java, semantic layers, similar software application detection, Time division multiplexing, CLAN, Large scale integration, Vectors, text search engines, Java application, Semantics, MUDABlue, class hierarchy, Search engines, Software, software engineering, source code search engines, statistical analysis]
Content classification of development emails
2012 34th International Conference on Software Engineering
None
2012
Emails related to the development of a software system contain information about design choices and issues encountered during the development process. Exploiting the knowledge embedded in emails with automatic tools is challenging, due to the unstructured, noisy, and mixed language nature of this communication medium. Natural language text is often not well-formed and is interleaved with languages with other syntaxes, such as code or stack traces. We present an approach to classify email content at line level. Our technique classifies email lines in five categories (i.e., text, junk, code, patch, and stack trace) to allow one to subsequently apply ad hoc analysis techniques for each category. We evaluated our approach on a statistically significant set of emails gathered from mailing lists of four unrelated open source systems.
[text analysis, code category, public domain software, Noise, electronic mail, Electronic mail, Data mining, content classification, software system development, text category, patch category, Text recognition, ad hoc analysis techniques, software engineering, Emails, Context, Java, pattern classification, stack traces, development emails, Empirical software engineering, natural language processing, code traces, natural language text, Unstructured Data Mining, junk category, Software, open source systems]
Identifying Linux bug fixing patches
2012 34th International Conference on Software Engineering
None
2012
In the evolution of an operating system there is a continuing tension between the need to develop and test new features, and the need to provide a stable and secure execution environment to users. A compromise, adopted by the developers of the Linux kernel, is to release new versions, including bug fixes and new features, frequently, while maintaining some older &#x201C;longterm&#x201D; versions. This strategy raises the problem of how to identify bug fixing patches that are submitted to the current version but should be applied to the longterm versions as well. The current approach is to rely on the individual subsystem maintainers to forward patches that seem relevant to the maintainers of the longterm kernels. The reactivity and diligence of the maintainers, however, varies, and thus many important patches could be missed by this approach. In this paper, we propose an approach that automatically identifies bug fixing patches based on the changes and commit messages recorded in code repositories. We compare our approach with the keyword-based approach for identifying bug-fixing patches used in the literature, in the context of the Linux kernel. The results show that our approach can achieve a 53.19% improvement in recall as compared to keyword-based approaches, with similar precision.
[Context, keyword-based approach, operating system kernels, program debugging, Linux bug fixing patch identification, Data acquisition, operating system evolution, longterm kernel maintainer, software maintenance, Support vector machines, Linux kernel developer, Linux, code repositories, Feature extraction, Data models, individual subsystem maintainers, Kernel]
Active refinement of clone anomaly reports
2012 34th International Conference on Software Engineering
None
2012
Software clones have been widely studied in the recent literature and shown useful for finding bugs because inconsistent changes among clones in a clone group may indicate potential bugs. However, many inconsistent clone groups are not real bugs (true positives). The excessive number of false positives could easily impede broad adoption of clone-based bug detection approaches. In this work, we aim to improve the usability of clone-based bug detection tools by increasing the rate of true positives found when a developer analyzes anomaly reports. Our idea is to control the number of anomaly reports a user can see at a time and actively incorporate incremental user feedback to continually refine the anomaly reports. Our system first presents top few anomaly reports from the list of reports generated by a tool in its default ordering. Users then either accept or reject each of the reports. Based on the feedback, our system automatically and iteratively refines a classification model for anomalies and re-sorts the rest of the reports. Our goal is to present the true positives to the users earlier than the default ordering. The rationale of the idea is based on our observation that false positives among the inconsistent clone groups could share common features (in terms of code structure, programming patterns, etc.), and these features can be learned from the incremental user feedback. We evaluate our refinement process on three sets of clone-based anomaly reports from three large real programs: the Linux Kernel (C), Eclipse, and ArgoUML (Java), extracted by a clone-based anomaly detection tool. The results show that compared to the original ordering of bug reports, we can improve the rate of true positives found (i.e., true positives are found faster) by 11%, 87%, and 86% for Linux kernel, Eclipse, and ArgoUML, respectively.
[operating system kernels, pattern classification, program debugging, Cloning, Linux kernel, Eclipse, ArgoUML, Programming, active refinement, bug finding, Engines, false positives, classification model, software clones, default ordering, clone-based bug detection tool usability, Linux, Computer bugs, clone-based bug detection approach, Feature extraction, true positives, incremental user feedback, Kernel, clone anomaly reports]
Automated analysis of CSS rules to support style maintenance
2012 34th International Conference on Software Engineering
None
2012
CSS is a widely used language for describing the presentation semantics of HTML elements on the web. The language has a number of characteristics, such as inheritance and cascading order, which makes maintaining CSS code a challenging task for web developers. As a result, it is common for unused rules to be accumulated over time. Despite these challenges, CSS analysis has not received much attention from the research community. We propose an automated technique to support styling code maintenance, which (1) analyzes the runtime relationship between the CSS rules and DOM elements of a given web application (2) detects unmatched and ineffective selectors, overridden declaration properties, and undefined class values. Our technique, implemented in an open source tool called Cilla, has a high precision and recall rate. The results of our case study, conducted on fifteen open source and industrial web-based systems, show an average of 60% unused CSS selectors in deployed applications, which points to the ubiquity of the problem.
[Web developers, public domain software, industrial Web-based systems, HTML, Runtime, document object model, open source tool, inheritance order, DOM elements, Cascading style sheets, hypermedia markup languages, unmatched selector detection, style code maintenance, CSS, ineffective selector detection, Color, Maintenance engineering, dynamic analysis, Browsers, software maintenance, web applications, undefined class values, HTML elements presentation semantics, cascading order, Layout, Web application, cascading style sheets, CSS rule automated analysis, Cilla, Internet, overridden declaration properties]
Graph-based analysis and prediction for software evolution
2012 34th International Conference on Software Engineering
None
2012
We exploit recent advances in analysis of graph topology to better understand software evolution, and to construct predictors that facilitate software development and maintenance. Managing an evolving, collaborative software system is a complex and expensive process, which still cannot ensure software reliability. Emerging techniques in graph mining have revolutionized the modeling of many complex systems and processes. We show how we can use a graph-based characterization of a software system to capture its evolution and facilitate development, by helping us estimate bug severity, prioritize refactoring efforts, and predict defect-prone releases. Our work consists of three main thrusts. First, we construct graphs that capture software structure at two different levels: (a) the product, i.e., source code and module level, and (b) the process, i.e., developer collaboration level. We identify a set of graph metrics that capture interesting properties of these graphs. Second, we study the evolution of eleven open source programs, including Firefox, Eclipse, MySQL, over the lifespan of the programs, typically a decade or more. Third, we show how our graph metrics can be used to construct predictors for bug severity, high-maintenance software parts, and failure-prone releases. Our work strongly suggests that using graph topology analysis concepts can open many actionable avenues in software engineering research and practice.
[Measurement, program debugging, public domain software, bug severity estimation, module level, graph theory, software reliability, defect-prone release prediction, software quality, productivity metrics, software evolution, open source program evolution, defect prediction, Fires, groupware, graph-based prediction, software structure, graph topology, evolving collaborative software system, graph-based analysis, software development, source code, Maintenance engineering, Topology, software maintenance, developer collaboration level, empirical studies, graph mining, Graph science, Collaboration, graph metrics, Software, software system graph-based characterization, Software engineering, refactoring effort prioritization]
Integrated impact analysis for managing software changes
2012 34th International Conference on Software Engineering
None
2012
The paper presents an adaptive approach to perform impact analysis from a given change request to source code. Given a textual change request (e.g., a bug report), a single snapshot (release) of source code, indexed using Latent Semantic Indexing, is used to estimate the impact set. Should additional contextual information be available, the approach configures the best-fit combination to produce an improved impact set. Contextual information includes the execution trace and an initial source code entity verified for change. Combinations of information retrieval, dynamic analysis, and data mining of past source code commits are considered. The research hypothesis is that these combinations help counter the precision or recall deficit of individual techniques and improve the overall accuracy. The tandem operation of the three techniques sets it apart from other related solutions. Automation along with the effective utilization of two key sources of developer knowledge, which are often overlooked in impact analysis at the change request level, is achieved. To validate our approach, we conducted an empirical evaluation on four open source software systems. A benchmark consisting of a number of maintenance issues, such as feature requests and bug fixes, and their associated source code changes was established by manual examination of these systems and their change history. Our results indicate that there are combinations formed from the augmented developer contextual information that show statistically significant improvement over standalone approaches.
[public domain software, adaptive approach, latent semantic indexing, developer knowledge source, data mining, software management, Data mining, History, best-fit combination, initial source code entity, open source software systems, textual change request, Automation, indexing, information retrieval, Maintenance engineering, Information retrieval, dynamic analysis, software maintenance, contextual information, integrated impact analysis, Couplings, execution trace, impact set estimation, software change management, management of change, source code single snapshot, system monitoring, maintenance issues, Software]
Detecting and visualizing inter-worksheet smells in spreadsheets
2012 34th International Conference on Software Engineering
None
2012
Spreadsheets are often used in business, for simple tasks, as well as for mission critical tasks such as finance or forecasting. Similar to software, some spreadsheets are of better quality than others, for instance with respect to usability, maintainability or reliability. In contrast with software however, spreadsheets are rarely checked, tested or certified. In this paper, we aim at developing an approach for detecting smells that indicate weak points in a spreadsheet's design. To that end we first study code smells and transform these code smells to their spreadsheet counterparts. We then present an approach to detect the smells, and to communicate located smells to spreadsheet users with data flow diagrams. To evaluate our apporach, we analyzed occurrences of these smells in the Euses corpus. Furthermore we conducted ten case studies in an industrial setting. The results of the evaluation indicate that smells can indeed reveal weaknesses in a spreadsheet's design, and that data flow diagrams are an appropriate way to show those weaknesses.
[Measurement, interworksheet smell detection, Euses corpus, data flow diagrams, data flow analysis, Educational institutions, spreadsheet programs, diagrams, Couplings, spreadsheets, spreadsheet design, code smells, refactoring, Data visualization, Surgery, data visualisation, interworksheet smell visualization, Software engineering, Business]
An empirical study about the effectiveness of debugging when random test cases are used
2012 34th International Conference on Software Engineering
None
2012
Automatically generated test cases are usually evaluated in terms of their fault revealing or coverage capability. Beside these two aspects, test cases are also the major source of information for fault localization and fixing. The impact of automatically generated test cases on the debugging activity, compared to the use of manually written test cases, has never been studied before. In this paper we report the results obtained from two controlled experiments with human subjects performing debugging tasks using automatically generated or manually written test cases. We investigate whether the features of the former type of test cases, which make them less readable and understandable (e.g., unclear test scenarios, meaningless identifiers), have an impact on accuracy and efficiency of debugging. The empirical study is aimed at investigating whether, despite the lack of readability in automatically generated test cases, subjects can still take advantage of them during debugging.
[Measurement, program debugging, test case automatic generation, program testing, Automatic Test Case Generation, Debugging, Manuals, random test cases, Complexity theory, Training, Accuracy, Empirical Software Engineering, debugging effectiveness, manual written test cases, fault fixing, coverage capability, fault revealing, Testing]
Reducing confounding bias in predicate-level statistical debugging metrics
2012 34th International Conference on Software Engineering
None
2012
Statistical debuggers use data collected during test case execution to automatically identify the location of faults within software. Recent work has applied causal inference to eliminate or reduce control and data flow dependence confounding bias in statement-level statistical debuggers. The result is improved effectiveness. This is encouraging but motivates two novel questions: (1) how can causal inference be applied in predicate-level statistical debuggers and (2) what other biases can be eliminated or reduced. Here we address both questions by providing a model that eliminates or reduces control flow dependence and failure flow confounding bias within predicate-level statistical debuggers. We present empirical results demonstrating that our model significantly improves the effectiveness of a variety of predicate-level statistical debuggers, including those that eliminate or reduce only a single source of confounding bias.
[Adaptation models, program debugging, program testing, control flow dependence, Instruments, Computational modeling, fault localization, test case execution, Debugging, Switches, data flow analysis, automated debugging, data flow dependence confounding bias reduction, predicate-level statistical debuggers, Sensitivity, software fault location identification, failure flow confounding bias, statistical analysis, causal inference, statement-level statistical debuggers, predicate-level statistical debugging metrics, software metrics]
BugRedux: Reproducing field failures for in-house debugging
2012 34th International Conference on Software Engineering
None
2012
A recent survey conducted among developers of the Apache, Eclipse, and Mozilla projects showed that the ability to recreate field failures is considered of fundamental importance when investigating bug reports. Unfortunately, the information typically contained in a bug report, such as memory dumps or call stacks, is usually insufficient for recreating the problem. Even more advanced approaches for gathering field data and help in-house debugging tend to collect either too little information, and be ineffective, or too much information, and be inefficient. To address these issues, we present BugRedux, a novel general approach for in-house debugging of field failures. BugRedux aims to synthesize, using execution data collected in the field, executions that mimic the observed field failures. We define several instances of BugRedux that collect different types of execution data and perform, through an empirical study, a cost-benefit analysis of the approach and its variations. In the study, we apply BugRedux to 16 failures of 14 real-world programs. Our results are promising in that they show that it is possible to synthesize in-house executions that reproduce failures observed in the field using a suitable set of execution data.
[Optical fibers, program debugging, field failure reproduction, Instruments, MIMICs, Debugging, bug report investigation, Computer crashes, Generators, execution data, Mozilla projects, in-house debugging, Apache projects, field data gathering, cost-benefit analysis, in-house execution synthesis, Software, memory dumps, call stacks, BugRedux, Eclipse projects]
Object-centric debugging
2012 34th International Conference on Software Engineering
None
2012
During the process of developing and maintaining a complex software system, developers pose detailed questions about the runtime behavior of the system. Source code views offer strictly limited insights, so developers often turn to tools like debuggers to inspect and interact with the running system. Unfortunately, traditional debuggers focus on the runtime stack as the key abstraction to support debugging operations, though the questions developers pose often have more to do with objects and their interactions. We propose object-centric debugging as an alternative approach to interacting with a running software system. We show how, by focusing on objects as the key abstraction, natural debugging operations can be defined to answer developer questions related to runtime behavior. We present a running prototype of an object-centric debugger, and demonstrate, with the help of a series of examples, how object-centric debugging offers more effective support for many typical developer tasks than a traditional stack-oriented debugger.
[program comprehension, program debugging, object-oriented programming, runtime stack, reflection, object-centric debugging, natural debugging operations, Debugging, Data structures, Indexes, software maintenance, software system development, Runtime, software system maintenance, stack-oriented debugger, Software systems, debugging, source code views, key abstraction, Monitoring, system runtime behavior]
Disengagement in pair programming: Does it matter?
2012 34th International Conference on Software Engineering
None
2012
Pair Programming (PP) requires close collaboration and mutual engagement. Most existing empirical studies of PP do not focus on developers' behaviour during PP sessions, and focus instead on the effects of PP such as productivity. However, disengagement, where a developer is not focusing on solving the task or understanding the problem and allows their partner to work by themselves, can hinder collaboration between developers and have a negative effect on their performance. This paper reports on an empirical study that investigates disengagement. Twenty-one industrial pair programming sessions were video and audio recorded and qualitatively analysed to investigate circumstances that lead to disengagement. We identified five reasons for disengagement: interruptions during the collaboration, the way the work is divided, the simplicity of the task involved, social pressure on inexperienced pair programmers, and time pressure. Our findings suggest that disengagement is sometimes acceptable and agreed upon between the developers in order to speed up problem solving. However, we also found episodes of disengagement where developers &#x201C;drop out&#x201D; of their PP sessions and are not able to follow their partner's work nor contribute to the task at hand, thus losing the expected benefits of pairing. Analysis of sessions conducted under similar circumstances but where mutual engagement was sustained identified three behaviours that help to maintain engagement: encouraging the novice to drive, verbalisation and feedback, and asking for clarification.
[software prototyping, close collaboration, Companies, Programming, PP effects, task involve simplicity, productivity, Focusing, groupware, pair programming disengagement interruption, verbalisation encouragement, Interviews, work division, Navigation, feedback encouragement, time pressure, empirical study, inexperienced pair programmers, Video recording, clarification asking, Collaboration, collaboration, mutual engagement, social pressure, agile software development]
Ambient awareness of build status in collocated software teams
2012 34th International Conference on Software Engineering
None
2012
We describe the evaluation of a build awareness system that assists agile software development teams to understand current build status and who is responsible for any build breakages. The system uses ambient awareness technologies, providing a separate, easily perceived communication channel distinct from standard team workflow. Multiple system configurations and behaviours were evaluated. An evaluation of the system showed that, while there was no significant change in the proportion of build breakages, the overall number of builds increased substantially, and the duration of broken builds decreased. Team members also reported an increased sense of awareness of, and responsibility for, broken builds and some noted the system dramatically changed their perception of the build process making them more cognisant of broken builds.
[continuous integration, software prototyping, ambient awareness technologies, build processes, status information, Educational institutions, team workflow, software teams, Electronic mail, build status, ambient awareness, Servers, behaviour evaluation, collocated software teams, team working, agile software development teams, build breakages, Prototypes, Universal Serial Bus, build awareness system evaluation, Software, multiple system configurations evaluation, Monitoring]
What make long term contributors: Willingness and opportunity in OSS community
2012 34th International Conference on Software Engineering
None
2012
To survive and succeed, software projects need to attract and retain contributors. We model the individual's chances to become a valuable contributor through her capacity, willingness, and the opportunity to contribute at the time of joining. Using issue tracking data of Mozilla and Gnome, we find that the probability for a new joiner to become a Long Term Contributor (LTC) is associated with her willingness and environment. Specifically, during their first month, future LTCs tend to be more active and show more community-oriented attitude than other joiners. Joiners who start by commenting on instead of reporting an issue or ones who succeed to get at least one reported issue to be fixed, more than double their odds of becoming an LTC. The micro-climate with a productive and clustered peer group increases the odds. On the contrary, the macro-climate with high project popularity and the micro-climate with low attention from peers reduce the odds. This implies that the interaction between individual's attitude and project's climate are associated with the odds that an individual would become a valuable contributor or disengage from the project. Our findings may provide a basis for empirical approaches to design a better community architecture and to improve the experience of contributors.
[long term contributor, Mozilla, community architecture, public domain software, Communities, software management, History, OSS community willingness, open source, software projects, Meteorology, Context, community-oriented attitude, project management, willingness, opportunity, interaction of person and environment, Long Term Contributor, Atmospheric measurements, OSS community opportunity, Particle measurements, Software, LTC, individual attitude-project climate interaction, Gnome]
Development of auxiliary functions: Should you be agile? An empirical assessment of pair programming and test-first programming
2012 34th International Conference on Software Engineering
None
2012
A considerable part of software systems is comprised of functions that support the main modules, such as array or string manipulation and basic math computation. These auxiliary functions are usually considered less complex, and thus tend to receive less attention from developers. However, failures in these functions might propagate to more critical modules, thereby affecting the system's overall reliability. Given the complementary role of auxiliary functions, a question that arises is whether agile practices, such as pair programming and test-first programming, can improve their correctness without affecting time-to-market. This paper presents an empirical assessment comparing the application of these agile practices with more traditional approaches. Our study comprises independent experiments of pair versus solo programming, and test-first versus test-last programming. The first study involved 85 novice programmers who applied both traditional and agile approaches in the development of six auxiliary functions within three different domains. Our results suggest that the agile practices might bring benefits in this context. In particular, pair programmers delivered correct implementations much more often, and test-first programming encouraged the production of larger and higher coverage test sets. On the downside, the main experiment showed that both practices significantly increase total development time. A replication of the test-first experiment with professional developers shows similar results.
[Software testing, Measurement, critical modules, program testing, software prototyping, software reliability, software systems, string manipulation, auxiliary function development, agile methods, pair programming, TDD, agile practices, test-first programming, experimental software engineering, solo programming, time-to-market, test-first experiment, Programming profession, time to market, empirical assessment, system overall reliability, test-last programming, math computation, Arrays, Reliability, array manipulation, coverage test sets]
Maintaining invariant traceability through bidirectional transformations
2012 34th International Conference on Software Engineering
None
2012
Following the &#x201C;convention over configuration&#x201D; paradigm, model-driven development (MDD) generates code to implement the &#x201C;default&#x201D; behaviour that has been specified by a template separate from the input model, reducing the decision effort of developers. For flexibility, users of MDD are allowed to customise the model and the generated code in parallel. A synchronisation of changed model or code is maintained by reflecting them on the other end of the code generation, as long as the traceability is unchanged. However, such invariant traceability between corresponding model and code elements can be violated either when (a) users of MDD protect custom changes from the generated code, or when (b) developers of MDD change the template for generating the default behaviour. A mismatch between user and template code is inevitable as they evolve for their own purposes. In this paper, we propose a two-layered invariant traceability framework that reduces the number of mismatches through bidirectional transformations. On top of existing vertical (model&#x2194;code) synchronisations between a model and the template code, a horizontal (code&#x2194;code) synchronisation between user and template code is supported, aligning the changes in both directions. Our blinkit tool is evaluated using the data set available from the CVS repositories of a MDD project: Eclipse MDT/GMF.
[Adaptation models, convention over configuration paradigm, two-layered invariant traceability maintenance framework, Eclipse MDT-GMF, MDD, program compilers, model-driven development, Prototypes, template code, code elements, user code, bidirectional transformations, CVS repositories, Java, Computational modeling, program diagnostics, vertical synchronisations, Educational institutions, Generators, blinkit tool, Synchronization, software maintenance, horizontal synchronisation, code generation, default behaviour generation]
Slicing MATLAB Simulink models
2012 34th International Conference on Software Engineering
None
2012
MATLAB Simulink is the most widely used industrial tool for developing complex embedded systems in the automotive sector. The resulting Simulink models often consist of more than ten thousand blocks and a large number of hierarchy levels. To ensure the quality of such models, automated static analyses and slicing are necessary to cope with this complexity. In particular, static analyses are required that operate directly on the models. In this article, we present an approach for slicing Simulink Models using dependence graphs and demonstrate its efficiency using case studies from the automotive and avionics domain. With slicing, the complexity of a model can be reduced for a given point of interest by removing unrelated model elements, thus paving the way for subsequent static quality assurance methods.
[Context, automotive sector, dependence graphs, graph theory, mathematics computing, Switches, Dependence Analysis, Slicing, digital simulation, avionics, Simulink, MATLAB, Analytical models, Embedded systems, automated static analysis, embedded systems, quality assurance, automotive engineering, static quality assurance methods, MATLAB Simulink model slicing, Data models, industrial tool, avionics domain, Context modeling]
Partial evaluation of model transformations
2012 34th International Conference on Software Engineering
None
2012
Model Transformation is considered an important enabling factor for Model Driven Development. Transformations can be applied not only for the generation of new models from existing ones, but also for the consistent co-evolution of software artifacts that pertain to various phases of software lifecycle such as requirement models, design documents and source code. Furthermore, it is often common in practical scenarios to apply such transformations repeatedly and frequently; an activity that can take a significant amount of time and resources, especially when the affected models are complex and highly interdependent. In this paper, we discuss a novel approach for deriving incremental model transformations by the partial evaluation of original model transformation programs. Partial evaluation involves pre-computing parts of the transformation program based on known model dependencies and the type of the applied model change. Such pre-evaluation allows for significant reduction of transformation time in large and complex model repositories. To evaluate the approach, we have implemented QvtMix, a prototype partial evaluator for the Query, View and Transformation Operational Mappings (QVT-OM) language. The experiments indicate that the proposed technique can be used for significantly improving the performance of repetitive applications of model transformations.
[incremental model transformations, model repositories, QvtMix, software artifact coevolution, design documents, Computational modeling, Unified modeling language, software lifecycle, source code, QVT-OM, model transformation program partial evaluation, model dependency, Synchronization, software maintenance, query-view-transformation operational mappings language, prototype partial evaluator, requirement models, model driven development, transformation program precomputing parts, Abstracts, Syntactics, Software, Context modeling]
Partial models: Towards modeling and reasoning with uncertainty
2012 34th International Conference on Software Engineering
None
2012
Models are good at expressing information about software but not as good at expressing modelers' uncertainty about it. The highly incremental and iterative nature of software development nonetheless requires the ability to express uncertainty and reason with models containing it. In this paper, we build on our earlier work on expressing uncertainty using partial models, by elaborating an approach to reasoning with such models. We evaluate our approach by experimentally comparing it to traditional strategies for dealing with uncertainty as well as by conducting a case study using open source software. We conclude that we are able to reap the benefits of well-managed uncertainty while incurring minimal additional cost.
[partial models, Vocabulary, Uncertainty, software development, public domain software, open source software, Unified modeling language, reasoning, uncertainty handling, Cognition, Encoding, inference mechanisms, modeler uncertainty, Software, software engineering, Finishing]
Static detection of resource contention problems in server-side scripts
2012 34th International Conference on Software Engineering
None
2012
With modern multi-core architectures, web applications are usually configured to serve multiple requests simultaneously by spawning multiple instances. These instances may access the same external resources such as database tables and files. Such contentions may become severe during peak time, leading to violations of atomic business logic. In this paper, we propose a novel static analysis that detects atomicity violations of external operations for server side scripts. The analysis differs from traditional atomicity violation detection techniques by focusing on external resources instead of shared memory. It consists of three components. The first one is an interprocedural and path-sensitive resource identity analysis that determines whether multiple operations access the same external resource, which is critical to identifying contentions. The second component infers pairs of external operations that should be executed atomically. Finally, violations are detected by reasoning about serializability of interleaved atomic pairs. Experimental results show that the analysis is highly effective in detecting atomicity violations in real-world web apps.
[constraint solving, atomicity violation detection techniques, Web applications, Cognition, Servers, atomic business logic violation, interleaved atomic pair serializability reasoning, path-sensitive resource identity analysis, Databases, Abstracts, Context, multiprocessing systems, Computational modeling, program diagnostics, server-side script external operation, resource contention, static analysis, Encoding, inference mechanisms, static detection, resource contention problems, multicore architectures, interprocedural resource identity analysis, php, Internet, external resources]
Amplifying tests to validate exception handling code
2012 34th International Conference on Software Engineering
None
2012
Validating code handling exceptional behavior is difficult, particularly when dealing with external resources that may be noisy and unreliable, as it requires: 1) the systematic exploration of the space of exceptions that may be thrown by the external resources, and 2) the setup of the context to trigger specific patterns of exceptions. In this work we present an approach that addresses those difficulties by performing an exhaustive amplification of the space of exceptional behavior associated with an external resource that is exercised by a test suite. Each amplification attempts to expose a program exception handling construct to new behavior by mocking an external resource so that it returns normally or throws an exception following a predefined pattern. Our assessment of the approach indicates that it can be fully automated, is powerful enough to detect 65% of the faults reported in the bug reports of this kind, and is precise enough that 77% of the detected anomalies correspond to faults fixed by the developers.
[program debugging, Instruments, Humanoid robots, exception handling, exception space systematic exploration, Aerospace electronics, Media, execution specific patterns, anomaly detection, space exhaustive amplification, Noise measurement, program exception handling, Test transformation, test amplification, bug reports, Space exploration, Androids, exception behavior handling code validation, external resource]
MagicFuzzer: Scalable deadlock detection for large-scale applications
2012 34th International Conference on Software Engineering
None
2012
We present MagicFuzzer, a novel dynamic deadlock detection technique. Unlike existing techniques to locate potential deadlock cycles from an execution, it iteratively prunes lock dependencies that each has no incoming or outgoing edge. Combining with a novel thread-specific strategy, it dramatically shrinks the size of lock dependency set for cycle detection, improving the efficiency and scalability of such a detection significantly. In the real deadlock confirmation phase, it uses a new strategy to actively schedule threads of an execution against the whole set of potential deadlock cycles. We have implemented a prototype and evaluated it on large-scale C/C++ programs. The experimental results confirm that our technique is significantly more effective and efficient than existing techniques.
[large-scale C-C++ programs, Multicore processing, multi-threading, Image edge detection, Instruction sets, dynamic deadlock detection technique, Classification algorithms, C++ language, thread-specific strategy, MagicFuzzer, large-scale applications, deadlock detection, lock dependency set, concurrency control, deadlock confirmation phase, cycle detection, System recovery, scheduling, multithreaded programs, execution thread scheduling, Monitoring, Message systems]
Does organizing security patterns focus architectural choices?
2012 34th International Conference on Software Engineering
None
2012
Security patterns can be a valuable vehicle to design secure software. Several proposals have been advanced to improve the usability of security patterns. They often describe extra annotations to be included in the pattern documentation. This paper presents an empirical study that validates whether those proposals provide any real benefit for software architects. A controlled experiment has been executed with 90 master students, who have performed several design tasks involving the hardening of a software architecture via security patterns. The results show that annotations produce benefits in terms of a reduced number of alternatives that need to be considered during the selection of a suitable pattern. However, they do not reduce the time spent in the selection process.
[Context, security pattern usability, secure software engineering, Security, Proposals, design engineering, selection process, pattern documentation, software architecture, security patterns, Software architecture, security of data, experiment, Computer architecture, software security design, architectural choices, Software, security pattern organization, Catalogs]
Enhancing architecture-implementation conformance with change management and support for behavioral mapping
2012 34th International Conference on Software Engineering
None
2012
It is essential for software architecture to be consistent with implementation during software development. Existing architecture-implementation mapping approaches are not sufficient due to a variety of reasons, including lack of support for change management and mapping of behavioral architecture specification. A new approach called 1.x-way architecture-implementation mapping is presented in this paper to address these issues. Its contribution includes deep separation of generated and non-generated code, an architecture change model, architecture-based code regeneration, and architecture change notification. The approach is implemented in ArchStudio 4, an Eclipse-based architecture development environment. To evaluate its utility, we refactored the code of ArchStudio, and replayed changes that had been made to ArchStudio in two research projects by redoing them with the developed tool.
[ArchStudio code refactoring, 1.x-way architecture-implementation mapping approach, Unified modeling language, Manuals, Programming, Registers, formal specification, program compilers, change management, architecture-implementation conformance, software architecture, Software architecture, ArchStudio 4, Computer architecture, architecture-based code regeneration, architecture change model, software development, nongenerated code deep separation, architecture change notification, software maintenance, behavioral architecture specification mapping, architecture-implementation mapping, management of change, Software, Eclipse-based architecture development environment, architecture change management]
A tactic-centric approach for automating traceability of quality concerns
2012 34th International Conference on Software Engineering
None
2012
The software architectures of business, mission, or safety critical systems must be carefully designed to balance an exacting set of quality concerns describing characteristics such as security, reliability, and performance. Unfortunately, software architectures tend to degrade over time as maintainers modify the system without understanding the underlying architectural decisions. Although this problem can be mitigated by manually tracing architectural decisions into the code, the cost and effort required to do this can be prohibitively expensive. In this paper we therefore present a novel approach for automating the construction of traceability links for architectural tactics. Our approach utilizes machine learning methods and lightweight structural analysis to detect tactic-related classes. The detected tactic-related classes are then mapped to a Tactic Traceability Information Model. We train our trace algorithm using code extracted from fifteen performance-centric and safety-critical open source software systems and then evaluate it against the Apache Hadoop framework. Our results show that automatically generated traceability links can support software maintenance activities while helping to preserve architectural qualities.
[public domain software, software architectures, Apache Hadoop framework, distributed processing, software quality, Open source software, Training, Fault tolerance, software architecture, Heart beat, structural analysis, quality concern traceability automation, safety-critical open source software systems, traceability information models, learning (artificial intelligence), tactic traceability information model, Architecture, tactic-centric approach, machine learning methods, software maintenance, performance-centric open source software systems, traceability, software maintenance activities, tactic-related class detection, trace algorithm, Authentication, tactics]
Build code analysis with symbolic evaluation
2012 34th International Conference on Software Engineering
None
2012
Build process is crucial in software development. However, the analysis support for build code is still limited. In this paper, we present SYMake, an infrastructure and tool for the analysis of build code in make. Due to the dynamic nature of make language, it is challenging to understand and maintain complex Makefiles. SYMake provides a symbolic evaluation algorithm that processes Makefiles and produces a symbolic dependency graph (SDG), which represents the build dependencies (i.e. rules) among files via commands. During the symbolic evaluation, for each resulting string value in an SDG that represents a part of a file name or a command in a rule, SYMake provides also an acyclic graph (called T-model) to represent its symbolic evaluation trace. We have used SYMake to develop algorithms and a tool 1) to detect several types of code smells and errors in Makefiles, and 2) to support build code refactoring, e.g. renaming a variable/target even if its name is fragmented and built from multiple substrings. Our empirical evaluation for SYMake's renaming on several real-world systems showed its high accuracy in entity renaming. Our controlled experiment showed that with SYMake, developers were able to understand Makefiles better and to detect more code smells as well as to perform refactoring more accurately.
[Protocols, software development, Makefiles, code refactoring, graph theory, symbolic evaluation, build code maintenance, Maintenance engineering, Servers, software maintenance, symbolic evaluation algorithm, build process, code error detection, Reactive power, Linux, build code analysis, symbolic dependency graph, SDG, SYMake, Concrete, code smell detection, symbolic evaluation trace, acyclic graph]
An automated approach to generating efficient constraint solvers
2012 34th International Conference on Software Engineering
None
2012
Combinatorial problems appear in numerous settings, from timetabling to industrial design. Constraint solving aims to find solutions to such problems efficiently and automatically. Current constraint solvers are monolithic in design, accepting a broad range of problems. The cost of this convenience is a complex architecture, inhibiting efficiency, extensibility and scalability. Solver components are also tightly coupled with complex restrictions on their configuration, making automated generation of solvers difficult. We describe a novel, automated, model-driven approach to generating efficient solvers tailored to individual problems and present some results from applying the approach. The main contribution of this work is a solver generation framework called Dominion, which analyses a problem and, based on its characteristics, generates a solver using components chosen from a library. The key benefit of this approach is the ability to solve larger and more difficult problems as a result of applying finer-grained optimisations and using specialised techniques as required.
[constraint solving, automated model-driven approach, Generative programming, constraint solvers, Maintenance engineering, finer-grained optimisations, Generators, Complexity theory, model-driven development, combinatorial problems, software architecture, Software architecture, constraint solver generation, Computer architecture, Libraries, specialised techniques, constraint handling, Dominion, Electronics packaging]
Simulation-based abstractions for software product-line model checking
2012 34th International Conference on Software Engineering
None
2012
Software Product Line (SPL) engineering is a software engineering paradigm that exploits the commonality between similar software products to reduce life cycle costs and time-to-market. Many SPLs are critical and would benefit from efficient verification through model checking. Model checking SPLs is more difficult than for single systems, since the number of different products is potentially huge. In previous work, we introduced Featured Transition Systems (FTS), a formal, compact representation of SPL behaviour, and provided efficient algorithms to verify FTS. Yet, we still face the state explosion problem, like any model checking-based verification. Model abstraction is the most relevant answer to state explosion. In this paper, we define a novel simulation relation for FTS and provide an algorithm to compute it. We extend well-known simulation preservation properties to FTS and thus lay the theoretical foundations for abstraction-based model checking of SPLs. We evaluate our approach by comparing the cost of FTS-based simulation and abstraction with respect to product-by-product methods. Our results show that FTS are a solid foundation for simulation-based model checking of SPL.
[software products, program verification, FTS-based simulation, costing, digital simulation, Feature, software engineering paradigm, Radio frequency, software product-line model checking, Model Checking, Semantics, Abstracts, abstraction-based model checking, Silicon, Formal methods, software product line engineering, time-to-market, Computational modeling, simulation-based abstractions, Educational institutions, life cycle cost reduction, Abstraction, time to market, model checking-based verification, Software Product Lines, simulation preservation properties, Simulation, SPL behaviour, state explosion problem, software reusability, Software, featured transition systems]
Using dynamic analysis to discover polynomial and array invariants
2012 34th International Conference on Software Engineering
None
2012
Dynamic invariant analysis identifies likely properties over variables from observed program traces. These properties can aid programmers in refactoring, documenting, and debugging tasks by making dynamic patterns visible statically. Two useful forms of invariants involve relations among polynomials over program variables and relations among array variables. Current dynamic analysis methods support such invariants in only very limited forms. We combine mathematical techniques that have not previously been applied to this problem, namely equation solving, polyhedra construction, and SMT solving, to bring new capabilities to dynamic invariant detection. Using these methods, we show how to find equalities and inequalities among nonlinear polynomials over program variables, and linear relations among array variables of multiple dimensions. Preliminary experiments on 24 mathematical algorithms and an implementation of AES encryption provide evidence that the approach is effective at finding these invariants.
[SMT solving, program debugging, refactoring tasks, documenting tasks, polyhedra construction, Complexity theory, dynamic invariant detection, polynomial invariant discovery, nonlinear polynomials, program analysis, Polynomials, Performance analysis, array variables, linear relations, nonlinear invariants, debugging tasks, polynomials, program diagnostics, array invariants, Educational institutions, cryptography, program variables, dynamic analysis, software maintenance, array invariant discovery, dynamic invariant analysis, program traces, invariant generation, equation solving, AES encryption, Arrays, satisfiability modulo theory]
Metadata invariants: Checking and inferring metadata coding conventions
2012 34th International Conference on Software Engineering
None
2012
As the prevailing programming model of enterprise applications is becoming more declarative, programmers are spending an increasing amount of their time and efforts writing and maintaining metadata, such as XML or annotations. Although metadata is a cornerstone of modern software, automatic bug finding tools cannot ensure that metadata maintains its correctness during refactoring and enhancement. To address this shortcoming, this paper presents metadata invariants, a new abstraction that codifies various naming and typing relationships between metadata and the main source code of a program. We reify this abstraction as a domain-specific language. We also introduce algorithms to infer likely metadata invariants and to apply them to check metadata correctness in the presence of program evolution. We demonstrate how metadata invariant checking can help ensure that metadata remains consistent and correct during program evolution; it finds metadata-related inconsistencies and recommends how they should be corrected. Similar to static bug finding tools, a metadata invariant checker identifies metadata-related bugs as a program is being refactored and enhanced. Because metadata is omnipresent in modern software applications, our approach can help ensure the overall consistency and correctness of software as it evolves.
[enterprise application programming model, program debugging, metadata coding convention checking, metadata, frameworks, Programming, enhancement, Runtime, refactoring, specification languages, naming relationships, Java, meta data, static bug finding tools, domain-specific languages, metadata coding convention inferring, metadata invariant checking, typing relationships, software maintenance, bug finding, domain-specific language, program evolution, Computer bugs, XML, Syntactics, Inference algorithms, program source code, invariants]
Generating obstacle conditions for requirements completeness
2012 34th International Conference on Software Engineering
None
2012
Missing requirements are known to be among the major causes of software failure. They often result from a natural inclination to conceive over-ideal systems where the software-to-be and its environment always behave as expected. Obstacle analysis is a goal-anchored form of risk analysis whereby exceptional conditions that may obstruct system goals are identified, assessed and resolved to produce complete requirements. Various techniques have been proposed for identifying obstacle conditions systematically. Among these, the formal ones have limited applicability or are costly to automate. This paper describes a tool-supported technique for generating a set of obstacle conditions guaranteed to be complete and consistent with respect to the known domain properties. The approach relies on a novel combination of model checking and learning technologies. Obstacles are iteratively learned from counterexample and witness traces produced by model checking against a goal and converted into positive and negative examples, respectively. A comparative evaluation is provided with respect to published results on the manual derivation of obstacles in a real safety-critical system for which failures have been reported.
[obstacle condition generation, goal-oriented requirements engineering, safety-critical software, Learning systems, Analytical models, inductive learning, over-ideal systems, formal verification, obstacle analysis, Semantics, tool-supported technique, exceptional conditions, learning technologies, learning (artificial intelligence), obstacle, requirements completeness, Computational modeling, model synthesis, Knowledge based systems, software failure, Encoding, risk analysis, model checking, witness traces, safety-critical system, Requirements completeness, risk identification, Software]
make test-zesti: A symbolic execution solution for improving regression testing
2012 34th International Conference on Software Engineering
None
2012
Software testing is an expensive and time consuming process, often involving the manual creation of comprehensive regression test suites. However, current testing methodologies do not take full advantage of these tests. In this paper, we present a technique for amplifying the effect of existing test suites using a lightweight symbolic execution mechanism, which thoroughly checks all sensitive operations (e.g., pointer dereferences) executed by the test suite for errors, and explores additional paths around sensitive operations. We implemented this technique in a prototype system called ZESTI (Zero-Effort Symbolic Test Improvement), and applied it to three open-source code bases - GNU Coreutils, libdwarf and readelf - where it found 52 previously unknown bugs, many of which are out of reach of standard symbolic execution. Our technique works transparently to the tester, requiring no additional human effort or changes to source code or tests.
[Measurement, program debugging, program testing, make test-zesti, regression testing improvement, libdwarf open-source code bases, regression testing, Manuals, regression analysis, additional path exploration, GNU Coreutils open-source code bases, Open source software, bugs, test improvement, zero-effort symbolic test improvement, symbolic execution solution, Testing, software testing, Standards, sensitive operation checking, Computer bugs, readelf open-source code bases, symbolic execution, Concrete, comprehensive regression test suite creation, symbolic execution mechanism]
Ballerina: Automatic generation and clustering of efficient random unit tests for multithreaded code
2012 34th International Conference on Software Engineering
None
2012
Testing multithreaded code is hard and expensive. A multithreaded unit test creates two or more threads, each executing one or more methods on shared objects of the class under test. Such unit tests can be generated at random, but basic random generation produces tests that are either slow or do not trigger concurrency bugs. Worse, such tests have many false alarms, which require human effort to filter out. We present Ballerina, a novel technique for automated random generation of efficient multithreaded tests that effectively trigger concurrency bugs. Ballerina makes tests efficient by having only two threads, each executing a single, randomly selected method. Ballerina increases chances that such simple parallel code finds bugs by appending it to more complex, randomly generated sequential code. We also propose a clustering technique to reduce the manual effort in inspecting failures of automatically generated multithreaded tests. We evaluate Ballerina on 14 real-world bugs from six popular codebases: Groovy, JDK, JFreeChart, Apache Log4j, Apache Lucene, and Apache Pool. The experiments show that tests generated by Ballerina find bugs on average 2&#x00D7;-10&#x00D7; faster than basic random generation, and our clustering technique reduces the number of inspected failures on average 4&#x00D7;-8&#x00D7;. Using Ballerina, we found three previously unknown bugs, two of which were already confirmed and fixed.
[program debugging, multithreaded unit test, random unit tests automatic generation, multi-threading, program testing, Instruction sets, sequential code random generation, JFreeChart codebase, Receivers, Inspection, Apache Pool codebase, program compilers, Concurrent computing, multithreaded code testing, pattern clustering, Computer bugs, Ballerina, JDK codebase, Apache Lucene codebase, Apache Log4j codebase, concurrency bugs, Groovy codebase, Testing, random unit tests clustering]
On-demand test suite reduction
2012 34th International Conference on Software Engineering
None
2012
Most test suite reduction techniques aim to select, from a given test suite, a minimal representative subset of test cases that retains the same code coverage as the suite. Empirical studies have shown, however, that test suites reduced in this manner may lose fault detection capability. Techniques have been proposed to retain certain redundant test cases in the reduced test suite so as to reduce the loss in fault-detection capability, but these still do concede some degree of loss. Thus, these techniques may be applicable only in cases where loose demands are placed on the upper limit of loss in fault-detection capability. In this work we present an on-demand test suite reduction approach, which attempts to select a representative subset satisfying the same test requirements as an initial test suite conceding at most l% loss in fault-detection capability for at least c% of the instances in which it is applied. Our technique collects statistics about loss in fault-detection capability at the level of individual statements and models the problem of test suite reduction as an integer linear programming problem. We have evaluated our approach in the contexts of three scenarios in which it might be used. Our results show that most test suites reduced by our approach satisfy given fault detection capability demands, and that the approach compares favorably with an existing test suite reduction approach.
[integer linear programming problem, program testing, code coverage, Computational modeling, integer programming, software testing, Educational institutions, linear programming, fault detection capability, representative subset selection, test suite reduction, Fault detection, on-demand, on-demand test suite reduction approach, Integer linear programming, Software, redundant test cases, Testing, statistics]
Automated detection of client-state manipulation vulnerabilities
2012 34th International Conference on Software Engineering
None
2012
Web application programmers must be aware of a wide range of potential security risks. Although the most common pitfalls are well described and categorized in the literature, it remains a challenging task to ensure that all guidelines are followed. For this reason, it is desirable to construct automated tools that can assist the programmers in the application development process by detecting weaknesses. Many vulnerabilities are related to web application code that stores references to application state in the generated HTML documents to work around the statelessness of the HTTP protocol. In this paper, we show that such client-state manipulation vulnerabilities are amenable to tool supported detection. We present a static analysis for the widely used frameworks Java Servlets, JSP, and Struts. Given a web application archive as input, the analysis identifies occurrences of client state and infers the information flow between the client state and the shared application state on the server. This makes it possible to check how client-state manipulation performed by malicious users may affect the shared application state and cause leakage or modifications of sensitive information. The warnings produced by the tool help the application programmer identify vulnerabilities. Moreover, the inferred information can be applied to configure a security filter that automatically guards against attacks. Experiments on a collection of open source web applications indicate that the static analysis is able to effectively help the programmer prevent client-state manipulation vulnerabilities.
[Java Servlets, public domain software, HTML, HTTP protocol, Electronic mail, Servers, Security, sensitive information modification, client-state manipulation vulnerability automated detection, open source Web application programmers, weakness detection, Libraries, Safety, security risks, sensitive information leakage, information flow analysis, HTML documents, hypermedia markup languages, Java, risk management, program diagnostics, security filter configuration, JSP, static analysis, shared application state, Web application code, Web application security, security of data, Struts, Internet]
Understanding integer overflow in C/C&#x002B;&#x002B;
2012 34th International Conference on Software Engineering
None
2012
Integer overflow bugs in C and C++ programs are difficult to track down and may lead to fatal errors or exploitable vulnerabilities. Although a number of tools for finding these bugs exist, the situation is complicated because not all overflows are bugs. Better tools need to be constructed - but a thorough understanding of the issues behind these errors does not yet exist. We developed IOC, a dynamic checking tool for integer overflows, and used it to conduct the first detailed empirical study of the prevalence and patterns of occurrence of integer overflows in C and C++ code. Our results show that intentional uses of wraparound behaviors are more common than is widely believed; for example, there are over 200 distinct locations in the SPEC CINT2000 benchmarks where overflow occurs. Although many overflows are intentional, a large number of accidental overflows also occur. Orthogonal to programmers' intent, overflows are found in both well-defined and undefined flavors. Applications executing undefined operations can be, and have been, broken by improvements in compiler optimizations. Looking beyond SPEC, we found and reported undefined integer overflows in SQLite, PostgreSQL, SafeInt, GNU MPC and GMP, Firefox, GCC, LLVM, Python, BIND, and OpenSSL; many of these have since been fixed. Our results show that integer overflow issues in C and C++ are subtle and complex, that they are common even in mature, widely used programs, and that they are widely misunderstood by developers.
[program debugging, wraparound behaviors, PostgreSQL, Optimization, undefined behavior, Program processors, Runtime, Weapons, LLVM, Semantics, SQLite, integer overflow understanding, IOC, SPEC, GNU MPC, Python, optimising compilers, BIND, GCC, C++ language, GMP, C++ programs, integer overflow bugs, Standards, SQL, Firefox, OpenSSL, dynamic checking tool, integer overflow, Computer bugs, compiler optimizations, SafeInt, integer wraparound]
A large scale exploratory analysis of software vulnerability life cycles
2012 34th International Conference on Software Engineering
None
2012
Software systems inherently contain vulnerabilities that have been exploited in the past resulting in significant revenue losses. The study of vulnerability life cycles can help in the development, deployment, and maintenance of software systems. It can also help in designing future security policies and conducting audits of past incidents. Furthermore, such an analysis can help customers to assess the security risks associated with software products of different vendors. In this paper, we conduct an exploratory measurement study of a large software vulnerability data set containing 46310 vulnerabilities disclosed since 1988 till 2011. We investigate vulnerabilities along following seven dimensions: (1) phases in the life cycle of vulnerabilities, (2) evolution of vulnerabilities over the years, (3) functionality of vulnerabilities, (4) access requirement for exploitation of vulnerabilities, (5) risk level of vulnerabilities, (6) software vendors, and (7) software products. Our exploratory analysis uncovers several statistically significant findings that have important implications for software development and deployment.
[Measurement, software vendors, software products, vulnerability functionality, vulnerability evolution, software systems, software vulnerability life cycles, Complexity theory, Open source software, software system development, patch, Computer hacking, security policy design, vulnerability exploitation, risk management, disclosure, exploratory measurement study, vulnerability, software maintenance, vulnerability risk level, large scale exploratory analysis, OSVDB, software system maintenance, past incident audit conduction, security of data, Aggregates, software system deployment, security risk assessment, exploit, NVD]
Synthesizing API usage examples
2012 34th International Conference on Software Engineering
None
2012
Key program interfaces are sometimes documented with usage examples: concrete code snippets that characterize common use cases for a particular data type. While such documentation is known to be of great utility, it is burdensome to create and can be incomplete, out of date, or not representative of actual practice. We present an automatic technique for mining and synthesizing succinct and representative human-readable documentation of program interfaces. Our algorithm is based on a combination of path sensitive dataflow analysis, clustering, and pattern abstraction. It produces output in the form of well-typed program snippets which document initialization, method calls, assignments, looping constructs, and exception handling. In a human study involving over 150 participants, 82% of our generated examples were found to be at least as good at human-written instances and 94% were strictly preferred to state of the art code search.
[Algorithm design and analysis, application program interfaces, Humans, data mining, exception handling, program interface documentation synthesis, program snippets, program interface documentation mining, document initialization, Clustering algorithms, Abstracts, path sensitive dataflow analysis, document handling, Java, Documentation, data flow analysis, method calls, looping constructs, concrete code snippets, API usage example synthesis, pattern abstraction, method assignment, pattern clustering, key program interfaces, Concrete]
Semi-automatically extracting FAQs to improve accessibility of software development knowledge
2012 34th International Conference on Software Engineering
None
2012
Frequently asked questions (FAQs) are a popular way to document software development knowledge. As creating such documents is expensive, this paper presents an approach for automatically extracting FAQs from sources of software development discussion, such as mailing lists and Internet forums, by combining techniques of text mining and natural language processing. We apply the approach to popular mailing lists and carry out a survey among software developers to show that it is able to extract high-quality FAQs that may be further improved by experts.
[document handling, Java, text analysis, natural language processing, Noise, data mining, Documentation, Programming, frequently asked questions, FAQ semiautomatic extraction, Data mining, software development knowledge accessibility, software development knowledge documentation, mailing lists, Software, Data models, software engineering, text mining]
Temporal analysis of API usage concepts
2012 34th International Conference on Software Engineering
None
2012
Software reuse through Application Programming Interfaces (APIs) is an integral part of software development. The functionality offered by an API is not always accessed uniformly throughout the lifetime of a client program. We propose Temporal API Usage Pattern Mining to detect API usage patterns in terms of their time of introduction into client programs. We detect concepts as distinct groups of API functionality from the change history of a client program. We locate those concepts in the client change history and detect temporal usage patterns, where a pattern contains a set of concepts that were added into the client program in a specific temporal order. We investigated the properties of temporal API usage patterns through a multiple-case study of three APIs and their use in up to 19 client software projects. Our technique was able to detect a number of valuable patterns in two out of three of the APIs investigated. Further investigation showed some patterns to be relatively consistent between clients, produced by multiple developers, and not trivially derivable from program structure or API documentation.
[Availability, application program interfaces, software reuse, software development, temporal API usage pattern mining, data mining, Documentation, application programming interfaces, client program, Software Reuse, Usage Pattern, API usage concepts, History, Data mining, Application programming interfaces, Mining Software Repositories, API usage pattern detection, software reusability, API Usage, API Usability, temporal analysis, Joining processes, Principal component analysis, client change history]
Inferring method specifications from natural language API descriptions
2012 34th International Conference on Software Engineering
None
2012
Application Programming Interface (API) documents are a typical way of describing legal usage of reusable software libraries, thus facilitating software reuse. However, even with such documents, developers often overlook some documents and build software systems that are inconsistent with the legal usage of those libraries. Existing software verification tools require formal specifications (such as code contracts), and therefore cannot directly verify the legal usage described in natural language text in API documents against code using that library. However, in practice, most libraries do not come with formal specifications, thus hindering tool-based verification. To address this issue, we propose a novel approach to infer formal specifications from natural language text of API documents. Our evaluation results show that our approach achieves an average of 92% precision and 93% recall in identifying sentences that describe code contracts from more than 2500 sentences of API documents. Furthermore, our results show that our approach has an average 83% accuracy in inferring specifications from over 1600 sentences describing code contracts.
[document handling, text analysis, Law, application program interfaces, program verification, natural language processing, Natural languages, tool-based verification, software libraries reusability, natural language text, formal specification, formal specifications, software libraries, natural language API descriptions, Accuracy, inferring method specifications, software verification tools, Semantics, software reusability, Libraries, Contracts, application programming interface documents]
Automatic parameter recommendation for practical API usage
2012 34th International Conference on Software Engineering
None
2012
Programmers extensively use application programming interfaces (APIs) to leverage existing libraries and frameworks. However, correctly and efficiently choosing and using APIs from unfamiliar libraries and frameworks is still a non-trivial task. Programmers often need to ruminate on API documentations (that are often incomplete) or inspect code examples (that are often absent) to learn API usage patterns. Recently, various techniques have been proposed to alleviate this problem by creating API summarizations, mining code examples, or showing common API call sequences. However, few techniques focus on recommending API parameters. In this paper, we propose an automated technique, called Precise, to address this problem. Differing from common code completion systems, Precise mines existing code bases, uses an abstract usage instance representation for each API usage example, and then builds a parameter usage database. Upon a request, Precise queries the database for abstract usage instances in similar contexts and generates parameter candidates by concretizing the instances adaptively. The experimental results show that our technique is more general and applicable than existing code completion systems, specially, 64% of the parameter recommendations are useful and 53% of the recommendations are exactly the same as the actual parameters needed. We have also performed a user study to show our technique is useful in practice.
[code base mining, argument, application program interfaces, Precise, data mining, practical API usage, application programming interfaces, recommendation, Complexity theory, API summarizations, query processing, code completion systems, inspect code examples, Abstracts, parameter, database querying, learning (artificial intelligence), API documentations, Context, abstract usage instance representation, Documentation, API usage pattern learning, parameter usage database, Indexes, recommender systems, code completion, automatic parameter recommendation, code example mining, API, Arrays, API call sequences]
On the naturalness of software
2012 34th International Conference on Software Engineering
None
2012
Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations - and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's built-in completion capability. We conclude the paper by laying out a vision for future research in this area.
[Eclipse built-in completion capability, n-gram, Entropy, speech recognition, code suggestion, Natural language processing, software engineering, text mining, text comprehension, language models, Java, Computational modeling, natural language processing, n-gram model, Tamil, statistical language models, English, natural language translation, code completion, Speech recognition, software naturalness, Software, question-answering, statistical analysis, statistical methods]
Recommending source code for use in rapid software prototypes
2012 34th International Conference on Software Engineering
None
2012
Rapid prototypes are often developed early in the software development process in order to help project stakeholders explore ideas for possible features, and to discover, analyze, and specify requirements for the project. As prototypes are typically thrown-away following the initial analysis phase, it is imperative for them to be created quickly with little cost and effort. Tool support for finding and reusing components from open-source repositories offers a major opportunity to reduce this manual effort. In this paper, we present a system for rapid prototyping that facilitates software reuse by mining feature descriptions and source code from open-source repositories. Our system identifies and recommends features and associated source code modules that are relevant to the software product under development. The modules are selected such that they implement as many of the desired features as possible while exhibiting the lowest possible levels of external coupling. We conducted a user study to evaluate our approach and the results indicated that our proposed system returned packages that implemented more features and were considered more relevant than the state-of-the-art approach.
[Java, software development process, source coding, open-source repositories, software prototyping, project stakeholders, recommending source code, domain analysis, program compilers, Couplings, recommender systems, rapid software prototypes, Prototypes, initial analysis phase, Search engines, Feature extraction, Software, feature description mining, Portfolios]
Active code completion
2012 34th International Conference on Software Engineering
None
2012
Code completion menus have replaced standalone API browsers for most developers because they are more tightly integrated into the development workflow. Refinements to the code completion menu that incorporate additional sources of information have similarly been shown to be valuable, even relative to standalone counterparts offering similar functionality. In this paper, we describe active code completion, an architecture that allows library developers to introduce interactive and highly-specialized code generation interfaces, called palettes, directly into the editor. Using several empirical methods, we examine the contexts in which such a system could be useful, describe the design constraints governing the system architecture as well as particular code completion interfaces, and design one such system, named Graphite, for the Eclipse Java development environment. Using Graphite, we implement a palette for writing regular expressions as our primary example and conduct a small pilot study. In addition to showing the feasibility of this approach, it provides further evidence in support of the claim that integrating specialized code completion interfaces directly into the editor is valuable to professional developers.
[offering similar functionality, Java, highly-specialized code generation interfaces, application program interfaces, API browsers, Standards, library developers, code completion, Image color analysis, Databases, development environments, Computer architecture, Graphite, Syntactics, active code completion menu, software engineering, Eclipse Java development environment]
Automated oracle creation support, or: How I learned to stop worrying about fault propagation and love mutation testing
2012 34th International Conference on Software Engineering
None
2012
In testing, the test oracle is the artifact that determines whether an application under test executes correctly. The choice of test oracle can significantly impact the effectiveness of the testing process. However, despite the prevalence of tools that support the selection of test inputs, little work exists for supporting oracle creation. In this work, we propose a method of supporting test oracle creation. This method automatically selects the oracle data - the set of variables monitored during testing - for expected value test oracles. This approach is based on the use of mutation analysis to rank variables in terms of fault-finding effectiveness, thus automating the selection of the oracle data. Experiments over four industrial examples demonstrate that our method may be a cost-effective approach for producing small, effective oracle data, with fault finding improvements over current industrial best practice of up to 145.8% observed.
[Greedy algorithms, oracle data, testing process, fault diagnosis, program testing, fault-finding effectiveness, testing, test oracles, Aerospace electronics, oracle data selection, Training, fault propagation, love mutation testing, automated oracle creation support, mutation analysis, oracle selection, Software systems, rank variables, Monitoring, verification, Testing]
Automating test automation
2012 34th International Conference on Software Engineering
None
2012
Mention &#x201C;test case&#x201D;, and it conjures up the image of a script or a program that exercises a system under test. In industrial practice, however, test cases often start out as steps described in natural language. These are essentially directions a human tester needs to follow to interact with an application, exercising a given scenario. Since tests need to be executed repeatedly, such manual tests then have to go through test automation to create scripts or programs out of them. Test automation can be expensive in programmer time. We describe a technique to automate test automation. The input to our technique is a sequence of steps written in natural language, and the output is a sequence of procedure calls with accompanying parameters that can drive the application without human intervention. The technique is based on looking at the natural language test steps as consisting of segments that describe actions on targets, except that there can be ambiguity in identifying segments, in identifying the action in a segment, as well as in the specification of the target of the action. The technique resolves this ambiguity by backtracking, until it can synthesize a successful sequence of calls. We present an evaluation of our technique on professionally created manual test cases for two open-source web applications as well as a proprietary enterprise application. Our technique could automate over 82% of the steps contained in these test cases with no human intervention, indicating that the technique can reduce the cost of test automation quite effectively.
[Automation, program testing, Natural languages, Humans, programmer time, Manuals, Optimization, Programming profession, human intervention, automating test automation, test case, natural languages, natural language]
Stride: Search-based deterministic replay in polynomial time via bounded linkage
2012 34th International Conference on Software Engineering
None
2012
Deterministic replay remains as one of the most effective ways to comprehend concurrent bugs. Existing approaches either maintain the exact shared read-write linkages with a large runtime overhead or use exponential off-line algorithms to search for a feasible interleaved execution. In this paper, we propose Stride, a hybrid solution that records the bounded shared memory access linkages at runtime and infers an equivalent interleaving in polynomial time, under the sequential consistency assumption. The recording scheme eliminates the need for synchronizing the shared read operations, which results in a significant overhead reduction. Comparing to the previous state-of-the-art approach of deterministic replay, Stride reduces, on average, 2.5 times of runtime overhead and produces, on average, 3.88 times smaller logs.
[Schedules, program debugging, Law, Stride, Instruction sets, Instruments, polynomials, read write linkages, exponential offline algorithms, Debugging, shared memory access, sequential consistency assumption, Replaying, Couplings, search based deterministic replay, Runtime, Concurrency, concurrent bugs, shared memory systems, polynomial time, bounded linkage, search problems]
iTree: Efficiently discovering high-coverage configurations using interaction trees
2012 34th International Conference on Software Engineering
None
2012
Software configurability has many benefits, but it also makes programs much harder to test, as in the worst case the program must be tested under every possible configuration. One potential remedy to this problem is combinatorial interaction testing (CIT), in which typically the developer selects a strength t and then computes a covering array containing all t-way configuration option combinations. However, in a prior study we showed that several programs have important high-strength interactions (combinations of a subset of configuration options) that CIT is highly unlikely to generate in practice. In this paper, we propose a new algorithm called interaction tree discovery (iTree) that aims to identify sets of configurations to test that are smaller than those generated by CIT, while also including important high-strength interactions missed by practical applications of CIT. On each iteration of iTree, we first use low-strength CIT to test the program under a set of configurations, and then apply machine learning techniques to discover new interactions that are potentially responsible for any new coverage seen. By repeating this process, iTree builds up a set of configurations likely to contain key high-strength interactions. We evaluated iTree by comparing the coverage it achieves versus covering arrays and randomly generated configuration sets. Our results strongly suggest that iTree can identify high-coverage sets of configurations more effectively than traditional CIT or random sampling.
[combinatorial mathematics, program testing, Instruments, Software Configurations, iTree, software configurability, machine learning techniques, CIT, interaction tree discovery, Empirical Software Engineering, combinatorial interaction testing, Software Testing and Analysis, Clustering algorithms, high strength interactions, Machine learning, efficiently discovering high coverage configurations, potential remedy, Arrays, Decision trees, learning (artificial intelligence), Testing, Software engineering]
Inferring class level specifications for distributed systems
2012 34th International Conference on Software Engineering
None
2012
Distributed systems often contain many behaviorally similar processes, which are conveniently grouped into classes. In system modeling, it is common to specify such systems by describing the class level behavior, instead of object level behavior. While there have been techniques that mine specifications of such distributed systems from their execution traces, these methods only mine object-level specifications involving concrete process objects. This leads to specifications which are large, hard to comprehend, and sensitive to simple changes in the system (such as the number of objects). In this paper, we develop a class level specification mining framework for distributed systems. A specification that describes interaction snippets between various processes in a distributed system forms a natural and intuitive way to document their behavior. Our mining method groups together such interactions between behaviorally similar processes, and presents a mined specification involving &#x201C;symbolic&#x201D; Message Sequence Charts. Our experiments indicate that our mined symbolic specifications are significantly smaller than mined concrete specifications, while at the same time achieving better precision and recall.
[Context, object level behavior, Barium, message sequence charts, inferring class level specifications, Specification Mining, distributed processing, execution traces, History, Data mining, Distributed Systems, formal specification, class level behavior, interaction snippets, Aggregates, Abstracts, distributed systems, Concrete, object level specifications]
Statically checking API protocol conformance with mined multi-object specifications
2012 34th International Conference on Software Engineering
None
2012
Programmers using an API often must follow protocols that specify when it is legal to call particular methods. Several techniques have been proposed to find violations of such protocols based on mined specifications. However, existing techniques either focus on single-object protocols or on particular kinds of bugs, such as missing method calls. There is no practical technique to find multi-object protocol bugs without a priori known specifications. In this paper, we combine a dynamic analysis that infers multi-object protocols and a static checker of API usage constraints into a fully automatic protocol conformance checker. The combined system statically detects illegal uses of an API without human-written specifications. Our approach finds 41 bugs and code smells in mature, real-world Java programs with a true positive rate of 51%. Furthermore, we show that the analysis reveals bugs not found by state of the art approaches.
[Java, program debugging, Protocols, Law, Error analysis, application program interfaces, single object protocols, multiobject protocol bugs, dynamic analysis, Specification mining, formal specification, statically checking API protocol conformance, Training, human written specifications, Computer bugs, mined multiobject specifications, Typestate, Static analysis, Java programs]
Behavioral validation of JFSL specifications through model synthesis
2012 34th International Conference on Software Engineering
None
2012
Contracts are a popular declarative specification technique to describe the behavior of stateful components in terms of pre/post conditions and invariants. Since each operation is specified separately in terms of an abstract implementation, it may be hard to understand and validate the resulting component behavior from contracts in terms of method interactions. In particular, properties expressed through algebraic axioms, which specify the effect of sequences of operations, require complex theorem proving techniques to be validated. In this paper, we propose an automatic small-scope based approach to synthesize incomplete behavioral abstractions for contracts expressed in the JFSL notation. The proposed abstraction technique enables the possibility to check that the contract behavior is coherent with behavioral properties expressed as axioms of an algebraic specifications. We assess the applicability of our approach by showing how the synthesis methodology can be applied to some classes of contract-based artifacts like specifications of data abstractions and requirement engineering models.
[Algorithm design and analysis, Metals, requirement engineering models, algebra, abstract implementation, algebraic specifications, contracts, formal specification, specifications, algebraic axioms, Algebra, automatic small-scope based approach, Abstracts, specification languages, complex theorem proving techniques, incomplete behavioral abstractions, data structures, theorem proving, data abstractions, declarative specification technique, Contracts, Context, object-oriented programming, model synthesis, Observers, JFSL specifications, component behavior, behavioral validation]
Verifying client-side input validation functions using string analysis
2012 34th International Conference on Software Engineering
None
2012
Client-side computation in web applications is becoming increasingly common due to the popularity of powerful client-side programming languages such as JavaScript. Clientside computation is commonly used to improve an application's responsiveness by validating user inputs before they are sent to the server. In this paper, we present an analysis technique for checking if a client-side input validation function conforms to a given policy. In our approach, input validation policies are expressed using two regular expressions, one specifying the maximum policy (the upper bound for the set of inputs that should be allowed) and the other specifying the minimum policy (the lower bound for the set of inputs that should be allowed). Using our analysis we can identify two types of errors 1) the input validation function accepts an input that is not permitted by the maximum policy, or 2) the input validation function rejects an input that is permitted by the minimum policy. We implemented our analysis using dynamic slicing to automatically extract the input validation functions from web applications and using automata-based string analysis to analyze the extracted functions. Our experiments demonstrate that our approach is effective in finding errors in input validation functions that we collected from real-world applications and from tutorials and books for teaching JavaScript.
[Algorithm design and analysis, Java, client side computation, Lattices, Doped fiber amplifiers, Web applications, HTML, Electronic mail, Browsers, client side input validation function verification, automata based string analysis, Reactive power, formal verification, extracted functions, client-side programming languages, JavaScript, string analysis, clientside computation]
Digital formations of the powerful and the powerless (Keynote)
2012 34th International Conference on Software Engineering
None
2012
Summary form only given. This talk is a whirlwind tour through human auditory perception. First, there is a mention of the actual acoustical cues in a performance venue, and then the ear's effects are discussed. In this, some discussion of loudness comes first, along with a bit of the presumed mechanisms. This will be followed by discussion of binaural auditory cues such as ITD, ILD, and HRTF's, and then a bit of discussion on the mechanisms for direct perception vs. diffuse perception follow. A bit of an introduction to the psychology of hearing will be covered as well, in order to explain what happens to the information present on the auditory nerve. Along the way, requirements for reproducing this in a standard acoustic space will be addressed in several fashions. All in all, this talk will summarize many years of work (starting in the late 1800's) on hearing and spatial sensation, as well as a bit of acoustics, and end with some recommendations on where one might improve the presentation of audio in the modern world, either for rooms or &#x201C;virtualization&#x201D; applications.
[]
Supporting sustainability with software &#x2014; An industrial perspective (Keynote)
2012 34th International Conference on Software Engineering
None
2012
Summary form only given. TechnoAware research and develops technologies and solutions for ambient intelligence. Established in 2003 TechnoAware was born from the experiences and competencies of the ISIP40 research group of the University of Genova. This research group is studying and implementing video analytics algorithms since 1985 and is considered nowadays one of the major actors in this filed worldwide. Entirely made up by researchers and experts in the video analytics field, TechnoAware main principles are: proprietary technologies (highly customizable and modular solutions), scientific competencies (high quality level and performances), continuous research and technological innovation (cutting edge products).
[]
Whither software architecture? (Keynote)
2012 34th International Conference on Software Engineering
None
2012
Summary form only given. Social media has revolutionized how humans create and curate knowledge artifacts [1]. It has increased individual engagement, broadened community participation and led to the formation of new social networks. This paradigm shift is particularly evident in software engineering in three distinct ways: firstly, in how software stakeholders co-develop and form communities of practice; secondly, in the complex and distributed software ecosystems that are enabled through insourcing, outsourcing, open sourcing and crowdsourcing of components and related artifacts; and thirdly, by the emergence of socially-enabled software repositories and collaborative development environments [2].
[]
Towards a federated cloud ecosystem (Invited industrial talk)
2012 34th International Conference on Software Engineering
None
2012
Summary form only given. Collects the abstracts for the constituents of the 2012 IEEE International Power Engineering and Optimization Conference (PEOCO2012) proceedings.
[]
Specification patterns from research to industry: A case study in service-based applications
2012 34th International Conference on Software Engineering
None
2012
Specification patterns have proven to help developers to state precise system requirements, as well as formalize them by means of dedicated specification languages. Most of the past work has focused its applicability area to the specification of concurrent and real-time systems, and has been limited to a research setting. In this paper we present the results of our study on specification patterns for service-based applications (SBAs). The study focuses on industrial SBAs in the banking domain. We started by performing an extensive analysis of the usage of specification patterns in published research case studies - representing almost ten years of research in the area of specification, verification, and validation of SBAs. We then compared these patterns with a large body of specifications written by our industrial partner over a similar time period. The paper discusses the outcome of this comparison, indicating that some needs of the industry, especially in the area of requirements specification languages, are not fully met by current software engineering research.
[Industries, Real time systems, Context, industry research, service based applications, services, formal specification, banking domain, case study, SBA, real-time systems, specification languages, requirements specifications, Software, software engineering, Time factors, specification patterns, Pattern matching, Software engineering]
Methodology for migration of long running process instances in a global large scale BPM environment in Credit Suisse's SOA landscape
2012 34th International Conference on Software Engineering
None
2012
Research about process instance migration covers mainly changes in process models during the process evolution and their effects on the same runtime environment. But what if the runtime environment - a legacy Business Process Execution (BPE) platform - had to be replaced with a new solution? Several migration aspects must be taken into account. (1) Process models from the old BPE platform have to be converted to the target process definition language on the target BPE platform. (2) Existing Business Process Management (BPM) applications must be integrated via new BPE platform interfaces. (3) Process instances and process instance data state must be migrated. For each of these points an appropriate migration strategy must be chosen. This paper describes the migration methodology which was applied for the BPE platform renewal in Credit Suisse.
[process definition model conversion, Banking, global large scale BPM environment, History, process evolution, BPM application migration, Standards, business process execution, long running process instances, Analytical models, legacy workflow system replacement, business process execution platform, business process management, credit suisse SOA landscape, BPE, Computer architecture, Data models, workflow application migration, runtime environment, business data processing, service-oriented architecture, BPM, migration of long running process instances, Business]
Information needs for software development analytics
2012 34th International Conference on Software Engineering
None
2012
Software development is a data rich activity with many sophisticated metrics. Yet engineers often lack the tools and techniques necessary to leverage these potentially powerful information resources toward decision making. In this paper, we present the data and analysis needs of professional software engineers, which we identified among 110 developers and managers in a survey. We asked about their decision making process, their needs for artifacts and indicators, and scenarios in which they would use analytics. The survey responses lead us to propose several guidelines for analytics tools in software development including: Engineers do not necessarily have much expertise in data analysis; thus tools should be easy to use, fast, and produce concise output. Engineers have diverse analysis needs and consider most indicators to be important; thus tools should at the same time support many different types of artifacts and many indicators. In addition, engineers want to drill down into data based on time, organizational structure, and system architecture.
[Measurement, software engineers, data analysis, Decision making, Programming, Complexity theory, organizational structure, Guidelines, powerful information resources, software development analytics, sophisticated metrics, system architecture, decision making, data rich activity, Software, software engineering, Software engineering]
Software analytics in practice: Mini tutorial
2012 34th International Conference on Software Engineering
None
2012
Summary form only given. A huge wealth of various data exists in the software development process, and hidden in the data is information about the quality of software and services as well as the dynamics of software development. With various analytic and computing technologies, software analytics is to enable software practitioners to performance data exploration and analysis in order to obtain insightful and actionable information for data-driven tasks around software and services [1].
[]
Software as an engineering material: How the affordances of programming have changed and what to do about it (Invited industrial talk)
2012 34th International Conference on Software Engineering
None
2012
Summary form only given. A huge wealth of various data exist in the practice of software development. Further rich data are produced by modern software and services in operation, many of which tend to be data-driven and/or data-producing in nature. Hidden in the data is information about the quality of software and services and the dynamics of software development. Software analytics is to develop and apply data exploration and analysis technologies, such as pattern recognition, machine learning, and information visualization, on software data to obtain insightful and actionable information for modern software and services. This tutorial presents latest research and practice on principles, techniques, and applications of software analytics in practice, highlighting success stories in industry, research achievements that are transferred to industrial practice, and future research and practice directions in software analytics. The attendees can acquire the skills and knowledge needed to perform industrial research or conduct industrial practice in the field of software analytics and to integrate analytics in their own industrial research, practice, and training.
[]
Software architecture &#x2014; What does it mean in industry? (Invited industrial talk)
2012 34th International Conference on Software Engineering
None
2012
Summary form only given. As communications get embedded in any objects we will see more and more instances of services and information seamlessly coupled to objects. Kindle has been the forerunner and we are going to see many more kind of objects being connected to the Internet. This will create a new slate of opportunities for many companies and Telecom Operators will be able to have a significant and economically relevant role in this space. The talk will address the technologies enabling this transformation and the evolution in value chains that may result emphasizing the role of Telecom Operators in this transformation. A concrete example of a territorial transformation taking place in Trento, Italy, will be given. This example may be discussed to evaluate the extent to which it can be applied in different context.
[]
How software engineering can benefit from traditional industries &#x2014; A practical experience report (Invited industrial talk)
2012 34th International Conference on Software Engineering
None
2012
To be competitive in today's market, the IT industry faces many challenges in the development and maintenance of enterprise information systems. Engineering these largescaled systems efficiently requires making decisions about a number of issues. In addition, customers expectations imply continuous software delivery in predictable quality. The operation such systems demands for transparency of the software in regard to lifecycle, change and incident management as well as cost efficiency. Addressing these challenges, we learned how to benefit from traditional industries. Contrary to the fact that the IT business calls itself gladly an industry, the industrialization of software engineering in most cases moves on a rather modest level. Industrialization means not only to build a solution or product on top of managed and well-defined processes, but also to have access to structured information about the current conditions of manufacturing at any time. Comparably with test series and assembly lines of the automobile industry, each individual component and each step from the beginning of manufacturing up to the final product should be equipped with measuring points for quality and progress. Even one step further the product itself, after it has left the factory, should be able to continuously provide analytic data for diagnostic reasons. Information is automatically collected and builds the basic essentials for process control, optimization and continuous improvement of the software engineering process. This presentation shows by means of a practical experience report how AdNovum managed to build its software engineering based on a well-balanced system of processes, continuous measurement and control &#x2014; as well as a healthy portion of pragmatism. We implemented an efficient and predictable software delivery pipeline based on five cornerstones that enables us to ship more than 1500 customer deliveries per year.
[]
Ten years of automated code analysis at Microsoft (Invited industrial talk)
2012 34th International Conference on Software Engineering
None
2012
Summary form only given. Based on a unique video stream analysis and combined with the Sony Smartcamera architecture, Blue Eye Video stand alone solution is able to determine how many persons are waiting in a queue, the customer behaviour when moving in a department store, airports, theatre or stadium.
[]
Large-scale formal verification in practice: A process perspective
2012 34th International Conference on Software Engineering
None
2012
The L4.verified project was a rare success in large-scale, formal verification: it provided a formal, machine-checked, code-level proof of the full functional correctness of the seL4 microkernel. In this paper we report on the development process and management issues of this project, highlighting key success factors. We formulate a detailed descriptive model of its middle-out development process, and analyze the evolution and dependencies of code and proof artifacts. We compare our key findings on verification and re-verification with insights from other verification efforts in the literature. Our analysis of the project is based on complete access to project logs, meeting notes, and version control data over its entire history, including its long-term, ongoing maintenance phase. The aim of this work is to aid understanding of how to successfully run large-scale formal software verification projects.
[program verification, code-level proof, Analytical models, machine-checked proof, project logs, large-scale formal software verification projects, Prototypes, Abstracts, maintenance phase, Kernel, proof artifacts, full functional correctness, detailed descriptive model, Maintenance engineering, L4, version control data, microkernel, middle-out development process, meeting notes, Computer bugs, seL4 microkernel, management issues, formal methods, L4 verified project, software process]
Constructing parser for industrial software specifications containing formal and natural language description
2012 34th International Conference on Software Engineering
None
2012
This paper describes a novel framework for creating a parser to process and analyze texts written in a &#x201C;partially structured&#x201D; natural language. In many projects, the contents of document artifacts tend to be described as a mixture of formal parts (i.e. the text constructs follow specific conventions) and parts written in arbitrary free text. Formal parsers, typically defined and used to process a description with rigidly defined syntax such as program source code are very precise and efficient in processing the formal part, while parsers developed for natural language processing (NLP) are good at robustly interpreting the free-text part. Therefore, combining these parsers with different characteristics can allow for more flexible and practical processing of various project documents. Unfortunately, conventional approaches to constructing a parser from multiple small parsers were studied extensively only for formal language parsers and are not directly applicable to NLP parsers due to the differences in the way the input text is extracted and evaluated. We propose a method to configure and generate a combined parser by extending an approach based on parser combinator, the operators for composing multiple formal parsers, to support both NLP and formal parsers. The resulting text parser is based on Parsing Expression Grammars, and it benefits from the strength of both parser types. We demonstrate an application of such combined parser in practical situations and show that the proposed approach can efficiently construct a parser for analyzing project-specific industrial specification documents.
[natural language processing, Formal languages, NLP parsers, parsing expression grammars, Grammar, document artifacts, Parsing Expression Grammars, formal specification, formal language description, parser combinator, grammars, Semantics, natural language description, Abstracts, Syntactics, Document Analysis, Parser Combinator, arbitrary free text, Natural language processing, Requirement Engineering, formal parsers, program source code, industrial software specifications]
Formal correctness, safety, dependability, and performance analysis of a satellite
2012 34th International Conference on Software Engineering
None
2012
This paper reports on the usage of a broad palette of formal modeling and analysis techniques on a regular industrial-size design of an ultra-modern satellite platform. These efforts were carried out in parallel with the conventional software development of the satellite platform. The model itself is expressed in a formalized dialect of AADL. Its formal nature enables rigorous and automated analysis, for which the recently developed COMPASS toolset was used. The whole effort revealed numerous inconsistencies in the early design documents, and the use of formal analyses provided additional insight on discrete system behavior (comprising nearly 50 million states), on hybrid system behavior involving discrete and continuous variables, and enabled the automated generation of large fault trees (66 nodes) for safety analysis that typically are constructed by hand. The model's size pushed the computational tractability of the algorithms underlying the formal analyses, and revealed bottlenecks for future theoretical research. Additionally, the effort led to newly learned practices from which subsequent formal modeling and analysis efforts shall benefit, especially when they are injected in the conventional software development lifecycle. The case demonstrates the feasibility of fully capturing a system-level design as a single comprehensive formal model and analyze it automatically using a toolset based on (probabilistic) model checkers.
[hybrid system behavior, Satellite broadcasting, single comprehensive formal model, FDIR, design engineering, formal analysis techniques, satellite formal correctness, Space vehicles, Analytical models, COMPASS toolset, discrete system behavior, formal verification, safety, specification languages, satellite performance analysis, software engineering, Safety, system-level design, satellite, satellite safety, air safety, dependability, fault trees, satellite dependability, Compass, modelling, ultra-modern satellite platform regular industrial-size design, fault management, Satellites, large fault trees generation, model checking, performance, artificial satellites, formal modeling, AADL, satellite platform software development, model checkers, formal methods, Software]
Goldfish bowl panel: Software development analytics
2012 34th International Conference on Software Engineering
None
2012
Gaming companies now routinely apply data mining to their user data in order to plan the next release of their software. We predict that such software development analytics will become commonplace, in the near future. For example, as large software systems migrate to the cloud, they are divided and sold as dozens of smaller apps; when shopping inside the cloud, users are free to mix and match their apps from multiple vendors (e.g. Google Docs' word processor with Zoho's slide manager); to extend, or even retain, market share cloud vendors must mine their user data in order to understand what features best attract their clients. This panel will address the open issues with analytics. Issues addressed will include the following. What is the potential for software development analytics? What are the strengths and weaknesses of the current generation of analytics tools? How best can we mature those tools?
[Conferences, software systems, data mining, software development management, mining software repositories, Programming, Educational institutions, industry, Data mining, cloud vendors, analytics, software development analytics, Zoho slide manager, goldfish bowl panel, empirical software engineering, Software, gaming companies, cloud computing, Google Docs word processor, Software engineering, Business]
Making sense of healthcare benefits
2012 34th International Conference on Software Engineering
None
2012
A key piece of information in healthcare is a patient's benefit plan. It details which treatments and procedures are covered by the health insurer (or payer), and at which conditions. While the most accurate and complete implementation of the plan resides in the payers claims adjudication systems, the inherent complexity of these systems forces payers to maintain multiple repositories of benefit information for other service and regulatory needs. In this paper we present a technology that deals with this complexity. We show how a large US health payer benefited from using the visualization, search, summarization and other capabilities of the technology. We argue that this technology can be used to improve productivity and reduce error rate in the benefits administration workflow, leading to lower administrative overhead and cost for health payers, which benefits both payers and patients.
[technology search, Natural languages, healthcare benefits, Medical services, claims adjudication systems, medical information systems, Complexity theory, health insurer, Data mining, US health payer, Engines, technology summarization, productivity improvement, error rate reduction, technology visualization, Insurance, Search engines, benefits administration workflow, insurance data processing, patient benefit plan, health care]
On the proactive and interactive visualization for feature evolution comprehension: An industrial investigation
2012 34th International Conference on Software Engineering
None
2012
Program comprehension is a key activity through maintenance and evolution of large-scale software systems. The understanding of a program often requires the evolution analysis of individual functionalities, so-called features. The comprehension of evolving features is not trivial as their implementations are often tangled and scattered through many modules. Even worse, existing techniques are limited in providing developers with direct means for visualizing the evolution of features' code. This work presents a proactive and interactive visualization strategy to enable feature evolution analysis. It proactively identifies code elements of evolving features and provides multiple views to present their structure under different perspectives. The novel visualization strategy was compared to a lightweight visualization strategy based on a tree-structure. We ran a controlled experiment with industry developers, who performed feature evolution comprehension tasks on an industrial-strength software. The results showed that the use of the proposed strategy presented significant gains in terms of correctness and execution time for feature evolution comprehension tasks.
[Industries, Context, program comprehension, Visualization, large-scale software systems, tree structure, feature evolution, Color, proactive visualization, History, software maintenance, software visualization, experimental evaluation, individual functionalities, data visualisation, industrial investigation, feature evolution comprehension, evolution analysis, Software systems, lightweight visualization, interactive visualization]
Extending static analysis by mining project-specific rules
2012 34th International Conference on Software Engineering
None
2012
Commercial static program analysis tools can be used to detect many defects that are common across applications. However, such tools currently have limited ability to reveal defects that are specific to individual projects, unless specialized checkers are devised and implemented by tool users. Developers do not typically exploit this capability. By contrast, defect mining tools developed by researchers can discover project-specific defects, but they require specialized expertise to employ and they may not be robust enough for general use. We present a hybrid approach in which a sophisticated dependence-based rule mining tool is used to discover project-specific programming rules, which are then transformed automatically into checkers that a commercial static analysis tool can run against a code base to reveal defects. We also present the results of an empirical study in which this approach was applied successfully to two large industrial code bases. Finally, we analyze the potential implications of this approach for software development practice.
[defect mining, individual projects, data mining, Transforms, Programming, software management, static program analysis, Data mining, Engines, program dependence graphs, defects detection, industrial code bases, software engineering, software tools, commercial analysis tools, project-specific defects, project management, program diagnostics, Generators, specialized checkers, project-specific programming rules, dependence-based rule mining tool, defect mining tools, software development practice, XML, Software, project-specific rules mining]
Debugger Canvas: Industrial experience with the code bubbles paradigm
2012 34th International Conference on Software Engineering
None
2012
At ICSE 2010, the Code Bubbles team from Brown University and the Code Canvas team from Microsoft Research presented similar ideas for new user experiences for an integrated development environment. Since then, the two teams formed a collaboration, along with the Microsoft Visual Studio team, to release Debugger Canvas, an industrial version of the Code Bubbles paradigm. With Debugger Canvas, a programmer debugs her code as a collection of code bubbles, annotated with call paths and variable values, on a two-dimensional pan-and-zoom surface. In this experience report, we describe new user interface ideas, describe the rationale behind our design choices, evaluate the performance overhead of the new design, and provide user feedback based on lab participants, post-release usage data, and a user survey and interviews. We conclude that the code bubbles paradigm does scale to existing customer code bases, is best implemented as a mode in the existing user experience rather than a replacement, and is most useful when the user has a long or complex call paths, a large or unfamiliar code base, or complex control patterns, like factories or dynamic linking.
[Visualization, program debugging, Navigation, human factors, Debugging, code bubbles paradigm, Educational institutions, integrated development environments, experience report, Web servers, user interfaces, code canvas team, user interface, debugger canvas, Brown University, two-dimensional pan-and-zoom surface, Microsoft Visual Studio team, integrated development environment, User interfaces, user feedback, Testing]
Characterizing and predicting which bugs get reopened
2012 34th International Conference on Software Engineering
None
2012
Fixing bugs is an important part of the software development process. An underlying aspect is the effectiveness of fixes: if a fair number of fixed bugs are reopened, it could indicate instability in the software system. To the best of our knowledge there has been on little prior work on understanding the dynamics of bug reopens. Towards that end, in this paper, we characterize when bug reports are reopened by using the Microsoft Windows operating system project as an empirical case study. Our analysis is based on a mixed-methods approach. First, we categorize the primary reasons for reopens based on a survey of 358 Microsoft employees. We then reinforce these results with a large-scale quantitative study of Windows bug reports, focusing on factors related to bug report edits and relationships between people involved in handling the bug. Finally, we build statistical models to describe the impact of various metrics on reopening bugs ranging from the reputation of the opener to how the bug was found.
[program debugging, software development process, empirical case study, Humans, Programming, Microsoft Windows operating system, Personnel, bugs fixing, software system, bug triage, bug report, Databases, Computer bugs, large-scale quantitative study, statistical models, operating systems (computers), bug reopen, Software, software engineering, bug reports, Testing]
ReBucket: A method for clustering duplicate crash reports based on call stack similarity
2012 34th International Conference on Software Engineering
None
2012
Software often crashes. Once a crash happens, a crash report could be sent to software developers for investigation upon user permission. To facilitate efficient handling of crashes, crash reports received by Microsoft's Windows Error Reporting (WER) system are organized into a set of &#x201C;buckets&#x201D;. Each bucket contains duplicate crash reports that are deemed as manifestations of the same bug. The bucket information is important for prioritizing efforts to resolve crashing bugs. To improve the accuracy of bucketing, we propose ReBucket, a method for clustering crash reports based on call stack matching. ReBucket measures the similarities of call stacks in crash reports and then assigns the reports to appropriate buckets based on the similarity values. We evaluate ReBucket using crash data collected from five widely-used Microsoft products. The results show that ReBucket achieves better overall performance than the existing methods. On average, the F-measure obtained by ReBucket is about 0.88.
[Measurement, program debugging, stack matching, Microsoft Windows Error Reporting, WER, duplicate crash report clustering, call stack trace, call stack similarity, ReBucket, software developers, Equations, Training, crashing bugs, pattern clustering, Computer bugs, bucket information, operating systems (computers), Software, software engineering, clustering, duplicate crash report detection, crash reports, Mathematical model]
Understanding the impact of Pair Programming on developers attention: A case study on a large industrial experimentation
2012 34th International Conference on Software Engineering
None
2012
Pair Programming is one of the most studied and debated development techniques. However, at present, we do not have a clear, objective, and quantitative understanding of the claimed benefits of such development approach. All the available studies focus on the analysis of the effects of Pair Programming (e.g., code quality, development speed, etc.) with different findings and limited replicability of the experiments. This paper adopts a different approach that could be replicated in an easier way: it investigates how Pair Programming affects the way developers write code and interact with their development machine. In particular, the paper focuses on the effects that Pair Programming has on developers' attention and productivity. The study was performed on a professional development team observed for ten months and it finds out that Pair Programming helps developers to eliminate distracting activities and to focus on productive activities.
[Productivity, Visualization, developers attention, software prototyping, PROM, Switches, Browsers, development machine, Programming profession, case study, debated development techniques, quantitative understanding, pair programming, productivity, large industrial experimentation, agile software development, Software development process]
How much does unused code matter for maintenance?
2012 34th International Conference on Software Engineering
None
2012
Software systems contain unnecessary code. Its maintenance causes unnecessary costs. We present tool-support that employs dynamic analysis of deployed software to detect unused code as an approximation of unnecessary code, and static analysis to reveal its changes during maintenance. We present a case study on maintenance of unused code in an industrial software system over the course of two years. It quantifies the amount of code that is unused, the amount of maintenance activity that went into it and makes the potential benefit of tool support explicit, which informs maintainers that are about to modify unused code.
[unnecessary costs, approximation theory, Software maintenance, maintenance activity, program diagnostics, Maintenance engineering, static analysis, dynamic analysis, unnecessary code, software maintenance, Information systems, unnecessary code approximation, unused code detection, industrial software system, Production, Software systems, unused code, Assembly, Business]
Using knowledge elicitation to improve Web effort estimation: Lessons from six industrial case studies
2012 34th International Conference on Software Engineering
None
2012
This paper details our experience building and validating six different expert-based Web effort estimation models for ICT companies in New Zealand and Brazil. All models were created using Bayesian networks, via eliciting knowledge from domain experts, and validated using data from past finished projects. Post-mortem interviews with the participating companies showed that they found the entire process extremely beneficial and worthwhile, and that all the models created remained in use by those companies.
[Context, project management, estimation theory, Estimation, knowledge acquisition, knowledge elicitation, Companies, Predictive models, ICT companies, Web/software effort estimation, industrial case studies, New Zealand, Accuracy, domain experts, Bayesian methods, Web pages, Brazil, Internet, Bayesian networks, belief networks, expert-based Web effort estimation models, expert-based models, bayesian networks]
Large-scale test automation in the cloud (Invited industrial talk)
2012 34th International Conference on Software Engineering
None
2012
Summary form only given. Software Engineering (SE) and Systems Engineering (Sys) are knowledge intensive, specialized, rapidly changing disciplines; their educational infrastructure faces significant challenges including the need to rapidly, widely, and cost effectively introduce new or revised course material; encourage the broad participation of students; address changing student motivations and attitudes; support undergraduate, graduate and lifelong learning; and incorporate the skills needed by industry. Games have a reputation for being fun and engaging; more importantly, they are immersive, requiring deep thinking and complex problem solving. We believe educational games are essential in the next generation of e-learning tools. An extensible, web-enabled, freely available, engaging, problem-based game platform that provides students with an interactive simulated experience closely resembling the activities performed in a (real) industry development project would transform the SE/Sys education infrastructure.
[]
Efficient reuse of domain-specific test knowledge: An industrial case in the smart card domain
2012 34th International Conference on Software Engineering
None
2012
While testing is heavily used and largely automated in software development projects, the reuse of test practices across similar projects in a given domain is seldom systematized and supported by adequate methods and tools. This paper presents a practical approach that emerged from a concrete industrial case in the smart card domain at STMicroelectronics Belgium in order to better address this kind of challenge. The central concept is a test knowledge repository organized as a collection of specific patterns named QPatterns. A systematic process was followed, first to gather, structure and abstract the test practices, then to produce and validate an initial repository, and finally to make it evolve later on Testers can then rely on this repository to produce high quality test plans identifying all the functional and nonfunctional aspects that have to be addressed, as well as the concrete tests that have to be developed within the context of a new project. A tool support was also developed and integrated in a traceable way into the existing industrial test environment. The approach was validated and is currently under deployment at STMicroelectronics Belgium.
[generation, Smart cards, project management, program testing, STMicroelectronics Belgium, test, industrial test environment, patterns, smart cards, software management, smart card domain, Security, industrial case, domain specific test knowledge, smartcard, software development projects, Libraries, Software, Concrete, Testing]
The Quamoco product quality modelling and assessment approach
2012 34th International Conference on Software Engineering
None
2012
Published software quality models either provide abstract quality attributes or concrete quality assessments. There are no models that seamlessly integrate both aspects. In the project Quamoco, we built a comprehensive approach with the aim to close this gap. For this, we developed in several iterations a meta quality model specifying general concepts, a quality base model covering the most important quality factors and a quality assessment approach. The meta model introduces the new concept of a product factor, which bridges the gap between concrete measurements and abstract quality aspects. Product factors have measures and instruments to operationalise quality by measurements from manual inspection and tool analysis. The base model uses the ISO 25010 quality attributes, which we refine by 200 factors and 600 measures for Java and C# systems. We found in several empirical validations that the assessment results fit to the expectations of experts for the corresponding systems. The empirical analyses also showed that several of the correlations are statistically significant and that the maintainability part of the base model has the highest correlation, which fits to the fact that this part is the most comprehensive. Although we still see room for extending and improving the base model, it shows a high correspondence with expert opinions and hence is able to form the basis for repeatable and understandable quality assessments in practice.
[concrete quality assessments, quality base model, Unified modeling language, product factor, software quality, empirical validation, abstract quality aspects, meta quality model, meta model, quality assessment, ISO 25010 quality attributes, Quamoco product quality assessment, software quality models, abstract quality attributes, Java, Object oriented modeling, Biological system modeling, Java systems, concrete measurements, quality model, quality factors, C# systems, Software quality, Concrete, Quality assessment, Quamoco product quality modelling, software metrics]
Industrial application of concolic testing approach: A case study on libexif by using CREST-BV and KLEE
2012 34th International Conference on Software Engineering
None
2012
As smartphones become popular, manufacturers such as Samsung Electronics are developing smartphones with rich functionality such as a camera and photo editing quickly, which accelerates the adoption of open source applications in the smartphone platforms. However, developers often do not know the detail of open source applications, because they did not develop the applications themselves. Thus, it is a challenging problem to test open source applications effectively in short time. This paper reports our experience of applying concolic testing technique to test libexif, an open source library to manipulate EXIF information in image files. We have demonstrated that concolic testing technique is effective and efficient at detecting bugs with modest effort in industrial setting. We also compare two concolic testing tools, CREST-BV and KLEE, in this testing project. Furthermore, we compare the concolic testing results with the analysis result of the Coverity Prevent static analyzer. We detected a memory access bug, a null pointer dereference bug, and four divide-by-zero bugs in libexif through concolic testing, none of which were detected by Coverity Prevent.
[null pointer dereference bug, program debugging, KLEE, program testing, open source applications, Instruments, public domain software, memory access bug, EXIF information, Search problems, smart phones, open source library, concolic testing approach, CREST-BV, divide-by-zero bugs, bugs detection, Computer bugs, libexif, Cameras, Concrete, smartphones, coverity prevent static analyzer, Smart phones, Testing]
Teaching software engineering and software project management: An integrated and practical approach
2012 34th International Conference on Software Engineering
None
2012
We present a practical approach for teaching two different courses of Software Engineering (SE) and Software Project Management (SPM) in an integrated way. The two courses are taught in the same semester, thus allowing to build mixed project teams composed of five-eight Bachelor's students (with development roles) and one or two Master's students (with management roles). The main goal of our approach is to simulate a real-life development scenario giving to the students the possibility to deal with issues arising from typical project situations, such as working in a team, organising the division of work, and coping with time pressure and strict deadlines.
[software engineering teaching, Unified modeling language, Project management, software engineering course, Programming, strict deadlines, master students, teaching, practical approach, team working, SPM, SE, Education, real-life development scenario simulation, software engineering, Quality management, computer science education, time pressure, bachelor students, educational courses, integrated approach, Software, work division organisation, Software engineering, software project management course]
Teaching collaborative software development: A case study
2012 34th International Conference on Software Engineering
None
2012
Software development is today done in teams of software developers who may be distributed all over the world. Software development has also become to contain more social aspects and the need for collaboration has become more evident. The importance of teaching development methods used in collaborative development is of importance, as skills beyond traditional software development are needed in this modern setting. A novel, student centric approach was tried out at Tampere University of Technology where a new environment called KommGame was introduced. This environment includes a reputation system to support the social aspect of the environment and thus supporting the learners collaboration with each other. In this paper, we present the KommGame environment and how it was applied on a course for practical results.
[KommGame environment, computer science education, social aspects of automation, Collaborative software, Communities, environment social aspects, Programming, University of Technology, teaching, SE education, case study, collaborative software development teaching, collaborative development, student centric approach, collaborative software development, Education, educational courses, groupware, Software, software engineering, software developer team, reputation system, learners collaboration, Software engineering]
Using continuous integration of code and content to teach software engineering with limited resources
2012 34th International Conference on Software Engineering
None
2012
Previous courses addressing the gap between student and professional programming practice have either isolated small groups' development in such a way that larger scale difficulties that motivate many professional practices do not arise, or have required significant additional staffing that would be expensive to provide in a large cohort core undergraduate software engineering course. We describe the first iteration of a course that enabled 73 students to work together to improve a large common legacy code base using professional practices and tools, staffed only by two lecturers and two undergraduate students employed as part-time tutors. The course relies on continuous integration and automated metrics, that coalesce frequently updated information in a manner that is visible to students and can be monitored by a small number of staff. The course is supported by a just-in-time teaching programme of thirty-two technical topics. We describe the constraints that determined the design of the course, and quantitative and qualitative data from the first iteration of the course.
[legacy code, computer science education, software engineering teaching, Experience Report, Educational institutions, teaching, qualitative data, Resource Constraints, Programming profession, undergraduate software engineering course, course design, code continuous integration, Software Engineering, educational courses, Continuous Integration, Studio Course, limited resources, automated metrics, Software, software engineering, quantitative data, Robots, Software engineering, just-in-time teaching programme]
Stages in teaching software testing
2012 34th International Conference on Software Engineering
None
2012
This paper describes how a staged approach to the development of students' abilities to engineer software systems applies to the specific issue of teaching software testing. It evaluates the courses relating to software testing in the Software Engineering volume of Computing Curriculum 2001 against a theoretical model that has been developed from a well-established programme in software engineering, from the perspectives of how well the courses support the progressive development of both students' knowledge of software testing and their ability to test software systems. It is shown that this progressive development is not well supported, and that to improve this some software testing material should be taught earlier than recommended.
[Software testing, computer science education, program testing, student abilities, software development, Materials, Debugging, Computing Curriculum 2001, software education, Programming, software testing teaching, teaching, progressive development, test software systems, students knowledge, educational courses, software testing courses, Software systems, software engineering, software testing material, development of skills]
Integrating tools and frameworks in undergraduate software engineering curriculum
2012 34th International Conference on Software Engineering
None
2012
We share our experience over the last 10 years for finding, deploying and evaluating software engineering (SE) technologies in an undergraduate program at the E&#x0301;TS in Montreal, Canada. We identify challenges and propose strategies to integrate technologies into an SE curriculum. We demonstrate how technologies are integrated throughout our program, and provide details of the integration in two specific courses.
[Industries, Context, Java, computer science education, further education, frameworks integration, ETS, frameworks, Tutorials, Companies, tools integration, Educational institutions, SE curriculum, SE technologies evaluation, technology, tools, software engineering curricula, undergraduate software engineering curriculum, educational courses, integration, Software, software engineering]
What scope is there for adopting evidence-informed teaching in SE?
2012 34th International Conference on Software Engineering
None
2012
Context: In teaching about software engineering we currently make little use of any empirical knowledge. Aim: To examine the outcomes available from the use of Evidence-Based Software Engineering (EBSE) practices, so as to identify where these can provide support for, and inform, teaching activities. Method: We have examined all known secondary studies published up to the end of 2009, together with those published in major journals to mid-2011, and identified where these provide practical results that are relevant to student needs. Results: Starting with 145 candidate systematic literature reviews (SLRs), we were able to identify and classify potentially useful teaching material from 43 of them. Conclusions: EBSE can potentially lend authority to our teaching, although the coverage of key topics is uneven. Additionally, mapping studies can provide support for research-led teaching.
[computer science education, education, Materials, empirical, teaching, Data mining, Guidelines, evidence-informed teaching, evidence-based software engineering practices, Systematics, secondary studies, Education, EBSE practices, systematic literature reviews, SLR, Software, software engineering, teaching activities, research-led teaching, mapping studies, evidence-based, Software engineering]
FOCUS: An adaptation of a SWEBOK-based curriculum for industry requirements
2012 34th International Conference on Software Engineering
None
2012
Siemens Corporate Development Center India (CT DC IN) develops software applications for the industry, energy, health-care, and infrastructure &amp; cities sectors of Siemens. These applications are typically critical in nature and require software practitioners who have considerable competency in the area of software engineering. To enhance the competency of engineers, CT DC IN has introduced an internal curriculum titled &#x201C;FOundation CUrriculum for Software engineers&#x201D; (FOCUS) which is an adapted version of IEEE's SWEBOK curriculum. The FOCUS program has been used to train more than 500 engineers in the last three years. In this experience report, we describe the motivation for FOCUS, how it was structured to address the specific needs of CT DC IN, and how the FOCUS program was rolled out within the organization. We also provide results obtained from a survey of the FOCUS participants, their managers, and FOCUS trainers that was conducted to throw light on the effectiveness of the program. We believe the insights from the survey results provide useful pointers to other organizations and academic institutions that are planning to adopt a SWEBOK-based curriculum.
[Knowledge engineering, Industries, computer science education, ISO TR 19759, Software engineering education, CT DC IN, IEEE SWEBOK curriculum, Programming, Industry experience, Training, SWEBOK, software engineering body of knowledge, foundation curriculum for software engineers, educational courses, Organizations, industry requirements, SWEBOK-based curriculum, Software, software engineering, Siemens Corporate Development Center India, FOCUS program, software application development, Software engineering]
Ten tips to succeed in Global Software Engineering education
2012 34th International Conference on Software Engineering
None
2012
The most effective setting for training in Global Software Engineering is to provide a distributed environment for students. In such an environment, students will meet challenges in recognizing problems first-hand. Teaching in a distributed environment is, however, very demanding, challenging and unpredictable compared to teaching in a local environment. Based on nine years of experience, in this paper we present the most important issues that should be taken into consideration to increase the probability of success in teaching a Global Software Engineering course.
[computer science education, Programming, distributed processing, Educational institutions, distributed environment, teaching, Training, Distrubuted software development, Education, Collaboration, educational courses, Communication channels, global software engineering course, software engineering, global software engineering education, Global software engineering, Software engineering]
Collaboration patterns in distributed software development projects
2012 34th International Conference on Software Engineering
None
2012
The need for educating future software engineers in the field of global software engineering is recognized by many educational institutions. In this paper we outline the characteristics of an existing global software development course run over a period of nine years, and present a flexible project framework for conducting student projects in a distributed environment. Based on data collected from fourteen distributed student projects, a set of collaboration patterns is identified and their causes and implications described. Collaboration patterns are a result of the analysis of collaboration links within distributed student teams, and can assist teachers in better understanding of the dynamics found in distributed projects.
[computer science education, project management, software engineer education, distributed student projects, Programming, distributed processing, Educational institutions, collaboration link analysis, distributed software development projects, Teaching, Distributed software development, Green products, flexible project framework, Collaboration, groupware, software engineering, Teamwork, educational institutions, global software development course, global software engineering, collaboration patterns, Patterns, Software engineering, distributed student teams]
Improving PSP education by pairing: An empirical study
2012 34th International Conference on Software Engineering
None
2012
Handling large-sized classes and maintaining students' involvement are two of the major challenges in Personal Software Process (PSP) education in universities. In order to tackle these two challenges, we adapted and incorporated some typical practices of Pair Programming (PP) into the PSP class at summer school in Software Institute of Nanjing University in 2010, and received positive results, such as higher students' involvement and conformity of process discipline, as well as (half) workload reduction in evaluating assignments. However, the experiment did not confirm the improved performance of the paired students as expected. Based on the experience and feedbacks, we improved this approach in our PSP course in 2011. Accordingly, by analyzing the previous experiment results, we redesigned the experiment with a number of improvements, such as lab environment, evaluation methods and student selection, to further investigate the effects of this approach in PSP education, in particular students' performance. We also introduced several new metrics to enable the comparison analysis of the data collected from both paired students and solo students. The new experiment confirms the value of pairing practices in PSP education. The results show that in PSP class, compared to solo students, paired students can achieve better performance in terms of program quality and exam scores.
[PP, large-sized classes handling, Training, PSP class, students' involvement, pair programming, universities, groupware, software process improvement, software engineering, Software Institute of Nanjing University, computer science education, further education, exam scores, summer school, process discipline conformity, Estimation, Educational institutions, PSP education, solo students, Programming profession, personal software process, program quality, paired students, collaborative learning, Software, educational institutions, personal software process education, students performance]
Five days of empirical software engineering: The PASED experience
2012 34th International Conference on Software Engineering
None
2012
Acquiring the skills to plan and conduct different kinds of empirical studies is a mandatory requirement for graduate students working in the field of software engineering. These skills typically can only be developed based on the teaching and experience of the students' supervisor, because of the lack of specific, practical courses providing these skills. To fill this gap, we organized the first Canadian Summer School on Practical Analyses of Software Engineering Data (PASED). The aim of PASED is to provide - using a &#x201C;learning by doing&#x201D; model of teaching - a solid foundation to software engineering graduate students on conducting empirical studies. This paper describes our experience in organizing the PASED school, i.e., what challenges we encountered, how we designed the lectures and laboratories, and what could be improved in the future based on the participants' feedback.
[computer science education, empirical software engineering graduate student, Laboratories, Canadian Summer School, Educational institutions, teaching, Data mining, Organizing, learning by doing model, Empirical Software Engineering, participant feedback, PASED experience, Software, software engineering, Software Engineering Education, Software engineering, practical analysis of software engineering data]
Automatically detecting developer activities and problems in software development work
2012 34th International Conference on Software Engineering
None
2012
Detecting the current activity of developers and problems they are facing is a prerequisite for a context-aware assistance and for capturing developers' experiences during their work. We present an approach to detect the current activity of software developers and if they are facing a problem. By observing developer actions like changing code or searching the web, we detect whether developers are locating the cause of a problem, searching for a solution, or applying a solution. We model development work as recurring problem solution cycle, detect developer's actions by instrumenting the IDE, translate developer actions to observations using ontologies, and infer developer activities by using Hidden Markov Models. In a preliminary evaluation, our approach was able to correctly detect 72% of all activities. However, a broader more reliable evaluation is still needed.
[Context, activity detection, task management, Switches, Ontologies, Search problems, ubiquitous computing, machine learning, context-aware assistance, automatically detecting developer activities, hidden Markov models, software development work, context-aware software engineering, Hidden Markov models, ontologies (artificial intelligence), Software, software engineering, recurring problem solution cycle, automatically detecting developer problems, Web searching, Hidden Markov Models, ontologies, Software engineering]
Software process improvement through the identification and removal of project-level knowledge flow obstacles
2012 34th International Conference on Software Engineering
None
2012
Uncontrollable costs, schedule overruns, and poor end product quality continue to plague the software engineering field. This research investigates software process improvement (SPI) through the application of knowledge management (KM) at the software project level. A pilot study was conducted to investigate what types of obstacles to knowledge flow exist within a software development project, as well as the potential influence on SPI of their mitigation or removal. The KM technique of &#x201C;knowledge mapping&#x201D; was used as a research technique to characterize knowledge flow. Results show that such mitigation or removal was acknowledged by project team members as having the potential for lowering project labor cost, improving schedule adherence, and enhancing final product quality.
[Knowledge engineering, software project level, project management, KM, knowledge flow, Observers, software management, Knowledge management, project level knowledge flow obstacle removal, knowledge management, SPI, project level knowledge flow obstacle identification, knowledge mapping, software process improvement, Software, software engineering, Interviews, Software engineering, software process]
Symbiotic general-purpose and domain-specific languages
2012 34th International Conference on Software Engineering
None
2012
Domain-Specific Modeling Languages (DSMLs) have received great attention in recent years and are expected to play a big role in the future of software engineering as processes become more view-centric. However, they are a &#x201C;two-edged sword&#x201D;. While they provide strong support for communication within communities, allowing experts to express themselves using concepts tailored to their exact needs, they are a poor vehicle for communication across communities because of their lack of common, transcending concepts. In contrast, General-Purpose Modeling Languages (GPMLs) have the opposite problem - they are poor at the former but good at the latter. The value of models in software engineering would therefore be significantly boosted if the advantages of DSMLs and GPMLs could be combined and models could be viewed in a domain-specific or general-purpose way depending on the needs of the user. In this paper we present an approach for achieving such a synergy based on the orthogonal classification architecture. In this architecture model elements have two classifiers: a linguistic one representing their &#x201C;general-purpose&#x201D; and an ontological one representing their &#x201C;domain-specific&#x201D; type. By associating visualization symbols with both classifiers it is possible to support two concrete syntaxes at the same time and allow the domain-specific and general-purpose notation to support each other - that is, to form a symbiotic relationship.
[ontological classification, Visualization, visualization symbols, Unified modeling language, computational linguistics, ontological classifiers, DSML, software architecture, data visualisation, transcending concepts, domain-specific modeling languages, software engineering, linguistic classifiers, concrete syntaxes, pattern classification, Biological system modeling, Object oriented modeling, orthogonal classification architecture-based synergy, GPML, Pragmatics, symbiotic relationship, Syntactics, ontologies (artificial intelligence), Concrete, simulation languages, linguistic classification, symbiotic domain-specific languages, symbiotic general-purpose modelling languages, orthogonal classification architecture]
Evaluating the specificity of text retrieval queries to support software engineering tasks
2012 34th International Conference on Software Engineering
None
2012
Text retrieval approaches have been used to address many software engineering tasks. In most cases, their use involves issuing a textual query to retrieve a set of relevant software artifacts from the system. The performance of all these approaches depends on the quality of the given query (i.e., its ability to describe the information need in such a way that the relevant software artifacts are retrieved during the search). Currently, the only way to tell that a query failed to lead to the expected software artifacts is by investing time and effort in analyzing the search results. In addition, it is often very difficult to ascertain what part of the query leads to poor results. We propose a novel pre-retrieval metric, which reflects the quality of a query by measuring the specificity of its terms. We exemplify the use of the new specificity metric on the task of concept location in source code. A preliminary empirical study shows that our metric is a good effort predictor for text retrieval-based concept location, outperforming existing techniques from the field of natural language document retrieval.
[Measurement, Context, text analysis, Query specificity, Correlation, query quality, specificity metric, Natural languages, text retrieval queries, text retrieval-based concept location, source code, Information retrieval, Entropy, natural language text, Text retrieval, query processing, preretrieval metric, software artifacts, natural languages, Software, software engineering tasks, Concept location, specificity evaluation, software metrics]
Co-adapting human collaborations and software architectures
2012 34th International Conference on Software Engineering
None
2012
Human collaboration has become an integral part of large-scale systems for massive online knowledge sharing, content distribution, and social networking. Maintenance of these complex systems, however, still relies on adaptation mechanisms that remain unaware of the prevailing user collaboration patterns. Consequently, a system cannot react to changes in the interaction behavior thereby impeding the collaboration's evolution. In this paper, we make the case for a human architecture model and its mapping onto software architecture elements as fundamental building blocks for system adaptation.
[Adaptation models, human collaboration co-adaptation, Humans, adaptation, social networking, knowledge management, ubiquitous computing, online knowledge sharing, software architecture, content distribution, Runtime, Software architecture, software architecture elements, Computer architecture, groupware, context awareness, collaboration patterns, complex system maintenance, software maintenance, user collaboration patterns, system adaptation, adaptation mechanisms, Collaboration, social networking (online), human architecture model, Software]
Release engineering practices and pitfalls
2012 34th International Conference on Software Engineering
None
2012
The release and deployment phase of the software development process is often overlooked as part of broader software engineering research. In this paper, we discuss early results from a set of multiple semi-structured interviews with practicing release engineers. Subjects for the interviews are drawn from a number of different commercial software development organizations, and our interviews focus on why release process faults and failures occur, how organizations recover from them, and how they can be predicted, avoided or prevented in the future. Along the way, the interviews provide insight into the state of release engineering today, and interesting relationships between software architecture and release processes.
[software development process, Standardization, Programming, deployment phase, History, software development organizations, broader software engineering research, multiple semistructured interviews, software architecture, Organizations, release engineering practices, Software, software engineering, release engineering pitfalls, Interviews, release engineering, Software engineering, software process]
Augmented intelligence &#x2014; The new AI &#x2014; Unleashing human capabilities in knowledge work
2012 34th International Conference on Software Engineering
None
2012
In this paper I describe a novel application of contemplative techniques to software engineering with the goal of augmenting the intellectual capabilities of knowledge workers within the field in four areas: flexibility, attention, creativity, and trust. The augmentation of software engineers' intellectual capabilities is proposed as a third complement to the traditional focus of methodologies on the process and environmental factors of the software development endeavor. I argue that these capabilities have been shown to be open to improvement through the practices traditionally used in spiritual traditions, but now used increasingly in other fields of knowledge work, such as in the medical profession and the education field. Historically, the intellectual capabilities of software engineers have been treated as a given within any particular software development effort. This is argued to be an aspect ripe for inclusion within software development methodologies.
[Knowledge engineering, knowledge engineering, intellectual capabilities, Humans, Contemplative Techniques, Awareness, Flexibility, knowledge work, software engineering, Development Methodologies, Productivity, Knowledge Workers, software development, contemplative techniques, Attention, AI, augmented intelligence, Contemplative Practices, Trust, Stress, Creativity, Augmented Intelligence, Software, Problem-solving, Software engineering]
On how often code is cloned across repositories
2012 34th International Conference on Software Engineering
None
2012
Detecting code duplication in large code bases, or even across project boundaries, is problematic due to the massive amount of data involved. Large-scale clone detection also opens new challenges beyond asking for the provenance of a single clone fragment, such as assessing the prevalence of code clones on the entire code base, and their evolution. We propose a set of lightweight techniques that may scale up to very large amounts of source code in the presence of multiple versions. The common idea behind these techniques is to use bad hashing to get a quick answer. We report on a case study, the Squeaksource ecosystem, which features thousands of software projects, with more than 40 million versions of methods, across more than seven years of evolution. We provide estimates for the prevalence of type-1, type-2, and type-3 clones in Squeaksource.
[project management, code duplication detection, Ecosystems, Cloning, Squeaksource ecosystem, source code, Educational institutions, Indexes, software maintenance, project boundaries, single clone fragment provenance, bad hashing, Clone detection, Layout, code base, Software, code evolution, Software ecosystems, code clone prevalence]
Mining input sanitization patterns for predicting SQL injection and cross site scripting vulnerabilities
2012 34th International Conference on Software Engineering
None
2012
Static code attributes such as lines of code and cyclomatic complexity have been shown to be useful indicators of defects in software modules. As web applications adopt input sanitization routines to prevent web security risks, static code attributes that represent the characteristics of these routines may be useful for predicting web application vulnerabilities. In this paper, we classify various input sanitization methods into different types and propose a set of static code attributes that represent these types. Then we use data mining methods to predict SQL injection and cross site scripting vulnerabilities in web applications. Preliminary experiments show that our proposed attributes are important indicators of such vulnerabilities.
[static code attributes, data mining, input sanitization methods, Predictive models, Web applications, HTML, Complexity theory, Web security risks, Security, Data mining, program compilers, cyclomatic complexity, defect prediction, input sanitization, cross site scripting vulnerabilities, mining input sanitization patterns, pattern classification, software modules, SQL injection prediction, SQL, Software, Data models, web security vulnerabilities, Internet, computational complexity]
Inferring developer expertise through defect analysis
2012 34th International Conference on Software Engineering
None
2012
Fixing defects is an essential software development activity. For commercial software vendors, the time to repair defects in deployed business-critical software products or applications is a key quality metric for sustained customer satisfaction. In this paper, we report on the analysis of about 1,500 defect records from an IBM middle-ware product collected over a five-year period. The analysis includes a characterization of each repaired defect by topic and a ranking of developers by inferred expertise on each topic. We find clear evidence that defect resolution time is strongly influenced by a specific developer and his/her expertise in the defect's topic. To validate our approach, we conducted interviews with the product's manager who provided us with his own ranking of developer expertise for comparison. We argue that our automated developer expertise ranking can be beneficial in the planning of a software project and is applicable beyond software support in the other phases of the software lifecycle.
[program debugging, quality metric, data mining, software lifecycle, maintenance engineering, Predictive models, Programming, software management, Bug Fixing, software development activity, business-critical software products, Mining Software Repositories, customer satisfaction, software project planning, software engineering, defect analysis, middleware, DP industry, Maintenance engineering, product manager, defect repair, commercial software vendors, IBM middle-ware product, Organizations, automated developer expertise ranking, Software, Planning, defect resolution time]
Green mining: Investigating power consumption across versions
2012 34th International Conference on Software Engineering
None
2012
Power consumption is increasingly becoming a concern for not only electrical engineers, but for software engineers as well, due to the increasing popularity of new power-limited contexts such as mobile-computing, smart-phones and cloud-computing. Software changes can alter software power consumption behaviour and can cause power performance regressions. By tracking software power consumption we can build models to provide suggestions to avoid power regressions. There is much research on software power consumption, but little focus on the relationship between software changes and power consumption. Most work measures the power consumption of a single software task; instead we seek to extend this work across the history (revisions) of a project. We develop a set of tests for a well established product and then run those tests across all versions of the product while recording the power usage of these tests. We provide and demonstrate a methodology that enables the analysis of power consumption performance for over 500 nightly builds of Firefox 3.6; we show that software change does induce changes in power consumption. This methodology and case study are a first step towards combining power measurement and mining software repositories research, thus enabling developers to avoid power regressions via power consumption awareness.
[green mining, data mining, mining software repositories, Data mining, cloud-computing, power consumption, smart-phones, software repositories research, Power measurement, power aware computing, Fires, mobile-computing, software engineering, power, Firefox 3.6, sustainable-software, software engineers, Power demand, power regressions, environmental factors, dynamic analysis, software power consumption behaviour, Green products, Air pollution, Software]
Multi-label software behavior learning
2012 34th International Conference on Software Engineering
None
2012
Software behavior learning is an important task in software engineering. Software behavior is usually represented as a program execution. It is expected that similar executions have similar behavior, i.e. revealing the same faults. Single-label learning has been used to assign a single label (fault) to a failing execution in the existing efforts. However, a failing execution may be caused by several faults simultaneously. Hence, it needs to assign multiple labels to support software engineering tasks in practice. In this paper, we present multi-label software behavior learning. A well-known multi-label learning algorithm ML-KNN is introduced to achieve comprehensive learning of software behavior. We conducted a preliminary experiment on two industrial programs: flex and grep. The experimental results show that multi-label learning can produce more precise and complete results than single-label learning.
[failure report classification, system recovery, Training, software development lifecycle, industrial programs, software engineering, learning (artificial intelligence), failure prediction, multi-label learning, pattern classification, multilabel software behavior learning, Software algorithms, Xenon, program execution, F-measure, flex industrial program, ML-KNN, Supervised learning, Software behavior learning, Machine learning, single-label learning, Software, software engineering tasks, failing execution, grep industrial program, Software engineering]
Trends in object-oriented software evolution: Investigating network properties
2012 34th International Conference on Software Engineering
None
2012
The rise of social networks and the accompanying interest to study their evolution has stimulated a number of research efforts to analyze their growth patterns by means of network analysis. The inherent graph-like structure of object-oriented systems calls for the application of the corresponding methods and tools to analyze software evolution. In this paper we investigate network properties of two open-source systems and observe interesting phenomena regarding their growth. Relating the observed evolutionary trends to principles and laws of software design enables a highlevel assessment of tendencies in the underlying design quality.
[object-oriented programming, object-oriented software evolution, Object oriented modeling, Social network services, public domain software, growth patterns, design quality, Communities, graph theory, software design, social networks, Maintenance engineering, object-oriented systems, software quality, software maintenance, open-source systems, software evolution, graph-like structure, object-oriented design, Vegetation, network properties, Software systems, social networking (online), network analysis]
Exploring techniques for rationale extraction from existing documents
2012 34th International Conference on Software Engineering
None
2012
The rationale for a software system captures the designers' and developers' intent behind the decisions made during its development. This information has many potential uses but is typically not captured explicitly. This paper describes an initial investigation into the use of text mining and parsing techniques for identifying rationale from existing documents. Initial results indicate that the use of linguistic features results in better precision but significantly lower recall than using text mining.
[Text mining, text analysis, exploring techniques, data mining, rationale extraction, Ontologies, parsing techniques, program compilers, software system, rationale, Pragmatics, grammars, linguistic features, Logic gates, Feature extraction, text mining, Software engineering]
Continuous social screencasting to facilitate software tool discovery
2012 34th International Conference on Software Engineering
None
2012
The wide variety of software development tools available today have a great potential to improve the way developers make software, but that potential goes unfulfilled when developers are not aware of useful tools. In this paper, I introduce the idea of continuous social screencasting, a novel mechanism to help developers gain awareness of relevant tools by enabling them to learn remotely and asychronously from their peers. The idea builds on the strength of several existing techniques that developers already use for discovering new tools, including screencasts and online social networks.
[Communities, Blogs, Buildings, online social networks, Programming, software tool discovery, Servers, continuous social screencasting, social networking (online), Software, software tools, software development tools, Monitoring]
UDesignIt: Towards social media for community-driven design
2012 34th International Conference on Software Engineering
None
2012
Online social networks are now common place in day-to-day lives. They are also increasingly used to drive social action initiatives, either led by government or communities themselves (e.g., SeeClickFix, LoveLewisham.org, mumsnet). However, such initiatives are mainly used for crowd sourcing community views or coordinating activities. With the changing global economic and political landscape, there is an ever pressing need to engage citizens on a large-scale, not only in consultations about systems that affect them, but also involve them directly in the design of these very systems. In this paper we present the UDesignIt platform that combines social media technologies with software engineering concepts to empower communities to discuss and extract high-level design features. It combines natural language processing, feature modelling and visual overlays in the form of &#x201C;image clouds&#x201D; to enable communities and software engineers alike to unlock the knowledge contained in the unstructured and unfiltered content of social media where people discuss social problems and their solutions. By automatically extracting key themes and presenting them in a structured and organised manner in near real-time, the approach drives a shift towards large-scale engagement of community stakeholders for system design.
[Visualization, End-user software engineering, crowd sourcing community views, feature modelling, Communities, community stakeholders, online social networks, community-driven design, political landscape, Software design, Semantics, Natural language processing, software engineering, Requirements engineering, software engineering concepts, high-level design features, Social network services, natural language processing, large-scale engagement, Media, UDesignIt platform, visual overlays, Humans and social aspects, global economic, social action initiatives, social media technologies, Feature extraction, social networking (online)]
Influencing the adoption of software engineering methods using social software
2012 34th International Conference on Software Engineering
None
2012
Software engineering research and practice provide a wealth of methods that improve the quality of software and lower the costs of producing it. Even though processes mandate their use, methods are not employed consequently. Software developers and development organizations thus cannot fully benefit from these methods. We propose a method that, for a given software engineering method, provides instructions on how to improve its adoption using social software. This employs the intrinsic motivation of software developers rather than prescribing behavior. As a result, we believe that software engineering methods will be applied better and more frequently.
[Measurement, social software, Social network services, human factors, Crystals, Media, software developers, Process, software quality, software development organizations, Virtual Communities, Systematics, Motivation, Social Network Sites, social networking (online), Software, intrinsic motivation, CSCW, Adoption, Software engineering, software engineering methods, Social Software]
Toward actionable, broadly accessible contests in Software Engineering
2012 34th International Conference on Software Engineering
None
2012
Software Engineering challenges and contests are becoming increasingly popular for focusing researchers' efforts on particular problems. Such contests tend to follow either an exploratory model, in which the contest holders provide data and ask the contestants to discover &#x201C;interesting things&#x201D; they can do with it, or task-oriented contests in which contestants must perform a specific task on a provided dataset. Only occasionally do contests provide more rigorous evaluation mechanisms that precisely specify the task to be performed and the metrics that will be used to evaluate the results. In this paper, we propose actionable and crowd-sourced contests: actionable because the contest describes a precise task, datasets, and evaluation metrics, and also provides a downloadable operating environment for the contest; and crowd-sourced because providing these features creates accessibility to Information Technology hobbyists and students who are attracted by the challenge. Our proposed approach is illustrated using research challenges from the software traceability area as well as an experimental workbench named TraceLab.
[Measurement, evaluation metrics, Conferences, program diagnostics, information technology, Communities, software traceability area, evaluation mechanisms, actionable broadly accessible contests, Information retrieval, datasets, Data mining, crowd-sourced contests, Traceability, Empirical Software Engineering, task-oriented contests, TraceLab, Software, social sciences computing, software engineering, Contest, Software engineering, software metrics]
CodeTimeline: Storytelling with versioning data
2012 34th International Conference on Software Engineering
None
2012
Working with a software system typically requires knowledge of the system's history, however this knowledge is often only tribal memory of the development team. In past user studies we have observed that when being presented with collaboration views and word clouds from the system's history engineers start sharing memories linked to those visualizations. In this paper we propose an approach based on a storytelling visualization, which is designed to entice engineers to share and document their tribal memory. Sticky notes can be used to share memories of a system's lifetime events, such as past design rationales but also more casual memories like pictures from after-work beer or a hackathon. We present an early-stage prototype implementation and include two design studies created using that prototype.
[Visualization, Software Evolution, tribal memory, Software Visualization, early-stage prototype implementation, codetimeline, Tools and Environments, History, software system, storytelling visualization, humanities, versioning data, Data visualization, Prototypes, Collaboration, data visualisation, Sticky notes, Tag clouds, Software, collaboration views, program visualisation, Humans and Social Aspects]
Analyzing multi-agent systems with probabilistic model checking approach
2012 34th International Conference on Software Engineering
None
2012
Multi-agent systems, which are composed of autonomous agents, have been successfully employed as a modeling paradigm in many scenarios. However, it is challenging to guarantee the correctness of their behaviors due to the complex nature of the autonomous agents, especially when they have stochastic characteristics. In this work, we propose to apply probabilistic model checking to analyze multi-agent systems. A modeling language called PMA is defined to specify such kind of systems, and LTL property and logic of knowledge combined with probabilistic requirements are supported to analyze system behaviors. Initial evaluation indicates the effectiveness of our current progress; meanwhile some challenges and possible solutions are discussed as our ongoing work.
[LTL property, modeling language, Multiagent systems, multi-agent systems, PMA, Probabilistic model checking, Probabilistic logic, Educational institutions, Cognition, stochastic characteristics, probabilistic model checking approach, Analytical models, formal verification, Semantics, Games, system behaviors, knowledge logic, stochastic processes, autonomous agents, Multi-agent systems]
BRACE: An assertion framework for debugging cyber-physical systems
2012 34th International Conference on Software Engineering
None
2012
Developing cyber-physical systems (CPS) is challenging because correctness depends on both logical and physical states, which are collectively difficult to observe. The developer often need to repeatedly rerun the system while observing its behavior and tweak the hardware and software until it meets minimum requirements. This process is tedious, error-prone, and lacks rigor. To address this, we propose BRACE, A framework that simplifies the process by enabling developers to correlate cyber (i.e., logical) and physical properties of the system via assertions. This paper presents our initial investigation into the requirements and semantics of such assertions, which we call CPS assertions. We discusses our experience implementing and using the framework with a mobile robot, and highlight key future research challenges.
[program debugging, physical properties, mobile robot, mobile robots, assertion framework, debugging tools, Temperature sensors, cyber-physical systems, Robot kinematics, Robot sensing systems, Cameras, BRACE, CPS assertions, Monitoring]
Augmenting test suites effectiveness by increasing output diversity
2012 34th International Conference on Software Engineering
None
2012
The uniqueness (or otherwise) of test outputs ought to have a bearing on test effectiveness, yet it has not previously been studied. In this paper we introduce a novel test suite adequacy criterion based on output uniqueness. We propose 4 definitions of output uniqueness with varying degrees of strictness. We present a preliminary evaluation for web application testing that confirms that output uniqueness enhances fault-finding effectiveness. The approach outperforms random augmentation in fault finding ability by an overall average of 280% in 5 medium sized, real world web applications.
[Web application testing, program testing, Instruments, test suite adequacy criterion, software testing, HTML output, fault-finding effectiveness, Cloning, SBSE, Web applications, Educational institutions, HTML, software fault tolerance, real world Web applications, augment test suites effectiveness, Databases, Web pages, Internet, output diversity, Testing]
Improving IDE recommendations by considering global implications of existing recommendations
2012 34th International Conference on Software Engineering
None
2012
Modern integrated development environments (IDEs) offer recommendations to aid development, such as auto-completions, refactorings, and fixes for compilation errors. Recommendations for each code location are typically computed independently of the other locations. We propose that an IDE should consider the whole codebase, not just the local context, before offering recommendations for a particular location. We demonstrate the potential benefits of our technique by presenting four concrete scenarios in which the Eclipse IDE fails to provide proper Quick Fixes at relevant locations, even though it offers those fixes at other locations. We describe a technique that can augment an existing IDE's recommendations to account for non-local information. For example, when some compilation errors depend on others, our technique helps the developer decide which errors to resolve first.
[Java, whole codebase, Quick Fixes, Educational institutions, integrated development environments, Eclipse IDE, compilation errors, program compilers, Engines, recommender systems, USA Councils, Whales, IDE recommendations, Abstracts, code location, concrete scenarios, Concrete, nonlocal information, existing recommendations global implications]
Towards flexible evolution of Dynamically Adaptive Systems
2012 34th International Conference on Software Engineering
None
2012
Modern software systems need to be continuously available under varying conditions. Their ability to dynamically adapt to their execution context is thus increasingly seen as a key to their success. Recently, many approaches were proposed to design and support the execution of Dynamically Adaptive Systems (DAS). However, the ability of a DAS to evolve is limited to the addition, update or removal of adaptation rules or reconfiguration scripts. These artifacts are very specific to the control loop managing such a DAS and runtime evolution of the DAS requirements may affect other parts of the DAS. In this paper, we argue to evolve all parts of the loop. We suggest leveraging recent advances in model-driven techniques to offer an approach that supports the evolution of both systems and their adaptation capabilities. The basic idea is to consider the control loop itself as an adaptive system.
[Adaptation models, Adaptive systems, open systems, dynamically adaptive systems, runtime evolution, open distributed systems, software systems, Models@Run.Time, adaptation rule update, Cognition, adaptation rule removal, model-driven techniques, Runtime, adaptation capabilities, Evolution, flexible evolution, Monitoring, Business, Context, program control structures, control loop manegement, reconfiguration script addition, software maintenance, DAS, reconfiguration script update, adaptation rule addition, reconfiguration script removal, Dynamically Adaptive Systems]
Towards business processes orchestrating the physical enterprise with wireless sensor networks
2012 34th International Conference on Software Engineering
None
2012
The industrial adoption of wireless sensor networks (WSNs) is hampered by two main factors. First, there is a lack of integration of WSNs with business process modeling languages and back-ends. Second, programming WSNs is still challenging as it is mainly performed at the operating system level. To this end, we provide makeSense: a unified programming framework and a compilation chain that, from high-level business process specifications, generates code ready for deployment on WSN nodes.
[Actuators, Protocols, wireless sensor networks, compilation chain, Programming, operating system level, makeSense, Wireless sensor networks, business process specifications, Program processors, back-ends, business process modeling languages, Ventilation, operating systems (computers), physical enterprise, business data processing, Business]
Engineering and verifying requirements for programmable self-assembling nanomachines
2012 34th International Conference on Software Engineering
None
2012
We propose an extension of van Lamsweerde's goal-oriented requirements engineering to the domain of programmable DNA nanotechnology. This is a domain in which individual devices (agents) are at most a few dozen nanometers in diameter. These devices are programmed to assemble themselves from molecular components and perform their assigned tasks. The devices carry out their tasks in the probabilistic world of chemical kinetics, so they are individually error-prone. However, the number of devices deployed is roughly on the order of a nanomole (a 6 followed by fourteen 0s), and some goals are achieved when enough of these agents achieve their assigned subgoals. We show that it is useful in this setting to augment the AND/OR goal diagrams to allow goal refinements that are mediated by threshold functions, rather than ANDs or ORs. We illustrate this method by engineering requirements for a system of molecular detectors (DNA origami &#x201C;pliers&#x201D; that capture target molecules) invented by Kuzuya, Sakai, Yamazaki, Xu, and Komiyama (2011). We model this system in the Prism probabilistic symbolic model checker, and we use Prism to verify that requirements are satisfied, provided that the ratio of target molecules to detectors is neither too high nor too low. This gives prima facie evidence that software engineering methods can be used to make DNA nanotechnology more productive, predictable and safe.
[molecular components, goal-oriented requirements engineering, requirement engineering, Shape, molecular detectors, goal refinements, formal specification, formal verification, chemical kinetics, biology computing, safety, nanotechnology, target molecules, DNA nanotechnology, Requirements engineering, validation and verification, probability, programmable DNA nanotechnology, Probabilistic logic, Nanobioscience, molecular programming, Self-assembly, DNA, Nanoscale devices, programmable self-assembling nanomachines, Prism probabilistic symbolic model checker, software engineering methods]
Facilitating communication between engineers with CARES
2012 34th International Conference on Software Engineering
None
2012
When software developers need to exchange information or coordinate work with colleagues on other teams, they are often faced with the challenge of finding the right person to communicate with. In this paper, we present our tool, called CARES (Colleagues and Relevant Engineers' Support), which is an integrated development environment-based (IDE) tool that enables engineers to easily discover and communicate with the people who have contributed to the source code. CARES has been deployed to 30 professional developers, and we interviewed 8 of them after 3 weeks of evaluation. They reported that CARES helped them to more quickly find, choose, and initiate contact with the most relevant and expedient person who could address their needs.
[Availability, Visualization, software products, Companies, Programming, colleagues and relevant engineers support, Electronic mail, IDE, CARES, development environment-based tool, coordination, USA Councils, groupware, Software, software engineering, CSCW]
Interactive refinement of combinatorial test plans
2012 34th International Conference on Software Engineering
None
2012
Combinatorial test design (CTD) is an effective test planning technique that reveals faulty feature interactions in a given system. The test space is modeled by a set of parameters, their respective values, and restrictions on the value combinations. A subset of the test space is then automatically constructed so that it covers all valid value combinations of every t parameters, where t is a user input. When applying CTD to real-life testing problems, it can often occur that the result of CTD cannot be used as is, and manual modifications to the tests are performed. One example is very limited resources that significantly reduce the number of tests that can be used. Another example is complex restrictions that are not captured in the model of the test space. The main concern is that manually modifying the result of CTD might potentially introduce coverage gaps that the user is unaware of. In this paper we present a tool that supports interactive modification of a combinatorial test plan, both manually and with tool assistance. For each modification, the tool displays the new coverage gaps that will be introduced, and enables the user to take educated decisions on what to include in the final set of tests.
[combinatorial mathematics, program testing, tool assistance, combinatorial test design, interactive modification, Educational institutions, Multiaccess communication, interactive refinement, Global Positioning System, Analytical models, value combination, Planning, software tools, test planning technique, combinatorial test plan, test space, Testing, Graphical user interfaces]
TraceLab: An experimental workbench for equipping researchers to innovate, synthesize, and comparatively evaluate traceability solutions
2012 34th International Conference on Software Engineering
None
2012
TraceLab is designed to empower future traceability research, through facilitating innovation and creativity, increasing collaboration between researchers, decreasing the startup costs and effort of new traceability research projects, and fostering technology transfer. To this end, it provides an experimental environment in which researchers can design and execute experiments in TraceLab's visual modeling environment using a library of reusable and user-defined components. TraceLab fosters research competitions by allowing researchers or industrial sponsors to launch research contests intended to focus attention on compelling traceability challenges. Contests are centered around specific traceability tasks, performed on publicly available datasets, and are evaluated using standard metrics incorporated into reusable TraceLab components. TraceLab has been released in beta-test mode to researchers at seven universities, and will be publicly released via CoEST.org in the summer of 2012. Furthermore, by late 2012 TraceLab's source code will be released as open source software, licensed under GPL. TraceLab currently runs on Windows but is designed with cross platforming issues in mind to allow easy ports to Unix and Mac environments.
[Measurement, Unix, Java, GPL, open source software, visual modeling environment, computerised instrumentation, Benchmarks, technology transfer, traceability research projects, Instrumentation, Mac environment, Experiments, Traceability, Unix environment, Benchmark testing, TraceLab, Software, Libraries, innovation management, eXtreme Software Engineering Lab, Software engineering, Principal component analysis]
Specification engineering and modular verification using a web-integrated verifying compiler
2012 34th International Conference on Software Engineering
None
2012
This demonstration will present the RESOLVE web-integrated environment, which has been especially built to capture component relationships and allow construction and composition of verified generic components. The environment facilitates team-based software development and has been used in undergraduate CS education at multiple institutions. The environment makes it easy to simulate &#x201C;what if&#x201D; scenarios, including the impact of alternative specification styles on verification, and has spawned much research and experimentation. The demonstration will illustrate the issues in generic software verification and the role of higher-order assertions. It will show how logical errors are pinpointed when verification fails. Introductory video URL: http://www.youtube.com/watch?v=9vg3WuxeOkA.
[RESOLVE web-integrated environment, program verification, education, Cognition, formal specification, program compilers, modular verification, automation, generic software verification, verification, team-based software development, Java, computer science education, Educational institutions, Web-integrated verifying compiler, specification, generic components, web IDE, Sorting, Component architectures, specification engineering, component relationships, system description, alternative specification styles, Software, logical errors, educational institutions, Internet, higher-order assertions, undergraduate CS education, Software engineering]
Writing dynamic service orchestrations with DSOL
2012 34th International Conference on Software Engineering
None
2012
We present the workflow language DSOL, its runtime system and the tools available to support the development of dynamic service orchestrations. DSOL aims at supporting dynamic, self-managed service compositions that can adapt to changes occurring at runtime.
[Java, self-managed service compositions, declarative language, workflow language, DSOL, Service oriented computing, runtime system, Runtime, Web services, XML, Abstracts, specification languages, dynamic service orchestrations, service orchestration, service oriented computing, Concrete, Monitoring, service-oriented architecture]
MASH: A tool for end-user plug-in composition
2012 34th International Conference on Software Engineering
None
2012
Most of the modern Integrated Development Environments are developed with plug-in based architectures that can be extended with additional functionalities and plug-ins, according to user needs. However, extending an IDE is still a possibility restricted to developers with deep knowledge about the specific development environment and its architecture. In this paper we present MASH, a tool that eases the programming of Integrated Development Environments. The tool supports the definition of workflows that can be quickly designed to integrate functionalities offered by multiple plugins, without the need of knowing anything about the internal architecture of the IDE. Workflows can be easily reshaped every time an analysis must be modified, without the need of producing Java code and deploying components in the IDE. Early results suggest that this approach can effectively facilitate programming of IDEs.
[Multi-stage noise shaping, add-on boards, Visualization, workflow definition, Debugging, end-user plug-in composition tool, Programming, integrated development environments, MASH, IDE, Engines, end-user programming, Computer architecture, software engineering, plug-in composition, workflow management software, plug-in based architectures, Graphical user interfaces]
BabelRef: Detection and renaming tool for cross-language program entities in dynamic web applications
2012 34th International Conference on Software Engineering
None
2012
In a dynamic web application, client-side code is often dynamically generated from server-side code. Client-side program entities such as HTML presentation elements and Javascript functions/variables are embedded within server-side string literals or variables' values. However, existing tools for code maintenance such as automatic renaming support only work for program entities in a single language on either the server side or the client side. In this paper, we introduce BabelRef, a novel tool that is able to automatically identify and rename client-side program entities and their references that are embedded within server-side code.
[client-side code, HTML presentation elements, dynamic Web application, automatic renaming support, Conferences, detection tool, Web applications, HTML, Servers, program compilers, cross-language program entity, variable values, Semantics, Javascript variables, Cross-language, Refactoring, hypermedia markup languages, Java, client-server systems, code maintenance, server-side code, Educational institutions, Browsers, server-side string literals, client-side program entity identification, software maintenance, renaming tool, BabelRef, Javascript functions, code dynamic generation, Internet, Web sites]
MDSheet: A framework for model-driven spreadsheet engineering
2012 34th International Conference on Software Engineering
None
2012
In this paper, we present MDSheet, a framework for the embedding, evolution and inference of spreadsheet models. This framework offers a model-driven software development mechanism for spreadsheet users.
[Visualization, Software Evolution, MDSheet, Object oriented modeling, spreadsheet model embedding, Embedded DSLs, Unified modeling language, Spreadsheets, Model Inference, Model-Driven Engineering (MDE), spreadsheet users, spreadsheet programs, model-driven spreadsheet engineering framework, Synchronization, software maintenance, Layout, spreadsheet model inference, Data models, model-driven software development mechanism, spreadsheet model evolution, Business]
WorkItemExplorer: Visualizing software development tasks using an interactive exploration environment
2012 34th International Conference on Software Engineering
None
2012
This demo introduces WorkItemExplorer, an interactive environment to visually explore data from software development tasks. WorkItemExplorer enables developers and managers to investigate activity and correlations in their task management system by making data exploration flexible and interactive, and by utilizing multiple coordinated views. Our preliminary evaluation shows that WorkItemExplorer is able to answer questions that developers ask, while also enabling them to gain new insights through the free exploration of data.
[task management, visualization, software development management, WorkItemExplorer, Programming, task management system, task analysis, software manager, coordinated views, Heating, Data visualization, data visualisation, interactive systems, interactive exploration environment, data exploration, data visualization, software development task, software developer, Usability, Bars, Software engineering]
Runtime monitoring of component changes with Spy@Runtime
2012 34th International Conference on Software Engineering
None
2012
We present Spy@Runtime, a tool to infer and work with behavior models. Spy@Runtime generates models through a dynamic black box approach and is able to keep them updated with observations coming from actual system execution. We also show how to use models describing the protocol of interaction of a software component to detect and report functional changes as soon as they are discovered. Monitoring functional properties is particularly useful in an open environment in which there is a distributed ownership of a software system. Parts of the system may be changed independently and therefore it becomes necessary to monitor the component's behavior at run time.
[behavior models, model inference, Java, Spy@Runtime, dynamic black box approach, Protocols, component changes, Model Inference, Runtime Monitoring, inference mechanisms, software system, software component, functional changes, functional properties, Analytical models, Runtime, runtime monitoring, system monitoring, interaction protocol, Software, protocols, distributed ownership, Integrated circuit modeling, Monitoring]
GraPacc: A graph-based pattern-oriented, context-sensitive code completion tool
2012 34th International Conference on Software Engineering
None
2012
Code completion tool plays an important role in daily development activities. It helps developers by auto-completing tedious and detailed code during an editing session. However, existing code completion tools are limited to recommending only context-free code templates and a single method call of the variable under editing. We introduce GraPacc, an advanced, context-sensitive code completion tool that is based on frequent API usage patterns. It extracts the context-sensitive features from the code under editing, for example, the API elements on focus and the current editing point, and their relations to other code elements. It then ranks the relevant API usage patterns and auto-completes the current code with the proper elements according to the chosen pattern.
[Context, context-sensitive code completion tool, application program interfaces, editing session, Switches, Programming, API usage pattern, Usage Pattern, Vectors, Indexes, graph-based pattern-oriented code completion tool, Pattern-oriented Code Completion, GraPacc, Feature extraction, context-free code templates, software tools]
Code Bubbles: A practical working-set programming environment
2012 34th International Conference on Software Engineering
None
2012
Our original work on the Code Bubbles environment demonstrated that a working-set based framework for software development showed promise. We have spent the past several years extending the underlying concepts into a fully-functional system. In our demonstration, we will show the current Code Bubbles environment for Java, how it works, how it can be used, and why we prefer it over more traditional programming environments. We will also show how we have extended the framework to enhance software development tasks such as complex debugging, testing, and collaboration. This paper describes the features we will demonstrate.
[Context, program debugging, Java, software development, software testing, complex debugging, Debugging, Programming, integrated development environments, History, fully-functional system, working sets, collaborative tools, Code Bubbles environment, Collaboration, collaboration, groupware, debugging, software engineering, Testing, Software engineering, working-set programming environment]
EVOSS: A tool for managing the evolution of free and open source software systems
2012 34th International Conference on Software Engineering
None
2012
Software systems increasingly require to deal with continuous evolution. In this paper we present the EVOSS tool that has been defined to support the upgrade of free and open source software systems. EVOSS is composed of a simulator and of a fault detector component. The simulator is able to predict failures before they can affect the real system. The fault detector component has been defined to discover inconsistencies in the system configuration model. EVOSS improves the state of the art of current tools, which are able to predict a very limited set of upgrade faults, while they leave a wide range of faults unpredicted.
[upgrade fault prediction, public domain software, evolution management, software management, software maintenance, Configuration management and deployment, Open source software, software fault tolerance, configuration management, Analytical models, simulator component, Software evolution, Fault detection, Linux, Tools and environments, Tagging, system configuration model, fault detector component, DSL, EVOSS tool, free and open source software systems, Model-driven software engineering]
Supporting extract class refactoring in Eclipse: The ARIES project
2012 34th International Conference on Software Engineering
None
2012
During software evolution changes are inevitable. These changes may lead to design erosion and the introduction of inadequate design solutions, such as design antipatterns. Several empirical studies provide evidence that the presence of antipatterns is generally associated with lower productivity, greater rework, and more significant design efforts for developers. In order to improve the quality and remove antipatterns, refactoring operations are needed. In this demo, we present the Extract class features of ARIES (Automated Refactoring In EclipSe), an Eclipse plug-in that supports the software engineer in removing the &#x201C;Blob&#x201D; antipattern.
[Measurement, design antipatterns, automated refactoring In EclipSe, Eclipse plug-in, software quality, Blob antipattern, software maintenance, ARIES project, software evolution, Couplings, Design, Databases, Education, Clustering algorithms, extract class refactoring, Quality, Feature extraction, Software, object-oriented methods, Refactoring]
EXSYST: Search-based GUI testing
2012 34th International Conference on Software Engineering
None
2012
Test generation tools commonly aim to cover structural artefacts of software, such as either the source code or the user interface. However, focusing only on source code can lead to unrealistic or irrelevant test cases, while only exploring a user interface often misses much of the underlying program behavior. Our EXSYST prototype takes a new approach by exploring user interfaces while aiming to maximize code coverage, thus combining the best of both worlds. Experiments show that such an approach can achieve high code coverage matching and exceeding the code coverage of traditional unit-based test generators; yet, by construction every test case is realistic and relevant, and every detected failure can be shown to be caused by a real sequence of input events.
[Calculators, Shape, program testing, program behavior, graphical user interfaces, EXSYST, source code, search-based GUI testing, system testing, Educational institutions, software structural artefact, unit-based test generator, Generators, system recovery, user interface, code coverage matching, test generation tool, test case generation, test coverage, GUI testing, failure detection, Graphical user interfaces, Testing]
JavaMOP: Efficient parametric runtime monitoring framework
2012 34th International Conference on Software Engineering
None
2012
Runtime monitoring is a technique usable in all phases of the software development cycle, from initial testing, to debugging, to actually maintaining proper function in production code. Of particular importance are parametric monitoring systems, which allow the specification of properties that relate objects in a program, rather than only global properties. In the past decade, a number of parametric runtime monitoring systems have been developed. Here we give a demonstration of our system, JavaMOP. It is the only parametric monitoring system that allows multiple differing logical formalisms. It is also the most efficient in terms of runtime overhead, and very competitive with respect to memory usage.
[Java, parametric runtime monitoring system, runtime overhead, testing, parametric runtime monitoring framework, memory usage, monitoring, production code, Runtime, JavaMOP, runtime monitoring, debugging, aspect-oriented programming, Software, software engineering, runtime verification, Object oriented programming, software development cycle, Monitoring, logical formalism, Testing]
Augmenting test suites automatically
2012 34th International Conference on Software Engineering
None
2012
We present an approach to augment test suites with automatically generated integration test cases. Our approach utilizes existing test cases to direct generation towards producing complex object interactions and execution sequences that have not been observed before.
[Java, program testing, software testing, Manuals, complex object interactions, Educational institutions, automatic test generation, automatic generated integration test cases, Data mining, execution sequences, augment test suites, Prototypes, Libraries, unit and integration testing, Testing]
Using the GPGPU for scaling up Mining Software Repositories
2012 34th International Conference on Software Engineering
None
2012
The Mining Software Repositories (MSR) field integrates and analyzes data stored in repositories such as source control and bug repositories to support practitioners. Given the abundance of repository data, scaling up MSR analyses has become a major challenge. Recently, researchers have experimented with conventional techniques like a supercomputer or cloud computing, but these are either too expensive or too hard to configure. This paper proposes to scale up MSR analysis using &#x201C;general-purpose computing on graphics processing units&#x201D; (GPGPU) on off-the-shelf video cards. In a representative MSR case study to measure co-change on version history of the Eclipse project, we find that the GPU approach is up to a factor of 43.9 faster than a CPU-only approach.
[supercomputer, source control, data mining, mining software repositories, off-the-shelf video cards, Supercomputers, general-purpose computing on graphics processing units, Data mining, History, bug repositories, graphics processing units, MSR, Eclipse project, GPGPU, Graphics processing unit, Computer bugs, software packages, Arrays, cloud computing]
FastFix: Monitoring control for remote software maintenance
2012 34th International Conference on Software Engineering
None
2012
Software maintenance and support services are key factors to the customer perception of software product quality. The overall goal of FastFix is to provide developers with a real-time maintenance environment that increases efficiency and reduces costs, improving accuracy in identification of failure causes and facilitating their resolution. To achieve this goal, FastFix observes application execution and user interaction at runtime. We give an overview of the functionality of FastFix and present one of its main application scenarios.
[Software maintenance, Correlation, fault replication, application execution, user interaction, software quality, remote software maintenance, event correlation, product quality, context-aware software engineering, monitoring control, Sensors, Monitoring, support services, real-time maintenance environment, Context, cost reduction, increase efficiency, Maintenance engineering, software maintenance, software fault tolerance, customer perception, failure cause identification accuracy improvement, FastFix, software product quality, self-healing]
Modeling Cloud performance with Kriging
2012 34th International Conference on Software Engineering
None
2012
Cloud infrastructures allow service providers to implement elastic applications. These can be scaled at runtime to dynamically adjust their resources allocation to maintain consistent quality of service in response to changing working conditions, like flash crowds or periodic peaks. Providers need models to predict the system performances of different resource allocations to fully exploit dynamic application scaling. Traditional performance models such as linear models and queueing networks might be simplistic for real Cloud applications; moreover, they are not robust to change. We propose a performance modeling approach that is practical for highly variable elastic applications in the Cloud and automatically adapts to changing working conditions. We show the effectiveness of the proposed approach for the synthesis of a self-adaptive controller.
[Adaptation models, Cloud computing, performance modeling approach, Computational modeling, self-adaptive controller, cloud performance, Quality of service, elastic applications, Predictive models, Performance modeling, Virtual machining, Surrogate Models, Kriging, Auto-Scaling, cloud infrastructures, Resource management, cloud computing, software performance evaluation]
SOA adoption in the Italian industry
2012 34th International Conference on Software Engineering
None
2012
We conducted a personal opinion survey in two rounds - years 2008 and 2011 - with the aim of investigating the level of knowledge and adoption of SOA in the Italian industry. We are also interested in understanding what is the trend of SOA (positive or negative?) and what are the methods, technologies and tools really used in the industry. The main findings of this survey are the following: (1) SOA is a relevant phenomenon in Italy, (2) Web services and RESTFul services are well-known/used and (3) orchestration languages and UDDI are little known and used. These results suggest that in Italy SOA is interpreted in a more simplistic way with respect to the current/real definition (i.e., without the concepts of orchestration/choreography and registry). Currently, the adoption of SOA is medium/low with a stable/positive trend of pervasiveness.
[Industries, orchestration languages, SOA, Service oriented architecture, DP industry, Companies, RESTFul services, Semiconductor optical amplifiers, Rest, Web services, UDDI, Italian industry, software houses, SOA adoption, service oriented architecture, service-oriented architecture, Industrial Survey]
A bidirectional model-driven spreadsheet environment
2012 34th International Conference on Software Engineering
None
2012
In this extended abstract we present a bidirectional model-driven framework to develop spreadsheets. By being model driven, our approach allows to evolve a spreadsheet model and automatically have the data co-evolved. The bidirectional component achieves precisely the inverse, that is, to evolve the data and automatically obtain a new model to which the data conforms.
[Visualization, Software Evolution, Object oriented modeling, Computational modeling, Spreadsheets, spreadsheet programs, bidirectional component, bidirectional model-driven spreadsheet environment, Bidirectional Transformations, Abstracts, Data models, Software, business applications, business data processing, Model-driven Engineering, Model Evolution, Software engineering]
A self-healing technique for Java applications
2012 34th International Conference on Software Engineering
None
2012
Despite the best design practices and testing techniques, many faults exist and manifest themselves in deployed software. In this paper we propose a self-healing framework that aims to mask fault manifestations at runtime in Java applications by automatically applying workarounds. The framework integrates a checkpoint-recovery mechanism to restore a consistent state after the failure, and a mechanism to replace the Java code at runtime to apply the workaround.
[checkpointing, Java, program testing, Redundancy, Containers, Java applications, testing techniques, Self-healing, Equivalent sequences, software fault tolerance, Runtime, self-healing technique, fault manifestations, Software systems, Failure avoidance, checkpoint-recovery mechanism, Testing, Checkpoint-recovery]
When open source turns cold on innovation &#x2014; The challenges of navigating licensing complexities in new research domains
2012 34th International Conference on Software Engineering
None
2012
In this poster, we review the limitations open source licences introduce to the application of Linked Data in Software Engineering. We investigate whether open source licences support special requirements to publish source code as Linked Data on the Internet.
[Law, public domain software, linked data in software engineering, source code, Licenses, licensing complexities, Complexity theory, license, open source, Databases, Linked Data, Software, software engineering, Internet, open source licences, Software engineering]
Language modularity with the MPS language workbench
2012 34th International Conference on Software Engineering
None
2012
JetBrains MPS is a comprehensive environment for language engineering. New languages can be defined as standalone languages or as modular extensions of existing languages. Since MPS is a projectional editor, syntactic forms other than text are possible, including tables or mathematical symbols. This demo will show MPS based on mbeddr C, a novel approach for embedded software development that makes use of incremental language extension on the basis of C.
[Licenses, language extension, mbeddr C, Grammar, embedded software development, C language, incremental language extension, language modularity, language engineering, language composition, Embedded software, MPS language workbench, Syntactics, Software systems, Concrete, software engineering, Software engineering]
Mining application repository to recommend XML configuration snippets
2012 34th International Conference on Software Engineering
None
2012
Framework-based applications controlled by XML configuration files are quite popularly used in current commercial applications. However, most of these frameworks are complex or not well documented, which poses a great challenge for programmers to correctly utilize them. To overcome these difficulties, we propose a new tool to recommend XML configuration snippets automatically through mining tree patterns and pattern associations from the application repository with the aim of assisting the programmer to generate proper XML configurations during the production phase. In this demo, we showcase this tool by presenting the major techniques behind the tool and the typical usage scenarios of our tool.
[Context, XML configuration generation, software development, data mining, Programming, application repository mining, framework-based applications, production phase, Association rules, tree pattern mining, code reuse, Databases, code generation, XML, pattern association mining, Syntactics, software reusability, XML configuration files, tree data structures, XML configuration snippets]
Locating features in dynamically configured avionics software
2012 34th International Conference on Software Engineering
None
2012
Locating features in software is an important activity for program comprehension and to support software reengineering. We present a novel automated approach to locate features in source code based on static analysis and model checking. The technique is aimed at dynamically configured software, which is software in which the activation of specific features is controlled by configuration variables. The approach is evaluated on an industrial avionics system.
[program comprehension, Program Comprehension, program diagnostics, source code, Reengineering, Aerospace electronics, static analysis, Information retrieval, feature location, Feature Location, avionics, Security, Feature Mapping, Feature, software reengineering, Analytical models, systems re-engineering, formal verification, model checking, configuration variables, industrial avionics system, Feature extraction, Software, Aircraft]
Detecting metadata bugs on the fly
2012 34th International Conference on Software Engineering
None
2012
Programmers are spending a large and increasing amount of their time writing and modifying metadata, such as Java annotations and XML deployment descriptors. And yet, automatic bug finding tools cannot find metadata-related bugs introduced during program refactoring and enhancement. To address this shortcoming, we have created metadata invariants, a new programming abstraction that expresses naming and typing relationships between metadata and the main source code of a program. A paper that appears in the main technical program of ICSE 2012 describes the idea, concept, and prototype of metadata invariants [4]. The goal of this demo is to supplement that paper with a demonstration of our Eclipse plugin, Metadata Bug Finder (MBF). MBF takes as input a script written in our domain-specific language that describes a set of metadata coding conventions the programmer wishes to enforce. Then after each file save operation, MBF checks the edited codebase for the presence of any violations of the given metadata programming conventions. These violations are immediately reported to the programmer as potential metadata-related bugs. By making the programmer aware of these potential bugs, MBF prevents them from seeping into production, thereby improving the overall correctness of the edited codebase.
[program debugging, metadata, frameworks, main source code, Programming, Java annotations, metadata bug finder, enhancement, program refactoring, MBF, refactoring, Testing, Java, meta data, domain-specific languages, metadata bugs detection, programming abstraction, metadata invariants, ICSE 2012, XML deployment descriptors, main technical program, Encoding, software maintenance, bug finding, program enhancement, metadata coding conventions, domain-specific language, Computer bugs, XML, metadata programming conventions, Software, Eclipse plugin, invariants]
Blaze
2012 34th International Conference on Software Engineering
None
2012
Understanding source code is crucial for successful software maintenance. To understand source code, navigation in the call graph has been shown to be particularly important. Programmers often employ a two-phased strategy for effective call graph exploration. We present Blaze, a source code exploration tool designed to explicitly support this strategy. In a study, we show that call graph exploration tools significantly increase success rates in typical software maintenance tasks and that using Blaze significantly reduces task completion times compared to using the Call Hierarchy or Xcode.
[Software maintenance, Visualization, Program comprehension, Navigation, Xcode, Maintenance engineering, Educational institutions, reverse engineering, two-phased strategy, call hierarchy, software maintenance, source code understanding, source code exploration tool, Tools and environments, Computer bugs, Blaze, call graph exploration, Software visualization]
ConTexter feedback system
2012 34th International Conference on Software Engineering
None
2012
Today's large-scale softwareintensive systems exhibit an increasing complexity due to a broad spectrum of technical and socio-technical components. Due to the very dynamic character of such systems as well as fast evolving technologies, most requirements cannot be planned a priori. To overcome this problem, we suggest a method to gather enduser needs for requirements engineers at any time by applying a geographical deployed feedback system. End-user needs are gathered in-situ by utilizing mobile devices. In this paper, we present the implementation of our feedback system enabling end-users to submit feedback with smartphones at very low effort and cost.
[Context, end-user involvement, software intensive systems, Mobile communication, geographical deployed feedback system, smart phones, Servers, formal specification, contexter feedback system, mobile computing, requirements engineering, Software, smartphones, Smart phones, requirements engineers]
xMapper: An architecture-implementation mapping tool
2012 34th International Conference on Software Engineering
None
2012
xMapper is an Eclipse-based tool that implements a new architecture-implementation mapping approach called 1.x-way mapping. xMapper is able to record various architecture changes during software development, and automatically map specific kinds of architecture changes to code in specific ways. In addition, xMapper supports the mapping of behavioral architecture specifications modeled as UML-like sequence diagrams and state diagrams.
[1.x-way mapping, Unified Modeling Language, software development, Unified modeling language, xMapper, Manuals, Programming, state diagrams, traceability, software architecture, behavioral architecture specifications, Software architecture, UML-like sequence diagrams, Eclipse-based tool, Computer architecture, Software, round-trip engineering, Lifting equipment, architecture-implementation mapping approach]
ConcernReCS: Finding code smells in software aspectization
2012 34th International Conference on Software Engineering
None
2012
Refactoring object-oriented (OO) code to aspects is an error-prone task. To support this task, this paper presents ConcernReCS, an Eclipse plug-in to help developers to avoid recurring mistakes during software aspectization. Based on a map of concerns, ConcernReCS automatically finds and reports error-prone scenarios in OO source code; i.e., before the concerns have been refactored to aspects.
[ConcernReCS, AOP, aspect oriented programming, Conferences, Code Smells, Tutorials, Programming, Eclipse plug-in, Educational institutions, object-oriented code, software maintenance, error-prone scenarios, Programming Mistakes, code smells, software aspectization, Organizations, aspect-oriented programming, Software, OO source code, software refactoring]
Egidio: A non-invasive approach for synthesizing organizational models
2012 34th International Conference on Software Engineering
None
2012
To understand and improve processes in organizations, six key questions need to be answered, namely, what, how, where, who, when, why. Organizations with established processes have IT system(s) that gather(s) information about some or all of the key questions. Software organizations usually have defined processes, but they usually lack information about how processes are actually executed. Moreover, there is no explicit information about process instances and activities. Existing process mining techniques face problems in coping with such environment. We propose a tool, Egidio, which uses non-invasively collected data and builds organizational models. In particular, we explain the tool within a software company, which is able to extract different aspects of development processes. The main contribution of Egidio is the ability to mine processes and organizational models from fine-grained data collected in a non-invasive manner, without interrupting the developers' work.
[Process mining, development processes, PROM, data mining, Companies, software company, noninvasive approach, Data mining, Processes, software organizational models, egidio, process mining techniques, Computer architecture, Software, Data models, software engineering, Software Organizational models, Non-invasive Data Collection]
SDiC: Context-based retrieval in Eclipse
2012 34th International Conference on Software Engineering
None
2012
While working in an IDE, developers typically deal with a large number of different artifacts at the same time. The software development process requires that they repeatedly switch between different artifacts, which often depends on searching for these artifacts in the source code structure. We propose a tool that integrates context-based search and recommendation of source code artifacts in Eclipse. The artifacts are collected from the workspace of the developer and represented using ontologies. A context model of the developer is used to improve search and give recommendations of these artifacts, which are ranked according to their relevance to the developer. The tool was tested by a group of developers and the results show that contextual information has an important role in retrieving relevant information for developers.
[Context, software development process, Computational modeling, source code structure, Knowledge based systems, Eclipse, software development in context, Switches, information retrieval, Ontologies, Programming, IDE, content-based retrieval, SDiC, source code artifact recommendation, recommender systems, ontologies (artificial intelligence), context-based retrieval, software engineering, context-based search, ontologies, Context modeling]
An integrated bug processing framework
2012 34th International Conference on Software Engineering
None
2012
Software debugging starts with bug reports. Test engineers confirm bugs and determine the corresponding developers to fix them. However, the analysis of bug reports is time-consuming and manual inspection is difficult and tedious. To improve the efficiency of the whole process, we propose a bug processing framework that integrates bug report analysis and fault localization. An instance of the framework is implemented for regression faults. Preliminary results on a large open source application demonstrate both efficiency and effectiveness.
[Isolators, Google, program debugging, software debugging, software development, public domain software, fault localization, integrated bug processing framework, Debugging, regression analysis, Educational institutions, Data mining, software maintenance, Computer bugs, regression faults, Software, bug report analysis, large open source application]
Repository for Model Driven Development (ReMoDD)
2012 34th International Conference on Software Engineering
None
2012
The Repository for Model-Driven Development (ReMoDD) contains artifacts that support Model-Driven Development (MDD) research and education. ReMoDD is collecting (1) documented MDD case studies, (2) examples of models reflecting good and bad modeling practices, (3) reference models (including metamodels) that can be used as the basis for comparing and evaluating MDD techniques, (4) generic models and transformations reflecting reusable modeling experience, (5) descriptions of modeling techniques, practices and experiences, and (6) modeling exercises and problems that can be used to develop classroom assignments and projects. ReMoDD provides a single point of access to shared artifacts reflecting high-quality MDD experience and knowledge from industry and academia. This access facilitates sharing of relevant knowledge and experience that improve MDD activities in research, education and industry.
[model repository, knowledge sharing, Unified modeling language, Communities, reference models, classroom projects, Programming, modeling experience reusability, modeling experience description, documented MDD case study collection, modeling practice description, model-driven development education, experience description, software engineering, repository for model driven development, classroom assignments, project management, Computational modeling, model-driven development research, Educational institutions, Model-driven development (MDD), modeling technique description, ReMoDD, Software systems, generic models]
Going global with agile service networks
2012 34th International Conference on Software Engineering
None
2012
ASNs are emergent networks of service-based applications (nodes) which collaborate through agile (i.e. adaptable) transactions. GSE comprises the management of project teams distanced in both space and time, collaborating in the same development effort. The GSE condition poses challenges both technical (e.g. geolocalization of resources, information continuity between timezones, etc.) and social (e.g. collaboration between different cultures, fear of competition, etc.). ASNs can be used to build an adaptable social network (ASN<sub>GSE</sub>) supporting the collaborations (edges of ASN<sub>GSE</sub>) of GSE teams (nodes of ASN<sub>GSE</sub>).
[Context, Social network services, SOA, adaptable social network, software engineering strategy, Programming, Global Software Engineering, Social Networks, Organizational Structures, Cloud Computing, Collaboration, Prototypes, agile service networks, social networking (online), software engineering, ASN, global software engineering, cloud computing, Agile Service Networks, Business, Software engineering, service-based applications, GSE condition]
Using structural and semantic information to support software refactoring
2012 34th International Conference on Software Engineering
None
2012
In the software life cycle the internal structure of the system undergoes continuous modifications. These changes push away the source code from its original design, often reducing its quality. In such cases refactoring techniques can be applied to improve the design quality of the system. Approaches existing in literature mainly exploit structural relationships present in the source code, e.g., method calls, to support the software engineer in identifying refactoring solutions. However, also semantic information is embedded in the source code by the developers, e.g., the terms used in the comments. This research investigates about the usefulness of combining structural and semantic information to support software refactoring.
[Weight measurement, system internal structure, source code, software maintenance, Couplings, Atmospheric measurements, refactoring, Semantics, Particle measurements, structural information, Software, software life cycle, semantic information, software refactoring]
An approach to variability management in service-oriented product lines
2012 34th International Conference on Software Engineering
None
2012
Service-Oriented product lines (SOPLs) are dynamic software product lines, in which, the products are developed based on services and service-oriented architecture. Although there are similarities between components and services, there are important differences so that we cannot use component-based product line engineering methods and techniques for SOPL engineering. These differences emerge from the fact that, services can be discovered as black box elements from external repositories. Moreover, services can be dynamically bound and are business-aligned. Therefore, analyzing the conformance of discovered external services with the variability of services in the SOPL - which must be aligned to the variable business needs-is necessary. Variability must be managed, that is, it must be represented (modeled), used (instantiated and capable of conformance checking) and maintained (evolved) over time. Feature Models are insufficient for modeling variability in SOPL, because, services cannot be simply mapped to one or more features, and identification of the mapping depends on knowing the detailed implementation of the services. This research aims at providing an approach to managing the variability in SOPLs so that external services can be involved in the SOPL engineering. This paper presents an overview of the proposal.
[Computers, feature models, Conformance Checking, Biological system modeling, Service oriented architecture, SOPL engineering, Modeling, dynamic software product lines, Variability Management, Support vector machines, variability management approach, service-oriented product lines, Service-Oriented Product Line, product development, software reusability, Variability Modeling, component-based product line engineering methods, service-oriented architecture, Business]
Using machine learning to enhance automated requirements model transformation
2012 34th International Conference on Software Engineering
None
2012
Textual specification documents do not represent a suitable starting point for software development. This issue is due to the inherent problems of natural language such as ambiguity, impreciseness and incompleteness. In order to overcome these shortcomings, experts derive analysis models such as requirements models. However, these models are difficult and costly to create manually. Furthermore, the level of abstraction of the models is too low, thus hindering the automated transformation process. We propose a novel approach which uses high abstraction requirements models in the form of Object System Models (OSMs) as targets for the transformation of natural language specifications in conjunction with appropriate text mining and machine learning techniques. OSMs allow the interpretation of the textual specification based on a small set of facts and provide structural and behavioral information. This approach will allow both (1) the enhancement of minimal specifications, and in the case of comprehensive specifications (2) the determination of the most suitable structure of reusable requirements.
[automated requirements model transformation, text analysis, software development, Object oriented modeling, natural language processing, Unified modeling language, Natural languages, data mining, Containers, OSM, Natural language specification, Object System Models, Object recognition, machine learning, textual specification documents, Analytical models, object system models, natural language specifications, software engineering, text mining, learning (artificial intelligence), high abstraction requirements models]
Security testing of web applications: A research plan
2012 34th International Conference on Software Engineering
None
2012
Cross-site scripting (XSS) vulnerabilities are specific flaws related to web applications, in which missing input validation can be exploited by attackers to inject malicious code into the application under attack. To guarantee high quality of web applications in terms of security, we propose a structured approach, inspired by software testing. In this paper we present our research plan and ongoing work to use security testing to address problems of potentially attackable code. Static analysis is used to reveal candidate vulnerabilities as a set of execution conditions that could lead to an attack. We then resort to automatic test case generation to obtain those input values that make the application execution satisfy such conditions. Eventually, we propose a security oracle to assess whether such test cases are instances of successful attacks.
[Software testing, malicious code, program testing, Conferences, program diagnostics, software testing, security oracle, Web applications, static analysis, HTML, Security, candidate vulnerabilities, Genetic algorithms, security of data, USA Councils, security testing, XSS, automatic test case generation, Internet, cross-site scripting vulnerabilities, structured approach]
Application of Self-Adaptive techniques to federated authorization models
2012 34th International Conference on Software Engineering
None
2012
Authorization infrastructures are an integral part of any network where resources need to be protected. As organisations start to federate access to their resources, authorization infrastructures become increasingly difficult to manage, to a point where relying only on human resources becomes unfeasible. In our work, we propose a Self-Adaptive Authorization Framework (SAAF) that is capable of monitoring the usage of resources, and controlling access to resources through the manipulation of authorization assets (e.g., authorization policies, access rights and sessions), due to the identification of abnormal usage. As part of this work, we explore the use of models for facilitating the autonomic management of federated authorization infrastructures by 1) classifying access behaviour exhibited by users, 2) modelling authorization assets, including usage, for identifying abnormal behaviour, and 3) managing authorization through the adaptation and reflection of modelled authorization assets. SAAF will be evaluated by integrating it into an existing authorization infrastructure that would allow the simulation of abnormal usage scenarios.
[Adaptation models, Unified modeling language, resource access control, model transformation, model driven engineering, access behaviour classification, Authorization, authorization asset adaptation, authorization, SAAF, authorisation, abnormal usage identification, Permission, self-adaptation, computing security, Monitoring, federated authorization infrastructure autonomic management, authorization asset modelling, resource usage monitoring, Computational modeling, authorization asset reflection, authorization infrastructures, self-adaptive authorization framework, authorization asset manipulation, self-adaptive technique application]
Improving information retrieval-based concept location using contextual relationships
2012 34th International Conference on Software Engineering
None
2012
For software engineers to find all the relevant program elements implementing a business concept, existing techniques based on information retrieval (IR) fall short in providing adequate solutions. Such techniques usually only consider the conceptual relations based on lexical similarities during concept mapping. However, it is also fundamental to consider the contextual relationships existing within an application's business domain to aid in concept location. As an example, this paper proposes to use domain specific ontological relations during concept mapping and location activities when implementing business requirements.
[Context, information retrieval-based concept location, concept location, software engineers, Correlation, contextual relationships, concept mapping, contextual relations, lexical similarities, business requirements, domain specific ontologies, information retrieval, Ontologies, Information retrieval, IR, location activities, program elements, domain specific ontological relations, Semantics, business concept, ontologies (artificial intelligence), Software, business data processing, Business]
Effective specification of decision rights and accountabilities for better performing software engineering projects
2012 34th International Conference on Software Engineering
None
2012
Governance of a software project involves the distribution and management of decision rights for significant decisions. A decision right grants authority to make decisions and be held accountable for decision outcomes. Though prior research indicates that the exercise and degree of ownership of decision rights has an impact on software project performance, there has been relatively little direct consideration of what the significant decisions should be or what might constitute an effective underlying specification and management of decision rights during the software project lifecycle. In this paper, a research agenda to reveal such knowledge is presented. This report represents the first output of our work in this area.
[project management, Decision making, Companies, software project governance, Programming, decision right specification, formal specification, software engineering project performance, decision right distribution, decision right management, decision process, Investments, software project lifecycle, decision making, Software, accountability specification, Monitoring, decision right, software process, governance]
Search based design of software product lines architectures
2012 34th International Conference on Software Engineering
None
2012
The Product-Line Architecture (PLA) is the main artifact of a Software Product Line (SPL). However, obtaining a modular, extensible and reusable PLA is a people-intensive and non-trivial task, related to different and possible conflicting factors. Hence, the PLA design is a hard problem and to find the best architecture can be formulated as an optimization problem with many factors. Similar Software Engineering problems have been efficiently solved by search-based algorithms in the field known as Search-based Software Engineering. The existing approaches used to optimize software architecture are not suitable since they do not encompass specific characteristics of SPL. To easy the SPL development and to automate the PLA design this work introduces a multi-objective optimization approach to the PLA design. The approach is now being implemented by using evolutionary algorithms. Empirical studies will be performed to validate the neighborhood operators, SPL measures and search algorithms chosen. Finally, we intend to compare the results of the proposed approach with PLAs designed by human architects.
[software product lines, evolutionary algorithms, Search problems, Optimization, SPL, software product lines architectures, search based design, multi-objective algorithms, evolutionary computation, software architecture, software architecture optimization, Software architecture, multi objective optimization approach, search-based algorithms, neighborhood operators, Programmable logic arrays, Computer architecture, product development, search-based software engineering, software reusability, PLA design, Software, search problems]
Software fault localization based on program slicing spectrum
2012 34th International Conference on Software Engineering
None
2012
During software development and maintenance stages, programmers have to frequently debug the software. One of the most difficult and complex tasks in the debugging activity is software fault localization. A commonly-used method to fix software fault is computing suspiciousness of program elements according to failed test executions and passed test executions. However, this technique does not give full consideration to dependences between program elements, thus its capacity for efficient fault localization is limited. Our research intends to introduce program slicing technique and statistical method which extracts dependencies between program elements and refines execution history, then builds program slicing spectra to rank suspicious elements by a statistical metric. We expect that our method will contribute directly to the improvement of the effectiveness and the accuracy of software fault localization and reduce the software development and maintenance effort and cost.
[Measurement, program debugging, program slicing spectrum, software debugging, Statistical analysis, program testing, software development, Software algorithms, execution history refinement, fault localization, statistical metric, Debugging, passed test executions, Generators, History, software maintenance, failed test executions, program element dependency extraction, statistical method, software fault localization, Software, statistical analysis, program slicing]
Architectural task allocation in distributed environment: A traceability perspective
2012 34th International Conference on Software Engineering
None
2012
Task allocation in distributed development is a challenging task due to intricate dependencies between distributed sites/teams and prior need of multifaceted information. Literature performs task allocation between distributed sites on limited criteria irrespective of the communication and coordination needs of the people. Conway's law relates product architecture with the communication and coordination needs of the people. Product architecture consists of multiple views based on different perspectives. Task allocation needs information about different architectural views and their interrelationships. Task allocation is also dependent on other factors not depicted in product architecture such as temporal, knowledge and cultural dependencies between distributed sites mentioned as external factors in the research. A well-conceived task allocation strategy will reduce communication and coordination dependency between sites/teams resulting in reduced time delay and smooth distributed development. The research aims to develop and validate a task allocation strategy based on information of system architecture for distributed environment. The strategy would consider all important factors during task allocation resulting in reduced communication and coordination overhead and time delay.
[Delay effects, Programming, distributed processing, distributed development, Cultural differences, task analysis, Product Architecture, coordination dependency, intricate dependencies, multifaceted information, Task Allocation, Distributed Development, system architecture, Computer architecture, Software, reduced time delay, Resource management, product architecture, Software engineering, architectural task allocation]
Using invariant relations in the termination analysis of while loops
2012 34th International Conference on Software Engineering
None
2012
Proving program termination plays an important role in ensuring reliability of software systems. Many researchers have lent much attention to this open long-standing problem, most of them were interested in proving that iterative programs terminate under a given input. In this paper, we present a method to solve a more interesting and challenging problem, namely, the generation of the termination condition of while loops i.e. condition over initial states under which a loop terminates normally. To this effect, we use a concept introduced by Mili et al., viz. invariant relation.
[Computers, program control structures, program verification, software reliability, Approximation methods, while loops, invariant relation, while loop, iterative program termination proving, invariant relations, Semantics, software system reliability, Prototypes, termination analysis, termination condition generation, Software, Arrays, termination condition]
Software regression as change of input partitioning
2012 34th International Conference on Software Engineering
None
2012
It has been known for more than 20 years. If the subdomains are not homogeneous, partition testing strategies, such as branch or statement testing, do neither perform significantly better than random input generation nor do they inspire confidence when a test suite succeeds. Yet, measuring the adequacy of test suites in terms of code coverage is still considered a common practice. The main target of our research is to develop strategies for the automatic evolution of a test suite that does inspire confidence. When the program is changed, test cases shall be augmented that witness changed output for the same input (test suite augmentation). If two test cases witness the same partition, one is to be discarded (test suite reduction).
[Automated Test Generation, input partitioning testing, Software Evolution, program testing, Debugging, regression analysis, Educational institutions, Partitioning algorithms, software regression, test suite, automatic evolution, Partition Testing, Semantics, statement testing, Software, Reliability, Testing, Software engineering]
A generic methodology to derive domain-specific performance feedback for developers
2012 34th International Conference on Software Engineering
None
2012
The performance of a system directly influences business critical metrics like total cost of ownership (TCO) and user satisfaction. However, building responsive, resource efficient and scalable applications is a challenging task. Thus, software engineering approaches are required to support software architects and developers in meeting these challenges. In this PhD research abstract, we propose a novel performance evaluation process applied during the software development phase. The goal is to increase the performance awareness of developers by providing feedback with respect to performance properties that is integrated in the every day development process. The feedback is based on domain-specific prediction functions derived by a generic methodology that executes a series of systematic measurements. We apply and validate the approach in different development scenarios at SAP.
[Performance evaluation, TCO, SAP, Software performance, Predictive models, business critical metrics, software developers, Time measurement, generic methodology, feedback, software architects, Systematics, domain-specific performance feedback, total cost of ownership, software development phase, software engineering, software engineering approach, developer performance awareness, user satisfaction, domain-specific prediction functions]
Towards the verification of multi-diagram UML models
2012 34th International Conference on Software Engineering
None
2012
UML is a general-purpose modeling language that offers a heterogeneous set of diagrams to describe the different views of a software system. While there seems to be a general consensus on the semantics of some individual diagrams, the composite semantics of the different views is still an open problem. During my PhD I am considering a significant and consistent set of UML diagrams, where timed-related properties can be modeled carefully, and I am ascribing them with a formal semantics based on metric temporal logic. The use of logic is aimed to help capture the composite semantics of the different views efficiently. The result is then used to feed a bounded model/satisfiability checker to allow users to verify these systems, even from the initial phases of the design. The final goal is to realize an advanced modeling framework where users can exploit both a well-known modeling notation and advanced verification capabilities seamlessly.
[Unified Modeling Language, Computational modeling, Unified modeling language, multi-diagram UML model verification, bounded model-satisfiability checker, temporal logic, general-purpose modeling language, software system, composite semantics, formal semantics, formal verification, Semantics, UML, Prototypes, Software systems, advanced modeling framework, timed-related properties, Clocks, metric temporal logic]
Documenting and sharing knowledge about code
2012 34th International Conference on Software Engineering
None
2012
Software engineers spend a considerable amount of time on program comprehension. Current research has primarily focused on assisting the developer trying to build up his understanding of the code. This knowledge remains only in the mind of the developer and, as time elapses, often &#x201C;disappears&#x201D;. In this research, we shift the focus to the developer who is using her Integrated Development Environment (IDE) for writing, modifying, or reading the code, and who actually understands the code she is working with. The objective of this PhD research is to seek ways to support this developer to document and share her knowledge with the rest of the team. In particular, we investigate the full potential of micro-blogging integrated into the IDE for addressing the program comprehension problem.
[Context, program comprehension, software engineers, Program comprehension, knowledge sharing, Programming, reverse engineering, IDE, IDEs, History, software maintenance, recommender systems, USA Councils, Prototypes, micro-blogging, groupware, integrated development environment, knowledge documentation, Software, CSCW, Web sites, Recommender systems]
Timely and efficient facilitation of coordination of software developers' activities
2012 34th International Conference on Software Engineering
None
2012
Work dependencies often exist between the developers of a software project. These dependencies frequently result in a need for coordination between the involved developers. However, developers are not always aware of these Coordination Requirements. Current methods which detect the need to coordinate rely on information which is available only after development work has been completed. This does not enable developers to act on their coordination needs. Furthermore, even if developers were aware of all Coordination Requirements, they likely would be overwhelmed by the large number and would not be able to effectively follow up directly with the developers involved in each dependent task. I will investigate a more timely method to determine Coordination Requirements in a software development team as they emerge and how to focus the developers attention on the most crucial ones. Further, I hope to prove that direct inter-personal communication is not always necessary to fulfill these requirements and gain insight on how we can develop tools that encourage cheaper forms of coordination.
[Productivity, Real time systems, coordination requirements, software development team, Proximity, Awareness, Programming, Tools, Management, Task Context, Coordination Requirements, Socio-Technical, software project developer activity coordination, Current measurement, interpersonal communication, groupware, Tagging, Software, software engineering, work dependency, IEEE Potentials]
Stack layout transformation: Towards diversity for securing binary programs
2012 34th International Conference on Software Engineering
None
2012
Despite protracted efforts by both researchers and practitioners, security vulnerabilities remain in modern software. Artificial diversity is an effective defense against many types of attack, and one form, address-space randomization, has been widely applied. Present artificial diversity implementations are either coarse-grained or require source code. Because of the widespread use of software of unknown provenance, e.g., libraries, where no source code is provided or available, building diversity into the source code is not always possible. I investigate an approach to stack layout transformation that operates on x86 binary programs, which would allow users to obfuscate vulnerabilities and increase their confidence in the software's dependability. The proposed approach is speculative: the stack frame layout for a function is inferred from the binary and assessed by executing the transformed program. Upon assessment failure, the inferred layout is refined in hopes to better reflect the actual function layout.
[Measurement, binary program security diversity, security vulnerabilities, software reliability, Transforms, source code, stack layout transformation, function layout, Security, binary programs, address randomization, artificial diversity, failure assessment, stack frame layout, security, security of data, address-space randomization, Layout, Benchmark testing, software dependability, Software, x86 binary programs]
Synthesis of event-based controllers: A software engineering challenge
2012 34th International Conference on Software Engineering
None
2012
Existing software engineering techniques for automatic synthesis of event-based controllers have various limitations. In the context of the world/machine approach such limitations can be seen as restrictions in the expressiveness of the controller goals and domain model specifications or in the relation between the controllable and monitorable actions. In this thesis we aim to provide techniques that overcome such limitations, e.g. supporting more expressive goal specifications, distinguishing controllable from monitorable actions or guaranteeing achievement of the desired goals, among others. Hence, improving the state of the art in the synthesis of event-based controllers. Moreover, we plan to provide efficient tools supporting the developed techniques and evaluate them by modelling known case studies from the software engineering literature. Ultimately, showing that by allowing more expressiveness of controller goals and domain model specifications, and explicitly distinguishing controllable and monitorable actions such case studies can be more accurately modelled and solutions guaranteeing satisfaction of the goals can be achieved.
[Context, Adaptation models, Computational modeling, behavioural modelling, formal specification, controllable actions, controller synthesis, world-machine approach, event-based controller automatic synthesis, Games, software engineering challenge, monitorable actions, domain model specifications, goal specifications, Monitoring, Software engineering, Context modeling]
Empirically researching development of international software
2012 34th International Conference on Software Engineering
None
2012
Software localization is an important process for international acceptance of software products. However, software development and localization does not always come together without friction. In our empirical software engineering research, we examine the interplay of software development and software localization by gathering and analyzing qualitative and quantitative data from professionals in relevant roles. Our aim is to co-validate issues and inform practice about the development of international software.
[Conferences, software engineering research, Human factors, Programming, software product international acceptance, qualitative data, Cultural differences, practice and experience, industrial, software localization, empirical software engineering, Software, software engineering, Interviews, quantitative data, Software engineering, international software development]
Model translations among big-step modeling languages
2012 34th International Conference on Software Engineering
None
2012
Model Driven Engineering (MDE) is a progressive area that tries to fill the gap between problem definition and software development. There are many modeling languages proposed for use in MDE. A challenge is how to provide automatic analysis for these models without having to create new analyzers for each different language. In this research, we tackle this problem for a family of modeling languages using a semantically configurable model translation framework.
[Big-Step Modeling Languages, big-step modeling languages, program verification, software development, Computational modeling, Unified modeling language, model translation framework, Model Translation, model driven engineering, Educational institutions, Formal Analysis, MDE, problem definition, Analytical models, modeling languages, Model Checking, Semantics, Syntactics, simulation languages, software engineering, Mathematical model]
HARPPIE: Hyper algorithmic recipe for productive parallelism intensive endeavors
2012 34th International Conference on Software Engineering
None
2012
Over the last few years, Parallelism has been gaining increasing importance and multicore processing is now common. Massification of parallelism is driving research and development of novel techniques to overcome current limits of Parallel Computing. However, the scope of parallelization research focuses mainly on ever-increasing performance and much still remains to be accomplished regarding improving productivity in the development of parallel software. This PhD research aims to develop methods and tools to dilute parallel programming complexity and enable nonexpert programmer to fully benefit from a new generation of parallelism-driven programming platforms. Although much work remains to be done to reduce the skill requirements for parallel programming to become within reach of medium-skill programming workforces, it is our belief that this research will help bridge that gap.
[Productivity, Algorithm design and analysis, Measurement, HARPPIE, parallel programming complexity, multiprocessing systems, Generative programming, hyper algorithmic recipe for productive parallelism intensive endeavors, parallel software development, multicore processing, parallelism massification, parallel programming, parallelism-driven programming platforms, medium-skill programming workforces, Parallel programming, Concurrency, Parallel processing, Parallelism, Software, parallel computing, Model-driven software engineering]
On the analysis of evolution of software artefacts and programs
2012 34th International Conference on Software Engineering
None
2012
The literature describes several approaches to identify the artefacts of programs that evolve together to reveal the (hidden) dependencies among these artefacts and to infer and describe their evolution trends. We propose the use of biological methods to group artefacts, to detect co-evolution among them, and to construct their phylogenic trees to express their evolution trends. First, we introduced the novel concepts of macro co-changes (MCCs), i.e., of artefacts that co-change within a large time interval and of dephase macro co-changes (DMCCs), i.e., macro co-changes that always happen with the same shifts in time. We developed an approach, Macocha, to identify these new patterns of artefacts co-evolution in large programs. Now, we are analysing the evolution of classes playing roles in design patterns and - or antipatterns. In parallel to previous work, we are detecting what classes are in macro co-change or in dephase macro co-change with the design motifs. Results try to show that classes playing roles in design motifs have specifics evolution trends. Finally, we are implementing an approach, Profilo, to achieve the analysis of the evolution of artefacts and versions of large object-oriented programs. Profilo creates a phylogenic tree of different versions of program that describes versions evolution and the relation among versions and programs. We will, also, evaluate the usefulness of our tools using lab and field studies.
[biological methods, design motifs, phylogenic trees construction, DMCC, Phylogeny, History, Macocha, coevolution detection, Profilo, design patterns, programs artifact, dephase macro co-changes, object-oriented programs, co-change, software artifact evolution analysis, phylogenic tree, stability, object-oriented programming, change impact, trees (mathematics), Stability analysis, Association rules, software maintenance, Software evolution, antipatterns, artifacts co-evolution pattern, Software, Software engineering, artifact grouping]
Societal Computing
2012 34th International Conference on Software Engineering
None
2012
Social Computing research focuses on online social behavior and using artifacts derived from it for providing recommendations and other useful community knowledge. Unfortunately, some of that behavior and knowledge incur societal costs, particularly with regards to Privacy, which is viewed quite differently by different populations as well as regulated differently in different locales. But clever technical solutions to those challenges may impose additional societal costs, e.g., by consuming substantial resources at odds with Green Computing, another major area of societal concern. We propose a new crosscutting research area, Societal Computing, that focuses on the technical tradeoffs among computational models and application domains that raise significant societal issues. This dissertation, advised by Prof. Gail Kaiser, will focus on privacy concerns in the context of Societal Computing and will aim to address research topics such as design patterns and architectures for privacy tradeoffs, better understanding of users' privacy requirements so that tradeoffs with other areas such as green computing can be dealt with in a more effective manner, and better visualization techniques for making privacy and its tradeoffs more understandable.
[Data privacy, technical solutions, technical tradeoffs, Conferences, substantial resources, user privacy requirements, Communities, online social behavior, community knowledge, social computing, societal issues, Privacy, Sustainability, object-oriented methods, visualization techniques, societal costs, computational models, Social network services, environmental factors, societal computing, Green Computing, sustainable development, social sciences, research topics, Software, data privacy, green computing, Software engineering]
Finding suitable programs: Semantic search with incomplete and lightweight specifications
2012 34th International Conference on Software Engineering
None
2012
Finding suitable code for reuse is a common task for programmers. Two general approaches dominate the code search literature: syntactic and semantic. While queries for syntactic search are easy to compose, the results are often vague or irrelevant. On the other hand, a semantic search may return relevant results, but current techniques require developers to write specifications by hand, are costly as potentially matching code need to be executed to verify congruence with the specifications, or only return exact matches. In this work, we propose an approach for semantic search in which programmers specify lightweight, incomplete specifications and an SMT solver automatically identifies programs from a repository, encoded as constraints, that match the specifications. The repository of programs is automatically encoded offline so the search for matching programs is efficient. The program encodings cover various levels of abstraction to enable partial matches when no or few exact matches exists. We instantiate this approach on a subset of the Yahoo! Pipes mashup language, and plan to extend our techniques to more traditional programming languages as the research progresses.
[incomplete specifications, semantic approach, search engines, Mashups, Lattices, suitable code finding, abstraction level, programming languages, SMT solvers, formal specification, matching program searching, constraints, syntactic approach, program repository, Semantics, Yahoo Pipes mashup language, program composition, semantic search, SMT solver, program encodings, information retrieval, lightweight specifications, Encoding, code reuse, code search literature, suitable program finding, Syntactics, Concrete]
Certification-based development of critical systems
2012 34th International Conference on Software Engineering
None
2012
Safety-critical systems certification is a complex endeavor. Regulating agencies are moving to goal-based standards in an effort to remedy significant problems of prescriptive standards. However, goal-based standards introduce new difficulties into the development and certification processes. In this work I introduce Certification-Based Development, or CBD. CBD is a process framework designed to mitigate these difficulties by meeting the needs of a specific certifying agency with regard to a specific system.
[Measurement, CBD, Educational institutions, Standards, certification, standards, safety-critical systems, Systematics, safety, safety case, goal-based standards, Software, Safety, safety critical system certification-based development, Electronics packaging]
Testing and debugging UML models based on fUML
2012 34th International Conference on Software Engineering
None
2012
Model-driven development, which has recently gained momentum in academia as well as in industry, changed the software engineering process significantly from being code-centric to being model-centric. Models are considered as the key artifacts and as a result the success of the whole software development process relies on these models and their quality. Consequently, there is an urgent need for adequate methods to ensure high quality of models. Model execution can serve as the crucial basis for such methods by enabling to automatically test and debug models. Therefore, lessons learned from testing and debugging of code may serve as a valuable source of inspiration. However, the peculiarities of models in comparison to code, such as multiple views and different abstraction levels, impede the direct adoption of existing methods for models. Thus, we claim that the currently available tool support for model testing and debugging is still insufficient because these peculiarities are not adequately addressed. In this work, we aim at tackling these shortcomings by proposing a novel model execution environment based on fUML, which enables to efficiently test and debug UML models.
[Adaptation models, program debugging, software development process, model testing, program testing, Unified modeling language, software engineering process, model debugging, code-centric, MDD, model-driven development, UML model debugging, UML model testing, Semantics, software engineering, foundational UML, Testing, model execution, fUML, Unified Modeling Language, Object oriented modeling, Debugging, model execution environment, Standards, model-centric]
Bridging the divide between software developers and operators using logs
2012 34th International Conference on Software Engineering
None
2012
There is a growing gap between the software development and operation worlds. Software developers rarely divulge development knowledge about the software to operators, while operators rarely communicate field knowledge to developers. To improve the quality and reduce the operational cost of large-scale software systems, bridging the gap between these two worlds is essential. This thesis proposes the use of logs as mechanism to bridge the gap between these two worlds. Logs are messages generated from statements inserted by developers in the source code and are often used by operators for monitoring the field operation of a system. However, the rich knowledge in logs has not yet been fully used because of their non-structured nature, their large scale, and the use of the ad hoc log analysis techniques. Through case studies on large commercial and open source systems, we plan to demonstrate the value of logs as a tool to support developers and operators.
[software development, Biological system modeling, public domain software, operation worlds, source code, ad hoc log analysis techniques, software developers, knowledge development, large-scale software systems quality, software quality, History, knowledge management, Bridges, messages generation, field knowledge, Computer bugs, operational cost, Software systems, logs, nonstructured nature, Testing, open source systems]
The co-evolution of socio-technical structures in sustainable software development: Lessons from the open source software communities
2012 34th International Conference on Software Engineering
None
2012
Software development depends on many factors, including technical, human and social aspects. Due to the complexity of this dependence, a unifying framework must be defined and for this purpose we adopt the complex networks methodology. We use a data-driven approach based on a large collection of open source software projects extracted from online project development platforms. The preliminary results presented in this article reveal that the network perspective yields key insights into the sustainability of software development.
[Measurement, public domain software, Communities, mining software repositories, Programming, complex networks, open source software communities, online project development platforms, Complex networks, human aspects, software engineering, socio-technical structure coevolution, quantitative analysis, project management, open source software, social networks, technical aspects, free software, complex networks methodology, sustainable development, statistical physics, software dependency graphs, social aspects, Collaboration, data-driven approach, Software, sustainable software development, Software engineering]
Log-based testing
2012 34th International Conference on Software Engineering
None
2012
This thesis presents an ongoing research on using logs for software testing. We propose a complex and generic logging and diagnosis framework, that can be efficiently used for continuous testing of future Internet applications. To simplify the diagnosis of logs we suggest to reduce its size by means of rewriting.
[Software testing, Automation, logging framework, program testing, Instruments, software testing, log-based testing, log file analysis, future Internet application continuous testing, rewriting, Libraries, Internet, diagnosis framework, instrumentation, Graphical user interfaces]
Moving mobile applications between mobile devices seamlessly
2012 34th International Conference on Software Engineering
None
2012
Users prefer using multiple mobile devices interchangeably by switching between the devices. A solution to this requirement is the migration of applications between mobile devices at runtime. In our vision to move the application from a device A to a device B, instead of synchronizing just the application's data, a simple swiping gesture can be used. Afterwards the user is able to use the same application including its current state on device B. To achieve this, we plan to put the running application on the device A into a paused state, take a snapshot afterwards, move the application to the device B by using a middleware on both devices, extract the snapshot on device B and finally resume it on device B from its paused state. The outcome of the research will be a framework and either a kernel module or an API to migrate mobile applications.
[snapshot extraction, handover, application program interfaces, mobile, Mobile communication, Mobile handsets, Browsers, History, Middleware, swiping gesture, mobile computing, Operating systems, application, mobile application migration, multiple mobile devices, migration, Hardware, API, kernel module, middleware]
Timely detection of Coordination Requirements to support collaboration among software developers
2012 34th International Conference on Software Engineering
None
2012
Work dependencies often exist between the developers of a software project. These dependencies frequently result in a need for coordination between the involved developers. However, developers are not always aware of these Coordination Requirements. Current methods which detect the need to coordinate rely on information which is available only after development work has been completed. This does not enable developers to act on their coordination needs. I have investigated a more timely method to determine Coordination Requirements in a software development team as they emerge.
[Productivity, Visualization, project management, coordination requirements, software development team, Programming, timely detection, DP management, software management, software developers, software project, team working, collaboration support, Collaboration, Software, work dependencies, IEEE Potentials]
Improving failure-inducing changes identification using coverage analysis
2012 34th International Conference on Software Engineering
None
2012
Delta debugging has been proposed for failure-inducing changes identification. Despite promising results, there are two practical factors that thwart the application of delta debugging: large number of tests and misleading false positives. To address the issues, we present a combination of coverage analysis and delta debugging that automatically isolates failure-inducing changes. Evaluations on twelve real regressions in GNU software demonstrate both the speed gain and effectiveness improvements.
[program debugging, coverage analysis, Debugging, automated debugging, Programming, Educational institutions, software maintenance, system recovery, software fault tolerance, delta debugging, software evolution, Fault diagnosis, regression fault, Computer bugs, failure-inducing changes identification, regression faults, Software, GNU software, Software engineering]
A study on improving static analysis tools: Why are we not using them?
2012 34th International Conference on Software Engineering
None
2012
Using static analysis tools for automating code inspections can be beneficial for software engineers. Despite the benefits of using static analysis tools, research suggests that these tools are underused. In this research, we propose to investigate why developers are not widely using static analysis tools and how current tools could potentially be improved to increase usage.
[Industries, software engineers, program diagnostics, code inspection automation, Programming, static analysis, tool development, Encoding, Analytical models, Computer bugs, Software, static analysis tool, software tools, Interviews, tool evaluation]
Winbook: A social networking based framework for collaborative requirements elicitation and WinWin negotiations
2012 34th International Conference on Software Engineering
None
2012
Easy-to-use groupware for diverse stakeholder negotiation has been a continuing challenge [7, 8, 9]. USC's fifth-generation wiki-based win-win negotiation support tool [1] was not as successful in improving over the previous four generations [2] as hoped - it encountered problems with non-technical stakeholder usage. The popularity of Facebook and Gmail ushered in a new era of widely-used social networking capabilities that I have been using to develop and experiment with a new way for collaborative requirements elicitation and management - marrying the way people collaborate on Facebook and organize their emails on Gmail to come up with a social networking-like platform to help achieve better usage of the WinWin negotiation framework [4]. Initial usage results on 14 small projects involving non-technical stakeholders have shown profound implications on the way requirements are negotiated and used, through the system and software definition and development processes. Subsequently, Winbook has also been adopted as a part of a project to bridge requirements and architecting for a major US government organization.
[Winbook, WinWin negotiations, software definition, Gmail, stakeholder negotiation, collaborative requirements elicitation, US government organization, Electronic mail, social networking, negotiation support systems, development process, wiki-based Win-Win negotiation support tool, Image color analysis, Collaboration, Organizations, groupware, social networking capability, social networking (online), software engineering, Facebook, nontechnical stakeholder usage, Software engineering]
Using automatic static analysis to identify technical debt
2012 34th International Conference on Software Engineering
None
2012
The technical debt (TD) metaphor describes a tradeoff between short-term and long-term goals in software development. Developers, in such situations, accept compromises in one dimension (e.g. maintainability) to meet an urgent demand in another dimension (e.g. delivering a release on time). Since TD produces interests in terms of time spent to correct the code and accomplish quality goals, accumulation of TD in software systems is dangerous because it could lead to more difficult and expensive maintenance. The research presented in this paper is focused on the usage of automatic static analysis to identify Technical Debt at code level with respect to different quality dimensions. The methodological approach is that of Empirical Software Engineering and both past and current achieved results are presented, focusing on functionality, efficiency and maintainability.
[software development, program diagnostics, Technical Debt, software systems, software development management, software quality, code level, software maintenance, technical debt metaphor, Software Quality Monitoring, quality goals, automatic static analysis, Automatic Static Analysis, empirical software engineering, quality dimensions, Software Maintenance]
Coupled evolution of model-driven spreadsheets
2012 34th International Conference on Software Engineering
None
2012
Spreadsheets are increasingly used as programming languages, in the construction of large and complex systems. The fact is that spreadsheets, being a highly flexible framework, lack important programming language features such as abstraction or encapsulation. This flexibility, however, comes with a price: spreadsheets are populated with significant amounts of errors. One of the approaches that try to overcome this problem advocates the use of model-driven spreadsheet development: a spreadsheet model is defined, from which a concrete spreadsheet is generated. Although this approach has been proved effective in other contexts, still it needs to accommodate for future evolution of both the model and its instance, so that they remain synchronized at all moments. In this paper, we propose a pair of transformation sets, one working at the model level and the other at the instance level, such that each transformation in one set is related to a transformation in the other set. With our approach, we ensure model/data compliance while allowing for model and data evolution.
[Context, data compliance, Visualization, Software Evolution, object-oriented programming, model-driven spreadsheets, Object oriented modeling, software prototyping, Unified modeling language, Spreadsheets, Model-Driven Engineering (MDE), data evolution, spreadsheet programs, transformation sets, programming languages, model evolution, Data models, Software, model compliance, Business, spreadsheet model]
Managing evolution of software product line
2012 34th International Conference on Software Engineering
None
2012
In software product line engineering, core assets are shared among multiple products. Core assets and products generally evolve independently. Developers need to capture evolution in both contexts and to propagate changes in both directions between the core assets and the products. We propose a version control system to support product line engineering by supporting the evolution of product line, product derivation, and change propagation from core assets to products and vice versa.
[software product line engineering, software reuse, version control system, version control, Programming, Control systems, software product line evolution, Standards, USA Councils, core assets, Prototypes, product development, software reusability, Software, Product development, product derivation]
Enabling dynamic metamodels through constraint-driven modeling
2012 34th International Conference on Software Engineering
None
2012
Metamodels are commonly used in Model-Driven Engineering to define available model elements and structures. However, metamodels are likely to change during development for various reasons like requirement changes or evolving domain knowledge. Updating a metamodel typically leads to non-conformance issues with existing models. Hence, evolution strategies must be developed. Additionally, the tool implementation must also be updated to support the evolved metamodel. We propose the use of metamodel-independent tools with unified modeling concepts for working with all kinds of metamodels and models. By applying the Constraint-Driven Modeling approach and generating model constraints from metamodels automatically, we solve the described issues and enable dynamic, evolving metamodels. A prototype implementation has shown the feasibility of the approach and performance tests suggest that it also scales with increasing model sizes.
[Adaptation models, Dynamic metamodeling, object-oriented programming, Unified modeling language, Metamodeling, constraints, dynamic metamodels, Runtime, metamodel evolution, constraint-driven modeling, Prototypes, model-driven engineering, software process improvement, model consistency, constraint handling, Load modeling]
Assisting end-user development in browser-based mashup tools
2012 34th International Conference on Software Engineering
None
2012
Despite the recent progresses in end-user development and particularly in mashup application development, developing even simple mashups is still non-trivial and requires intimate knowledge about the functionality of web APIs and services, their interfaces, parameter settings, data mappings, and so on. We aim to assist less skilled developers in composing own mashups by interactively recommending composition knowledge in the form of modeling patterns and fostering knowledge reuse. Our prototype system demonstrates our idea of interactive recommendation and automated pattern weaving, which involves recommending relevant composition patterns to the users during development, and once selected, applying automatically the changes as suggested in the selected pattern to the mashup model under development. The experimental evaluation of our prototype implementation demonstrates that even complex composition patterns can be efficiently stored, queried and weaved into the model under development in browser-based mashup tools.
[Web API, application program interfaces, assisted development, Mashups, knowledge reuse, Programming, data mapping, automated pattern weaving, composition pattern, Engines, browser-based mashup tool, end-user development, weaving, Web services, interactive recommendation, Prototypes, knowledge based systems, pattern recommendation, Weaving, composition knowledge, pattern modeling, Context modeling, Load modeling]
Hot clones: Combining search-driven development, clone management, and code provenance
2012 34th International Conference on Software Engineering
None
2012
Code duplication is common in current programming-practice: programmers search for snippets of code, incorporate them into their projects and then modify them to their needs. In today's practice, no automated scheme is in place to inform both parties of any distant changes of the code. As code snippets continue to evolve both on the side of the user and on the side of the author, both may wish to benefit from remote bug fixes or refinements - authors may be interested in the actual usage of their code snippets, and researchers could gather information on clone usage. We propose to maintain a link between software clones across repositories and outline how the links can be created and maintained.
[clone detection, Social network services, code provenance, code snippet, Cloning, software maintenance, code duplication, hot clones, programming practice, corrective clone management, search-driven development, software clones, Databases, USA Councils, Computer bugs, clone management, repositories, Search engines, Software, programming]
Capturing and exploiting fine-grained IDE interactions
2012 34th International Conference on Software Engineering
None
2012
Developers interact with IDEs intensively to maximize productivity. A developer's interactions with an IDE reflect his thought process and work habits. In this paper, we propose a general framework to capture and exploit all types of IDE interactions. We have two explicit goals for the framework: its systematic interception of comprehensive user interactions, and the ease of use in writing customized applications. To this end, we developed IDE++ on top of Eclipse IDE. For evaluation, we built applications upon the framework to illustrate 1) the need for capturing comprehensive, finegrained IDE interactions, and 2) IDE++'s ease of use. We believe that IDE++ is a step toward building next generation, customizable and intelligent IDEs.
[Productivity, Context, fine-grained interactions, systematic interception, productivity maximization, IDE&#x002B;&#x002B;, Eclipse IDE, History, IDE++, Systematics, customized application writing, productivity, comprehensive user interaction, Software, programming environments, Monitoring, Testing, fine-grained IDE interaction]
Restructuring unit tests with TestSurgeon
2012 34th International Conference on Software Engineering
None
2012
The software engineering community has produced great techniques for software maintainability, however, less effort is dedicated to have unit tests modular and extensible. TestSurgeon is a profiler for unit tests which collects information from tests execution. It proposes a metric for similarity between tests and provides a visualization to help developers restructure their unit tests.
[Measurement, Visualization, visualization, Shape, program testing, Communities, software maintainability, Educational institutions, unit test restructuring, software maintenance, test execution, unit test modular, data visualisation, TestSurgeon, software engineering community, Software, Software engineering]
A requirements-based approach for the design of adaptive systems
2012 34th International Conference on Software Engineering
None
2012
Complexity is now one of the major challenges for the IT industry [1]. Systems might become too complex to be managed by humans and, thus, will have to be self-managed: Self-configure themselves for operation, self-protect from attacks, self-heal from errors and self-tune for optimal performance [2]. (Self-)Adaptive systems evaluate their own behavior and change it when the evaluation indicates that it is not accomplishing the software's purpose or when better functionality and performance are possible [3]. To that end, we need to monitor the behavior of the running system and compare it to an explicit formulation of requirements and domain assumptions [4]. Feedback loops (e.g., the MAPE loop [2]) constitute an architectural solution for this and, as proposed by past research [5], should be a first class citizen in the design of such systems. We advocate that adaptive systems should be designed this way from as early as Requirements Engineering and that reasoning over requirements is fundamental for run-time adaptation. We therefore propose an approach for the design of adaptive systems based on requirements and inspired in control theory [6]. Our proposal is goal-oriented and targets softwareintensive socio-technical systems [7], in an attempt to integrate control-loop approaches with decentralized agents inspired approaches [8]. Our final objective is a set of extensions to state-of-the-art goal-oriented modeling languages that allow practitioners to clearly specify the requirements of adaptive systems and a run-time framework that helps developers implement such requirements. In this 2-page abstract paper, we summarize this approach.
[Adaptation models, Solid modeling, IT industry, Adaptive systems, feedback loops, Conferences, run-time adaptation, requirements-based approach, reasoning, goal-oriented modeling languages, Proposals, formal specification, Runtime, requirements engineering, decentralized agents, software intensive socio-technical systems, fault tolerant computing, simulation languages, self-adaptive systems]
Petri nets state space analysis in the cloud
2012 34th International Conference on Software Engineering
None
2012
Several techniques for addressing the state space explosion problem in model checking have been studied. One of these is to use distributed memory and computation for storing and exploring the state space of the model of a system. In this report, we present and compare different multi-thread, distributed, and cloud approaches to face the state-space explosion problem. The experiments report shows the convenience (in particular) of cloud approaches.
[Real time systems, Cloud computing, State-space explosion, multi-threading, Computational modeling, Instruction sets, Petri nets, Real-Time systems, State-space parallel exploration, Explosions, Petri nets state space analysis, state space explosion problem, Distributed computing, MapReduce, Analytical models, distributed approach, formal verification, model checking, distributed memory, Multithreaded computing, multithread approach, Space exploration, cloud computing, cloud approaches]
Mining Java class identifier naming conventions
2012 34th International Conference on Software Engineering
None
2012
Classes represent key elements of knowledge in object-orientated source code. Class identifier names describe the knowledge recorded in the class and, much of the time, record some detail of the lineage of the class. We investigate the structure of Java class names identifying common patterns of naming and the way components of class identifier names are repeated in inheritance hierarchies. Detailed knowledge of class identifier name structures can be used to improve the accuracy of concept location tools, to support reverse engineering of domain models and requirements traceability, and to support development teams through class identifier naming recommendation systems.
[Java, Software maintenance, object-orientated source code, development team, data mining, class identifier naming recommendation system, source code, Programming, Educational institutions, reverse engineering, inheritance, concept location tool, class identifier name structure, Accuracy, domain model, Java class identifier naming convention mining, Speech, identifier names, object-oriented methods]
Online sharing and integration of results from mining software repositories
2012 34th International Conference on Software Engineering
None
2012
The mining of software repository involves the extraction of both basic and value-added information from existing software repositories. Depending on stakeholders (e.g., researchers, management), these repositories are mined several times for different application purposes. To avoid unnecessary pre-processing steps and improve productivity, sharing, and integration of extracted facts and results are needed. The motivation of this research is to introduce a novel collaborative sharing platform for software datasets that supports on-the-fly inter-datasets integration. We want to facilitate and promote a paradigm shift in the source code analysis domain, similar to the one by Wikipedia in the knowledge-sharing domain. In this paper, we present the SeCold project, which is the first online, publicly available software ecosystem Linked Data dataset. As part of this research, not only theoretical background on how to publish such datasets is provided, but also the actual dataset. SeCold contains about two billion facts, such as source code statements, software licenses, and code clones from over 18.000 software projects. SeCold is also an official member of the Linked Data cloud and one of the eight largest online Linked Data datasets available on the cloud.
[SeCold project, source code analysis domain, Communities, data mining, on-the-fly interdatasets integration, Licenses, Wikipedia, Data mining, software mining, online sharing, groupware, source code statement, model, software license, Cloning, software repository mining, knowledge-sharing domain, sharing, software ecosystem linked data dataset, code clones, XML, Linked Data, collaborative sharing platform, Software, Data models, software dataset]
Refounding software engineering: The Semat initiative (Invited presentation)
2012 34th International Conference on Software Engineering
None
2012
The new software engineering initiative, Semat, is in the process of developing a kernel for software engineering that stands on a solid theoretical basis. So far, it has suggested a set of kernel elements for software engineering and basic language constructs for defining the elements and their usage. This paper describes a session during which Semat results and status will be presented. The presentation will be followed by a discussion panel.
[Industries, Communities, solid theoretical basis, practitioner, kernel, Educational institutions, Semat initiative, software engineering method, Semat, Jacobian matrices, OMG standard, kernel elements, software engineering, theoretical basis, Kernel, Software engineering]
Summary of the ICSE 2012 workshops
2012 34th International Conference on Software Engineering
None
2012
The workshops of ICSE 2012 provide a forum for researchers and practitioners to exchange and discuss scientific ideas before they have matured to warrant conzference or journal publication. ICSE Workshops also serve as incubators for scientific communities that form and share a particular research agenda.
[Conferences, Communities, Cloning, Programming, Software systems, Software engineering]
Summary of the ICSE 2012 tutorials and technical briefings
2012 34th International Conference on Software Engineering
None
2012
This year ICSE is offering a mix of half-day and full day tutorials in addition to shorter technical briefings in selected domains. Whereas tutorials cover a wide range of mature topics of both academic and practical interest, technical briefings are intended to provide a compact introduction to the state-of-the-art in an emerging area.
[Multicore processing, Ecosystems, Tutorials, Software, Proposals, Data mining, Software engineering]
Message from the chairs
2013 35th International Conference on Software Engineering
None
2013
ICSE 2013, the 35th International Conference on Software Engineering, was held May 18-26, 2013 in San Francisco, California, USA. The main conference of ICSE 2013 is a three-day event with keynotes, multiple tracks, award sessions, and social events. In addition, four days of pre-conference and two days of post-conference meetings include workshops, tutorials, collocated events, a trip to visit some Silicon Valley companies, and a variety of smaller meetings. The Technical Research Paper track provides participants an opportunity to hear and discuss innovative results and evaluations in software engineering research. This year, the track received a record number of 461 submissions, of which five were withdrawn and 10 (2%) were rejected without review due to scope or length issues. In the first phase of reviewing, two members of the 47-member program committee reviewed each of the remaining 446 submissions. Based on these reviews, 223 submissions (48.3%) were promoted to the second phase of reviewing, in which each submission received a third review from a member of the program committee. The international program committee selected 85 papers (18.5%) for publication and presentation at the conference. The Software Engineering in Practice (SEIP) track provides a dedicated forum for practitioners to share ideas and solutions that address industrial challenges. 100 papers were submitted and reviewed by the SEIP program committee, leading to the acceptance of 20 papers. The Software Engineering in Education (SEE) track presents novel ways to teach software engineering. There are three paper sessions, Teaching Introductory Courses, Advanced Software Engineering Education, and Problem-Based and Studio Learning. There were 49 papers submitted with 13 acceptances, including the Panel, for a 26% acceptance rate. The New Ideas and Emerging Results (NIER) track received 162 submissions, from which 147 fit the call for papers. There were 31 papers (22%) selected as part of the program.
[]
Organization committee
2013 35th International Conference on Software Engineering
None
2013
Provides a listing of current committee members and society officers.
[]
Automatic synthesis of modular connectors via composition of protocol mediation patterns
2013 35th International Conference on Software Engineering
None
2013
Ubiquitous and pervasive computing promotes the creation of an environment where Networked Systems (NSs) eternally provide connectivity and services without requiring explicit awareness of the underlying communications and computing technologies. In this context, achieving interoperability among heterogeneous NSs represents an important issue. In order to mediate the NSs interaction protocol and solve possible mismatches, connectors are often built. However, connector development is a never-ending and error-prone task and prevents the eternality of NSs. For this reason, in the literature, many approaches propose the automatic synthesis of connectors. However, solving the connector synthesis problem in general is hard and, when possible, it results in a monolithic connector hence preventing its evolution. In this paper, we define a method for the automatic synthesis of modular connectors, each of them expressed as the composition of independent mediators. A modular connector, as synthesized by our method, supports connector evolution and performs correct mediation.
[Protocols, open systems, Ontologies, connector evolution, interoperability, software maintenance, ubiquitous computing, pervasive computing, networked systems, Mediation, independent mediator composition, automatic modular connector synthesis, Connectors, Algebra, connectivity, Semantics, NS interaction protocol, protocols, protocol mediation pattern composition]
Robust reconfigurations of component assemblies
2013 35th International Conference on Software Engineering
None
2013
In this paper, we propose a reconfiguration protocol that can handle any number of failures during a reconfiguration, always producing an architecturally-consistent assembly of components that can be safely introspected and further reconfigured. Our protocol is based on the concept of Incrementally Consistent Sequences (ICS), ensuring that any reconfiguration incrementally respects the reconfiguration contract given to component developers: reconfiguration grammar and architectural invariants. We also propose two recovery policies, one rolls back the failed reconfiguration and the other rolls it forward, both going as far as possible, failure permitting. We specified and proved the reconfiguration contract, the protocol, and recovery policies in Coq.
[Protocols, Dynamic reconfiguration, architectural invariants, system recovery, robust component assembly reconfigurations, software architecture, failure handling, Component models, Wires, Computer architecture, Robustness, theorem proving, protocols, recovery policies, Assembly, Contracts, reconfiguration grammar, object-oriented programming, incrementally consistent sequences, Coq, failure permitting, reconfiguration protocol, reconfiguration contract, Grammar, software fault tolerance, grammars, ICS]
Drag-and-drop refactoring: Intuitive and efficient program transformation
2013 35th International Conference on Software Engineering
None
2013
Refactoring is a disciplined technique for restructuring code to improve its readability and maintainability. Almost all modern integrated development environments (IDEs) offer built-in support for automated refactoring tools. However, the user interface for refactoring tools has remained largely unchanged from the menu and dialog approach introduced in the Smalltalk Refactoring Browser, the first automated refactoring tool, more than a decade ago. As the number of supported refactorings and their options increase, invoking and configuring these tools through the traditional methods have become increasingly unintuitive and inefficient. The contribution of this paper is a novel approach that eliminates the use of menus and dialogs altogether. We streamline the invocation and configuration process through direct manipulation of program elements via drag-and-drop. We implemented and evaluated this approach in our tool, Drag-and-Drop Refactoring (DNDRefactoring), which supports up to 12 of 23 refactorings in the Eclipse IDE. Empirical evaluation through surveys and controlled user studies demonstrates that our approach is intuitive, more efficient, and less error-prone compared to traditional methods available in IDEs today. Our results bolster the need for researchers and tool developers to rethink the design of future refactoring tools.
[Java, Irrigation, Navigation, DNDRefactoring, automated refactoring tools, code restructuring, integrated development environments, Eclipse IDE, user interfaces, Data mining, software maintenance, user interface, Keyboards, User interfaces, software tools, program transformation, drag-and-drop refactoring, Usability]
Managing non-functional uncertainty via model-driven adaptivity
2013 35th International Conference on Software Engineering
None
2013
Modern software systems are often characterized by uncertainty and changes in the environment in which they are embedded. Hence, they must be designed as adaptive systems. We propose a framework that supports adaptation to non-functional manifestations of uncertainty. Our framework allows engineers to derive, from an initial model of the system, a finite state automaton augmented with probabilities. The system is then executed by an interpreter that navigates the automaton and invokes the component implementations associated to the states it traverses. The interpreter adapts the execution by choosing among alternative possible paths of the automaton in order to maximize the system's ability to meet its non-functional requirements. To demonstrate the adaptation capabilities of the proposed approach we implemented an adaptive application inspired by an existing worldwide distributed mobile application and we discussed several adaptation scenarios.
[Measurement, Uncertainty, program verification, Unified modeling language, software systems, probability, nonfunctional uncertainty management, uncertainty handling, interpreter, finite state machines, adaptive systems, program interpreters, distributed mobile application, mobile computing, Automata, Abstracts, finite state automaton, Time factors, model-driven adaptivity, system ability maximization, Usability, probabilities]
Coupling software architecture and human architecture for collaboration-aware system adaptation
2013 35th International Conference on Software Engineering
None
2013
The emergence of socio-technical systems characterized by significant user collaboration poses a new challenge for system adaptation. People are no longer just the &#x201C;users&#x201D; of a system but an integral part. Traditional self-adaptation mechanisms, however, consider only the software system and remain unaware of the ramifications arising from collaboration interdependencies. By neglecting collective user behavior, an adaptation mechanism is unfit to appropriately adapt to evolution of user activities, consider side-effects on collaborations during the adaptation process, or anticipate negative consequence upon reconfiguration completion. Inspired by existing architecture-centric system adaptation approaches, we propose linking the runtime software architecture to the human collaboration topology. We introduce a mapping mechanism and corresponding framework that enables a system adaptation manager to reason upon the effect of software-level changes on human interactions and vice versa. We outline the integration of the human architecture in the adaptation process and demonstrate the benefit of our approach in a case study.
[collaboration topology, runtime mapping, architecture reconfiguration, self-adaptation mechanisms, Topology, user interfaces, collaboration-aware system adaptation, software system, socio-technical systems, Connectors, software architecture, Runtime, Software architecture, dynamic adaptation, Collaboration, Computer architecture, groupware, human architecture, Software, user collaboration, architecture-centric system]
Learning revised models for planning in adaptive systems
2013 35th International Conference on Software Engineering
None
2013
Environment domain models are a key part of the information used by adaptive systems to determine their behaviour. These models can be incomplete or inaccurate. In addition, since adaptive systems generally operate in environments which are subject to change, these models are often also out of date. To update and correct these models, the system should observe how the environment responds to its actions, and compare these responses to those predicted by the model. In this paper, we use a probabilistic rule learning approach, NoMPRoL, to update models using feedback from the running system in the form of execution traces. NoMPRoL is a technique for nonmonotonic probabilistic rule learning based on a transformation of an inductive logic programming task into an equivalent abductive one. In essence, it exploits consistent observations by finding general rules which explain observations in terms of the conditions under which they occur. The updated models are then used to generate new behaviour with a greater chance of success in the actual environment encountered.
[Adaptation models, NoMPRoL technique, Adaptive systems, Computational modeling, nonmonotonic probabilistic rule learning, learning revised models, execution traces, Probabilistic logic, runtime model, environment domain models, machine learning, adaptive systems, feedback, planning (artificial intelligence), software architecture, inductive logic programming task, Robot sensing systems, Planning, learning (artificial intelligence), inductive logic programming]
RERAN: Timing- and touch-sensitive record and replay for Android
2013 35th International Conference on Software Engineering
None
2013
Touchscreen-based devices such as smartphones and tablets are gaining popularity, but their rich input capabilities pose new development and testing complications. To alleviate this problem, we present an approach and tool named Reran that permits record-and-replay for the Android smartphone platform. Existing GUI-level record-and-replay approaches are inadequate due to the expressiveness of the smartphone domain, in which applications support sophisticated GUI gestures, depend on inputs from a variety of sensors on the device, and have precise timing requirements among the various input events. We address these challenges by directly capturing the low-level event stream on the phone, which includes both GUI events and sensor events, and replaying it with microsecond accuracy. Moreover, Reran does not require access to app source code, perform any app rewriting, or perform any modifications to the virtual machine or Android platform. We demonstrate RERAN's applicability in a variety of scenarios, including (a) replaying 86 out of the Top-100 Android apps on Google Play; (b) reproducing bugs in popular apps, e.g., Firefox, Facebook, Quickoffice; and (c) fast-forwarding executions. We believe that our versatile approach can help both Android developers and researchers.
[program debugging, graphical user interfaces, RERAN, Android smartphone platform, sensor events, fast-forwarding executions, Android apps, Quickoffice, touchscreen-based devices, Presses, mobile computing, gesture recognition, Google Play, bug reproduction, GUI gestures, Google Android, Record-and-replay, Sensors, touch sensitive screens, testing complications, Facebook, Graphical user interfaces, Reran, Google, GUI events, timing-sensitive record-and-replay, smart phones, Compass, Firefox, operating systems (computers), human computer interaction, Timing, touch-sensitive record-and-replay, tablets, Smart phones]
Inferring likely mappings between APIs
2013 35th International Conference on Software Engineering
None
2013
Software developers often need to port applications written for a source platform to a target platform. In doing so, a key task is to replace an application's use of methods from the source platform API with corresponding methods from the target platform API. However, this task is challenging because developers must manually identify mappings between methods in the source and target APIs, e.g., using API documentation. We develop a novel approach to the problem of inferring mappings between the APIs of a source and target platform. Our approach is tailored to the case where the source and target platform each have independently-developed applications that implement similar functionality. We observe that in building these applications, developers exercised knowledge of the corresponding APIs. We develop a technique to systematically harvest this knowledge and infer likely mappings between the APIs of the source and target platform. The output of our approach is a ranked list of target API methods or method sequences that likely map to each source API method or method sequence. We have implemented this approach in a prototype tool called Rosetta, and have applied it to infer likely mappings between the Java2 Platform Mobile Edition and Android graphics APIs.
[Java, Android graphics APIs, application program interfaces, Humanoid robots, software developers, Probability distribution, independently-developed applications, API documentation, API methods, method sequence, mapping inferrence, target platform API, Java2 Platform Mobile Edition, Databases, Games, source platform API, Inference algorithms, Random variables, software tools, Androids, Rosetta]
Estimating mobile application energy consumption using program analysis
2013 35th International Conference on Software Engineering
None
2013
Optimizing the energy efficiency of mobile applications can greatly increase user satisfaction. However, developers lack viable techniques for estimating the energy consumption of their applications. This paper proposes a new approach that is both lightweight in terms of its developer requirements and provides fine-grained estimates of energy consumption at the code level. It achieves this using a novel combination of program analysis and per-instruction energy modeling. In evaluation, our approach is able to estimate energy consumption to within 10% of the ground truth for a set of mobile applications from the Google Play store. Additionally, it provides useful and meaningful feedback to developers that helps them to understand application energy consumption behavior.
[Energy consumption, per-instruction energy modeling, Instruments, program diagnostics, energy consumption estimation, Mobile communication, Generators, Mobile app, fine-grained energy estimation, mobile computing, Google Play store, program analysis, Cost function, Hardware, Software, mobile application energy consumption, user satisfaction]
Observable modified condition/decision coverage
2013 35th International Conference on Software Engineering
None
2013
In many critical systems domains, test suite adequacy is currently measured using structural coverage metrics over the source code. Of particular interest is the modified condition/decision coverage (MC/DC) criterion required for, e.g., critical avionics systems. In previous investigations we have found that the efficacy of such test suites is highly dependent on the structure of the program under test and the choice of variables monitored by the oracle. MC/DC adequate tests would frequently exercise faulty code, but the effects of the faults would not propagate to the monitored oracle variables. In this report, we combine the MC/DC coverage metric with a notion of observability that helps ensure that the result of a fault encountered when covering a structural obligation propagates to a monitored variable; we term this new coverage criterion Observable MC/DC (OMC/DC). We hypothesize this path requirement will make structural coverage metrics 1.) more effective at revealing faults, 2.) more robust to changes in program structure, and 3.) more robust to the choice of variables monitored. We assess the efficacy and sensitivity to program structure of OMC/DC as compared to masking MC/DC using four subsystems from the civil avionics domain and the control logic of a microwave. We have found that test suites satisfying OMC/DC are significantly more effective than test suites satisfying MC/DC, revealing up to 88% more faults, and are less sensitive to program structure and the choice of monitored variables.
[structural obligation, observable modified condition-decision coverage criterion, fault diagnosis, program testing, oracle variables, critical system, fault propagation, Semantics, aerospace computing, critical avionics system, Mathematical model, Observability, observability, civil avionics domain, Monitoring, microwave, Context, faulty code, source code, control logic, avionics, program structure, structural coverage metrics, Syntactics, Delays]
Creating a shared understanding of testing culture on a social coding site
2013 35th International Conference on Software Engineering
None
2013
Many software development projects struggle with creating and communicating a testing culture that is appropriate for the project's needs. This may degrade software quality by leaving defects undiscovered. Previous research suggests that social coding sites such as GitHub provide a collaborative environment with a high degree of social transparency. This makes developers' actions and interactions more visible and traceable. We conducted interviews with 33 active users of GitHub to investigate how the increased transparency found on GitHub influences developers' testing behaviors. Subsequently, we validated our findings with an online questionnaire that was answered by 569 members of GitHub. We found several strategies that software developers and managers can use to positively influence the testing behavior in their projects. However, project owners on GitHub may not be aware of them. We report on the challenges and risks caused by this and suggest guidelines for promoting a sustainable testing culture in software development projects.
[collaborative environment, program testing, GitHub, online questionnaire, Media, Encoding, software quality, Guidelines, social coding site, software development projects, Sociology, groupware, social networking (online), Software, Interviews, Testing, social transparency, sustainable testing culture]
Billions and billions of constraints: Whitebox fuzz testing in production
2013 35th International Conference on Software Engineering
None
2013
We report experiences with constraint-based whitebox fuzz testing in production across hundreds of large Windows applications and over 500 machine years of computation from 2007 to 2013. Whitebox fuzzing leverages symbolic execution on binary traces and constraint solving to construct new inputs to a program. These inputs execute previously uncovered paths or trigger security vulnerabilities. Whitebox fuzzing has found one-third of all file fuzzing bugs during the development of Windows 7, saving millions of dollars in potential security vulnerabilities. The technique is in use today across multiple products at Microsoft. We describe key challenges with running whitebox fuzzing in production. We give principles for addressing these challenges and describe two new systems built from these principles: SAGAN, which collects data from every fuzzing run for further analysis, and JobCenter, which controls deployment of our whitebox fuzzing infrastructure across commodity virtual machines. Since June 2010, SAGAN has logged over 3.4 billion constraints solved, millions of symbolic executions, and tens of millions of test cases generated. Our work represents the largest scale deployment of whitebox fuzzing to date, including the largest usage ever for a Satisfiability Modulo Theories (SMT) solver. We present specific data analyses that improved our production use of whitebox fuzzing. Finally we report data on the performance of constraint solving and dynamic test generation that points toward future research problems.
[satisfiability modulo theories, dynamic test generation, SAGAN system, SMT solver, data analysis, program testing, program diagnostics, computability, commodity virtual machines, Security, Servers, security of data, Computer bugs, constraint solving performance, JobCenter, Production, virtual machines, constraint-based whitebox fuzz testing, symbolic execution, Windows 7, security vulnerability, Monitoring, binary traces, Testing]
Feedback-directed unit test generation for C/C&#x002B;&#x002B; using concolic execution
2013 35th International Conference on Software Engineering
None
2013
In industry, software testing and coverage-based metrics are the predominant techniques to check correctness of software. This paper addresses automatic unit test generation for programs written in C/C++. The main idea is to improve the coverage obtained by feedback-directed random test generation methods, by utilizing concolic execution on the generated test drivers. Furthermore, for programs with numeric computations, we employ non-linear solvers in a lazy manner to generate new test inputs. These techniques significantly improve the coverage provided by a feedback-directed random unit testing framework, while retaining the benefits of full automation. We have implemented these techniques in a prototype platform, and describe promising experimental results on a number of C/C++ open source benchmarks.
[feedback-directed random unit testing framework, feedback-directed unit test generation, program testing, Instruments, program diagnostics, public domain software, nonlinear solvers, test drivers, C++ language, automatic unit test generation, concolic execution, automatic testing, feedback, feedback-directed random test generation methods, Runtime, C/C++ open source benchmarks, Iterative closest point algorithm, Concrete, Software, Testing]
A learning-based method for combining testing techniques
2013 35th International Conference on Software Engineering
None
2013
This work presents a method to combine testing techniques adaptively during the testing process. It intends to mitigate the sources of uncertainty of software testing processes, by learning from past experience and, at the same time, adapting the technique selection to the current testing session. The method is based on machine learning strategies. It uses offline strategies to take historical information into account about the techniques performance collected in past testing sessions; then, online strategies are used to adapt the selection of test cases to the data observed as the testing proceeds. Experimental results show that techniques performance can be accurately characterized from features of the past testing sessions, by means of machine learning algorithms, and that integrating this result into the online algorithm allows improving the fault detection effectiveness with respect to single testing techniques, as well as to their random combination.
[Measurement, technique selection, program testing, software testing, online algorithm, fault detection effectiveness, offline strategies, Complexity theory, machine learning, software fault tolerance, Prediction algorithms, Feature extraction, Software, Bayes methods, learning (artificial intelligence), Testing]
Human performance regression testing
2013 35th International Conference on Software Engineering
None
2013
As software systems evolve, new interface features such as keyboard shortcuts and toolbars are introduced. While it is common to regression test the new features for functional correctness, there has been less focus on systematic regression testing for usability, due to the effort and time involved in human studies. Cognitive modeling tools such as CogTool provide some help by computing predictions of user performance, but they still require manual effort to describe the user interface and tasks, limiting regression testing efforts. In recent work, we developed CogTool-Helper to reduce the effort required to generate human performance models of existing systems. We build on this work by providing task specific test case generation and present our vision for human performance regression testing (HPRT) that generates large numbers of test cases and evaluates a range of human performance predictions for the same task. We examine the feasibility of HPRT on four tasks in LibreOffice, find several regressions, and then discuss how a project team could use this information. We also illustrate that we can increase efficiency with sampling by leveraging an inference algorithm. Samples that take approximately 50% of the runtime lose at most 10% of the performance predictions.
[program testing, public domain software, software systems, human factors, regression analysis, human performance regression testing, Predictive models, user interfaces, cognitive modeling tools, test case generation, LibreOffice, user performance prediction, Testing, Graphical user interfaces, Computational modeling, HPRT, software maintenance, interface features, Human computer interaction, functional correctness, CogTool-Helper, inference algorithm, Keyboards, keyboard shortcuts, Usability, toolbars]
Guided test generation for web applications
2013 35th International Conference on Software Engineering
None
2013
We focus on functional testing of enterprise applications with the goal of exercising an application's interesting behaviors by driving it from its user interface. The difficulty in doing this is focusing on the interesting behaviors among an unbounded number of behaviors. We present a new technique for automatically generating tests that drive a web-based application along interesting behaviors, where the interesting behavior is specified in the form of &#x201C;business rules.&#x201D; Business rules are a general mechanism for describing business logic, access control, or even navigational properties of an application's GUI. Our technique is black box, in that it does not analyze the application's server-side implementation, but relies on directed crawling via the application's GUI. To handle the unbounded number of GUI states, the technique includes two phases. Phase 1 creates an abstract state-transition diagram using a relaxed notion of equivalence of GUI states without considering rules. Next, Phase 2 identifies rule-relevant abstract paths and refines those paths using a stricter notion of state equivalence. Our technique can be much more effective at covering business rules than an undirected technique, developed as an enhancement of an existing test-generation technique. Our experiments showed that the former was able to cover 92% of the rules, compared to 52% of the rules covered by the latter.
[Access control, program testing, graphical user interfaces, GUI states, Manuals, Web applications, automatic test generation, business rules, formal specification, rule-relevant abstract paths, guided test generation, functional testing, Abstracts, authorisation, access control, information systems, navigational properties, abstract state-transition diagram, Business, Graphical user interfaces, Testing, Navigation, state equivalence, enterprise applications, user interface, functional specification, business logic, Internet]
Comparing Multi-Point Stride Coverage and dataflow coverage
2013 35th International Conference on Software Engineering
None
2013
We introduce a family of coverage criteria, called Multi-Point Stride Coverage (MPSC). MPSC generalizes branch coverage to coverage of tuples of branches taken from the execution sequence of a program. We investigate its potential as a replacement for dataflow coverage, such as def-use coverage. We find that programs can be instrumented for MPSC easily, that the instrumentation usually incurs less overhead than that for def-use coverage, and that MPSC is comparable in usefulness to def-use in predicting test suite effectiveness. We also find that the space required to collect MPSC can be predicted from the number of branches in the program.
[Java, Control Flow Coverage, program testing, Instruments, software testing, Switches, data flow analysis, def-use coverage, Software Testing, Partial Execution Pattern, Educational institutions, Data structures, test suite effectiveness, dataflow coverage, Data Flow Coverage, Upper bound, coverage criteria family, program execution sequence, MPSC, multipoint stride coverage, Software measurement, branch coverage]
Interaction-based test-suite minimization
2013 35th International Conference on Software Engineering
None
2013
Combinatorial Test Design (CTD) is an effective test planning technique that reveals faults resulting from feature interactions in a system. The standard application of CTD requires manual modeling of the test space, including a precise definition of restrictions between the test space parameters, and produces a test suite that corresponds to new test cases to be implemented from scratch. In this work, we propose to use Interaction-based Test-Suite Minimization (ITSM) as a complementary approach to standard CTD. ITSM reduces a given test suite without impacting its coverage of feature interactions. ITSM requires much less modeling effort, and does not require a definition of restrictions. It is appealing where there has been a significant investment in an existing test suite, where creating new tests is expensive, and where restrictions are very complex. We discuss the tradeoffs between standard CTD and ITSM, and suggest an efficient algorithm for solving the latter. We also discuss the challenges and additional requirements that arise when applying ITSM to real-life test suites. We introduce solutions to these challenges and demonstrate them through two real-life case studies.
[Pediatrics, interaction-based test-suite minimization, program testing, software systems, ITSM, combinatorial test design, test cases, feature interactions, Minimization, Standards, test space modeling, CTD, Hospitals, Senior citizens, test space parameters, Surgery, system faults, test planning technique]
Bridging the gap between the total and additional test-case prioritization strategies
2013 35th International Conference on Software Engineering
None
2013
In recent years, researchers have intensively investigated various topics in test-case prioritization, which aims to re-order test cases to increase the rate of fault detection during regression testing. The total and additional prioritization strategies, which prioritize based on total numbers of elements covered per test, and numbers of additional (not-yet-covered) elements covered per test, are two widely-adopted generic strategies used for such prioritization. This paper proposes a basic model and an extended model that unify the total strategy and the additional strategy. Our models yield a spectrum of generic strategies ranging between the total and additional strategies, depending on a parameter referred to as the p value. We also propose four heuristics to obtain differentiated p values for different methods under test. We performed an empirical study on 19 versions of four Java programs to explore our results. Our results demonstrate that wide ranges of strategies in our basic and extended models with uniform p values can significantly outperform both the total and additional strategies. In addition, our results also demonstrate that using differentiated p values for both the basic and extended models with method coverage can even outperform the additional strategy using statement coverage.
[Measurement, Java, program testing, regression testing, fault detection, Educational institutions, test-case prioritization strategies, software fault tolerance, Fault detection, heuristics, Software, Arrays, Java programs, Testing]
Detecting spurious counterexamples efficiently in abstract model checking
2013 35th International Conference on Software Engineering
None
2013
Abstraction is one of the most important strategies for dealing with the state space explosion problem in model checking. With an abstract model, the state space is largely reduced, however, a counterexample found in such a model that does not satisfy the desired property may not exist in the concrete model. Therefore, how to check whether a reported counterexample is spurious is a key problem in the abstraction-refinement loop. Particularly, there are often thousands of millions of states in systems of industrial scale, how to check spurious counterexamples in these systems practically is a significant problem. In this paper, by re-analyzing spurious counterexamples, a new formal definition of spurious path is given. Based on it, efficient algorithms for detecting spurious counterexamples are presented. By the new algorithms, when dealing with infinite counterexamples, the finite prefix to be analyzed will be polynomially shorter than the one dealt by the existing algorithm. Moreover, in practical terms, the new algorithms can naturally be parallelized that makes multi-core processors contributes more in spurious counterexample checking. In addition, by the new algorithms, the state resulting in a spurious path (false state) that is hidden shallower will be reported earlier. Hence, as long as a false state is detected, lots of iterations for detecting all the false states will be avoided. Experimental results show that the new algorithms perform well along with the growth of system scale.
[Algorithm design and analysis, parallel algorithms, parallel algorithm, Color, abstraction, abstract model checking, finite prefix, state space explosion problem, formal spurious path definition, formal verification, model checking, Abstracts, Model checking, Concrete, Polynomials, abstraction-refinement loop, spurious counterexample checking, refinement, spurious counterexample detection, multicore processor, Integrated circuit modeling]
Segmented symbolic analysis
2013 35th International Conference on Software Engineering
None
2013
Symbolic analysis is indispensable for software tools that require program semantic information at compile time. However, determining symbolic values for program variables related to loops and library calls is challenging, as the computation and data related to loops can have statically unknown bounds, and the library sources are typically not available at compile time. In this paper, we propose segmented symbolic analysis, a hybrid technique that enables fully automatic symbolic analysis even for the traditionally challenging code of library calls and loops. The novelties of this work are threefold: 1) we flexibly weave symbolic and concrete executions on the selected parts of the program based on demand; 2) dynamic executions are performed on the unit tests constructed from the code segments to infer program semantics needed by static analysis; and 3) the dynamic information from multiple runs is aggregated via regression analysis. We developed the Helium framework, consisting of a static component that performs symbolic analysis and partitions a program, a dynamic analysis that synthesizes unit tests and automatically infers symbolic values for program variables, and a protocol that enables static and dynamic analyses to be run interactively and concurrently. Our experimental results show that by handling loops and library calls that a traditional symbolic analysis cannot process, segmented symbolic analysis detects 5 times more buffer overflows. The technique is scalable for real-world programs such as putty, tightvnc and snort.
[library loops, buffer overflow, regression analysis, Cognition, program compilers, code segments, software libraries, handling loops, Analytical models, concrete execution, program semantics, Semantics, library calls, Libraries, Performance analysis, software tools, program semantic information, helium framework, program diagnostics, static analysis, dynamic executions, dynamic analysis, program variables, Regression analysis, Helium, static component, symbolic execution, symbolic partitions, segmented symbolic analysis]
Explicating symbolic execution (xSymExe): An evidence-based verification framework
2013 35th International Conference on Software Engineering
None
2013
Previous applications of symbolic execution (Sym-Exe) have focused on bug-finding and test-case generation. However, SymExe has the potential to significantly improve usability and automation when applied to verification of software contracts in safety-critical systems. Due to the lack of support for processing software contracts and ad hoc approaches for introducing a variety of over/under-approximations and optimizations, most SymExe implementations cannot precisely characterize the verification status of contracts. Moreover, these tools do not provide explicit justifications for their conclusions, and thus they are not aligned with trends toward evidence-based verification and certification. We introduce the concept of explicating symbolic execution (xSymExe) that builds on a strong semantic foundation, supports full verification of rich software contracts, explicitly tracks where over/under-approximations are introduced or avoided, precisely characterizes the verification status of each contractual claim, and associates each claim with explications for its reported verification status. We report on case studies in the use of Bakar Kiasan, our open source xSymExe tool for Spark Ada.
[evidence-based verification framework, safety-critical software, Indexes, Sparks, open source xSymExe tool, software contract, Spark Ada, evidence-based certification, formal verification, Bakar Kiasan, Semantics, safety-critical system, symbolic execution, Concrete, Software, semantic foundation, Arrays, Contracts]
Aluminum: Principled scenario exploration through minimality
2013 35th International Conference on Software Engineering
None
2013
Scenario-finding tools such as Alloy are widely used to understand the consequences of specifications, with applications to software modeling, security analysis, and verification. This paper focuses on the exploration of scenarios: which scenarios are presented first, and how to traverse them in a well-defined way. We present Aluminum, a modification of Alloy that presents only minimal scenarios: those that contain no more than is necessary. Aluminum lets users explore the scenario space by adding to scenarios and backtracking. It also provides the ability to find what can consistently be used to extend each scenario. We describe the semantic basis of Aluminum in terms of minimal models of first-order logic formulas. We show how this theory can be implemented atop existing SAT-solvers and quantify both the benefits of minimality and its small computational overhead. Finally, we offer some qualitative observations about scenario exploration in Aluminum.
[Visualization, security analysis, software verification, Aluminum, Unified modeling language, computability, aluminum, SAT-solvers, first-order logic formulas, computational overhead, scenario-finding tools, formal verification, security of data, principled scenario exploration, Semantics, software modeling, aluminium alloys, backtracking, Space exploration, minimality, Software engineering]
Counter play-out: Executing unrealizable scenario-based specifications
2013 35th International Conference on Software Engineering
None
2013
The scenario-based approach to the specification and simulation of reactive systems has attracted much research efforts in recent years. While the problem of synthesizing a controller or a transition system from a scenario-based specification has been studied extensively, no work has yet effectively addressed the case where the specification is unrealizable and a controller cannot be synthesized. This has limited the effectiveness of using scenario-based specifications in requirements analysis and simulation. In this paper we present counter play-out, an interactive debugging method for unrealizable scenario-based specifications. When we identify an unrealizable specification, we generate a controller that plays the role of the environment and lets the engineer play the role of the system. During execution, the former chooses environment's moves such that the latter is forced to eventually fail in satisfying the system's requirements. This results in an interactive, guided execution, leading to the root causes of unrealizability. The generated controller constitutes a proof that the specification is conflicting and cannot be realized. Counter play-out is based on a counter strategy, which we compute by solving a Rabin game using a symbolic, BDD-based algorithm. The work is implemented and integrated with PlayGo, an IDE for scenario-based programming developed at the Weizmann Institute of Science. Case studies show the contribution of our work to the state-of-the-art in the scenario-based approach to specification and simulation.
[program debugging, counter play-out, Radiation detectors, Rabin game, Force, scenario-based specification, Debugging, BDD-based algorithm, IDE, formal specification, binary decision diagrams, Semantics, Games, Bismuth, Safety, scenario-based programming, interactive debugging method, PlayGo]
Unifying FSM-inference algorithms through declarative specification
2013 35th International Conference on Software Engineering
None
2013
Logging system behavior is a staple development practice. Numerous powerful model inference algorithms have been proposed to aid developers in log analysis and system understanding. Unfortunately, existing algorithms are difficult to understand, extend, and compare. This paper presents InvariMint, an approach to specify model inference algorithms declaratively. We applied InvariMint to two model inference algorithms and present evaluation results to illustrate that InvariMint (1) leads to new fundamental insights and better understanding of existing algorithms, (2) simplifies creation of new algorithms, including hybrids that extend existing algorithms, and (3) makes it easy to compare and contrast previously published algorithms. Finally, algorithms specified with InvariMint can outperform their procedural versions.
[Algorithm design and analysis, Doped fiber amplifiers, Educational institutions, Electronic mail, finite state machines, inference mechanisms, declarative specification, model inference algorithm, formal specification, Postal services, logging system behavior, FSM-inference algorithms, system understanding, Approximation algorithms, system monitoring, Inference algorithms, log analysis, InvariMint]
What good are strong specifications?
2013 35th International Conference on Software Engineering
None
2013
Experience with lightweight formal methods suggests that programmers are willing to write specification if it brings tangible benefits to their usual development activities. This paper considers stronger specifications and studies whether they can be deployed as an incremental practice that brings additional benefits without being unacceptably expensive. We introduce a methodology that extends Design by Contract to write strong specifications of functional properties in the form of preconditions, postconditions, and invariants. The methodology aims at being palatable to developers who are not fluent in formal techniques but are comfortable with writing simple specifications. We evaluate the cost and the benefits of using strong specifications by applying the methodology to testing data structure implementations written in Eiffel and C#. In our extensive experiments, testing against strong specifications detects twice as many bugs as standard contracts, with a reasonable overhead in terms of annotation burden and run-time performance while testing. In the wide spectrum of formal techniques for software quality, testing against strong specifications lies in a &#x201C;sweet spot&#x201D; with a favorable benefit to effort ratio.
[data structure testing, C#, program testing, development activities, C++ language, software quality, Indexes, specifications detect, formal specification, Standards, design by contract, lightweight formal method, Writing, Software, data structures, Mathematical model, functional property specification, Contracts, Eiffel, Testing]
Comparative causality: Explaining the differences between executions
2013 35th International Conference on Software Engineering
None
2013
We propose a novel fine-grained causal inference technique. Given two executions and some observed differences between them, the technique reasons about the causes of such differences. The technique does so by state replacement, i.e. replacing part of the program state at an earlier point to observe whether the target differences can be induced. It makes a number of key advances: it features a novel execution model that avoids undesirable entangling of the replaced state and the original state; it properly handles differences of omission by symmetrically analyzing both executions; it also leverages a recently developed slicing technique to limit the scope of causality testing while ensuring that no relevant state causes can be missed. The application of the technique on automated debugging shows that it substantially improves the precision and efficiency of causal inference compared to state of the art techniques.
[program debugging, program testing, Heuristic algorithms, slicing technique, Debugging, automated debugging, execution model, Minimization, Cognition, inference mechanisms, causality, Computer bugs, causality testing, fine-grained causal inference technique, Software, comparative causality, state replacement, program slicing, Testing]
Automatic testing of sequential and concurrent substitutability
2013 35th International Conference on Software Engineering
None
2013
Languages with inheritance and polymorphism assume that a subclass instance can substitute a superclass instance without causing behavioral differences for clients of the superclass. However, programmers may accidentally create subclasses that are semantically incompatible with their superclasses. Such subclasses lead to bugs, because a programmer may assign a subclass instance to a superclass reference. This paper presents an automatic testing technique to reveal subclasses that cannot safely substitute their superclasses. The key idea is to generate generic tests that analyze the behavior of both the subclass and its superclass. If using the subclass leads to behavior that cannot occur with the superclass, the analysis reports a warning. We find a high percentage of widely used Java classes, including classes from JBoss, Eclipse, and Apache Commons Collections, to be unsafe substitutes for their superclasses: 30% of these classes lead to crashes, and even more have other behavioral differences.
[Java, program debugging, program testing, sequential substitutability, languages, JBoss classes, Programming, superclass reference, Generators, polymorphism, subclass instance, automatic testing technique, bugs, Runtime, Eclipse classes, Computer bugs, concurrency control, concurrent substitutability, Java classes, Libraries, programmers, superclass instance, Apache commons collections, behavioral differences]
Data clone detection and visualization in spreadsheets
2013 35th International Conference on Software Engineering
None
2013
Spreadsheets are widely used in industry: it is estimated that end-user programmers outnumber programmers by a factor 5. However, spreadsheets are error-prone, numerous companies have lost money because of spreadsheet errors. One of the causes for spreadsheet problems is the prevalence of copy-pasting. In this paper, we study this cloning in spreadsheets. Based on existing text-based clone detection algorithms, we have developed an algorithm to detect data clones in spreadsheets: formulas whose values are copied as plain text in a different location. To evaluate the usefulness of the proposed approach, we conducted two evaluations. A quantitative evaluation in which we analyzed the EUSES corpus and a qualitative evaluation consisting of two case studies. The results of the evaluation clearly indicate that 1) data clones are common, 2) data clones pose threats to spreadsheet quality and 3) our approach supports users in finding and resolving data clones.
[Algorithm design and analysis, end-user programmers, clone detection, spreadsheet quality, Cloning, qualitative evaluation, Companies, data clone visualization, Educational institutions, spreadsheet programs, software quality, EUSES corpus, copy-pasting, spreadsheets, spreadsheet errors, code smells, text-based clone detection algorithms, Clustering algorithms, Data visualization, data visualisation, data clone detection, Detection algorithms, software performance evaluation, spreadsheet smells]
Partition-based regression verification
2013 35th International Conference on Software Engineering
None
2013
Regression verification (RV) seeks to guarantee the absence of regression errors in a changed program version. This paper presents Partition-based Regression Verification (PRV): an approach to RV based on the gradual exploration of differential input partitions. A differential input partition is a subset of the common input space of two program versions that serves as a unit of verification. Instead of proving the absence of regression for the complete input space at once, PRV verifies differential partitions in a gradual manner. If the exploration is interrupted, PRV retains partial verification guarantees at least for the explored differential partitions. This is crucial in practice as verifying the complete input space can be prohibitively expensive. Experiments show that PRV provides a useful alternative to state-of-the-art regression test generation techniques. During the exploration, PRV generates test cases which can expose different behaviour across two program versions. However, while test cases are generally single points in the common input space, PRV can verify entire partitions and moreover give feedback that allows programmers to relate a behavioral difference to those syntactic changes that contribute to this difference.
[automatic test software, automatic test pattern generation, program verification, regression error, regression analysis, Educational institutions, differential input partition, Partitioning algorithms, partial verification guarantee, Software Verification, Semantics, partition-based regression verification, PRV, Syntactics, Nickel, Concrete, Testing and Analysis, gradual exploration, Testing, regression test generation technique]
Automated diagnosis of software configuration errors
2013 35th International Conference on Software Engineering
None
2013
The behavior of a software system often depends on how that system is configured. Small configuration errors can lead to hard-to-diagnose undesired behaviors. We present a technique (and its tool implementation, called ConfDiagnoser) to identify the root cause of a configuration error - a single configuration option that can be changed to produce desired behavior. Our technique uses static analysis, dynamic profiling, and statistical analysis to link the undesired behavior to specific configuration options. It differs from existing approaches in two key aspects: it does not require users to provide a testing oracle (to check whether the software functions correctly) and thus is fully automated; and it can diagnose both crashing and non-crashing errors. We evaluated ConfDiagnoser on 5 non-crashing configuration errors and 9 crashing configuration errors from 5 configurable software systems written in Java. On average, the root cause was ConfDiagnoser's fifth-ranked suggestion; in 10 out of 14 errors, the root cause was one of the top 3 suggestions; and more than half of the time, the root cause was the first suggestion.
[noncrashing configuration errors, Java, configuration error identification, Instruments, program diagnostics, Debugging, static analysis, Computer crashes, configurable software system, configuration management, Databases, dynamic profiling, ConfDiagnoser, Software, Libraries, statistical analysis, software configuration error diagnosis]
Detecting deadlock in programs with data-centric synchronization
2013 35th International Conference on Software Engineering
None
2013
Previously, we developed a data-centric approach to concurrency control in which programmers specify synchronization constraints declaratively, by grouping shared locations into atomic sets. We implemented our ideas in a Java extension called AJ, using Java locks to implement synchronization. We proved that atomicity violations are prevented by construction, and demonstrated that realistic Java programs can be refactored into AJ without significant loss of performance. This paper presents an algorithm for detecting possible deadlock in AJ programs by ordering the locks associated with atomic sets. In our approach, a type-based static analysis is extended to handle recursive data structures by considering programmer-supplied, compiler-verified lock ordering annotations. In an evaluation of the algorithm, all 10 AJ programs under consideration were shown to be deadlock-free. One program needed 4 ordering annotations and 2 others required minor refactorings. For the remaining 7 programs, no programmer intervention of any kind was required.
[Algorithm design and analysis, Java, recursive data structures, Instruction sets, program diagnostics, data-centric synchronization, Data structures, Educational institutions, AJ, type-based static analysis, Synchronization, software maintenance, constraints synchronization, program compilers, compiler-verified lock ordering annotations, synchronisation, atomic sets, concurrency control, Java extension, System recovery, data structures, program deadlock detection, Java locks, atomicity violations]
The design of bug fixes
2013 35th International Conference on Software Engineering
None
2013
When software engineers fix bugs, they may have several options as to how to fix those bugs. Which fix they choose has many implications, both for practitioners and researchers: What is the risk of introducing other bugs during the fix? Is the bug fix in the same code that caused the bug? Is the change fixing the cause or just covering a symptom? In this paper, we investigate alternative fixes to bugs and present an empirical study of how engineers make design choices about how to fix bugs. Based on qualitative interviews with 40 engineers working on a variety of products, data from 6 bug triage meetings, and a survey filled out by 326 engineers, we found a number of factors, many of them non-technical, that influence how bugs are fixed, such as how close to release the software is. We also discuss several implications for research and practice, including ways to make bug prediction and localization more accurate.
[program debugging, Protocols, Data analysis, Buildings, empirical study, Encoding, bug prediction, design choice, faults, bug localization, bugs, software release, Computer bugs, bug triage meeting, design, bug introduction, Software, software engineering, Interviews, bug fix design]
PorchLight: A tag-based approach to bug triaging
2013 35th International Conference on Software Engineering
None
2013
Bug triaging is an important activity in any software development project. It involves developers working through the set of unassigned bugs, determining for each of the bugs whether it represents a new issue that should receive attention, and, if so, assigning it to a developer and a milestone. Current tools provide only minimal support for bug triaging and especially break down when developers must triage a large number of bug reports, since those reports can only be viewed one-by-one. This paper presents PorchLight, a novel tool that uses tags, attached to individual bug reports by queries expressed in a specialized bug query language, to organize bug reports into sets so developers can explore, work with, and ultimately assign bugs effectively in meaningful groups. We describe the challenges in supporting bug triaging, the design decisions upon which PorchLight rests, and the technical aspects of the implementation. We conclude with an early evaluation that involved six professional developers who assessed PorchLight and its potential for their day-to-day triaging duties.
[Context, program debugging, PorchLight tool, Automation, Communities, software development project, History, tags, Computer bugs, Tagging, software design decisions, tag-based approach, bug triaging, bug query language, Software, bug reports, software tools, bug trackers]
Expositor: Scriptable time-travel debugging with first-class traces
2013 35th International Conference on Software Engineering
None
2013
We present Expositor, a new debugging environment that combines scripting and time-travel debugging to allow programmers to automate complex debugging tasks. The fundamental abstraction provided by Expositor is the execution trace, which is a time-indexed sequence of program state snapshots. Programmers can manipulate traces as if they were simple lists with operations such as map and filter. Under the hood, Expositor efficiently implements traces as lazy, sparse interval trees whose contents are materialized on demand. Expositor also provides a novel data structure, the edit hash array mapped trie, which is a lazy implementation of sets, maps, multisets, and multimaps that enables programmers to maximize the efficiency of their debugging scripts. We have used Expositor to debug a stack overflow and to unravel a subtle data race in Firefox. We believe that Expositor represents an important step forward in improving the technology for diagnosing complex, hard-to-understand bugs.
[program debugging, program diagnostics, Force, Optimized production technology, program state snapshots, Debugging, Expositor, scriptable time-travel debugging, Programming, Data structures, time-indexed sequence, hard-to-understand bugs, Computer bugs, Writing, diagnosing complex, first-class traces]
Chronicler: Lightweight recording to reproduce field failures
2013 35th International Conference on Software Engineering
None
2013
When programs fail in the field, developers are often left with limited information to diagnose the failure. Automated error reporting tools can assist in bug report generation but without precise steps from the end user it is often difficult for developers to recreate the failure. Advanced remote debugging tools aim to capture sufficient information from field executions to recreate failures in the lab but often have too much overhead to practically deploy. We present Chronicler, an approach to remote debugging that captures non-deterministic inputs to applications in a lightweight manner, assuring faithful reproduction of client executions. We evaluated Chronicler by creating a Java implementation, ChroniclerJ, and then by using a set of benchmarks mimicking real world applications and workloads, showing its runtime overhead to be under 10% in most cases (worst case 86%), while an existing tool showed overhead over 100% in the same cases (worst case 2,322%).
[Java, program debugging, CHRONICLERJ, Software maintenance, Instruments, Instruction sets, Debugging, automated error reporting tools, Computer crashes, Java implementation, Maintainability, software fault tolerance, Runtime, nondeterministic inputs, advanced remote debugging tools, bug report generation, failure diagnosis, Error handling and recovery, Libraries, field failures, Debugging aids, software tools]
Does bug prediction support human developers? Findings from a Google case study
2013 35th International Conference on Software Engineering
None
2013
While many bug prediction algorithms have been developed by academia, they're often only tested and verified in the lab using automated means. We do not have a strong idea about whether such algorithms are useful to guide human developers. We deployed a bug prediction algorithm across Google, and found no identifiable change in developer behavior. Using our experience, we provide several characteristics that bug prediction algorithms need to meet in order to be accepted by human developers and truly change how developers evaluate their code.
[Algorithm design and analysis, Measurement, Google, program debugging, bug prediction algorithms, Software algorithms, human developers, prediction theory, Computer bugs, Prediction algorithms, Software, software engineering, developer behavior]
Transfer defect learning
2013 35th International Conference on Software Engineering
None
2013
Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance.
[Measurement, transfer learning, public domain software, Predictive models, transfer defect learning, within-project prediction settings, Vectors, software defect prediction approaches, open-source projects, Standards, Training, feature distributions, source projects, empirical software engineering, Data models, Software, software engineering, cross-project defect prediction, learning (artificial intelligence), TCA+]
It's not a bug, it's a feature: How misclassification impacts bug prediction
2013 35th International Conference on Software Engineering
None
2013
In a manual examination of more than 7,000 issue reports from the bug databases of five open-source projects, we found 33.8% of all bug reports to be misclassified - that is, rather than referring to a code fix, they resulted in a new feature, an update to documentation, or an internal refactoring. This misclassification introduces bias in bug prediction models, confusing bugs and features: On average, 39% of files marked as defective actually never had a bug. We discuss the impact of this misclassification on earlier studies and recommend manual data validation for future studies.
[program debugging, Noise, data mining, Manuals, Documentation, documentation, Mining software repositories, Inspection, Maintenance engineering, software maintenance, data quality, Databases, Computer bugs, internal refactoring, bias, noise, bug reports, bug prediction model, bug reports misclassification]
Assisting developers of Big Data Analytics Applications when deploying on Hadoop clouds
2013 35th International Conference on Software Engineering
None
2013
Big data analytics is the process of examining large amounts of data (big data) in an effort to uncover hidden patterns or unknown correlations. Big Data Analytics Applications (BDA Apps) are a new type of software applications, which analyze big data using massive parallel processing frameworks (e.g., Hadoop). Developers of such applications typically develop them using a small sample of data in a pseudo-cloud environment. Afterwards, they deploy the applications in a large-scale cloud environment with considerably more processing power and larger input data (reminiscent of the mainframe days). Working with BDA App developers in industry over the past three years, we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches. In this paper, as a first step in assisting developers of BDA Apps for cloud deployments, we propose a lightweight approach for uncovering differences between pseudo and large-scale cloud deployments. Our approach makes use of the readily-available yet rarely used execution logs from these platforms. Our approach abstracts the execution logs, recovers the execution sequences, and compares the sequences between the pseudo and cloud deployments. Through a case study on three representative Hadoop-based BDA Apps, we show that our approach can rapidly direct the attention of BDA App developers to the major differences between the two deployments. Knowledge of such differences is essential in verifying BDA Apps when analyzing big data in the cloud. Using injected deployment faults, we show that our approach not only significantly reduces the deployment verification effort, but also provides very few false positives when identifying deployment failures.
[program debugging, Data handling, public domain software, Programming, Information management, parallel processing, Big-Data Analytics Application, developer assistance, formal verification, Cloud Computing, execution log abstraction, Monitoring and Debugging, cloud computing, execution sequence recovery, parallel processing frameworks, Context, data analysis, big data analytics applications, Keyword search, Hadoop, Hadoop-based BDA Apps, software fault tolerance, Data storage systems, software applications, Log Analysis, deployment verification effort reduction, system monitoring, cloud deployments, Joining processes, Hadoop clouds, big data analysis]
Broken sets in software repository evolution
2013 35th International Conference on Software Engineering
None
2013
Modern software systems are built by composing components drawn from large repositories, whose size and complexity increase at a fast pace. Software systems built with components from a release of a repository should be seamlessly upgradeable using components from the next release. Unfortunately, users are often confronted with sets of components that were installed together, but cannot be upgraded together to the latest version from the new repository. Identifying these broken sets can be of great help for a quality assurance team, that could examine and fix these issues well before they reach the end user. Building on previous work on component co-installability, we show that it is possible to find these broken sets for any two releases of a component repository, computing extremely efficiently a concise representation of these upgrade issues, together with informative graphical explanations. A tool implementing the algorithm presented in this paper is available as free software, and is able to process the evolution between two major releases of the Debian GNU/Linux distribution in just a few seconds. These results make it possible to integrate seamlessly this analysis in a repository development process.
[Algorithm design and analysis, object-oriented programming, public domain software, software repository evolution, Buildings, component coinstallability, software quality, software maintenance, component complexity, software system, free software, broken sets, component composition, component size, Debian GNU/Linux distribution, Quality assurance, Linux, repository development process, Software systems, quality assurance team, upgrade issues, informative graphical explanation, Testing]
Boa: A language and infrastructure for analyzing ultra-large-scale software repositories
2013 35th International Conference on Software Engineering
None
2013
In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses. However, systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard, and best left for mining software repository (MSR) experts! The goal of Boa, a domain-specific language and infrastructure described here, is to ease testing MSR-related hypotheses. We have implemented Boa and provide a web-based interface to Boa's infrastructure. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also see drastic improvements in scalability. Last but not least, reproducing an experiment conducted using Boa is just a matter of re-running small Boa programs provided by previous researchers.
[scalable, ultra-large-scale software repositories analysis, MSR related hypotheses, Protocols, software, GitHub, Data mining, repository, domain specific language, Runtime, Boa, software centric world, software packages, SourceForge, lower barrier to entry, Libraries, Java, Google code, reproducible, systematic extraction, Boa infrastructure, mining, mining software repository, ease of use, Alexandria new library, Software, Internet]
How, and why, process metrics are better
2013 35th International Conference on Software Engineering
None
2013
Defect prediction techniques could potentially help us to focus quality-assurance efforts on the most defect-prone files. Modern statistical tools make it very easy to quickly build and deploy prediction models. Software metrics are at the heart of prediction models; understanding how and especially why different types of metrics are effective is very important for successful model deployment. In this paper we analyze the applicability and efficacy of process and code metrics from several different perspectives. We build many prediction models across 85 releases of 12 large open source projects to address the performance, stability, portability and stasis of different sets of metrics. Our results suggest that code metrics, despite widespread use in the defect prediction literature, are generally less useful than process metrics for prediction. Second, we find that code metrics have high stasis; they don't change very much from release to release. This leads to stagnation in the prediction models, leading to the same files being repeatedly predicted as defective; unfortunately, these recurringly defective files turn out to be comparatively less defect-dense.
[Measurement, defect-prone files, Object oriented modeling, statistical tools, Predictive models, model deployment, Complexity theory, software quality, process metrics, Training, Support vector machines, portability, performance, defect prediction, Software, code metrics, quality-assurance efforts, statistical analysis, software performance evaluation, stability, software metrics]
The role of domain knowledge and cross-functional communication in socio-technical coordination
2013 35th International Conference on Software Engineering
None
2013
Software projects involve diverse roles and artifacts that have dependencies to requirements. Project team members in different roles need to coordinate but their coordination is affected by the availability of domain knowledge, which is distributed among different project members, and organizational structures that control cross-functional communication. Our study examines how information flowed between different roles in two software projects that had contrasting distributions of domain knowledge and different communication structures. Using observations, interviews, and surveys, we examined how diverse roles working on requirements and their related artifacts coordinated along task dependencies. We found that communication only partially matched task dependencies and that team members that are boundary spanners have extensive domain knowledge and hold key positions in the control structure. These findings have implications on how organizational structures interfere with task assignments and influence communication in the project, suggesting how practitioners can adjust team configuration and communication structures.
[domain knowledge, project management, socio-technical coordination, communication structures, Documentation, global software teams, Educational institutions, software management, project members, organizational structures, distributed development, team configuration, team working, task assignments, Software coordination, Computer architecture, Software, software projects, Marine vehicles, cross-functional communication, Portfolios, organisational aspects, Business]
Dual ecological measures of focus in software development
2013 35th International Conference on Software Engineering
None
2013
Work practices vary among software developers. Some are highly focused on a few artifacts; others make wideranging contributions. Similarly, some artifacts are mostly authored, or &#x201C;owned&#x201D;, by one or few developers; others have very wide ownership. Focus and ownership are related but different phenomena, both with strong effect on software quality. Prior studies have mostly targeted ownership; the measures of ownership used have generally been based on either simple counts, information-theoretic views of ownership, or social-network views of contribution patterns. We argue for a more general conceptual view that unifies developer focus and artifact ownership. We analogize the developer-artifact contribution network to a predator-prey food web, and draw upon ideas from ecology to produce a novel, and conceptually unified view of measuring focus and ownership. These measures relate to both cross-entropy and Kullback-Liebler divergence, and simultaneously provide two normalized measures of focus from both the developer and artifact perspectives. We argue that these measures are theoretically well-founded, and yield novel predictive, conceptual, and actionable value in software projects. We find that more focused developers introduce fewer defects than defocused developers. In contrast, files that receive narrowly focused activity are more likely to contain defects than other files.
[project management, software development, cross entropy, software management, dual ecological measures, software developers, Entropy, Environmental factors, ecology, Kullback-Liebler divergence, Atmospheric measurements, Particle measurements, Software, software engineering, software projects, predator prey food web, Software measurement]
Not going to take this anymore: Multi-objective overtime planning for Software Engineering projects
2013 35th International Conference on Software Engineering
None
2013
Software Engineering and development is well-known to suffer from unplanned overtime, which causes stress and illness in engineers and can lead to poor quality software with higher defects. In this paper, we introduce a multi-objective decision support approach to help balance project risks and duration against overtime, so that software engineers can better plan overtime. We evaluate our approach on 6 real world software projects, drawn from 3 organisations using 3 standard evaluation measures and 3 different approaches to risk assessment. Our results show that our approach was significantly better (p &lt;; 0.05) than standard multi-objective search in 76% of experiments (with high Cohen effect size in 85% of these) and was significantly better than currently used overtime planning strategies in 100% of experiments (with high effect size in all). We also show how our approach provides actionable overtime planning results and investigate the impact of the three different forms of risk assessment.
[risk assessment, project risks, Search problems, software engineering projects, software quality, Integrated circuits, unplanned overtime, software development organisations, illness, risk management, software engineers, stress, project management, software development management, standard multiobjective search, multiobjective decision support approach, project duration, Standards, decision support systems, strategic planning, overtime planning strategies, occupational stress, standard evaluation measures, multiobjective overtime planning, Software, personnel, Planning, Risk management, Software engineering]
Beyond Boolean product-line model checking: Dealing with feature attributes and multi-features
2013 35th International Conference on Software Engineering
None
2013
Model checking techniques for software product lines (SPL) are actively researched. A major limitation they currently have is the inability to deal efficiently with non-Boolean features and multi-features. An example of a non-Boolean feature is a numeric attribute such as maximum number of users which can take different numeric values across the range of SPL products. Multi-features are features that can appear several times in the same product, such as processing units which number is variable from one product to another and which can be configured independently. Both constructs are extensively used in practice but currently not supported by existing SPL model checking techniques. To overcome this limitation, we formally define a language that integrates these constructs with SPL behavioural specifications. We generalize SPL model checking algorithms correspondingly and evaluate their applicability. Our results show that the algorithms remain efficient despite the generalization.
[Context, software product lines, Boolean product-line model checking, Frequency modulation, SPL products, program verification, Tools, maximum user number, SPL model checking techniques, Indexes, formal specification, nonBoolean multifeature Attributes, Numeric Features, Software Product Lines, Feature Cardinalities, Model Checking, Semantics, product development, Model checking, Syntactics, software reusability, numeric attribute, Reliability, SPL behavioural specifications]
Strategies for product-line verification: Case studies and experiments
2013 35th International Conference on Software Engineering
None
2013
Product-line technology is increasingly used in mission-critical and safety-critical applications. Hence, researchers are developing verification approaches that follow different strategies to cope with the specific properties of product lines. While the research community is discussing the mutual strengths and weaknesses of the different strategies - mostly at a conceptual level - there is a lack of evidence in terms of case studies, tool implementations, and experiments. We have collected and prepared six product lines as subject systems for experimentation. Furthermore, we have developed a model-checking tool chain for C-based and Java-based product lines, called SPLverifier, which we use to compare sample-based and family-based strategies with regard to verification performance and the ability to find defects. Based on the experimental results and an analytical model, we revisit the discussion of the strengths and weaknesses of product-line-verification strategies.
[C-based product line, Java, safety-critical application, program verification, software development management, safety-critical software, Data structures, Encoding, Electronic mail, Java-based product line, C language, SPLverifier, Analytical models, software product line, product development, software reusability, Feature extraction, product line verification, Cryptography, mission-critical application, model checking tool chain]
On the value of user preferences in search-based software engineering: A case study in software product lines
2013 35th International Conference on Software Engineering
None
2013
Software design is a process of trading off competing objectives. If the user objective space is rich, then we should use optimizers that can fully exploit that richness. For example, this study configures software product lines (expressed as feature maps) using various search-based software engineering methods. As we increase the number of optimization objectives, we find that methods in widespread use (e.g. NSGA-II, SPEA2) perform much worse than IBEA (Indicator-Based Evolutionary Algorithm). IBEA works best since it makes most use of user preference knowledge. Hence it does better on the standard measures (hypervolume and spread) but it also generates far more products with 0% violations of domain constraints. Our conclusion is that we need to change our methods for search-based software engineering, particularly when studying complex decision spaces.
[software product lines, NSGA-II, indicator-based evolutionary algorithm, Search-Based Software Engineering, Software algorithms, software design, Evolutionary computation, user preferences, Optimal Feature Selection, Mobile handsets, Feature Models, Multiobjective Optimization, Indicator-Based Evolutionary Algorithm, Optimization, SPEA2, Software Product Lines, evolutionary computation, Sociology, decision spaces, search-based software engineering methods, IBEA, Software, software engineering, Software engineering]
Lase: Locating and applying systematic edits by learning from examples
2013 35th International Conference on Software Engineering
None
2013
Adding features and fixing bugs often require systematic edits that make similar, but not identical, changes to many code locations. Finding all the relevant locations and making the correct edits is a tedious and error-prone process for developers. This paper addresses both problems using edit scripts learned from multiple examples. We design and implement a tool called LASE that (1) creates a context-aware edit script from two or more examples, and uses the script to (2) automatically identify edit locations and to (3) transform the code. We evaluate LASE on an oracle test suite of systematic edits from Eclipse JDT and SWT. LASE finds edit locations with 99% precision and 89% recall, and transforms them with 91% accuracy. We also evaluate LASE on 37 example systematic edits from other open source programs and find LASE is accurate and effective. Furthermore, we confirmed with developers that LASE found edit locations which they missed. Our novel algorithm that learns from multiple examples is critical to achieving high precision and recall; edit scripts created from only one example produce too many false positives, false negatives, or both. Our results indicate that LASE should help developers in automating systematic editing. Whereas most prior work either suggests edit locations or performs simple edits, LASE is the first to do both for nontrivial program edits.
[Context, nontrivial program edit, program debugging, code transformation, program testing, text editing, public domain software, LASE, Cloning, Transforms, bug fixing, false negatives, false positives, SWT, locating and applying systematic edit, Systematics, oracle test suite, Computer bugs, Abstracts, Concrete, open source programs, Eclipse JDT, context-aware edit script, automatic edit location identification]
Search-based genetic optimization for deployment and reconfiguration of software in the cloud
2013 35th International Conference on Software Engineering
None
2013
Migrating existing enterprise software to cloud platforms involves the comparison of competing cloud deployment options (CDOs). A CDO comprises a combination of a specific cloud environment, deployment architecture, and runtime reconfiguration rules for dynamic resource scaling. Our simulator CDOSim can evaluate CDOs, e.g., regarding response times and costs. However, the design space to be searched for well-suited solutions is extremely huge. In this paper, we approach this optimization problem with the novel genetic algorithm CDOXplorer. It uses techniques of the search-based software engineering field and CDOSim to assess the fitness of CDOs. An experimental evaluation that employs, among others, the cloud environments Amazon EC2 and Microsoft Windows Azure, shows that CDOXplorer can find solutions that surpass those of other state-of-the-art techniques by up to 60%. Our experiment code and data and an implementation of CDOXplorer are available as open source software.
[enterprise software, optimization problem, Cloud computing, Amazon EC2, public domain software, cloud deployment options, Biological cells, Optimization, Genetic algorithms, Search-based software engineering, genetic algorithm, software architecture, Sociology, cloud environment, CDOXplorer, cloud computing, software reconfiguration, search-based genetic optimization, search-based software engineering field, open source software, software deployment, cloud platforms, genetic algorithms, dynamic resource scaling, deployment architecture, Microsoft Windows Azure, runtime reconfiguration rules, Nickel, Software, Time factors, Deployment optimization, CDOSim simulator]
How to effectively use topic models for software engineering tasks? An approach based on Genetic Algorithms
2013 35th International Conference on Software Engineering
None
2013
Information Retrieval (IR) methods, and in particular topic models, have recently been used to support essential software engineering (SE) tasks, by enabling software textual retrieval and analysis. In all these approaches, topic models have been used on software artifacts in a similar manner as they were used on natural language documents (e.g., using the same settings and parameters) because the underlying assumption was that source code and natural language documents are similar. However, applying topic models on software data using the same settings as for natural language text did not always produce the expected results. Recent research investigated this assumption and showed that source code is much more repetitive and predictable as compared to the natural language text. Our paper builds on this new fundamental finding and proposes a novel solution to adapt, configure and effectively use a topic modeling technique, namely Latent Dirichlet Allocation (LDA), to achieve better (acceptable) performance across various SE tasks. Our paper introduces a novel solution called LDA-GA, which uses Genetic Algorithms (GA) to determine a near-optimal configuration for LDA in the context of three different SE tasks: (1) traceability link recovery, (2) feature location, and (3) software artifact labeling. The results of our empirical studies demonstrate that LDA-GA is able to identify robust LDA configurations, which lead to a higher accuracy on all the datasets for these SE tasks as compared to previously published results, heuristics, and the results of a combinatorial search.
[latent Dirichlet allocation, text analysis, Textual Analysis in Software Engineering, software artifact labeling, topic modeling technique, IR methods, Genetic algorithms, Latent Dirichlet Allocation, Accuracy, LDA-GA, software engineering, Labeling, Context, natural language processing, natural language documents, Natural languages, information retrieval methods, Genetic Algoritms, information retrieval, source code, feature location, genetic algorithms, natural language text, LDA configurations, near-optimal configuration, traceability link recovery, software textual retrieval and analysis, Software, software engineering tasks, SE tasks, Software engineering, software data]
Green Streams for data-intensive software
2013 35th International Conference on Software Engineering
None
2013
This paper introduces Green Streams, a novel solution to address a critical but often overlooked property of data-intensive software: energy efficiency. Green Streams is built around two key insights into data-intensive software. First, energy consumption of data-intensive software is strongly correlated to data volume and data processing, both of which are naturally abstracted in the stream programming paradigm; Second, energy efficiency can be improved if the data processing components of a stream program coordinate in a &#x201C;balanced&#x201D; way, much like an assembly line that runs most efficiently when participating workers coordinate their pace. Green Streams adopts a standard stream programming model, and applies Dynamic Voltage and Frequency Scaling (DVFS) to coordinate the pace of data processing among components, ultimately achieving energy efficiency without degrading performance in a parallel processing environment. At the core of Green Streams is a novel constraint-based inference to abstract the intrinsic relationships of data flow rates inside a stream program, that uses linear programming to minimize the frequencies - hence the energy consumption - for processing components while still maintaining the maximum output data flow rate. The core algorithm of Green Streams is formalized, and its optimality is established. The effectiveness of Green Streams is evaluated on top of the StreamIt framework, and preliminary results show the approach can save CPU energy by an average of 28% with a 7% performance improvement.
[Energy consumption, stream programming model, Programming, Linear programming, Data processing, linear programming, inference mechanisms, parallel programming, power aware computing, Green products, Abstracts, dynamic voltage and frequency scaling, energy efficiency, Green Streams, data processing components, Software, software engineering, data-intensive software, green computing, parallel processing environment, constraint-based inference, energy consumption, DVFS]
Dynamic synthesis of local time requirement for service composition
2013 35th International Conference on Software Engineering
None
2013
Service composition makes use of existing service-based applications as components to achieve a business goal. In time critical business environments, the response time of a service is crucial, which is also reflected as a clause in service level agreements (SLAs) between service providers and service users. To allow the composite service to fulfill the response time requirement as promised, it is important to find a feasible set of component services, such that their response time could collectively allow the satisfaction of the response time of the composite service. In this work, we propose a fully automated approach to synthesize the response time requirement of component services, in the form of a constraint on the local response times, that guarantees the global response time requirement. Our approach is based on parameter synthesis techniques for real-time systems. It has been implemented and evaluated with real-world case studies.
[service level agreements, global response time requirement, service response time, dynamic synthesis, Indexes, component service, Cost accounting, Reactive power, time critical business environments, parameter synthesis techniques, Web services, service-based application, real-time systems, Abstracts, Concrete, local time requirement, Time factors, service composition, local response times, business data processing, Business Process Execution Language, Clocks]
Supporting swift reaction: Automatically uncovering performance problems by systematic experiments
2013 35th International Conference on Software Engineering
None
2013
Performance problems pose a significant risk to software vendors. If left undetected, they can lead to lost customers, increased operational costs, and damaged reputation. Despite all efforts, software engineers cannot fully prevent performance problems being introduced into an application. Detecting and resolving such problems as early as possible with minimal effort is still an open challenge in software performance engineering. In this paper, we present a novel approach for Performance Problem Diagnostics (PPD) that systematically searches for well-known performance problems (also called performance antipatterns) within an application. PPD automatically isolates the problem's root cause, hence facilitating problem solving. We applied PPD to a well established transactional web e-Commerce benchmark (TPC-W) in two deployment scenarios. PPD automatically identified four performance problems in the benchmark implementation and its deployment environment. By fixing the problems, we increased the maximum throughput of the benchmark from 1800 requests per second to more than 3500.
[Measurement, swift reaction support, software vendors, software engineers, program diagnostics, PPD, problem detection, Search problems, performance problem diagnostics, operational costs, measurement, systematic experiments, Accuracy, Systematics, performance, TPC-W, damaged reputation, Benchmark testing, transactional Web e-Commerce benchmark, Software, Internet, Time factors, software performance evaluation, electronic commerce]
Toddler: Detecting performance problems via similar memory-access patterns
2013 35th International Conference on Software Engineering
None
2013
Performance bugs are programming errors that create significant performance degradation. While developers often use automated oracles for detecting functional bugs, detecting performance bugs usually requires time-consuming, manual analysis of execution profiles. The human effort for performance analysis limits the number of performance tests analyzed and enables performance bugs to easily escape to production. Unfortunately, while profilers can successfully localize slow executing code, profilers cannot be effectively used as automated oracles. This paper presents Toddler, a novel automated oracle for performance bugs, which enables testing for performance bugs to use the well established and automated process of testing for functional bugs. Toddler reports code loops whose computation has repetitive and partially similar memory-access patterns across loop iterations. Such repetitive work is likely unnecessary and can be done faster. We implement Toddler for Java and evaluate it on 9 popular Java codebases. Our experiments with 11 previously known, real-world performance bugs show that Toddler finds these bugs with a higher accuracy than the standard Java profiler. Using Toddler, we also found 42 new bugs in six Java projects: Ant, Google Core Libraries, JUnit, Apache Collections, JDK, and JFreeChart. Based on our bug reports, developers so far fixed 10 bugs and confirmed 6 more as real bugs.
[Pediatrics, program debugging, performance test analysis, program testing, automated functional bug testing process, JFreeChart project, Google Core Libraries project, Ant project, Libraries, code loop iterations, software performance evaluation, performance bug detection, Testing, JUnit project, Java, program control structures, functional bug detection, Instruments, performance problem detection, automated oracle, Data structures, JDK project, performance degradation, Java projects, Computer bugs, Toddler framework, Java codebases, Apache Collections project, programming errors, memory-access patterns]
Departures from optimality: Understanding human analyst's information foraging in assisted requirements tracing
2013 35th International Conference on Software Engineering
None
2013
Studying human analyst's behavior in automated tracing is a new research thrust. Building on a growing body of work in this area, we offer a novel approach to understanding requirements analyst's information seeking and gathering. We model analysts as predators in pursuit of prey - the relevant traceability information, and leverage the optimality models to characterize a rational decision process. The behavior of real analysts with that of the optimal information forager is then compared and contrasted. The results show that the analysts' information diets are much wider than the theory's predictions, and their residing in low-profitability information patches is much longer than the optimal residence time. These uncovered discrepancies not only offer concrete insights into the obstacles faced by analysts, but also lead to principled ways to increase practical tool support for overcoming the obstacles.
[automated tracing, low-profitability information patches, formal specification, optimality models, Traceability, Analytical models, human analyst behavior, formal verification, assisted requirements tracing, traceability information, study of human analysts, information foraging, Navigation, Computational modeling, Buildings, Debugging, information retrieval, rational decision process, human analyst information foraging, optimal residence time, optimal information forager, requirements engineering, Software, behavioural sciences computing, information needs, Software engineering]
Analysis of user comments: An approach for software requirements evolution
2013 35th International Conference on Software Engineering
None
2013
User feedback is imperative in improving software quality. In this paper, we explore the rich set of user feedback available for third party mobile applications as a way to extract new/changed requirements for next versions. A potential problem using this data is its volume and the time commitment involved in extracting new/changed requirements. Our goal is to alleviate part of the process through automatic topic extraction. We process user comments to extract the main topics mentioned as well as some sentences representative of those topics. This information can be useful for requirements engineers to revise the requirements for next releases. Our approach relies on adapting information retrieval techniques including topic modeling and evaluating them on different publicly available data sets. Results show that the automatically extracted topics match the manually extracted ones, while also significantly decreasing the manual effort.
[Adaptation models, sentence representation, requirement engineering, Software Evolution, software requirement evolution, Humanoid robots, Manuals, topic modeling, software quality, Data mining, Requirements, information retrieval technique, Analytical models, mobile computing, User Comments, user comment analysis, information retrieval, third party mobile application, Topic Modeling, automatic topic extraction, Software, user feedback, User feedback, Information Retrieval, Androids]
Requirements modelling by synthesis of deontic input-output automata
2013 35th International Conference on Software Engineering
None
2013
Requirements modelling helps software engineers understand a system's required behaviour and explore alternative system designs. It also generates a formal software specification that can be used for testing, verification, and debugging. However, elaborating such models requires expertise and significant human effort. The paper aims at reducing this effort by automating an essential activity of requirements modelling which consists in deriving a machine specification satisfying a set of goals in a domain. It introduces deontic input-output automata - an extension of input-output automata with permissions and obligations - and an automated synthesis technique over this formalism to support such derivation. This technique helps modellers identifying early when a goal is not realizable in a domain and can guide the exploration of alternative models to make goals realizable. Synthesis techniques for input-output or interface automata are not adequate for requirements modelling.
[Context, deontic input-output automata, Adaptation models, program debugging, software debugging, Tuners, machine specification, program testing, software verification, automated synthesis technique, automata theory, software testing, formal specification, formal verification, Semantics, Automata, formal software specification, software engineering, Transient analysis, requirements modelling]
Automated reliability estimation over partial systematic explorations
2013 35th International Conference on Software Engineering
None
2013
Model-based reliability estimation of software systems can provide useful insights early in the development process. However, computational complexity of estimating reliability metrics such as mean time to first failure (MTTF) can be prohibitive both in time, space and precision. In this paper we present an alternative to exhaustive model exploration-as in probabilistic model checking-and partial random exploration-as in statistical model checking. Our hypothesis is that a (carefully crafted) partial systematic exploration of a system model can provide better bounds for reliability metrics at lower computation cost. We present a novel automated technique for reliability estimation that combines simulation, invariant inference and probabilistic model checking. Simulation produces a probabilistically relevant set of traces from which a state invariant is inferred. The invariant characterises a partial model which is then exhaustively explored using probabilistic model checking. We report on experiments that suggest that reliability estimation using this technique can be more effective than (full model) probabilistic and statistical model checking for system models with rare failures.
[Measurement, probabilistic model checking, failures, Computational modeling, software reliability, Estimation, simulation, Probabilistic logic, system model, inference mechanisms, statistical model checking, reliability metrics, computation cost, invariant inference, formal verification, partial systematic explorations, state invariance, Model checking, Numerical models, Reliability, automated reliability estimation, partial model, computational complexity, software metrics]
Safe software updates via multi-version execution
2013 35th International Conference on Software Engineering
None
2013
Software systems are constantly evolving, with new versions and patches being released on a continuous basis. Unfortunately, software updates present a high risk, with many releases introducing new bugs and security vulnerabilities. We tackle this problem using a simple but effective multi-version based approach. Whenever a new update becomes available, instead of upgrading the software to the new version, we run the new version in parallel with the old one; by carefully coordinating their executions and selecting the behaviour of the more reliable version when they diverge, we create a more secure and dependable multi-version application. We implemented this technique in Mx, a system targeting Linux applications running on multi-core processors, and show that it can be applied successfully to several real applications such as Coreutils, a set of user-level UNIX applications; Lighttpd, a popular web server used by several high-traffic websites such as Wikipedia and YouTube; and Redis, an advanced key-value data structure server used by many well-known services such as GitHub and Flickr.
[Coreutils, program debugging, software reliability, software systems, GitHub, Lighttpd, Wikipedia, software versions, Linux applications, key-value data structure server, high-traffic websites, software updates, dependable multiversion application, Prototypes, surviving software crashes, software upgrading, user-level UNIX applications, multiversion based approach, Web server, Monitoring, security vulnerabilities, multiprocessing systems, software bugs, Redis, MX, YouTube, configuration management, software patches, Flickr, security of data, Linux, Computer bugs, Software, multicore processors, Reliability, multi-version execution, multiversion execution]
Reliability analysis in Symbolic PathFinder
2013 35th International Conference on Software Engineering
None
2013
Software reliability analysis tackles the problem of predicting the failure probability of software. Most of the current approaches base reliability analysis on architectural abstractions useful at early stages of design, but not directly applicable to source code. In this paper we propose a general methodology that exploit symbolic execution of source code for extracting failure and success paths to be used for probabilistic reliability assessment against relevant usage scenarios. Under the assumption of finite and countable input domains, we provide an efficient implementation based on Symbolic PathFinder that supports the analysis of sequential and parallel programs, even with structured data types, at the desired level of confidence. The tool has been validated on both NASA prototypes and other test cases showing a promising applicability scope.
[Actuators, Schedules, Java, structured data types, Instruction sets, software reliability, probability, source code, Software reliability, parallel programming, software architecture, software failure probability, probabilistic reliability assessment, symbolic execution, symbolic PathFinder, architectural abstractions, sequential program, parallel program, NASA prototypes, software reliability analysis]
Engineering adaptive privacy: On the role of privacy awareness requirements
2013 35th International Conference on Software Engineering
None
2013
Applications that continuously gather and disclose personal information about users are increasingly common. While disclosing this information may be essential for these applications to function, it may also raise privacy concerns. Partly, this is due to frequently changing context that introduces new privacy threats, and makes it difficult to continuously satisfy privacy requirements. To address this problem, applications may need to adapt in order to manage changing privacy concerns. Thus, we propose a framework that exploits the notion of privacy awareness requirements to identify runtime privacy properties to satisfy. These properties are used to support disclosure decision making by applications. Our evaluations suggest that applications that fail to satisfy privacy awareness requirements cannot regulate users' information disclosure. We also observe that the satisfaction of privacy awareness requirements is useful to users aiming to minimise exposure to privacy threats, and to users aiming to maximise functional benefits amidst increasing threat severity.
[Context, Adaptation models, privacy awareness requirement satisfaction, Receivers, utility, adaptation, History, functional benefit maximisation, personal information disclosure, disclosure decision making, Privacy, runtime privacy property identification, user personal information gathering, engineering adaptive privacy threats, selective disclosure, data privacy, Monitoring, Context modeling]
Mining SQL injection and cross site scripting vulnerabilities using hybrid program analysis
2013 35th International Conference on Software Engineering
None
2013
In previous work, we proposed a set of static attributes that characterize input validation and input sanitization code patterns. We showed that some of the proposed static attributes are significant predictors of SQL injection and cross site scripting vulnerabilities. Static attributes have the advantage of reflecting general properties of a program. Yet, dynamic attributes collected from execution traces may reflect more specific code characteristics that are complementary to static attributes. Hence, to improve our initial work, in this paper, we propose the use of dynamic attributes to complement static attributes in vulnerability prediction. Furthermore, since existing work relies on supervised learning, it is dependent on the availability of training data labeled with known vulnerabilities. This paper presents prediction models that are based on both classification and clustering in order to predict vulnerabilities, working in the presence or absence of labeled training data, respectively. In our experiments across six applications, our new supervised vulnerability predictors based on hybrid (static and dynamic) attributes achieved, on average, 90% recall and 85% precision, that is a sharp increase in recall when compared to static analysis-based predictions. Though not nearly as accurate, our unsupervised predictors based on clustering achieved, on average, 76% recall and 39% precision, thus suggesting they can be useful in the absence of labeled training data.
[data mining, Predictive models, privacy, HTML, Security, Data mining, security, Databases, Training data, hybrid program analysis, SQL injection mining, cross site scripting vulnerabilities, pattern classification, dynamic attributes, program diagnostics, supervised vulnerability predictors, hybrid attributes, empirical study, vulnerability, classification, Defect prediction, SQL, static analysis-based predictions, prediction models, security of data, static and dynamic analysis, pattern clustering, input validation and sanitization, Supervised learning, static attributes, data privacy, clustering]
Path sensitive static analysis of web applications for remote code execution vulnerability detection
2013 35th International Conference on Software Engineering
None
2013
Remote code execution (RCE) attacks are one of the most prominent security threats for web applications. It is a special kind of cross-site-scripting (XSS) attack that allows client inputs to be stored and executed as server side scripts. RCE attacks often require coordination of multiple requests and manipulation of string and non-string inputs from the client side to nullify the access control protocol and induce unusual execution paths on the server side. We propose a path- and context-sensitive interprocedural analysis to detect RCE vulnerabilities. The analysis features a novel way of analyzing both the string and non-string behavior of a web application in a path sensitive fashion. It thoroughly handles the practical challenges entailed by modeling RCE attacks. We develop a prototype system and evaluate it on ten real-world PHP applications. We have identified 21 true RCE vulnerabilities, with 8 unreported before.
[Access control, Context, program diagnostics, path sensitive static analysis, XSS attack, RCE attack modeling, Cognition, path-sensitive interprocedural analysis, remote code execution attacks, RCE vulnerabilities detection, Servers, cross-site-scripting attack, remote code execution vulnerability detection, Standards, server side scripts, security threats, PHP application, Semantics, Web application, access control protocol, authorisation, Internet, context-sensitive interprocedural analysis]
Automated software architecture security risk analysis using formalized signatures
2013 35th International Conference on Software Engineering
None
2013
Reviewing software system architecture to pinpoint potential security flaws before proceeding with system development is a critical milestone in secure software development lifecycles. This includes identifying possible attacks or threat scenarios that target the system and may result in breaching of system security. Additionally we may also assess the strength of the system and its security architecture using well-known security metrics such as system attack surface, Compartmentalization, least-privilege, etc. However, existing efforts are limited to specific, predefined security properties or scenarios that are checked either manually or using limited toolsets. We introduce a new approach to support architecture security analysis using security scenarios and metrics. Our approach is based on formalizing attack scenarios and security metrics signature specification using the Object Constraint Language (OCL). Using formal signatures we analyse a target system to locate signature matches (for attack scenarios), or to take measurements (for security metrics). New scenarios and metrics can be incorporated and calculated provided that a formal signature can be specified. Our approach supports defining security metrics and scenarios at architecture, design, and code levels. We have developed a prototype software system architecture security analysis tool. To the best of our knowledge this is the first extensible architecture security risk analysis tool that supports both metric-based and scenario-based architecture security analysis. We have validated our approach by using it to capture and evaluate signatures from the NIST security principals and attack scenarios defined in the CAPEC database.
[Measurement, security flaws, Architecture Security Risk analysis, Unified modeling language, Formal attack patterns specification, scenario-based architecture security analysis, Security, system security, security metrics signature specification, software architecture, signature matches, Software architecture, metric-based architecture security analysis, Computer architecture, system development, Software security, NIST security principals, automated software architecture security risk analysis, Risk analysis, CAPEC database, secure software development lifecycles, object constraint language, formalized signatures, Common attack patterns enumeration and classification (CAPEC), security architecture, object-oriented languages, formal signatures, OCL, Software, digital signatures, software metrics]
Why don't software developers use static analysis tools to find bugs?
2013 35th International Conference on Software Engineering
None
2013
Using static analysis tools for automating code inspections can be beneficial for software engineers. Such tools can make finding bugs, or software defects, faster and cheaper than manual inspections. Despite the benefits of using static analysis tools to find bugs, research suggests that these tools are underused. In this paper, we investigate why developers are not widely using static analysis tools and how current tools could potentially be improved. We conducted interviews with 20 developers and found that although all of our participants felt that use is beneficial, false positives and the way in which the warnings are presented, among other things, are barriers to use. We discuss several implications of these results, such as the need for an interactive mechanism to help developers fix defects.
[program debugging, interactive mechanism, software development, program diagnostics, Companies, Encoding, software quality, bug detection, Standards, false positives, static analysis tools, Computer bugs, interactive systems, software defects, Software, software engineering, Teamwork, Interviews, automatic code inspection]
Exploring the impact of inter-smell relations on software maintainability: An empirical study
2013 35th International Conference on Software Engineering
None
2013
Code smells are indicators of issues with source code quality that may hinder evolution. While previous studies mainly focused on the effects of individual code smells on maintainability, we conjecture that not only the individual code smells but also the interactions between code smells affect maintenance. We empirically investigate the interactions amongst 12 code smells and analyze how those interactions relate to maintenance problems. Professional developers were hired for a period of four weeks to implement change requests on four medium-sized Java systems with known smells. On a daily basis, we recorded what specific problems they faced and which artifacts were associated with them. Code smells were automatically detected in the pre-maintenance versions of the systems and analyzed using Principal Component Analysis (PCA) to identify patterns of co-located code smells. Analysis of these factors with the observed maintenance problems revealed how smells that were co-located in the same artifact interacted with each other, and affected maintainability. Moreover, we found that code smell interactions occurred across coupled artifacts, with comparable negative effects as same-artifact co-location. We argue that future studies into the effects of code smells on maintainability should integrate dependency analysis in their process so that they can obtain a more complete understanding by including such coupled interactions.
[Context, Java, dependency analysis, medium-sized Java systems, Laboratories, intersmell relations impact, Maintenance engineering, software maintainability, software quality, software maintenance, PCA, smell interaction, source code quality, code smells, bad smells, inter-smell relations, Software, Interviews, principal component analysis, Principal component analysis]
An empirical study on the developers' perception of software coupling
2013 35th International Conference on Software Engineering
None
2013
Coupling is a fundamental property of software systems, and numerous coupling measures have been proposed to support various development and maintenance activities. However, little is known about how developers actually perceive coupling, what mechanisms constitute coupling, and if existing measures align with this perception. In this paper we bridge this gap, by empirically investigating how class coupling - as captured by structural, dynamic, semantic, and logical coupling measures - aligns with developers' perception of coupling. The study has been conducted on three Java open-source systems - namely ArgoUML, JHotDraw and jEdit - and involved 64 students, academics, and industrial practitioners from around the world, as well as 12 active developers of these three systems. We asked participants to assess the coupling between the given pairs of classes and provide their ratings and some rationale. The results indicate that the peculiarity of the semantic coupling measure allows it to better estimate the mental model of developers than the other coupling measures. This is because, in several cases, the interactions between classes are encapsulated in the source code vocabulary, and cannot be easily derived by only looking at structural relationships, such as method calls.
[Java, jEdit, Unified Modeling Language, developers perception, JHotDraw, public domain software, Java open-source systems, software systems, ArgoUML, software coupling, semantic coupling measures, Software Coupling, Empirical Studies, source code vocabulary, Couplings, structural coupling measures, Iterative closest point algorithm, Semantics, dynamic coupling measures, Software, software engineering, logical coupling measures, Software measurement]
X-PERT: Accurate identification of cross-browser issues in web applications
2013 35th International Conference on Software Engineering
None
2013
Due to the increasing popularity of web applications, and the number of browsers and platforms on which such applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious concern for organizations that develop web-based software. Most of the techniques for XBI detection developed to date are either manual, and thus costly and error-prone, or partial and imprecise, and thus prone to generating both false positives and false negatives. To address these limitations of existing techniques, we developed X-PERT, a new automated, precise, and comprehensive approach for XBI detection. X-PERT combines several new and existing differencing techniques and is based on our findings from an extensive study of XBIs in real-world web applications. The key strength of our approach is that it handles each aspects of a web application using the differencing technique that is best suited to accurately detect XBIs related to that aspect. Our empirical evaluation shows that X-PERT is effective in detecting real-world XBIs, improves on the state of the art, and can provide useful support to developers for the diagnosis and (eventually) elimination of XBIs.
[Visualization, Navigation, cross-browser issue identification, X-PERT, cross-browser incompatibilities, XBI detection, differencing technique, Browsers, browsers, Web-based software development, Layout, Web pages, Web application, online front-ends, Feature extraction, software engineering, Internet]
Expectations, outcomes, and challenges of modern code review
2013 35th International Conference on Software Engineering
None
2013
Code review is a common software engineering practice employed both in open source and industrial contexts. Review today is less formal and more &#x201C;lightweight&#x201D; than the code inspections performed and studied in the 70s and 80s. We empirically explore the motivations, challenges, and outcomes of tool-based code reviews. We observed, interviewed, and surveyed developers and managers and manually classified hundreds of review comments across diverse teams at Microsoft. Our study reveals that while finding defects remains the main motivation for review, reviews are less about defects than expected and instead provide additional benefits such as knowledge transfer, increased team awareness, and creation of alternative solutions to problems. Moreover, we find that code and change understanding is the key aspect of code reviewing and that developers employ a wide range of mechanisms to meet their understanding needs, most of which are not met by current tools. We provide recommendations for practitioners and researchers.
[Context, team awareness, software development management, Inspection, software engineering practice, software quality, Knowledge transfer, Sorting, Guidelines, open source, knowledge transfer, industrial contexts, Software, Microsoft, Interviews, tool-based code reviews]
UML in practice
2013 35th International Conference on Software Engineering
None
2013
UML has been described by some as &#x201C;the lingua franca of software engineering&#x201D;. Evidence from industry does not necessarily support such endorsements. How exactly is UML being used in industry - if it is? This paper presents a corpus of interviews with 50 professional software engineers in 50 companies and identifies 5 patterns of UML use.
[Context, Industries, professional software engineers, Unified Modeling Language, software development, lingua franca of software engineering, Unified modeling language, software design, Companies, empirical studies, Analytical models, UML, notation, Software, software engineering, Interviews]
Cassandra: Proactive conflict minimization through optimized task scheduling
2013 35th International Conference on Software Engineering
None
2013
Software conflicts arising because of conflicting changes are a regular occurrence and delay projects. The main precept of workspace awareness tools has been to identify potential conflicts early, while changes are still small and easier to resolve. However, in this approach conflicts still occur and require developer time and effort to resolve. We present a novel conflict minimization technique that proactively identifies potential conflicts, encodes them as constraints, and solves the constraint space to recommend a set of conflict-minimal development paths for the team. Here we present a study of four open source projects to characterize the distribution of conflicts and their resolution efforts. We then explain our conflict minimization technique and the design and implementation of this technique in our prototype, Cassandra. We show that Cassandra would have successfully avoided a majority of conflicts in the four open source test subjects. We demonstrate the efficiency of our approach by applying the technique to a simulated set of scenarios with higher than normal incidence of conflicts.
[Context, workspace awareness tools, open source projects, open source test subjects, Shape, Cassandra, Collaborative development, collaboration conflicts, Minimization, Control systems, optimized task scheduling, Iron, constraint space, task analysis, coordination, conflict minimal development paths, Syntactics, scheduling, Software, task scheduling, minimisation, proactive conflict minimization, software conflicts]
Are your incoming aliases really necessary? Counting the cost of object ownership
2013 35th International Conference on Software Engineering
None
2013
Object ownership enforces encapsulation within object-oriented programs by forbidding incoming aliases into objects' representations. Many common data structures, such as collections with iterators, require incoming aliases, so there has been much work on relaxing ownership's encapsulation to permit multiple incoming aliases. This research asks the opposite question: Are your aliases really necessary? In this paper, we count the cost of programming with strong object encapsulation. We refactored the JDK 5.0 collection classes so that they did not use incoming aliases, following either the owner-as-dominator or the owner-as-accessor encapsulation discipline. We measured the performance time overhead the refactored collections impose on a set of microbenchmarks and on the DaCapo, SPECjbb and SPECjvm benchmark suites. While the microbenchmarks show that individual operations and iterations can be significantly slower on encapsulated collection (especially for owner-as-dominator), we found less than 3% slowdown for owner-as-accessor across the large scale benchmarks. As a result, we propose that well-known design patterns such as Iterator commonly used by software engineers around the world need to be adjusted to take ownership into account. As most design patterns are used as a building block in constructing larger pieces of software, a small adjustment to respect ownership will not have any impact on the productivity of programmers but will have a huge impact on the quality of the resulting code with respect to aliasing.
[Encapsulation, object encapsulation, object representations, SPECjbb benchmark suites, JDK 5.0 collection classes, design patterns, object oriented programs, Benchmark testing, Iterator, data structures, Java, software engineers, object-oriented programming, owner-as-accessor, programming cost, Vectors, ownership encapsulation, object ownership, Standards, SPECjvm benchmark suites, security of data, DaCapo benchmark suites, Software, microbenchmarks, Arrays, data encapsulation]
Efficient construction of approximate call graphs for JavaScript IDE services
2013 35th International Conference on Software Engineering
None
2013
The rapid rise of JavaScript as one of the most popular programming languages of the present day has led to a demand for sophisticated IDE support similar to what is available for Java or C#. However, advanced tooling is hampered by the dynamic nature of the language, which makes any form of static analysis very difficult. We single out efficient call graph construction as a key problem to be solved in order to improve development tools for JavaScript. To address this problem, we present a scalable field-based flow analysis for constructing call graphs. Our evaluation on large real-world programs shows that the analysis, while in principle unsound, produces highly accurate call graphs in practice. Previous analyses do not scale to these programs, but our analysis handles them in a matter of seconds, thus proving its suitability for use in an interactive setting.
[Algorithm design and analysis, Java, program diagnostics, Buildings, development tools, Programming, data flow graphs, static analysis, HTML, C language, JavaScript IDE services, scalable field-based flow analysis, Reactive power, approximate call graph construction, Abstracts, Java language, programming language, IDE support, programming environments, C# language]
Improving feature location practice with multi-faceted interactive exploration
2013 35th International Conference on Software Engineering
None
2013
Feature location is a human-oriented and information-intensive process. When performing feature location tasks with existing tools, developers often feel it difficult to formulate an accurate feature query (e.g., keywords) and determine the relevance of returned results. In this paper, we propose a feature location approach that supports multi-faceted interactive program exploration. Our approach automatically extracts and mines multiple syntactic and semantic facets from candidate program elements. Furthermore, it allows developers to interactively group, sort, and filter feature location results in a centralized, multi-faceted, and intelligent search User Interface (UI). We have implemented our approach as a web-based tool MFIE and conducted an experimental study. The results show that the developers using MFIE can accomplish their feature location tasks 32% faster and the quality of their feature location results (in terms of F-measure) is 51% higher than that of the developers using regular Eclipse IDE.
[Java, semantic facets, information-intensive process, Navigation, intelligent search user interface, multifaceted interactive exploration, multifaceted user interface, Educational institutions, reverse engineering, human-oriented process, feature location, user interfaces, History, software maintenance, syntactic facets, Semantics, Syntactics, feature query, Feature extraction, centralized user interface, Web-based tool MFIE]
SemFix: Program repair via semantic analysis
2013 35th International Conference on Software Engineering
None
2013
Debugging consumes significant time and effort in any major software development project. Moreover, even after the root cause of a bug is identified, fixing the bug is non-trivial. Given this situation, automated program repair methods are of value. In this paper, we present an automated repair method based on symbolic execution, constraint solving and program synthesis. In our approach, the requirement on the repaired code to pass a given set of tests is formulated as a constraint. Such a constraint is then solved by iterating over a layered space of repair expressions, layered by the complexity of the repair code. We compare our method with recently proposed genetic programming based repair on SIR programs with seeded bugs, as well as fragments of GNU Coreutils with real bugs. On these subjects, our approach reports a higher success-rate than genetic programming based repair, and produces a repair faster.
[program debugging, constraint solving, program testing, Input variables, Semantics, Genetic programming, debugging, seeded bugs, software engineering, genetic programming based repair, repair expressions, repair code complexity, GNU Coreutils, Maintenance engineering, software development project, program synthesis, Educational institutions, genetic algorithms, SIR programs, automated program repair methods, Computer bugs, Syntactics, semantic analysis, symbolic execution, SemFix]
Automatic recovery from runtime failures
2013 35th International Conference on Software Engineering
None
2013
We present a technique to make applications resilient to failures. This technique is intended to maintain a faulty application functional in the field while the developers work on permanent and radical fixes. We target field failures in applications built on reusable components. In particular, the technique exploits the intrinsic redundancy of those components by identifying workarounds consisting of alternative uses of the faulty components that avoid the failure. The technique is currently implemented for Java applications but makes little or no assumptions about the nature of the application, and works without interrupting the execution flow of the application and without restarting its components. We demonstrate and evaluate this technique on four mid-size applications and two popular libraries of reusable components affected by real and seeded faults. In these cases the technique is effective, maintaining the application fully functional with between 19% and 48% of the failure-causing faults, depending on the application. The experiments also show that the technique incurs an acceptable runtime overhead in all cases.
[Encapsulation, Java, workaround identification, object-oriented programming, Redundancy, runtime failures, reusable component libraries, software maintenance, system recovery, software libraries, Runtime, automatic recovery, software reusability, intrinsic redundancy, Libraries, Software, field failures, faulty application maintenance]
Program transformations to fix C integers
2013 35th International Conference on Software Engineering
None
2013
C makes it easy to misuse integer types; even mature programs harbor many badly-written integer code. Traditional approaches at best detect these problems; they cannot guide developers to write correct code. We describe three program transformations that fix integer problems - one explicitly introduces casts to disambiguate type mismatch, another adds runtime checks to arithmetic operations, and the third one changes the type of a wrongly-declared integer. Together, these transformations fixed all variants of integer problems featured in 7,147 programs of NIST's SAMATE reference dataset, making the changes automatically on over 15 million lines of code. We also applied the transformations automatically on 5 open source software. The transformations made hundreds of changes on over 700,000 lines of code, but did not break the programs. Being integrated with source code and development process, these program transformations can fix integer problems, along with developers' misconceptions about integer usage.
[Context, integer programming, fix C integers, source code, Integer Problem, integer usage, program transformations, Program Transformation, Guidelines, Runtime, Semantics, Computer bugs, digital arithmetic, arithmetic operations, integer problems, Libraries, Arrays, programming, integer types, integer code]
Automatic patch generation learned from human-written patches
2013 35th International Conference on Software Engineering
None
2013
Patch generation is an essential software maintenance task because most software systems inevitably have bugs that need to be fixed. Unfortunately, human resources are often insufficient to fix all reported and known bugs. To address this issue, several automated patch generation techniques have been proposed. In particular, a genetic-programming-based patch generation technique, GenProg, proposed by Weimer et al., has shown promising results. However, these techniques can generate nonsensical patches due to the randomness of their mutation operations. To address this limitation, we propose a novel patch generation approach, Pattern-based Automatic program Repair (Par), using fix patterns learned from existing human-written patches. We manually inspected more than 60,000 human-written patches and found there are several common fix patterns. Our approach leverages these fix patterns to generate program patches automatically. We experimentally evaluated Par on 119 real bugs. In addition, a user study involving 89 students and 164 developers confirmed that patches generated by our approach are more acceptable than those generated by GenProg. Par successfully generated patches for 27 out of 119 bugs, while GenProg was successful for only 16 bugs.
[Context, GenProg, pattern based automatic program repair, human written patches, software systems, genetic programming based patch generation, automated patch generation, Manuals, Fault location, Maintenance engineering, genetic algorithms, user interfaces, human resources, software maintenance, software maintenance task, Computer bugs, Semantics, automatic patch generation, Arrays]
Reverb: Recommending code-related web pages
2013 35th International Conference on Software Engineering
None
2013
The web is an important source of development-related resources, such as code examples, tutorials, and API documentation. Yet existing development environments are largely disconnected from these resources. In this work, we explore how to provide useful web page recommendations to developers by focusing on the problem of refinding web pages that a developer has previously used. We present the results of a study about developer browsing activity in which we found that 13.7% of developers visits to code-related pages are revisits and that only a small fraction (7.4%) of these were initiated through a low-cost mechanism, such as a bookmark. To assist with code-related revisits, we introduce Reverb, a tool which recommends previously visited web pages that pertain to the code visible in the developer's editor. Through a field study, we found that, on average, Reverb can recommend a useful web page in 51% of revisitation cases.
[Google, Reverb, code-related revisits, software development, Browsers, History, bookmark, recommender systems, Web pages, Software, software engineering, Web page recommendations, software tools, Web sites, developer browsing activity, code-related pages, Indexing]
Dynamic injection of sketching features into GEF based diagram editors
2013 35th International Conference on Software Engineering
None
2013
Software Engineering in general is a very creative process, especially in the early stages of development like requirements engineering or architectural design where sketching techniques are used to manifest ideas and share thoughts. On the one hand, a lot of diagram tools with sophisticated editing features exist, aiming to support the engineers for this task. On the other hand, research has shown that most formal tools limit designer's creativity by restricting input to valid data. This raises the need for combining the flexibility of sketch-based input with the power of formal tools. With an increasing amount of available touch-enabled input devices, plenty of tools supporting these and similar features were created but either they require the developer to use a special diagram editor generation framework or have very limited extension capabilities. In this paper we propose Scribble: A generic, extensible framework which brings sketching functionality to any new or existing GEF based diagram editor in the Eclipse ecosystem. Sketch features can be dynamically injected and used without writing a single line of code. We designed Scribble to be open for new shape recognition algorithms and to provide a great degree of user control. We successfully tested Scribble in three diagram tools, each having a different level of complexity.
[Eclipse ecosystem, dynamic sketching feature injection, Shape, modeling, graphical user interfaces, Switches, formal tools, shape recognition algorithms, recognition, diagram tools, formal specification, extension capabilities, Training, Training data, touch-enabled input devices, shape recognition, software engineering, software tools, touch sensitive screens, Contracts, Context, architectural design, Scribble, GEF based diagram editors, complexity level, sophisticated editing features, Sketching, diagram editor generation framework, requirements engineering, user control, graphical editor, Software engineering]
Discovering essential code elements in informal documentation
2013 35th International Conference on Software Engineering
None
2013
To access the knowledge contained in developer communication, such as forum posts, it is useful to determine automatically the code elements referred to in the discussions. We propose a novel traceability recovery approach to extract the code elements contained in various documents. As opposed to previous work, our approach does not require an index of code elements to find links, which makes it particularly well-suited for the analysis of informal documentation. When evaluated on 188 StackOverflow answer posts containing 993 code elements, the technique performs with average 0.92 precision and 0.90 recall. As a major refinement on traditional traceability approaches, we also propose to detect which of the code elements in a document are salient, or germane, to the topic of the post. To this end we developed a three-feature decision tree classifier that performs with a precision of 0.65-0.74 and recall of 0.30-0.65, depending on the subject of the document.
[Context, Java, Documentation, informal documentation, essential code elements, Information retrieval, StackOverflow answer posts, Indexes, Compounds, system recovery, traceability recovery approach, three-feature decision tree classifier, decision trees, Benchmark testing, software engineering]
Automatic query reformulations for text retrieval in software engineering
2013 35th International Conference on Software Engineering
None
2013
There are more than twenty distinct software engineering tasks addressed with text retrieval (TR) techniques, such as, traceability link recovery, feature location, refactoring, reuse, etc. A common issue with all TR applications is that the results of the retrieval depend largely on the quality of the query. When a query performs poorly, it has to be reformulated and this is a difficult task for someone who had trouble writing a good query in the first place. We propose a recommender (called Refoqus) based on machine learning, which is trained with a sample of queries and relevant results. Then, for a given query, it automatically recommends a reformulation strategy that should improve its performance, based on the properties of the query. We evaluated Refoqus empirically against four baseline approaches that are used in natural language document retrieval. The data used for the evaluation corresponds to changes from five open source systems in Java and C++ and it is used in the context of TR-based concept location in source code. Refoqus outperformed the baselines and its recommendations lead to query performance improvement or preservation in 84% of the cases (in average).
[Context, Text Retrieval, Java, C++, Query Reformulation, Natural languages, automatic query reformulation, text retrieval, feature location, Frequency measurement, C++ language, machine learning, Engines, Training, recommender systems, recommender system, traceability link recovery, Training data, Robustness, software engineering, Refoqus, learning (artificial intelligence), query formulation]
Are software patents bad? (Keynote)
2013 35th International Conference on Software Engineering
None
2013
Editor's note: Professor Samuelson has, within her right and expertise, declined to yield copyright for her abstract to IEEE, which is required by IEEE's rules. We encourage the reader to find and read her abstract on the conference web site, and to support vibrant and broad-based discussions of intellectual-property policies.
[]
The connection between movie making and software development (Keynote)
2013 35th International Conference on Software Engineering
None
2013
About the Speaker Tony DeRose is currently a Senior Scientist and lead of the Research Group at Pixar Animation Studios. He received a BS in Physics in from the University of California, Davis, and a Ph.D. in Computer Science from the University of California, Berkeley. From 1986 to 1995 Dr. DeRose was a Professor of Computer Science and Engineering at the University of Washington. In 1998, he was a major contributor to the Oscar (c) winning short film "Geri's game", in 1999 he received the ACM SIGGRAPH Computer Graphics Achievement Award, and in 2006 he received a Scientific and Technical Academy Award (c) for his work on surface representations. In addition to his research interests, Tony is also involved in a number of initiatives to help make math, science, and engineering education more inspiring and relevant for middle and high school students. One such initiative is the Young Makers Program (youngmakers.org) that supports youth in building ambitious hands-on projects of their own choosing.
[]
Does scale really matter? Ultra-Large-Scale Systems seven years after the study (Keynote)
2013 35th International Conference on Software Engineering
None
2013
In 2006, Ultra-Large-Scale Systems: The Software Challenge of the Future (ISBN 0-9786956-0-7) documented the results of a year-long study on ultra-large, complex, distributed systems. Ultra-large-scale (ULS) systems are socio-technical ecosystems of ultra-large size on one or many dimensions &#x2014; number of lines of code; number of people employing the system for different purposes; amount of data stored, accessed, manipulated, and refined; number of connections and interdependencies among software components; number of hardware elements to which they interface. The characteristics of such systems require changes in traditional software development and management practices, which in turn require a new multi-disciplinary perspective and research. A carefully prescribed research agenda was suggested. What has happened since the study results were published? This talk shares a perspective on the post study reality &#x2014; a perspective based on research motivated by the study and direct experiences with ULS systems.
[]
Technical Debt: Past, present, and future (Panel)
2013 35th International Conference on Software Engineering
None
2013
The term &#x201C;Technical Debt&#x201D; was coined over 20 years ago by Ward Cunningham in a 1992 OOPSL A experience report to de scribe the tr ade-offs between delivering the most appropriate &#x2014; albeit likely immature &#x2014; product, in the shortest time possible. Since then the repercussions of going into &#x201C;technical debt&#x201D; have become more visible, yet not necessarily more broadly understood. This panel will bring together practitioners to discuss and debate strategies for debt relief.
[Technological innovation, Technical Debt, Communities, Blogs, software systems, Medical services, Educational institutions, Software, strategy, Software engineering]
Scaling agile methods to regulated environments: An industry case study
2013 35th International Conference on Software Engineering
None
2013
Agile development methods are growing in popularity with a recent survey reporting that more than 80% of organizations now following an agile approach. Agile methods were seen initially as best suited to small, co-located teams developing non-critical systems. The first two constraining characteristics (small and co-located teams) have been addressed as research has emerged describing successful agile adoption involving large teams and distributed contexts. However, the applicability of agile methods for developing safety-critical systems in regulated environments has not yet been demonstrated unequivocally, and very little rigorous research exists in this area. Some of the essential characteristics of agile approaches appear to be incompatible with the constraints imposed by regulated environments. In this study we identify these tension points and illustrate through a detailed case study how an agile approach was implemented successfully in a regulated environment. Among the interesting concepts to emerge from the research are the notions of continuous compliance and living traceability.
[regulated environment, program diagnostics, software prototyping, Documentation, safety-critical software, agile approach, Standards, continuous compliance, Agile methods, case study, agile adoption, colocated teams, safety-critical system, Organizations, living traceability, Software, Product development, Scrum, Safety, agile development method, Quality management, distributed teams, regulated environments]
Agility at scale: Economic governance, measured improvement, and disciplined delivery
2013 35th International Conference on Software Engineering
None
2013
Agility without discipline cannot scale, and discipline without agility cannot compete. Agile methods are now mainstream. Software enterprises are adopting these practices in broad, comprehensive delivery contexts. There have been many successes, and there have been disappointments. IBM's framework for achieving agility at scale is based on hundreds of successful deployments and dozens of disappointing experiences in accelerating software delivery cycles within large-scale organizations. Our collective know-how points to three key principles to deliver measured improvements in agility with high confidence: Steer using economic governance, measure incremental improvements honestly, and empower teams with disciplined agile delivery. This paper elaborates these three principles and presents practical recommendations for achieving improved agility in large-scale software delivery enterprises.
[Economics, Context, economic governance, Uncertainty, software prototyping, team empowerment, steering leadership, DP industry, measured improvement, large-scale software delivery enterprises, Complexity theory, accelerating software delivery, agile development, agile methods, team working, disciplined agile delivery, Software process improvement, Organizations, integration first, Software, agility, honest incremental improvement measurement]
Distributed development considered harmful?
2013 35th International Conference on Software Engineering
None
2013
We offer a case study illustrating three rules for reporting research to industrial practitioners. Firstly, report &#x201C;relevant&#x201D; results; e.g. this paper explores the effects of distributed development on software products. Second: &#x201C;recheck&#x201D; old results if new results call them into question. Many papers say distributed development can be harmful to software quality. Previous work by Bird et al. allayed that concern but a recent paper by Posnett et al. suggests that the Bird result was biased by the kinds of files it explored. Hence, this paper rechecks that result and finds significant differences in Microsoft products (Office 2010) between software built by distributed or collocated teams. At first glance, this recheck calls into question the widespread practice of distributed development. Our third rule is to &#x201C;reflect&#x201D; on results to avoid confusing practitioners with an arcane mathematical analysis. For example, on reflection, we found that the effect size of the differences seen in the collocated and distributed software was so small that it need not concern industrial practitioners. Our conclusion is that at least for Microsoft products, distributed development is not considered harmful.
[Measurement, software products, arcane mathematical analysis, Buildings, distributed processing, Birds, software quality, distributed development, Standards, Dispersion, industrial practitioners, Microsoft products, Office 2010, collocated software, distributed software, Software quality]
Measuring architecture quality by structure plus history analysis
2013 35th International Conference on Software Engineering
None
2013
This case study combines known software structure and revision history analysis techniques, in known and new ways, to predict bug-related change frequency, and uncover architecture-related risks in an agile industrial software development project. We applied a suite of structure and history measures and statistically analyzed the correlations between them. We detected architecture issues by identifying outliers in the distributions of measured values and investigating the architectural significance of the associated classes. We used a clustering method to identify sets of files that often change together without being structurally close together, investigating whether architecture issues were among the root causes. The development team confirmed that the identified clusters reflected significant architectural violations, unstable key interfaces, and important undocumented assumptions shared between modules. The combined structure diagrams and history data justified a refactoring proposal that was accepted by the project manager and implemented.
[software structure, program debugging, bug related change frequency, change history, Correlation, architecture quality measurement, clustering method, Size measurement, Complexity theory, software quality, History, agile industrial software development project, structure, revision history analysis techniques, software architecture, measure, structure plus history analysis, project manager, Computer bugs, Computer architecture, Software, fault prediction]
Obtaining ground-truth software architectures
2013 35th International Conference on Software Engineering
None
2013
Undocumented evolution of a software system and its underlying architecture drives the need for the architecture's recovery from the system's implementation-level artifacts. While a number of recovery techniques have been proposed, they suffer from known inaccuracies. Furthermore, these techniques are difficult to evaluate due to a lack of &#x201C;ground-truth&#x201D; architectures that are known to be accurate. To address this problem, we argue for establishing a suite of ground-truth architectures, using a recovery framework proposed in our recent work. This framework considers domain-, application-, and context-specific information about a system, and addresses an inherent obstacle in establishing a ground-truth architecture - the limited availability of engineers who are closely familiar with the system in question. In this paper, we present our experience in recovering the ground-truth architectures of four open-source systems. We discuss the primary insights gained in the process, analyze the characteristics of the obtained ground-truth architectures, and reflect on the involvement of the systems' engineers in a limited but critical fashion. Our findings suggest the practical feasibility of obtaining ground-truth architectures for large systems and encourage future efforts directed at establishing a large scale repository of such architectures.
[Java, application-specific information, Protocols, public domain software, Documentation, architecture recovery framework, large scale repository, Web servers, software maintenance, open-source systems, domain-specific information, software architecture, ground-truth software architectures, Computer architecture, Software systems, context-specific information]
MIDAS: A design quality assessment method for industrial software
2013 35th International Conference on Software Engineering
None
2013
Siemens Corporate Development Center Asia Australia (CT DC AA) develops and maintains software applications for the Industry, Energy, Healthcare, and Infrastructure &amp; Cities sectors of Siemens. The critical nature of these applications necessitates a high level of software design quality. A survey of software architects indicated a low level of satisfaction with existing design assessment practices in CT DC AA and highlighted several shortcomings of existing practices. To address this, we have developed a design assessment method called MIDAS (Method for Intensive Design ASsessments). MIDAS is an expert-based method wherein manual assessment of design quality by experts is directed by the systematic application of design analysis tools through the use of a three view-model consisting of design principles, project-specific constraints, and an &#x201C;ility&#x201D;-based quality model. In this paper, we describe the motivation for MIDAS, its design, and its application to three projects in CT DC AA. We believe that the insights from our MIDAS experience not only provide useful pointers to other organizations and practitioners looking to assess and improve software design quality but also suggest research questions for the software engineering community to explore.
[CT DC AA, expert systems, design analysis tools, program testing, Design methodology, MIDAS, Manuals, Siemens Corporate Development Center Asia Australia, software quality, software design quality, method for intensive design assessments, Analytical models, software architecture, Software design, design quality assessment method, software engineering community, design assessment practices, software performance evaluation, industrial software, Context, project-specific constraints, ility-based quality model, expert-based method, software architects, software applications, software design assessment method, Quality assessment]
Evaluating usefulness of software metrics: An industrial experience report
2013 35th International Conference on Software Engineering
None
2013
A wide range of software metrics targeting various abstraction levels and quality attributes have been proposed by the research community. For many of these metrics the evaluation consists of verifying the mathematical properties of the metric, investigating the behavior of the metric for a number of open-source systems or comparing the value of the metric against other metrics quantifying related quality attributes. Unfortunately, a structural analysis of the usefulness of metrics in a real-world evaluation setting is often missing. Such an evaluation is important to understand the situations in which a metric can be applied, to identify areas of possible improvements, to explore general problems detected by the metrics and to define generally applicable solution strategies. In this paper we execute such an analysis for two architecture level metrics, Component Balance and Dependency Profiles, by analyzing the challenges involved in applying these metrics in an industrial setting. In addition, we explore the usefulness of the metrics by conducting semi-structured interviews with experienced assessors. We document the lessons learned both for the application of these specific metrics, as well as for the method of evaluating metrics in practice.
[Context, research community, architecture level metrics, component balance and dependency profiles, public domain software, mathematical properties, abstraction levels, software quality, open-source systems, Standards, real-world evaluation setting, industrial experience report, Software metrics, structural analysis, quality attributes, Computer architecture, Software, Interviews, software performance evaluation, software metrics]
Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation
2013 35th International Conference on Software Engineering
None
2013
Peer code review is a cost-effective software defect detection technique. Tool assisted code review is a form of peer code review, which can improve both quality and quantity of reviews. However, there is a significant amount of human effort involved even in tool based code reviews. Using static analysis tools, it is possible to reduce the human effort by automating the checks for coding standard violations and common defect patterns. Towards this goal, we propose a tool called Review Bot for the integration of automatic static analysis with the code review process. Review Bot uses output of multiple static analysis tools to publish reviews automatically. Through a user study, we show that integrating static analysis tools with code review process can improve the quality of code review. The developer feedback for a subset of comments from automatic reviews shows that the developers agree to fix 93% of all the automatically generated comments. There is only 14.71% of all the accepted comments which need improvements in terms of priority, comment message, etc. Another problem with tool assisted code review is the assignment of appropriate reviewers. Review Bot solves this problem by generating reviewer recommendations based on change history of source code lines. Our experimental results show that the recommendation accuracy is in the range of 60%-92%, which is significantly better than a comparable method based on file change history.
[Algorithm design and analysis, Java, developer feedback, program testing, program diagnostics, coding standard violation checking automation, Inspection, tool assisted code review, source code line change history, Encoding, file change history, comment message, History, peer code review, Standards, reviewer recommendation, static analysis tools, cost-effective software defect detection technique, automatic static analysis, quality improvement, Software, software engineering, Review Bot tool, defect pattern]
Estimating software-intensive projects in the absence of historical data
2013 35th International Conference on Software Engineering
None
2013
This paper describes a software estimation technique that can be used in situations where there is no reliable historical data available to develop the initial effort estimate of a software development project. The technique described incorporates a set of key estimation principles and three estimation methods that are utilized in tandem to deliver the estimation results needed to have a robust initial estimation. An important contribution of this paper is bringing together into ONe Software Estimation Tool-kit (ONSET) multiple concepts, principles, and methods in the software estimation field, which are typically discussed separately in the estimation literature and can be employed when an organization does not have reliable historical data. The paper shows how these principles and methods are applied to derive estimates without the need of using complex or expensive tools. A case study is presented using ONSET which was carried out as an estimation pilot study conducted in one of the software development Business Units of ABB. The results of this pilot project provided insights on how to implement ONSET across ABB software development business units. Practical guidance is offered in this paper on how an organization that does not have reliable historical data can begin to collect data to use in future projects using ONSET. In contrast to many papers that describe estimation approaches, this paper explains how to use a combination of judgment-based and model-based methods such as the Planning Poker, Modified Wideband Delphi, and Monte Carlo simulation to derive the initial estimates. Once an organization begins collecting reliable historical data, ONSET will provide even more accurate estimation results and a smoother transition to the use of model-based estimation methods and tools can be achieved.
[Uncertainty, software-intensive project estimation technique, software management, insert, Monte Carlo methods, styling, modified wideband Delphi, planning poker, software tools, one software estimation tool-kit, Component, project management, robust initial estimation, judgment-based methods, Estimation, software development project, ABB software development business units, Software reliability, historical data, software estimation field, Organizations, model-based estimation methods, Software, style, ONSET, Monte Carlo simulation, formatting]
Pathways to technology transfer and adoption: Achievements and challenges (mini-tutorial)
2013 35th International Conference on Software Engineering
None
2013
Producing industrial impact has often been one of the important goals of academic or industrial researchers when conducting research. However, it is generally challenging to transfer research results into industrial practices. There are some common challenges faced when pursuing technology transfer and adoption while particular challenges for some particular research areas. At the same time, various opportunities also exist for technology transfer and adoption. This mini-tutorial presents achievements and challenges of technology transfer and adoption in various areas in software engineering, with examples drawn from research areas such as software analytics along with software testing and analysis. This mini-tutorial highlights success stories in industry, research achievements that are transferred to industrial practice, and challenges and lessons learned in technology transfer and adoption.
[program testing, technology adoption, academic researchers, software testing, Cloning, technology transfer, industrial researchers, software analysis, Technology transfer, Collaboration, research and development, Speech, Software, software engineering, industrial impact, software analytics, Software engineering, Testing]
User involvement in software evolution practice: A case study
2013 35th International Conference on Software Engineering
None
2013
User involvement in software engineering has been researched over the last three decades. However, existing studies concentrate mainly on early phases of user-centered design projects, while little is known about how professionals work with post-deployment end-user feedback. In this paper we report on an empirical case study that explores the current practice of user involvement during software evolution. We found that user feedback contains important information for developers, helps to improve software quality and to identify missing features. In order to assess its relevance and potential impact, developers need to analyze the gathered feedback, which is mostly accomplished manually and consequently requires high effort. Overall, our results show the need for tool support to consolidate, structure, analyze, and track user feedback, particularly when feedback volume is high. Our findings call for a hypothesis-driven analysis of user feedback to establish the foundations for future user feedback tools.
[software evolution practice, Companies, Mobile communication, Electronic mail, software quality, user interfaces, software maintenance, software evolution, missing feature identification, hypothesis-driven analysis, post-deployment end-user feedback, user-centered design projects, software quality improvement, Software quality, user involvement, Data collection, software engineering, user feedback, Interviews, user centred design]
A characteristic study on failures of production distributed data-parallel programs
2013 35th International Conference on Software Engineering
None
2013
SCOPE is adopted by thousands of developers from tens of different product teams in Microsoft Bing for daily web-scale data processing, including index building, search ranking, and advertisement display. A SCOPE job is composed of declarative SQL-like queries and imperative C# user-defined functions (UDFs), which are executed in pipeline by thousands of machines. There are tens of thousands of SCOPE jobs executed on Microsoft clusters per day, while some of them fail after a long execution time and thus waste tremendous resources. Reducing SCOPE failures would save significant resources. This paper presents a comprehensive characteristic study on 200 SCOPE failures/fixes and 50 SCOPE failures with debugging statistics from Microsoft Bing, investigating not only major failure types, failure sources, and fixes, but also current debugging practice. Our major findings include (1) most of the failures (84.5%) are caused by defects in data processing rather than defects in code logic; (2) table-level failures (22.5%) are mainly caused by programmers' mistakes and frequent data-schema changes while row-level failures (62%) are mainly caused by exceptional data; (3) 93% fixes do not change data processing logic; (4) there are 8% failures with root cause not at the failure-exposing stage, making current debugging practice insufficient in this case. Our study results provide valuable guidelines for future development of data-parallel programs. We believe that these guidelines are not limited to SCOPE, but can also be generalized to other similar data-parallel platforms.
[frequent data-schema changes, program debugging, debugging statistics, programmer mistakes, failure types, Data mining, C language, production distributed data-parallel program failures, parallel programming, query processing, team working, imperative C# user-defined functions, Runtime, table-level failures, code logic defects, declarative SQL-like queries, UDF, failure-exposing stage, Production, distributed databases, Microsoft clusters, data processing defects, index building, SCOPE job, SCOPE failures/fixes, search ranking, failure sources, debugging practice, product teams, Debugging, row-level failures, Data processing, advertisement display, Indexes, data processing logic, software fault tolerance, SQL, Microsoft Bing, daily Web-scale data processing, Data models, distributed storage data]
Is time-zone proximity an advantage for software development? The case of the Brazilian IT industry
2013 35th International Conference on Software Engineering
None
2013
Brazil has been emerging as a destination for IT software and services. The country already had a strong domestic base of IT clients to global companies. One of the competitive factors is time zone location. Brazil has positioned itself as easy for collaboration because of time zone overlap with its primary partners in North America and Europe. In this paper we examine whether time zone proximity is an advantage for software development by conducting a country-level field study of the Brazilian IT industry using a cross section of firms. The results provide some support for the claims of proximity benefits. The Brazil-North dyads use moderate timeshifting that is perceived as comfortable for both sides. The voice coordination that the time overlap permits helps address coordination challenges and foster relationships. One company, in particular, practiced such intense time zone aligned collaboration using agile methods that we labeled this Real-time Simulated Co-location.
[Industries, voice coordination, IT clients, Teleworking, Companies, global companies, Brazil-North dyads, moderate timeshifting, competitive intelligence, North America, agile methods, time zone overlap, Time-zone proximity, Brazilian IT industry, globalisation, firms cross section, time zone proximity, real-time simulated colocation, IT services, country-level field study, software development, Europe, DP industry, software development management, time zone location, Globalization of IT, proximity benefits, Asia, Collaboration, Software, IT software, competitive factors]
A study of enabling factors for rapid fielding combined practices to balance speed and stability
2013 35th International Conference on Software Engineering
None
2013
Agile projects are showing greater promise in rapid fielding as compared to waterfall projects. However, there is a lack of clarity regarding what really constitutes and contributes to success. We interviewed project teams with incremental development lifecycles, from five government and commercial organizations, to gain a better understanding of success and failure factors for rapid fielding on their projects. A key area we explored involves how Agile projects deal with the pressure to rapidly deliver high-value capability, while maintaining project speed (delivering functionality to the users quickly) and product stability (providing reliable and flexible product architecture). For example, due to schedule pressure we often see a pattern of high initial velocity for weeks or months, followed by a slowing of velocity due to stability issues. Business stakeholders find this to be disruptive as the rate of capability delivery slows while the team addresses stability problems. We found that experienced practitioners, when faced with these challenges, do not apply Agile practices alone. Instead they combine practices - Agile, architecture, or other - in creative ways to respond quickly to unanticipated stability problems. In this paper, we summarize the practices practitioners we interviewed from Agile projects found most valuable and provide an overarching scenario that provides insight into how and why these practices emerge.
[rapid fielding, failure factors, government organizations, software prototyping, agile projects, high-value capability, speed, software architecture, waterfall projects, Computer architecture, agile practices, Interviews, stability, product stability, product architecture reliability, architecture, software development practices, project speed maintenance, Stability analysis, unanticipated stability problems, commercial organizations, incremental development lifecycles, product architecture flexibility, success factors, Organizations, Software, Planning, project teams, agile software development]
JST: An automatic test generation tool for industrial Java applications with strings
2013 35th International Conference on Software Engineering
None
2013
In this paper we present JST, a tool that automatically generates a high coverage test suite for industrial strength Java applications. This tool uses a numeric-string hybrid symbolic execution engine at its core which is based on the Symbolic Java PathFinder platform. However, in order to make the tool applicable to industrial applications the existing generic platform had to be enhanced in numerous ways that we describe in this paper. The JST tool consists of newly supported essential Java library components and widely used data structures; novel solving techniques for string constraints, regular expressions, and their interactions with integer and floating point numbers; and key optimizations that make the tool more efficient. We present a methodology to seamlessly integrate the features mentioned above to make the tool scalable to industrial applications that are beyond the reach of the original platform in terms of both applicability and performance. We also present extensive experimental data to illustrate the effectiveness of our tool.
[automatic test generation tool, Java, JST tool, industrial Java applications, program testing, industrial strength Java applications, numeric-string hybrid symbolic execution engine, high coverage test suite, software libraries, string constraints, Java library components, symbolic Java PathFinder platform, Semantics, Automata, integer numbers, regular expressions, Libraries, Concrete, data structures, Numerical models, software tools, string matching, Testing, floating point numbers]
Efficient and change-resilient test automation: An industrial case study
2013 35th International Conference on Software Engineering
None
2013
Test automation, which involves the conversion of manual test cases to executable test scripts, is necessary to carry out efficient regression testing of GUI-based applications. However, test automation takes significant investment of time and skilled effort. Moreover, it is not a one-time investment: as the application or its environment evolves, test scripts demand continuous patching. Thus, it is challenging to perform test automation in a cost-effective manner. At IBM, we developed a tool, called ATA [1], [2], to meet this challenge. ATA has novel features that are designed to lower the cost of initial test automation significantly. Moreover, ATA has the ability to patch scripts automatically for certain types of application or environment changes. How well does ATA meet its objectives in the real world? In this paper, we present a detailed case study in the context of a challenging production environment: an enterprise web application that has over 6500 manual test cases, comes in two variants, evolves frequently, and needs to be tested on multiple browsers in time-constrained and resource-constrained regression cycles. We measured how well ATA improved the efficiency in initial automation. We also evaluated the effectiveness of ATA's change-resilience along multiple dimensions: application versions, browsers, and browser versions. Our study highlights several lessons for test-automation practitioners as well as open research problems in test automation.
[automatic test software, program testing, graphical user interfaces, application version, industrial practice, regression testing, Manuals, regression analysis, Automating Test Automation, Engines, browser version, time constrained regression cycle, Production, software houses, change resilient test automation, Testing, automatic programming, Automation, DP industry, Maintenance engineering, enterprise Web application, investment, Browsers, ATA, production environment, GUI-based application, resource constrained regression cycle]
Automatic detection of performance deviations in the load testing of Large Scale Systems
2013 35th International Conference on Software Engineering
None
2013
Load testing is one of the means for evaluating the performance of Large Scale Systems (LSS). At the end of a load test, performance analysts must analyze thousands of performance counters from hundreds of machines under test. These performance counters are measures of run-time system properties such as CPU utilization, Disk I/O, memory consumption, and network traffic. Analysts observe counters to find out if the system is meeting its Service Level Agreements (SLAs). In this paper, we present and evaluate one supervised and three unsupervised approaches to help performance analysts to 1) more effectively compare load tests in order to detect performance deviations which may lead to SLA violations, and 2) to provide them with a smaller and manageable set of important performance counters to assist in root-cause analysis of the detected deviations. Our case study is based on load test data obtained from both a large scale industrial system and an open source benchmark application. The case study shows, that our wrapper-based supervised approach, which uses a search-based technique to find the best subset of performance counters and a logistic regression model for deviation prediction, can provide up to 89% reduction in the set of performance counters while detecting performance deviations with few false positives (i.e., 95% average precision). The study also shows that the supervised approach is more stable and effective than the unsupervised approaches but it has more overhead due to its semi-automated training phase.
[automatic performance deviation detection, logistic regression model, program testing, public domain software, memory consumption, regression analysis, input-output programs, run-time system properties, Machine Learning, SLA violations, load testing, Control charts, Large-scale systems, disk I-O, Monitoring, software performance evaluation, Testing, service level agreements, Radiation detectors, open source benchmark application, machine learning, LSS, unsupervised learning, network traffic, performance counters, CPU utilization, Signature, large scale systems, search-based technique, deviation prediction, Performance, wrapper-based supervised approach, Principal component analysis, Logistics, root-cause analysis]
Detecting inconsistencies in wrappers: A case study
2013 35th International Conference on Software Engineering
None
2013
Exchangeability between software components such as operating systems, middleware, databases, and hardware components is a common requirement in many software systems. One way to enable exchangeability is to promote indirect use through a common interface and an implementation for each component that wraps the original component. As developers use the interface instead of the underlying component, they assume that the software system will behave in a specific way independently of the actual component in use. However, differences in the implementations of the wrappers may lead to different behavior when one component is changed for another, which might lead to failures in the field. This work reports on a simple, yet effective approach to detect these differences. The approach is based on tool-supported reviews leveraging lightweight static analysis and machine learning. The approach is evaluated in a case study that analyzes NASA's Operating System Abstraction Layer (OSAL), which is used in various space missions. We detected 84 corner-case issues of which 57 turned out to be bugs that could have resulted in runtime failures.
[Measurement, tool-supported review, wrapper inconsistency detection, software system behavior, Data mining, lightweight static analysis, system recovery, Machine Learning, Training, runtime failure, Wrappers, database, learning (artificial intelligence), middleware, object-oriented programming, program diagnostics, operating system, Inconsistencies, software component exchangeability, OSAL, machine learning, Abstraction, Interfaces, hardware components, space mission, Equivalence, Computer bugs, common interface, Feature extraction, Software systems, operating systems (computers), NASA Operating System Abstraction Layer]
Categorizing bugs with social networks: A case study on four open source software communities
2013 35th International Conference on Software Engineering
None
2013
Efficient bug triaging procedures are an important precondition for successful collaborative software engineering projects. Triaging bugs can become a laborious task particularly in open source software (OSS) projects with a large base of comparably inexperienced part-time contributors. In this paper, we propose an efficient and practical method to identify valid bug reports which a) refer to an actual software bug, b) are not duplicates and c) contain enough information to be processed right away. Our classification is based on nine measures to quantify the social embeddedness of bug reporters in the collaboration network. We demonstrate its applicability in a case study, using a comprehensive data set of more than 700, 000 bug reports obtained from the Bugzilla installation of four major OSS communities, for a period of more than ten years. For those projects that exhibit the lowest fraction of valid bug reports, we find that the bug reporters' position in the collaboration network is a strong indicator for the quality of bug reports. Based on this finding, we develop an automated classification scheme that can easily be integrated into bug tracking platforms and analyze its performance in the considered OSS communities. A support vector machine (SVM) to identify valid bug reports based on the nine measures yields a precision of up to 90.3% with an associated recall of 38.9%. With this, we significantly improve the results obtained in previous case studies for an automated early identification of bugs that are eventually fixed. Furthermore, our study highlights the potential of using quantitative measures of social organization in collaborative software engineering. It also opens a broad perspective for the integration of social awareness in the design of support infrastructures.
[program debugging, collaboration network, public domain software, Communities, social awareness, BUGZILLA installation, SVM, History, open source software communities, OSS communities, bug triaging procedures, groupware, social embeddedness, pattern classification, project management, support vector machines, bug report quality, Social network services, social networks, automated classification scheme, bug tracking platforms, part-time contributors, social organization, Support vector machines, bug categorization, actual software bug, support vector machine, collaborative software engineering projects, Computer bugs, Collaboration, social networking (online), Software, bug reporters]
Predicting bug-fixing time: An empirical study of commercial software projects
2013 35th International Conference on Software Engineering
None
2013
For a large and evolving software system, the project team could receive many bug reports over a long period of time. It is important to achieve a quantitative understanding of bug-fixing time. The ability to predict bug-fixing time can help a project team better estimate software maintenance efforts and better manage software projects. In this paper, we perform an empirical study of bug-fixing time for three CA Technologies projects. We propose a Markov-based method for predicting the number of bugs that will be fixed in future. For a given number of defects, we propose a method for estimating the total amount of time required to fix them based on the empirical distribution of bug-fixing time derived from historical data. For a given bug report, we can also construct a classification model to predict slow or quick fix (e.g., below or above a time threshold). We evaluate our methods using real maintenance data from three CA Technologies projects. The results show that the proposed methods are effective.
[Bugs, program debugging, Companies, Predictive models, software management, commercial software projects, project team, software system, team working, Monte Carlo methods, bug fixing time, maintenance data, bug reports, effort estimation, software project management, project management, bug-fixing time, Maintenance engineering, software maintenance, Standards, classification model, software maintenance estimation, Computer bugs, Markov-based method, Markov processes, prediction]
Authentic assessment in Software Engineering education based on PBL principles a case study in the telecom market
2013 35th International Conference on Software Engineering
None
2013
The continuous growth of the use of Information and Communication Technology in different sectors of the market calls out for software professionals with the qualifications needed to solve complex and diverse problems. Innovative teaching methodologies, such as the "Software Internship" model and PBL teaching approaches that are learner-centered and focus on bringing market reality to the learning environment, have been developed and implemented with a view to meeting this demand. However, the effectiveness of these methods cannot always be satisfactorily proved. Prompted by this, this paper proposes a model for assessing students based on real market practices while preserving the authenticity of the learning environment. To evaluate this model, a case study on skills training for software specialists for the Telecom market is discussed, and presents important results that show the applicability of the proposed model for teaching Software Engineering.
[Context, computer science education, telecom market, learning environment authenticity preservation, software engineering teaching methodologies, on-the-job training, PBL principles, Production facilities, teaching, Telecommunications, Training, problem-based learning, skills training, authentic assessment model, information and communication technology, Assessment processes, learner-centered PBL teaching approaches, software engineering education, Software, software engineering, Software Engineering Education, software internship model, Monitoring]
Studios in software engineering education: Towards an evaluable model
2013 35th International Conference on Software Engineering
None
2013
Studio-based teaching is a method commonly used in arts and design that emphasizes a physical "home" for students, problem-based and peer-based learning, and mentoring by academic staff rather than formal lectures. There have been some attempts to transfer studio-based teaching to software engineering education. In many ways, this is natural as software engineering has significant practical elements. However, attempts at software studios have usually ignored experiences and theory from arts and design studio teaching. There is therefore a lack of understanding of what "studio" really means, how well the concepts transfer to software engineering, and how effective studios are in practice. Without a clear definition of "studio\
[studio education, Art, Aerospace electronics, teaching, studio-based teaching, Studio, Design, design, Software Studio, software engineering, Software Engineering Education, Interviews, arts, computer science education, art, Educational institutions, Creativity, problem-based learning, Collocation, Atelier, Collaboration, qualitative analysis, software engineering education, Software, physical home, peer-based learning, Software engineering, digital technology]
Enabling a classroom design studio with a collaborative sketch design tool
2013 35th International Conference on Software Engineering
None
2013
The use of a studio approach - a hands-on teaching method that emphasizes in-class discussion and activities - is becoming an increasingly accepted method of teaching within software engineering. In such studios, emphasis is placed not only on the artifacts to be produced, but also on the process used to arrive at those artifacts. In this paper, we introduce Calico, a sketch-based collaborative software design tool, and discuss how it supports the delivery of a studio approach to software design education. We particularly describe our experiences with Calico in Software Design I, a course aimed at introducing students to the early, creative phases of software design. Our results show that Calico enabled students to work effectively in teams on their design problems, quickly developing, refining, and evaluating their designs.
[hands-on teaching method, design development, Software Design I course, teaching, software design education, design problems, Software design studio, team working, Software design, Education, Computer architecture, groupware, design, software engineering, software tools, sketch-based collaborative software design tool, courseware, design evaluation, computer science education, in-class discussion, sketching, Collaboration, User interfaces, Calico, classroom design studio, Software engineering]
A framework to evaluate software engineering student contests: Evaluation and integration with academic programs
2013 35th International Conference on Software Engineering
None
2013
There are hundreds of general contests targeting undergraduate and graduate students. The prizes vary from cash, trip, fame, conference participation and others. Contests could be class, school, national, regional or global contests. In this paper, we compare between existing student contests that can be integrated with software engineering courses. We classify the contests and propose a framework to choose which one to suit curriculum. We also include best practices and samples of our practices in integrating software engineering course with class, regional, national and global contests.
[contests and games in education, Google, computer science education, further education, Software Engineering education, national contests, Programming, Educational institutions, software engineering courses, curriculum, class contests, educational courses, global contests, regional contests, Games, Software, software engineering, software engineering student contests, Software engineering, academic programs]
An evaluation of interactive test-driven labs with WebIDE in CS0
2013 35th International Conference on Software Engineering
None
2013
WebIDE is a framework that enables instructors to develop and deliver online lab content with interactive feedback. The ability to create lock-step labs enables the instructor to guide students through learning experiences, demonstrating mastery as they proceed. Feedback is provided through automated evaluators that vary from simple regular expression evaluation to syntactic parsers to applications that compile and run programs and unit tests. This paper describes WebIDE and its use in a CS0 course that taught introductory Java and Android programming using a test-driven learning approach. We report results from a controlled experiment that compared the use of dynamic WebIDE labs with more traditional static programming labs. Despite weaker performance on pre-study assessments, students who used WebIDE performed two to twelve percent better on all assessments than the students who used traditional labs. In addition, WebIDE students were consistently more positive about their experience in CS0.
[Java, computer science education, online lab content, Programming, program compilers, syntactic parsers, Android programming, introductory Java, CS0 course, dynamic WebIDE labs, Semantics, interactive feedback, XML, Computer architecture, static programming labs, Syntactics, Writing, test-driven learning approach, interactive test-driven labs]
POPT: A Problem-Oriented Programming and Testing approach for novice students
2013 35th International Conference on Software Engineering
None
2013
There is a growing interest of the Computer Science education community for including testing concepts on introductory programming courses. Aiming at contributing to this issue, we introduce POPT, a Problem-Oriented Programming and Testing approach for Introductory Programming Courses. POPT main goal is to improve the traditional method of teaching introductory programming that concentrates mainly on implementation and neglects testing. According to POPT, students' skills must be developed by dealing with ill-defined problems, from which students are stimulated to develop test cases in a table-like manner in order to enlighten the problems' requirements and also to improve the quality of generated code. This paper presents POPT and a case study performed in an Introductory Programming course of a Computer Science program at the Federal University of Rio Grande do Norte, Brazil. The study results have shown that, when compared to a Blind Testing approach, POPT stimulates the implementation of programs of better external quality - the first program version submitted by POPT students passed in twice the number of test cases (professor-defined ones) when compared to non-POPT students. Moreover, POPT students submitted fewer program versions and spent more time to submit the first version to the automatic evaluation system, which lead us to think that POPT students are stimulated to think better about the solution they are implementing.
[Measurement, CS1, introductory programming teaching, program testing, Teaching Software Testing Concepts, teaching, software quality, table-like test case development, quality improvement, Table-based test cases, novice students, Testing, computer science education, further education, Introductory Courses, POPT, Educational institutions, Programming profession, Standards, problem-oriented testing, automatic evaluation system, code generation, educational courses, problem-oriented programming, introductory programming courses, computer science education community]
Teaching developer skills in the first software engineering course
2013 35th International Conference on Software Engineering
None
2013
Both employers and graduate schools expect computer science graduates to be able to work as developers on software projects. Software engineering courses present the opportunity in the curriculum to learn the relevant skills. This paper presents our experience from Wayne State University and reviews challenges and constraints that we faced while trying to teach these skills. In our first software engineering course, we teach the iterative software development that includes practices of software change, summarized in the phased model of software change. The required resources for our software engineering course are comparable to the other computer science courses. The students - while working in teams - are graded based on their individual contribution to the team effort rather than on the work of the other team members, which improves the fairness of the grading and considerably lessens the stress for the best students in the course. Our students have expressed a high level of satisfaction, and in a survey, they indicated that the skills that they learned in the course are highly applicable to their careers.
[iterative software development, Unified modeling language, first software engineering course, teaching, computer science courses, open source, refactoring, software engineering, software projects, Wayne State University, phased software change model, phased model of software change, Portfolios, developer skills teaching, concept location, computer science education, evolutionary-iterative-agile development, Educational institutions, First software engineering course, impact analysis, realistic code, project technologies, educational courses, actualization, Software, educational institutions, developer role, Software engineering]
Teaching and learning programming and software engineering via interactive gaming
2013 35th International Conference on Software Engineering
None
2013
Massive Open Online Courses (MOOCs) have recently gained high popularity among various universities and even in global societies. A critical factor for their success in teaching and learning effectiveness is assignment grading. Traditional ways of assignment grading are not scalable and do not give timely or interactive feedback to students. To address these issues, we present an interactive-gaming-based teaching and learning platform called Pex4Fun. Pex4Fun is a browser-based teaching and learning environment targeting teachers and students for introductory to advanced programming or software engineering courses. At the core of the platform is an automated grading engine based on symbolic execution. In Pex4Fun, teachers can create virtual classrooms, customize existing courses, and publish new learning material including learning games. Pex4Fun was released to the public in June 2010 and since then the number of attempts made by users to solve games has reached over one million. Our work on Pex4Fun illustrates that a sophisticated software engineering technique-automated test generation-can be successfully used to underpin automatic grading in an online programming system that can scale to hundreds of thousands of users.
[program testing, browser-based teaching, Programming, teaching, software engineering courses, serious games (computing), Pex4Fun, interactive-gaming-based teaching, interactive feedback, learning platform, software engineering, automatic grading, Testing, computer science education, programming learning, massive open online courses, Educational institutions, assignment grading, Encoding, automated test generation, programming teaching, educational courses, Games, computer aided instruction, Software engineering]
Town hall discussion of SE 2004 revisions (panel)
2013 35th International Conference on Software Engineering
None
2013
This panel will engage participants in a discussion of recent changes in software engineering practice that should be reflected in curriculum guidelines for undergraduate software engineering programs. Current progress in revising the guidelines will be presented, including suggestions to update coverage of agile methods, security and service-oriented computing.
[software engineering 2004, IEEE Computer Society, Curriculum guidelines, ACM, undergraduate programs]
Teaching students global software engineering skills using distributed Scrum
2013 35th International Conference on Software Engineering
None
2013
In this paper we describe distributed Scrum augmented with best practices in global software engineering (GSE) as an important paradigm for teaching critical competencies in GSE. We report on a globally distributed project course between the University of Victoria, Canada and Aalto University, Finland. The project-driven course involved 16 students in Canada and 9 students in Finland, divided into three cross-site Scrum teams working on a single large project. To assess learning of GSE competencies we employed a mixed-method approach including 13 post-course interviews, pre-, post-course and iteration questionnaires, observations, recordings of Daily Scrums as well as collection of project asynchronous communication data. Our analysis indicates that the Scrum method, along with supporting collaboration practices and tools, supports the learning of important GSE competencies, such as distributed communication and teamwork, building and maintaining trust, using appropriate collaboration tools, and inter-cultural collaboration.
[software prototyping, distributed scrum, global software engineering skills, iteration questionnaires, teaching, Electronic mail, post-course interviews, University of Victoria, collaboration tools, Aalto University, GSE, mixed-method approach, globally distributed project course, Google, computer science education, student teaching, intercultural collaboration, Educational institutions, software maintenance, cross-site scrum teams, Canada, project-driven course, teamwork, educational courses, Finland, trust maintainance, Teamwork, Planning, educational institutions]
Teaching software process modeling
2013 35th International Conference on Software Engineering
None
2013
Most university curricula consider software processes to be on the fringes of software engineering (SE). Students are told there exists a plethora of software processes ranging from RUP over V-shaped processes to agile methods. Furthermore, the usual students' programming tasks are of a size that either one student or a small group of students can manage the work. Comprehensive processes being essential for large companies in terms of reflecting the organization structure, coordinating teams, or interfaces to business processes such as contracting or sales, are complex and hard to teach in a lecture, and, therefore, often out of scope. We experienced tutorials on using Java or C#, or on developing applications for the iPhone to gather more attention by students, simply speaking, as these are more fun for them. So, why should students spend their time in software processes? From our experiences and the discussions with a variety of industrial partners, we learned that students often face trouble when taking their first &#x201C;real&#x201D; jobs, even if the company is organized in a lean or agile shape. Therefore, we propose to include software processes more explicitly into the SE curricula. We designed and implemented a course at Master's level in which students learn why software processes are necessary, and how they can be analyzed, designed, implemented, and continuously improved. In this paper, we present our course's structure, its goals, and corresponding teaching methods. We evaluate the course and further discuss our experiences so that lecturers and researchers can directly use our lessons learned in their own curricula.
[SE curricula, iPhone, Conferences, teaching, agile methods, Training, iPhone application development, software process education teaching methods, software process improvement, teaching methods, student programming tasks, university curricula, software engineering, organization structure, programming, computer science education, C# tutorials, Educational institutions, RUP, Java tutorials, software process modeling teaching, team coordination, business process, educational courses, Organizations, Software, V-shaped process, Software engineering]
Industry involvement in ICT curriculum: A comparative survey
2013 35th International Conference on Software Engineering
None
2013
Stakeholder consultation during course accreditation is now a requirement of new Australian government regulations as well as the Australian ICT professional society accreditation. Despite these requirements there remains some differences between universities and industry regarding the purpose, nature and extent of industry involvement in the curriculum. Surveys of industry and university leaders in ICT were undertaken to provide a representative set of views on these issues. The results provided insights into the perceptions of universities and industry regarding industry involvement into the curriculum. The results also confirmed previous research that identified a tension between industry's desire for relevant skills and the role of universities in providing a broader education for lifelong learning.
[Industries, computer science education, university leaders, Government, stakeholder consultation, Australian ICT professional society accreditation, Educational institutions, industry leaders, continuing professional development, course accreditation, lifelong learning, Standards, industry involvement, industry consultation, Australian government regulations, regulation, Employment, educational courses, Professional practice, Accreditation, accreditation, ICT curriculum]
Vulnerability of the Day: Concrete demonstrations for software engineering undergraduates
2013 35th International Conference on Software Engineering
None
2013
Software security is a tough reality that affects the many facets of our modern, digital world. The pressure to produce secure software is felt particularly strongly by software engineers. Today's software engineering students will need to deal with software security in their profession. However, these students will also not be security experts, rather, they need to balance security concerns with the myriad of other draws of their attention, such as reliability, performance, and delivering the product on-time and on-budget. At the Department of Software Engineering at the Rochester Institute of Technology, we developed a course called Engineering Secure Software, designed for applying security principles to each stage of the software development lifecycle. As a part of this course, we developed a component called Vulnerability of the Day, which is a set of selected example software vulnerabilities. We selected these vulnerabilities to be simple, demonstrable, and relevant so that the vulnerability could be demonstrated in the first 10 minutes of each class session. For each vulnerability demonstration, we provide historical examples, realistic scenarios, and mitigations. With student reaction being overwhelmingly positive, we have created an open source project for our Vulnerabilities of the Day, and have defined guiding principles for developing and contributing effective examples.
[open source project, public domain software, software reliability, product on-budget delivery, security principles, Security, software development lifecycle, security, Education, design, historicla, Vulnerability of the Day, class session, software vulnerability, Common Weakness Enumeration, Engineering Secure Software course, security concerns, Java, computer science education, Rochester Institute of Technology, further education, software development management, product on-time delivery, Encoding, Department of Software Engineering, vulnerability, Information technology, vulnerability demonstration, security of data, educational courses, software engineering undergraduates, Software, software security, software performance, Software engineering, software engineering students]
Eliminative induction: A basis for arguing system confidence
2013 35th International Conference on Software Engineering
None
2013
Assurance cases provide a structured method of explaining why a system has some desired property, e.g., that the system is safe. But there is no agreed approach for explaining what degree of confidence one should have in the conclusions of such a case. In this paper, we use the principle of eliminative induction to provide a justified basis for assessing how much confidence one should have in an assurance case argument.
[system safety, eliminative induction, Educational institutions, Birds, Cognition, Hazards, inference mechanisms, assurance case, Human computer interaction, system confidence, safety case, software engineering, assurance case argument, defeasible reasoning, Software engineering]
Exploring the internal state of user interfaces by combining computer vision techniques with grammatical inference
2013 35th International Conference on Software Engineering
None
2013
In this paper, we present a promising approach to systematically testing graphical user interfaces (GUI) in a platform independent manner. Our framework uses standard computer vision techniques through a python-based scripting language (Sikuli script) to identify key graphical elements in the screen and automatically interact with these elements by simulating keypresses and pointer clicks. The sequence of inputs and outputs resulting from the interaction is analyzed using grammatical inference techniques that can infer the likely internal states and transitions of the GUI based on the observations. Our framework handles a wide variety of user interfaces ranging from traditional pull down menus to interfaces built for mobile platforms such as Android and iOS. Furthermore, the automaton inferred by our approach can be used to check for potentially harmful patterns in the interface's internal state machine such as design inconsistencies (eg,. a keypress does not have the intended effect) and mode confusion that can make the interface hard to use. We describe an implementation of the framework and demonstrate its working on a variety of interfaces including the user-interface of a safety critical insulin infusion pump that is commonly used by type-1 diabetic patients.
[graphical user interfaces, Sikuli script, iOS, interface internal state machine, safety critical insulin infusion pump, mode confusion, pointer clicks, mobile platforms, authoring languages, grammatical inference techniques, keypresses, GUI, pull down menus, learning (artificial intelligence), Graphical user interfaces, Computer vision, Calculators, computer vision techniques, natural language processing, Insulin, Android, python-based scripting language, Pumps, Automata, computer vision, design inconsistencies, human computer interaction, type-1 diabetic patients]
Semantic smells and errors in access control models: A case study in PHP
2013 35th International Conference on Software Engineering
None
2013
Access control models implement mechanisms to restrict access to sensitive data from unprivileged users. Access controls typically check privileges that capture the semantics of the operations they protect. Semantic smells and errors in access control models stem from privileges that are partially or totally unrelated to the action they protect. This paper presents a novel approach, partly based on static analysis and information retrieval techniques, for the automatic detection of semantic smells and errors in access control models. Investigation of the case study application revealed 31 smells and 2 errors. Errors were reported to developers who quickly confirmed their relevance and took actions to correct them. Based on the obtained results, we also propose three categories of semantic smells and errors to lay the foundations for further research on access control smells in other systems and domains.
[Access control, Context, privilege checking, program diagnostics, information retrieval, static analysis, Information retrieval, Analytical models, semantic smells, security, code smells, Semantics, access control models, authorisation, sensitive data, information retrieval techniques, PHP, access control smells, unprivileged users, Logistics]
Practical semantic test simplification
2013 35th International Conference on Software Engineering
None
2013
We present a technique that simplifies tests at the semantic level. We first formalize the semantic test simplification problem, and prove it is NP-hard. Then, we propose a heuristic algorithm, SimpleTest, that automatically transforms a test into a simpler test, while still preserving a given property. The key insight of SimpleTest is to reconstruct an executable and simpler test that exhibits the given property from the original one. Our preliminary study on 7 real-world programs showed the usefulness of SimpleTest.
[Java, program debugging, practical semantic test simplification problem, program testing, Input variables, Debugging, Indexes, heuristic algorithm, optimisation, NP-hard problem, real-world programs, Semantics, Computer bugs, SimpleTest, Testing]
Understanding regression failures through test-passing and test-failing code changes
2013 35th International Conference on Software Engineering
None
2013
Debugging and isolating changes responsible for regression test failures are some of the most challenging aspects of modern software development. Automatic bug localization techniques reduce the manual effort developers spend examining code, for example, by focusing attention on the minimal subset of recent changes that results in the test failure, or on changes to components with most dependencies or highest churn. We observe that another subset of changes is worth the developers' attention: the complement of the maximal set of changes that does not produce the failure. While for simple, independent source-code changes, existing techniques localize the failure cause to a small subset of those changes, we find that when changes interact, the failure cause is often in our proposed subset and not in the subset existing techniques identify. In studying 45 regression failures in a large, open-source project, we find that for 87% of those failures, the complement of the maximal passing set of changes is different from the minimal failing set of changes, and that for 78% of the failures, our technique identifies relevant changes ignored by existing work. These preliminary results suggest that combining our ideas with existing techniques, as opposed to using either in isolation, can improve the effectiveness of bug localization tools.
[Java, program debugging, program testing, software development, independent source-code changes, public domain software, test-failing code changes, Debugging, Interference, Educational institutions, set theory, History, system recovery, Open source software, regression test failures, minimal failing change set, failure cause localization, test-passing code changes, automatic bug localization techniques, software isolation change, software debugging change, open-source project, maximal passing change set]
Temporal code completion and navigation
2013 35th International Conference on Software Engineering
None
2013
Modern IDEs make many software engineering tasks easier by automating functionality such as code completion and navigation. However, this functionality operates on one version of the code at a time. We envision a new approach that makes code completion and navigation aware of code evolution and enables them to operate on multiple versions at a time, without having to manually switch across these versions. We illustrate our approach on several example scenarios. We also describe a prototype Eclipse plugin that embodies our approach for code completion and navigation for Java code. We believe our approach opens a new line of research that adds a novel, temporal dimension for treating code in IDEs in the context of tasks that previously required manual switching across different code versions.
[Context, Java, Navigation, software prototyping, temporal code completion, Switches, IDE, History, Proposals, prototype Eclipse plugin, configuration management, Prototypes, temporal code navigation, integrated development environment, temporal dimension, Software, software engineering tasks, code evolution, Java code, functionality automating]
Situational awareness: Personalizing issue tracking systems
2013 35th International Conference on Software Engineering
None
2013
Issue tracking systems play a central role in ongoing software development; they are used by developers to support collaborative bug fixing and the implementation of new features, but they are also used by other stakeholders including managers, QA, and end-users for tasks such as project management, communication and discussion, code reviews, and history tracking. Most such systems are designed around the central metaphor of the &#x201C;issue&#x201D; (bug, defect, ticket, feature, etc.), yet increasingly this model seems ill fitted to the practical needs of growing software projects; for example, our analysis of interviews with 20 Mozilla developers who use Bugzilla heavily revealed that developers face challenges maintaining a global understanding of the issues they are involved with, and that they desire improved support for situational awareness that is difficult to achieve with current issue management systems. In this paper we motivate the need for personalized issue tracking that is centered around the information needs of individual developers together with improved logistical support for the tasks they perform. We also describe an initial approach to implement such a system - extending Bugzilla - that enhances a developer's situational awareness of their working context by providing views that are tailored to specific tasks they frequently perform; we are actively improving this prototype with input from Mozilla developers.
[logistical support, Target tracking, project management, software development, issue tracking system personalization, software development management, collaborative bug fixing, Electronic mail, software project, Bugzilla, Mozilla developers, issue management system, Computer bugs, Prototypes, online front-ends, Software, code review, developer situational awareness, information needs, Interviews, history tracking, Software engineering]
GROPG: A graphical on-phone debugger
2013 35th International Conference on Software Engineering
None
2013
Debugging mobile phone applications is hard, as current debugging techniques either require multiple computing devices or do not support graphical debugging. To address this problem we present GROPG, the first graphical on-phone debugger. We implement GROPG for Android and perform a preliminary evaluation on third-party applications. Our experiments suggest that GROPG can lower the overall debugging time of a comparable text-based on-phone debugger by up to 2/3.
[Java, program debugging, graphical user interfaces, multiple computing devices, Random access memory, Debugging, graphical debugging, mobile phone applications debugging, Standards, graphical on-phone debugger, Android, mobile computing, text-based on-phone debugger, GROPG, User interfaces, operating systems (computers), debugging, third-party applications, Smart phones, Mobile computing, mobile handsets]
Why did this code change?
2013 35th International Conference on Software Engineering
None
2013
When a developer works on code that is shared with other developers, she needs to know why the code has been changed in particular ways to avoid reintroducing bugs. A developer looking at a code change may have access to a short commit message or a link to a bug report which may provide detailed information about how the code changed but which often lacks information about what motivated the change. This motivational information can sometimes be found by piecing together information from a set of relevant project documents, but few developers have the time to find and read the right documentation. We propose the use of multi-document summarization techniques to generate a concise natural language description of why code changed so that a developer can choose the right course of action.
[program debugging, natural language processing, Natural languages, system documentation, code developer, documentation, Vectors, Electronic mail, bug report, natural language description, Logic gates, Feature extraction, Software, code change, multidocument summarization techniques, motivational information, project documents, Software engineering]
Deciphering the story of software development through frequent pattern mining
2013 35th International Conference on Software Engineering
None
2013
Software teams record their work progress in task repositories which often require them to encode their activities in a set of edits to field values in a form-based user interface. When others read the tasks, they must decode the schema used to write the activities down. We interviewed four software teams and found out how they used the task repository fields to record their work activities. However, we also found that they had trouble interpreting task revisions that encoded for multiple activities at the same time. To assist engineers in decoding tasks, we developed a scalable method based on frequent pattern mining to identify patterns of frequently co-edited fields that each represent a conceptual work activity. We applied our method to our two years of our interviewee's task repositories and were able to abstract 83,000 field changes into just 27 patterns that cover 95% of the task revisions. We used the 27 patterns to render the teams' tasks in web-based English newsfeeds and evaluated them with the product teams. The team agreed with most of our patterns and English interpretations, but outlined a number of improvements that we will incorporate into future work.
[form-based user interface, software development, Web-based English newsfeeds, pattern mining, Noise, data mining, software development management, mining software repositories, software teams, Vectors, user interfaces, Data mining, History, Itemsets, task decoding, task tracking, Software, task repositories, Pattern matching, pattern recognition, frequent pattern mining]
Liberating pair programming research from the oppressive driver/observer regime
2013 35th International Conference on Software Engineering
None
2013
The classical definition of pair programming (PP) describes it via two obvious roles: driver (the person currently having the keyboard) and observer (the other, alternatively called navigator). Although prior research has found some assumptions regarding these roles to be false, so far no alternative PP role model took hold. Instead, most PP research tacitly assumes the classical model to be true and thus PP to be no more difficult than solo programming. We perform qualitative research (using Grounded Theory Methodology) to find a more realistic role model, and have uncovered a suprising complexity: There are more than two roles, they are assumed and unassumed gradually, multiple roles can be held by one person at the same time, and some of their facets are subtle. Mastering this complexity requires specific PP skills beyond mere programming and communication skills. By ignoring such skills, previous PP studies (in particular the controlled experiments) have investigated a rather mixed bag of situations, which explains their heterogeneous results. The emerging result is that qualitative research on the PP process will lead to constructive behavioral advice (process patterns) for pair members and to more meaningful designs for quantitative PP research.
[Context, constructive behavioral advice, program debugging, Navigation, grounded theory methodology, quantitative PP research, Switches, Observers, process patterns, programming skills, pair programming research, Programming profession, parallel programming, PP role model, oppressive driver-observer regime, Keyboards, observers, communication skills]
Pricing crowdsourcing-based software development tasks
2013 35th International Conference on Software Engineering
None
2013
Many organisations have turned to crowdsource their software development projects. This raises important pricing questions, a problem that has not previously been addressed for the emerging crowdsourcing development paradigm. We address this problem by introducing 16 cost drivers for crowdsourced development activities and evaluate 12 predictive pricing models using 4 popular performance measures. We evaluate our predictive models on TopCoder, the largest current crowdsourcing platform for software development. We analyse all 5,910 software development tasks (for which partial data is available), using these to extract our proposed cost drivers. We evaluate our predictive models using the 490 completed projects (for which full details are available). Our results provide evidence to support our primary finding that useful prediction quality is achievable (Pred(30)&gt;0.8). We also show that simple actionable advice can be extracted from our models to assist the 430,000 developers who are members of the TopCoder software development market.
[pricing crowdsourcing, crowdsourcing, Unified modeling language, Linear regression, software development management, Predictive models, software development project, Educational institutions, crowdsourcing development, TopCoder, predictive pricing model, Pricing, software measurement, Software, software development task, pricing, Software engineering]
Building test suites in social coding sites by leveraging drive-by commits
2013 35th International Conference on Software Engineering
None
2013
GitHub projects attract contributions from a community of users with varying coding and quality assurance skills. Developers on GitHub feel a need for automated tests and rely on test suites for regression testing and continuous integration. However, project owners report to often struggle with implementing an exhaustive test suite. Convincing contributors to provide automated test cases remains a challenge. The absence of an adequate test suite or using tests of low quality can degrade the quality of the software product. We present an approach for reducing the effort required by project owners for extending their test suites. We aim to utilize the phenomenon of drive-by commits: capable users quickly and easily solve problems in others' projects - even though they are not particularly involved in that project - and move on. By analyzing and directing the drive-by commit phenomenon, we hope to use crowdsourcing to improve projects' quality assurance efforts. Valuable test cases and maintenance tasks would be completed by capable users, giving core developers more resources to work on the more complicated issues.
[continuous integration, program testing, crowdsourcing, Communities, regression testing, GitHub projects, Companies, automated test suites, Encoding, software quality, software maintenance, software maintenance task, social coding sites, quality assurance skills, Quality assurance, drive-by commits phenomenon, Collaboration, quality assurance, social networking (online), project quality assurance improvement, Software, coding skills, software product quality, Testing]
Supporting application development with structured queries in the cloud
2013 35th International Conference on Software Engineering
None
2013
To facilitate software development for multiple, federated cloud systems, abstraction layers have been introduced to mask the differences in the offerings, APIs, and terminology of various cloud providers. Such layers rely on a common ontology, which a) is difficult to create, and b) requires developers to understand both the common ontology and how various providers deviate from it. In this paper we propose and describe a structured query language for the cloud, Cloud SQL, along with a system and methodology for acquiring and organizing information from cloud providers and other entities in the cloud ecosystem such that it can be queried. It allows developers to run queries on data organized based on their semantic understanding of the cloud. Like the original SQL, we believe the use of a declarative query language will reduce development costs and make the multi-cloud accessible to a broader set of developers.
[Cloud computing, application program interfaces, information acquisition, structured queries, Ontologies, query languages, Database languages, abstraction layers, Cloud SQL, multiple cloud systems, Semantics, software engineering, application development support, cloud computing, Monitoring, ontology, software development, cloud ecosystem, adaptive systems, SQL, information organization, declarative query language, structured query language, Data models, API, federated cloud systems]
Hunting for smells in natural language tests
2013 35th International Conference on Software Engineering
None
2013
Tests are central artifacts of software systems and play a crucial role for software quality. In system testing, a lot of test execution is performed manually using tests in natural language. However, those test cases are often poorly written without best practices in mind. This leads to tests which are not maintainable, hard to understand and inefficient to execute. For source code and unit tests, so called code smells and test smells have been established as indicators to identify poorly written code. We apply the idea of smells to natural language tests by defining a set of common Natural Language Test Smells (NLTS). Furthermore, we report on an empirical study analyzing the extent in more than 2800 tests of seven industrial test suites.
[Measurement, program testing, test smells, natural language processing, Natural languages, software systems, Cloning, Manuals, source code, Maintenance engineering, industrial test suites, system testing, software quality, unit tests, code smells, natural language test smells, NLTS, Quality assessment, natural language, Testing]
Bottom-up model-driven development
2013 35th International Conference on Software Engineering
None
2013
Prominent researchers and leading practitioners are questioning the long-term viability of model-driven development (MDD). Finkelstein recently ranked MDD as a bottom-ten research area, arguing that an approach based entirely on development and refinement of abstract representations is untenable. His view is that working with concrete artifacts is necessary for learning what to build and how to build it. What if this view is correct? Could MDD be rescued from such a critique? We suggest the answer is yes, but that it requires an inversion of traditional views of transformational MDD. Rather than develop complete, abstract system models, in ad-hoc modeling languages, followed by top-down synthesis of hidden concrete artifacts, we envision that engineers will continue to develop concrete artifacts, but over time will recognize patterns and concerns that can profitably be lifted, from the bottom-up, to the level of partial models, in general-purpose specification languages, from which visible concrete artifacts are generated, becoming part of the base of both concrete and abstract artifacts for subsequent rounds of development. This paper reports on recent work that suggests this approach is viable, and explores ramifications of such a rethinking of MDD. Early validation flows from experience applying these ideas to a healthcare-related experimental system in our lab.
[Productivity, long-term viability, bottom-up model-driven development, Object oriented modeling, Metals, Bottom-up, MDD, ad-hoc modeling languages, Partial synthesis, software architecture, healthcare-related experimental system, Abstracts, specification languages, abstract representations, general-purpose specification languages, Concrete, Software, simulation languages, Software engineering, Model-driven development]
An approach for restructuring text content
2013 35th International Conference on Software Engineering
None
2013
Software engineers have successfully used Natural Language Processing for refactoring source code. Conversely, in this paper we investigate the possibility to apply software refactoring techniques to textual content. As a procedural program is composed of functions calling each other, a document can be modeled as content fragments connected each other through links. Inspired by software engineering refactoring strategies, we propose an approach for refactoring wiki content. The approach has been applied to the EMF category of Eclipsepedia with encouraging results.
[Electronic publishing, text analysis, functions, wiki content refactoring, Concept Location, software engineering refactoring strategies, Reverse Engineering, content management, procedural program, EMF category, Natural language processing, Refactoring, Eclipsepedia, natural language processing, text content restructuring, source code refactoring, Documentation, Reengineering, Information services, Wiki, Software, content fragments, Internet, Web sites, Software engineering]
A case for human-driven software development
2013 35th International Conference on Software Engineering
None
2013
Human-Computer Interaction (HCI) plays a critical role in software systems, especially when targeting vulnerable individuals (e.g., assistive technologies). However, there exists a gap between well-tooled software development methodologies and HCI techniques, which are generally isolated from the development toolchain and require specific expertise. In this paper, we propose a human-driven software development methodology making User Interface (UI) a full-fledged dimension of software design. To make this methodology useful in practice, a UI design language and a user modeling language are integrated into a tool suite that guides the stakeholders during the development process, while ensuring the conformance between the UI design and its implementation.
[Context, user modeling language, Computational modeling, software design, Programming, HCI techniques, human-driven software development methodology, Electronic mail, user interfaces, tool suite, user interface, Human computer interaction, human-computer interaction, UI design language, Abstracts, Software, human computer interaction, simulation languages, software engineering]
A framework for managing cloned product variants
2013 35th International Conference on Software Engineering
None
2013
We focus on the problem of managing a collection of related software products realized via cloning. We contribute a framework that explicates operators required for developing and maintaining such products, and demonstrate their usage on two concrete scenarios observed in industrial settings: sharing of features between cloned variants and re-engineering the variants into &#x201C;single-copy&#x201D; representations advocated by software product line engineering approaches. We discuss possible implementations of the operators, including synergies with existing work developed in seemingly unrelated contexts, with the goal of helping understand and structure existing work and identify opportunities for future research.
[Frequency selective surfaces, Context, Cloning, software development management, related software product collection management, Iron, software product line engineering approaches, software maintenance, Semantics, software product maintenance, Feature extraction, software re-engineering, software product development, Software, cloned product variant management, single-copy representations]
Sketching software in the wild
2013 35th International Conference on Software Engineering
None
2013
This paper argues that understanding how professional software developers use diagrams and sketches in their work is an underexplored terrain. We illustrate this by summarizing a number of studies on sketching and diagramming across a variety of domains, and arguing for their limited generalizability. In order to develop further insight, we describe the design of a research project we are embarking upon and its grounding theoretical assumptions.
[Context, Visualization, professional software developers, Educational institutions, Cognition, diagrams, distributed cognition, Diagramming, interaction analysis, Presses, sketching, diagramming, Employment, Software, software engineering, research project, software sketching]
On extracting unit tests from interactive live programming sessions
2013 35th International Conference on Software Engineering
None
2013
Software engineering methodologies, such as unit testing, propose that any effort made to ensuring that programs run correctly should be captured in repeatable and automated artifacts. However, when looking at developer activities on a spectrum from exploratory testing to scripted testing we find that many engineering activities include bursts of exploratory testing. In this paper we propose to leverage these exploratory testing bursts by automatically extracting scripted tests from a recording of live programming sessions. In order to do so, we wiretap the development environment so we can record all program input, all user-issued functions calls, and all program output of an exploratory testing session. We propose to then use clustering to extract scripted test cases from these recordings. We outline two early-stage prototypes, one for a static and one for a dynamic language. And we outline how this idea fits into the bigger research direction of live programming.
[Context, Java, program testing, unit testing, software engineering methodologies, Programming profession, interactive live programming sessions, exploratory testing, live programming research direction, Prototypes, scripted testing, user-issued functions calls, interactive systems, Software, software engineering, Testing]
Towards automated testing and fixing of re-engineered Feature Models
2013 35th International Conference on Software Engineering
None
2013
Mass customization of software products requires their efficient tailoring performed through combination of features. Such features and the constraints linking them can be represented by Feature Models (FMs), allowing formal analysis, derivation of specific variants and interactive configuration. Since they are seldom present in existing systems, techniques to re-engineer FMs have been proposed. There are nevertheless error-prone and require human intervention. This paper introduces an automated search-based process to test and fix FMs so that they adequately represent actual products. Preliminary evaluation on the Linux kernel FM exhibit erroneous FM constraints and significant reduction of the inconsistencies.
[Context, operating system kernels, Frequency modulation, program testing, program verification, Computational modeling, erroneous FM constraint, mass customization, reengineered feature model, automated testing, automated search-based process, formal specification, feature combination, interactive configuration, Linux, Search-based, Feature Model, formal analysis, Linux kernel FM, software product, Kernel, Fixing, Testing]
Computational alignment of goals and scenarios for complex systems
2013 35th International Conference on Software Engineering
None
2013
The purpose of requirements validation is to determine whether a large requirements set will lead to the achievement of system-related goals under different conditions - a task that needs automation if it is to be performed quickly and accurately. One reason for the current lack of software tools to undertake such validation is the absence of the computational mechanisms needed to associate scenario, system specification and goal analysis tools. Therefore, in this paper, we report first research experiments in developing these new capabilities, and demonstrate them with a non-trivial example associated with a Rolls Royce aircraft engine software component.
[Rolls Royce aircraft engine software component, goal achievement, object-oriented programming, Computational modeling, Unified modeling language, operational requirements, Requirements validation, Control systems, goal computational alignment, system specification, scenario computational alignment, scenarios, formal specification, Engines, aerospace engines, large-scale systems, Shafts, Analytical models, complex systems, goal analysis tools, Sensors, software tools, requirements validation, system-related goals]
Service networks for development communities
2013 35th International Conference on Software Engineering
None
2013
Communities of developers have rapidly become global, encompassing multiple timezones and cultures alike. In previous work we investigated the possible shapes of communities for software development. In addition, we explored mechanisms to uncover communities emerging during development. However, we barely scratched the surface. We found that development communities yield properties of dynamic change and organic evolution. Much work is still needed to support such communities with mechanisms able to proactively react to community dynamism. We argue that service-networks can be used to deliver this support. Service-networks are sets of people and information brought together by the internet. This paper is a first attempt at studying this research area by means of a real-life case-study in a large global software development organisation.
[organic evolution, Communities, development community, Companies, Educational institutions, service networks, software development organisation, Communication channels, community dynamism, Software, software engineering, Internet, Software engineering]
Formal specifications better than function points for code sizing
2013 35th International Conference on Software Engineering
None
2013
Size and effort estimation is a significant challenge for the management of large-scale formal verification projects. We report on an initial study of relationships between the sizes of artefacts from the development of seL4, a formally-verified embedded systems microkernel. For each API function we first determined its COSMIC Function Point (CFP) count (based on the seL4 user manual), then sliced the formal specifications and source code, and performed a normalised line count on these artefact slices. We found strong and significant relationships between the sizes of the artefact slices, but no significant relationships between them and the CFP counts. Our finding that CFP is poorly correlated with lines of code is based on just one system, but is largely consistent with prior literature. We find CFP is also poorly correlated with the size of formal specifications. Nonetheless, lines of formal specification correlate with lines of source code, and this may provide a basis for size prediction in future formal verification projects. In future work we will investigate proof sizing.
[formally-verified embedded systems microkernel, operating system kernels, application program interfaces, program verification, Estimation, software development management, Manuals, source code, COSMIC Function Point, Size measurement, Complexity theory, formal specification, formal specifications, large-scale formal verification projects, seL4, code sizing, Embedded systems, embedded systems, Abstracts, program slicing, API function]
Using mutation analysis for a model-clone detector comparison framework
2013 35th International Conference on Software Engineering
None
2013
Model-clone detection is a relatively new area and there are a number of different approaches in the literature. As the area continues to mature, it becomes necessary to evaluate and compare these approaches and validate new ones that are introduced. We present a mutation-analysis based model-clone detection framework that attempts to automate and standardize the process of comparing multiple Simulink model-clone detection tools or variations of the same tool. By having such a framework, new research directions in the area of model-clone detection can be facilitated as the framework can be used to validate new techniques as they arise. We begin by presenting challenges unique to model-clone tool comparison including recall calculation, the nature of the clones, and the clone report representation. We propose our framework, which we believe addresses these challenges. This is followed by a presentation of the mutation operators that we plan to inject into our Simulink models that will introduce variations of all the different model clone types that can then be searched for by each respective model-clone detector.
[Adaptation models, Computational modeling, clone nature, Cloning, recall calculation, clone report representation, Analytical models, Software packages, Layout, mutation operators, mutation analysis, model clone type search, model-clone detector comparison framework, Detectors, software engineering, Simulink model-clone detection tools]
On the relationships between domain-based coupling and code clones: An exploratory study
2013 35th International Conference on Software Engineering
None
2013
Knowledge of similar code fragments, also known as code clones, is important to many software maintenance activities including bug fixing, refactoring, impact analysis and program comprehension. While a great deal of research has been conducted for finding techniques and implementing tools to identify code clones, little research has been done to analyze the relationships between code clones and other aspects of software. In this paper, we attempt to uncover the relationships between code clones and coupling among domain-level components. We report on a case study of a large-scale open source enterprise system, where we demonstrate that the probability of finding code clones among components with domain-based coupling is more than 90%. While such a probabilistic view does not replace a clone detection tool per se, it certainly has the potential to complement the existing tools by providing the probability of having code clones between software components. For example, it can both reduce the clone search space and provide a flexible and language independent way of focusing only on a specific part of the system. It can also provide a higher level of abstraction to look at the cloning relationships among software components.
[program comprehension, program debugging, domain based coupling, Cloning, Manuals, open source enterprise system, Maintenance engineering, code fragments, bug fixing, software maintenance, Couplings, impact analysis, code clones, software maintenance activities, domain level components, security of data, refactoring, cloning relationships, Computer bugs, clone detection tool, User interfaces, Software, software components]
Quantitative program slicing: Separating statements by relevance
2013 35th International Conference on Software Engineering
None
2013
Program slicing is a popular but imprecise technique for identifying which parts of a program affect or are affected by a particular value. A major reason for this imprecision is that slicing reports all program statements possibly affected by a value, regardless of how relevant to that value they really are. In this paper, we introduce quantitative slicing (q-slicing), a novel approach that quantifies the relevance of each statement in a slice. Q-slicing helps users and tools focus their attention first on the parts of slices that matter the most. We present two methods for quantifying slices and we show the promise of q-slicing for a particular application: predicting the impacts of changes.
[quantitative program slicing, Sensitivity analysis, Debugging, Educational institutions, q-slicing approach, History, statement relevance, program statements, Runtime, program analyses, slicing reports, Semantics, program identification, program slicing, Testing]
Example-Driven Modeling: Model &#x003D; Abstractions &#x002B; Examples
2013 35th International Conference on Software Engineering
None
2013
We propose Example-Driven Modeling (EDM), an approach that systematically uses explicit examples for eliciting, modeling, verifying, and validating complex business knowledge. It emphasizes the use of explicit examples together with abstractions, both for presenting information and when exchanging models. We formulate hypotheses as to why modeling should include explicit examples, discuss how to use the examples, and the required tool support. Building upon results from cognitive psychology and software engineering, we challenge mainstream practices in structural modeling and suggest future directions.
[program testing, program verification, cognitive psychology, Unified modeling language, knowledge management, EDM, Systematics, information presentation, example-driven modeling, complex business knowledge validation, complex business knowledge verification, complex business knowledge modeling, complex business knowledge elicition, software engineering, software tools, Testing, Computational modeling, software testing, structural modeling, Grammar, abstractions, tool support, explicit examples, Software, Concrete, business data processing]
Towards recognizing and rewarding efficient developer work patterns
2013 35th International Conference on Software Engineering
None
2013
Software engineering researchers develop great techniques consisting of practices and tools that improve efficiency and quality of software development. Prior work evaluates developers' use of techniques such as Test-Driven-Development and refactoring by measuring actions in the development environment. What we still lack is a method to communicate effectively and motivate developers to adopt best practices and tools. This work proposes a game-like system to motivate adoption while continuously measuring developers' use of more efficient development techniques.
[game-like system, Navigation, program testing, software development management, software developer work pattern recognition, test driven development, Browsers, software quality, Data mining, software maintenance, Best practices, software developer motivation, software tool, Software, software engineering, software tools, software development quality improvement, software refactoring, Monitoring, Software engineering]
Selecting checkpoints along the time line: A novel temporal checkpoint selection strategy for monitoring a batch of parallel business processes
2013 35th International Conference on Software Engineering
None
2013
Nowadays, most business processes are running in a parallel, distributed and time-constrained manner. How to guarantee their on-time completion is a challenging issue. In the past few years, temporal checkpoint selection which selects a subset of workflow activities for verification of temporal consistency has been proved to be very successful in monitoring single, complex and large size scientific workflows. An intuitive approach is to apply those strategies to individual business processes. However, in such a case, the total number of checkpoints will be enormous, namely the cost for system monitoring and exception handling could be excessive. To address such an issue, we propose a brand new idea which selects time points along the workflow execution time line as checkpoints to monitor a batch of parallel business processes simultaneously instead of individually. Based on such an idea, a set of new definitions as well as a time-point based checkpoint selection strategy are presented in this paper. Our preliminary results demonstrate that it can achieve an order of magnitude reduction in the number of checkpoints while maintaining satisfactory on-time completion rates compared with the state-of-the-art activity-point based checkpoint selection strategy.
[checkpointing, Cloud computing, parallel business process monitoring, program verification, exception handling, Throughput, Checkpoint Selection, parallel processing, Parallel Processes, temporal consistency verification, Runtime, Temporal Verification, scientific workflow monitoring, temporal checkpoint selection strategy, system monitoring, on-time completion, Time factors, Monitoring, business data processing, Business]
LambdaFicator: From imperative to functional programming through automated refactoring
2013 35th International Conference on Software Engineering
None
2013
Java 8 introduces two functional features: lambda expressions and functional operations like map or filter that apply a lambda expression over the elements of a Collection. Refactoring existing code to use these new features enables explicit but unobtrusive parallelism and makes the code more succinct. However, refactoring is tedious (it requires changing many lines of code) and error-prone (the programmer must reason about the control-flow, data-flow, and side-effects). Fortunately, these refactorings can be automated. We present LambdaFicator, a tool which automates two refactorings. The first refactoring converts anonymous inner classes to lambda expressions. The second refactoring converts for loops that iterate over Collections to functional operations that use lambda expressions. In 9 open-source projects we have applied these two refactorings 1263 and 1595 times, respectively. The results show that LambdaFicator is useful. A video highlighting the main features can be found at: http://www.youtube.com/watch?v=EIyAflgHVpU.
[Java, functional programming, Educational institutions, software maintenance, LambdaFicator, automated refactoring, Open source software, Java 8, lambda expressions, Semantics, imperative programming, Parallel processing, object-oriented languages, Libraries, functional operations, Functional programming]
JITTAC: A Just-in-Time tool for architectural consistency
2013 35th International Conference on Software Engineering
None
2013
Architectural drift is a widely cited problem in software engineering, where the implementation of a software system diverges from the designed architecture over time causing architecture inconsistencies. Previous work suggests that this architectural drift is, in part, due to programmers' lack of architecture awareness as they develop code. JITTAC is a tool that uses a real-time Reflexion Modeling approach to inform programmers of the architectural consequences of their programming actions as, and often just before, they perform them. Thus, it provides developers with Just-In-Time architectural awareness towards promoting consistency between the as-designed architecture and the as-implemented system. JITTAC also allows programmers to give real-time feedback on introduced inconsistencies to the architect. This facilitates programmer-driven architectural change, when validated by the architect, and allows for more timely team-awareness of the actual architectural consistency of the system. Thus, it is anticipated that the tool will decrease architectural inconsistency over time and improve both developers' and architect's knowledge of their software's architecture. The JITTAC demo is available at: http://www.youtube.com/watch?v=BNqhp40PDD4.
[Navigation, software architecture consistency, Software architecture discovery, real-time reflexion modeling approach, architectural drift, Reverse Engineering, team-awareness, Analytical models, software architecture, Software architecture, JITTAC, programmer-driven architectural change, Prototypes, compliance, Computer architecture, architectural awareness, Real-time systems, Software, software engineering, architectural consistency, just-in-time tool]
Seahawk: Stack Overflow in the IDE
2013 35th International Conference on Software Engineering
None
2013
Services, such as Stack Overflow, offer a web platform to programmers for discussing technical issues, in form of Question and Answers (Q&amp;A). Since Q&amp;A services store the discussions, the generated &#x201C;crowd knowledge&#x201D; can be accessed and consumed by a large audience for a long time. Nevertheless, Q&amp;A services are detached from the development environments used by programmers: Developers have to tap into this crowd knowledge through web browsers and cannot smoothly integrate it into their workflow. This situation hinders part of the benefits of Q&amp;A services. To better leverage the crowd knowledge of Q&amp;A services, we created Seahawk, an Eclipse plugin that supports an integrated and largely automated approach to assist programmers using Stack Overflow. Seahawk formulates queries automatically from the active context in the IDE, presents a ranked and interactive list of results, lets users import code samples in discussions through drag &amp; drop and link Stack Overflow discussions and source code persistently as a support for team work. Video Demo URL: http://youtu.be/DkqhiU9FYPI.
[Navigation, system documentation, question and answers services, Documentation, Programming, stack overflow, IDE, Q&amp;A services, Servers, crowd knowledge, Engines, Sockets, XML, knowledge based systems, Seahawk, Eclipse plugin, Internet, Web platform]
DRC: A detection tool for dangling references in PHP-based web applications
2013 35th International Conference on Software Engineering
None
2013
PHP is a server-side language that is widely used for creating dynamic Web applications. However, as a dynamic language, PHP may induce certain programming errors that reveal themselves only at run time. A common type of error is dangling references, which occur if the referred program entities have not been declared in the current program execution. To prevent the run-time errors caused by such dangling references, we introduce Dangling Reference Checker (DRC), a novel tool to statically detect those references in the source code of PHP-based Web applications. DRC first identifies the path constraints of the program executions in which a program entity appears and then matches the path constraints of the entity's declarations and references to detect dangling ones. DRC is able to detect dangling reference errors in several real-world PHP systems with high accuracy. The video demonstration for DRC is available at http://www.youtube.com/watch?v=3Dy_AKZYhLlU4.
[Dictionaries, program verification, dangling reference error detection, dangling reference checker, Programming, HTML, Servers, Approximation methods, Accuracy, Databases, authoring languages, dangling reference detection tool, dynamic Web applications, program entity declarations, program entity references, server-side language, source coding, path constraints, source code, program execution, real-world PHP systems, run-time errors, DRC, PHP-based Web applications, programming errors, Internet, video demonstration]
TestEvol: A tool for analyzing test-suite evolution
2013 35th International Conference on Software Engineering
None
2013
Test suites, just like the applications they are testing, evolve throughout their lifetime. One of the main reasons for test-suite evolution is test obsolescence: test cases cease to work because of changes in the code and must be suitably repaired. There are several reasons why it is important to achieve a thorough understanding of how test cases evolve in practice. In particular, researchers who investigate automated test repair - an increasingly active research area - can use such understanding to develop more effective repair techniques that can be successfully applied in real-world scenarios. More generally, analyzing testsuite evolution can help testers better understand how test cases are modified during maintenance and improve the test evolution process, an extremely time consuming activity for any nontrivial test suite. Unfortunately, there are no existing tools that facilitate investigation of test evolution. To tackle this problem, we developed TestEvol, a tool that enables the systematic study of test-suite evolution for Java programs and JUnit test cases. This demonstration presents TestEvol and illustrates its usefulness and practical applicability by showing how TestEvol can be successfully used on real-world software and test suites. Demo video at http://www.cc.gatech.edu/~orso/software/testevol/.
[Software testing, Java, object-oriented programming, program testing, Maintenance engineering, test-suite evolution analysis tool, software maintenance, Runtime, repair technique, automated test repair, Software systems, TestEvol, Java programs, JUnit test case, Graphical user interfaces]
Query quality prediction and reformulation for source code search: The Refoqus tool
2013 35th International Conference on Software Engineering
None
2013
Developers search source code frequently during their daily tasks, to find pieces of code to reuse, to find where to implement changes, etc. Code search based on text retrieval (TR) techniques has been widely used in the software engineering community during the past decade. The accuracy of the TR-based search results depends largely on the quality of the query used. We introduce Refoqus, an Eclipse plugin which is able to automatically detect the quality of a text retrieval query and to propose reformulations for it, when needed, in order to improve the results of TR-based code search. A video of Refoqus is found online at http://www.youtube.com/watch?v=UQlWGiauyk4.
[Context, Refoqus tool, text retrieval technique, Text Retrieval, Software maintenance, source code search, Query Reformulation, text retrieval query, TR-based search, Training, query quality prediction, query reformulation, Training data, Feature extraction, Software systems, software engineering, Eclipse plugin, Source Code Search, Query Quality, query formulation, Software engineering]
A large scale Linux-Kernel based benchmark for feature location research
2013 35th International Conference on Software Engineering
None
2013
Many software maintenance tasks require locating code units that implement a certain feature (termed as feature location). Feature location has been an active research area for more than two decades. However, there is lack of publicly available, large scale benchmarks for e valuating and comparing feature location approaches. In this paper, we present a LinuxKernel based benchmark for feature location research. This benchmark is large scale and extensible. By providing rich feature and program information and accurate ground-truth links between features and code units, it supports the e valuation of a wide range of feature location approaches. It allows researchers to gain deeper insights into existing approaches and how they can be improved. It also enables communication and collaboration among different researchers. (video: http://www.youtube.com/watch?v=3D_HihwRNeK3I).
[code unit location, ground-truth links, public domain software, Natural languages, Random access memory, Feature Location, software maintenance, publicly available large-scale benchmarks, Empirical Studies, feature location research, large-scale Linux-kernel-based benchmark, Linux, software maintenance tasks, Large-Scale Benchmark, Benchmark testing, Feature extraction, benchmark testing, program information, Kernel, Linux Kernel]
NavClus: A graphical recommender for assisting code exploration
2013 35th International Conference on Software Engineering
None
2013
Recently, several graphical tools have been proposed to help developers avoid becoming disoriented when working with large software projects. These tools visualize the locations that developers have visited, allowing them to quickly recall where they have already visited. However, developers also spend a significant amount of time exploring source locations to visit, which is a task that is not currently supported by existing tools. In this work, we propose a graphical code recommender NavClus, which helps developers find relevant, unexplored source locations to visit. NavClus operates by mining a developer's daily interaction traces, comparing the developer's current working context with previously seen contexts, and then predicting relevant source locations to visit. These locations are displayed graphically along with the already explored locations in a class diagram. As a result, with NavClus developers can quickly find, reach, and focus on source locations relevant to their working contexts. http://www.youtube.com/watch?v=rbrc5ERyWjQ.
[Context, graphical code recommender, Visualization, NavClus, developer working context, class diagram, Navigation, Unified Modeling Language, graphical tools, code exploration, data mining, source locations, diagrams, Engines, recommender systems, data visualisation, Position measurement, User interfaces, developer daily interaction trace mining, software projects, Monitoring]
LASE: An example-based program transformation tool for locating and applying systematic edits
2013 35th International Conference on Software Engineering
None
2013
Adding features and fixing bugs in software often require systematic edits which are similar, but not identical, changes to many code locations. Finding all edit locations and editing them correctly is tedious and error-prone. In this paper, we demonstrate an Eclipse plug-in called Lase that (1) creates context-aware edit scripts from two or more examples, and uses these scripts to (2) automatically identify edit locations and (3) transform the code. In Lase, users can view syntactic edit operations and corresponding context for each input example. They can also choose a different subset of the examples to adjust the abstraction level of inferred edits. When Lase locates target methods matching the inferred edit context and suggests customized edits, users can review and correct LASE's edit suggestion. These features can reduce developers' burden in repetitively applying similar edits to different methods. The tool's video demonstration is available at https://www.youtube.com/ watch?v=npDqMVP2e9Q.
[Context, code transformation, systematic edits, program diagnostics, LASE, Cloning, edit location identification, Eclipse plug-in, ubiquitous computing, context-aware edit scripts, example-based program transformation tool, edit suggestion, Systematics, edit context, Abstracts, Syntactics, customized edits, Software, Concrete, software engineering, software tools, inferred edit abstraction level, syntactic edit operations]
CEL: Modeling everywhere
2013 35th International Conference on Software Engineering
None
2013
The design of object-oriented systems starts with modeling, a process to identify core concepts and their relations. Mainstream modeling techniques can be either informal (white board, CRC cards, etc.) or formal (e.g., UML editors). The former support well the creative modeling process, but their output is difficult to store, process and maintain. The latter reduce these problems, at the expense of creativity and productivity because they are tedious and not trivial to use. We present Cel, a touch- and gesture-based iPad application to rapidly create, manipulate, and store language agnostic object-oriented software models, based on a minimal set of constructs. Demo video URL: http://youtu.be/icQVS6w0jTE.
[mainstream modeling, Visualization, gesture-based iPad application, creative modeling process, Object oriented modeling, Computational modeling, Unified modeling language, language agnostic object oriented software models, Tablet computers, CEL, formal specification, modelling, touch-based iPad application, productivity, object oriented systems, Software, object-oriented methods, creativity, Software engineering]
V:Issue:lizer: Exploring requirements clarification in online communication over time
2013 35th International Conference on Software Engineering
None
2013
This demo introduces V:ISSUE:LIZER, a tool for exploring online communication and analyzing clarification of requirements over time. V:Issue:lizer supports managers and developers to identify requirements with insufficient shared understanding, to analyze communication problems, and to identify developers that are knowledgeable about domain or project related issues through visualizations. Our preliminary evaluation shows that V:Issue:lizer offers managers valuable information for their decision making. (Demo video: http://youtu.be/Oy3xvzjy3BQ).
[Knowledge engineering, Visualization, Social network services, domain related issues, distributed requirements engineering, formal specification, project related issues, online communication, communication of requirements, requirements clarification analysis, Collaboration, Data visualization, data visualisation, V:ISSUE:LIZER tool, computer mediated communication, groupware, decision making, Software, Trajectory, data visualization, requirements clarification patterns]
YODA: Young and newcOmer Developer Assistant
2013 35th International Conference on Software Engineering
None
2013
Mentoring project newcomers is a crucial activity in software projects, and requires to identify people having good communication and teaching skills, other than high expertise on specific technical topics. In this demo we present Yoda (Young and newcOmer Developer Assistant), an Eclipse plugin that identifies and recommends mentors for newcomers joining a software project. Yoda mines developers' communication (e.g., mailing lists) and project versioning systems to identify mentors using an approach inspired to what ArnetMiner does when mining advisor/student relations. Then, it recommends appropriate mentors based on the specific expertise required by the newcomer. The demo shows Yoda in action, illustrating how the tool is able to identify and visualize mentoring relations in a project, and suggest appropriate mentors for a developer who is going to work on certain source code files, or on a given topic. Demo URL: http://youtu.be/4yrbYT-LAXA.
[data mining, software management, Electronic mail, Data mining, project versioning systems, Presses, source code files, Mining Software Repositories, mentoring relations identification, Developer Mentoring, Developer Recommenders, employee welfare, project management, ArnetMiner, young and newcomer developer assistant tool, software project, developers communication, Employee welfare, Collaboration, mentoring relations visualization, Software, Eclipse plugin, advisor-student relations, Yoda tool, Software engineering]
RADAR: A tool for debugging regression problems in C/C&#x002B;&#x002B; Software
2013 35th International Conference on Software Engineering
None
2013
Multiple tools can assist developers when debugging programs, but only a few solutions specifically target the common case of regression failures, to provide a more focused and effective support to debugging. In this paper we present RADAR, a tool that combines change identification and dynamic analysis to automatically explain regression problems with a list of suspicious differences in the behavior of the base and upgraded version of a program. The output produced by the tool is particularly beneficial to understand why an application failed. A demo video is available at http://www.youtube.com/watch?v=DMGUgALG-yE.
[RADAR tool, program debugging, debugging regression problems, Law, program diagnostics, program upgraded version, Debugging, regression analysis, dynamic analysis, program base version, C++ language, C++ software, Remuneration, software maintenance, C software, change identification, regression failure, Radar, Software, Monitoring]
MCT: A tool for commenting programs by multimedia comments
2013 35th International Conference on Software Engineering
None
2013
Program comments have always been the key to understanding code. However, typical text comments can easily become verbose or evasive. Thus sometimes code reviewers find an audio or video code narration quite helpful. In this paper, we present our tool, called MCT (Multimedia Commenting Tool), which is an integrated development environment-based tool that enables programmers to easily explain their code by voice, video and mouse movement in the form of comments. With this tool, programmers can replay the audio or video when they feel like. A demonstration video can be accessed at: http://www.youtube.com/watch?v=tHEHqZme4VE.
[Java, software reuse, audio code narration, integrated development environment-based tool, program comments, Multimedia communication, multimedia computing, software maintenance, multimedia comments, multimedia commenting tool, software system development, video code narration, integrated software, Speech recognition, Streaming media, software reusability, Speech, Mice, Software, MCT]
Memoise: A tool for memoized symbolic execution
2013 35th International Conference on Software Engineering
None
2013
This tool paper presents a tool for performing memoized symbolic execution (Memoise), an approach we developed in previous work for more efficient application of symbolic execution. The key idea in Memoise is to allow re-use of symbolic execution results across different runs of symbolic execution without having to re-compute previously computed results as done in earlier approaches. Specifically, Memoise builds a trie-based data structure to record path exploration information during a run of symbolic execution, optimizes the trie for the next run, and re-uses the resulting trie during the next run. Our tool optimizes symbolic execution in three standard scenarios where it is commonly applied: iterative deepening, regression analysis, and heuristic search. Our tool Memoise builds on the Symbolic PathFinder framework to provide more efficient symbolic execution of Java programs and is available online for download. The tool demonstration video is available at http://www.youtube.com/watch?v=ppfYOB0Z2vY.
[Java, iterative methods, path exploration information, tool demonstration video, memoized symbolic execution, program verification, NASA, symbolic PathFinder framework, regression analysis, Data structures, Regression analysis, Memoise, Standards, heuristic search, iterative deepening, trie-based data structure, data structures, Iterative methods, Java programs, Testing]
Controller synthesis: From modelling to enactment
2013 35th International Conference on Software Engineering
None
2013
Controller synthesis provides an automated means to produce architecture-level behaviour models that are enacted by a composition of lower-level software components, ensuring correct behaviour. Such controllers ensure that goals are satisfied for any model-consistent environment behaviour. This paper presents a tool for developing environment models, synthesising controllers efficiently, and enacting those controllers using a composition of existing third-party components. Video: www.youtube.com/watch?v=RnetgVihpV4.
[Adaptation models, object-oriented programming, Computational modeling, Controller Synthesis, Control systems, lower-level software components, LTS, model-consistent environment behaviour, controller synthesis, software architecture, architecture-level behaviour models, Reactive Planning, third-party components, Safety, Robots, Paints, Software engineering]
A study of variability spaces in open source software
2013 35th International Conference on Software Engineering
None
2013
Configurable software systems allow users to customize them according to their needs. Supporting such variability is commonly divided into three parts: configuration space, build space, and code space. In this research abstract, we describe our work in exploring what information these spaces contain in practice, and if this information is consistent. This involves investigating how these spaces work together to ensure that variability is correctly implemented, and to avoid any inconsistencies or anomalies. Our work identifies how variability is implemented in several configurable systems, and initially focuses on less studied parts such as the build system. Our goals include: 1) investigating what information each space provides, 2) quantifying the variability in the build system, 3) studying the effect of build system constraints on variability anomalies, and 4) analyzing how variability anomalies are introduced and fixed. Achieving these goals would help developers make informed decisions when designing variable software, and improve maintainability of existing configurable systems.
[Conferences, public domain software, open source software, Variability Anomalies, Aerospace electronics, software maintainability, configurable systems, Data mining, software maintenance, configurable software systems, software code space, software build space, Mining Software Repositories, Linux, Software Variability, Feature extraction, software configuration space, Kernel, build system]
Implementing database access control policy from unconstrained natural language text
2013 35th International Conference on Software Engineering
None
2013
Although software can and does implement access control at the application layer, failure to enforce data access at the data layer often allows uncontrolled data access when individuals bypass application controls. The goal of this research is to improve security and compliance by ensuring access controls rules explicitly and implicitly defined within unconstrained natural language texts are appropriately enforced within a system's relational database. Access control implemented in both the application and data layers strongly supports a defense in depth strategy. We propose a tool-based process to 1) parse existing, unaltered natural language documents; 2) classify whether or not a statement implies access control and whether or not the statement implies database design; and, as appropriate, 3) extract policy elements; 4) extract database design; 5) map data objects found in the text to a database schema; and 6) automatically generate the necessary SQL commands to enable the database to enforce access control. Our initial studies of the first three steps indicate that we can effectively identify access control sentences and extract the relevant policy elements.
[Access control, map data objects, software, data layer, Ontologies, Security, application controls, database design, security, Databases, Semantics, authorisation, access control, SQL commands, policy, natural language processing, Natural languages, Process control, access controls rules, role based access control, natural language parsing, relational database, relational databases, database schema, classification, database access control policy, unconstrained natural language text, uncontrolled data access, compliance, unaltered natural language documents, application layer, persistence]
Increasing anomaly handling efficiency in large organizations using applied machine learning
2013 35th International Conference on Software Engineering
None
2013
Maintenance costs can be substantial for large organizations (several hundreds of programmers) with very large and complex software systems. By large we mean lines of code in the range of hundreds of thousands or millions. Our research objective is to improve the process of handling anomaly reports for large organizations. Specifically, we are addressing the problem of the manual, laborious and time consuming process of assigning anomaly reports to the correct design teams and the related issue of localizing faults in the system architecture. In large organizations, with complex systems, this is particularly problematic because the receiver of an anomaly report may not have detailed knowledge of the whole system. As a consequence, anomaly reports may be assigned to the wrong team in the organization, causing delays and unnecessary work. We have so far developed two machine learning prototypes to validate our approach. The latest, a re-implementation and extension, of the first is being evaluated on four large systems at Ericsson AB. Our main goal is to investigate how large software development organizations can significantly improve development efficiency by replacing manual anomaly report assignment and fault localization with machine learning techniques. Our approach focuses on training machine learning systems on anomaly report databases; this is in contrast to many other approaches that are based on test case execution combined with program sampling and/or source code analysis.
[program testing, test case execution, Ontologies, training, anomaly report database, Machine Learning, software architecture, Accuracy, Databases, Semantics, learning (artificial intelligence), software development efficiency, manual anomaly report assignment, source coding, Bug Assignment, fault localization, Stacked generalization, software development management, Routing, software development organization, software maintenance, machine learning, Anomaly Handling, Large Software Systems, software fault tolerance, anomaly handling efficiency, complex software system, Automatic Fault Localization, system architecture, Organizations, Software, program sampling, source code analysis, organisational aspects]
Analyzing the change-proneness of service-oriented systems from an industrial perspective
2013 35th International Conference on Software Engineering
None
2013
Antipatterns and code smells have been widely proved to affect the change-proneness of software components. However, there is a lack of studies that propose indicators of changes for service-oriented systems. Like any other software systems, such systems evolve to address functional and non functional requirements. In this research, we investigate the change-proneness of service-oriented systems from the perspective of software engineers. Based on the feedback from our industrial partners we investigate which indicators can be used to highlight change-prone application programming interfaces (APIs) and service interfaces in order to improve their reusability and response time. The output of this PhD research will assist software engineers in designing stable APIs and reusable services with adequate response time.
[Measurement, application program interfaces, web services, software systems, change-prone application programming interfaces, service interfaces, WSDL, Service-oriented systems, code smells, Antipatterns, functional requirements, software components, change-proneness analysis, service-oriented architecture, change-proneness, Java, software engineers, service-oriented systems, APIs, Couplings, antipatterns, nonfunctional requirements, software reusability, Service-oriented architecture, metrics, Time factors]
Supporting maintenance tasks on transformational code generation environments
2013 35th International Conference on Software Engineering
None
2013
At the core of model-driven software development, model-transformation compositions enable automatic generation of executable artifacts from models. Although the advantages of transformational software development have been explored by numerous academics and industry practitioners, adoption of the paradigm continues to be slow, and limited to specific domains. The main challenge to adoption is the fact that maintenance tasks, such as analysis and management of model-transformation compositions and reflecting code changes to model transformations, are still largely unsupported by tools. My dissertation aims at enhancing the field's understanding around the maintenance issues in transformational software development, and at supporting the tasks involved in the synchronization of evolving system features with their generation environments. This paper discusses the three main aspects of the envisioned thesis: (a) complexity analysis of model-transformation compositions, (b) system feature localization and tracking in model-transformation compositions, and (c) refactoring of transformation compositions to improve their qualities.
[reflecting code management, transformation complexity, Object oriented modeling, transformational code generation environments, Maintenance engineering, Complexity theory, software quality, software maintenance, transformation refactoring, maintenance tasks support, Analytical models, model-driven software development, model-transformation compositions, Semantics, Games, transformational software development, Software, transformation composition, software metrics]
An approach to documenting and evolving architectural design decisions
2013 35th International Conference on Software Engineering
None
2013
Software architecture is considered as a set of architectural design decisions (ADDs). Capturing and representing ADDs during the architecting process is necessary for reducing architectural knowledge evaporation. Moreover, managing the evolution of ADDs helps to maintain consistency between requirements and the deployed system. In this work, we create the Triple View Model (TVM) as a general architecture framework for documenting ADDs. The TVM clarifies the notion of ADDs in three different views and covers key features of the architecting process. Based on the TVM, we propose a scenario-based method (SceMethod) to manage the documentation and the evolution of ADDs. Furthermore, we also develop a UML metamodel that incorporates evolution-centered characteristics to manage evolutionary architectural knowledge. We conduct a case study to validate the applicability and the effectiveness of our model and method. In our future work, we plan to investigate how to support ADD documentation and evolution in geographically separated software development (GSD).
[Unified modeling language, system documentation, software management, TVM, knowledge management, triple view model, ADDs evolution management, software architecture, UML metamodel, Software architecture, ADDs documentation management, Computer architecture, GSD, architectural design decisions, evolutionary architectural knowledge management, Unified Modeling Language, Documentation, Knowledge management, scenario-based method, Connectors, SceMethod, geographically separated software development, Software, architectural knowledge evaporation reduction, architecting process, evolution-centered characteristics]
An observable and controllable testing framework for modern systems
2013 35th International Conference on Software Engineering
None
2013
Modern computer systems are prone to various classes of runtime faults due to their reliance on features such as concurrency and peripheral devices such as sensors. Testing remains a common method for uncovering faults in these systems. However, commonly used testing techniques that execute the program with test inputs and inspect program outputs to detect failures are often ineffective. To test for concurrency and temporal faults, test engineers need to be able to observe faults as they occur instead of relying on observable incorrect outputs. Furthermore, they need to be able to control thread or process interleavings so that they are deterministic. This research will provide a framework that allows engineers to effectively test for subtle and intermittent faults in modern systems by providing them with greater observability and controllability.
[test inputs, runtime faults, program testing, process interleavings, program outputs, modern computer systems, system recovery, software fault tolerance, Concurrent computing, controllability, peripheral devices, observable testing framework, controllable testing framework, concurrency devices, Controllability, Software, Hardware, Observability, Monitoring, failure detection, Testing, control thread]
Toward a software product line for affective-driven self-adaptive systems
2013 35th International Conference on Software Engineering
None
2013
One expected characteristic in modern systems is self-adaptation, the capability of monitoring and reacting to changes into the environment. A particular case of self-adaptation is affective-driven self-adaptation. Affective-driven self-adaptation is about having consciousness of user's affects (emotions) and drive self-adaptation reacting to changes in those affects. Most of the previous work around self-adaptive systems deals with performance, resources, and error recovery as variables that trigger a system reaction. Moreover, most effort around affect recognition has been put towards offline analysis of affect, and to date only few applications exist that are able to infer user's affect in real-time and trigger self-adaptation mechanisms. In response to this deficit, this work proposes a software product line approach to jump-start the development of affect-driven self-adaptive systems by offering the definition of a domain-specific architecture, a set of components (organized as a framework), and guidelines to tailor those components. Study cases with systems for learning and gaming will confirm the capability of the software product line to provide desired functionalities and qualities.
[Adaptation models, Adaptive systems, domain-specific architecture, Software Architecture, software product line approach, emotion recognition, Guidelines, software architecture, user affect recognition, Computer architecture, product development, Sensors, offline affect analysis, Patterns, Framework, Emotion recognition, Self-Adaptation, user emotions, learning systems, Affective Computing, affective-driven self-adaptive systems, gaming systems, Software Product Line, software reusability, Software]
Normalizing source code vocabulary to support program comprehension and software quality
2013 35th International Conference on Software Engineering
None
2013
The literature reports that source code lexicon plays a paramount role in program comprehension, especially when software documentation is scarce, outdated or simply not available. In source code, a significant proportion of vocabulary can be either acronyms and-or abbreviations or concatenation of terms that can not be identified using consistent mechanisms such as naming conventions. It is, therefore, essential to disambiguate concepts conveyed by identifiers to support program comprehension and reap the full benefit of Information Retrieval-based techniques (e.g., feature location and traceability) whose linguistic information (i.e., source code identifiers and comments) used across all software artifacts (e.g., requirements, design, change requests, tests, and source code) must be consistent. To this aim, we propose source code vocabulary normalization approaches that exploit contextual information to align the vocabulary found in the source code with that found in other software artifacts. We were inspired in the choice of context levels by prior works and by our findings. Normalization consists of two tasks: splitting and expansion of source code identifiers. We also investigate the effect of source code vocabulary normalization approaches on software maintenance tasks. Results of our evaluation show that our contextual-aware techniques are accurate and efficient in terms of computation time than state of the art alternatives. In addition, our findings reveal that feature location techniques can benefit from vocabulary normalization when no dynamic information is available.
[Vocabulary, Software maintenance, Dictionaries, system documentation, Source code linguistic analysis, information retrieval-based techniques, software quality, ubiquitous computing, contextual-aware techniques, vocabulary, software artifacts, source code lexicon, software documentation, Context, program comprehension, source code vocabulary normalization approaches, information retrieval, reverse engineering, linguistic information, source code identifiers expansion, feature location, software maintenance, contextual information, source code identifiers splitting, software maintenance tasks, Software quality, context levels, source code comments]
Integrating Systematic exploration, analysis, and maintenance in software development
2013 35th International Conference on Software Engineering
None
2013
Modern integrated development environments (IDEs) support one live codebase at a given moment, which imposes limitations to software development. For example, with only one codebase, the developer must pause development while running tests, or a static analysis, as any edit could invalidate the ongoing computation. Were the IDEs supported a copy of developer's codebase, the analyses could have run on this copy, in parallel with the development process. In this paper, we propose techniques and tools that integrate multiple live codebases support to the software development process. Our hypothesis is that IDE support for multiple live codebases can provide a richer development process and aid developers.
[integrating systematic exploration, program testing, software development, Maintenance engineering, static analysis, Control systems, integrated development environments, IDE, History, software maintenance, program compilers, Systematics, codebase support, Computer bugs, codebase, Software, Testing]
Fostering software quality assessment
2013 35th International Conference on Software Engineering
None
2013
Software quality assessment shall monitor and guide the evolution of a system based on quality measurements. This continuous process should ideally involve multiple stakeholders and provide adequate information for each of them to use. We want to support an effective selection of quality measurements based on the type of software and individual information needs of the involved stakeholders. We propose an approach that brings together quality measurements and individual information needs for a context-sensitive tailoring of information related to a software quality assessment. We address the following research question: How can we better support different stakeholders in the quality assessment of a software system? For that we will devise theories, models, and prototypes to capture their individual information needs, tailor information from software repositories to these needs, and enable a contextual analysis of the quality aspects. Such a context-sensitive tailoring will provide a effective and individual view on the latest development trends in a project. We outline the milestones as well as evaluation approaches in this paper.
[Context, stakeholder information needs, context-sensitive information tailoring, software system quality assessment, quality measurements, Sonar, software quality, software repositories, continuous process, Software quality, software models, software type, contextual analysis, system evolution, Software measurement, software prototypes, Context modeling, software metrics]
A framework for self-healing software systems
2013 35th International Conference on Software Engineering
None
2013
I present an approach to avoid functional failures at runtime in component-based application systems. The approach exploits the intrinsic redundancy of components to find workarounds as alternative sequences of operations to avoid a failure. A first Java prototype is presented, and an evaluation plan, as some preliminary results, are discussed.
[Java, object-oriented programming, Redundancy, intrinsic component redundancy, Java prototype, software fault tolerance, self-healing software systems, component-based application systems, Runtime, Prototypes, Libraries, Software, functional failures]
Building high assurance secure applications using security patterns for capability-based platforms
2013 35th International Conference on Software Engineering
None
2013
Building high assurance secure applications requires the proper use of security mechanisms and assurances provided by the underlying secure platform. However, applications are often built using security patterns and best practices that are agnostic with respect to the intricate specifics of the different underlying platforms. This independence from the underlying platform leaves a gap between security patterns and underlying secure platforms. In this PhD research abstract, we propose a novel approach to bridge this gap. Specifically, we propose reusable capability-specific design fragments for security patterns, which are specialization for patterns in a capability-based system. The focus is on systems that adhere to a capability-based security model, which we consider as the underlying platforms, to provide desired application-wide security properties. We also discuss assumptions and levels of assurance for these reusable designs and their use in the verification of application designs.
[capability-based platforms, Capability Maturity Model, Assurance, Capability, Platform, Unified modeling language, Buildings, capability-based security model, Whole System, security mechanisms, reusable designs, Security, application-wide security property, Analytical models, security patterns, Software architecture, security of data, Security Patterns, secure platform, reusable capability-specific design fragments, security assurances, software reusability, capability-based system, Formal verification]
Systematically selecting a software module during opportunistic reuse
2013 35th International Conference on Software Engineering
None
2013
Opportunistic reuse, a need based sourcing of software modules without any prior plan is a common practice in software development. It is popular due to rapid productivity improvement and fewer impediments while undertaking reuse task. However, developers use informal criteria to select an external module for reuse. The composition of such a module may introduce undesirable emergent behavior due to new or unknown design decisions. Hence, we propose to systematize selection of an external module by defining selection criteria based on extracted design decisions from source code. This would help developers in making informed selection of external modules there by avoiding or being aware of design mismatches when reusing opportunistically.
[Electronic publishing, systematic external software module selection, software development, source code, module composition, Open source software, opportunistic reuse, software module sourcing, design decisions, Tiles, external module selection, Information services, software reusability, design mismatch, Software systems, Internet, informal selection criteria, undesirable emergent behavior]
Informing development decisions: From data to information
2013 35th International Conference on Software Engineering
None
2013
Software engineers generate vast quantities of development artifacts such as source code, bug reports, test cases, usage logs, etc., as they create and maintain their projects. The information contained in these artifacts could provide valuable insights into the software quality and adoption, as well as development process. However, very little of it is available in the way that is immediately useful to various stakeholders. This research aims to extract and analyze data from software repositories to provide software practitioners with up-to-date and insightful information that can support informed decisions related to the business, management, design, or development of software systems. This data-centric decision-making is known as analytics. In particular, we demonstrate that by employing software development analytics, we can help developers make informed decisions around user adoption of a software project, code review process, as well as improve developers' awareness of their working context.
[program testing, Communities, software repository, test cases, data-centric decision-making, software quality, Data mining, software system, Open source software, software development decision, software development analytics, Market research, software engineering, bug reports, usage logs, code review process, project management, program diagnostics, Decision making, software development management, development artifacts, source code, informed decision, developer awareness improvement, software adoption, development process, software project adoption, Software systems]
Understanding and simulating software evolution
2013 35th International Conference on Software Engineering
None
2013
Simulations have been used in various areas, yielding good results, but their application to software evolution is still limited. Simulations of software evolution can help people understand the driving forces that shape software evolution, and predict future evolutionary paths. To move towards simulation of software evolution, this research tries to explore possible models to simulate software evolution, and the applicability of different data to parameterize the models. The simulations will both be based on fine-grained code changes obtained by comparing the abstract syntax trees of source code. The use of fine-grain code changes could reveal information about software evolution that is unavailable by other means.
[source coding, software evolution simulation, Computational modeling, Conferences, computational linguistics, source code, Predictive models, History, evolutionary paths, fine-grained code changes, abstract syntax trees, evolutionary computation, Evolution (biology), software evolution understanding, Software, Data models, software engineering]
An ontology toolkit for problem domain concept location in program comprehension
2013 35th International Conference on Software Engineering
None
2013
Programmers are able to understand source code because they are able to relate program elements (e.g. modules, objects, or functions), with the real world concepts these elements are addressing. The main goal of this work is to enhance current program comprehension by systematically creating bidirectional mappings between domain concepts and source code. To achieve this, semantic bridges are required between natural language terms used in the problem domain and program elements written using formal programming languages. These bridges are created by an inference engine over a multi-ontology environment, including an ontological representation of the program, the problem domain, and the real world effects program execution produces. These ontologies are populated with data collected from both domains, and enriched using available Natural Language Processing and Information Retrieval techniques.
[Software maintenance, Conferences, natural language terms, Ontologies, problem domain concept location, Data mining, programming languages, Engines, semantic bridges, inference engine, bidirectional mapping, program comprehension, formal languages, multiontology environment, natural language processing, Natural languages, information retrieval, source code, program execution, reverse engineering, ontology toolkit, inference mechanisms, program elements, ontological representation, formal programming languages, information retrieval techniques, ontologies (artificial intelligence)]
Measuring the forensic-ability of audit logs for nonrepudiation
2013 35th International Conference on Software Engineering
None
2013
Forensic analysis of software log files is used to extract user behavior profiles, detect fraud, and check compliance with policies and regulations. Software systems maintain several types of log files for different purposes. For example, a system may maintain logs for debugging, monitoring application performance, and/or tracking user access to system resources. The objective of my research is to develop and validate a minimum set of log file attributes and software security metrics for user nonrepudiation by measuring the degree to which a given audit log file captures the data necessary to allow for meaningful forensic analysis of user behavior within the software system. For a log to enable user nonrepudiation, the log file must record certain data fields, such as a unique user identifier. The log must also record relevant user activity, such as creating, viewing, updating, and deleting system resources, as well as software security events, such as the addition or revocation of user privileges. Using a grounded theory method, I propose a methodology for observing the current state of activity logging mechanisms in healthcare, education, and finance, then I quantify differences between activity logs and logs not specifically intended to capture user activity. I will then propose software security metrics for quantifying the forensic-ability of log files. I will evaluate my work with empirical analysis by comparing the performance of my metrics on several types of log files, including both activity logs and logs not directly intended to record user activity. My research will help software developers strengthen user activity logs for facilitating forensic analysis for user nonrepudiation.
[Measurement, educational computing, education, activity logging mechanism, user access tracking, Medical services, user behavior profile extraction, grounded theory, data field, software logs, auditing, software security metrics, Security, software system, grounded theory method, user privilege revocation, healthcare, security, system resources, debugging, audit log, log file attribute, financial data processing, application performance monitoring, health care, compliance checking, digital forensics, unique user identifier, forensic ability measurement, Forensics, software log files, nonrepudiation, Standards, forensic analysis, software security events, forensics, user nonrepudiation, metric, fraud, fraud detection, Software systems, system monitoring, logging, finance, software metrics]
SNIPR: Complementing code search with code retargeting capabilities
2013 35th International Conference on Software Engineering
None
2013
This paper sketches a research path that seeks to examine the search for suitable code problem, based on the observation that when code retargeting is included within a code search activity, developers can justify the suitability of these results upfront and thus reduce their searching efforts looking for suitable code. To support this observation, this paper introduces the Snippet Retargeting Approach, or simply SNIPR. SNIPR complements code search with code retargeting capabilities. These capabilities' intent is to help expedite the process of determining if a found example is a best fit. They do that by allowing developers to explore code modification ideas in place, without requiring to leave the search interface. With SNIPR, developers engage in a virtuous loop where they find code, retarget code, and select only code choices they can justify as suitable. This assures immediate feedback on retargeted examples and thus saves valuable time searching for appropriate code.
[Conferences, SNIPR, Programming, Search problems, program compilers, Presses, code problem, code search activity, Prototypes, code retargeting capabilities, complementing code search, Search engines, research path, snippet retargeting approach, Software engineering]
Supporting incremental programming with ghosts
2013 35th International Conference on Software Engineering
None
2013
Best practices in programming typically imply coding using classes and interfaces that are not (fully) defined yet. However, integrated development environments (IDEs) do not support such incremental programming seamlessly. Instead, they get in the way by reporting ineffective error messages. Ignoring these messages altogether prevents the programmer from getting useful feedback regarding actual inconsistencies and type errors. But attending to these error messages repeatedly breaks the programming workflow. In order to smoothly support incremental programming, we propose to extend IDEs with support of undefined entities, called Ghosts. Ghosts are implicitly reified in the IDE through their usages. Programmers can explicitly identify ghosts, get appropriate type feedback, interact with them, and bust them when ready, yielding actual code.
[Context, checkpointing, Visualization, message passing, program verification, software development management, ineffective error message, Switches, Programming, programming workflow, Encoding, type feedback, IDE, incremental programming, Ghost, integrated development environment, Writing, Skeleton, type error, inconsistency]
Novice understanding of program analysis tool notifications
2013 35th International Conference on Software Engineering
None
2013
Program analysis tools are available to make programmers' jobs easier by automating tasks that would otherwise be performed manually or not at all. To communicate with the programmer, these tools use notifications, which may be textual, visual, or a combination of both. Research has shown that these notifications need improvement in two areas: expressiveness and scalability. In the research described here, I begin an investigation into the expressiveness and scalability of existing program analysis tools and potential improvements in expressiveness and scalability in and across these tools.
[Visualization, Java, Scalability, program diagnostics, Programming, Security, scalability, program analysis tool notifications, Program processors, program analysis, expressiveness, software tools, novice understanding, notifications, tool evaluation]
Energy aware self-adaptation in mobile systems
2013 35th International Conference on Software Engineering
None
2013
The increasing proliferation of mobile handsets, and the migration of the information access paradigm to mobile platforms, leads researchers to study the energy consumption of this class of devices. The literature still lacks metrics and tools that allow software developers to easily measure and optimize the energy efficiency of their code. Energy efficiency can definitely improve user experience increasing battery life. This paper aims to describe a technique to adapt the execution of a mobile application, based on the actual energy consumption of the device, without using external equipment.
[energy aware self-adaptation, Energy consumption, mobile systems, Self-Adaptation, mobile application, Context-Awareness, Mobile communication, information access paradigm, Batteries, Energy Aware Software, mobile platforms, Android, battery life, mobile computing, optimisation, power aware computing, energy efficiency optimization, Energy Awareness, Software, Energy efficiency, Smart phones, mobile handsets]
ConfDiagnoser: An automated configuration error diagnosis tool for Java software
2013 35th International Conference on Software Engineering
None
2013
This paper presents ConfDiagnoser, an automated configuration error diagnosis tool for Java software. Conf-Diagnoser identifies the root cause of a configuration error - a single configuration option that can be changed to produce desired behavior. It uses static analysis, dynamic profiling, and statistical analysis to link the undesired behavior to specific configuration options. ConfDiagnoser differs from existing approaches in two key aspects: it does not require users to provide a testing oracle (to check whether the software functions correctly) and thus is fully-automated; and it can diagnose both crashing and non-crashing errors. We demonstrated ConfDiagnoser's accuracy and speed on 5 non-crashing configuration errors and 9 crashing configuration errors from 5 configurable software systems.
[Java, Instruments, program diagnostics, crashing errors, static analysis, specific configuration options, Computer crashes, Accuracy, Databases, dynamic profiling, ConfDiagnoser, Software systems, Java software, noncrashing errors, statistical analysis, automated configuration error diagnosis tool]
Reproducing and debugging field failures in house
2013 35th International Conference on Software Engineering
None
2013
As confirmed by a recent survey among developers of the Apache, Eclipse, and Mozilla projects, failures of the software that occur in the field, after deployment, are difficult to reproduce and investigate in house. To address this problem, we propose an approach for in-house reproducing and debugging failures observed in the field. This approach can synthesize several executions similar to an observed field execution to help reproduce the observed field behaviors, and use these executions, in conjunction with several debugging techniques, to identify causes of the field failure. Our initial results are promising and provide evidence that our approach is able to reproduce failures using limited field execution information and help debugging.
[Optical fibers, program debugging, Instruments, Europe, Eclipse, Debugging, software failure, Computer crashes, Apache, debugging field failure, Software, Mozilla project, in-house reproducing failure, Software engineering]
Fault comprehension for concurrent programs
2013 35th International Conference on Software Engineering
None
2013
Concurrency bugs are difficult to find because they occur with specific memory-access orderings between threads. Traditional bug-finding techniques for concurrent programs have focused on detecting raw-memory accesses representing the bugs, and they do not identify memory accesses that are responsible for the same bug. To address these limitations, we present an approach that uses memory-access patterns and their suspicious-ness scores, which indicate how likely they are to be buggy, and clusters the patterns responsible for the same bug. The evaluation on our prototype shows that our approach is effective in handling multiple concurrency bugs and in clustering patterns for the same bugs, which improves understanding of the bugs.
[Context, program debugging, fault comprehension, Instruction sets, memory access identification, Programming, prototype evaluation, memory-access pattern, suspiciousness scores, concurrent programs, parallel programming, Concurrent computing, storage management, pattern clustering, Computer bugs, Clustering algorithms, concurrency control, concurrency bug handling, concurrent bugs, Facebook, memory-access ordering]
A proposal for the improvement of project's cost predictability using EVM and historical data of cost
2013 35th International Conference on Software Engineering
None
2013
This paper proposes an extension of the Earned Value Management - EVM technique through the integration of historical cost performance data of processes under statistical control as a means to improve the predictability of the cost of projects. The proposed technique was evaluated through a case-study in industry, which evaluated the implementation of the proposed technique in 22 software development projects Hypotheses tests with 95% significance level were performed, and the proposed technique was more accurate and more precise than the traditional technique for calculating the Cost Performance Index - CPI and Estimates at Completion - EAC.
[Cost Performance Index &#x2014; CPI, Earned Value Management &#x2014; EVM, cost historical data, statistical control, earned value management, Accuracy, Performance analysis, Mathematical model, project management, High Maturity, project cost predictability, EVM technique, historical cost performance data, software development management, EAC, cost performance index, Equations, software development projects, Stability criteria, Software Metrics, Software, estimates at completion, software cost estimation, statistical analysis, CPI, hypothesis tests]
Studying the effect of co-change dispersion on software quality
2013 35th International Conference on Software Engineering
None
2013
Software change history plays an important role in measuring software quality and predicting defects. Co-change metrics such as number of files changed together has been used as a predictor of bugs. In this study, we further investigate the impact of specific characteristics of co-change dispersion on software quality. Using statistical regression models we show that co-changes that include files from different subsystems result in more bugs than co-changes that include files only from the same subsystem. This can be used to improve bug prediction models based on co-changes.
[Measurement, regression analysis, changes, Complexity theory, software quality, History, cochange dispersion, mining software repository, bugs, software change history, statistical regression models, Computer bugs, cochange metrics, bug prediction models, Software quality, Data collection, software metrics]
A roadmap for software maintainability measurement
2013 35th International Conference on Software Engineering
None
2013
Object-Oriented Programming (OOP) is one of the most used programming paradigms. Thus, researches dedicated in improvement of software quality that adhere to this paradigm are demanded. Complementarily, maintainability is considered a software attribute that plays an important role in its quality level. In this context, Object-Oriented Software Maintainability (OOSM) has been studied through years and several researchers proposed a high number of metrics to measure it. Nevertheless, there is no standardization or a catalogue to summarize all the information about these metrics, helping the researchers to make decision about which metrics can be adopted to perform their experiments in OOSM. Actually, distinct areas in both academic and industrial environment, such as Software Development, Project Management, and Software Research can adopt them to support decision-making processes. Thus, this work researched about the usage of OOSM metrics in academia and industry in order to help researchers in making decision about the metrics suite to be adopted. We found 570 OOSM metrics. Additionally, as a preliminary result we proposed a catalog with 36 metrics that were most used in academic works/experiments, trying to guide researchers with their decision-make about which metrics are more indicated to be adopted in their experiments.
[Object-Oriented Development, software management, software maintainability measurement, software quality, Systematics, software research, Metrics, object oriented programming, Software measurement, project management, software development, software attribute, Software Maintainability, software maintenance, object oriented software maintainability, Catalog, Couplings, OOP, Experimental Software Engineering, OOSM metrics, decision making processes, software quality level, Software, Catalogs, Software engineering]
Reasoning with qualitative preferences to develop optimal component-based systems
2013 35th International Conference on Software Engineering
None
2013
To produce an optimal component-based software system for a given application, it is necessary to consider both the required functionality of the system and its stakeholders' preferences over various non-functional properties. We propose a new modular end-to-end framework for component-based system development that combines formal specification and verification of functional requirements with a novel method for representing and reasoning with stakeholders' qualitative preferences over properties of the system. This framework will facilitate the use of formal verification to ensure system correctness while making it easier to identify truly optimal component-based system designs.
[reasoning method, nonfunctional properties, object-oriented programming, Unified modeling language, Minimization, Cognition, stakeholder qualitative preferences, formal specification, System analysis and design, Optimization, Component architectures, formal verification, optimal component-based software system development, system functionality, system correctness, modular end-to-end framework, functional requirements, reasoning about programs, Software engineering]
From models to code and back: Correct-by-construction code from UML and ALF
2013 35th International Conference on Software Engineering
None
2013
Ever increasing complexity of modern software systems demands new powerful development mechanisms. Model-driven engineering (MDE) can ease the development process through problem abstraction and automated code generation from models. In order for MDE solutions to be trusted, such generation should preserve the system's properties defined at modelling level, both functional and extra-functional, all the way down to the target code. The outcome of our research is an approach that aids the preservation of system's properties in MDE of embedded systems. More specifically, we provide generation of full source code from design models defined using the CHESS-ML, monitoring of selected extra-functional properties at code level, and back-propagation of observed values to design models. The approach is validated against industrial case-studies in the telecommunications applicative domain.
[Unified modeling language, CHESS-ML, program compilers, extra-functional properties, Analytical models, embedded systems, telecommunications applicative domain, Real-time systems, back-propagation, Monitoring, Unified Modeling Language, Computational modeling, automated code generation, source code, correct-by-construction code, MDE solutions, modern software systems complexity, design models, Embedded systems, industrial case-studies, UML, problem abstraction, model-driven engineering, backpropagation, ALF, computational complexity]
Mitigating the obsolescence of specification models of service-based systems
2013 35th International Conference on Software Engineering
None
2013
Service-based systems (SBS) must be able to adapt their architectural configurations during runtime in order to keep satisfied their specification models. These models are the result of design time derivation of requirements into precise and verifiable specifications by using the knowledge about the current service offerings. Unfortunately, the design time knowledge may be no longer valid during runtime. Then, nonfunctional constraints may have different numerical meanings at different time even for the same observers. Thus, specification models become obsolete affecting the SBS' capability of detecting requirement violations during runtime and therefore they trigger reconfigurations when appropriated. In order to mitigate the obsolescence of specification models, we propose to specify and verify them using the computing with words (CWW) methodology. First, non-functional properties (NFPs) of functionally-equivalent services are modeled as linguistic variables, whose domains are concepts or linguistic values instead of precise numbers. Second, architects specify at design time their requirements as linguistic decision models (LDMs) using these concepts. Third, during runtime, the CWW engine monitors the requirements satisfaction by the current chosen architectural configuration. And fourth, each time a global concept drift is detected in the NFPs of the services market, the numerical meanings are updated. Our initial results are encouraging, where our approach mitigates effectively and efficiently the obsolescence of the specification models used by SBS to drive their reconfigurations.
[CWW engine, Uncertainty, Computational modeling, Scattering, computing with words methodology, specification model obsolescence mitigation, computing with words, nonfunctional constraints, requirement violations detection, market-aware requirements, requirements@run.time, uncertainty, formal specification, linguistic decision models, service-based systems, software architecture, Runtime, formal verification, LDM, architectural configurations, linguistic variables, Aging, Numerical models, Monitoring]
Decision theoretic requirements prioritization A two-step approach for sliding towards value realization
2013 35th International Conference on Software Engineering
None
2013
Budget and schedule constraints limit the number of requirements that can be worked on for a software system and is thus necessary to select the most valuable requirements for implementation. However, selecting from a large number of requirements is a decision problem that requires negotiating with multiple stakeholders and satisficing their value propositions. In this paper I present a two-step value-based requirements prioritization approach based on TOPSIS, a decision analysis framework that tightly integrates decision theory with the process of requirements prioritization. In this two-step approach the software system is initially decomposed into high-level Minimal Marketable Features (MMFs) which the business stakeholders prioritize against business goals. Each individual MMF is further decomposed into low-level requirements/features that are primarily prioritized by the technical stakeholders. The priorities of the low-level requirements are influenced by the MMFs they belong to. This approach has been integrated into Winbook, a social-networking influenced collaborative requirements management framework and deployed for use by 10 real-client project teams for the Software Engineering project course at the University of Southern California in Fall 2012. This model allowed the clients and project teams to effectively gauge the importance of each MMF and low-level requirement and perform various sensitivity analyses and take value-informed decisions when selecting requirements for implementation.
[decision problem, Winbook, decision theory, formal specification, software system, Requirements, multiple stakeholders, budget constraints, Quality function deployment, software engineering project course, decision analysis framework, business goals, sliding towards value realization, social-networking influenced collaborative requirements management framework, Sensitivity analysis, Decision making, Educational institutions, low-level requirements, decision theoretic requirements prioritization, value-based requirements prioritization approach, business stakeholders, University of Southern California, technical stakeholders, prioritization, value-based prioritization, high-level minimal marketable features, value-informed decisions, Software systems, social networking (online), schedule constraints, sensitivity analyses, TOPSIS]
Changeset based developer communication to detect software failures
2013 35th International Conference on Software Engineering
None
2013
As software systems get more complex, the companies developing them consist of larger teams and therefore results in more complex communication artifacts. As these software systems grow, so does the impact of every action to the product. To prevent software failure created by this growth and complexity, companies need to find more efficient and effective ways to communicate. The method used in this paper presents developer communication in the form of social networks of which have properties that can be used to detect software failures.
[fault diagnosis, Social network services, software reliability, software systems, social networks, failure-inducing communication pattern, Data mining, changeset based developer communication, Collaboration, software failure detection, developer communication, groupware, complex communication artifacts, Software systems, social networking (online), software engineering, Joining processes, Software engineering]
Identifying failure inducing developer pairs within developer networks
2013 35th International Conference on Software Engineering
None
2013
Software systems have not only become larger over time, but the amount of technical contributors and dependencies have also increased. With these expansions also comes the increasing risk of introducing a software failure into a pre-existing system. Software failures are a multi-billion dollar problem in the industry today and while integration and other forms of testing are helping to ensure a minimal number of failures, research to understand full impacts of code changes and their social implications is still a major concern. This paper describes how analysis of code changes and the technical relationships they infer can be used to detect pairs of developers whose technical dependencies may induce software failures. These developer pairs may also be used to predict future software failures as well as provide recommendations to contributors to solve these failures caused by source code changes.
[Java, Conferences, program diagnostics, code change analysis, software systems, Data mining, source code changes, Computer bugs, developer networks, software failures, Software systems, software engineering, Software engineering]
On identifying user complaints of iOS apps
2013 35th International Conference on Software Engineering
None
2013
In the past few years, the number of smartphone apps has grown at a tremendous rate. To compete in this market, both independent developers and large companies seek to improve the ratings of their apps. Therefore, understanding the user's perspective of mobile apps is extremely important. In this paper, we study the user's perspective of iOS apps by qualitatively analyzing app reviews. In total, we manually tag 6,390 reviews for 20 iOS apps. We find that there are 12 types of user complaints. Functional errors, requests for additional features, and app crashes are examples of the most common complaints. In addition, we find that almost 11% of the studied complaints were reported after a recent update. This highlights the importance of regression testing before updating apps. This study contributes a listing of the most frequent complaints about iOS apps to aid developers and researchers in better understanding the user's perspective of apps.
[user complaint identification, mobile apps, program testing, Conferences, regression testing, smartphone apps, Mobile communication, Educational institutions, user perspective, Computer crashes, smart phones, functional errors, app crashes, Privacy, mobile computing, Software, statistical testing, Testing, iOS apps]
Automated testing of GUI applications: Models, tools, and controlling flakiness
2013 35th International Conference on Software Engineering
None
2013
System testing of applications with graphical user interfaces (GUIs) such as web browsers, desktop, or mobile apps, is more complex than testing from the command line. Specialized tools are needed to generate and run test cases, models are needed to quantify behavioral coverage, and changes in the environment, such as the operating system, virtual machine or system load, as well as starting states of the executions, impact the repeatability of the outcome of tests making tests appear flaky. In this tutorial, we present an overview of the state of the art in GUI testing, consisting of both lectures and demonstrations on various platforms (desktop, web and mobile applications), using an open source testing tool, GUITAR. We show how to setup a system under test, how to extract models without source code, and how to then use those models to generate and replay test cases. We then present a lecture on the various factors that may cause flakiness in the execution of GUI-centric software, and hence impact the results of analyses and experiments based on such software. We end with a demonstration of a community resource for sharing GUI testing artifacts aimed at controlling these factors. This tutorial targets both researchers who develop techniques for testing GUI software, and practitioners from industry who want to learn more about model-based GUI testing or who run and rerun GUI tests and often find their runs are flaky.
[GUI software testing, program testing, graphical user interfaces, public domain software, open source testing tool, GUI-centric software, automatic testing, model-based GUI testing, Benchmark testing, Web browsers, Graphical user interfaces, Computational modeling, behavioral coverage, operating system, Tutorials, source code, system testing, GUITAR, virtual machine, system load, online front-ends, flakiness control, Software, automated GUI application testing, Software engineering]
Build your own model checker in one month
2013 35th International Conference on Software Engineering
None
2013
Model checking has established as an effective method for automatic system analysis and verification. It is making its way into many domains and methodologies. Applying model checking techniques to a new domain (which probably has its own dedicated modeling language) is, however, far from trivial. Translation-based approach works by translating domain specific languages into input languages of a model checker. Because the model checker is not designed for the domain (or equivalently, the language), translation-based approach is often ad hoc. Ideally, it is desirable to have an optimized model checker for each application domain. Implementing one with reasonable efficiency, however, requires years of dedicated efforts. In this tutorial, we will briefly survey a variety of model checking techniques. Then we will show how to develop a model checker for a language combining real-time and probabilistic features using the PAT (Process Analysis Toolkit) step-by-step, and show that it could take as short as a few weeks to develop your own model checker with reasonable efficiency. The PAT system is designed to facilitate development of customized model checkers. It has an extensible and modularized architecture to support new languages (and their operational semantics), new state reduction or abstraction techniques, new model checking algorithms, etc. Since its introduction 5 years ago, PAT has attracted more than 2500 registered users (from 500+ organisations in 60 countries) and has been applied to develop model checkers for 20 different languages.
[Algorithm design and analysis, PAT, program diagnostics, model checking techniques, process analysis toolkit, probability, translation-based approach, Tutorials, automatic system analysis, state reduction, probabilistic features, Sun, abstraction techniques, formal verification, Semantics, Model checking, Syntactics, domain specific languages, Software, automatic system verification, model checker]
Data science for software engineering
2013 35th International Conference on Software Engineering
None
2013
Target audience: Software practitioners and researchers wanting to understand the state of the art in using data science for software engineering (SE). Content: In the age of big data, data science (the knowledge of deriving meaningful outcomes from data) is an essential skill that should be equipped by software engineers. It can be used to predict useful information on new projects based on completed projects. This tutorial offers core insights about the state-of-the-art in this important field. What participants will learn: Before data science: this tutorial discusses the tasks needed to deploy machine-learning algorithms to organizations (Part 1: Organization Issues). During data science: from discretization to clustering to dichotomization and statistical analysis. And the rest: When local data is scarce, we show how to adapt data from other organizations to local problems. When privacy concerns block access, we show how to privatize data while still being able to mine it. When working with data of dubious quality, we show how to prune spurious information. When data or models seem too complex, we show how to simplify data mining results. When data is too scarce to support intricate models, we show methods for generating predictions. When the world changes, and old models need to be updated, we show how to handle those updates. When the effect is too complex for one model, we show how to reason across ensembles of models. Pre-requisites: This tutorial makes minimal use of maths of advanced algorithms and would be understandable by developers and technical managers.
[data dichotomization, software researchers, data mining, Tutorials, Predictive models, data clustering, Educational institutions, machine-learning algorithms, data discretization, software practitioners, Data mining, pattern clustering, data science, Software, Data models, data privacy, software engineering, data handling, learning (artificial intelligence), statistical analysis, big data, Software engineering]
Software analytics: Achievements and challenges
2013 35th International Conference on Software Engineering
None
2013
A huge wealth of various data exist in the practice of software development. Further rich data are produced by modern software and services in operation, many of which tend to be data-driven and/or data-producing in nature. Hidden in the data is information about the quality of software and services or the dynamics of software development. Software analytics is to utilize a data-driven approach to enable software practitioners to perform data exploration and analysis in order to obtain insightful and actionable information; such information is used for completing various tasks around software systems, software users, and software development process. This tutorial presents achievements and challenges of research and practice on principles, techniques, and applications of software analytics, highlighting success stories in industry, research achievements that are transferred to industrial practice, and future research and practice directions in software analytics.
[]
Developing verified programs with Dafny
2013 35th International Conference on Software Engineering
None
2013
Dafny is a programming language and program verifier. The language includes specification constructs and the verifier checks that the program lives up to its specifications. These tutorial notes give some Dafny programs used as examples in the tutorial.
[program verification, Tutorials, Educational institutions, Cognition, Security, program verifier, Dafny programs, Reactive power, Computer languages, specification languages, programming language, Arrays, specification langauge]
Software metrics: Pitfalls and best practices
2013 35th International Conference on Software Engineering
None
2013
Using software metrics to keep track of the progress and quality of products and processes is a common practice in industry. Additionally, designing, validating and improving metrics is an important research area. Although using software metrics can help in reaching goals, the effects of using metrics incorrectly can be devastating. In this tutorial we leverage 10 years of metrics-based risk assessment experience to illustrate the benefits of software metrics, discuss different types of metrics and explain typical usage scenario's. Additionally, we explore various ways in which metrics can be interpreted using examples solicited from participants and practical assignments based on industry cases. During this process we will present the four common pitfalls of using software metrics. In particular, we explain why metrics should be placed in a context in order to maximize their benefits. A methodology based on benchmarking to provide such a context is discussed and illustrated by a model designed to quantify the technical quality of a software system. Examples of applying this model in industry are given and challenges involved in interpreting such a model are discussed. This tutorial provides an in-depth overview of the benefits and challenges involved in applying software metrics. At the end you will have all the information you need to use, develop and evaluate metrics constructively.
[technical quality, risk management, Tutorials, Educational institutions, software quality, software system, Software metrics, product quality, Software quality, Benchmark testing, metrics-based risk assessment experience, software metrics, process quality]
A hands-on Java Pathfinder tutorial
2013 35th International Conference on Software Engineering
None
2013
Java Pathfinder (JPF) is an open source analysis system that automatically verifies Java programs. The JPF tutorial provides an opportunity to software engineering researchers and practitioners to learn about JPF, be able to install and run JPF, and understand the concepts required to extend JPF. The hands-on tutorial will expose the attendees to the basic architecture framework of JPF, demonstrate the ways to use it for analyzing their artifacts, and illustrate how they can extend JPF to implement their own analyses. One of the defining qualities of JPF is its extensibility. JPF has been extended to support symbolic execution, directed automated random testing, different choice generation, configurable state abstractions, various heuristics for enabling bug detection, configurable search strategies, checking temporal properties and many more. JPF supports these extensions at the design level through a set of stable well defined interfaces. The interfaces are designed to not require changes to the core, yet enable the development of various JPF extensions. In this tutorial we provide attendees a hands on experience of developing different interfaces in order to extend JPF. The tutorial is targeted toward a general software engineering audience-software engineering researchers and practitioners. The attendees need to have a good understanding of the Java programming language and be fairly comfortable with Java program development. The attendees are not required to have any background in Java Pathfinder, software model checking or any other formal verification techniques. The tutorial will be self-contained.
[Java program development, program debugging, program testing, program verification, public domain software, software engineering practitioners, JPF tutorial, Java programming language, open source analysis system, architecture framework, software architecture, configurable state abstractions, Model checking, symbol manipulation, automatic Java program verification, configurable search strategies, search problems, software engineering researchers, Java, formal verification techniques, NASA, Tutorials, Educational institutions, bug detection, temporal property checking, software model checking, automated random testing, hands-on Java PathFinder tutorial, symbolic execution, Software, software engineering audience, Software engineering]
Efficient quality assurance of variability-intensive systems
2013 35th International Conference on Software Engineering
None
2013
Variability is becoming an increasingly important concern in software development but techniques to cost-effectively verify and validate software in the presence of variability have yet to become widespread. This half-day tutorial offers an overview of the state of the art in an emerging discipline at the crossroads of formal methods and software engineering: quality assurance of variability-intensive systems. We will present the most significant results obtained during the last four years or so, ranging from conceptual foundations to readily usable tools. Among the various quality assurance techniques, we focus on model checking, but also extend the discussion to other techniques. With its lightweight usage of mathematics and balance between theory and practice, this tutorial is designed to be accessible to a broad audience. Researchers working in the area, willing to join it, or simply curious, will get a comprehensive picture of the recent developments. Practitioners developing variability-intensive systems are invited to discover the capabilities of our techniques and tools, and to consider integrating them in their processes.
[software development, Conferences, Tutorials, half-day tutorial, Educational institutions, software quality, Reactive power, Quality assurance, formal verification, model checking, variability-intensive systems, software validate, quality assurance, formal methods, Model checking, Software, software engineering]
Software requirement patterns
2013 35th International Conference on Software Engineering
None
2013
Software requirements reuse becomes a fundamental activity for those IT organizations that conduct requirements engineering processes in similar settings. One strategy to implement this reuse is by exploiting a catalogue of software requirement patterns (SRPs). In this tutorial, we provide an introduction to the concept of SRP, summarise several existing approaches, and reflect on the consequences on several requirements engineering processes and activities. We take one of these approaches, the PABRE framework, as exemplar for the tutorial and analyse in more depth the catalogue of SRP that is proposed. We apply the concepts given on a practical exercise.
[Context, project management, IT organizations, software reuse, Software Requirements Patterns, ISO standards, Tutorials, Documentation, software requirement specifications, Software Reuse, Requirements Engineering, formal specification, SRP, software requirement patterns, Organizations, software reusability, Software, software projects, object-oriented methods, Patterns, Software engineering]
1st International workshop on assurance cases for software-intensive systems (ASSURE 2013)
2013 35th International Conference on Software Engineering
None
2013
Software plays a key role in high-risk systems, i.e., safety and security-critical systems. Several certification standards and guidelines, e.g., in the defense, transportation (aviation, automotive, rail), and healthcare domains, now recommend and/or mandate the development of assurance cases for software-intensive systems. As such, there is a need to understand and evaluate (a) the application of assurance cases to software, and (b) the relationship between the development and assessment of assurance cases, and software engineering concepts, processes and techniques. The ICSE 2013 Workshop on Assurance Cases for Software-intensive Systems (ASSURE) aims to provide an international forum for high-quality contributions (research, practice, and position papers) on the application of assurance case principles and techniques for software assurance, and on the treatment of assurance cases as artifacts to which the full range of software engineering techniques can be applied.
[Conferences, argumentation, evidence, NASA, Educational institutions, Standards, certification, security, safety, Software, assurance cases, Safety, Software engineering]
8th International Workshop on Automation of Software Test (AST 2013)
2013 35th International Conference on Software Engineering
None
2013
This paper is a report on The 8th IEEE/ACM International Workshop on Automation of Software Test (AST 2013) at the 35th International Conference on Software Engineering (ICSE 2013). It sets a special theme on testing-as-a-service (TaaS). Keynote speech and charette discussions are organized around this special theme. Eighteen full research papers and six short papers will be presented in the two-day workshop. The report will give the background of the workshop and the selection of the special theme, and report on the organization of the workshop. The provisional program will be presented with a list of the sessions and papers to be presented at the workshop.
[Software testing, Cloud computing, Automation, Conferences, Software as a service, Service-oriented computing, Educational institutions, Test-as-a-service (TaaS), Automation of Software Test, Software tools, Testing]
1st International workshop on conducting empirical studies in industry (CESI 2013)
2013 35th International Conference on Software Engineering
None
2013
The quality of empirical studies is critical for the success of the Software Engineering (SE) discipline. More and more SE researchers are conducting empirical studies involving the software industry. While there are established empirical procedures, relatively little is known about the dynamics of conducting empirical studies in the complex industrial environments. What are the impediments and how to best handle them? This was the primary driver for organising CESI 2013. The goals of this workshop include having a dialogue amongst the participating practitioners and academics on the theme of this workshop with the aim to produce tangible output that will be summarised in a post-workshop report.
[Industries, software industry, Conferences, Empirical studies, Estimation, Educational institutions, Software, Indexes, Software engineering]
6th International workshop on cooperative and human aspects of software engineering (CHASE 2013)
2013 35th International Conference on Software Engineering
None
2013
Software is created by people for people working in a range of environments and under various conditions. Understanding the cooperative and human aspects of software development is crucial in order to comprehend how methods and tools are used, and thereby improve the creation and maintenance of software. Both researchers and practitioners have recognized the need to investigate these aspects, but the results of such investigations are dispersed in different conferences and communities. The goal of this workshop is to provide a forum for discussing high quality research on human and cooperative aspects of software engineering. We aim to provide both a meeting place for the community and the possibility for researchers interested in joining the field to present and discuss their work in progress and to get an overview over the field.
[Conferences, Communities, Collaboration, Educational institutions, Collaborative work, Software, Management, Performance, Human Factors, Software engineering]
1st International workshop on combining modelling and search-based software engineering (CMSBSE 2013)
2013 35th International Conference on Software Engineering
None
2013
Modelling plays a vital and pervasive role in software engineering: it provides means to manage complexity via abstraction, and enables the creation of larger, more complex systems. Search-based software engineering (SBSE) offers a productive and proven approach to software engineering through automated discovery of near-optimal solutions to problems, and has proven itself to be effective on a wide variety of software-and systems engineering problems. CMSBSE 2013 was a forum allowing researchers from both communities to meet, discuss synergies and differences, and present topics related to the intersection of search and modelling. Particular goals of CMSBSE were to highlight that SBSE and modelling have substantial conceptual and technical synergy, and to identify and present opportunities in which they can be combined, whilst also aiming to grow the community working in this area.
[Conferences, Communities, Educational institutions, Search problems, Software, Modelling, Modeling, Software engineering, Search-based software engineering]
3rd International workshop on collaborative teaching of globally distributed software development (CTGDSD 2013)
2013 35th International Conference on Software Engineering
None
2013
Software engineering project courses where student teams are geographically distributed can effectively simulate the problems of globally distributed software development (DSD). However, this pedagogical model has proven difficult to adopt or sustain. It requires significant pedagogical resources and collaboration infrastructure. Institutionalizing such courses also requires compatible and reliable teaching partners. The purpose of this workshop is to continue building on our outreach efforts to foster a community of international faculty and institutions committed to developing, teaching and researching DSD. Foundational materials presented will include pedagogical materials and infrastructure developed and used in teaching DSD courses along with results and lessons learned. The third CTGDSD workshop will also focus on publishing workshop results and collaborating with the larger DSD community. Longrange goals include: lowering adoption barriers by providing common pedagogical materials, collaboration infrastructure, and a pool of potential teaching partners from around the globe.
[Distributed software development, Conferences, Education, Communities, Collaboration, Materials, Computational thinking, Software, Software engineering]
1st International workshop on data analysis patterns in software engineering (DAPSE 2013)
2013 35th International Conference on Software Engineering
None
2013
Data scientists in software engineering seek insight in data collected from software projects to improve software development. The demand for data scientists with domain knowledge in software development is growing rapidly and there is already a shortage of such data scientists. Data science is a skilled art with a steep learning curve. To shorten that learning curve, this workshop will collect best practices in form of data analysis patterns, that is, analyses of data that leads to meaningful conclusions and can be reused for comparable data. In the workshop we compiled a catalog of such patterns that will help experienced data scientists to better communicate about data analysis. The workshop was targeted at experienced data scientists and researchers and anyone interested in how to analyze data correctly and efficiently in a community accepted way.
[Data analysis, Conferences, data mining, predictive analytics, Data mining, machine learning, Statistics, business intelligence, data science, Sociology, Software, software engineering, software intelligence, big data, smart data, software analytics, Software engineering]
1st FME workshop on formal methods in software engineering (FormaliSE 2013)
2013 35th International Conference on Software Engineering
None
2013
Despite their significant advantages, formal methods are not widely used in industrial software development. FormaliSE is a workshop with the main goal to promote integration between the formal methods and the software engineering communities with the purpose to examine the link between the two more carefully than is done currently, and to better understand the reasons why this still is the case.
[Industries, Conferences, Communities, Europe, Educational institutions, Software, Software engineering]
3rd International workshop on games and software engineering: Engineering computer games to Enable Positive, Progressive Change (GAS 2013)
2013 35th International Conference on Software Engineering
None
2013
We present a summary of the 3rd ICSE Workshop on Games and Software Engineering: Engineering Computer Games to Enable Positive, Progressive Change in this article. The full day workshop is planned to include a keynote speaker, panel discussion, and paper presentations on game software engineering topics related to requirements specification and verification, software engineering education, re-use, and infrastructure. The accepted papers are overviewed here.
[Game engineering, Conferences, Games, Educational institutions, Software, software engineering, Artificial intelligence, Software engineering]
2nd International workshop on green and sustainable software (GREENS 2013)
2013 35th International Conference on Software Engineering
None
2013
ICT accounts for approximately 2% of world CO2 emissions, a figure equivalent to aviation, according to Gartner estimates. In the remaining 98% software counts for both operationalizing the private sector in doing its business and the public sector in supporting the society, as well as delivering enduser applications that permeate personal life of individuals and families. Software can contribute to decrease power consumption (i.e., become greener) in at least two ways. First, by being more energy efficient, hence using fewer resources and causing fewer CO2 emissions. Second, by making its processes more sustainable, i.e. decreasing the emissions of governments, companies and individuals. To this end, enterprise software must be rethought to address sustainability issues and support innovative business models and processes. The special theme of the second edition of GREENS is &#x201C;Leveraging energy efficiency to software users&#x201D;. This workshop brings together software engineering researchers and practitioners to discuss the state-of-the-art and state-of-the-practice in green software, as well as research challenges, novel ideas, methods, experiences, and tools to support the engineering of sustainable and energy efficient software systems.
[green IT, Conferences, Green products, green design, energy efficiency, Air pollution, Educational institutions, Software, Energy efficiency, sustainability, Software engineering]
2nd SEMAT workshop on a general theory of software engineering (GTSE 2013)
2013 35th International Conference on Software Engineering
None
2013
Most academic disciplines emphasize the importance of their general theories. Examples of well-known general theories include the Big Bang theory, Maxwell's equations, the theory of the cell, the theory of evolution, and the theory of demand and supply. Less known to the wider audience, but established within their respective fields, are theories with names such as the general theory of crime and the theory of marriage. Few general theories of software engineering have, however, been proposed, and none have achieved significant recognition. This workshop, organized by the SEMAT initiative, aims to provide a forum for discussing the concept of a general theory of software engineering. The topics considered include the benefits, the desired qualities, the core components and the form of a such a theory.
[Jacobian matrices, General Theory, Conferences, Communities, Software Engineering, Psychology, Educational institutions, Software, Software engineering]
7th International Workshop on Software Clones (IWSC 2013)
2013 35th International Conference on Software Engineering
None
2013
Software Clones are identical or similar pieces of code, models or designs. In this, the 7th International Workshop on Software Clones (IWSC), we will discuss issues in software clone detection, analysis and management, as well as applications to software engineering contexts that can benefit from knowledge of clones. These are important emerging topics in software engineering research and practice. Special emphasis will be given this time to clone management in practice, emphasizing use cases and experiences. We will also discuss broader topics on software clones, such as clone detection methods, clone classification, management, and evolution, the role of clones in software system architecture, quality and evolution, clones in plagiarism, licensing, and copyright, and other topics related to similarity in software systems. The format of this workshop will give enough time for intense discussions.
[Conferences, Biological system modeling, Cloning, Computer architecture, Educational institutions, Software systems]
1st International workshop on live programming (LIVE 2013)
2013 35th International Conference on Software Engineering
None
2013
Live programming is an idea espoused by programming environments from the earliest days of computing (such as Lisp machines and SmallTalk) but have since lain dormant. Recently, the prevalence of asynchronous feedback in programming languages such as Javascript and advances in visualizations and user interfaces have lead to a resurgence of live programming in online education communities (such as Khan Academy) and in experimental IDEs (such as LightTable). The LIVE 2013 workshop includes 11 papers describing visions, implementations, mashups, and new directions of live programming environments. The participants include both practitioners of live coding and researchers in programming languages and software engineering. Finally, several demos curated on the live workshop page are presented.
[Visualization, Computer languages, Conferences, Encoding, Programming profession, Software engineering]
5th International workshop on Modeling in Software Engineering (MiSE 2013)
2013 35th International Conference on Software Engineering
None
2013
Models are an important tool in conquering the increasing complexity of modern software systems. Key industries are strategically directing their development environments towards more extensive use of modeling techniques. This workshop sought to understand, through critical analysis, the current and future uses of models in the engineering of software-intensive systems. The MISE-workshop series has proven to be an effective forum for discussing modeling techniques from the MDD and the software engineering perspectives. An important goal of this workshop was to foster exchange between these two communities. The 2013 Modeling in Software Engineering (MiSE) workshop was held at ICSE 2013 in San Francisco, California, during May 18&#x2013;19, 2013. The focus this year was analysis of successful applications of modeling techniques in specific application domains to determine how experiences can be carried over to other domains. Details are available at: https://sselab.de/lab2/public/wiki/MiSE/index.php.
[Analytical models, Adaptation models, Conferences, Unified modeling language, model-driven engineering, model transformations, Educational institutions, Software, model management, Modeling languages]
1st International workshop on the engineering of mobile-enabled systems (MOBS 2013)
2013 35th International Conference on Software Engineering
None
2013
Mobile-enabled systems make use of mobile devices, RFID tags, sensor nodes, and other computing-enabled mobile devices to gather contextual data from users and the surrounding changing environment. Such systems produce computational data that can be stored and used in the field, shared between mobile and resident devices, and potentially uploaded to local servers or the cloud &#x2014; a distributed, heterogeneous, context-aware, data production and consumption paradigm. Mobile-enabled systems have characteristics that make them different from traditional systems, such as limited resources, increased vulnerability, performance and reliability variability, and a finite energy source. There is significantly higher unpredictability in the execution environment of mobile apps. This workshop brings together experts from the software engineering and mobile computing communities &#x2014; with notable participation from researchers and practitioners in the field of distributed systems, enterprise systems, cloud systems, ubiquitous computing, wireless sensor networks, and pervasive computing &#x2014; to share results and open issues in the area of software engineering of mobile-enabled systems.
[mobile-enabled systems, Conferences, mobile systems, Mobile communication, Educational institutions, systems architecture, pervasive computing, ubiquitous computing, software architecture, mobile computing, BYOD, software engineering, cloud computing, Mobile computing, Smart phones, Software engineering]
4th International workshop on managing technical debt (MTD 2013)
2013 35th International Conference on Software Engineering
None
2013
Although now 20 years old, only recently has the concept of technical debt gained some momentum and credibility in the software engineering community. The goal of this fourth workshop on managing technical debt is to engage researchers and practitioners in exchanging ideas on viable research directions and on how to put the concept to actual use, beyond its usage as a rhetorical instrument to discuss the fate and ailments of software development projects. The workshop participants presented and discussed approaches to detect, analyze, visualize, and manage technical debt, in its various forms, on large software-intensive system developments.
[Economics, Conferences, Communities, Blogs, Technical debt, Educational institutions, Software, software quality, software economics, maintainability, Software engineering, software evolution]
1st International workshop on natural language analysis in software engineering (NaturaLiSE 2013)
2013 35th International Conference on Software Engineering
None
2013
Software engineers produce code that has formal syntax and semantics, which establishes its formal meaning. However, the code also includes significant natural language found primarily in identifier names and comments. Furthermore, the code is surrounded by non-source artifacts, predominantly written in natural language. The NaturaLiSE workshop focuses on natural language analysis of software. The workshop brings together researchers and practitioners interested in exploiting natural language information to create improved software engineering tools. Participants will explore natural language analysis applied to software artifacts, combining natural language and traditional program analysis, integration of natural language analyses into client tools, mining natural language data, and empirical studies focused on evaluating the usefulness of natural language analysis.
[Software maintenance, Conferences, Natural language analysis of software artifacts, Natural languages, textual analysis, Educational institutions, Data mining, software engineering tools, Software engineering, software evolution]
5th International workshop on principles of engineering service-oriented systems (PESOS 2013)
2013 35th International Conference on Software Engineering
None
2013
PESOS 2013 is a forum that brings together software engineering researchers from academia and industry, as well as practitioners working in the areas of service-oriented systems to discuss research challenges, recent developments, novel application scenarios, as well as methods, techniques, experiences, and tools to support engineering, evolution and adaptation of service-oriented systems. The special theme of the 5th edition of PESOS is &#x201C;Service Engineering for the Cloud&#x201D; The goal is to explore approaches to better engineer service-oriented systems, to either take advantage of the qualities offered by cloud infrastructures or to account for lack of full control over important quality attributes. PESOS 2013 also continues to be the key forum for collecting case studies and artifacts for educators and researchers in this area.
[Cloud computing, service-oriented systems, cloud services, Conferences, SOA, Computer architecture, Educational institutions, Service-oriented architecture, services, cloud computing, service-oriented architecture, Software engineering]
4th International Workshop on Product LinE Approaches in Software Engineering (PLEASE 2013)
2013 35th International Conference on Software Engineering
None
2013
This paper summarizes PLEASE 2013, the Fourth International Workshop on Product LinE Approaches in Software Engineering. The main goal of PLEASE is to encourage and promote the adoption of Software Product Line Engineering. To this end, we aim at bringing together researchers and industrial practitioners involved in developing families of related products in order to (1) facilitate a dialogue between these two groups and (2) initiate and foster long-term collaborations.
[Conferences, Collaboration, Educational institutions, Software, Electronic mail, Complexity theory, Software engineering]
2nd International workshop on realizing artificial intelligence synergies in software engineering (RAISE 2013)
2013 35th International Conference on Software Engineering
None
2013
The RAISE'13 workshop brought together researchers from the AI and software engineering disciplines to build on the interdisciplinary synergies which exist and to stimulate research across these disciplines. The first part of the workshop was devoted to current results and consisted of presentations and discussion of the state of the art. This was followed by a second part which looked over the horizon to seek future directions, inspired by a number of selected vision statements concerning the AI-and-SE crossover. The goal of the RAISE workshop was to strengthen the AI-and-SE community and also develop a roadmap of strategic research directions for AI and software engineering.
[computational intelligence, Conferences, Communities, data mining, AI, Educational institutions, Software, Cognition, software engineering, machine learning, Artificial intelligence, Software engineering]
1st International Workshop on Release Engineering (RELENG 2013)
2013 35th International Conference on Software Engineering
None
2013
Release engineering deals with all activities in between regular development and actual usage of a software product by the end user, i.e., integration, build, test execution, packaging and delivery of software. Although research on this topic goes back for decades, the increasing heterogeneity and variability of software products along with the recent trend to reduce the release cycle to days or even hours starts to question some of the common beliefs and practices of the field. For example, a project like Mozilla Firefox releases every 6 weeks, generating updates for dozens of existing Firefox versions on 5 desktop, 2 mobile and 3 mobile desktop platforms, each of which for more than 80 locales. In this context, the International Workshop on Release Engineering (RELENG) aims to provide a highly interactive forum for researchers and practitioners to address the challenges of, find solutions for and share experiences with release engineering, and to build connections between the various communities.
[Conferences, Packaging, Maintenance engineering, Educational institutions, Software, Software engineering, Testing]
5th International workshop on software engineering for Computational Science and Engineering (SE-CSE 2013)
2013 35th International Conference on Software Engineering
None
2013
Computational Science and Engineering (CSE) software supports a wide variety of domains including nuclear physics, crash simulation, satellite data processing, fluid dynamics, climate modeling, bioinformatics, and vehicle development. The increases importance of CSE software motivates the need to identify and understand appropriate software engineering (SE) practices for CSE. Because of the uniqueness of the CSE domain, existing SE tools and techniques developed for the business/IT community are often not efficient or effective. Appropriate SE solutions must account for the salient characteristics of the CSE development environment. SE community members must interact with CSE community members to understand this domain and to identify effective SE practices tailored to CSE's needs. This workshop facilitates that collaboration by bringing together members of the CSE and SE communities to share perspectives and present findings from research and practice relevant to CSE software and CSE SE education. A significant portion of the workshop is devoted to focused interaction among the participants with the goal of generating a research agenda to improve tools, techniques, and experimental methods for CSE software engineering.
[Scientific computing, Conferences, Communities, Education, Software Engineering, Computational Engineering, Computational Science, Software, Software engineering, Business]
5th International workshop on Software Engineering in Health Care (SEHC 2013)
2013 35th International Conference on Software Engineering
None
2013
Our ability to deliver timely, effective and cost efficient healthcare services remains one of the world's foremost challenges. The challenge has numerous dimensions including the need to develop: (a) a highly functional yet secure electronic health record system that integrates a multitude of incompatible existing systems, (b) in-home patient support systems to reduce demand on professional health-care facilities, and (c) innovative technical devices such as advanced pacemakers that support other healthcare procedures. Responding to this challenge will necessitate increased development and usage of softwareintensive systems in all aspects of healthcare services. However the increased digitization of healthcare has identified extensive requirements related to the development, use, evolution, and integration of health software in areas such as the volume and dependability of software required, and the safety and security of the associated devices. The goal of the fifth workshop on Software Engineering for Health Care was to discuss recent research innovations and to continue developing an interdisciplinary community to develop a research, educational and industrial agenda for supporting software engineering in the health care sector.
[evaluation, Conferences, Communities, work processes, Medical services, Educational institutions, systems design, Software, interoperability, Electronic medical records, Software engineering, healthcare]
4th International Workshop on Software Engineering for sensor network applications (SESENA 2013)
2013 35th International Conference on Software Engineering
None
2013
We introduce SESENA 2013, the fourth in a series of workshops devoted to software engineering for sensor network applications. The workshop took place in San Francisco (USA) on May 21, 2013, in conjunction with the 35th ACM/IEEE International Conference on Software Engineering (ICSE). The goal was to bring together research from both the field of software engineering (SE) and wireless sensor networks (WSN) to engender exchange and discussion on shared research goals and agendas. More information can be found on the workshop website http://www.sesena.info.
[Wireless sensor networks, Conferences, Communities, Educational institutions, Emergency services, Software, Software engineering]
2nd International Workshop on Software Engineering Challenges for the Smart Grid (SE4SG 2013)
2013 35th International Conference on Software Engineering
None
2013
The 2nd International Workshop on Software Engineering Challenges for the Smart Grid focuses on understanding and identifying the unique challenges and opportunities for SE to contribute to and enhance the design and development of the smart grid. In smart grids, the geographical scale, requirements on real-time performance and reliability, and diversity of application functionality all combine to produce a unique, highly demanding problem domain for SE to address. The objective of this workshop is to bring together members of the SE community and the power engineering community to understand these requirements and determine the most appropriate SE tools, methods and techniques.
[Industries, Conferences, Communities, Software Engineering, Software, Smart grids, Reliability, Software engineering, Smart Grid]
3rd International workshop on developing tools as plug-ins (TOPI 2013)
2013 35th International Conference on Software Engineering
None
2013
TOPI (http://se.inf.ethz.ch/events/topi2013/) is a workshop started in 2011 to address research questions involving plug-ins: software components designed and written to execute within an extensible platform. Most such software components are tools meant to be used within a development environment for constructing software. Other environments are middle-ware platforms and web browsers. Research on plug-ins encompasses the characteristics that differentiate them from other types of software, their interactions with each other, and the platforms they extend.
[]
2nd International workshop on the twin peaks of requirements and architecture (TwinPeaks 2013)
2013 35th International Conference on Software Engineering
None
2013
The disciplines of requirements engineering (RE) and software architecture (SA) are fundamental to the success of software projects. Even though RE and SA are often considered separately, it has been argued that drawing a line between RE and SA is neither feasible nor reasonable as requirements and architectural design processes impact each other. Requirements are constrained by what is feasible technically and also by time and budget restrictions. On the other hand, feedback from the architecture leads to renegotiating architecturally significant requirements with stakeholders. The topic of bridging RE and SA has been discussed in both the RE and SA communities, but mostly independently. Therefore, the motivation for this ICSE workshop is to bring both communities together in order to identify key issues, explore the state of the art in research and practice, identify emerging trends, and define challenges related to the transition and the relationship between RE and SA.
[software architecture, Software architecture, Conferences, Communities, Collaboration, Computer architecture, twin peaks, Educational institutions, Software, Requirements engineering]
2nd International workshop on user evaluations for software engineering researchers (USER 2013)
2013 35th International Conference on Software Engineering
None
2013
We have met many software engineering researchers who would like to evaluate a tool or system they developed with real users, but do not know how to begin. In this second iteration of the USER workshop, attendees will collaboratively design, develop, and pilot plans for conducting user evaluations of their own tools and/or software engineering research projects. Attendees will gain practical experience with various user evaluation methods through scaffolded group exercises, panel discussions, and mentoring by a panel of user-focused software engineering researchers. Together, we will establish a community of like-minded researchers and developers to help one another improve our research and practice through user evaluation.
[Conferences, Programming, Educational institutions, Usability, Software engineering]
4th International workshop on Emerging Trends in Software Metrics (WETSoM 2013)
2013 35th International Conference on Software Engineering
None
2013
The International Workshop on Emerging Trends in Software Metrics aims at gathering together researchers and practitioners to discuss the progress of software metrics. The motivation for this workshop is the low impact that software metrics has on current software development. The goals of this workshop includes critically examining the evidence for the effectiveness of existing metrics and identifying new directions for metrics. Evidence for existing metrics includes how the metrics have been used in practice and studies showing their effectiveness. Identifying new directions includes use of new theories, such as complex network theory, on which to base metrics.
[Software metrics, Conferences, Complex networks, Software Metrics, Educational institutions, Market research, Software, Software Quality]
Message from the General and Program Chairs - Volume 2
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Committees - Volume 2
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Provides a listing of current committee members and society officers.
[]
The Future of Software Engineering (SEIP Keynote)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Summary form only given. No matter what future we may envision, it relies on software that has not yet been written. Even now, software-intensive systems have woven themselves into the interstitial spaces of civilization, and we as individuals and as a species have slowly surrendered ourselves to computing. Looking back, we can identify several major and distinct styles whereby we have built such systems. We have come a long way, and even today, we certainly can name a number of best practices for software development that yield systems of quality. However, by no means can we stand still: the nature of the systems we build continues to change, and as they collectively weave themselves into our live, we must attend not only to the technical elements of software development, we must also attend to human needs. In this presentation we will look at the history of software engineering and offer some grand challenges for the future.
[software development, Conferences, history, History, interstitial civilization spaces, software-intensive systems, Best practices, future, Computer architecture, Software, Weaving, software engineering, Software engineering]
Enron's Spreadsheets and Related Emails: A Dataset and Analysis
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Spreadsheets are used extensively in business processes around the world and as such, are a topic of research interest. Over the past few years, many spreadsheet studies have been performed on the EUSES spreadsheet corpus. While this corpus has served the spreadsheet community well, the spreadsheets it contains are mainly gathered with search engines and might therefore not represent spreadsheets used in companies. This paper presents an analysis of a new dataset, extracted from the Enron email archive, containing over 15,000 spreadsheets used within the Enron Corporation. In addition to the spreadsheets, we also present an analysis of the associated emails, where we look into spreadsheet-specific email behavior. Our analysis shows that 1) 24% of Enron spreadsheets with at least one formula contain an Excel error, 2) there is little diversity in the functions used in spreadsheets: 76% of spreadsheets in the presented corpus use the same 15 functions and, 3) the spreadsheets are substantially more smelly than the EUSES corpus, especially in terms of long calculation chains. Regarding the emails, we observe that spreadsheets 1) are a frequent topic of email conversation with 10% of emails either referring to or sending spreadsheets and 2) the emails are frequently discussing errors in and updates to spreadsheets.
[Measurement, Economics, Industries, Excel error, Enron Corporation, email analysis, data analysis, calculation chain, electronic mail, Companies, spreadsheet programs, Electronic mail, Enron email archive, spreadsheet-specific email behavior, emails, email conversation, business process, Software, financial data processing, EUSES spreadsheet corpus, Software engineering, Enron spreadsheet]
An Empirical Study on Quality Issues of Production Big Data Platform
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Big Data computing platform has evolved to be a multi-tenant service. The service quality matters because system failure or performance slowdown could adversely affect business and user experience. There is few study in literature on service quality issues of production Big Data computing platform. In this paper, we present an empirical study on the service quality issues of Microsoft ProductA, which is a company-wide multi-tenant Big Data computing platform, serving thousands of customers from hundreds of teams. ProductA has a well-defined incident management process, which helps customers report and mitigate service quality issues on 24/7 basis. This paper explores the common symptom, causes and mitigation of service quality issues in Big Data computing. We conduct an empirical study on 210 real service quality issues in ProductA. Our major findings include (1) 21.0% of escalations are caused by hardware faults; (2) 36.2% are caused by system side defects; (3) 37.2% are due to customer side faults. We also studied the general diagnosis process and the commonly adopted mitigation solutions. Our findings can help improve current development and maintenance practice of Big Data computing platform, and motivate tool support.
[system side defects, service quality, Programming, Big Data, hardware faults, Iron, incident management process, Electronic mail, software quality, production Big Data computing platform, multitenant Big Data computing platform, Microsoft ProductA, customer side faults, Big data, Hardware, Software engineering, Business]
Code Reviews Do Not Find Bugs. How the Current Code Review Best Practice Slows Us Down
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Because of its many uses and benefits, code reviews are a standard part of the modern software engineering workflow. Since they require involvement of people, code reviewing is often the longest part of the code integration activities. Using experience gained at Microsoft and with support of data, we posit (1) that code reviews often do not find functionality issues that should block a code submission; (2) that effective code reviews should be performed by people with specific set of skills; and (3) that the social aspect of code reviews cannot be ignored. We find that we need to be more sophisticated with our guidelines for the code review workflow. We show how our findings from code reviewing practice influence our code review tools at Microsoft. Finally, we assert that, due to its costs, code reviewing practice is a topic deserving to be better understood, systematized and applied to software engineering workflow with more precision than the best practice currently prescribes.
[code integration, program diagnostics, code review workflow, Switches, Inspection, code submission, Software engineering workflow, code reviews, bug finding, program compilers, Best practices, Standards, software engineering workflow, Guidelines, code review tools, Software, software engineering, Microsoft, code review best practice, Software engineering]
Systematic Testing of Reactive Software with Non-Deterministic Events: A Case Study on LG Electric Oven
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Most home appliance devices such as electric ovens are reactive systems which repeat receiving a user input/event through an event handler, updating their internal state based on the input, and generating outputs. A challenge to test a reactive program is to check if the program correctly reacts to various non-deterministic sequence of events because an unexpected sequence of events may make the system fail due to the race conditions between the main loop and asynchronous event handlers. Thus, it is important to systematically generate/test various sequences of events by controlling the order of events and relative timing of event occurrences with respect to the main loop execution. In this paper, we report our industrial experience to solve the aforementioned problem by developing a systematic event generation framework based on concolic testing technique. We have applied the framework to a LG electric oven and detected several critical bugs including one that makes the oven ignore user inputs due to the illegal state transition.
[program debugging, program testing, systematic reactive software testing, reactive systems, systematic event generation framework, LG electric oven, Systematics, Ovens, event handler, main loop execution, critical bugs, Probes, Testing, electrical engineering computing, user input-event, asynchronous event handlers, illegal state transition, Light emitting diodes, ovens, home appliance devices, Computer bugs, concolic testing technique, Software, nondeterministic events, nondeterministic events sequence]
Empirically Detecting False Test Alarms Using Association Rules
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Applying code changes to software systems and testing these code changes can be a complex task that involves many different types of software testing strategies, e.g. system and integration tests. However, not all test failures reported during code integration are hinting towards code defects. Testing large systems such as the Microsoft Windows operating system requires complex test infrastructures, which may lead to test failures caused by faulty tests and test infrastructure issues. Such false test alarms are particular annoying as they raise engineer attention and require manual inspection without providing any benefit. The goal of this work is to use empirical data to minimize the number of false test alarms reported during system and integration testing. To achieve this goal, we use association rule learning to identify patterns among failing test steps that are typically for false test alarms and can be used to automatically classify them. A successful classification of false test alarms is particularly valuable for product teams as manual test failure inspection is an expensive and time-consuming process that not only costs engineering time and money but also slows down product development. We evaluating our approach on system and integration tests executed during Windows 8.1 and Microsoft Dynamics AX development. Performing more than 10,000 classifications for each product, our model shows a mean precision between 0.85 and 0.90 predicting between 34% and 48% of all false test alarms.
[code integration, association rule learning, integration tests, program testing, Windows 8.1, software reliability, software systems, data mining, Manuals, test failures, Inspection, code defects, Microsoft Windows operating system, manual test failure inspection, Association rules, false test alarm detection, complex test infrastructures, Software systems, operating systems (computers), software testing strategy, learning (artificial intelligence), Microsoft Dynamics AX development, Testing, Software engineering]
Striving for Failure: An Industrial Case Study about Test Failure Prediction
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software regression testing is an important, yet very costly, part of most major software projects. When regression tests run, any failures that are found help catch bugs early and smooth the future development work. The act of executing large numbers of tests takes significant resources that could, otherwise, be applied elsewhere. If tests could be accurately classified as likely to pass or fail prior to the run, it could save significant time while maintaining the benefits of early bug detection. In this paper, we present a case study to build a classifier for regression tests based on industrial software, Microsoft Dynamics AX. In this study, we examine the effectiveness of this classification as well as which aspects of the software are the most important in predicting regression test failures.
[software regression testing, Microsoft Dynamics AX industrial software, pattern classification, program testing, data-mining software repositories, software reliability, regression testing, regression analysis, Predictive models, regression test failure prediction, Complexity theory, History, case study, Test failure prediction, software classification, early bug detection, Prediction algorithms, Software, software projects, Testing, Software engineering]
Comparing Software Architecture Recovery Techniques Using Accurate Dependencies
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Many techniques have been proposed to automatically recover software architectures from software implementations. A thorough comparison among the recovery techniques is needed to understand their effectiveness and applicability. This study improves on previous studies in two ways. First, we study the impact of leveraging more accurate symbol dependencies on the accuracy of architecture recovery techniques. Previous studies have not seriously considered how the quality of the input might affect the quality of the output for architecture recovery techniques. Second, we study a system (Chromium) that is substantially larger (9.7 million lines of code) than those included in previous studies. Obtaining the ground-truth architecture of Chromium involved two years of collaboration with its developers. As part of this work we developed a new sub module-based technique to recover preliminary versions of ground-truth architectures. The other systems that we study have been examined previously. In some cases, we have updated the ground-truth architectures to newer versions, and in other cases we have corrected newly discovered inconsistencies. Our evaluation of nine variants of six state-of-the-art architecture recovery techniques shows that symbol dependencies generally produce architectures with higher accuracies than include dependencies. Despite this improvement, the overall accuracy is low for all recovery techniques. The results suggest that (1) in addition to architecture recovery techniques, the accuracy of dependencies used as their inputs is another factor to consider for high recovery accuracy, and (2) more accurate recovery techniques are needed. Our results show that some of the studied architecture recovery techniques scale to the 10M lines-of-code range (the size of Chromium), whereas others do not.
[Java, submodule-based technique, software quality, software architecture recovery techniques, software architecture, Accuracy, Software architecture, accurate symbol dependency, Clustering algorithms, Computer architecture, Chromium, Software, chromium ground-truth architecture]
SPF: A Middleware for Social Interaction in Mobile Proximity Environments
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Smart interconnected devices are changing our lives and are turning conventional spaces into smart ones. Physical proximity, a key enabler of social interactions in the old days is not exploited by smart solutions, where the social dimension is always managed through the Internet. This paper aims to blend the two forces and proposes the idea of social smart space, where modern technologies can help regain and renew social interactions, and where proximity is seen as enabler for dedicated and customized functionality provided by users to users. A Social Proximity Framework (SPF) provides the basis for the creation of this new flavor of smart spaces. Two different versions of the SPF, based on different communication infrastructures, help explain the characteristics of the different components, and show how the SPF can benefit from emerging connection-less communication protocols. A first assessment of the two implementations concludes the paper.
[Context, smart interconnected devices, Protocols, Humanoid robots, social interaction middleware, conventional spaces, SPF, Middleware, smart solutions, mobile proximity environments, mobile computing, social proximity framework, social smart space, communication infrastructures, Internet, connection-less communication protocols, Androids, protocols, IEEE 802.11 Standard, middleware]
Merits of Organizational Metrics in Defect Prediction: An Industrial Replication
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Defect prediction models presented in the literature lack generalization unless the original study can be replicated using new datasets and in different organizational settings. Practitioners can also benefit from replicating studies in their own environment by gaining insights and comparing their findings with those reported. In this work, we replicated an earlier study in order to investigate the merits of organizational metrics in building defect prediction models for large-scale enterprise software. We mined the organizational, code complexity, code churn and pre-release bug metrics of that large scale software and built defect prediction models for each metric set. In the original study, organizational metrics were found to achieve the highest performance. In our case, models based on organizational metrics performed better than models based on churn metrics but were outperformed by pre-release metric models. Further, we verified four individual organizational metrics as indicators for defects. We conclude that the performance of different metric sets in building defect prediction models depends on the project's characteristics and the targeted prediction level. Our replication of earlier research enabled assessing the validity and limitations of organizational metrics in a different context.
[Measurement, Context, organizational metrics, replication, large-scale enterprise software, pre-release bug metric, organizational metric, software reliability, model comparison, Predictive models, code churn metric, defect prediction models, defect prediction, Organizations, code complexity metric, Software, software engineering, Software engineering, Principal component analysis, software metrics]
Online Defect Prediction for Imbalanced Data
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Many defect prediction techniques are proposed to improve software reliability. Change classification predicts defects at the change level, where a change is the modifications to one file in a commit. In this paper, we conduct the first study of applying change classification in practice. We identify two issues in the prediction process, both of which contribute to the low prediction performance. First, the data are imbalanced -- there are much fewer buggy changes than clean changes. Second, the commonly used cross-validation approach is inappropriate for evaluating the performance of change classification. To address these challenges, we apply and adapt online change classification, resampling, and updatable classification techniques to improve the classification performance. We perform the improved change classification techniques on one proprietary and six open source projects. Our results show that these techniques improve the precision of change classification by 12.2-89.5% or 6.4 -- 34.8 percentage points (pp.) on the seven projects. In addition, we integrate change classification in the development process of the proprietary project. We have learned the following lessons: 1) new solutions are needed to convince developers to use and believe prediction results, and prediction results need to be actionable, 2) new and improved classification algorithms are needed to explain the prediction results, and insensible and unactionable explanations need to be filtered or refined, and 3) new techniques are needed to improve the relatively low precision.
[pattern classification, open source projects, imbalanced data, resampling classification techniques, public domain software, software reliability, Predictive models, low prediction performance, online change classification techniques, online defect prediction techniques, cross-validation approach, change classification performance evaluation, Training, updatable classification techniques, proprietary project development process, Computer bugs, Feature extraction, Software, Data models, software performance evaluation, Software engineering]
Measuring Dependency Freshness in Software Systems
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Modern software systems often make use of third-party components to speed-up development and reduce maintenance costs. In return, developers need to update to new releases of these dependencies to avoid, for example, security and compatibility risks. In practice, prioritizing these updates is difficult because the use of outdated dependencies is often opaque. In this paper we aim to make this concept more transparent by introducing metrics to quantify the use of recent versions of dependencies, i.e. The system's "dependency freshness". We propose and investigate a system-level metric based on an industry benchmark. We validate the usefulness of the metric using interviews, analyze the variance of the metric through time, and investigate the relationship between outdated dependencies and security vulnerabilities. The results show that the measurements are considered useful, and that systems using outdated dependencies four times as likely to have security issues as opposed to systems that are up-to-date.
[Context, Industries, cost reduction, security vulnerabilities, object-oriented programming, maintenance cost reduction, software systems, outdated dependencies, dependency freshness measurement, Security, software maintenance, industry benchmark, security of data, system-level metric, third-party components, Software systems, Software measurement, Software engineering, software metrics]
A Large-Scale Technology Evaluation Study: Effects of Model-based Analysis and Testing
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Besides model-based development, model-based quality assurance and the tighter integration of static and dynamic quality assurance activities are becoming increasingly relevant in the development of software-intensive systems. Thus, this paper reports on an empirical study aimed at investigating the promises regarding quality improvements and cost savings. The evaluation comprises data from 13 industry case studies conducted during a three-year large-scale research project in the transportation domain (automotive, avionics, rail system). During the evaluation, we identified major goals and strategies associated with (integrated) model-based analysis and testing and evaluated the improvements achieved. The aggregated results indicate an average cost reduction of between 29% and 34% for verification and validation and of between 22% and 32% for defect removal. Compared with these cost savings, improvements regarding test coverage (~8%), number of remaining defects (~13%), and time to market (~8%) appear less noticeable.
[multiple case study, program testing, program verification, cost savings, defect removal, transportation domain, static quality assurance activities, quality improvements, embedded software quality assurance, model-based quality assurance, software quality, software-intensive systems development, automotive system, GQM+Strategies, Analytical models, software quality assurance, verification, validation, Testing, internal baselines, Context, cost reduction, rail system, quantitative technology evaluation, testing, dynamic quality assurance activities, model-based analysis, avionics, time to market, Atmospheric measurements, Empirical study, model-based testing, model-based development, test coverage, Data models, Software, large-scale technology evaluation study, software cost estimation, Software engineering]
Metamorphic Model-Based Testing Applied on NASA DAT -- An Experience Report
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Testing is necessary for all types of systems, but becomes difficult when the tester cannot easily determine whether the system delivers the correct result or not. NASA's Data Access Toolkit allows NASA analysts to query a large database of telemetry data. Since the user is unfamiliar with the data and several data transformations can occur, it is impossible to determine whether the system behaves correctly or not in full scale production situations. Small scale testing was already conducted manually by other teams and unit testing was conducted on individual functions. However, there was still a need for full scale acceptance testing on a broad scale. We describe how we addressed this testing problem by applying the idea of metamorphic testing [1]. Specifically, we base it on equivalence of queries and by using the system itself for testing. The approach is implemented using a model-based testing approach in combination with a test data generation and test case outcome analysis strategy. We also discuss some of the issues that were detected using this approach.
[metamorphic model-based testing, program testing, data transformations, Computational modeling, NASA, large database, test case outcome analysis strategy, query, small scale testing, unit testing, test data generation, Grammar, Telemetry, telemetry data, NASA DAT, systems testing, query processing, Databases, NASA data access toolkit, aerospace computing, full scale acceptance testing, Testing, Software engineering]
Improving Predictability, Efficiency and Trust of Model-Based Proof Activity
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We report on our industrial experience in using formal methods for the analysis of safety-critical systems developed in a model-based design framework. We first highlight the formal proof workflow devised for the verification and validation of embedded systems developed in Matlab/Simulink. In particular, we show that there is a need to: determine the compatibility of the model to be analysed with the proof engine, establish whether the model facilitates proof convergence or when optimisation is required, and avoid over-specification when specifying the hypotheses constraining the inputs of the model during analysis. We also stress on the importance of having a certain harness over the proof activity and present a set of tools we developed to achieve this purpose. Finally, we give a list of best practices, methods and any necessary tools aiming at guaranteeing the validity of the verification results obtained.
[efficiency improvement, formal proof workflow, safety-critical system analysis, program verification, predictability improvement, safety-critical software, model-based proof activity, embedded system validation, Complexity theory, Verification and Validation, formal specification, Convergence, Analytical models, Model Checking, embedded system verification, model compatibility analysis, Safety, Matlab/Simulink, Mathematical model, Computational modeling, model-based design framework, proof convergence, Model-Based Design, trust improvement, Software packages, Functional Hazard Analysis (FHA), proof engine, formal methods]
Performance Analysis Using Subsuming Methods: An Industrial Case Study
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Large-scale object-oriented applications consist of tens of thousands of methods and exhibit highly complex runtime behaviour that is difficult to analyse for performance. Typical performance analysis approaches that aggregate performance measures in a method-centric manner result in thinly distributed costs and few easily identifiable optimisation opportunities. Subsuming methods analysis is a new approach that aggregates performance costs across repeated patterns of method calls that occur in the application's runtime behaviour. This allows automatic identification of patterns that are expensive and represent practical optimisation opportunities. To evaluate the practicality of this analysis with a real world large-scale object-oriented application we completed a case study with the developers of letterboxd.com - a social network website for movie goers. Using the results of the analysis we were able to rapidly implement changes resulting in a 54.8% reduction in CPU load and an 49.6% reduction in average response time.
[social network Website, average response time reduction, Subsuming methods, application runtime behaviour, Servers, Optimization, Runtime, optimisation, Databases, subsuming methods, complex runtime behaviour, Performance analysis, object-oriented methods, Object oriented software, software performance evaluation, optimisation opportunities, Runtime bloat, pattern recognition, large-scale object-oriented application, Context, subsuming method analysis, CPU load reduction, large-scale object-oriented applications, automatic pattern identification, social networking (online), Time factors, performance analysis]
An Industrial Case Study on the Automated Detection of Performance Regressions in Heterogeneous Environments
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
A key goal of performance testing is the detection of performance degradations (i.e., regressions) compared to previous releases. Prior research has proposed the automation of such analysis through the mining of historical performance data (e.g., CPU and memory usage) from prior test runs. Nevertheless, such research has had limited adoption in practice. Working with a large industrial performance testing lab, we noted that a major hurdle in the adoption of prior work (including our own work) is the incorrect assumption that prior tests are always executed in the same environment (i.e., labs). All too often, tests are performed in heterogenous environments with each test being run in a possibly different lab with different hardware and software configurations. To make automated performance regression analysis techniques work in industry, we propose to model the global expected behaviour of a system as an ensemble (combination) of individual models, one for each successful previous test run (and hence configuration). The ensemble of models of prior test runs are used to flag performance deviations (e.g., CPU counters showing higher usage) in new tests. The deviations are then aggregated using simple voting or more advanced weighting to determine whether the counters really deviate from the expected behaviour or whether it was simply due to an environment-specific variation. Case studies on two open-source systems and a very large scale industrial application show that our weighting approach outperforms a state-of-the-art environment-agnostic approach. Feedback from practitioners who used our approach over a 4 year period (across several major versions) has been very positive.
[program testing, Radiation detectors, public domain software, performance testing, heterogeneous environments, regression analysis, Throughput, performance degradation detection, automated performance regression detection, Association rules, open-source systems, load testing, automated performance regression analysis techniques, Databases, weighting approach, performance regression testing, Bagging, software performance evaluation, performance analysis, performance regression, Testing]
Industry Practices and Event Logging: Assessment of a Critical Software Development Process
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Practitioners widely recognize the importance of event logging for a variety of tasks, such as accounting, system measurements and troubleshooting. Nevertheless, in spite of the importance of the tasks based on the logs collected under real workload conditions, event logging lacks systematic design and implementation practices. The implementation of the logging mechanism strongly relies on the human expertise. This paper proposes a measurement study of event logging practices in a critical industrial domain. We assess a software development process at Selex ES, a leading Finmeccanica company in electronic and information solutions for critical systems. Our study combines source code analysis, inspection of around 2.3 millions log entries, and direct feedback from the development team to gain process-wide insights ranging from programming practices, logging objectives and issues impacting log analysis. The findings of our study were extremely valuable to prioritize event logging reengineering tasks at Selex ES.
[Industries, Industry domain, log entry inspection, Programming, critical industrial domain, Runtime, Source code analysis, software engineering, electronic and information solutions, log analysis, critical software development process assessment, event logging practices, program diagnostics, Inspection, Selex ES, Encoding, Coding practices, event logging mechanism, Development process, Event logging, system monitoring, measurement study, Software, source code analysis, event logging reengineering tasks]
A Case Study in Locating the Architectural Roots of Technical Debt
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Our recent research has shown that, in large-scale software systems, defective files seldom exist alone. They are usually architecturally connected, and their architectural structures exhibit significant design flaws which propagate bugginess among files. We call these flawed structures the architecture roots, a type of technical debt that incurs high maintenance penalties. Removing the architecture roots of bugginess requires refactoring, but the benefits of refactoring have historically been difficult for architects to quantify or justify. In this paper, we present a case study of identifying and quantifying such architecture debts in a large-scale industrial software project. Our approach is to model and analyze software architecture as a set of design rule spaces (DRSpaces). Using data extracted from the project's development artifacts, we were able to identify the files implicated in architecture flaws and suggest refactorings based on removing these flaws. Then we built economic models of the before and (predicted) after states, which gave the organization confidence that doing the refactorings made business sense, in terms of a handsome return on investment.
[large-scale software systems, large-scale industrial software project, History, software maintenance, technical debt, architecture roots, software architecture, design rule spaces, DRSpaces, Microprocessors, Computer architecture, software refactoring, software architecture flaws, Software engineering, Business, Sonar detection]
Design and Evaluation of a Customizable Multi-Domain Reference Architecture on Top of Product Lines of Self-Driving Heavy Vehicles - An Industrial Case Study
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Self-driving vehicles for commercial use cases like logistics or overcast mines increase their owners' economic competitiveness. Volvo maintains, evolves, and distributes a vehicle control product line for different brands like Volvo Trucks, Renault, and Mack in more than 190 markets world-wide. From the different application domains of their customers originates the need for a multi-domain reference architecture concerned with transport mission planning, execution, and tracking on top of the vehicle control product line. This industrial case study is the first of its kind reporting about the systematic process to design such a reference architecture involving all relevant external and internal stakeholders, development documents, low level artifacts, and literature. Quantitative and qualitative metrics were applied to evaluate non-functional requirements on the reference architecture level before a concrete variant was evaluated using a Volvo FMX truck in an exemplary construction site setting.
[reference architecture, customizable multidomain reference architecture, Conferences, logistics, mobile robots, construction site setting, variability, Vehicles, evaluation, economic competitiveness, Bibliographies, road vehicles, Volvo FMX truck, Computer architecture, design, industrial case study, Interviews, control engineering computing, qualitative metrics, transport mission planning, Renault, self-driving vehicles, vehicle control product line, quantitative metrics, overcast mines, commercial use cases, nonfunctional requirements, self-driving heavy vehicles, Mack, product lines, Planning, Stakeholders, multidomain reference architecture]
Approximating Attack Surfaces with Stack Traces
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its attack surface, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Windows 8, the attack surface approximation selected 48.4% of the binaries and contained 94.6% of known vulnerabilities. Compared with vulnerability prediction models (VPMs) run on the entire codebase, VPMs run on the attack surface approximation improved recall from .07 to .1 for binaries and from .02 to .05 for source files. Precision remained at .5 for binaries, while improving from .5 to .69 for source files.
[Measurement, reliability, Predictive models, Security, Approximation methods, vulnerable code identification, security, security testing, software engineering, software projects, models, project management, stack traces, effort reviewing, software development, program diagnostics, attack surface measurement, testing, Computer crashes, attack surface, vulnerability, Surface treatment, decision-making, decision making, Windows 8, attack surface approximation, Software, stack trace analysis]
Avoiding Security Pitfalls with Functional Programming: A Report on the Development of a Secure XML Validator
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
While the use of XML is pervading all areas of IT, security challenges arise when XML files are used to transfer security data such as security policies. To tackle this issue, we have developed a lightweight secure XML validator and have chosen to base the development on the strongly typed functional language OCaml. The initial development took place as part of the LaFoSec Study which aimed at investigating the impact of using functional languages for security. We then turned the validator into an industrial application, which was successfully evaluated at EAL4+ level by independent assessors. In this paper, we explain the challenges involved in processing XML data in a critical context, we describe our choices in designing a secure XML validator, and we detail how we used features of functional languages to enforce security requirements.
[Context, functional programming, XML data processing, XML Security, Security, Computer crime, Standards, security pitfalls avoidance, OCaml functional language, security of data, LaFoSec Study, Software Engineering, XML, Functional Programming, Syntactics, extensible markup language, security policy, security requirements, secure XML validator, Software engineering]
"Should We Move to Stack Overflow?" Measuring the Utility of Social Media for Developer Support
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Stack Overflow is an enormously popular question-and-answer web site intended for software developers to help each other with programming issues. Some software projects aimed at developers (for example, application programming interfaces, application engines, cloud services, development frameworks, and the like) are closing their self-supported developer discussion forums and mailing lists and instead directing developers to use special-purpose tags on Stack Overflow. The goals of this paper are to document the main reasons given for moving developer support to Stack Overflow, and then to collect and analyze data from a group of software projects that have done this, in order to show whether the expected quality of support was actually achieved. The analysis shows that for all four software projects in this study, two of the desired quality indicators, developer participation and response time, did show improvements on Stack Overflow as compared to mailing lists and forums. However, we also found several projects that moved back from Stack Overflow, despite achieving these desired improvements. The results of this study are applicable to a wide variety of software projects that provide developer support using social media.
[self-supported developer discussion forums, developer support, stack overflow, software developers, technical support, forums, mailing lists, mailing list, software engineering, software projects, social media, Message systems, Google, support quality, question-and-answer Web site, Documentation, Media, Stack Overflow, quality, social networking (online), Software, metrics, Time factors, special-purpose tags, Software engineering]
A Field Study on Fostering Structural Navigation with Prodet
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Past studies show that developers who navigate code in a structural manner complete tasks faster and more correctly than those whose behavior is more opportunistic. The goal of this work is to move professional developers towards more effective program comprehension and maintenance habits by providing an approach that fosters structural code navigation. To this end, we created a Visual Studio plugin called Prodet that integrates an always-on navigable visualization of the most contextually relevant portions of the call graph. We evaluated the effectiveness of our approach by deploying it in a six week field study with professional software developers. The study results show a statistically significant increase in developers' use of structural navigation after installing Prodet. The results also show that developers continuously used the filtered and navigable call graph over the three week period in which it was deployed in production. These results indicate the maturity and value of our approach to increase developers' effectiveness in a practical and professional environment.
[Context, source code (software), program comprehension, Visualization, Navigation, Structural Navigation, Maintenance engineering, professional software developers, Code Recommendation, structural code navigation, History, software maintenance, Visual Studio plugin, Prodet, always-on navigable visualization, Software, Field Study, program maintenance habits, program visualisation, call graph, Software engineering, Software Maintenance]
How and When to Transfer Software Engineering Research via Extensions
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
It is often reported that there is a large gap between software engineering research and practice, with little transfer from research to practice. While this is true in general, one transfer technique is increasingly breaking down this barrier: extensions to integrated development environments (IDEs). With the proliferation of app stores for IDEs and increasing transfer effort from researchers several research-based extensions have seen significant adoption. In this talk we'll discuss our experience transferring code search research, which currently is in the top 5% of Visual Studio extensions with over 13,000 downloads, as well as other research techniques transferred via extensions such as NCrunch, FindBugs, Code Recommenders, Mylyn, and Instasearch. We'll use the lessons learned from our transfer experience to provide case study evidence as to best practices for successful transfer, supplementing it with the quantitative evidence offered by app store and usage data across the broader set of extensions. The goal of this 30 minute talk is to provide researchers with a realistic view on which research techniques can be transferred to practice as well as concrete steps to execute such a transfer.
[Visualization, Visual Studio extensions, software engineering research, plugins, integrated development environments, technology transfer, Electronic mail, IDE, case study, research-based extensions, Computer bugs, Prototypes, integrated development environment, Software, software engineering, Software engineering, Testing]
Evolution of Software Development Strategies
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The development of discipline-specific cognitive and meta-cognitive skills is fundamental to the successful mastery of software development skills and processes. This development happens over time and is influenced by many factors, however its understanding by teachers is crucial in order to develop activities and materials to transform students from novice to expert software engineers. In this paper, we analyse the evolution of learning strategies of novice, first year students, to expert, final year students. We analyse reflections on software development processes from students in an introductory software development course, and compare them to those of final year students, in a distributed systems development course. Our study shows that computer science - specific strategies evolve as expected, with the majority of final year students including design before coding in their software development process, but that several areas still require scaffolding activities to assist in learning development.
[Context, computer science education, software development skills, learning strategies, scaffolding activities, discipline-specific meta-cognitive skills development, software coding, software development course, Encoding, teaching, software development strategies, computer science specific strategies, Programming profession, distributed system development course, self regulated learning behavior, educational courses, Software, software engineering, Planning, Software engineering]
Drawing Insight from Student Perceptions of Reflective Design Learning
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
While design and designing are core elements in computer science and software engineering, conventional curricular structures do not adequately support design learning. Current methods tend to isolate the study of design within specific subject matter and lack a strong emphasis on reflection. This paper reports on insights and lessons learned from a user study in the context of ongoing work on developing an educational intervention that better supports design learning with a particular emphasis on learner-driven reflection. Insights drawn from this study relate to general aspects of design learning, such as the importance of collaborative reflection and the impact of learner perceptions regarding their abilities, as well as to specific improvements to our approach.
[Context, computer science education, design learning, reflection, software design, learner-driven reflection, curricular structures, Computer science, Training, educational intervention, structured reflection, computer science, reflective design learning, student perceptions, software engineering education, Software, software engineering, Marine vehicles, Joints, Software engineering, collaborative reflection]
Effectiveness of Persona with Personality Traits on Conceptual Design
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Conceptual design is an important skill in Software Engineering. Teaching conceptual design that can deliver a useful product is challenging, particularly when access to real users is limited. This study explores the effects of the use of Holistic Personas (i.e. a persona enriched with personality traits) on students' performance in creating conceptual designs. Our results indicate that the students were able to identify the personality traits of personas and their ratings of the personalities match closely with the intended personalities. A majority of the participants stated that their designs were tailored to meet the needs of the given personas' personality traits. Results suggest that the Holistic Personas can help students to take into account personality traits in the conceptual design process. Further studies are warranted to assess the value of incorporating Holistic Personas in conceptual design training for imparting skills of producing in-depth design by taking personalities into account.
[Context, computer science education, in-depth design, persona, conceptual design process, Stability analysis, teaching, personality traits, Training, conceptual design, personality, educational courses, conceptual design training, holistic personas, User-Centered Design, software engineering education, Software, software engineering, conceptual design teaching, Joints, Software engineering]
Industry/University Collaboration in Software Engineering Education: Refreshing and Retuning Our Strategies
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This panel session will explore strategies for industry/university collaboration in software engineering education. Specific discussion topics will include new strategies for successful industry/university collaboration, exploration of reasons why some of the old strategies no longer work, and regional/geographical differences noted by the international set of panelists. The panel hopes to identify new promising strategies for such collaborations. Specific industry representatives will be invited to attend and participate in the discussion.
[Industries, computer science education, Conferences, industry-university collaboration, geographical differences, Training, regional differences, Collaboration, computer based training, software engineering education, Software, educational institutions, organisational aspects, Software engineering]
Novice Code Understanding Strategies during a Software Maintenance Assignment
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Existing efforts on teaching software maintenance have focussed on constructing adequate codebases that students with limited knowledge could maintain, with little focus on the learning outcomes of such exercises and of the approaches that students employ while performing maintenance. An analysis of the code understanding strategies employed by novice students as they perform software maintenance exercises is fundamental for the effective teaching of software maintenance. In this paper, we analyze the strategies employed by second year students in a maintenance exercise over a large codebase. We analyze student reflections on their code understanding, maintenance process and the use of tools. We show that students are generally capable of working with large codebases. Our study also finds that the majority of students follow a systematic approach to code understanding, but that their approach can be significantly improved through the use of tools and a better understanding of reverse engineering approaches.
[software maintenance assignment, Software maintenance, computer science education, adequate codebases, Maintenance engineering, reverse engineering, software maintenance, software maintenance exercises, reverse engineering approaches, Systematics, Education, second year students, cognitive models, novice code understanding strategies, learning outcomes, software engineering, novice students, Software engineering, Testing]
Learning Global Agile Software Engineering Using Same-Site and Cross-Site Teams
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We describe an experience in teaching global software engineering (GSE) using distributed Scrum augmented with industrial best practices. Our unique instructional technique had students work in both same-site and cross-site teams to contrast the two modes of working. The course was a collaboration between Aalto University, Finland and University of Victoria, Canada. Fifteen Canadian and eight Finnish students worked on a single large project, divided into four teams, working on interdependent user stories as negotiated with the industrial product owner located in Finland. Half way through the course, we changed the teams so each student worked in both a local and a distributed team. We studied student learning using a mixed-method approach including 14 post-course interviews, pre-course and Sprint questionnaires, observations, meeting recordings, and repository data from git and Flow dock, the primary communication tool. Our results show no significant differences between working in distributed vs. Non-distributed teams, suggesting that Scrum helps alleviate many GSE problems. Our post-course interviews and survey data allows us to explain this effect, we found that students over time learned to better self-select tasks with less inter-team dependencies, to communicate more, and to work better in teams.
[software prototyping, distributed scrum, project, teaching, University of Victoria, Training, team working, Aalto University, Finnish students, GSE, Canadian students, mixed-method approach, global software engineering, Interviews, instructional technique, student learning, computer science education, same-site teams, communication tool, interteam dependencies, distributed Scrum, global agile software engineering learning, cross-site teams, educational courses, Finland, scrum, Software, Teamwork, Planning, educational institutions]
Code Repurposing as an Assessment Tool
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Code repurposing is often used for system development and to learn both APIs and techniques. Repurposing code typically requires that you understand the code first. This makes it an excellent candidate as an assessment tool in computer science and software engineering education. This technique might have a special application in combatting plagiarism. This paper discusses experiences using code repurposing as an assessment tool in different courses and with different sections.
[computer science education, code repurposing, code understanding, plagiarism, Plagiarism Softwaredevelopment, Assessment, Standards, Programming profession, Code Repurposing, Plagiarism, Education, application program interface, system development, software engineering education, Software, API, Cloze Testing, Joints, programming]
Remote Development and Distance Delivery of Innovative Courses: Challenges and Opportunities
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The Rochester Institute of Technology (RIT) offers programs of study at several of RIT's international campuses: Dubrovnik and Zagreb (Croatia), Dubai (United Arab Emirates) and Priatina (Kosovo). At RIT Croatia, some courses are delivered as distance education courses using Polycom, a video conferencing system, supported by other online education tools. Although distance learning methods and tools provide an effective way to offer instructions remotely, delivering a course that emphasizes team-based software development, with laboratory exercises and in-class team activities, creates new challenges that need to be addressed. This paper discusses the authors' experiences with the remote development and delivery of one of those courses - the SWEN-383 Software Design Principles and Patterns course in the Information Technology program at RIT Croatia. The paper first explains the role and need for offering this particular course. It then discusses how the collaborative development of this new course was conducted between the U.S. And the Croatian campuses, including remote delivery from Zagreb to Dubrovnik. The paper concludes with observations and suggestions for those who may engage in such a project in the future.
[Computers, laboratory exercises, course delivery, distance education, SWEN-383 software design principles and patterns course, Dubai, online education tools, remote collaboration, Kosovo, Zagreb, Education, video conferencing system, information technology program, distance education courses, software engineering, Croatia, team-based software development, Context, Google, computer science education, Rochester Institute of Technology, course development, Polycom system, United Arab Emirates, in-class team activities, information technology education, distance learning, team projects, Collaboration, educational courses, Software, RIT, Priatina, Dubrovnik, Software engineering]
Improving Student Group Work with Collaboration Patterns: A Case Study
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Group work skills are essential for Computer Scientists and especially Software Engineers. Group work is included in most CS curricula in order to support students in acquiring these skills. During group work, problems can occur related to a variety of factors, such as unstable group constellations or (missing) instructor support. Students need to find strategies for solving or preventing such problems. Student collaboration patterns offer a way of supporting students by providing problem-solving strategies that other students have already applied successfully. In this work we describe how student collaboration patterns were applied in an interdisciplinary software engineering project, and show that their application was generally experienced as helpful by the students.
[computer science education, student collaboration patterns, Conferences, student group work skills, Team working, Group Work, Computer science, CS curricula, problem-solving strategy, Collaboration Patterns, interdisciplinary software engineering project, Student Projects, Software, software engineering, Teamwork, Software engineering]
Teaching Software Systems Thinking at The Open University
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The Open University is a distance-based higher education institution. Most of our students are in employment and study from home, contacting their tutor and fellow students via e-mail and discussion forums. In this paper, we describe our undergraduate and postgraduate modules in the software systems area, how we teach them at a distance, and our focus on shifting our students' minds into a reflective, critical, holistic socio-technical view of software systems that is relevant to their particular professional contexts.
[critical view, teaching, discussion forums, Information systems, Training, Open University, e-mail, software engineering, distance-based higher education institution, Context, computer science education, further education, reflective view, software system thinking teaching, postgraduate modules, holistic socio-technical view, professional contexts, distance learning, professional aspects, undergraduate modules, Software systems, student employment, computer aided instruction, educational institutions, Software engineering]
Masters-Level Software Engineering Education and the Enriched Student Context
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Currently, adult higher education software engineering pedagogy isolates the student in a controlled environment during delivery, with application of their learning temporally distant from their professional practice. Delivering software engineering teaching that is immediately relevant to professional practice remains an open challenge. In this paper, we discuss a new pedagogical model which addresses this problem by embedding the validation of the student's learning within their rich professional context. We discuss our experience of applying the model to the design and delivery of a new post-graduate software development module, a core component in our new software engineering Masters qualification at the Open University, UK, a market leader in adult higher education at a distance.
[Context, Knowledge engineering, computer science education, Masters, professional context, further education, masters-level software engineering education, Unified modeling language, distance education, pedagogical model, post-graduate software development module, Open University, Education, software engineering education, Software, software engineering, adult higher education software engineering pedagogy, Software engineering, Context modeling]
Combining Mastery Learning with Project-Based Learning in a First Programming Course: An Experience Report
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
One of the challenges in teaching a first programming course is that in the same course, the students must learn basic programming techniques and high level abstraction abilities, and the application of those techniques and concepts in problem solving and (engineering) design. To confront this challenge, in previous years, we have included a project-based learning phase at the end of our course to encourage the acquisition of high level design and creativity. To address some of the shortcomings of our previous editions, we have recently included a mastery phase to the course. While project-based learning is suitable for teaching high-level skills that require design and creativity and prepare the students for the study of software engineering, mastery-based learning is suitable for concrete skills such as basic programming tasks. Our particular innovation is to allow students into the project phase only if they have demonstrated a minimum predefined competency level in programming. The combination of the two approaches seems to address most of the requirements of a first programming course. We present our motivation for combining the two pedagogical techniques and our experience with the course.
[Java, computer science education, mastery learning, first programming course, high-level skills teaching, teaching, basic programming techniques, introductory programming, high level abstraction abilities, Programming profession, Education, educational courses, programming course, project-based learning, CS1/CS2, software engineering education, Concrete, Software, programming competency level, programming, Software engineering]
Collaborative and Cooperative-Learning in Software Engineering Courses
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Collaborative learning is a key component of software engineering (SE) courses in most undergraduate computing curricula. Thus these courses include fairly intensive team projects, the intent being to ensure that not only do students develop an understanding of key software engineering concepts and practices, but also develop the skills needed to work effectively in large design and development teams. But there is a definite risk in collaborative learning in that there is a potential that individual learning gets lost in the focus on the team's success in completing the project (s). While the team's success is indeed the primary goal of an industrial SE team, ensuring individual learning is obviously an essential goal of SE courses. We have developed a novel approach that exploits the affordances of mobile and web technologies to help ensure that individual students in teams in SE courses develop a thorough understanding of the relevant concepts and practices while working on team projects, indeed, that the team contributes in an essential manner to the learning of each member of the team. We describe the learning theory underlying our approach, provide some details concerning the prototype implementation of a tool based on the approach, and describe how we are using it in an SE course in our program.
[Electronic publishing, computer science education, Cooperative Learning, Web technology, mobile technology, software engineering courses, SE courses, Collaborative Learning, cooperative-learning, mobile computing, Education, Information services, Collaboration, educational courses, collaborative learning, industrial SE team, Collaborative work, software engineering, Use of technology in classroom, Internet, undergraduate computing curricula, learning theory, Software engineering]
Using GSwE2009 for the Evaluation of a Master Degree in Software Engineering in the Universidad de la Rep&#x00FA;blica
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This paper presents an adoption and adaptation of the Curriculum Guidelines for Graduate Degree Programs in Software Engineering (GSwE2009) proposed by the IEEE-CS and the ACM for the creation of a curriculum for a Master's degree in software engineering at the Universidad de la Repu&#x0301;blica (Uruguay). A method for evaluating contents and its application is also presented. This evaluation allows us to know the obtained thematic coverage, effort and balance. It also provides information that enables the detection of numerous opportunities for the improvement in the implementation of the program.
[computer science education, further education, master degree, contents evaluation, Uruguay, curriculum guidelines adaptation, educational administrative data processing, Universidad de la Repu&#x0301;blica, ACM, GSwE2009, graduate degree programs, Guidelines, Training, IEEE-CS, Computer architecture, Software, software engineering, educational institutions, curriculum guidelines adoption, Joints, Software engineering, curriculum creation]
System Thinking: Educating T-Shaped Software Engineers
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
With respect to system thinking, a T-shaped person is one who has technical depth in at least one aspect of the system's content, and a workable level of understanding of a fair number of the other system aspects. Many pure computer science graduates are strongly I-shaped, with a great deal of depth in software technology, but little understanding of the other disciplines involved in such areas as business, medicine, transportation, or Internets of Things. This leaves them poorly prepared to participate in the increasing numbers of projects involving multi-discipline system thinking, and in strong need of software skills. We have developed and evolved an MS-level software engineering curriculum that enables CS majors to become considerably more T-shaped than when they entered. It includes courses in software management and economics, human-computer interaction, embedded software systems, systems and software requirements, architecture, and V&amp;V, and a two-semester, real-client team project course that gives students experience in applying these skills. We find via feedback on the students' internships and job experiences that they and their employers have high rates of success in job offers and job performance.
[MS-level software engineering curriculum, T-shaped, embedded software system, software management, system thinking, software architecture, embedded systems, Hardware, Systems Engineering, computer science education, further education, software technology, multidiscipline system, The Incremental Commitment Spiral Model, Internet of Things, Aircraft propulsion, software skill, human-computer interaction, computer science graduate, I-shaped, Software Engineering, systems engineering, System Thinking, Curriculum, Software, human computer interaction, software requirement, Stakeholders, real-client team project course, Aircraft, Education and Training, Software engineering, system content]
Transparently Teaching in the Context of Game-based Learning: the Case of SimulES-W
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This work presents a pedagogical proposal, in the context of game-based learning (GBL), that uses the concept of Transparency Pedagogy. As such, it aims to improve the quality of teaching, and the relationship between student, teacher and teaching methods. Transparency is anchored in the principle of information disclosure. In pedagogy, transparency emerges as an important issue that proposes to raise student awareness about the educational processes. Using GBL as an educational strategy we managed to make the game, a software, transparent. That is we made the inner processes of the game known to the students. As such, besides learning by playing, students had access to the game design, through intentional modeling. We collected evidence that, by disclosure of the information about the design, students better performed on learning software engineering.
[Transparency, game-based learning, SimulES-W, GBL, learning software engineering, transparency pedagogy, educational process, educational strategy, Pedagogy, computer games, software engineering, Games-based Learning, computer aided instruction]
Educating Software Engineering Managers - Revisited What Software Project Managers Need to Know Today
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
In 2003, the original paper with this title was published as part of CSEET 2003. It focused on resolving communication issues between software project managers and developers and introduced a corporate strategy based means of evaluating software engineers. Now, more than a decade later, we could benefit from what we have learned in other fields about managing people involved in knowledge work and how to improve our success in software development. But are we? This paper is intended to present what Software Engineering students can be taught today that will help them to be successful as software project managers now and in the future. It is based on the premise that effective software project managers are not born but made through education.
[Schedules, computer science education, project management, Project management, software development management, software engineering manager education, Training, software project managers, Software Project Management, Project Management, Software Project Management Curriculum, Software, software project developers, corporate strategy, Software engineering, software engineering students]
Contest Based Learning with Blending Software Engineering and Business Management: For Students' High Motivation and High Practice Ability
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We began implementing contest-based learning with a blend of software engineering and business management 10 years ago. At first, a project subject was assigned. However, several problems occurred: For example, the students became absorbed in programming rather than design and analysis activities. Therefore, the curriculum changed from project subjects to contest-based learning. Business management, marketing, and accounting subjects were added to the new curriculum, and students made information technology (IT) business plans using their knowledge of software engineering and business management. The IT business plans were submitted to various contests held by public newspaper companies and the federation of economic organizations in Japan. As a result, in the 10 years of the contest-based learning implementation, 20 teams have received awards in various IT business plan contests. We investigated 10 persons who had experience submitting business plans. We confirmed that contest-based learning had clearer goals, such as to win the contest prize, compared to project-based learning. Further, the abilities to solve problems and to investigate increased more in comparison with lecture-style and project-style education.
[information technology, human factors, Companies, lecture-style education, blending software engineering, accounting subjects, accounting, blending education, management accounting, high practice ability, Education, software engineering, Contest, project-style education, computer science education, business management, contest based learning, IT business plans, Programming profession, student high motivation, IT business plan, marketing, project-based learning, Software, Software engineering, management education]
Concurrent Software Engineering and Robotics Education
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This paper presents a new, multidisciplinary robotics programming course, reports initial results, and describes subsequent improvements. With equal emphasis on software engineering and robotics, the course teaches students how software engineering applies to robotics. Students learn independently and interactively and gain hands-on experience by implementing robotics algorithms on a real robot. To understand the effects of the course, we conducted an exit and an 8-month survey and measured software quality of the students' solutions. The analysis shows that the hands-on experience helped everyone learn and retain robotics well, but the students' knowledge gain in software engineering depended on their prior programming knowledge. Based on these findings, we propose improvements to the course. Lastly, we reflect our experience on andragogy, minimalism, and interactive learning.
[computer science education, control engineering education, students independent learning, hands-on experience, student knowledge gain, interactive learning, teaching, robotics education, robotics algorithms, robot programming, software quality, concurrent software engineering, programming knowledge, andragogy, Computer science, minimalism, multidisciplinary robotics programming course, Education, concurrency control, Software quality, Robot sensing systems, control engineering computing, Software engineering]
The Development of a Dashboard Tool for Visualising Online Teamwork Discussions
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Many software development organisations today adopt global software engineering (GSE) and agile models, requiring software engineers to collaborate and develop software in flexible, distributed, online teams. However, many employers have expressed concern that graduates lack teamwork skills and one of the most commonly occurring problems with GSE models are issues with project management. Team managers and educators often oversee a number of teams and the large corpus of data, in combination with agile models, make it difficult to efficiently assess factors such as team role distribution and emotional climate. Current methods and tools for monitoring software engineering (SE) teamwork in both industry and education settings typically focus on member contributions, reflection, or product outcomes, which are limited in terms of real-time feedback and accurate behavioural analysis. We have created a dashboard that extracts and communicates team role distribution and team emotion information in real-time. Our proof of concept provides a real-time analysis of teamwork discussions and visualises team member emotions, the roles they have adopted and overall team sentiment during the course of a collaborative problem-solving project. We demonstrate and discuss how such a tool could be useful for SE team management and training and the development of teamwork skills in SE university courses.
[Industries, SE university courses, real-time analysis, team role distribution, software prototyping, behavioural analysis, training, SE team management, dashboard tool development, real-time feedback, Training, teamwork skill development, data visualisation, groupware, GSE, software development organisations, software tools, global software engineering, Monitoring, software engineering teamwork monitoring, computer science education, project management, software development management, online teamwork discussion visualisation, agile models, emotional climate, Software, Teamwork, collaborative problem-solving project]
Software Design Studio: A Practical Example
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We have been generally successful for transferring software engineering knowledge to industry through various forms of education. However, many challenges in software engineering training remain. A key amongst these is how best to energise software engineering education with real-world software engineering practices. This paper describes our experience of delivering a radically different approach based on the notion of a Software Design Studio. The Software Design Studio is both a lab for students engaged in conceiving, designing and developing software products as well as an approach for teaching software engineering in the lab which emphasizes practical hands-on work and experimentation. The feedback on the Software Design Studio -- from both staff and students -- has been outstanding. Although the programme is designed as a small, elite programme there is interest to see if the teaching methods can be transferred across to the much larger undergraduate programme in Computer Science. In this paper, we provide a detailed description of how our studio works in practice so that others, thinking of tak-ing a studio or studio-inspired approach, can use in designing their own courses.
[software engineering training, Conferences, software engineering teaching, Software engineering education, Software Design Studio, teaching, software product design, software product conceiving, Training, Software design, computer science, product development, teaching methods, software product development, studio approach, studio-inspired approach, Testing, computer science education, courses, software development management, software engineering knowledge, educational courses, software design studio, software engineering education, reflective teaching approach., Software engineering]
Code Hunt: Experience with Coding Contests at Scale
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Mastering a complex skill like programming takes many hours. In order to encourage students to put in these hours, we built Code Hunt, a game that enables players to program against the computer with clues provided as unit tests. The game has become very popular and we are now running worldwide contests where students have a fixed amount of time to solve a set of puzzles. This paper describes Code Hunt and the contest experience it offers. We then show some early results that demonstrate how Code Hunt can accurately discriminate between good and bad coders. The challenges of creating and selecting puzzles for contests are covered. We end up with a short description of our course experience, and some figures that show that Code Hunt is enjoyed by women and men alike.
[Algorithm design and analysis, computer science education, programming skill, coding contest, Code Hunt game, Encoding, Programming profession, Programming contests, Training, unit tests, contest experience, computer games, Games, symbolic execution, computer aided instruction, Joints, programming]
Does Outside-In Teaching Improve the Learning of Object-Oriented Programming?
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Object-oriented programming (OOP) is widely used in the software industry and university introductory courses today. Following the structure of most textbooks, such courses frequently are organised starting with the concepts of imperative and structured programming and only later introducing OOP. An alternative approach is to begin directly with OOP following the Outside-In teaching method as proposed by Meyer. Empirical results for the effects of Outside-In teaching on students and lecturers are sparse, however. We describe the conceptual design and empirical evaluation of two OOP introductory courses from different universities based on Outside-In teaching. The evaluation results are compared to those from a third course serving as the control group, which was taught OOP the "traditional" way. We evaluate the initial motivation and knowledge of the participants and the learning outcomes. In addition, we analyse results of the end term exams and qualitatively analyse the results of interviews with the lecturers and tutors. Regarding the learning outcomes, the results show no significant differences between the Outside-In and the "traditional" teaching method. In general, students found it harder to solve and implement algorithmic problems than to understand object oriented (OO) concepts. Students taught OOP by the Outside-In method, however, were less afraid that they would not pass the exam at the end of term and understood the OO paradigm more quickly. Therefore, the Outside-In method is no silver bullet for teaching OOP regarding the learning outcomes but has positive effects on motivation and interest.
[Computers, algorithmic problems, Java, computer science education, object-oriented programming, structured programming, Programming profession, OOP, software industry, university introductory courses, Education, imperative programming, Software, computer aided instruction, outside-in teaching method]
Active and Inductive Learning in Software Engineering Education
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
If software engineering education is done in a traditional lecture-oriented style students have no other choice than believing that the solutions they are told actually work for a problem that they never encountered themselves. In order to overcome this problem, this paper describes an approach which allows students to better understand why software engineering and several of its core methods and techniques are needed, thus preparing them better for their professional life. This approach builds on active and inductive learning. Exercises that make students actively discover relevant software engineering issues are described in detail together with their pedagogical underpinning.
[computer science education, higher education, Programming profession, Vehicles, inductive learning, active learning, lecture-oriented style students, didactical approach, Education, software engineering education, Software, software engineering, Capability maturity model]
In Search of the Emotional Design Effect in Programming
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
A small number of recent studies have suggested that learning is enhanced when the illustrations in instructional materials are designed to appeal to the learners' emotions through the use of color and the personification of key elements. We sought to replicate this emotional design effect in the context of introductory object-oriented programming (OOP). In this preliminary study, a group of freshmen studied a text on objects which was illustrated using anthropomorphic graphics while a control group had access to abstract graphics. We found no significant difference in the groups' scores on a comprehension post-test, but the experimental group spent substantially less time on the task than the control group. Among those participants who had no prior programming experience, the materials inspired by emotional design were perceived as less intelligible and appealing and led to lower self-reported concentration levels. Although this result does not match the pattern of results from earlier studies, it shows that the choice of illustrations in learning materials matters and calls for more research that addresses the limitations of this preliminary study.
[Visualization, computer science education, social aspects of automation, object-oriented programming, emotional design effect, learning, control group, Multimedia communication, emotional design, Programming profession, OOP, multimedia, instructional materials, anthropomorphic graphics, comprehension post-test, Education, abstract graphics, self-reported concentration levels, Object oriented programming, programming, learning materials]
Experiences in Developing and Delivering a Programme of Part-Time Education in Software and Systems Security
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We report upon our experiences in developing and delivering a programme of part-time education in Software and Systems Security at the University of Oxford. The MSc in Software and Systems Security is delivered as part of the Software Engineering Programme at Oxford - a collection of one-week intensive courses aimed at individuals who are responsible for the procurement, development, deployment and maintenance of large-scale software-based systems. We expect that our experiences will be useful to those considering a similar journey.
[Access control, Context, Data privacy, computer science education, software development, software engineering programme, software deployment, security education, software maintenance, systems security, security of data, Education, educational courses, one-week intensive courses, software engineering education, Software, large-scale software-based systems, part-time education, University of Oxford, software security, MSc, Software engineering]
Teaching Software Architecture to Undergraduate Students: An Experience Report
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software architecture lies at the heart of system thinking skills for software. Teaching software architecture requires contending with the problem of how to make the learning realistic -- most systems which students can learn quickly are too simple for them to express architectural issues. We address here the ten years' history of teaching an undergraduate software architecture course, as a part of a bachelor's program in software engineering. Included are descriptions of what we perceive the realistic goals to be, of teaching software architecture at this level. We go on to analyze the successes and issues of various approaches we have taken over the years. We finish with recommendations for others who teach this same subject, either as a standalone undergraduate course or integrated into a software engineering course.
[computer science education, further education, Software Architecture, software engineering course, undergraduate students, Course Evolution, teaching, History, Project-Based Learning, Training, software architecture, Software architecture, educational courses, Computer architecture, undergraduate software architecture course teaching, Software]
CS/SE Instructors Can Improve Student Writing without Reducing Class Time Devoted to Technical Content: Experimental Results
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The Computer Science and Software Engineering (CS/SE) profession reports that new college graduates lack the communication skills needed for personal and organizational success. Many CS/SE faculty may omit communication instruction from their courses because they do not want to reduce technical content. We experimented in a software-engineering-intensive second-semester programming course with strategies for improving students' writing of black box test plans that included no instruction on writing the plans beyond the standard lecture on testing. The treatment version of the course used 1) a modified assignment that focused on the plan's readers, 2) a model plan students could consult online, and 3) a modified grading rubric that identified the readers' needs. Three external raters found that students in the treatment sections outperformed students in the control sections on writing for five of nine criteria on rubrics for evaluating the plans and on the raters' holistic impression of the students' technical and communication abilities from the perspectives of a manager and a tester.
[Software testing, software-engineering-intensive second-semester programming course, computer science education, computer science and software engineering profession, communication instruction, Programming, communication across the curriculum, student writing improvement, black box test plans, Employment, Education, educational courses, Writing, software engineering education, software engineering, CS-SE instructors, Software engineering]
Sustainability Design and Software: The Karlskrona Manifesto
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Sustainability has emerged as a broad concern for society. Many engineering disciplines have been grappling with challenges in how we sustain technical, social and ecological systems. In the software engineering community, for example, maintainability has been a concern for a long time. But too often, these issues are treated in isolation from one another. Misperceptions among practitioners and research communities persist, rooted in a lack of coherent understanding of sustainability, and how it relates to software systems research and practice. This article presents a cross-disciplinary initiative to create a common ground and a point of reference for the global community of research and practice in software and sustainability, to be used for effectively communicating key issues, goals, values and principles of sustainability design for software-intensive systems.The centrepiece of this effort is the Karlskrona Manifesto for Sustainability Design, a vehicle for a much needed conversation about sustainability within and beyond the software community, and an articulation of the fundamental principles underpinning design choices that affect sustainability. We describe the motivation for developing this manifesto, including some considerations of the genre of the manifesto as well as the dynamics of its creation. We illustrate the collaborative reflective writing process and present the current edition of the manifesto itself. We assess immediate implications and applications of the articulated principles, compare these to current practice, and suggest future steps.
[Economics, sustainability design, social sustainability, Conferences, Karlskrona manifesto, social systems, History, software maintenance, ethics, software-intensive systems, systems thinking, long-term thinking, environmental sustainability, collaborative reflective writing process, ecological systems, software engineering community, Sustainability, societal sustainability, Software systems, software engineering, economic sustainability, Software engineering, Meteorology, technical sustainability]
Interdisciplinary Design Patterns for Socially Aware Computing
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The success of software applications that collect and process personal data does not only depend on technical aspects, but is also linked to social compatibility and user acceptance. It requires experts from different disciplines to ensure legal compliance, to foster the users' trust, to enhance the usability of the application and to finally realize the application. Multidisciplinary requirements have to be formulated, interwoven and implemented. We advocate the use of interdisciplinary design patterns that capture the design know-how of typical, recurring features in socially aware applications with particular concern for socio-technical requirements. The proposed patterns address interdisciplinary concerns in a tightly interwoven manner and are intended to facilitate the development of accepted and acceptable applications that in particular deal with sensitive user context information.
[Context, Socially Aware Computing, Law, user context information, Interdisciplinary Design Patterns, socio-technical requirements, human factors, software applications, legal compliance, interdisciplinary design patterns, socially aware computing, Venus, software engineering, social compatibility, user acceptance, Usability, user trust, Software engineering, socially aware applications]
The Role of Design Thinking and Physical Prototyping in Social Software Engineering
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Social Software Engineering (Social SE), that is SE aiming to promote positive social change, is a rapidly emerging area. Here, software and digital artefacts are seen as tools for social change, rather than end products or 'solutions'. Moreover, Social SE requires a sustained buy-in from a range of stakeholders and end-users working in partnership with multidisciplinary software development teams often at a distance. This context poses new challenges to software engineering: it requires both an agile approach for handling uncertainties in the software development process, and the application of participatory, creative design processes to bridge the knowledge asymmetries and the geographical distances in the partnership. This paper argues for the role of design thinking in Social SE and highlights its implications for software engineering in general. It does so by reporting on the contributions that design thinking - and in particular physical design - has brought to (1) the problem space definition, (2) user requirements capture and (3) system feature design of a renewable energy forecasting system developed in partnership with a remote Scottish Island community.
[system feature design, physical design, physical prototyping, Conferences, software prototyping, agile approach, multidisciplinary software development teams, remote Scottish island community, Prototypes, knowledge asymmetries, problem space definition, Space exploration, design thinking, Context, social software engineering, renewable energy sources, geographical distances, renewable energy forecasting system, power engineering computing, sustained buy-in, user requirements capture, Games, load forecasting, Software, creative design processes, social SE, Software engineering]
On the Role of Value Sensitive Concerns in Software Engineering Practice
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The role of software systems on societal sustainability has generally not been the subject of substantive research activity. In this paper we examine the role of software engineering practice as an agent of change/impact for societal sustainability through the manifestation of value sensitive concerns. These concerns remain relatively neglected by software design processes except at early stages of user interface design. Here, we propose a conceptual model that can contribute to a translation of value sensitive design from its current focus in participatory design to one located in mainstream software engineering processes. Addressing this need will have an impact of societal sustainability and we outline some of the key research challenges for that journey.
[Computers, value sensitive concerns, social aspects of automation, Conferences, Computational modeling, Unified modeling language, software systems, mainstream software engineering processes, Values, participatory design, Requirements elicitation, user interfaces, Co-Design, value sensitive design, Privacy, societal sustainability, software engineering, user interface design, Stakeholders, Value Sensitive Design, Software engineering]
Engineering Sustainability Through Language
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
As our understanding and care for sustainability concerns increases, so does the demand for incorporating these concerns into software. Yet, existing programming language constructs are not well-aligned with concepts of the sustainability domain. This undermines what we term technical sustainability of the software due to (i) increased complexity in programming of such concerns and (ii) continuous code changes to keep up with changes in (environmental, social, legal and other) sustainability-related requirements. In this paper we present a proof-of-concept approach on how technical sustainability support for new and existing concerns can be provided through flexible language-level programming. We propose to incorporate sustainability-related behaviour into programs through micro-languages enabling such behaviour to be updated and/or redefined as and when required.
[sustainability-related requirements, continuous code, Programming, sustainabiity, Batteries, adaptability, programming languages, technical sustainability support, engineering sustainability, change management, micro-languages, sustainable development, Computer languages, Semantics, Syntactics, microlanguages, Software, software engineering, programming language, flexible language-level programming, Software engineering, sustainability-related behaviour]
Enabling the Definition and Enforcement of Governance Rules in Open Source Systems
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Governance rules in software development projects help to prioritize and manage their development tasks, and contribute to the long-term sustainability of the project by clarifying how core and external contributors should collaborate in order to advance the project during its whole lifespan. Despite their importance, specially in Open Source Software (OSS) projects, these rules are usually implicit or scattered in the project documentation/tools (e.g., Tracking-systems or forums), hampering the correct understanding of the development process. We propose to enable the explicit definition and enforcement of governance rules for OSS projects. We believe this brings several important benefits, including improvements in the transparency of the process, its traceability and the semi-automation of the governance itself. Our approach has been implemented on top of My Lyn, a project-management Eclipse plug-in supporting most popular tracking-systems.
[public domain software, governance rules, OSS projects, open source software projects, Documentation, Eclipse plug-in, sustainability, My Lyn, software development projects, project-management, Computer bugs, Organizations, Syntactics, Software, software engineering, DSL, long-term sustainability, Software engineering, governance, open source systems]
AppCivist - A Service-Oriented Software Platform for Socially Sustainable Activism
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The increased adoption of mobile devices and social networking is drastically changing the way people monitor and share knowledge about their environment. Here, information and communication technologies (ICT) offer significant new ways to support social activism in cities by providing residents with new digital tools to articulate projects and mobilize activities. However, the development of ICT for activism is still in its infancy, with activists using basic tools stitched together in an ad hoc manner for their needs. Still, Internet-based technologies and related software architectures feature various enablers for civic action beyond base social networking. To that end, this paper discusses the vision and initial details of AppCivist, a platform that builds on cross-domain research among social scientists and computer scientists to revisit service-oriented architecture and relevant services to further social activism. We discuss the ICT challenges inherent in this project and present our recent work to address them.
[Context, social sustainability, digital tools, software architectures, Ontologies, social networking, service-oriented software platform, service-oriented computing, information and communication technology, Internet-based technology, Organizations, Computer architecture, social activism, mobile devices, social networking (online), AppCivist platform, Software, Internet, ICT, socially sustainable activism, Assembly, service-oriented architecture, Software engineering]
SOA4DM: Applying an SOA Paradigm to Coordination in Humanitarian Disaster Response
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Despite efforts to achieve a sustainable state of control over the management of global crises, disasters are occurring with greater frequency, intensity, and affecting many more people than ever before while the resources to deal with them do not grow apace. As we enter 2015, with continued concerns that mega-crises may become the new normal, we need to develop novel methods to improve the efficiency and effectiveness of our management of disasters. Software engineering as a discipline has long had an impact on society beyond its role in the development of software systems. In fact, software engineers have been described as the developers of prototypes for future knowledge workers; tools such as Github and Stack Overflow have demonstrated applications beyond the domain of software engineering. In this paper, we take the potential influence of software engineering one-step further and propose using the software service engineering paradigm as a new approach to managing disasters. Specifically, we show how the underlying principles of service-oriented architectures (SOA) can be applied to the coordination of disaster response operations. We describe key challenges in coordinating disaster response and discuss how an SOA approach can address those challenges.
[efficiency improvement, disaster management, Conferences, Communities, software service engineering paradigm, stack overflow, SOA paradigm, effectiveness improvement, Semiconductor optical amplifiers, software system development, github overflow, Disaster Response, service-oriented architectures, humanitarian disaster response, service-oriented architecture, Economics, SOA4DM, SOA, knowledge workers, emergency management, global crises management, Societies, Indexes, disaster response operation coordination, sustainable state, Software engineering]
Managing Emergent Ethical Concerns for Software Engineering in Society
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This paper presents an initial framework for managing emergent ethical concerns during software engineering in society projects. We argue that such emergent considerations can neither be framed as absolute rules about how to act in relation to fixed and measurable conditions. Nor can they be addressed by simply framing them as non-functional requirements to be satisficed. Instead, a continuous process is needed that accepts the 'messiness' of social life and social research, seeks to understand complexity (rather than seek clarity), demands collective (not just individual) responsibility and focuses on dialogue over solutions. The framework has been derived based on retrospective analysis of ethical considerations in four software engineering in society projects in three different domains.
[software in society, Media, software management, citizen science, Societies, emergent ethical concern management, ethics, Ethics, cyber crime, Law enforcement, society projects, Software, software engineering, Stakeholders, ethical aspects, Software engineering]
Dementia and Social Sustainability: Challenges for Software Engineering
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Dementia is a serious threat to social sustainability. As life expectancy increases, more people are developing dementia. At the same time, demographic change is reducing the economically active part of the population. Care of people with dementia imposes great emotional and financial strain on sufferers, their families and society at large. In response, significant research resources are being focused on dementia. One research thread is focused on using computer technology to monitor people in at-risk groups to improve rates of early diagnosis. In this paper we provide an overview of dementia monitoring research and identify a set of scientific challenges for the engineering of dementia-monitoring software, with implications for other mental health self-management systems.
[Computers, mental health self-management systems, Social sustainability, social sustainability, demographic change, computer technology, dementia monitoring research, Games, Software, social sciences computing, software engineering, medical computing, Monitoring, Dementia, Software engineering]
Cognitively Sustainable ICT with Ubiquitous Mobile Services - Challenges and Opportunities
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Information and Communication Technology (ICT) has led to an unprecedented development in almost all areas of human life. It forms the basis for what is called "the cognitive revolution" -- a fundamental change in the way we communicate, feel, think and learn based on an extension of individual information processing capacities by communication with other people through technology. This so-called "extended cognition" shapes human relations in a radically new way. It is accompanied by a decrease of shared attention and affective presence within closely related groups. This weakens the deepest and most important bonds, that used to shape human identity. Sustainability, both environmental and social (economic, technological, political and cultural) is one of the most important issues of our time. In connection with "extended cognition" we have identified a new, basic type of social sustainability that everyone takes for granted, and which we claim is in danger due to our changed ways of communication. We base our conclusion on a detailed analysis of the current state of the practice and observed trends. The contribution of our article consists of identifying cognitive sustainability and explaining its central role for all other aspects of sustainability, showing how it relates to the cognitive revolution, its opportunities and challenges. Complex social structures with different degrees of proximity have always functioned as mechanisms behind belongingness and identity. To create a long-term cognitive sustainability, we need to rethink and design new communication technologies that support differentiated and complex social relationships.
[Computers, politics, Social sustainability, social sustainability, information processing capacities, complex social structures, Mobile communication, Cognition, Mobile handsets, cognitively sustainable ICT, Privacy, mobile computing, environmental sustainability, Shared attention, information and communication technology, Cognitive revolution, cultural sustainability, social sciences computing, human identity, socio-economic effects, economic sustainability, Sustainable ICT, Software engineering for social good., Cognitive sustainability, cognition, affective presence, cultural aspects, ubiquitous mobile services, shared attention, Social cognition, sustainable development, closely-related groups, cognitive sustainability, differentiated relationships, Games, extended cognition, environmental economics, technological sustainability, proximity degrees, cognitive revolution, political sustainability, Software engineering]
New Initiative: The Naturalness of Software
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This paper describes a new research consortium, studying the Naturalness of Software. This initiative is supported by a pair of grants by the US National Science Foundation, totaling $2,600,000: the first, exploratory ("EAGER") grant of $600,000 helped kickstart an inter-disciplinary effort, and demonstrate feasibility; a follow-on full grant of $2,000,000 was recently awarded. The initiative is led by the author, who is at UC Davis, and includes investigators from Iowa State University and Carnegie-Mellon University (Language Technologies Institute).
[Computers, Java, US National Science Foundation, computational linguistics, Language Technologies Institute, Programming, Predictive models, language modeling, Iowa State University, Pragmatics, EAGER, NLP, Carnegie-Mellon University, software naturalness, Software, software engineering, big code, UC Davis, Software engineering]
Virtual Reality in Software Engineering: Affordances, Applications, and Challenges
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software engineers primarily interact with source code using a keyboard and mouse, and typically view software on a small number of 2D monitors. This interaction paradigm does not take advantage of many affordances of natural human movement and perception. Virtual reality (VR) can use these affordances more fully than existing developer environments to enable new creative opportunities and potentially result in higher productivity, lower learning curves, and increased user satisfaction. This paper describes the affordances offered by VR, demonstrates the benefits of VR and software engineering in prototypes for live coding and code review, and discusses future work, open questions, and the challenges of VR.
[source code (software), live coding, virtual reality, Navigation, source code, Cognition, Encoding, software reviews, Three-dimensional displays, Keyboards, Software, software engineering, code review, VR, Software engineering]
CodeAware: Sensor-Based Fine-Grained Monitoring and Management of Software Artifacts
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Current continuous integration (CI) tools, although extensible, can be limiting in terms of flexibility. In particular, artifact analysis capabilities available through plug in mechanisms are both coarse-grained and centralized. To address this limitation, this paper introduces a new paradigm, Code Aware, for distributed and fine-grained artifact analysis. Code Aware is an ecosystem inspired by sensor networks, consisting of monitors and actuators, aimed at improving code quality and team productivity. Code ware's vision entails (a) the ability to probe software artifacts of any granularity and localization, from variables to classes or files to entire systems, (b) the ability to perform both static and dynamic analyses on these artifacts, and (c) the ability to describe targeted remediation actions, for example to notify interested developers, through automated actuators. We provide motivational examples for the use of Code Aware that leverage current CI solutions, sketch the architecture of its underlying ecosystem, and outline research challenges.
[automated actuators, continuous integration tools, sensor-based fine-grained monitoring, ecosystem, Ecosystems, team productivity improvement, Electronic mail, software quality, artifact analysis capabilities, CodeAware, code quality improvement, productivity, distributed artifact analysis, DSL, Probes, Monitoring, program diagnostics, CI tools, static analysis, dynamic analysis, software artifact management, fine-grained artifact analysis, Software, plug in mechanisms, Software engineering, monitors]
Free Hugs -- Praising Developers for Their Actions
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Developing software is a complex, intrinsically intellectual, and therefore ephemeral activity, also due to the intangible nature of the end product, the source code. There is a thin red line between a productive development session, where a developer actually does something useful and productive, and a session where the developer essentially produces "fried air\
[Productivity, source code (software), object-oriented programming, Navigation, software development, object-oriented IDE, source code, Programming, fine-grained interaction information, ide, gamification, doctor, microgamification layer, coding behavior, window plague, Games, integrated development environment, interaction data, User interfaces, gamification mechanisms, Software, software engineering, role-playing game mechanics, Software engineering]
How (Much) Do Developers Test?
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
What do we know about software testing in the real world? It seems we know from Fred Brooks' seminal work "The Mythical Man-Month" that 50% of project effort is spent on testing. However, due to the enormous advances in software engineering in the past 40 years, the question stands: Is this observation still true? In fact, was it ever true? The vision for our research is to settle the discussion about Brooks' estimation once and for all: How much do developers test? Does developers' estimation on how much they test match reality? How frequently do they execute their tests, and is there a relationship between test runtime and execution frequency? What are the typical reactions to failing tests? Do developers solve actual defects in the production code, or do they merely relax their test assertions? Emerging results from 40 software engineering students show that students overestimate their testing time threefold, and 50% of them test as little as 4% of their time, or less. Having proven the scalability of our infrastructure, we are now extending our case study with professional software engineers from open-source and industrial organizations.
[professional software engineers, Java, program testing, software testing, open-source organizations, Estimation, execution frequency, Open source software, production code, industrial organizations, Production, test runtime, Testing, Software engineering]
A Vision of Crowd Development
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Crowdsourcing has had extraordinary success in solving a diverse set of problems, ranging from digitization of libraries and translation of the Internet, to scientific challenges such as classifying elements in the galaxy or determining the 3D shape of an enzyme. By leveraging the power of the masses, it is feasible to complete tasks in mere days and sometimes even hours, and to take on tasks that were previously impossible because of their sheer scale. Underlying the success of crowdsourcing is a common theme - the microtask. By breaking down the overall task at hand into microtasks providing short, self-contained pieces of work, work can be performed independently, quickly, and in parallel - enabling numerous and often untrained participants to chip in. This paper puts forth a research agenda, examining the question of whether the same kinds of successes that microtask crowdsourcing is having in revolutionizing other domains can be brought to software development. That is, we ask whether it is possible to push well beyond the open source paradigm, which still relies on traditional, coarse-grained tasks, to a model in which programming proceeds through microtasks performed by vast numbers of crowd developers.
[Crowdsourcing, Context, open source software development, software development, coarse-grained tasks, crowdsourcing, public domain software, open source paradigm, Programming, digital libraries, microtask crowdsourcing, Open source software, collaborative software development, 3D shape, Games, Libraries, software engineering, Internet, crowd development vision]
When App Stores Listen to the Crowd to Fight Bugs in the Wild
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
App stores are digital distribution platforms that put available apps that run on mobile devices. Current stores are software repositories that deliver apps upon user requests. However, when an app has a bug, the store continues delivering defective apps until the developer uploads a fixed version, thus impacting on the reputation of both store and app developer. In this paper, we envision a new generation of app stores that: (a) reduce human intervention to maintain mobile apps; and (b) enhance store services with smart and autonomous functionalities to automatically increase the quality of the delivered apps. We sketch a prototype of our envisioned app store and we discuss the functionalities that current stores an enhance by incorporating automatic software repair techniques.
[Google, App stores, software repository, Humanoid robots, Maintenance engineering, automatic software repair techniques, software quality, digital distribution platforms, mobile computing, Computer bugs, mobile devices, Androids, Monitoring]
Incorporating Human Intention into Self-Adaptive Systems
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Self-adaptive systems are fed with contextual information from the environments in which the systems operate,from within themselves, and from the users. Traditional self-adaptive systems research has focused on inputs of systems performance, resources, exception, and error recovery that drive systems' reaction to their environments. The intelligent ability ofthese self-adaptive systems is impoverished without knowledge ofa user's covert attention (thoughts, emotions, feelings). As a result, it is difficult to build effective systems that anticipate and react to users' needs as projected by covert behavior. This paperpresents the preliminary research results on capturing users'intention through neural input, and in reaction, commanding actions from software systems (e.g., load an application) based on human intention. Further, systems can self-adapt and refine their behaviors driven by such human covert behavior. The long-term research goal is to incorporate and synergize human neural input.Thus establishing software systems with a self-adaptive capability to "feel" and "anticipate" users intentions and put the human in the loop.
[Computers, Adaptive systems, human covert behavior, software systems, human computer interface, human intention, human neural input, Electroencephalography, human computer interface (HCI), neural input, contextual information, overt and covert behavior, Brain computer interface (BCI), human in the loop, Digital signal processing, Software systems, Mice, human computer interaction, software engineering, self-adaptive systems, Software engineering]
An Initiative to Improve Reproducibility and Empirical Evaluation of Software Testing Techniques
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The current concern regarding quality of evaluation performed in existing studies reveals the need for methods and tools to assist in the definition and execution of empirical studies and experiments. However, when trying to apply general methods from empirical software engineering in specific fields, such as evaluation of software testing techniques, new obstacles and threats to validity appears, hindering researchers' use of empirical methods. This paper discusses those issues specific for evaluation of software testing techniques and proposes an initiative for a collaborative effort to encourage reproducibility of experiments evaluating software testing techniques (STT). We also propose the development of a tool that enables automatic execution and analysis of experiments producing a reproducible research compendia as output that is, in turn, shared among researchers. There are many expected benefits from this Endeavour, such as providing a foundation for evaluation of existing and upcoming STT, and allowing researchers to devise and publish better experiments.
[Software testing, program testing, Conferences, software testing technique, reproducibility, Software Testing, Reproducibility in Software Engineering, Guidelines, STT, Empirical Software Engineering, Collaboration, empirical software engineering, Software, software engineering, Software engineering]
Inferring Behavioral Specifications from Large-scale Repositories by Leveraging Collective Intelligence
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Despite their proven benefits, useful, comprehensible, and efficiently checkable specifications are not widely available. This is primarily because writing useful, non-trivial specifications from scratch is too hard, time consuming, and requires expertise that is not broadly available. Furthermore, the lack of specifications for widely-used libraries and frameworks, caused by the high cost of writing specifications, tends to have a snowball effect. Core libraries lack specifications, which makes specifying applications that use them expensive. To contain the skyrocketing development and maintenance costs of high assurance systems, this self-perpetuating cycle must be broken. The labor cost of specifying programs can be significantly decreased via advances in specification inference and synthesis, and this has been attempted several times, but with limited success. We believe that practical specification inference and synthesis is an idea whose time has come. Fundamental breakthroughs in this area can be achieved by leveraging the collective intelligence available in software artifacts from millions of open source projects. Fine-grained access to such data sets has been unprecedented, but is now easily available. We identify research directions and report our preliminary results on advances in specification inference that can be had by using such data sets to infer specifications.
[open source projects, public domain software, specification writing, Maintenance engineering, specification synthesis, Data mining, History, formal specification, behavioral specification, collective intelligence, program specification, core libraries, software artifacts, Writing, Software, Libraries, specification inference, Software engineering]
Fast Feedback Cycles in Empirical Software Engineering Research
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Background/Context: Gathering empirical knowledge is a time consuming task and the results from empirical studies often are soon outdated by new technological solutions. As a result, the impact of empirical results on software engineering practice is often not guaranteed.Objective/Aim: In this paper, we summarise the ongoing discussion on "Empirical Software Engineering 2.0" as a way to improve the impact of empirical results on industrial practices. We propose a way to combine data mining and analysis with domain knowledge to enable fast feedback cycles in empirical software engineering research.Method: We identify the key concepts on gathering fast feedback in empirical software engineering by following an experience-based line of reasoning by argument. Based on the identified key concepts, we design and execute a small proof of concept with a company to demonstrate potential benefits of the approach.Results: In our example, we observed that a simple double feedback mechanism notably increased the precision of the data analysis and improved the quality of the knowledge gathered.Conclusion: Our results serve as a basis to foster discussion and collaboration within the research community for a development of the idea.
[EMSE 2.0, domain knowledge, Data analysis, data analysis, Empirical methods, data mining, double feedback mechanism, Data mining, inference mechanisms, Physics, Knowledge transfer, empirical software engineering 2.0, Collaboration, feedback cycle, Software, software engineering, Stakeholders, Software engineering, Research methods]
Dynamic Safety Cases for Through-Life Safety Assurance
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We describe dynamic safety cases, a novel operationalization of the concept of through-life safety assurance, whose goal is to enable proactive safety management. Using an example from the aviation systems domain, we motivate our approach, its underlying principles, and a lifecycle. We then identify the key elements required to move towards a formalization of the associated framework.
[safety lifecycle, safety-critical software, Cognition, dynamic safety cases, through-life safety assurance concept, Temperature sensors, Runtime, Safety management, aviation systems domain, safety principles, through-life safety assurance, Dynamic safety case, Biomedical monitoring, Monitoring, Lifecycle processes, proactive safety management, Safety assurance]
Correctness and Relative Correctness
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
In the process of trying to define what is a software fault, we have found that to formally define software faults we need to introduce the concept of relative correctness, i.e. the property of a program to be more-correct than another with respect to a given specification. A feature of a program is a fault (for a given specification)only because there exists an alternative to it that would make the program more-correct with respect to the specification.In this paper, we explore applications of the concept of relative correctness in program testing, program repair, and program design.Specifically, we argue that in many situations of software testing, fault removal and program repair, testing for relative correctness rather than absolute correctness leads to clearer conclusions and better outcomes. Also, we find that designing programs by stepwise correctness-enhancing transformations rather than by stepwise correctness-preserving refinements leads to simpler programs and is more tolerant of designer mistakes.
[Software testing, program testing, relative correctness, software fault, program design, Maintenance engineering, program repair, Generators, software maintenance, formal specification, software fault tolerance, Semantics, Software, Software engineering]
On Architectural Diversity of Dynamic Adaptive Systems
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We introduce a novel concept of ``architecture diversity'' for adaptive systems and posit that increased diversity has an inverse correlation with adaptation costs. We propose an index to quantify diversity and a static method to estimate the adaptation cost, and conduct an initial experiment on an exemplar cloud-based system which reveals the posited correlation.
[Context, adaptation cost estimation, Adaptive systems, constraint solving, exemplar cloud-based system, dynamic adaptive systems, software diversity, Routing, Indexes, software architecture, Diversity reception, Computer architecture, architectural diversity, Software, self-adaptive systems, cloud computing]
Information Transformation: An Underpinning Theory for Software Engineering
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software engineering lacks underpinning scientific theories both for the software it produces and the processes by which it does so. We propose that an approach based on information theory can provide such a theory, or rather many theories. We envision that such a benefit will be realised primarily through research based on the quantification of information involved and a mathematical study of the limiting laws that arise. However, we also argue that less formal but more qualitative uses for information theory will be useful. The main argument in support of our vision is based on the fact that both a program and an engineering process to develop such a program are fundamentally processes that transform information. To illustrate our argument we focus on software testing and develop an initial theory in which a test suite is input/output adequate if it achieves the channel capacity of the program as measured by the mutual information between its inputs and its outputs. We outline a number of problems, metrics and concrete strategies for improving software engineering, based on information theoretical analyses. We find it likely that similar analyses and subsequent future research to detail them would be generally fruitful for software engineering.
[Software testing, Measurement, Channel capacity, program testing, information transformation theory, software testing, information quantification, Entropy, Software, software engineering, information theory, Software engineering]
A Unified Framework for the Comprehension of Software's Time
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The dimension of time in software appears in both program execution and software evolution. Much research has been devoted to the understanding of either program execution or software evolution, but these two research communities have developed tools and solutions exclusively in their respective context. In this paper, we claim that a common comprehension framework should apply to the time dimension of software. We formalize this as a meta-model that we instantiate and apply to the two different comprehension problems.
[Measurement, Context, Visualization, comprehension framework, software time dimension, meta-model, program execution, History, software maintenance, software evolution, Heating, Collaboration, Software]
Smart Programming Playgrounds
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Modern IDEs contain sophisticated components for inferring missing types, correcting bad syntax and completing partial expressions in code, but they are limited to the context that is explicitly defined in a project's configuration. These tools are ill-suited for quick prototyping of incomplete code snippets, such as those found on the Web in Q&amp;A forums or walk-through tutorials, since such code snippets often assume the availability of external dependencies and may even contain implicit references to an execution environment that provides data or compute services. We propose an architecture for smart programming playgrounds that can facilitate rapid prototyping of incomplete code snippets through a semi-automatic context resolution that involves identifying static dependencies, provisioning external resources on the cloud and injecting resource bindings to handles in the original code fragment. Such a system could be potentially useful in a range of different scenarios, from sharing code snippets on the Web to experimenting with new ideas during traditional software development.
[Context, software development, software prototyping, incomplete code snippet prototyping, code snippets, Programming, integrated development environments, IDEs, smart programming playgrounds, Engines, software architecture, code fragment, semiautomatic context resolution, Databases, IEEE catalogs, dependency injection, Libraries, Internet, playgrounds, cloud computing, programming environments, Graphical user interfaces]
Capsule-Oriented Programming
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
&#x201C;Explicit concurrency should be abolished from all higher-level programming languages (i.e. everything except - perhaps- plain machine code.).&#x201D; Dijkstra [1] (paraphrased). A promising class of concurrency abstractions replaces explicit concurrency mechanisms with a single linguistic mechanism that combines state and control and uses asynchronous messages for communications, e.g. active objects or actors, but that doesn't remove the hurdle of understanding non-local control transfer. What if the programming model enabled programmers to simply do what they do best, that is, to describe a system in terms of its modular structure and write sequential code to implement the operations of those modules and handles details of concurrency? In a recently sponsored NSF project we are developing such a model that we call capsule-oriented programming and its realization in the Panini project. This model favors modularity over explicit concurrency, encourages concurrency correctness by construction, and exploits modular structure of programs to expose implicit concurrency.
[modular structure, high level languages, implicit concurrency, Programming, Modularity, single linguistic mechanism, Concurrent computing, Program processors, concurrency abstractions, programming model, sequential code, software engineering, capsules, concurrency (computers), Message systems, Java, higher-level programming languages, concurrency abstraction, Synchronization, Panini project, Global Positioning System, NSF project, nonlocal control transfer, capsule-oriented programming, explicit concurrency mechanisms]
Evolution-Aware Monitoring-Oriented Programming
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Monitoring-Oriented Programming (MOP) helps develop more reliable software by means of monitoring against formal specifications. While MOP showed promising results, all prior research has focused on checking a single version of software. We propose to extend MOP to support multiple software versions and thus be more relevant in the context of rapid software evolution. Our approach, called eMOP, is inspired by regression test selection -- a well studied, evolution-centered technique. The key idea in eMOP is to monitor only the parts of code that changed between versions. We illustrate eMOP by means of a running example, and show the results of preliminary experiments. eMOP opens up a new line of research on MOP -- it can significantly improve usability and performance when applied across multiple versions of software and is complementary to algorithmic MOP advances on a single version.
[Regression Testing, program testing, program verification, software prototyping, software reliability, eMOP, Programming, software versions, Runtime Monitoring, Monitoring-Oriented Programming, formal specification, Open source software, Runtime, software checking, Monitoring, Testing, code monitoring, Java, regression test selection, evolution-centered technique, rapid software evolution, software maintenance, Runtime Verification, configuration management, evolution-aware monitoring-oriented programming]
Towards Explicitly Elastic Programming Frameworks
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
It is a widely held view that software engineers should not be "burdened" with the responsibility of making their application components elastic, and that elasticity should be either be implicit and automatic in the programming framework; or that it is the responsibility of the cloud provider's operational staff (DevOps) to make distributed applications written for dedicated clusters elastic and execute them on cloud environments. In this paper, we argue the opposite - we present a case for explicit elasticity, where software engineers are given the flexibility to explicitly engineer elasticity into their distributed applications. We present several scenarios where elasticity retrofitted to applications by DevOps is ineffective, present preliminary empirical evidence that explicit elasticity improves efficiency, and argue for elastic programming languages and frameworks to reduce programmer effort in engineering elastic distributed applications. We also present a bird's eye view of ongoing work on two explicitly elastic programming frameworks - Elastic Thrift (based on Apache Thrift) and Elastic Java, an extension of Java with support for explicit elasticity.
[Measurement, cloud provider operational staff, Java, software engineers, elastic Java, explicit elastic programming frameworks, Elasticity, Programming, cloud environments, Servers, elastic thrift programming frameworks, Runtime, DevOps, elastic programming languages, Software, software engineering, cloud computing]
Optimising Energy Consumption of Design Patterns
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software design patterns are widely used in software engineering to enhance productivity and maintainability.However, recent empirical studies revealed the high energy overhead in these patterns. Our vision is to automatically detect and transform design patterns during compilation for better energy efficiency without impacting existing coding practices. In this paper, we propose compiler transformations for two design patterns, Observer and Decorator, and perform an initial evaluation of their energy efficiency.
[Computers, Energy consumption, Transforms, Observers, coding practices, software maintenance, software design patterns, compiler transformations, program compilers, Optimization, power aware computing, energy consumption optimisation, compilation, energy efficiency, Software, software engineering, maintainability, Software engineering]
Mining Software Repositories for Social Norms
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Social norms facilitate coordination and cooperation among individuals, thus enable smoother functioning of social groups such as the highly distributed and diverse open source software development (OSSD) communities. In these communities, norms are mostly implicit and hidden in huge records of human-interaction information such as emails, discussions threads, bug reports, commit messages and even source code. This paper aims to introduce a new line of research on extracting social norms from the rich data available in software repositories. Initial results include a study of coding convention violations in JEdit, Argo UML and Glassfish projects. It also presents a new life-cycle model for norms in OSSD communities and demonstrates how a number of norms extracted from the Python development community follow this life-cycle model.
[source code (software), Java, social norm extraction, public domain software, data mining, open source software development community, source code, JEdit, Glassfish projects, software repository mining, OSSD community, Encoding, life-cycle model, Electronic mail, Proposals, Data mining, Argo UML, Python development community, human-interaction information, Software, software engineering, Monitoring]
Commit Bubbles
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Developers who use version control are expected to produce systematic commit histories that show well-defined steps with logical forward progress. Existing version control tools assume that developers also write code systematically. Unfortunately, the process by which developers write source code is often evolutionary, or as-needed, rather than systematic. Our contribution is a fragment-oriented concept called Commit Bubbles that will allow developers to construct systematic commit histories that adhere to version control best practices with less cognitive effort, and in a way that integrates with their as-needed coding workflows.
[Context, source code (software), commit bubbles, fragment-oriented concept, version control, Switches, source code, integrated development environments, Encoding, systematic commit histories, History, version control tools, software maintenance, Best practices, configuration management, Systematics, as-needed coding workflows, Software, software tools]
Rapid Multi-Purpose, Multi-Commit Code Analysis
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Existing code- and software evolution studies typically operate on the scale of a few revisions of a small number of projects, mostly because existing tools are unsuited for performing large-scale studies. We present a novel approach, which can be used to analyze an arbitrary number of revisions of a software project simultaneously and which can be adapted for the analysis of mixed-language projects. It lays the foundation for building high-performance code analyzers for a variety of scenarios. We show that for one particular scenario, namely code metric computation, our prototype outperforms existing tools by multiple orders of magnitude when analyzing thousands of revisions.
[Measurement, Conferences, program diagnostics, high-performance code analyzers, abstract syntax tree, code metric computation, rapid multipurpose multicommit code analysis, Data mining, graph, software evolution, mixed-language projects, code analysis, Computer languages, Prototypes, Software, code evolution, software project revisions, Software engineering, software metrics]
Leveraging Informal Documentation to Summarize Classes and Methods in Context
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Critical information related to a software developer'scurrent task is trapped in technical developer discussions,bug reports, code reviews, and other software artefacts. Muchof this information pertains to the proper use of code elements(e.g., methods and classes) that capture vital problem domainknowledge. To understand the purpose of these code elements, software developers must either access documentation and online posts and understand the source code or peruse a substantial amount of text. In this paper, we use the context that surrounds code elements in StackOverflow posts to summarize the use and purpose of code elements. To provide focus to our investigation, we consider the generation of summaries for library identifiers discussed in StackOverflow. Our automatic summarization approach was evaluated on a sample of 100 randomly-selected library identifiers with respect to a benchmark of summaries provided by two annotators. The results show that the approach attains an R-precision of 54%, which is appropriate given the diverse ways in which code elements can be used.
[Context, source code (software), Java, software artefacts, StackOverflow, Humanoid robots, Documentation, source code, automatic summarization approach, informal documentation, library identifiers, Libraries, Software, software engineering, code elements, Androids]
Bixie: Finding and Understanding Inconsistent Code
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We present Bixie, a tool to detect inconsistencies in Java code. Bixie detectsinconsistent code at a higher precision than previous tools and provides novelfault localization techniques to explain why code is inconsistent. Wedemonstrate the usefulness of Bixie on over one million lines of code, showthat it can detect inconsistencies at a low false alarm rate, and fix a numberof inconsistencies in popular open-source projects. Watch our Demo at http://youtu.be/QpsoUBJMxhk.
[Java, program diagnostics, Unsolicited electronic mail, fault localization technique, inconsistent code understanding, open-source projects, program compilers, Open source software, software fault tolerance, Runtime, Computer bugs, Benchmark testing, Java code, Arrays, Bixie, inconsistent code finding]
TaskNav: Task-Based Navigation of Software Documentation
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
To help developers navigate documentation, we introduce Task Nav, a tool that automatically discovers and indexes task descriptions in software documentation. With Task Nav, we conceptualize tasks as specific programming actions that have been described in the documentation. Task Nav presents these extracted task descriptions along with concepts, code elements, and section headers in an auto-complete search interface. Our preliminary evaluation indicates that search results identified through extracted task descriptions are more helpful to developers than those found through other means, and that they help bridge the gap between documentation structure and the information needs of software developers. Video: https://www.youtube.com/watch?v=opnGYmMGnqY.
[document handling, Auto-Complete, Navigation, Instruction sets, Development Tasks, task-based navigation, Documentation, Interference, information retrieval, Software Documentation, Natural Language Processing, TaskNav, software developers, Electronic mail, documentation structure, task descriptions, extracted task descriptions, programming actions, software engineering, code elements, software documentation, section headers, auto-complete search interface, Software engineering]
ViDI: The Visual Design Inspector
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We present ViDI (Visual Design Inspector), a novel code review tool which focuses on quality concerns and design inspection as its cornerstones. It leverages visualization techniques to represent the reviewed software and augments the visualization with the results of quality analysis tools. To effectively understand the contribution of a reviewer in terms of the impact of her changes on the overall system quality, ViDI supports the recording and further inspection of reviewing sessions. ViDI is an advanced prototype which we will soon release to the Pharo open-source community.
[Visualization, code review tool, Inspection, Birds, software quality, software reviews, ViDI, visualization technique, Lugano, data visualisation, visual design inspector, Software systems, Quality assessment, software tools, Software engineering, system quality]
Bootstrapping Mobile App Development
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Modern IDEs provide limited support for developers when starting a new data-driven mobile app. App developers are currently required to write copious amounts of boilerplate code, scripts, organise complex directories, and author actual functionality. Although this scenario is ripe for automation, current tools are yet to address it adequately. In this paper we present RAPPT, a tool that generates the scaffolding of a mobile app based on a high level description specified in a Domain Specific Language (DSL). We demonstrate the feasibility of our approach by an example case study and feedback from a professional development team. Demo at: https://www.youtube.com/watch?v=ffquVgBYpLM.
[Productivity, data-driven mobile app, application program interfaces, Humanoid robots, Mobile communication, IDE, mobile application development, domain specific language, mobile computing, RAPPT tool, Model Driven Development, Mobile App Prototyping, specification languages, integrated development environment, Code Generation, Motion pictures, Androids, DSL, Software engineering]
Source Code Curation on StackOverflow: The Vesperin System
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The past few years have witnessed the rise of software question and answer sites like StackOverflow, where developers can pose detailed coding questions and receive quality answers. Developers using these sites engage in a complex code foraging process of understanding and adapting the code snippets they encounter. We introduce the notion of source code curation to cover the act of discovering some source code of interest, cleaning and transforming (refining) it, and then presenting it in a meaningful and organized way. In this paper, we present Vesperin, a source code curation system geared towards curating Java code examples on StackOverflow.
[Context, source code (software), Java, Vesperin system, StackOverflow, Aerospace electronics, Programming, software quality, source code curation system, Software, Java code, complex code foraging process, Software engineering]
The ECCO Tool: Extraction and Composition for Clone-and-Own
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software reuse has become mandatory for companies to compete and a wide range of reuse techniques are available today. However, ad hoc practices such as copying existing systems and customizing them to meet customer-specific needs are still pervasive, and are generically called clone-and-own. We have developed a conceptual framework to support this practice named ECCO that stands for Extraction and Composition for Clone-and-Own. In this paper we present our Eclipse-based tool to support this approach. Our tool can automatically locate reusable parts from previously developed products and subsequently compose a new product from a selection of desired features. The tools demonstration video can be found here: http://youtu.be/N6gPekuxU6o.
[software product lines, reuse, software reuse, customer-specific needs, Conferences, ECCO tool, product variants, feature interactions, Data mining, features, Systematics, clone-and-own, Eclipse-based tool, extraction-and-composition-for-clone-and-own, Feature extraction, Software, Distance measurement, feature selection, Software engineering]
Extract Package Refactoring in ARIES
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software evolution often leads to the degradation of software design quality. In Object-Oriented (OO) systems, this often results in packages that are hard to understand and maintain, as they group together heterogeneous classes with unrelated responsibilities. In such cases, state-of-the-art re-modularization tools solve the problem by proposing a new organization of the existing classes into packages. However, as indicated by recent empirical studies, such approaches require changing thousands of lines of code to implement the new recommended modularization. In this demo, we present the implementation of an Extract Package refactoring approach in ARIES (Automated Refactoring In EclipSe), a tool supporting refactoring operations in Eclipse. Unlike state-of-the-art approaches, ARIES automatically identifies and removes single low-cohesive packages from software systems, which represent localized design flaws in the package organization, with the aim to incrementally improve the overall quality of the software modularisation.
[Measurement, object-oriented programming, ARIES tool, software systems, software design quality degradation, extract package refactoring, object-oriented systems, software quality, localized design flaws, software maintenance, software evolution, software modularisation, Couplings, Iterative closest point algorithm, re-modularization tools, Organizations, Software systems, automated refactoring in EclipSe, software tools, OO systems, single low-cohesive packages, Software engineering]
scvRipper: Video Scraping Tool for Modeling Developers' Behavior Using Interaction Data
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Screen-capture tool can record a user's interaction with software and application content as a stream of screenshots which is usually stored in certain video format. Researchers have used screen-captured videos to study the programming activities that the developers carry out. In these studies, screen-captured videos had to be manually transcribed to extract software usage and application content data for the study purpose. This paper presents a computer-vision based video scraping tool (called scvRipper) that can automatically transcribe a screen-captured video into time-series interaction data according to the analyst's need. This tool can address the increasing need for automatic behavioral data collection methods in the studies of human aspects of software engineering.
[Visualization, Computational modeling, Instruments, screen-captured videos, user interaction, time-series interaction data, user interfaces, Data mining, computer-vision based video scraping tool, application content data, scvRipper, Video Scraping, computer vision, interaction data, Software, software engineering, automatic behavioral data collection methods, Interaction Data, video signal processing, developer behavior modelling, Graphical user interfaces, screen-capture tool]
Chiminey: Reliable Computing and Data Management Platform in the Cloud
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The enabling of scientific experiments that are embarrassingly parallel, long running and data-intensive into a cloud-based execution environment is a desirable, though complex undertaking for many researchers. The management of such virtual environments is cumbersome and not necessarily within the core skill set for scientists and engineers. We present here Chiminey, a software platform that enables researchers to (i) run applications on both traditional high-performance computing and cloud-based computing infrastructures, (ii) handle failure during execution, (iii) curate and visualise execution outputs, (iv) share such data with collaborators or the public, and (v) search for publicly available data.
[Cloud computing, software platform, data management, software reliability, data publication, reliability, virtualisation, parallel processing, data curation, Fault tolerance, Fault tolerant systems, Chiminey, data discovery, cloud computing, high-performance computing, fault tolerance, cloud-based computing infrastructure, Physics, Connectors, Data visualization, virtual environment, data management platform, HPC, data handling, visualisation]
Automated Program Repair in an Integrated Development Environment
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We present the integration of the AutoFix automated program repair technique into the EiffelStudio Development Environment. AutoFix presents itself like a recommendation system capable of automatically finding bugs and suggesting fixes in the form of source-code patches. Its performance suggests usage scenarios where it runs in the background or during work interruptions, displaying fix suggestions as they become available. This is a contribution towards the vision of semantic Integrated Development Environments, which offer powerful automated functionality within interfaces familiar to developers. A screencast highlighting the main features of AutoFix can be found at: http://youtu.be/Ff2ULiyL-80.
[Algorithm design and analysis, source code (software), Heuristic algorithms, program diagnostics, Maintenance engineering, software maintenance, recommendation system, AutoFix automated program repair technique, Computer bugs, Semantics, EiffelStudio development environment, source-code patches, semantic integrated development environments, Contracts, Testing]
FLEXISKETCH TEAM: Collaborative Sketching and Notation Creation on the Fly
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
When software engineers collaborate, they frequently use whiteboards or paper for sketching diagrams. This is fast and flexible, but the resulting diagrams cannot be interpreted by software modeling tools. We present FLEXISKETCH TEAM, a tool solution consisting of a significantly extended version of our previous, single-user FLEXISKETCH tool for Android devices and a new desktop tool. Our solution for collaborative, model-based sketching of free-form diagrams allows users to define and re-use diagramming notations on the fly. Several users can work simultaneously on the same model sketch with multiple tablets. The desktop tool provides a shared view of the drawing canvas which can be projected onto an electronic whiteboard. Preliminary results from an exploratory study show that our tool motivates meeting participants to actively take part in sketching as well as defining ad-hoc notations.
[collaborative model-based sketching, Android devices, single-user FLEXISKETCH tool, Object oriented modeling, Conferences, Unified modeling language, collaborative sketching, Metamodeling, software modeling tools, mobile computing, multiple tablets, free-form diagrams, Collaboration, electronic whiteboard, FLEXISKETCH team, desktop tool, diagramming notation creation on the fly, Libraries, Software, software engineering, software tools]
Interactive Synthesis Using Free-Form Queries
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We present a new code assistance tool for integrated development environments. Our system accepts free-form queries allowing a mixture of English and Java as an input, and produces Java code fragments that take the query into account and respect syntax, types, and scoping rules of Java as well as statistical usage patterns. The returned results need not have the structure of any previously seen code fragment. As part of our system we have constructed a probabilistic context free grammar for Java constructs and library invocations, as well as an algorithm that uses a customized natural language processing tool chain to extract information from free-form text queries. The evaluation results show that our technique can tolerate much of the flexibility present in natural language, and can also be used to repair incorrect Java expressions that contain useful information about the developer's intent. Our demo video is available at http://youtu.be/tx4-XgAZkKU.
[Context, Java, customized natural language processing tool chain, natural language processing, Program Repair, free-form text queries, Natural Language Processing, Programming, integrated development environments, Grammar, Code Completion, software libraries, Java code fragments, query processing, English, code assistance tool, statistical usage patterns, library invocations, Natural language processing, Chapters, statistical analysis, interactive synthesis, free-form queries, Program Synthesis]
Varis: IDE Support for Embedded Client Code in PHP Web Applications
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
In software development, IDE services are used to assist developers in programming tasks. In dynamic web applications, however, since the client-side code is embedded in the server-side program as string literals, providing IDE services for such embedded code is challenging. We introduce Varis, a tool that provides services on the embedded client-side code. We perform symbolic execution on a PHP program to approximate its output and parse it into a VarDOM that compactly represents all its DOM variations. Using the VarDOM, we implement various types of IDE services for embedded client code including syntax highlighting, code completion, and 'find declaration'.
[editor services, Varis tool, IDE support services, IDE services, software development, HTML, Servers, Approximation methods, web applications, PHP Web applications, server-side program, Presses, programming tasks, embedded code, embedded systems, VarDOM, Syntactics, symbolic execution, Internet, programming environments, Cascading style sheets, embedded client-side code]
MU-MMINT: An IDE for Model Uncertainty
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Developers have to work with ever-present design-time uncertainty, i.e., Uncertainty about selecting among alternative design decisions. However, existing tools do not support working in the presence of uncertainty, forcing developers to either make provisional, premature decisions, or to avoid using the tools altogether until uncertainty is resolved. In this paper, we present a tool, called MU-MMINT, that allows developers to express their uncertainty within software artifacts and perform a variety of model management tasks such as reasoning, transformation and refinement in an interactive environment. In turn, this allows developers to defer the resolution of uncertainty, thus avoiding having to undo provisional decisions. See the companion video: http://youtu.be/kAWUm-iFatM.
[Visualization, Adaptation models, reasoning task, Uncertainty, refinement task, Unified modeling language, Cognition, IDE, design-time uncertainty, model management tasks, design decisions, Analytical models, model uncertainty, software artifacts, integrated development environment, Software, software engineering, software tools, MU-MMINT tool, transformation task]
StriSynth: Synthesis for Live Programming
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Motivated by applications in automating repetitive file manipulations, we present a tool called StriSynth, which allows end-users to perform transformations over data using examples. Based on provided examples, our tool automatically generates scripts for non-trivial file manipulations. Although the current focus of StriSynth are file manipulations, it implements a more general string transformation framework. This framework builds on and further extends the functionality of Flash Fill -- a Microsoft Excel extension for string transformations. An accompanying video to this paper is available at the following website http://youtu.be/kkDZphqIdFM.
[Computers, Programming by Example, Radiation detectors, File Manipulation, Microsoft Excel extension, Programming, script generation, string transformation framework, Partitioning algorithms, Electronic mail, nontrivial file manipulations, string transformations, Scripting, StriSynth, authoring languages, Synthesis, live programming synthesis, automating repetitive file manipulations, Writing, Benchmark testing, Live Programming, Flash Fill, programming]
CACHECA: A Cache Language Model Based Code Suggestion Tool
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Nearly every Integrated Development Environment includes a form of code completion. The suggested completions ("suggestions") are typically based on information available at compile time, such as type signatures and variables in scope. A statistical approach, based on estimated models of code patterns in large code corpora, has been demonstrated to be effective at predicting tokens given a context. In this demo, we present CACHECA, an Eclipse plug in that combines the native suggestions with a statistical suggestion regime. We demonstrate that a combination of the two approaches more than doubles Eclipse's suggestion accuracy. A video demonstration is available at https://www.youtube.com/watch?v=3INk0N3JNtc.
[Context, source code (software), Java, Computational modeling, Predictive models, cache language model, code suggestion tool, programming languages, Engines, code pattern model estimation, Accuracy, code completion, integrated development environment, Eclipse plugin, software tools, CACHECA, Context modeling, statistical approach]
ChangeScribe: A Tool for Automatically Generating Commit Messages
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
During software maintenances tasks, commit messages are an important source of information, knowledge, and documentation that developers rely upon. However, the number and nature of daily activities and interruptions can influence the quality of resulting commit messages. This formal demonstration paper presents ChangeScribe, a tool for automatically generating commit messages. ChangeScribe is available at http://www.cs.wm.edu/semeru/changescribe (Eclipse plugin, instructions, demos and the source code).
[Context, Java, Visualization, ChangeScribe, summarization, system documentation, Documentation, documentation, software maintenance, software maintenance task, code changes, Commit message, automatic commit message generation, Semantics, XML, information source, knowledge source, Software, Eclipse plugin]
Ekstazi: Lightweight Test Selection
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Regression testing is a crucial, but potentially time-consuming, part of software development. Regression test selection (RTS), which runs only a subset of tests, was proposed over three decades ago as a promising way to speed up regression testing. However, RTS has not been widely adopted in practice. We propose EKSTAZI , a lightweight RTS tool, that can integrate well with testing frameworks and build systems, increasing the chance for adoption. EKSTAZI tracks dynamic dependencies of tests on files and requires no integration with version-control systems. We implemented EKSTAZI for Java+JUnit and Scala+ScalaTest, and evaluated it on 615 revisions of 32 open-source projects (totaling almost 5M LOC). The results show that EKSTAZI reduced the end-to-end testing time by 32% on average compared to executing all tests. EKSTAZI has been adopted for day-to-day use by several Apache developers. The demo video for EKSTAZI can be found at http://www.youtube.com/watch?v=jE8K5_UCP28.
[Java, Google, program testing, lightweight test selection, software development, Instruments, dynamic dependency tracking, regression testing, regression analysis, regression test selection, lightweight RTS tool, EKSTAZI, open-source projects, Open source software, Java+JUnit, Apache developers, software tools, Scala+ScalaTest, Monitoring, Testing]
TesMa and CATG: Automated Test Generation Tools for Models of Enterprise Applications
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We present CATG, an open-source concolic test generation tool for Java and its integration with TesMa, a model-based testing tool which automatically generates test cases from formal design documents. TesMa takes as input a set of design documents of an application under test. The design documents are provided in the form of database table definitions, process-flow diagrams, and screen definitions. From these design documents, TesMa creates Java programs for the feasible execution scenarios of the application. CATG performs concolic testing on these Java programs to generate suitable databases and test inputs required to test the application under test. A demo video of the tool is available at https://www.youtube.com/watch?v=9lEvPwR7g-Q.
[Java, program testing, enterprise application model, public domain software, automated test generation tool, process-flow diagram, data flow analysis, database table definition, Open source software, CATG, TesMa program, design document, open-source concolic test generation tool, Databases, screen definition, Java program, Concrete, Libraries, business data processing, Testing, Business]
StressCloud: A Tool for Analysing Performance and Energy Consumption of Cloud Applications
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Finding the best deployment configuration that maximises energy efficiency while guaranteeing system performance of cloud applications is an extremely challenging task. It requires the evaluation of system performance and energy consumption under a wide variety of realistic workloads and deployment configurations. This paper demonstrates StressCloud, an automatic performance and energy consumption analysis tool for cloud applications in real-world cloud environments. StressCloud supports 1) the modelling of realistic cloud application workloads, 2) the automatic generation and running of load tests, and 3) the profiling of system performance and energy consumption.
[Energy consumption, Cloud computing, Computational modeling, cloud performance analysis, energy consumption analysis tool, Servers, real-world cloud environments, StressCloud tool, load tests, power aware computing, System performance, energy efficiency, energy conservation, system performance evaluation, Data models, software tools, cloud computing, energy consumption, software performance evaluation, Load modeling]
Analysis of Android Inter-App Security Vulnerabilities Using COVERT
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The state-of-the-art in securing mobile software systems are substantially intended to detect and mitigate vulnerabilities in a single app, but fail to identify vulnerabilities that arise due to the interaction of multiple apps, such as collusion attacks and privilege escalation chaining, shown to be quite common in the apps on the market. This paper demonstrates COVERT, a novel approach and accompanying tool-suite that relies on a hybrid static analysis and lightweight formal analysis technique to enable compositional security assessment of complex software. Through static analysis of Android application packages, it extracts relevant security specifications in an analyzable formal specification language, and checks them as a whole for inter-app vulnerabilities. To our knowledge, COVERT is the first formally-precise analysis tool for automated compositional analysis of Android apps. Our study of hundreds of Android apps revealed dozens of inter-app vulnerabilities, many of which were previously unknown.
[lightweight formal analysis technique, program diagnostics, Humanoid robots, Metals, Mobile communication, Android inter-app security vulnerability analysis, mobile software systems, formal specification language, Security, privilege escalation chaining, hybrid static analysis, formal specification, complex software compositional security assessment, Analytical models, Android (operating system), mobile computing, security of data, specification languages, Android application package static analysis, COVERT approach, collusion attacks, formally-precise analysis tool, Androids, Smart phones]
Ariadne: Topology Aware Adaptive Security for Cyber-Physical Systems
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This paper presents Ariadne, a tool for engineering topology aware adaptive security for cyber-physical systems. It allows security software engineers to model security requirements together with the topology of the operational environment. This model is then used at runtime to perform speculative threat analysis to reason about the consequences that topological changes arising from the movement of agents and assets can have on the satisfaction of security requirements. Our tool also identifies an adaptation strategy that applies security controls when necessary to prevent potential security requirements violations.
[adaptation strategy, Adaptation models, Ports (Computers), Mobile handsets, Verification, Topology, security software engineers, Adaptive Systems, Security, Servers, engineering topology aware adaptive security, cyber-physical systems, Runtime, security of data, Ariadne tool, software tools, speculative threat analysis]
Security Toolbox for Detecting Novel and Sophisticated Android Malware
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This paper presents a demo of our Security Toolbox to detect novel malware in Android apps. This Toolbox is developed through our recent research project funded by the DARPA Automated Program Analysis for Cybersecurity (APAC) project. The adversarial challenge ("Red") teams in the DARPA APAC program are tasked with designing sophisticated malware to test the bounds of malware detection technology being developed by the research and development ("Blue") teams. Our research group, a Blue team in the DARPA APAC program, proposed a "human-in-the-loop program analysis" approach to detect malware given the source or Java bytecode for an Android app. Our malware detection apparatus consists of two components: a general-purpose program analysis platform called Atlas, and a Security Toolbox built on the Atlas platform. This paper describes the major design goals, the Toolbox components to achieve the goals, and the workflow for auditing Android apps. The accompanying video illustrates features of the Toolbox through a live audit.
[source code (software), invasive software, human-in-the-loop program analysis approach, Humanoid robots, malware detection apparatus, Android apps, general-purpose program analysis platform, mobile security, Semantics, research and development teams, program analysis, research and development, blue teams, Atlas platform, Malware, Java, APAC program, Java bytecode, malware, program diagnostics, smart phones, security toolbox, Android, live audit, DARPA automated program analysis for cybersecurity, XML, Software, Androids, source bytecode]
VERMEER: A Tool for Tracing and Explaining Faulty C Programs
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We present VERMEER, a new automated debugging tool for C. VERMEER combines two functionalities: (1) a dynamic tracer that produces a linearized trace from a faulty C program and a given test input; and (2) a static analyzer that explains why the trace fails. The tool works in phases that simplify the input program to a linear trace, which is then analyzed using an automated theorem prover to produce the explanation. The output of each phase is a valid C program. VERMEER is able to produce useful explanations of non trivial traces for real C programs within a few seconds. The tool demo can be found at http://youtu.be/E5lKHNJVerU.
[Algorithm design and analysis, program debugging, program diagnostics, Debugging, dynamic tracer, C language, automated theorem prover, Interpolation, VERMEER, Benchmark testing, linearized trace, Concrete, Libraries, Software, software tools, theorem proving, faulty C program tracing, automated debugging tool, static analyzer]
JRebel.Android: Runtime Class- and Resource Reloading for Android
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Developers writing Android applications suffer from a dreadful redeploy time every time they need to test changes to the source code. While runtime class reloading systems are widely used for the underlying programming language, Java, there is currently no support for reloading code on the Android platform. This paper presents a new tool, JRebel.Android that enables automatic runtime class- and resource reloading capabilities for Android. The target of this paper is the Android developer as well as the researcher for which dynamic updating capabilities on mobile devices can serve as a basic building block within areas such as runtime maintenance or self-adaptive systems. JRebel.Android is able to reload classes in much less than 1 second, saving more than 91% of the total redeploy time for small apps, more than 95% for medium size apps, and even more for larger apps.
[source code (software), Java, Google, dynamic updating capability, Instruments, Humanoid robots, source code, Dynamic Software Updating, Standards, runtime maintenance, Class Reloading, Android, JRebel.Android tool, Runtime, Android (operating system), mobile devices, programming language, self-adaptive systems, Resource Reloading, software tools, Androids, runtime class-resource reloading system]
FormTester: Effective Integration of Model-Based and Manually Specified Test Cases
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Whilst Model Based Testing (MBT) is an improvement over manual test specification, the leap from it to MBT can be hard. Only recently MBT tools for Web applications have emerged that can recover models from existing manually specified test cases. However, there are further requirements for supporting both MBT and manually specified tests. First, we need support for the generation of test initialization procedures. Also, we want to identify areas of the system that are not testable due to defects. We present Form Tester, a new MBT tool addressing these limitations. An evaluation with real Web applications shows that Form Tester helps to reduce the time spent on developing test cases.
[Adaptation models, Automation, program testing, Computational modeling, test initialization procedure generation, Manuals, Web applications, FormTester, formal specification, model-based test case, manually-specified test case, Writing, model-based testing, MBT tools, Testing, Software engineering]
Mining Temporal Properties of Data Invariants
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
System specifications are important in maintaining program correctness, detecting bugs, understanding systems and guiding test case generation. Often, these specifications are not explicitly written by developers. If we want to use them for analysis, we need to obtain them through other methods; for example, by mining them out of program behavior. Several tools exist to mine data invariants and temporal properties from program traces, but few examine the temporal relationships between data invariants. An example of this kind of relationship would be "the return value of the method isFull? is false until the field size reaches the value capacity". We propose a data-temporal property miner, Quarry, which mines Linear Temporal Logic (LTL) relations of arbitrary length and complexity between Daikon-style data invariants. We infer data invariants from systems using Daikon, recompose these data invariants into sequences, and mine temporal properties over these sequences. Our preliminary results suggest that this method may recover important system properties.
[program debugging, LTL relation mining, program verification, Conferences, data mining, isFull method, temporal logic, Data mining, Daikon, test case generation, Hardware, value capacity, program correctness, data-temporal property miner, Context, program behavior, linear temporal logic relation mining, Instruments, system specifications, data invariants, bug detection, Quarry, program traces, Daikon-style data invariants, Software, temporal property mining, Software engineering, computational complexity]
Profiling Kernels Behavior to Improve CPU / GPU Interactions
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Most modern computer and mobile devices have a graphical processing unit (GPU) available for any general purpose computation. GPU supports a programming model that is radically different from traditional sequential programming. As such, programming GPU is known to be hard and error prone, despite the large number of available APIs and dedicated programming languages. In this paper we describe a profiling technique that reports on the interaction between a CPU and GPUs. The resulting execution profile may then reveal anomalies and suboptimal situations, in particular due to an improper memory configuration. Our profiler has been effective at identifying suboptimal memory allocation usage in one image processing application.
[Computers, Measurement, Visualization, kernel behavior profiling technique, CPU-GPU interactions, suboptimal memory allocation usage, Graphics processing units, graphical processing unit, memory profiling, Programming, graphics processing units, image processing application, opencl, storage management, programming model, profiling, mobile devices, Central Processing Unit, gpgpu, Kernel, improper memory configuration]
Fast and Precise Statistical Code Completion
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The main problem we try to solve is API code completion which is both precise and works in real-time. We describe an efficient implementation of an N-gram language model combined with several smoothing methods and a completion algorithm based on beam search. We show that our system is both fast and precise using a thorough experimental evaluation. With optimal parameters we are able to find completions in milliseconds and the desired completion is in the top 3 suggestions in 89% of the time.
[Smoothing methods, application program interfaces, Computational modeling, statistical code completion, smoothing methods, APIs, Probability, N-gram language model, API code completion, programming languages, completion algorithm, Code Completion, beam search, experimental evaluation, Runtime, Synthesis, Semantics, Real-time systems, Libraries, statistical analysis, Statistical Language Models]
A Combined Technique of GUI Ripping and Input Perturbation Testing for Android Apps
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Mobile applications have become an integral part of the daily lives of millions of users, thus making necessary to ensure their security and reliability. Moreover the increasing number of mobile applications with rich Graphical User Interfaces (GUI) creates a growing need for automated techniques of GUI Testing for mobile applications. In this paper, the GUI Ripping Technique is combined with the Input Perturbation Testing to improve the quality of Android Application Testing. The proposed technique, based on a systematic and automatic exploration of the behavior of Android applications, creates a model of the explored GUI and then uses it to generate the perturbed text inputs. The technique was evaluated on many Android apps and its results were compared with random input tests.
[program testing, graphical user interfaces, software reliability, Android application testing, Humanoid robots, software quality, Android (operating system), mobile computing, input perturbation testing, mobile applications, Testing Automation, Input Perturbation Testing, Graphical user interfaces, Testing, GUI Ripping, GUI testing automated techniques, Mobile applications, GUI ripping technique, security of data, Computer bugs, perturbed text input generation, Android Application Testing, random input tests, Androids, Software engineering]
Enabling Testing of Android Apps
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Existing approaches for automated testing of An- droid apps are designed to achieve different goals and exhibit some pros and cons that should be carefully considered by developers and testers. For instance, random testing (RT) provides a high ratio of infeasible inputs or events, and test cases generated with RT and systematic exploration-based testing (SEBT) are not representative of natural (i.e., real) application usage scenarios. In addition, collecting test scripts for automated testing is expensive. We address limitations of existing tools for GUI-based testing of Android apps in a novel hybrid approach called T+. Our approach is based on a novel framework, which is aimed at generating actionable test cases for different testing goals. The framework also enables GUI-based testing without expensive test scripts collection for the stakeholders.
[Vocabulary, T+ approach, SEBT, program testing, graphical user interfaces, systematic exploration-based testing, Humanoid robots, automated testing, GUI-based testing, Systematics, Android (operating system), test script collection, Android apps testing, testing goal, natural application usage scenario, Androids, actionable test case generation, Testing, Graphical user interfaces, Software engineering, random testing]
An Approach to Detect Android Antipatterns
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Mobile applications are becoming complex software systems that must be developed quickly and evolve regularly to fit new user requirements and execution contexts. However, addressing these constraints may result in poor design choices, known as antipatterns, which may degrade software quality and performance. Thus, the automatic detection of antipatterns is an important activity that eases the future maintenance and evolution tasks. Moreover, it helps developers to refactor their applications and thus, to improve their quality. While antipatterns are well-known in object-oriented applications, their study in mobile applications is still in their infancy. In this paper, we presents a tooled approach, called Paprika, to analyze Android applications and to detect object-oriented and Android-specific antipatterns from binaries of applications.
[Measurement, Java, antipattern automatic detection, Humanoid robots, Android antipattern detection, Mobile communication, object-oriented detection, Mobile applications, software quality, software maintenance, software quality degradation, Paprika tooled approach, Android (operating system), mobile computing, mobile applications, object-oriented applications, complex software systems, Software, Androids, software performance degradation]
Textual Analysis for Code Smell Detection
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The negative impact of smells on the quality of a software systems has been empirical investigated in several studies. This has recalled the need to have approaches for the identification and the removal of smells. While approaches to remove smells have investigated the use of both structural and conceptual information extracted from source code, approaches to identify smells are based on structural information only. In this paper, we bridge the gap analyzing to what extent conceptual information, extracted using textual analysis techniques, can be used to identify smells in source code. The proposed textual-based approach for detecting smells in source code, coined as TACO (Textual Analysis for Code smell detectiOn), has been instantiated for detecting the Long Method smell and has been evaluated on three Java open source projects. The results indicate that TACO is able to detect between 50% and 77% of the smell instances with a precision ranging between 63% and 67%. In addition, the results show that TACO identifies smells that are not identified by approaches based on solely structural information.
[Java, Conferences, long method smell, software systems, information retrieval, textual analysis techniques, Societies, Data mining, textual analysis for code smell detection, Accuracy, Software systems, structural information, Java open source projects, TACO, Software engineering]
A Large Scale Study of License Usage on GitHub
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The open source community relies upon licensing in order to govern the distribution, modification, and reuse of existing code. These licenses evolve to better suit the requirements of the development communities and to cope with unaddressed or new legal issues. In this paper, we report the results of a large empirical study conducted over the change history of 16,221 open source Java projects mined from Git Hub. Our study investigates how licensing usage and adoption changes over a period of ten years. We consider both the distribution of license usage within projects of a rapidly growing forge and the extent that new versions of licenses are introduced in these projects.
[Java, Law, law, Conferences, public domain software, Software Licenses, legal issues, GitHub, Licenses, Data mining, Empirical Studies, code reuse, Mining Software Repositories, license usage, code modification, development communities, software reusability, code distribution, Software, open source Java projects, license distribution, open source community, licensing, Software engineering]
Understanding Conflicts Arising from Collaborative Development
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
When working in a collaborative development environment, developers implement tasks separately. Consequently, during the integration process, one might have to deal with conflicting changes. Previous studies indicate that conflicts occur frequently and impair developers' productivity. Such evidence motivates the development of tools that try to tackle this problem. However, despite the existing evidence, there are still many unanswered questions. The goal of this research is to investigate conflict characteristics in practice through empirical studies and use this body of knowledge to improve strategies that support software developers working collaboratively.
[Productivity, Conferences, Crystals, software developers, Conflicts, Empirical Studies, conflict characteristics, collaborative development, Semantics, Collaboration, groupware, Syntactics, Collaborative Software Development, software engineering, Software engineering]
Deep Representations for Software Engineering
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields. We propose that software engineering (SE) research is a unique opportunity to use these transformative approaches. Our research examines applications of deep architectures such as recurrent neural networks and stacked restricted Boltzmann machines to SE tasks.
[Context, Computational modeling, Conferences, stacked restricted Boltzmann machine, recurrent neural nets, deep architecture, compositional representation learning, Boltzmann machines, recurrent neural networks, Machine learning, Computer architecture, Software, software engineering, transformative approach, learning (artificial intelligence), deep learning subsumes algorithm, deep representation, Software engineering]
Automatic Categorization of Software Libraries Using Bytecode
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Automatic software categorization is the task of assigning categories or tags to software libraries in order to summarize their functionality. Correctly assigning these categories is essential to ensure that relevant libraries can be easily retrieved by developers from large repositories. Current categorization approaches rely on the semantics reflected in the source code, or use supervised machine learning techniques, which require a set of labeled software as a training data. These approaches fail when such information is not available. We propose a novel unsupervised approach for the automatic categorization of Java libraries, which uses the bytecode of a library in order to determine its category. We show that the approach is able to successfully categorize libraries from the Apache Foundation Repository.
[source code (software), bytecode, Java, Conferences, source code, Data mining, software libraries, unsupervised learning, Java libraries, automatic labeling, Software libraries, Accuracy, software categorization, unsupervised approach, Semantics, dirichlet process, Software, automatic software library categorization, Apache Foundation Repository, clustering]
Post-Dominator Analysis for Precisely Handling Implicit Flows
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Most web applications today use JavaScript for including third-party scripts, advertisements etc., which pose a major security threat in the form of confidentiality and integrity violations. Dynamic information flow control helps address this issue of information stealing. Most of the approaches over-approximate when unstructured control flow comes into picture, thereby raising a lot of false alarms. We utilize the post-dominator analysis technique to determine the context of the program at a given point and prove that this approach is the most precise technique to handle implicit flows.
[Context, Java, dynamic information flow control, Conferences, program diagnostics, post-dominator analysis technique, unstructured control flow, Lattices, security threat, Web applications, integrity violations, Security, Computer languages, authoring languages, security of data, confidentiality violations, implicit flow handling, Programmable logic arrays, JavaScript, Software engineering]
Casper: Using Ghosts to Debug Null Deferences with Dynamic Causality Traces
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Fixing software errors requires understanding their root cause. In this paper, we introduce "causality traces'', they are specially crafted execution traces augmented with the information needed to reconstruct a causal chain from a root cause to an execution error. We propose an approach and a tool, called Casper, for dynamically constructing causality traces for null dereference errors. The core idea of Casper is to inject special values, called "ghosts\
[Java, program debugging, software error fixing, Debugging, Casper, History, open-source projects, Open source software, Runtime, null deference error debug, ghosts, Computer bugs, Null value, execution error, software tools, dynamic causality traces]
Poster: Static Detection of Configuration-Dependent Bugs in Configurable Software
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Configurable software systems enable developers to configure at compile time a single variant of the system to tailor it towards specific environments and features. Although traditional static analysis tools can assist developers in software development and maintenance, they can only run on a concrete configuration of a configurable software system. Thus, it is necessary to derive many configurations so that the configuration-specific parts of the source code can be checked. To avoid this tedious and error-prone process, we propose an approach to automatically derive a set of configurations that cover as many combinations of configuration-specific blocks of code or source files as possible. We represent a C program with CPP directives (e.g., #ifdef) with a CPP control-flow graph (CPP-CFG) in which CPP expressions are condition nodes and #ifdef blocks are statement nodes. We then explore possible paths on CPP-CFG with dynamic symbolic execution and depth-first search algorithms, and correspondingly, producing possible combinations of concrete blocks of C code, on which an existing static analysis tool can run. Our preliminary evaluation on a benchmark of configuration-dependent bugs on Linux shows that our approach can detect more bugs than a state-of-the-art tool.
[software development, Heuristic algorithms, program diagnostics, graph theory, software maintenance, tree searching, static detection, configurable software systems, static analysis tools, configurable software, Linux, depth-first search algorithms, Computer bugs, CPP-CFG, Software systems, CPP directives, Concrete, C program, dynamic symbolic execution, configuration-specific blocks, configuration-dependent bugs, CPP control-flow graph, Kernel]
Poster: Improving Cloud-Based Continuous Integration Environments
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We propose a novel technique for improving the efficiency of cloud-based continuous integration development environments. Our technique identifies repetitive, expensive and time-consuming setup activities that are required to run integration and system tests in the cloud, and consolidates them into preconfigured testing virtual machines such that the overall costs of test execution are minimized. We create such testing machines by reconfiguring and opportunistically snapshotting the virtual machines already registered in the cloud.
[Software testing, testing machines, system tests, Virtual machining, linear programming, cost flow, Servers, Standards, integration test, test execution cost, test-driven development, flow constraints, virtual machines, Software, cloud computing, of cloud-based continuous integration development environments, snapshot, Load modeling, preconfigured testing virtual machines]
Poster: Interactive and Collaborative Source Code Annotation
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software documentation plays an important role in sharing the knowledge behind source code between distributed programmers. Good documentation makes source code easier to understand; on the other hand, developers have to constantly update the documentation whenever the source code changes. Developers will benefit from an automated tool that simplifies keeping documentation up-to-date and facilitates collaborative editing. In this paper, we explore the concept of collaborative code annotation by combining the idea from crowdsourcing. We introduce Cumiki, a web-based collaborative annotation tool that makes it easier for crowds of developers to collaboratively create the up-to-date documentation. This paper describes the user interface, the mechanism, and its implementation, and discusses the possible usage scenarios.
[Crowdsourcing, source code (software), distributed programmers, collaborative source code annotation, crowdsourcing, system documentation, Documentation, Cumiki tool, Programming, interactive source code annotation, collaborative editing, user interface, Collaboration, User interfaces, Web-based collaborative annotation tool, Software, Internet, software tools, distributed programming, software documentation, Software engineering]
Poster: Discovering Code Dependencies by Harnessing Developer's Activity
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Monitoring software developer's interactions in an integrated development environment is sought for revealing new information about developers and developed software. In this paper we present an approach for identifying potential source code dependencies solely from interaction data. We identify three kinds of potential dependencies and additionally assign them to developer's activity as well, to reveal detailed task-related connections in the source code. Interaction data as a source allow us to identify these candidates for dependencies even for dynamically typed programming languages, or across multiple languages in the source code. After first evaluations and positive results we continue with collecting data in professional environment of Web developers, and evaluating our approach.
[Context, source code (software), source code dependency discovery, implicit feedback, Web developers, software developer interaction monitoring, Navigation, Source code dependency, Debugging, Maintenance engineering, programming languages, task context, Computer languages, integrated development environment, interaction data, dynamic typing, Syntactics, system monitoring, Software, software engineering, programming environments]
Poster: Filtering Code Smells Detection Results
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Many tools for code smell detection have been developed, providing often different results. This is due to the informal definition of code smells and to the subjective interpretation of them. Usually, aspects related to the domain, size, and design of the system are not taken into account when detecting and analyzing smells. These aspects can be used to filter out the noise and achieve more relevant results. In this paper, we propose different filters that we have identified for five code smells. We provide two kind of filters, Strong and Weak Filters, that can be integrated as part of a detection approach.
[Couplings, Matched filters, strong filters, weak filters, Accuracy, code smell filtering, software reliability, Surgery, subjective code smell interpretation, Information filters, Libraries, code smell detection]
Poster: Enhancing Partition Testing through Output Variation
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
A major test case generation approach is to divide the input domain into disjoint partitions, from which test cases can be selected. However, we observe that in some traditional approaches to partition testing, the same partition may be associated with different output scenarios. Such an observation implies that the partitioning of the input domain may not be precise enough for effective software fault detection. To solve this problem, partition testing should be fine-tuned to additionally use the information of output scenarios in test case generation, such that these test cases are more fine-grained not only with respect to the input partitions but also from the perspective of output scenarios.
[fault diagnosis, output variation, program testing, software fault detection, Conferences, partition testing, Indexes, software fault tolerance, disjoint partitions, Fault detection, input domain partitioning, output scenario, Software, Concrete, choice relation framework, test case generation approach, Testing, Software engineering]
Poster: Segmentation Based Online Performance Problem Diagnosis
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Currently, the performance problems of software systems gets more and more attentions. Among various diagnosis methods based on system traces, principal component analysis (PCA) based methods are widely used due to the high accuracy of the diagnosis results and requiring no specific domain knowledge. However, according to our experiments, we have validated several shortcomings existed in PCA-based methods, including requiring traces with a same call sequence, inefficiency when the traces are long, and missing performance problems. To cope with these issues, we introduce a segmentation based online diagnosis method in this poster.
[Measurement, software systems performance problems, performance problem diagnosis, system trace, Conferences, program diagnostics, software reliability, PCA based methods, online performance problem diagnosis, segmentation based online diagnosis method, Accuracy, diagnosis methods, Computer architecture, Software systems, system traces, Monitoring, principal component analysis, software performance evaluation, Principal component analysis]
Poster: Symbolic Execution of MPI Programs
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
MPI is widely used in high performance computing. In this extended abstract, we report our current status of analyzing MPI programs. Our method can provide coverage of both input and non-determinism for MPI programs with mixed blocking and non-blocking operations. In addition, to improve the scalability further, a deadlock-oriented guiding method for symbolic execution is proposed. We have implemented our methods, and the preliminary experimental results are promising.
[message passing, MPI program analysis, Symbolic Execution, Asynchronous, application program interfaces, Scalability, program diagnostics, software reliability, MPI program symbolic execution, MPI, parallel processing, Standards, deadlock-oriented guiding method, Deadlock, Synchronous, Runtime, High performance computing, Message passing, System recovery, high performance computing, Space exploration]
Poster: Automatically Fixing Real-World JavaScript Performance Bugs
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Programs often suffer from poor performance that can be fixed by relatively simple changes. Currently, developers either manually identify and fix such performance problems, or they rely on compilers to optimize their code. Unfortunately, manually fixing performance bugs is non-trivial, and compilers are limited to a predefined set of optimizations. This paper presents an approach for automatically finding and fixing performance bugs in JavaScript programs. To focus our work on relevant problems, we study 37 real-world performance bug fixes from eleven popular JavaScript projects and identify several recurring fix patterns. Based on the results of the study, we present a static analysis that identifies occurrences of common fix patterns and a fix generation technique that proposes to transform a given program into a more efficient program. Applying the fix generation technique to three libraries with known performance bugs yields fixes that are equal or equivalent to those proposed by the developers, and that lead to speedups between 10% and 25%.
[real-world performance bugs, Java, program debugging, program diagnostics, fix generation technique, Transforms, Maintenance engineering, static analysis, JavaScript programs, compilers, Optimization, Reactive power, Computer bugs, Semantics, Libraries, common fix patterns]
Poster: Dynamic Analysis Using JavaScript Proxies
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
JavaScript has become a popular programming language. However, its highly dynamic nature encumbers static analysis for quality assurance purposes. Only dynamic techniques such as concolic testing seem to cope. Often, these involve an instrumentation phase in which source code is extended with analysis-specific concerns. The corresponding implementations represent a duplication of engineering efforts. To facilitate developing dynamic analyses for JavaScript, we introduce Aran; a general-purpose JavaScript instrumenter that takes advantage of proxies, a recent addition to the JavaScript reflection APIs.
[source code (software), program testing, Instrumentation, software quality, static program analysis, Dynamic Analysis, Runtime, ECMAScript6, authoring languages, JavaScript, programming language, Performance analysis, JavaScript proxies, Testing, Java, dynamic program analysis, Instruments, program diagnostics, source code, Harmony Proxy, Browsers, Aran general-purpose JavaScript instrumenter, Shadow mapping, instrumentation phase, concolic testing, quality assurance, JavaScript reflection APIs]
Poster: Is Carmen Better than George? Testing the Exploratory Tester Using HCI Techniques
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Exploratory software testing is an activity which can be carried out by both untrained and formally trained testers. In this paper, we propose using Human Computer Interaction (HCI) techniques to carry out a study of exploratory testing strategies used by the two groups of testers. This data will be used to make recommendations to companies with regards to the mix of skills and training required for testing teams.
[Human computer interaction, Software testing, Training, program testing, Computer bugs, Exploratory Testing Strategies, Data collection, Software, human computer interaction, HCI techniques, Human-Computer Interaction, exploratory software testing]
Poster: VIBeS, Transition System Mutation Made Easy
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Mutation testing is an established technique used to evaluate the quality of a set of test cases. As model-based testing took momentum, mutation techniques were lifted to the model level. However, as for code mutation analysis, assessing test cases on a large set of mutants can be costly. In this paper, we introduce the Variability-Intensive Behavioural teSting (VIBeS) framework. Relying on Featured Transition Systems (FTSs), we represent all possible mutants in a single model constrained by a feature model for mutant (in)activation. This allow to assess all mutants in a single test case execution. We present VIBeS implementation steps and the DSL we defined to ease model-based mutation analysis.
[single test case execution, Java, Adaptation models, Featured Transition Systems, program testing, VIBeS, variability-intensive behavioural testing framework, Computational modeling, mutation testing, software quality, transition system mutation techniques, FTSs, Analytical models, Model-Based Mutation Testing, model-based testing, featured transition systems, code mutation analysis, DSL, Testing, Load modeling, model-based mutation analysis]
Poster: ProNat: An Agent-Based System Design for Programming in Spoken Natural Language
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The emergence of natural language interfaces has led to first attempts of programming in natural language. We present ProNat, a tool for script-like programming in spoken natural language (SNL). Its agent-based architecture unifies deep natural language understanding (NLU) with modular software design. ProNat focuses on the extraction of processing flows and control structures from spoken utterances. For evaluation we have begun to build a speech corpus. First experiments are conducted in the domain of domestic robotics, but ProNat's architecture makes domain acquisition easy. Test results with spoken utterances in ProNat seem promising, but much work has to be done to achieve deep NLU.
[ProNat architecture, speech corpus, Unified modeling language, agent-based architecture, Programming, Ontologies, robotics, processing flow extraction, programming languages, control structures, deep natural language understanding, SNL, software architecture, authoring languages, NLU, spoken natural language, software tools, programming in natural language, agent-based software design, Robots, ontology, natural language processing, Natural languages, natural language interfaces, software agents, modular software design, end user programming, agent-based system design, natural lanuage understanding, script-like programming, spoken utterances, Speech recognition, Speech, domestic robotics]
Poster: Static Analysis of Concurrent Higher-Order Programs
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Few static analyses support concurrent higher-order programs. Tools for detecting concurrency bugs such as deadlocks and race conditions are nonetheless invaluable to developers. Concurrency can be implemented using a variety of models, each supported by different synchronization primitives. Using this poster, we present an approach for analyzing concurrent higher-order programs in a precise manner through abstract interpretation. We instantiate the approach for two static analyses that are capable of detecting deadlocks and race conditions in programs that rely either on compare-and-swap (cas), or on conventional locks for synchronization. We observe few false positives and false negatives on a corpus of small concurrent programs, with better results for the lock-based analyses. We also observe that these programs lead to a smaller state space to be explored by the analyses. Our results show that the choice of synchronization primitives supported by an abstract interpreter has an important impact on the complexity of the static analyses performed with this abstract interpreter.
[Adaptation models, synchronization primitives, program diagnostics, abstract interpretation, Programming, static analysis, Explosions, Synchronization, synchronisation, deadlock detection, concurrency, Concurrent computing, abstract interpreter, lock-based analysis, System recovery, Model checking, higher-order, compare-and-swap, static concurrent higher-order program analysis, concurrency bug detection, race conditions, concurrency (computers)]
Poster: Conquering Uncertainty in Java Programming
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Uncertainty in programming is one of the challenging issues to be tackled, because it is error-prone for many programmers to temporally avoid uncertain concerns only using simple language constructs such as comments and conditional statements. This paper proposes ucJava, a new Java programming environment for conquering uncertainty. Our environment provides a modular programming style for uncertainty and supports test-driven development taking uncertainty into consideration.
[Java, Uncertainty, Java programming environment, comments, Java programming uncertainty, Programming, modular programming style, ucJava, Programming environments, Connectors, conditional statements, test-driven development, language constructs, Testing]
Poster: MAPP: The Berkeley Model and Algorithm Prototyping Platform
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We describe the Berkeley Model and Algorithm Prototyping Platform (MAPP), designed to facilitate experimentation with numerical algorithms and models. MAPP is written entirely in MATLAB and is available as open source under the GNU GPL.
[Algorithm design and analysis, GNU GPL, Object oriented modeling, graphical user interfaces, public domain software, open source software, MATLAB, MAPP, SPICE, numerical algorithms, Numerical models, Mathematical model, Integrated circuit modeling, model-and-algorithm prototyping platform, Matlab]
Poster: An Efficient Equivalence Checking Method for Petri Net Based Models of Programs
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The initial behavioural specification of any software programs goes through significant optimizing and parallelizing transformations, automated and also human guided, before being mapped to an architecture. Establishing validity of these transformations is crucial to ensure that they preserve the original behaviour. PRES+ model (Petri net based Representation of Embedded Systems) encompassing data processing is used to model parallel behaviours. Being value based with inherent scope of capturing parallelism, PRES+ models depict such data dependencies more directly; accordingly, they are likely to be more convenient as the intermediate representations (IRs) of both the source and the transformed codes for translation validation than strictly sequential variable-based IRs like Finite State Machines with Data path (FSMDs) (which are essentially sequential control flow graphs (CFGs)). In this work, a path based equivalence checking method for PRES+ models is presented.
[source code (software), program verification, Petri nets, FSMD, CFG, parallelism, finite state machines with data path, parallelizing transformations, formal specification, parallel behaviours, source codes, parallel programming, software architecture, embedded systems, Benchmark testing, Equivalence checking, transformed codes, sequential control flow graphs, FSMD model, optimizing transformations, intermediate representations, architecture, path based equivalence checking method, data dependencies, Computational modeling, sequential variable-based IR, data processing, PRES+ model, Sparks, software programs, program interpreters, Embedded systems, Petri net based models, behavioural specification, Petri net based representation of embedded systems, translation validation, Data models]
Poster: Model-based Run-time Variability Resolution for Robotic Applications
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
In this paper we present our ongoing work on Robotics Run-time Adaptation (RRA). RRA is a model-driven approach that addresses robotics runtime adaptation by modeling and resolving run-time variability of robotic applications.
[Context, RRA, Adaptation models, Model-based Approach, robotics run-time adaptation, model-driven approach, Runtime Adaptation, Engines, Software architecture, robotic applications, Computer architecture, model-based run-time variability resolution, robots, Variability Resolution, Robots, Context modeling]
Poster: Tierless Programming in JavaScript
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Whereas "responsive" web applications already offered a more desktop-like experience, there is an increasing user demand for "rich" web applications (RIAs) that offer collaborative and even off-line functionality. Realizing these qualities requires distributing previously centralized application logic and state vertically from the server to a client tier (e.g., for desktop-like and off-line client functionality), and horizontally between instances of the same tier (e.g., for collaborative client functionality and for scaling of resource-starved services). Both bring about the essential complexity of distributing application assets and maintaining their consistency, along with the accidental complexity of reconciling a myriad of heterogenous tier-specific technology. Tierless programming languages enable developing web applications as a single artefact that is automatically split in tier-specific code - resulting in a development process akin to that of a desktop application. This relieves developers of distribution and consistency concerns, as well as the need to align different tier-specific technologies. However, programmers still have to adopt a new and perhaps esoteric language. We therefore advocate developing tierless programs in a general-purpose language instead. In this poster, we introduce our approach to tierless programming in JavaScript. We expand upon our previous work by identifying development challenges arising from this approach that could be resolved through tool support.
[Java, accidental complexity, Programming, general-purpose language, Complexity theory, RIA, Servers, programming languages, Reactive power, tierless programming languages, Prototypes, Collaboration, JavaScript, responsive Web applications, Libraries, Internet]
Poster: Software Development Risk Management: Using Machine Learning for Generating Risk Prompts
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software risk management is a critical component of software development management. Due to the magnitude of potential losses, risk identification and mitigation early on become paramount. Lists containing hundreds of possible risk prompts are available both in academic literature as well as in practice. Given the large number of risks documented, scanning the lists for risks and pinning down relevant risks, though comprehensive, becomes impractical. In this work, a machine learning algorithm is developed to generate risk prompts, based on software project characteristics and other factors. The work also explores the utility of post-classification tagging of risks.
[risk management, Machine learning algorithms, Taxonomy, identification technology, software development management, post-classification tagging, risk prompts, software management, Software risk, machine learning, risk mitigation, Neural networks, Tagging, software risk management, risk identification, Software, Distance measurement, Risk management, learning (artificial intelligence)]
Poster: Reasoning Based on Imperfect Context Data in Adaptive Security
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Enabling software systems to adjust their protection in continuously changing environments with imperfect context information is a grand challenging problem. The issue of uncertain reasoning based on imperfect information has been overlooked in traditional logic programming with classical negation when applied to dynamic systems. This paper sketches a non-monotonic approach based on Answer Set Programming to reason with imperfect context data in adaptive security where there is little or no knowledge about certainty of the actions and events.
[Context, non-monotonic logic, Adaptation models, Patents, software systems, adaptive security, Cognition, Security, Adaptive security, nonmonotonic approach, imperfect data, imperfect context data, security of data, dynamic systems, logic programming, data protection, Monitoring, answer set programming, Context modeling]
Automated Planning for Self-Adaptive Systems
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Self-adaptation has been proposed as a viable solution to alleviate the management burden that is induced by the dynamic nature and increasing complexity of computer systems. In this context, architectural-based self-adaptation has emerged as one of the most promising approaches to automatically manage such systems, resorting to a control loop that includes monitoring, analyzing, planning, and executing adequate actions. This work addresses the challenges of adaptation planning -the decision-making process for selecting an appropriate course of action- with a focus on the problem of provisioning automated mechanisms for assembling adaptation plans, as a means to enhance adaptive capabilities under uncertainty. To this purpose, adaptations are modeled in a hierarchical manner, defining primitive actions, guarded actions, and deliberate plans, which may guide the system towards a desired state.
[Measurement, Adaptation models, Uncertainty, Adaptive systems, architectural-based self-adaptation, system management, Medical services, self-adaptive, self-management, control loop, planning, automated planning, computer systems, decision making, adaptation planning, decision-making process, PDDL, Software, fault tolerant computing, self-adaptive systems, Planning]
Understanding the Software Fault Introduction Process
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Testing and debugging research revolves around faults, yet we have a limited understanding of the processes by which faults are introduced and removed. Previous work in this area has focused on describing faults rather than explaining the introduction and removal processes, meaning that a great deal of testing and debugging research depends on assumptions that have not been empirically validated. We propose a three-phase project to develop an explanatory theory of the fault introduction process and describe how the project will be completed.
[program debugging, program testing, Debugging, Medical services, testing, debugging research, Computer crashes, removal processes, software fault tolerance, software faults, Fault diagnosis, testing research, three-phase project, software fault introduction process, research and development, debugging, Software, Testing, Software engineering]
Scalable Formal Verification of UML Models
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
UML (Unified Modeling Language) has been used for years in diverse domains. Its notations usually come with a reasonably well-defined syntax, but its semantics is left under-specified and open to different interpretations. This freedom hampers the formal verification of produced specifications and calls for more rigor and precision. This work aims to bridge this gap and proposes a flexible and modular formalization approach based on temporal logic. We studied the different interpretations for some of its constructs, and our framework allows one to assemble the semantics of interest by composing the selected formalizations for the different pieces. However, the formalization per-se is not enough. The verification process, in general, becomes slow and impossible -as the model grows in size. To tackle the scalability problem, this work also proposes a bit-vector-based encoding of LTL formulae. The first results witness a significant increase in the size of analyzable models, not only for our formalization of UML models, but also for numerous other models that can be reduced to bounded satisfiability checking of LTL formulae.
[UML model, Unified Modeling Language, Scalability, Conferences, Unified modeling language, temporal logic, Encoding, scalable formal verification, Temporal Logic, unified modeling language, Analytical models, Bounded Model Checking, formal verification, modular formalization approach, Formal Methods, Semantics, LTL formulae, UML, Model checking, Bit-Vector Logic, satisfiability checking, bit-vector-based encoding]
Scalability Studies on Selective Mutation Testing
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Mutation testing is a test method which is designed to evaluate a test suite's quality. Due to the expensive cost of mutation testing, selective mutation testing was first proposed in 1991 by Mathur, in which a subset of mutants are selected aiming to achieve the same effectiveness as the whole set of mutants in evaluating the quality of test suites. Though selective mutation testing has been widely investigated in recent years, many people still doubt if it can suit well for large programs. Realizing that none of the existing work has systematically studied the scalability of selective mutation testing, I plan to work on the scalability of selective mutation testing through several studies.
[Measurement, scalability studies, Java, program testing, Scalability, Medical services, Predictive models, Software, selective mutation testing, Testing]
Qualitative Analysis of Knowledge Transfer in Pair Programming
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Knowledge transfer in the context of pair programming is both a desired effect and a necessary precondition. There is no detailed understanding yet of how effective and efficient knowledge transfer in this particular context actually works. My qualitative research is concerned with the analysis of professional software developer's sessions to capture their specific knowledge transfer skill in the form of comprehensible, relevant, and practical patterns.
[Context, software prototyping, knowledge management, Programming profession, Knowledge transfer, professional aspects, pair programming, professional software developer session analysis, qualitative analysis, Propulsion, knowledge transfer skill, Software, Software engineering]
DIETs: Recommender Systems for Mobile API Developers
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The increasing number of posts related to mobile app development indicates unaddressed problems in the usage of mobile APIs. Arguing that these problems result from in- adequate documentation and shortcomings in the design and implementation of the APIs, the goal of this research is to develop and evaluate two developers' issues elimination tools (DIETs) for mobile API developers to diminish the problems of mobile applications (apps) development.After categorizing the problems, we investigate their causes, by exploring the relationships between the topics and trends of posts on Stack Overflow, the app developers' experience, the API and test code, and its changes. The results of these studies will be used to develop two DIETs that support API developers to improve the documentation, design, and implementation of their APIs.
[developer issues elimination tool, application program interfaces, app developer experience, system documentation, Humanoid robots, mobile API developers, Documentation, Mobile communication, Data mining, mobile application development, Stack Overflow, mobile app development, mobile computing, recommender systems, test code, DIET, mobile api developers, api, Androids, Smart phones, Software engineering]
Statistical Learning and Software Mining for Agent Based Simulation of Software Evolution
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
In the process of software development it is of high interest for a project manager to gain insights about the ongoing process and possible development trends at several points in time. Substantial factors influencing this process are, e.g., the constellation of the development team, the growth and complexity of the system, and the error-proneness of software entities. For this purpose we build an agent based simulation tool which predicts the future of a project under given circumstances, stored in parameters, which control the simulation process. We estimate these parameters with the help of software mining. Our work exposed the need for a more fine-grained model for the developer behavior. Due to this we create a learning model, which helps us to understand the contribution behavior of developers and, thereby, to determine simulation parameters close to reality. In this paper we present our agent based simulation model for software evolution and describe how methods from statistical learning and data mining serves us to estimate suitable simulation parameters.
[Adaptation models, software development process, statistical learning, Object oriented modeling, Agent Based Modeling, data mining, Predictive models, digital simulation, Software Process Simulation, Data mining, software agents, software mining, software evolution, Hidden Markov models, Developer Behavior, parameter estimation, Software, Data models, software engineering, agent based simulation tool, Hidden Markov Model, learning (artificial intelligence)]
Towards Model Driven Architecture and Analysis of System of Systems Access Control
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Nowadays there is growing awareness of the importance of Systems of Systems (SoS) which are large-scale systems composed of complex systems. SoS possess specific properties when compared with monolithic complex systems, in particular: operational independence, managerial independence, evolutionary development, emergent behavior and geographic distribution. One of the current main challenges is the impact of these properties on SoS security modeling and analysis. In this research proposal, we introduce a new method incorporating a process, a language and a software architectural tool to model, analyze and predict security architectural alternatives of SoS. Thus security will be taken into account as soon as possible in the life cycle of the SoS, making it less expensive.
[geographic distribution, model driven architecture, Conferences, system-of-systems access control analysis, Unified modeling language, Medical services, safety-critical software, Security, Analytical models, software architecture, Maritime Security, complex systems, Computer architecture, authorisation, managerial independence, Systems of systems, evolutionary development, Architectural Alternatives, SoS life cycle, SoS security analysis, marine safety, Simulation., Model Driven Engineering, large-scale systems, emergent behavior, software architectural tool, SoS security modeling, systems engineering, operational independence]
A Unified Approach to Automatic Testing of Architectural Constraints
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Architectural decisions are often encoded in the form of constraints and guidelines. Non-functional requirements can be ensured by checking the conformance of the implementation against this kind of invariant. Conformance checking is often a costly and error-prone process that involves the use of multiple tools, differing in effectiveness, complexity and scope of applicability. To reduce the overall effort entailed by this activity, we propose a novel approach that supports verification of human-readable declarative rules through the use of adapted off-the-shelf tools. Our approach consists of a rule specification DSL, called Dicto, and a tool coordination framework, called Probo. The approach has been implemented in a soon to be evaluated prototype.
[automatic test software, conformance checking, program testing, architectural decisions, Probo, human-readable declarative rule verification, formal specification, unified approach, Guidelines, error-prone process, rule specification DSL, software architecture, Software architecture, automatic architectural constraint testing, nonfunctional requirements, Dicto, Computer architecture, tool coordination framework, Software, DSL, Stakeholders, architectural constraints, Testing]
Safe Evolution Patterns for Software Product Lines
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Despite a global recognition of the problem, and massive investment from researchers and practitioners, the evolution of complex software systems is still a major challenge for today's architects and developers. In the context of product lines, or highly configurable systems, variability in the implementation and design makes many of the pre-existing challenges even more difficult to tackle. Many approaches and tools have been designed, but developers still miss the tools and methods enabling safe evolution of complex, variable systems. In this paper, we present our research plans toward this goal: making the evolution of software product lines safer. We show, by use of two concrete examples of changes that occurred in Linux, that simple heuristics can be applied to facilitate change comprehension and avoid common mistakes, without relying on heavy tooling. Based on those observations, we present the steps we intend to take to build a framework to regroup and classify changes, run simple checks, and eventually increase the quality of code deliveries affecting the variability model, mapping and implementation of software product lines.
[Context, software product lines, change comprehension, Frequency modulation, code delivery quality, Conferences, evolution patterns, software systems evolution, Software product lines, software quality, evolution, software maintenance, variability, Product line, Linux, variability model, Feature extraction, Kernel]
Strategies for Prioritizing Test Cases Generated Through Model-Based Testing Approaches
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software testing is expensive and time consuming,especially for complex software. In order to deal with the costof testing, researchers develop Model-Based Testing (MBT). InMBT, test cases are generated automatically and a drawback isa huge generated test suite. Our research aims at studying the Test Case Prioritization problem in MBT context. So far, we already evaluated the influence of the model structure and the characteristics of the test cases that fail. Results suggest that the former does not affect significantly the performance of techniques, however, the latter indeed represents a major impact. Therefore, a worthy information in this context might be an expert who knows the crucial parts of the software, thus we propose the first version of a prioritization technique that considers hints from the expert and the distance notion in order to prioritize test cases. Evaluation and tuning of the technique are ongoing, but preliminary evaluation reveals promising results.
[Context, program testing, Conferences, software testing, Unified modeling language, model-based testing approach, test case prioritization problem, test case generation, Software, MBT, prioritization technique, Testing, Context modeling, Software engineering]
Towards a Practical Security Analysis Methodology
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The research community has proposed numerous techniques to perform security-oriented analyses based on a software design model. Such a formal analysis can provide precise security guarantees to the software designer, and facilitate the discovery of subtle flaws. Nevertheless, using such techniques in practice poses a big challenge for the average software designer, due to the narrow scope of each technique, the heterogeneous set of modelling languages that are required, and the analysis results that are often hard to interpret. Within the course of our research, we intend to provide practitioners with an integrated, easy-to-use modelling and analysis environment that enables them to work on a broad range of common security concerns without leaving the software design's level of abstraction.
[modelling languages, Vocabulary, security analysis methodology, security-oriented analysis, Unified modeling language, security guarantee, software designabstraction level, Analytical models, Software design, security of data, software design model, formal analysis, software engineering, Cryptography]
Measuring Software Developers' Perceived Difficulty with Biometric Sensors
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
As a developer works on a change task, he or she might perceive some parts of the task as easy and other parts as being very difficult. Currently, little is known about when a developer experiences different difficulty levels, although being able to assess these difficulty levels would be helpful for many reasons. For instance, a developer's perceived difficulty might be used to determine the likelihood of a bug being introduced into the code or the quality of the code a developer is working with. In psychology, biometric measurements, such as electro-dermal activity or heart rate, have already been extensively used to assess a person's mental state and emotions, but only little research has been conducted to investigate how these sensors can be used in the context of software engineering. In our research we want to take advantage of the insights gained in these psychological studies and investigate whether such biometric sensors can be used to measure developers' perceived difficulty while working on a change task and support them in their work.
[heart rate, person mental state assessment, Psychology, biometric sensors, software quality, code quality, biometric measurements, software developer measurement, sensors, psychology, emotion assessment, electro-dermal activity, Software, software engineering, Biosensors, Software measurement, Software engineering]
Mining Patterns of Sensitive Data Usage
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
When a user downloads an Android application from a market, she does not know much about its actual behavior. A brief description, a set of screenshots, and the list of permissions, which give a high level intuition of what the application might be doing, are all the user sees before installing and running the application on his device. These elements are not enough to decide whether the application is secure, and for sure they do not indicate whether it might violate the user's privacy by leaking some sensitive data. The goal of my thesis is to employ both static and dynamic taint analyses to gather information on how Android applications use sensitive data. The main hypothesis of this work is that malicious and benign mobile applications differ in how they use sensitive data, and consequently information flow can be used effectively to identify malware.
[invasive software, sensitive data usage pattern, program diagnostics, pattern mining, Humanoid robots, data mining, Medical services, Data mining, static taint analysis, Android (operating system), mobile computing, application security, user privacy, dynamic taint analysis, mobile applications, malware identification, Malware, data privacy, Androids, Android application, Software engineering]
Search-Based Migration of Model Variants to Software Product Line Architectures
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software Product Lines (SPLs) are families of related software systems developed for specific market segments or domains. Commonly, SPLs emerge from sets of existing variants when their individual maintenance becomes infeasible. However, current approaches for SPL migration do not support design models, are partially automated, or do not reflect constraints from SPL domains. To tackle these limitations, the goal of this doctoral research plan is to propose an automated approach to the SPL migration process at the design level. This approach consists of three phases: detection, analysis and transformation. It uses as input the class diagrams and lists of features for each system variant, and relies on search-based algorithms to create a product line architecture that best captures the variability present in the variants. Our expected contribution is to support the adoption of SPL practices in companies that face the scenario of migrating variants to SPLs.
[software product lines, search-based algorithm, search-based migration algorithm, Search-Based Software Engineering, Unified modeling language, SPL architecture, Migration, Medical services, Reuse, Software product lines, software product line architecture, software architecture, Software Product Line, Re-engineering, Programmable logic arrays, Feature extraction, Software, product line architecture, SPL migration process]
On the Architecture-Driven Development of Software-Intensive Systems-of-Systems
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Nowadays, complex software-intensive systems have resulted from the integration of heterogeneous independent systems, thus leading to a new class of systems called Systems-of-Systems (SoS). As in any system, SoS architectures have been regarded as an important element for determining their success. However, the state of the art reveals shortcomings that contribute to compromise the quality of these systems, as their inherent characteristics (such as emergent behavior and evolutionary development) are often not properly addressed. In this context, this PhD research aims at investigating how SoS software architectures can be used to model and evolve these systems. As main contribution, an architecture-centric approach for developing software-intensive SoS with focus on the formal specification and dynamic reconfiguration of their architectures is proposed. Such an approach mainly intends to contribute to fill some of the relevant existing gaps regarding the development of software-intensive SoS driven by their software architectures.
[Context, systems-of-systems, software quality, Middleware, formal specification, SoS software architectures, architecture-driven engineering, software architecture, Runtime, software-intensive systems-of-systems, SoS, Computer architecture, architecture-driven development, architecture-centric approach, heterogeneous independent systems]
Automatic Documentation Generation via Source Code Summarization
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Programmers need software documentation. However, documentation is expensive to produce and maintain, and often becomes outdated over time. Programmers often lack the time and resources to write documentation. Therefore, automated solutions are desirable. Designers of automatic documentation tools are limited because there is not yet a clear understanding of what characteristics are important to generating high quality summaries. I propose three specific research objectives to improving automatic documentation generation. I propose to study the similarity between source code and summary. Second, I propose studying whether or not including contextual information about source code improves summary quality. Finally, I propose to study the problem of similarity in source code structure and source code documentation. This paper discusses my work on these three objectives towards my Ph.D. dissertation, including my preliminary and proposed work.
[Context, Measurement, source code (software), summary quality, Java, source code structure similarity, Natural languages, system documentation, automatic documentation tools, Documentation, source code summarization, software quality, automatic documentation, source code documentation, Semantics, automatic documentation generation, Software, programmers, software engineering, software documentation]
A Declarative Foundation for Comprehensive History Querying
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Researchers in the field of Mining Software Repositories perform studies about the evolution of software projects. To this end, they use the version control system storing the changes made to a single software project. Such studies are concerned with the source code characteristics in one particular revision, the commit data for that revision, how the code evolves over time and what concrete, fine-grained changes were applied to the source code between two revisions. Although tools exist to analyse an individual concern, scripts and manual work is required to combine these tools to perform a single experiment. We present a general-purpose history querying tool named QwalKeko that enables expressing these concerns in a single uniform language, and having them detected in a git repository. We have validated our work by means of replication studies as well as through MSR studies of our own.
[source code (software), general-purpose history querying tool, program querying, data mining, mining software repositories, Medical services, Programming, History, Database languages, git repository, comprehensive history querying, single uniform language, Libraries, Java, project management, declarative foundation, version control system, software repository mining, software project evolution, software maintenance, source code characteristics, QwalKeko, configuration management, declarative programming, Software, history querying]
An Integrated Multi-Agent-Based Simulation Approach to Support Software Project Management
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software projects often do not accomplish what is expected. They fail to comply with the planned schedule, cost more than predicted, or are simply not completed at all owing to issues such as bad planning, a poorly chosen team or an incorrect definition of the tasks to be performed. Although simulation methods and tools have been introduced to alleviate these problems, there is a lack of simulation approaches that integrate software project knowledge, software development processes, project-related situation-awareness, and learning techniques to help project managers to make more informed decisions and hence reach successful conclusions with software projects. In addition, in order to be more proactive, such approaches need to provide simulations based on both static and dynamic situation-awareness data, support (self-)adaptive project planning and execution, and recommend remedial courses of action when real-time project anomalies occur. In this context, this PhD research aims to create an integrated multi-agent-based simulation to support software project management in a more comprehensive way.
[Adaptation models, multi-agent systems, project management, dynamic situation-awareness data, Decision making, Project management, software development management, static situation-awareness data, multi-agent-based simulation, planning, project-related situation-awareness, integrated multiagent-based simulation approach, self-adaptive project planning, learning techniques, Software, Data models, Real-time systems, Planning, learning (artificial intelligence), software project management, software development processes]
Towards Generation of Software Development Tasks
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The presence of well defined fine-grained sub-tasks is important to the development process: having a fine-grained task context has been shown to allow developers to more efficiently resume work. However, determining how to break a high level task down into sub-tasks is not always straightforward. Sometimes developers lack experience, and at other times, the task definition is not clear enough to afford confident decomposition. In my research I intend to show that by using syntactic mining of past task descriptions and their decomposition, I can provide automatically derived sub-task suggestions to afford more confident task decomposition by developers.
[data mining, Medical services, Programming, Logic gates, fine-grained sub-tasks, syntactic mining, Software, software engineering, software development tasks, Electronic mail, Data mining, Stress]
Contributor's Performance, Participation Intentions, Its Influencers and Project Performance
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software project performance largely depends on the software development team. Studies have shown that interest and activity levels of contributors at any time significantly affect project success measures. This dissertation provides suggestions to enhance contributors' performance and participation intentions to help improve project performance. To do so, we mine historical data in software repositories from a two-pronged approach: 1) To assess contributors' performance to identify strengths and areas of improvement. 2) To measure the influence of factors on contributors' participation and performance, and provide suggestions that help advance contributor's engagement. The methodology used in this study leverage empirical techniques, both quantitative and qualitative, to conduct the analysis. We believe that the insights presented here will help contributors improve their performance. Also, we expect managers and business analysts to benefit from the suggestions to revise factors that negatively influence contributors' engagement and hence improve project performance.
[historical data mining, project management, contributor engagement, software development team, software repository, data mining, software development management, Data mining, contributor performance, Open source software, project performance, Atmospheric measurements, Mining Software Repositories, participation intentions, Participation Intentions, software project performance, Particle measurements, Software Projects, Software measurement, Performance, Software engineering]
Supporting Scientific SE Process Improvement
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The increasing complexity of scientific software can result in significant impacts on the research itself. In traditional software development projects, teams adopt historical best practices into their development processes to mitigate the risk of such problems. In contrast, the gap that has formed between the traditional and scientific software communities leaves scientists to rely on only their own experience when facing software process improvement (SPI) decisions. Rather than expect scientists to become software engineering (SE) experts or the SE community to learn all of the intricacies involved in scientific software development projects, we seek a middle ground. The Scientific Software Process Improvement Framework (SciSPIF) will allow scientists to self-drive their own SPI efforts while leveraging the collective experiences of their peers and linking their concerns to established SE best practices. This proposal outlines the known challenges of scientific software development, relevant concepts from traditional SE research, and our planned methodology for collecting the data required to construct SciSPIF while staying grounded in the actual goals and concerns of the scientists.
[software development process, Scientific computing, Conferences, risk mitigation, Best practices, scientific software complexity, software process improvement, software engineering experts, software engineering, natural sciences computing, Interviews, Scientific Software Process Improvement Framework, scientific software, risk management, project management, scientific software development project, scientific SE process improvement, software development management, scientific software community, Encoding, process improvement, SciSPIF, SPI decision, Software, Software engineering]
A Comprehensive Framework for the Development of Dynamic Smart Spaces
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The conception of reliable smart spaces requires a suitable and comprehensive framework for their design, implementation, testing, and deployment. Numerous solutions have been proposed to solve different aspects related to smart spaces, but we still lack a concrete framework that provides solutions suitable for the whole development life-cycle. This work aims to fill the gap and proposes a framework that provides: (i) well-defined abstractions for designing smart spaces, (ii) a middleware infrastructure to implement them and plug physical objects, (iii) a semantic layer to support heterogeneous elements, and (iv) plugs to integrate external simulators and be able to always work on "complete'' systems in the different phases of the development.
[Context, Conferences, Buildings, Middleware, Intelligent sensors, smart space designing, middleware infrastructure, development life-cycle, external simulators, Semantics, dynamic smart space development, semantic layer, heterogeneous elements, middleware]
Verification of Android Applications
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This study investigates an alternative approach to analyze Android applications using model checking. We develop an extension to Java Path Finder (JPF) called JPF-Android to verify Android applications outside of the Android platform. JPF is a powerful Java model checker and analysis engine that is very effective at detecting corner-case and hard-to-find errors using its fine-grained analysis capabilities. JPF-Android provides a simplified model of the Android application framework on which an Android application can run and it can generate input events or parse an input script containing sequences of input events to drive the execution of the application. JPF-Android traverses all execution paths of the application by simulating these input events and can detect common property violations such as deadlocks and runtime exceptions in Android applications. It also introduces user defined execution specifications called Checklists to verify the flow of application execution.
[Java, corner-case errors detection, program verification, Java model checker, Humanoid robots, runtime exceptions, hard-to-find errors detection, application execution, execution paths, Analytical models, Android (operating system), Java path finder, Checklists, model checking, analysis engine, Model checking, Android applications, Android platform, JPF-Android, Libraries, Androids, property violations, deadlocks, verification]
A Security Practices Evaluation Framework
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software development teams need guidance on choosing security practices so they can develop code securely. The academic and practitioner literature on software development security practices is large, and expanding. However, published empirical evidence for security practice use in software development is limited and fragmented, making choosing appropriate practices difficult. Measurement frameworks offer a tool for collecting and comparing software engineering data. The goal of this work is to aid software practitioners in evaluating security practice use in the development process by defining and validating a measurement framework for software development security practice use and outcomes. We define the Security Practices Evaluation Framework (SP-EF), a measurement framework for software development security practices. SP-EF supports evidence-based practice selection. To enable comparison of practices across publications and projects, we define an ontology of software development security practices. We evaluate the framework and ontology on historical data and industrial projects.
[Context, software development security practices, software development process, software development teams, evidence-based practice selection, Measurement Frameworks, Process control, software development management, SP-EF, industrial projects, Ontologies, Size measurement, security practice evaluation framework, software quality, Security, software engineering data, security of data, Quality, Software Development Lifecycle., Software, Software measurement, software development security practice ontology, software metrics]
The Green Lab: Experimentation in Software Energy Efficiency
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software energy efficiency is a research topic where experimentation is widely adopted. Nevertheless, current studies and research approaches struggle to find generalizable findings that can be used to build a consistent knowledge base for energy-efficient software. To this end, we will discuss how to combine the traditional hypothesis-driven (top-down) approach with a bottom-up discovery approach. In this technical briefing, participants will learn the challenges that characterize the research in software energy efficiency. They will experience the complexity in this field and its implications for experimentation.
[Energy consumption, Conferences, Energy measurement, Complexity theory, software energy efficiency, Energy Efficiency, Empirical Methods, Software Engineering, energy conservation, hypothesis-driven approach, green laboratory, Software, software engineering, green computing, Software measurement, bottom-up discovery approach, Software engineering]
Software Requirements Patterns - A State of the Art and the Practice
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software requirement patterns are an increasingly popular approach to knowledge reuse in the requirements engineering phase. Several research proposals have been formulated in the last years, and this technical briefing presents them. Beyond that, a report on the current adoption of these proposals (or any other ad-hoc approach) in industry is presented. This state of the practice will show that the need to pave the road to successful adoption still persists.
[Industries, object-oriented programming, Software Requirements Patterns, knowledge reuse, Requirements Engineering, Knowledge Engineering, Proposals, formal specification, software requirement patterns, Requirements Reuse, Systematics, requirements engineering, Survey, formal verification, Literature Review, Bibliographies, systems analysis, Empirical Study, Software, Requirements engineering, Patterns, Software engineering]
Agile Project Management: From Self-Managing Teams to Large-Scale Development
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Agile software development represents a new approach for planning and managing software projects. It puts less emphasis on up-front plans and strict control and relies more on informal collaboration, coordination, and learning. This briefing provides a characterization and definition of agile project management based on extensive studies of large-scale industrial projects. It explains the circumstances behind the change from traditional management with its focus on direct supervision and standardization of work processes, to the newer, agile focus on self-managing teams, including its opportunities and benefits, but also its complexity and challenges. The main focus of the briefing is the four principles of agile project management: minimum critical specification, autonomous teams, redundancy, and feedback and learning. The briefing is intended for researchers, practitioners and educators in software engineering, especially project managers. For researchers, an updated state of the art will be uncovered, and the presentation will be based on current best evidence. For practitioners, principles, processes, and key success factors will be outlined and a successful large-scale case study of agile project management will be presented. For educators, the briefing will provide the basis for developing course material.
[Uncertainty, project management, Portfolio Management, software prototyping, Redundancy, Project management, software development management, self-managing teams, Self-Management, Complexity theory, Large-Scale, large-scale industrial project development, Project Management, agile project management, Software Engineering, software project planning, software projects management, Software, software engineering, Planning, agile software development, Agile Development]
Software Engineering for Privacy in-the-Large
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
There will be an estimated 35 zettabytes (35 &#x00D7; 1021) of digital records worldwide by the year 2020. This effectively amounts to privacy management on an ultra-large-scale. In this briefing, we discuss the privacy challenges posed by such an ultra-large-scale ecosystem - we term this &#x201C;Privacy in the Large&#x201D;. We will contrast existing approaches to privacy management, reflect on their strengths and limitations in this regard and outline key software engineering research and practice challenges to be addressed in the future.
[Privacy, Data privacy, Biological system modeling, data privacy, software engineering, Security, privacy management, Usability, ultra-large-scale ecosystem, privacy-in-the-large, Software engineering]
The Use of Text Retrieval and Natural Language Processing in Software Engineering
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This technical briefing presents the state of the art Text Retrieval and Natural Language Processing techniques used in Software Engineering and discusses their applications in the field.
[text analysis, Conferences, natural language processing, Committees, Tutorials, information retrieval, Information retrieval, text retrieval, Text retrieval, natural language processing techniques, Software, Natural language processing, software engineering, Software engineering]
Exploration, Analysis, and Manipulation of&#x00A0;&#x00A0;Source Code Using srcML
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This technology briefing is intended for those interested in constructing custom software analysis and manipulation tools to support research or commercial applications. srcML (srcML.org) is an infrastructure consisting of an XML representation for C/C++/C#/Java source code along with efficient parsing technology to convert source code to-and-from the srcML format. The briefing describes srcML, the toolkit, and the application of XPath and XSLT to query and modify source code. Additionally, a hands-on tutorial of how to use srcML and XML tools to construct custom analysis and manipulation tools will be conducted.
[source code (software), Conferences, srcML, static program analysis, program compilers, parsing technology, C-C++-C#-Java source code, source code exploration, custom analysis, Robustness, manipulation tools, custom software analysis, XPath, Java, program diagnostics, Tutorials, source code manipulation, XSLT, C++ language, XML representation, XML, Software, program transformation, source code analysis, Software engineering]
Reactive Programming: A Walkthrough
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Over the last few years, Reactive Programming has emerged as the trend to support the development of reactive software through dedicated programming abstractions. Reactive Programming has been increasingly investigated in the programming languages community and it is now gaining the interest of practitioners. Conversely, it has received so far less attention from the software engineering community. This technical briefing bridges this gap through an accurate overview of Reactive Programming, discussing the available frameworks and outlining open research challenges with an emphasis on cross-field research opportunities.
[Computer languages, programming abstraction, reactive programming, Observers, Software, software engineering, programming language, programming languages, reactive software development, Programming profession, Software engineering, Graphical user interfaces]
Load Testing Large-Scale Software Systems
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Large-scale software systems (e.g., Amazon and Dropbox) must be load tested to ensure that they can service thousands or millions of concurrent requests every day. In this technical briefing, we will describe the state of research and practices in the area of load testing. We will focus on the techniques used in the three phases of a load test: (1) designing a load test, (2) executing a load test, and (3) analyzing the results of a load test. This technical briefing is targeted at load testing practitioners and software engineering researchers interested in testing and analyzing the behavior of large-scale software systems.
[program testing, Conferences, software testing, large-scale software system load testing, scalability, Computer science, load testing, performance, Software systems, Dropbox, Amazon, Monitoring, Testing, Software engineering]
Big(ger) Data in Software Engineering
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
"Big Data" analytics has become the next hot topic for most companies - from financial institutions to technology companies to service providers. Likewise in software engineering, data collected about the development of software, the operation of the software in the field, and the users feedback on software have been used before. However, collecting and analyzing this information across hundreds of thousands or millions of software projects gives us the unique ability to reason about the ecosystem at large, and software in general. At no time in history has there been easier access to extremely powerful computational resources as it is today, thanks to the advances in cloud computing, both from the technology and business perspectives. In this technical briefing, we will present the state-of-the-art with respect to the research carried out in the area of big data analytics in software engineering research.
[data analysis, Conferences, Ecosystems, Companies, Big Data, Data mining, software project, Big Data analytics, Software Engineering, Software, Big data, software engineering, Software engineering]
The Art and Science of Analyzing Software Data; Quantitative Methods
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Using the tools of quantitative data science, software engineers that can predict useful information on new projects based on past projects. This tutorial reflects on the state-of-the-art in quantitative reasoning in this important field. This tutorial discusses the following: (a) when local data is scarce, we show how to adapt data from other organizations to local problems; (b) when working with data of dubious quality, we show how to prune spurious information; (c) when data or models seem too complex, we show how to simplify data mining results; (d) when the world changes, and old models need to be updated, we show how to handle those updates; (e) when the effect is too complex for one model, we show to how reason over ensembles.
[software engineers, Art, data analysis, data mining, Tutorials, quantitative methods, quantitative data science tools, software quality, software data analysis, Data mining, Computer science, Software, Data models, Software engineering]
10th International Workshop on Automation of Software Test (AST 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This paper is a report on The 10th IEEE/ACMInternational Workshop on Automation of Software Test (AST2015) at the 37th International Conference on Software Engineering(ICSE 2015). It sets a special theme on testing oracles.Keynote speeches and charette discussions are organized aroundthis special theme. 16 full research papers and 2 keynotes willbe presented in the two-day workshop. The report will give thebackground of the workshop and the selection of the specialtheme, and report on the organization of the workshop. Theprovisional program will be presented with a list of the sessionsand papers to be presented at the workshop.
[Software testing, Automation, Conferences, Software Testing, Test Oracle, Speech, Software, Automation of Software Test, Graphical user interfaces, Software Tools]
1st International Workshop on Big Data Software Engineering (BIGDSE 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Big Data is about extracting valuable information from data in order to use it in intelligent ways such as to revolutionize decision-making in businesses, science and society. BIGDSE 2015 discusses the link between Big Data and software engineering and critically looks into issues such as cost-benefit of big data.
[Conferences, Software systems, Market research, Big data, Data mining, Software engineering]
3rd International Workshop on Conducting Empirical Studies in Industry (CESI 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Few would deny today the importance of empirical studies in the field of Software Engineering (SE) and, indeed, an increasing number of studies are being conducted involving the software industry. While literature abounds on empirical procedures, relatively little is known about the dynamics and complexity of conducting empirical studies in the software industry. What are the impediments and how to best handle them? This driver underlies the organisation of the third in a series of workshops, CESI 2015. Apart from structured presentations and discussions from academic and industry participants, this workshop (like predecessor workshops) includes a "wall of ideas" session where all participants asynchronously post their ideas on the wall, literally, which are then analysed. As a tangible output, the workshop's discussions will be summarised in a post-workshop report.
[Industries, Context, software industry, Conferences, Empirical studies, Committees, Software, Australia, Software engineering]
8th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software is created for and with a wide range of stakeholders, from customers to management, from value-added providers to customer service personnel. These stakeholders work with teams of software engineers to develop and evolve software systems that support their activities. All of these people and their interactions are central to software development. Thus, it is crucial to investigate the dynamic and frequently changing Cooperative and Human Aspects of Software Engineering (CHASE), both before and after deployment, in order to understand current software practices, processes, and tools. In turn, this enables us to design tools and support mechanisms that improve software creation, software maintenance, and customer communication.Researchers and practitioners have long recognized the need to investigate these aspects, however, their articles are scattered across conferences and communities. This workshop will provide a unified forum for discussing high quality research studies, models, methods, and tools for human and cooperative aspects of software engineering. This will be the 8th in a series of workshops, which continue to be a meeting place for the academic, industrial, and practitioner communities interested in this area, and will give opportunities to present and discuss works-in-progress.
[Conferences, Committees, Software, Teamwork, Management, Stakeholders, Performance, Human Factors, Software engineering]
1st International Workshop on Complex faUlts and Failures in LargE Software Systems (COUFLESS 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
COUFLESS is a one-day workshop that starts with keynote speaker, Prof. Kishor S. Trivedi from Duke University, North Carolina, USA whose talk's title is titled: "Why Does Software Fail and What Should be Done About It?"&#x00A0;&#x00A0;&#x00A0;A total of 15 papers were submitted to COUFLESS with 53 authors from nine countries and each paper received at least three reviews by the 26 members of the program committee from 11 countries, making it a truly International Workshop. After a long discussion, 11 papers were accepted with the acceptance rate of 73%. Accepted papers address the issues of localizing and debugging complex faults in large-scale software applications.
[Databases, Conferences, failure, Debugging, Committees, Mandelbug, Software systems, debugging, Workshop]
2nd International Workshop on Context for Software Development (CSD 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The goal of this one-day workshop is to bring together researchers interested in techniques and tools that leverage context information that accumulates around development activities. Developers continuously make use of context to make decisions, coordinate their work, understand the purpose behind their tasks, and understand how their tasks fit with the rest of the project. However, there is little research on defining what context is, how we can model it, and how we can use those models to better support software development at large. This workshop brings together scholars interested in identifying, gathering and modelling context information in software development, as well as discussing its applications.
[Context, Computer science, Conferences, Committees, Software, Software engineering, Context modeling]
2nd International Workshop on Crowd Sourcing in Software Engineering (CSI-SE 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Crowdsourcing is increasingly revolutionizing the ways in which software is engineered. Programmers increasingly crowdsource answering their questions through Q&amp;amp;A sites. Non-programmers may contribute human-intelligence to development projects, by, for example, usability testing software or even play games with a purpose to implicitly construct formal specifications. Crowdfunding helps to democratize decisions about what software to build. Software engineering researchers may even benefit from new opportunities to evaluate their work with real developers by recruiting developers from the crowd. CSI- SE will inform the software engineering community of current techniques and trends in crowdsourcing, discuss the application of crowdsourcing to software engineering to date, and identify new opportunities to apply crowdsourcing to solve software engineering problems.
[Crowdsourcing, Conferences, Games, Usability, Software engineering, Testing]
3rd FME Workshop on Formal Methods in Software Engineering (FormaliSE 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Despite their significant advantages, formal methods are not widely used in industrial software development. Following the successful workshops we organized at ICSE 2103 in San Francisco, and ICSE 2014 in Hyderabad, we organize a third edition of the FormaliSE workshop with the main goal to promote the integration between the formal methods and the software engineering communities.
[Industries, Conferences, Collaboration, Committees, Software, Security, Formal methods, Software engineering]
4th International Workshop on Games and Software Engineering (GAS 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
We present a summary of the 4th ICSE Workshop on Games and Software Engineering. The full day workshop is planned to include a keynote speaker, game-jam demonstration session, and paper presentations on game software engineering topics related to software engineering education, frameworks for game development and infrastructure, quality assurance, and model-based game development. The accepted papers are overviewed here.
[Game engineering, Quality assurance, Conferences, Education, Games, Computer architecture, software engineering, Engines, Software engineering]
4th International Workshop on Green and Sustainable Software (GREENS 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Engineering green software-intensive systems is critical in our drive towards a sustainable, smarter planet. The goal of green software engineering is to apply green principles to the design and operation of software-intensive systems. Green and self-greening software systems have tremendous potential to decrease energy consumption. Moreover, enterprise software can and should be re-thought to address sustainability issues using innovative business models, processes, and incentives. Monitoring and measuring the greenness of software is critical towards the notion of sustainable and green software. Demonstrating improvement is paramount for users to achieve and affect change. Thus, the theme of GREENS 2015 is Towards a Green Software Body of Knowledge. The GREENS workshop series brings together researchers and practitioners to discuss both the state-of-the-art and state-of-the-practice in green software, including novel ideas, research challenges, methods, experiences, and tools to support the engineering of sustainable and energy efficient software systems.
[Industries, Energy consumption, green monitoring, smart green sensors and actuators, Conferences, green design, key green indicators (KGIs), green adaptation, sustainability, green IT, self-greening, Green products, energy efficiency, Software systems, Green software engineering, green computing, green scheduling, Software engineering]
4th SEMAT Workshop on General Theory of Software Engineering (GTSE 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
General theories explain the fundamental phenomena that constitute a research domain. They apply across a domain and often integrate many theories and concepts into a single cohesive view. While general theories are extremely important for education and research coordination, and common in many disciplines (e.g. sociology, criminology, electrical engineering, biology, physics), software engineering lacks a well-accepted general theory. The General Theory of Software Engineering workshop seeks to rectify this situation by promoting theory development in software engineering. The fourth workshop in this series, held in conjunction with the International Conference on Software Engineering, displayed a promising trend toward more theory development papers.
[questionnaire, Conferences, General Theory of Software Engineering, GTSE workshop, Complexity theory, process theory, case study, Jacobian matrices, field study, Software Engineering Method and Theory, Education, Research methodology, Software, Concrete, software engineering, SEMAT, Software engineering]
7th International Workshop on Modeling in Software Engineering (MiSE 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Models are an important tool in conquering the increasing complexity of modern software systems. Key industries are strategically directing their development environments towards more extensive use of modeling techniques. MiSE 2015 aimed to understand, through critical analysis, the current and future uses of models in the engineering of software-intensive systems. The MiSE workshop series has proven to be an effective forum for discussing modeling techniques from both the MDE and software engineering perspectives. An important goal of this workshop is to foster exchange between these two communities. In 2015 the focus was on considering the current state of tool support and the challenges that need to be addressed to improve the maturity of tools. There was also analysis of successful applications of modeling techniques in specific application domains, with attempts to determine how the participants' experiences can be carried over to other domains.
[Analytical models, Adaptation models, modeling, Conferences, Software, Electronic mail, software tools, Software engineering, quality]
7th International Workshop on Principles of Engineering Service-Oriented and Cloud Systems (PESOS 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
PESOS has established itself as a forum that brings together software engineering researchers and practitioners working in the areas of service-oriented systems to discuss research challenges, new developments and applications, as well as methods, techniques, experiences, and tools to support engineering, evolution and adaptation of service-oriented systems. The technical advances and growing adoption of Cloud computing is creating new challenges for the PESOS the software services community to explore the approaches to better engineer software systems that are designed, developed, operated and governed in the context of the Cloud. We again attracted high-quality submissions on a diverse set of relevant topics such as better approaches to engineering service-based collaborative systems, Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS) models of cloud computing and associated software quality attributes. PESOS 2015 will continue to be the key forum for collecting case studies and artifacts for educators and researchers in this area.
[Cloud computing, cloud services, service-oriented systems, Conferences, SOA, Software as a service, software services, Collaboration, collaborative services, Service-oriented architecture, cloud computing, service-oriented architecture, Software engineering]
5th International Workshop on Product LinE Approaches in Software Engineering PLE for a Sustainable Society (PLEASE 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This paper summarizes the motivation, objectives, and format of the 5th International Workshop on Product LinE Approaches in Software Engineering (PLEASE15). The main goal of the PLEASE workshop series is to encourage and promote the adoption of Software Product Line Engineering. This year's edition focuses on the link between software product line engineering (SPLE) and new challenges posed by emerging societal trends. Towards this end, we invited reports on (1) opportunities posed by societal challenges for SPLE research and practice and (2) concrete solutions exemplifying application of SPLE techniques to societal challenges.
[Conferences, Committees, Software, Concrete, Software product lines, Electronic mail]
4th International Workshop on Realizing AI Synergies in Software Engineering (RAISE 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This workshop is the fourth in the series and continued to build upon the work carried out at the previous iterations of the International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, which were held at ICSE in 2012, 2013 and 2014. RAISE 2015 brought together researchers and practitioners from the artificial intelligence (AI) and software engineering (SE) disciplines to build on the interdis- ciplinary synergies that exist and to stimulate further interaction across these disciplines. Mutually beneficial characteristics have appeared in the past few decades and are still evolving due to new challenges and technological advances. Hence, the question that motivates and drives the RAISE Workshop series is: "Are SE and AI researchers ignoring important insights from AI and SE?". To pursue this question, RAISE'15 explored not only the application of AI techniques to SE problems but also the application of SE techniques to AI problems. RAISE not only strengthens the AI- and-SE community but also continues to develop a roadmap of strategic research directions for AI and SE.
[Computer science, Conferences, Software, Electronic mail, Artificial intelligence, Robots, Software engineering]
2nd International Workshop on Rapid Continuous Software Engineering (RCoSE 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Continuous software engineering refers to the organizational&#x00A0;&#x00A0;capability to develop, release and learn from software in very short&#x00A0;&#x00A0;rapid cycles, typically hours, days or a very small numbers of&#x00A0;&#x00A0;weeks.&#x00A0;&#x00A0;This requires not only agile processes in teams but in the&#x00A0;&#x00A0;complete research and development organization. Additionally, the&#x00A0;&#x00A0;technology used in the different development phases, like&#x00A0;&#x00A0;requirements engineering and system integration, must support the&#x00A0;&#x00A0;quick development cycles. Finally, automatic live experimentation&#x00A0;&#x00A0;for different system alternatives enables fast gathering of required&#x00A0;&#x00A0;data for decision making. The workshop, the second in the series&#x00A0;&#x00A0;after the first one at ICSE 2014, aims to bring the research&#x00A0;&#x00A0;communities of the aforementioned areas together to exchange&#x00A0;&#x00A0;challenges, ideas, and solutions to bring software engineering a&#x00A0;&#x00A0;step further to being a holistic continuous process. The workshop&#x00A0;&#x00A0;program is based on eight papers selected in the peer-review process&#x00A0;&#x00A0;and supplemented by interaction and discussions at the workshop. The&#x00A0;&#x00A0;topics range from agile methods, continuous software engineering&#x00A0;&#x00A0;practices to specific techniques, like visualization and testing.
[Conferences, Companies, Software, Software engineering, Testing]
3rd International Workshop on Release Engineering (RELENG 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Release engineering deals with all activities inbetween regular development and actual usage of asoftware product by the end user, i.e., integration, build, testexecution, packaging and delivery of software. Although re-search on this topic goes back for decades, the increasing heterogeneity and variability of software products along withthe recent trend to reduce the release cycle to days or even hoursstarts to question some of the common beliefs and practicesof the field. For example, a project like Mozilla Firefox releasesevery 6 weeks, generating updates for dozens of existing Fire-fox versions on 5 desktop, 2 mobile and 3 mobile desktopplatforms, each of which for more than 80 locales. In this con-text, the International Workshop on Release Engineering(RELENG) aims to provide a highly interactive forum for re-searchers and practitioners to address the challenges of, findsolutions for and share experiences with release engineering, and to build connections between the various communities.
[Google, Conferences, Maintenance engineering, packaging, test execution, continuous delivery, Packaging, integration, Software, release engineering, Software engineering, Testing, build system, deployment]
2nd International Workshop on Requirements Engineering and Testing (RET 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The RET (Requirements Engineering and Testing) workshop provides a meeting point for researchers and practitioners from the two separate fields of Requirements Engineering (RE) and Testing. The goal is to improve the connection and alignment of these two areas through an exchange of ideas, challenges, practices, experiences and results. The long term aim is to build a community and a body of knowledge within the intersection of RE and Testing. One of the main outputs of the 1st workshop was a collaboratively constructed map of the area of RET showing the topics relevant to RET for these. The 2nd workshop will continue in the same interactive vein and include a keynote, paper presentations with ample time for discussions, and a group exercise. For true impact and relevance this cross-cutting area requires contribution from both RE and Testing, and from both researchers and practitioners. For that reason we welcome a range of paper contributions from short experience papers to full research papers that both clearly cover connections between the two fields.
[Industries, requirements engineering, Conferences, testing, Software, Requirements engineering, alignment, Joining processes, Testing, Software engineering]
Second International Workshop on Software Architecture and Metrics (SAM 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Software engineers and architects of complex software systems need to balance hard quality attribute requirements while at the same time manage risks and make decisions with a system-wide and long-lasting impact. To achieve these tasks efficiently, they need quantitative information about design-time and run-time system aspects through usable and quick tools. While there is body of work focusing on code quality and metrics, their applicability at the design and architecture level and at scale are inconsistent and not proven. We are interested in exploring whether architecture can assist with better contextualizing existing system and code quality and metrics approaches. Furthermore, we ask whether we need additional architecture-level metrics to make progress and whether something as complex and subtle as software architecture can be quantified. The goal of this workshop is to discuss progress, gather empirical evidence, and identify priorities for a research agenda on architecture and metrics in the software engineering field.
[Software architecture, Software architecture; metrics; software analytics; technical debt; software quality; software maintenance and evolution; empirical software engineering; qualitative methods, Conferences, Computer architecture, Software, Software measurement]
8th International Workshop on Search-Based Software Testing (SBST 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This paper is a report on the 8th International Workshop on Search-Based Software Testing at the 37th International Conference on Sofrware Engineering (ICSE). Search-Based Software Testing (SBST) is a form of Search-Based Software Engineering (SBSE) that optimizes testing through the use of computational search. SBST is used to generate test data, prioritize test cases, minimize test suites, reduce human oracle cost, verify software models, test service-orientated architectures, construct test suites for interaction testing, and validate real time properties. The objectives of this workshop are to bring together researchers and industrial practitioners from SBST and the wider software engineering community to share experience and provide directions for future research, and to encourage the use of search techniques to combine aspects of testing with other aspects of the software engineering lifecycle.Three full research papers, three short papers, and threeposition papers will be presented in the two-day workshop. Additionally, six development groups have pitted their test generation tools against a common set of programs and benchmarks, and will present their techniques and results. This report will give the background of the workshop and detail the provisional program.
[Software testing, Measurement, Conferences, Search problems, Software, Software engineering]
SE4HPCS'15: The 2015 International Workshop on Software Engineering for High Performance Computing in Science
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
HPC software is developed and used in a wide variety of scientific domains including nuclear physics, computational chemistry, crash simulation, satellite data processing, fluid dynamics, climate modeling, bioinformatics, and vehicle development. The increase in the importance of this software motivates the need to identify and understand appropriate software engineering (SE) practices for HPC architectures. Because of the variety of the scientific domains addressed using HPC, existing SE tools and techniques developed for the business/IT community are often not efficient or effective. Appropriate SE solutions must account for the salient characteristics of the HPC, research oriented development environment. This situation creates a need for members of the SE community to interact with members of the scientific and HPC communities to address this need. This workshop facilitates that collaboration by bringing together members of the SE, the scientific, and the HPC communities to share perspectives and present findings relevant to research, practice, and education. A significant portion of the workshop is devoted to focused interaction among the participants with the goal of generating a research agenda to improve tools, techniques, and experimental methods regarding SE for HPC science.
[Scientific computing, High Performance Computing, Conferences, Computational modeling, High performance computing, Software Engineering, Computer architecture, Computational Engineering, Computational Science, Software, Software engineering]
2nd International Workshop on Software Engineering Methods in Spreadsheets (SEMS 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Spreadsheets are heavily used in industry, becausethey are easily written and adjusted, using an intuitive visual interface. They often start out as simple tools; however, over time spreadsheets can become increasingly complex, up to the point where they become complicated and inflexible. In many ways, spreadsheet are similar to software: both concern the storage and manipulation of data and the presentation of results to the user. Because of this similarity, many methods and techniques from software engineering can be applied to spreadsheets. The role of SEMS, the International Workshop on Software Engineering Methods in Spreadsheets is to explore the possibilities of applying successful methods from software engineering to spreadsheets. Some, like testing and visualization, have been tried before and can be built upon. For methods that have not yet been tried on spreadsheets, SEMS will serve as a platform for early feedback. The SEMS program included an industrial keynote, "spreadsheet stories" (success or failure), short and long research papers,a good mix of industrial and academic researchers, as well as lively discussion and debate.
[Industries, Conferences, Committees, Software, Electronic mail, Software engineering, Testing]
2nd International Workshop on Software Engineering Research and Industrial Practice (SER&amp;amp;IP 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Differing perceptions and expectations are obstaclesto collaboration between software engineering (SE) researchersand practitioners: Researchers often have a view thatpractitioners are reluctant to share real data. Practitionersbelieve that researchers are mostly working on topics which aredivorced from real industrial needs. Researchers believe thatpractitioners are looking for quick fixes. Practitioners have aview that case studies in research do not represent thecomplexities of real projects. Researchers may expect a few yearsto do research on a problem whereas practitioners expect a quicksolution that pays off immediately.Researchers and practitioners need to identify the gaps and todiscover the ways to collaborate to strengthen SE research andindustrial practice (IP). The main purpose of this workshop is tobring together researchers and practitioners to discuss thecurrent state of SE research and IP and to enhance collaborationbetween them. The SER&amp;amp;IP 2015 workshop provided a platformto share success stories of SE research-practice partnerships aswell as to discuss the challenges, through a day-long agenda ofkeynotes, paper presentations and round table discussions.
[Industries, Conferences, industrial practice, researchers, challenges, Software engineering research, practitioners, Collaboration, collaboration, Software, Australia, Software engineering, Business]
1st International Workshop on Software Engineering for Smart Cyber-Physical Systems (SEsCPS 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Cyber-physical system (CPS) have been recognized as a top-priority in research and development. The innovations sought for CPS demand them to deal effectively with dynamicity of their environment, to be scalable, adaptive, tolerant to threats, etc. -- i.e. they have to be smart. Although approaches in software engineering (SE) exist that individually meet these demands, their synergy to address the challenges of smart CPS (sCPS) in a holistic manner remains an open challenge. The workshop focuses on software engineering challenges for sCPS. The goals are to increase the understanding of problems of SE for sCPS, study foundational principles for engineering sCPS, and identify promising SE solutions for sCPS. Based on these goals, the workshop aims to formulate a research agenda for SE of sCPS.
[Adaptation models, Technological innovation, Conferences, Committees, Cyber-physical systems, Software, Software engineering]
3rd International Workshop on Software Engineering for Systems-of-Systems (SESoS 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Systems-of-Systems (SoS) refer to a new class of software-intensive systems, where their constituent systems work cooperatively in order to fulfill specific missions. Characterized by managerial and operational independence, geographic distribution, evolutionary development, and emergent behavior, SoS bring substantial challenges to the software engineering area. SESoS 2015, held in Florence, Italy, on May 17, 2015, as a joint workshop of the 37th International Conference on Software Engineering (ICSE), provided a forum to exchange ideas and experiences, analyze current research and development issues, discuss promising solutions, and to explore inspiring visions for the future of Software Engineering (SE) for SoS.
[Ultra-large Scale Systems, Conferences, System-of-Systems, Software Engineering, Committees, Software, Electronic mail, Vehicle dynamics, Research and development, Software engineering]
1st International Workshop on Software Protection (SPRO 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
There are many reasons to protect software: your banking app needs to be protected to prevent fraud; software operating on critical infrastructures needs to be protected against vulnerability discovery; software vendors and service companies need it to protect their business; etc. In the past decade, many techniques to protect software have been presented and broken. Beyond making individual techniques better, the challenge includes to be able to deploy them in practice and be able to evaluate them. This is the objective of SPRO, the first International Workshop on Software Protection: to bring together researchers and industrial practitioners both from software protection and the wider software engineering community to share experience and provide directions for future research, in order to stimulate the use of software engineering techniques in novel aspects of software protection. This first edition of the workshop is held at ICSE 2015 in Florence (Italy) with the aim of creating a community working in this new growing area of security, and to highlight its synergies with different research fields of software engineering, like: formal models, program analysis, reverse engineering, code transformations, empirical evaluation, and software metrics. This paper presents the research themes and challenges of the workshop, describes the workshop organization, and summarizes the research papers.
[Analytical models, Software protection, Conferences, Obfuscation, Reverse engineering, security metrics, security modelling, Software, Security, Virtualization, Information Hiding, Software engineering]
1st International Workshop on TEchnical and LEgal aspects of data pRIvacy and Security (TELERISE 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
This paper is the report on the 1st International Workshop on TEchnical and LEgal aspects of data pRIvacy and SEcurity (TELERISE 2015) at the 37th International Conference on Software Engineering (ICSE 2015). TELERISE investigates privacy and security issues in data sharing from a technical and legal perspective. Keynote speech as well as selected papers presented at the event fit the topics of the workshop. This report gives the rationale of TELERISE and it provides a provisional program.
[Data privacy, Privacy, Law, Conferences, Organizations, Security]
5th International Workshop on the Twin Peaks of Requirements and Architecture (TwinPeaks 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
The relationships and interdependencies between software requirements and the architectures of software-intensive systems are described in the Twin Peaks model. The fundamental idea of the Twin Peaks model is that Requirements Engineering and Software Architecture should not be treated in isolation. Instead, we need to progressively discover and specify requirements while concurrently exploring alternative architectural solutions. However, bridging the gap between Requirements Engineering and Software Architecture has mainly been discussed independently in the respective communities. Therefore, this ICSE workshop aims at bringing together researchers, practitioners and educators from the Requirements Engineering and Software Architecture fields to jointly explore the strong interdependencies between requirements and architecture. Based on the results from previous editions of the workshop, this edition focuses on agile software development contexts and on exploring lightweight techniques for integrating requirements and architectural thinking.
[Context, software architecture, requirements engineering, Software architecture, Conferences, Computer architecture, twin peaks, Software, agile, Requirements engineering]
Workshop on Applications of Human Error Research to Improve Software Engineering (WAHESE 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Advances in the psychological understanding of the origins and manifestations of human error have led to tremendous reductions in errors in fields such as medicine, aviation, and nuclear power plants. This workshop is intended to foster a better understanding of software engineering errors and how a psychological perspective can reduce them, improving software quality and reducing maintenance costs. The workshop goal is to develop a body of knowledge that can advance our understanding of the psychological processes (of human reasoning, planning, and problem solving) and how they fail during the software development. Applying human error research to software quality improvement will provide insights to the cognitive aspects of software development. The workshop will include interactive session to discuss common themes of errors in different fields, and structure software error information to detect and prevent software errors during the development.
[Conferences, Taxonomy, Psychology, Software quality, Software engineering, Accidents]
6th International Workshop on Emerging Trends in Software Metrics (WETSoM 2015)
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
WETSoM is a gathering of researchers and practitioners to discuss the progress on software metrics knowledge. Motivations for this workshop include the low impact that software metrics have on current software development and the increased interest in research. The goals of this workshop include critically examining the evidence for the effectiveness of existing metrics and identifying new directions for metrics. Evidence for existing metrics includes how the metrics have been used in practice and studies showing their effectiveness. Identifying new directions includes use of new theories, such as complex network theory, on which to base metrics.
[Software metrics, Conferences, Complex networks, Software Metrics, Market research, Software, Software Quality, Software engineering]
[Publisher's information - Vol 2]
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
None
2015
Provides a listing of current committee members and society officers.
[]
Welcome Message from the Chairs
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Provides a listing of current committee members and society officers.
[]
Program Board
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Provides a listing of current committee members and society officers.
[]
Program Committee
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Provides a listing of current committee members and society officers.
[]
Web Committee
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Provides a listing of current committee members and society officers.
[]
Publicity Committee
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Provides a listing of current committee members and society officers.
[]
Publications Committee
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Provides a listing of current committee members and society officers.
[]
Workshops Committee
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Provides a listing of current committee members and society officers.
[]
PRADA: Prioritizing Android Devices for Apps by Mining Large-Scale Usage Data
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Selecting and prioritizing major device models are critical for mobile app developers to select testbeds and optimize resources such as marketing and quality-assurance resources. The heavily fragmented distribution of Android devices makes it challenging to select a few major device models out of thousands of models available on the market. Currently app developers usually rely on some reported or estimated general market share of device models. However, these estimates can be quite inaccurate, and more problematically, can be irrelevant to the particular app under consideration. To address this issue, we propose PRADA, the first approach to prioritizing Android device models for individual apps, based on mining large-scale usage data. PRADA adapts the concept of operational profiling (popularly used in software reliability engineering) for mobile apps - the usage of an app on a specific device model reflects the importance of that device model for the app. PRADA includes a collaborative filtering technique to predict the usage of an app on different device models, even if the app is entirely new (without its actual usage in the market yet), based on the usage data of a large collection of apps. We empirically demonstrate the effectiveness of PRADA over two popular app categories, i.e., Game and Media, covering over 3.86 million users and 14,000 device models collected through a leading Android management app in China.
[collaborative filtering, mobile apps, Android management app, usage data, Humanoid robots, data mining, Mobile communication, smart phones, Mobile apps, Data mining, Android fragmentation, prioritization, prioritizing android devices for apps, PRADA, collaborative filtering technique, Games, Data models, large-scale usage data mining, Androids, Testing, operational profiling]
Release Planning of Mobile Apps Based on User Reviews
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Developers have to to constantly improve their apps by fixing critical bugs and implementing the most desired features in order to gain shares in the continuously increasing and competitive market of mobile apps. A precious source of information to plan such activities is represented by reviews left by users on the app store. However, in order to exploit such information developers need to manually analyze such reviews. This is something not doable if, as frequently happens, the app receives hundreds of reviews per day. In this paper we introduce CLAP (Crowd Listener for releAse Planning), a thorough solution to (i) categorize user reviews based on the information they carry out (e.g., bug reporting), (ii) cluster together related reviews (e.g., all reviews reporting the same bug), and (iii) automatically prioritize the clusters of reviews to be implemented when planning the subsequent app release. We evaluated all the steps behind CLAP, showing its high accuracy in categorizing and clustering reviews and the meaningfulness of the recommended prioritizations. Also, given the availability of CLAP as a working tool, we assessed its practical applicability in industrial environments.
[Machine learning algorithms, Merging, CLAP, Mobile communication, Thesauri, software maintenance, mobile computing, user review, app store, crowd listener for release planning, Computer bugs, mobile applications, information source, Software, Planning, release planning]
Mining Sandboxes
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
We present sandbox mining, a technique to confine an application to resources accessed during automatic testing. Sandbox mining first explores software behavior by means of automatic test generation, and extracts the set of resources accessed during these tests. This set is then used as a sandbox, blocking access to resources not used during testing. The mined sandbox thus protects against behavior changes such as the activation of latent malware, infections, targeted attacks, or malicious updates. The use of test generation makes sandbox mining a fully automatic process that can be run by vendors and end users alike. Our BOXMATE prototype requires less than one hour to extract a sandbox from an Android app, with few to no confirmations required for frequently used functionality.
[policy enforcement, program testing, Humanoid robots, data mining, android, automatic test generation, Generators, dynamic analysis, specification mining, test generation, sandboxing, sandbox mining technique, security, resource extraction, BOXMATE, program analysis, Androids, Android application, Monitoring, Smart phones, Testing, Graphical user interfaces]
Generating Performance Distributions via Probabilistic Symbolic Execution
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Analyzing performance and understanding the potential best-case, worst-case and distribution of program execution times are very important software engineering tasks. There have been model-based and program analysis-based approaches for performance analysis. Model-based approaches rely on analytical or design models derived from mathematical theories or software architecture abstraction, which are typically coarse-grained and could be imprecise. Program analysis-based approaches collect program profiles to identify performance bottlenecks, which often fail to capture the overall program performance. In this paper, we propose a performance analysis framework PerfPlotter. It takes the program source code and usage profile as inputs and generates a performance distribution that captures the input probability distribution over execution times for the program. It heuristically explores high-probability and low-probability paths through probabilistic symbolic execution. Once a path is explored, it generates and runs a set of test inputs to model the performance of the path. Finally, it constructs the performance distribution for the program. We have implemented PerfPlotter based on the Symbolic PathFinder infrastructure, and experimentally demonstrated that PerfPlotter could accurately capture the best-case, worst-case and distribution of program execution times. We also show that performance distributions can be applied to various important tasks such as performance understanding, bug validation, and algorithm selection.
[Algorithm design and analysis, software architecture abstraction, Symbolic Execution, Computational modeling, program diagnostics, PerfPlotter framework, bug validation, Probabilistic logic, model-based analysis approach, Analytical models, software architecture, probabilistic symbolic execution, performance distribution generation, program analysis-based approach, algorithm selection, performance understanding, Symbolic PathFinder infrastructure, probability distribution, software engineering, Performance analysis, Mathematical model, Performance Analysis, Software engineering]
Performance Issues and Optimizations in JavaScript: An Empirical Study
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
As JavaScript is becoming increasingly popular, the performance of JavaScript programs is crucial to ensure the responsiveness and energy-efficiency of thousands of pro- grams. Yet, little is known about performance issues that developers face in practice and they address these issues. This paper presents an empirical study of 98 fixed performance issues from 16 popular client-side and server-side JavaScript projects. We identify eight root causes of issues and show that inefficient usage of APIs is the most prevalent root cause. Furthermore, we find that most is- sues are addressed by optimizations that modify only a few lines of code, without significantly affecting the complexity of the source code. By studying the performance impact of optimizations on several versions of the SpiderMonkey and V8 engines, we find that only 42.68% of all optimizations improve performance consistently across all versions of both engines. Finally, we observe that many optimizations are instances of patterns applicable across projects, as evidenced by 139 previously unknown optimization opportunities that we find based on the patterns identified during the study. The results of the study help application developers to avoid common mistakes, researchers to develop performance-related techniques that address relevant problems, and engine developers to address prevalent bottleneck patterns.
[Java, V8 engines, JavaScript performance, SpiderMonkey, Servers, JavaScript programs, Optimization, Engines, Computer science, Computer bugs, Empirical study, Performance issue, JavaScript, Libraries, API, Reliability, software performance evaluation]
Reliability of Run-Time Quality-of-Service Evaluation Using Parametric Model Checking
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Run-time Quality-of-Service (QoS) assurance is crucial for business-critical systems. Complex behavioral performance metrics (PMs) are useful but often difficult to monitor or measure. Probabilistic model checking, especially parametric model checking, can support the computation of aggre- gate functions for a broad range of those PMs. In practice, those PMs may be defined with parameters determined by run-time data. In this paper, we address the reliability of QoS evaluation using parametric model checking. Due to the imprecision with the instantiation of parameters, an evaluation outcome may mislead the judgment about requirement violations. Based on a general assumption of run-time data distribution, we present a novel framework that contains light-weight statistical inference methods to analyze the re- liability of a parametric model checking output with respect to an intuitive criterion. We also present case studies in which we test the stability and accuracy of our inference methods and describe an application of our framework to a cloud server management problem.
[probabilistic model checking, Quality-of-Service, Unified modeling language, Quality of service, run-time data distribution, cloud server management, reliability, formal verification, run-time quality-of-service evaluation reliability, Model checking, lightweight statistical inference method, Data distribution, run-time quality-of-service assurance, cloud computing, run-time evaluation, parametric model checking, complex behavioral performance metrics, probability, Probabilistic logic, Parametric statistics, quality of service, QoS evaluation reliability, business-critical systems, requirement violations, Random variables, Reliability, business data processing]
Optimizing Selection of Competing Services with Probabilistic Hierarchical Refinement
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Recently, many large enterprises (e.g., Netflix, Amazon) have decomposed their monolithic application into services, and composed them to fulfill their business functionalities. Many hosting services on the cloud, with different Quality of Service (QoS) (e.g., availability, cost), can be used to host the services. This is an example of competing services. QoS is crucial for the satisfaction of users. It is important to choose a set of services that maximize the overall QoS, and satisfy all QoS requirements for the service composition. This problem, known as optimal service selection, is NP-hard. Therefore, an effective method for reducing the search space and guiding the search process is highly desirable. To this end, we introduce a novel technique, called Probabilistic Hierarchical Refinement (PROHR). PROHR effectively reduces the search space by removing competing services that cannot be part of the selection. PROHR provides two methods, probabilistic ranking and hierarchical refinement, that enable smart exploration of the reduced search space. Unlike existing approaches that perform poorly when QoS requirements become stricter, PROHR maintains high performance and accuracy, independent of the strictness of the QoS requirements. PROHR has been evaluated on a publicly available dataset, and has shown significant improvement over existing approaches.
[Service Composition, probability, Quality of service, Probabilistic logic, hierarchical refinement technique, Mixed Integer Programming, quality of service, Automobiles, business functionalities, PROHR technique, hosting services, probabilistic ranking technique, Service Selection, Web services, optimal service selection problem, NP-hard problem, QoS, Concrete, Time factors, cloud computing, probabilistic hierarchical refinement, computational complexity, user satisfaction]
The Emerging Role of Data Scientists on Software Development Teams
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.
[software development process, data scientists, software development teams, polymaths, Software Analytics, software development management, Companies, Data science, software-oriented data analytics, software creation, insight providers, modeling specialists, team working, customer usage, Data Science, team leaders, Data Scientist, platform builders, Software, Interviews, software engineering skills, Software engineering]
Belief &amp; Evidence in Empirical Software Engineering
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Empirical software engineering has produced a steady stream of evidence-based results concerning the factors that affect important outcomes such as cost, quality, and interval. However, programmers often also have strongly-held a priori opinions about these issues. These opinions are important, since developers are highlytrained professionals whose beliefs would doubtless affect their practice. As in evidence-based medicine, disseminating empirical findings to developers is a key step in ensuring that the findings impact practice. In this paper, we describe a case study, on the prior beliefs of developers at Microsoft, and the relationship of these beliefs to actual empirical data on the projects in which these developers work. Our findings are that a) programmers do indeed have very strong beliefs on certain topics b) their beliefs are primarily formed based on personal experience, rather than on findings in empirical research and c) beliefs can vary with each project, but do not necessarily correspond with actual evidence in that project. Our findings suggest that more effort should be taken to disseminate empirical findings to developers and that more in-depth study the interplay of belief and evidence in software practice is needed.
[Practitioner Belief, Media, empirical research, Empirical Evidence, Computer science, empirical software engineering, Software, software engineering, Microsoft, Bayes methods, software practice, Empirical Software Engineering., Software engineering, Immune system]
Grounded Theory in Software Engineering Research: A Critical Review and Guidelines
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Grounded Theory (GT) has proved an extremely useful research approach in several fields including medical sociology, nursing, education and management theory. However, GT is a complex method based on an inductive paradigm that is fundamentally different from the traditional hypothetico-deductive research model. As there are at least three variants of GT, some ostensibly GT research suffers from method slurring, where researchers adopt an arbitrary subset of GT practices that are not recognizable as GT. In this paper, we describe the variants of GT and identify the core set of GT practices. We then analyze the use of grounded theory in software engineering. We carefully and systematically selected 98 articles that mention GT, of which 52 explicitly claim to use GT, with the other 46 using GT techniques only. Only 16 articles provide detailed accounts of their research procedures. We offer guidelines to improve the quality of both conducting and reporting GT studies. The latter is an important extension since current GT guidelines in software engineering do not cover the reporting process, despite good reporting being necessary for evaluating a study and informing subsequent research.
[software engineering research, grounded theory, Encoding, Guidelines, Sorting, Computer science, GT practices, Grounded theory, review, guidelines, Software, software engineering, Interviews, Software engineering]
On the Techniques We Create, the Tools We Build, and Their Misalignments: A Study of KLEE
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Our community constantly pushes the state-of-the-art by introducing &#x201C;new&#x201D; techniques. These techniques often build on top of, and are compared against, existing systems that realize previously published techniques. The underlying assumption is that existing systems correctly represent the techniques they implement. This pa- per examines that assumption through a study of KLEE, a popular and well-cited tool in our community. We briefly describe six improvements we made to KLEE, none of which can be considered &#x201C;new&#x201D; techniques, that provide order-of-magnitude performance gains. Given these improvements, we then investigate how the results and conclusions of a sample of papers that cite KLEE are affected. Our findings indicate that the strong emphasis on introducing &#x201C;new&#x201D; techniques may lead to wasted effort, missed opportunities for progress, an accretion of artifact complexity, and questionable research conclusions (in our study, 27% of the papers that depend on KLEE can be questioned). We conclude by revisiting initiatives that may help to realign the incentives to better support the foundations on which we build.
[replication, Engineering profession, artifact complexity, Buildings, Research incentives, software techniques, Complexity theory, performance gain, Optimization, Computer bugs, research tools and infrastructure, Software, software engineering, KLEE tool, Software engineering]
Guiding Dynamic Symbolic Execution toward Unverified Program Executions
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Most techniques to detect program errors, such as testing, code reviews, and static program analysis, do not fully verify all possible executions of a program. They leave executions unverified when they do not check certain properties, fail to verify properties, or check properties under certain unsound assumptions such as the absence of arithmetic overflow. In this paper, we present a technique to complement partial verification results by automatic test case generation. In contrast to existing work, our technique supports the common case that the verification results are based on unsound assumptions. We annotate programs to reflect which executions have been verified, and under which assumptions. These annotations are then used to guide dynamic symbolic execution toward unverified program executions. Our main technical contribution is a code instrumentation that causes dynamic symbolic execution to abort tests that lead to verified executions, to prune parts of the search space, and to prioritize tests that cover more properties that are not fully verified. We have implemented our technique for the .NET static analyzer Clousot and the dynamic symbolic execution tool Pex. It produces smaller test suites (by up to 19.2%), covers more unverified executions (by up to 7.1%), and reduces testing time (by up to 52.4%) compared to combining Clousot and Pex without our technique.
[partial verification, unverified program execution, program testing, program verification, Instruments, Conferences, Redundancy, Pex tool, testing, Aerospace electronics, program execution, static analysis, Clousot, code instrumentation, program error detection, arithmetic overflow, dynamic symbolic execution, automatic test case generation, Performance analysis, Testing, Software engineering]
Synthesizing Framework Models for Symbolic Execution
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Symbolic execution is a powerful program analysis technique, but it is difficult to apply to programs built using frameworks such as Swing and Android, because the framework code itself is hard to symbolically execute. The standard solution is to manually create a framework model that can be symbolically executed, but developing and maintaining a model is difficult and error-prone. In this paper, we present Pasket, a new system that takes a first step toward automatically generating Java framework models to support symbolic execution. Pasket's focus is on creating models by instantiating design patterns. Pasket takes as input class, method, and type information from the framework API, together with tutorial programs that exercise the framework. From these artifacts and Pasket's internal knowledge of design patterns, Pasket synthesizes a framework model whose behavior on the tutorial programs matches that of the original framework. We evaluated Pasket by synthesizing models for subsets of Swing and Android. Our results show that the models derived by Pasket are sufficient to allow us to use off-the-shelf symbolic execution tools to analyze Java programs that rely on frameworks.
[Java, object-oriented programming, Symbolic Execution, Synthesizers, program diagnostics, Framework Model, Humanoid robots, PASKET, Tutorials, Observers, Java framework models, design pattern instantiation, Android, Analytical models, program analysis, Swing, symbolic execution, Sketch, API, Androids, Program Synthesis]
Type-Aware Concolic Testing of JavaScript Programs
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Conventional concolic testing has been used to provide high coverage of paths in statically typed languages. While it has also been applied in the context of JavaScript (JS) programs, we observe that applying concolic testing to dynamically-typed JS programs involves tackling unique problems to ensure scalability. In particular, a naive type-agnostic extension of concolic testing to JS programs causes generation of large number of inputs. Consequently, many executions operate on undefined values and repeatedly explore same paths resulting in redundant tests, thus diminishing the scalability of testing drastically. In this paper, we address this problem by proposing a simple yet effective approach that incorporates type-awareness intelligently in conventional concolic testing to reduce the number of generated inputs for JS programs. We extend our approach inter-procedurally by generating preconditions for each function that provide a summary of the relation between the variable types and paths. Employing the function preconditions when testing reduces the number of inputs generated even further. We implement our ideas and validate it on a number of open-source JS programs (and libraries). For a significant percentage (on average 50%) of the functions, we observe that type-aware concolic testing generates a minuscule percentage (less than 5%) of the inputs as compared to conventional concolic testing approach implemented on top of Jalangi. On average, this approach achieves over 97% of line coverage and over 94% of branch coverage for all the functions across all benchmarks. Moreover, the use of function preconditions reduces the number of inputs generated by 50%. We also demonstrate the use of function preconditions in automatically avoiding real crashes due to incorrectly typed objects.
[Algorithm design and analysis, Java, object-oriented programming, program testing, Scalability, Redundancy, open-source JS programs, dynamically-typed JS programs, Computer crashes, Dynamic Analysis, JavaScript programs, Runtime, JavaScript, type-aware concolic testing, Libraries, Testing]
An Empirical Comparison of Compiler Testing Techniques
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Compilers, as one of the most important infrastructure of today's digital world, are expected to be trustworthy. Different testing techniques are developed for testing compilers automatically. However, it is unknown so far how these testing techniques compared to each other in terms of testing effectiveness: how many bugs a testing technique can find within a time limit. In this paper, we conduct a systematic and comprehensive empirical comparison of three compiler testing techniques, namely, Randomized Differential Testing (RDT), a variant of RDT-Different Optimization Levels (DOL), and Equivalence Modulo Inputs (EMI). Our results show that DOL is more effective at detecting bugs related to optimization, whereas RDT is more effective at detecting other types of bugs, and the three techniques can complement each other to a certain degree. Furthermore, in order to understand why their effectiveness differs, we investigate three factors that influence the effectiveness of compiler testing, namely, efficiency, strength of test oracles, and effectiveness of generated test programs. The results indicate that all the three factors are statistically significant, and efficiency has the most significant impact.
[DOL, EMI, program testing, RDT, test oracles, testing effectiveness, different optimization levels, bug detection, History, test program generation, program compilers, Optimization, Program processors, Computer bugs, Electromagnetic interference, randomized differential testing, equivalence modulo inputs, compiler testing techniques, Testing]
Termination-Checking for LLVM Peephole Optimizations
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Mainstream compilers contain a large number of peephole optimizations, which perform algebraic simplification of the input program with local rewriting of the code. These optimizations are a persistent source of bugs. Our recent research on Alive, a domain-specific language for expressing peephole optimizations in LLVM, addresses a part of the problem by automatically verifying the correctness of these optimizations and generating C++ code for use with LLVM. This paper identifies a class of non-termination bugs that arise when a suite of peephole optimizations is executed until a fixed point. An optimization can undo the effect of another optimization in the suite, which results in non-terminating compilation. This paper (1) proposes a methodology to detect non-termination bugs with a suite of peephole optimizations, (2) identifies the necessary condition to ensure termination while composing peephole optimizations, and (3) provides debugging support by generating concrete input programs that cause non-terminating compilation. We have discovered 184 optimization sequences, involving 38 optimizations, that cause non-terminating compilation in LLVM with Alive-generated C++ code.
[Alive-generated C++ code, program debugging, nonterminating compilation, Peephole Optimization, C++ languages, input programs, Termination, C++ language, Alive, program compilers, Optimization, termination-checking, nontermination bugs, Toxicology, Compiler Verification, Computer bugs, Semantics, debugging, Concrete, Pattern matching, LLVM peephole optimizations]
Finding and Analyzing Compiler Warning Defects
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Good compiler diagnostic warnings facilitate software development as they indicate likely programming mistakes or code smells. However, due to compiler bugs, the warnings may be erroneous, superfluous or missing, even for mature production compilers like GCC and Clang. In this paper, we (1) propose the first randomized differential testing technique to detect compiler warning defects and (2) describe our extensive evaluation in finding warning defects in widely-used C compilers.At the high level, our technique starts with generating random programs to trigger compilers to emit a variety of compiler warnings, aligns the warnings from different compilers, and identifies inconsistencies as potential bugs. We develop effective techniques to overcome three specific challenges: (1) How to generate random programs, (2) how to align textual warnings, and (3) how to reduce test programs for bug reporting?Our technique is very effective - we have found and reported 60 bugs for GCC (38 confirmed, assigned or fixed) and 39 for Clang (14 confirmed or fixed). This case study not only demonstrates our technique's effectiveness, but also highlights the need to continue improving compilers' warning support, an essential, but rather neglected aspect of compilers.
[compiler warning defects, textual warning alignment, program debugging, GCC, Maintenance engineering, Security, program compilers, random programs, bugs, Program processors, Clang, Computer bugs, C compilers, Production, compiler warnings, differential testing, compiler testing, randomized differential testing technique, Testing]
iDice: Problem Identification for Emerging Issues
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
One challenge for maintaining a large-scale software system, especially an online service system, is to quickly respond to customer issues. The issue reports typically have many categorical attributes that reflect the characteristics of the issues. For a commercial system, most of the time the volume of reported issues is relatively constant. Sometimes, there are emerging issues that lead to significant volume increase. It is important for support engineers to efficiently and effectively identify and resolve such emerging issues, since they have impacted a large number of customers. Currently, problem identification for an emerging issue is a tedious and error-prone process, because it requires support engineers to manually identify a particular attribute combination that characterizes the emerging issue among a large number of attribute combinations. We call such an attribute combination effective combination, which is important for issue isolation and diagnosis. In this paper, we propose iDice, an approach that can identify the effective combination for an emerging issue with high quality and performance. We evaluate the effectiveness and efficiency of iDice through experiments. We have also successfully applied iDice to several Microsoft online service systems in production. The results confirm that iDice can help identify emerging issues and reduce maintenance effort.
[issue reports, Time series analysis, attribute combination effective combination, problem diagnostic, Manuals, Maintenance engineering, Emerging issues, effective combination, software maintenance, large-scale software system, categorical attributes, software system maintenance, Microsoft online service systems, problem identification, iDice, Software systems, Market research, Software engineering]
Energy Profiles of Java Collections Classes
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
We created detailed profiles of the energy consumed by common operations done on Java List, Map, and Set abstractions. The results show that the alternative data types for these abstractions differ significantly in terms of energy consumption depending on the operations. For example, an ArrayList consumes less energy than a LinkedList if items are inserted at the middle or at the end, but consumes more energy than a LinkedList if items are inserted at the start of the list. To explain the results, we explored the memory usage and the bytecode executed during an operation. Expensive computation tasks in the analyzed bytecode traces appeared to have an energy impact, but memory usage did not contribute. We evaluated our profiles by using them to selectively replace Collections types used in six applications and libraries. We found that choosing the wrong Collections type, as indicated by our profiles, can cost even 300% more energy than the most efficient choice. Our work shows that the usage context of a data structure and our measured energy profiles can be used to decide between alternative Collections implementations.
[Energy consumption, Java, Energy Profile, Java collections classes, Energy measurement, ArrayList, Encoding, LinkedList, Semiconductor device measurement, Set abstractions, energy profiles, Software, Java List, API, Collections, Software engineering]
An Empirical Study of Practitioners' Perspectives on Green Software Engineering
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
The energy consumption of software is an increasing concern as the use of mobile applications, embedded systems, and data center-based services expands. While research in green software engineering is correspondingly increasing, little is known about the current practices and perspectives of software engineers in the field. This paper describes the first empirical study of how practitioners think about energy when they write requirements, design, construct, test, and maintain their software. We report findings from a quantitative,targeted survey of 464 practitioners from ABB, Google, IBM, and Microsoft, which was motivated by and supported with qualitative data from 18 in-depth interviews with Microsoft employees. The major findings and implications from the collected data contextualize existing green software engineering research and suggest directions for researchers aiming to develop strategies and tools to help practitioners improve the energy usage of their applications.
[green software engineering, Google, data center-based services, ABB, Conferences, Encoding, Green Software Engineering, Survey, energy usage, Green products, IBM, embedded systems, mobile applications, practitioner perspective, Empirical Study, Software, software engineering, Microsoft, green computing, Interviews, energy consumption, Software engineering]
Automated Energy Optimization of HTTP Requests for Mobile Applications
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Energy is a critical resource for apps that run on mobile devices. Among all operations, making HTTP requests is one of the most energy consuming. Previous studies have shown that bundling smaller HTTP requests into a single larger HTTP request can be an effective way to improve energy efficiency of network communication, but have not defined an automated way to detect when apps can be bundled nor to transform the apps to do this bundling. In this paper we propose an approach to reduce the energy consumption of HTTP requests in Android apps by automatically detecting and then bundling multiple HTTP requests. Our approach first detects HTTP requests that can be bundled using static analysis, then uses a proxy based technique to bundle HTTP requests at runtime. We evaluated our approach on a set of real world marketplace Android apps. In this evaluation, our approach achieved an average energy reduction of 15% for the subject apps and did not impose a significant runtime overhead on the optimized apps.
[Energy consumption, Protocols, HTTP requests, program diagnostics, network communication, static analysis, Mobile communication, Mobile apps, Servers, proxy based technique, Optimization, Energy optimization, Uniform resource locators, Runtime, mobile computing, power aware computing, energy optimization, mobile applications, energy efficiency, energy conservation, mobile devices, Android applications]
Too Long; Didn't Watch! Extracting Relevant Fragments from Software Development Video Tutorials
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
When knowledgeable colleagues are not available, developers resort to offline and online resources, e.g., tutorials, mailing lists, and Q&amp;A websites. These, however, need to be found, read, and understood, which takes its toll in terms of time and mental energy. A more immediate and accessible resource are video tutorials found on the web, which in recent years have seen a steep increase in popularity. Nonetheless, videos are an intrinsically noisy data source, and finding the right piece of information might be even more cumbersome than using the previously mentioned resources. We present CodeTube, an approach which mines video tutorials found on the web, and enables developers to query their contents. The video tutorials are split into coherent fragments, to return only fragments related to the query. These are complemented with information from additional sources, such as Stack Overflow discussions. The results of two studies to assess CodeTube indicate that video tutorials-if appropriately processed-represent a useful, yet still under-utilized source of information for software development.
[Java, data mining, Tutorials, Streaming media, software engineering, Optical character recognition software, Data mining, software development video tutorials, YouTube, CodeTube]
Overcoming Open Source Project Entry Barriers with a Portal for Newcomers
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Community-based Open Source Software (OSS) projects are usually self-organized and dynamic, receiving contributions from distributed volunteers. Newcomer are important to the survival, long-term success, and continuity of these communities. However, newcomers face many barriers when making their first contribution to an OSS project, leading in many cases to dropouts. Therefore, a major challenge for OSS projects is to provide ways to support newcomers during their first contribution. In this paper, we propose and evaluate FLOSScoach, a portal created to support newcomers to OSS projects. FLOSScoach was designed based on a conceptual model of barriers created in our previous work. To evaluate the portal, we conducted a study with 65 students, relying on qualitative data from diaries, self-efficacy questionnaires, and the Technology Acceptance Model. The results indicate that FLOSScoach played an important role in guiding newcomers and in lowering barriers related to the orientation and contribution process, whereas it was not effective in lowering technical barriers. We also found that FLOSScoach is useful, easy to use, and increased newcomers' confidence to contribute. Our results can help project maintainers on deciding the points that need more attention in order to help OSS project newcomers overcome entry barriers.
[Industries, FLOSScoach, community-based open source software projects, public domain software, OSS projects, Documentation, portals, Beginners, Open source software, Newbies, Onboarding, Newcomers, Obstacles, Open Source Software, Computer bugs, Barriers, Web portal, open source project entry barriers, technology acceptance model, Joining processes, Novices, Portals, Joining Process]
Work Practices and Challenges in Pull-Based Development: The Contributor's Perspective
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
The pull-based development model is an emerging way of contributing to distributed software projects that is gaining enormous popularity within the open source software (OSS) world. Previous work has examined this model by focusing on projects and their owners-we complement it by examining the work practices of project contributors and the challenges they face.We conducted a survey with 645 top contributors to active OSS projects using the pull-based model on GitHub, the prevalent social coding site. We also analyzed traces extracted from corresponding GitHub repositories. Our research shows that: contributors have a strong interest in maintaining awareness of project status to get inspiration and avoid duplicating work, but they do not actively propagate information; communication within pull requests is reportedly limited to low-level concerns and contributors often use communication channels external to pull requests; challenges are mostly social in nature, with most reporting poor responsiveness from integrators; and the increased transparency of this setting is a confirmed motivation to contribute. Based on these findings, we present recommendations for practitioners to streamline the contribution process and discuss potential future research directions.
[public domain software, open source software, distributed software projects, GitHub, contribution process, open source contribution, pull-based development model, Encoding, Electronic mail, OSS, dis- tributed software development, project status awareness, Collaboration, Focusing, pull-based development, pull request, Software, software engineering, Face, Software engineering]
Automatically Learning Semantic Features for Defect Prediction
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models. To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs). Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7% in precision, 11.5% in recall, and 14.2% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9% in F1.
[source code (software), public domain software, Predictive models, within-project defect prediction, token vectors, WPDP, semantic feature learning, abstract syntax trees, Training, deep learning, DBN, Semantics, defect prediction, deep belief network, belief networks, open source projects, AST, program diagnostics, programs semantic representation, Buildings, representation-learning algorithm, CPDP, source code, programming language semantics, feature generation, Syntactics, Feature extraction, Data models, cross-project defect prediction, neural nets, software defect prediction]
Cross-Project Defect Prediction Using a Connectivity-Based Unsupervised Classifier
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Defect prediction on projects with limited historical data has attracted great interest from both researchers and practitioners. Cross-project defect prediction has been the main area of progress by reusing classifiers from other projects. However, existing approaches require some degree of homogeneity (e.g., a similar distribution of metric values) between the training projects and the target project. Satisfying the homogeneity requirement often requires significant effort (currently a very active area of research). An unsupervised classifier does not require any training data, therefore the heterogeneity challenge is no longer an issue. In this paper, we examine two types of unsupervised classifiers: a) distance-based classifiers (e.g., k-means); and b) connectivity-based classifiers. While distance-based unsupervised classifiers have been previously used in the defect prediction literature with disappointing performance, connectivity-based classifiers have never been explored before in our community. We compare the performance of unsupervised classifiers versus supervised classifiers using data from 26 projects from three publicly available datasets (i.e., AEEEM, NASA, and PROMISE). In the cross-project setting, our proposed connectivity-based classifier (via spectral clustering) ranks as one of the top classifiers among five widely-used supervised classifiers (i.e., random forest, naive Bayes, logistic regression, decision tree, and logistic model tree) and five unsupervised classifiers (i.e., k-means, partition around medoids, fuzzy C-means, neural-gas, and spectral clustering). In the within-project setting (i.e., models are built and applied on the same project), our spectral classifier ranks in the second tier, while only random forest ranks in the first tier. Hence, connectivity-based unsupervised classifiers offer a viable solution for cross and within project defect predictions.
[pattern classification, public domain software, Predictive models, software maintenance, distance-based classifiers, heterogeneity, unsupervised learning, Training, graph mining, Software metrics, defect prediction, Training data, Clustering algorithms, unsupervised, connectivity-based unsupervised classifier, connectivity-based classifiers, Software, cross-project defect prediction, cross-project, homogeneity degree, spectral clustering]
Automated Parameter Optimization of Classification Techniques for Defect Prediction Models
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Defect prediction models are classifiers that are trained to identify defect-prone software modules. Such classifiers have configurable parameters that control their characteristics (e.g., the number of trees in a random forest classifier). Recent studies show that these classifiers may underperform due to the use of suboptimal default parameter settings. However, it is impractical to assess all of the possible settings in the parameter spaces. In this paper, we investigate the performance of defect prediction models where Caret - an automated parameter optimization technique - has been applied. Through a case study of 18 datasets from systems that span both proprietary and open source domains, we find that (1) Caret improves the AUC performance of defect prediction models by as much as 40 percentage points; (2) Caret-optimized classifiers are at least as stable as (with 35% of them being more stable than) classifiers that are trained using the default settings; and (3) Caret increases the likelihood of producing a top-performing classifier by as much as 83%. Hence, we conclude that parameter settings can indeed have a large impact on the performance of defect prediction models, suggesting that researchers should experiment with the parameters of the classification techniques. Since automated parameter optimization techniques like Caret yield substantially benefits in terms of performance improvement and stability, while incurring a manageable additional computational cost, they should be included in future defect prediction studies.
[pattern classification, proprietary domain, public domain software, Predictive models, Boosting, software quality, Optimization, software fault tolerance, configurable parameters, classification techniques, defect prediction models, Caret-optimized classifiers, open source domain, Vegetation, experimental design, parameter optimization, Decision trees, Kernel, Caret technique, software defect prediction]
AntMiner: Mining More Bugs by Reducing Noise Interference
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Detecting bugs with code mining has proven to be an effective approach. However, the existing methods suffer from reporting serious false positives and false negatives. In this paper, we developed an approach called AntMiner to improve the precision of code mining by carefully preprocessing the source code. Specifically, we employ the program slicing technique to decompose the original source repository into independent sub-repositories, taking critical operations (automatically extracted from source code) as slicing criteria. In this way, the statements irrelevant to a critical operation are excluded from the corresponding sub-repository. Besides, various semantics-equivalent representations are normalized into a canonical form. Eventually, the mining process can be performed on a refined code database, and false positives and false negatives can be significantly pruned. We have implemented AntMiner and applied it to detect bugs in the Linux kernel. It reported 52 violations that have been either confirmed as real bugs by the kernel development community or fixed in new kernel versions. Among them, 41 cannot be detected by a widely used representative analysis tool Coverity. Besides, the result of a comparative analysis shows that our approach can effectively improve the precision of code mining and detect subtle bugs that have previously been missed.
[operating system kernels, program debugging, Code mining, noise interference reduction, data mining, source code preprocessing, Linux kernel, Programming, bug detection, Bug detection, Data mining, semantics-equivalent representation, Coverity tool, Databases, Program slicing, Linux, Computer bugs, AntMiner approach, program slicing technique, Kernel, program slicing, code mining]
Program Synthesis Using Natural Language
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Interacting with computers is a ubiquitous activity for millions of people. Repetitive or specialized tasks often require creation of small, often one-off, programs. End-users struggle with learning and using the myriad of domain-specific languages (DSLs) to effectively accomplish these tasks. We present a general framework for constructing program synthesizers that take natural language (NL) inputs and produce expressions in a target DSL. The framework takes as input a DSL definition and training data consisting of NL/DSL pairs. From these it constructs a synthesizer by learning optimal weights and classifiers (using NLP features) that rank the outputs of a keyword-programming based translation. We applied our framework to three domains: repetitive text editing, an intelligent tutoring system, and flight information queries. On 1200+ English descriptions, the respective synthesizers rank the desired program as the top-1 and top-3 for 80% and 90% descriptions respectively.
[domain-specific languages, User Intent, intelligent tutoring system, Synthesizers, natural language processing, Natural languages, intelligent tutoring systems, keyword-programming based translation, program synthesis, formal specification, Training, query processing, flight information queries, End-user Programming, Natrual Language Programming, Domain Specific Languages, Automata, Training data, Benchmark testing, repetitive text editing, DSL, natural language, Program Synthesis]
SWIM: Synthesizing What I Mean - Code Search and Idiomatic Snippet Synthesis
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Modern programming frameworks come with large libraries, with diverse applications such as for matching regular expressions, parsing XML files and sending email. Programmers often use search engines such as Google and Bing to learn about existing APIs. In this paper, we describe SWIM, a tool which suggests code snippets given API-related natural language queries such as &#x201C;generate md5 hash code&#x201D;. The query does not need to contain framework-specific trivia such as the type names or methods of interest. We translate user queries into the APIs of interest using clickthrough data from the Bing search engine. Then, based on patterns learned from open-source code repositories, we synthesize idiomatic code describing the use of these APIs. We introduce structured call sequences to capture API-usage patterns. Structured call sequences are a generalized form of method call sequences, with if-branches and while-loops to represent conditional and repeated API usage patterns, and are simple to extract and amenable to synthesis. We evaluated SWIM with 30 common C# API-related queries received by Bing. For 70% of the queries, the first suggested snippet was a relevant solution, and a relevant solution was present in the top 10 results for all benchmarked queries. The online portion of the workflow is also very responsive, at an average of 1.5 seconds per snippet.
[idiomatic snippet synthesis, Google, API-related natural language queries, structured call sequences, search engines, application program interfaces, Natural languages, code search, synthesizing what i mean, method call sequences, Free form queries, C# languages, query processing, programming framework, application program interface, Web pages, API usage patterns, Search engines, Data models, Libraries, SWIM framework, Pattern matching, Bing]
Cross-Supervised Synthesis of Web-Crawlers
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
A web-crawler is a program that automatically and systematically tracks the links of a website and extracts information from its pages. Due to the different formats of websites, the crawling scheme for different sites can differ dramatically. Manually customizing a crawler for each specific site is time consuming and error-prone. Furthermore, because sites periodically change their format and presentation, crawling schemes have to be manually updated and adjusted. In this paper, we present a technique for automatic synthesis of web-crawlers from examples. The main idea is to use hand-crafted (possibly partial) crawlers for some websites as the basis for crawling other sites that contain the same kind of information. Technically, we use the data on one site to identify data on another site. We then use the identified data to learn the website structure and synthesize an appropriate extraction scheme. We iterate this process, as synthesized extraction schemes result in additional data to be used for re-learning the website structure. We implemented our approach and automatically synthesized 30 crawlers for websites from nine different categories: books, TVs, conferences, universities, cameras, phones, movies, songs, and hotels.
[synthesis, Web crawler, Crawlers, Glass, information retrieval, Containers, wrapper, Data mining, Database languages, information extraction, Layout, crawler customization, cross-supervised synthesis, crawling schemes, Concrete, Internet, Data extraction, scarpper]
Automatic Model Generation from Documentation for Java API Functions
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Modern software systems are becoming increasingly complex, relying on a lot of third-party library support. Library behaviors are hence an integral part of software behaviors. Analyzing them is as important as analyzing the software itself. However, analyzing libraries is highly challenging due to the lack of source code, implementation in different languages, and complex optimizations. We observe that many Java library functions provide excellent documentation, which concisely describes the functionalities of the functions. We develop a novel technique that can construct models for Java API functions by analyzing the documentation. These models are simpler implementations in Java compared to the original ones and hence easier to analyze. More importantly, they provide the same functionalities as the original functions. Our technique successfully models 326 functions from 14 widely used Java classes. We also use these models in static taint analysis on Android apps and dynamic slicing for Java programs, demonstrating the effectiveness and efficiency of our models.
[Java, application program interfaces, natural language processing, auto-testing, Documentation, Generators, Indexes, static taint analysis, Analytical models, environment modeling, library behavior, documentation analysis, Java classes, Android applications, Libraries, Software, Java API functions, automatic model generation, program slicing, Java programs, dynamic slicing, Java library functions]
Augmenting API Documentation with Insights from Stack Overflow
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Software developers need access to different kinds of information which is often dispersed among different documentation sources, such as API documentation or Stack Overflow. We present an approach to automatically augment API documentation with "insight sentences" from Stack Overflow -- sentences that are related to a particular API type and that provide insight not contained in the API documentation of that type. Based on a development set of 1,574 sentences, we compare the performance of two state-of-the-art summarization techniques as well as a pattern-based approach for insight sentence extraction. We then present SISE, a novel machine learning based approach that uses as features the sentences themselves, their formatting, their question, their answer, and their authors as well as part-of-speech tags and the similarity of a sentence to the corresponding API documentation. With SISE, we were able to achieve a precision of 0.64 and a coverage of 0.7 on the development set. In a comparative study with eight software developers, we found that SISE resulted in the highest number of sentences that were considered to add useful information not found in the API documentation. These results indicate that taking into account the meta data available on Stack Overflow as well as part-of-speech tags can significantly improve unsupervised extraction approaches when applied to Stack Overflow data.
[Java, insight sentences, application program interfaces, Stack overflow, insight sentence extraction, Documentation, SISE approach, API documentation augmentation, API documentation, Data mining, machine learning based approach, Stack Overflow, Uniform resource locators, summarization techniques, application program interface, Feature extraction, Software, pattern-based approach, learning (artificial intelligence), Joining processes]
From Word Embeddings to Document Similarities for Improved Information Retrieval in Software Engineering
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
The application of information retrieval techniques to search tasks in software engineering is made difficult by the lexical gap between search queries, usually expressed in natural language (e.g. English), and retrieved documents, usually expressed in code (e.g. programming languages). This is often the case in bug and feature location, community question answering, or more generally the communication between technical personnel and non-technical stake holders in a software project. In this paper, we propose bridging the lexical gap by projecting natural language statements and code snippets as meaning vectors in a shared representation space. In the proposed architecture, word embeddings are rst trained on API documents, tutorials, and reference documents, and then aggregated in order to estimate semantic similarities between documents. Empirical evaluations show that the learned vector space embeddings lead to improvements in a previously explored bug localization task and a newly de ned task of linking API documents to computer programming questions.
[Vocabulary, vector space embeddings, application program interfaces, system documentation, document similarities, bug localization, Semantics, software engineering, bug reports, Mathematical model, Context, document handling, natural language processing, Natural languages, code snippets, information retrieval, reference documents, word embeddings, bug localization task, Computer bugs, tutorials, skip-gram model, natural language statements, information retrieval techniques, API documents, Software]
Learning API Usages from Bytecode: A Statistical Approach
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Mobile app developers rely heavily on standard API frameworks and libraries. However, learning API usages is often challenging due to the fast-changing nature of API frameworks for mobile systems and the insufficiency of API documentation and source code examples. In this paper, we propose a novel approach to learn API usages from bytecode of Android mobile apps. Our core contributions include HAPI, a statistical model of API usages and three algorithms to extract method call sequences from apps' bytecode, to train HAPI based on those sequences, and to recommend method calls in code completion using the trained HAPIs. Our empirical evaluation shows that our prototype tool can effectively learn API usages from 200 thousand apps containing 350 million method sequences. It recommends next method calls with top-3 accuracy of 90% and outperforms baseline approaches on average 10-20%.
[Algorithm design and analysis, HAPI model, mobile apps, application program interfaces, Humanoid robots, Documentation, Mobile communication, Probabilistic logic, API frameworks, API documentation, API usages learning, mobile app developers, mobile computing, code completion, application program interface, Hidden Markov models, Statistical model, Bytecode, Androids, statistical analysis, API usage, statistical approach]
On the "Naturalness" of Buggy Code
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Real software, the kind working programmers produce by the kLOC to solve real-world problems, tends to be &#x201C;natural&#x201D;, like speech or natural language; it tends to be highly repetitive and predictable. Researchers have captured this naturalness of software through statistical models and used them to good effect in suggestion engines, porting tools, coding standards checkers, and idiom miners. This suggests that code that appears improbable, or surprising, to a good statistical language model is &#x201C;unnatural&#x201D; in some sense, and thus possibly suspicious. In this paper, we investigate this hypothesis. We consider a large corpus of bug fix commits (ca. 7,139), from 10 different Java projects, and focus on its language statistics, evaluating the naturalness of buggy code and the corresponding fixes. We find that code with bugs tends to be more entropic (i.e. unnatural), becoming less so as bugs are fixed. Ordering files for inspection by their average entropy yields cost-effectiveness scores comparable to popular defect prediction methods. At a finer granularity, focusing on highly entropic lines is similar in cost-effectiveness to some well-known static bug finders (PMD, FindBugs) and or- dering warnings from these bug finders using an entropy measure improves the cost-effectiveness of inspecting code implicated in warnings. This suggests that entropy may be a valid, simple way to complement the effectiveness of PMD or FindBugs, and that search-based bug-fixing methods may benefit from using entropy both for fault-localization and searching for fixes.
[program debugging, entropy measure, statistical language model, bug fix commits, Biological system modeling, PMD, Predictive models, Inspection, cost-effectiveness score, Entropy, Java projects, language statistics, Standards, FindBugs, bug finders, Computer bugs, buggy code naturalness, software naturalness, Software]
Code Anomalies Flock Together: Exploring Code Anomaly Agglomerations for Locating Design Problems
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Design problems affect every software system. Diverse software systems have been discontinued or reengineered due to design problems. As design documentation is often informal or nonexistent, design problems need to be located in the source code. The main difficulty to identify a design problem in the implementation stems from the fact that such problem is often scattered through several program elements. Previous work assumed that code anomalies -- popularly known as code smells -- may provide sufficient hints about the location of a design problem. However, each code anomaly alone may represent only a partial embodiment of a design problem. In this paper, we hypothesize that code anomalies tend to ``flock together'' to realize a design problem. We analyze to what extent groups of inter-related code anomalies, named agglomerations, suffice to locate design problems. We analyze more than 2200 agglomerations found in seven software systems of different sizes and from different domains. Our analysis indicates that certain forms of agglomerations are consistent indicators of both congenital and evolutionary design problems, with accuracy often higher than 80%.
[code anomaly, software design, Documentation, agglomeration, design documentation, congenital design problems, code anomaly agglomerations, code smells, Semantics, Surgery, desing problem, Syntactics, Software systems, software engineering, Fats, evolutionary design problems, diverse software systems]
Using (Bio)Metrics to Predict Code Quality Online
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Finding and fixing code quality concerns, such as defects or poor understandability of code, decreases software development and evolution costs. A common industrial practice to identify code quality concerns early on are code reviews. While code reviews help to identify problems early on, they also impose costs on development and only take place after a code change is already completed. The goal of our research is to automatically identify code quality concerns while a developer is making a change to the code. By using biometrics, such as heart rate variability, we aim to determine the difficulty a developer experiences working on a part of the code as well as identify and help to fix code quality concerns before they are even committed to the repository. In a field study with ten professional developers over a two-week period we investigated the use of biometrics to determine code quality concerns. Our results show that biometrics are indeed able to predict quality concerns of parts of the code while a developer is working on, improving upon a naive classifier by more than 26% and outperforming classifiers based on more traditional metrics. In a second study with five professional developers from a different country and company, we found evidence that some of our findings from our initial study can be replicated. Overall, the results from the presented studies suggest that biometrics have the potential to predict code quality concerns online and thus lower development and evolution costs.
[Biometrics (access control), software evolution costs, Companies, Manuals, code reviews, software quality, code quality prediction, Temperature measurement, biometrics, heart rate variability, software development costs, Software, Heart rate variability, code understandability]
CUSTODES: Automatic Spreadsheet Cell Clustering and Smell Detection Using Strong and Weak Features
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Various techniques have been proposed to detect smells in spreadsheets, which are susceptible to errors. These techniques typically detect spreadsheet smells through a mechanism based on a fixed set of patterns or metric thresholds. Unlike conventional programs, tabulation styles vary greatly across spreadsheets. Smell detection based on fixed patterns or metric thresholds, which are insensitive to the varying tabulation styles, can miss many smells in one spreadsheet while reporting many spurious smells in another. In this paper, we propose CUSTODES to effectively cluster spreadsheet cells and detect smells in these clusters. The clustering mechanism can automatically adapt to the tabulation styles of each spreadsheet using strong and weak features. These strong and weak features capture the invariant and variant parts of tabulation styles, respectively. As smelly cells in a spreadsheet normally occur in minority, they can be mechanically detected as clusters' outliers in feature spaces. We implemented and applied CUSTODES to 70 spreadsheets files randomly sampled from the EUSES corpus. These spreadsheets contain 1,610 formula cell clusters. Experimental results confirmed that CUSTODES is effective. It successfully detected harmful smells that can induce computation anomalies in spreadsheets with an F-measure of 0.72, outperforming state-of-the-art techniques.
[Measurement, Computers, Adaptation models, feature modeling, Computational modeling, program diagnostics, smell detection, tabulation styles, Spreadsheets, spreadsheet programs, EUSES corpus, end-user programming, F-measure, CUSTODES, Quality assurance, pattern clustering, Feature extraction, Software, cell clustering, automatic spreadsheet cell clustering]
Disseminating Architectural Knowledge on Open-Source Projects: A Case Study of the Book "Architecture of Open-Source Applications"
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
This paper reports on an interview-based study of 18 authors of different chapters of the two-volume book "Architecture of Open-Source Applications". The main contributions are a synthesis of the process of authoring essay-style documents (ESDs) on software architecture, a series of observations on important factors that influence the content and presentation of architectural knowledge in this documentation form, and a set of recommendations for readers and writers of ESDs on software architecture. We analyzed the influence of three factors in particular: the evolution of a system, the community involvement in the project, and the personal characteristics of the author. This study provides the first systematic investigation of the creation of ESDs on software architecture. The observations we collected have implications for both readers and writers of ESDs, and for architecture documentation in general.
[Context, public domain software, Open-Source Software, Documentation, Electrostatic discharges, open-source projects, Architecture of Open-Source Applications, software architecture, Software architecture, architecture documentation, Computer architecture, ESD, architectural knowledge dissemination, essay-style documents, Software, Architecture Description, Interviews]
Identifying and Quantifying Architectural Debt
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Our prior work showed that the majority of error-prone source files in a software system are architecturally connected. Flawed architectural relations propagate defectsamong these files and accumulate high maintenance costs over time, just like debts accumulate interest. We model groups of architecturally connected files that accumulate high maintenance costs as architectural debts. To quantify such debts, we formally define architectural debt, and show how to automatically identify debts, quantify their maintenance costs, and model these costs over time. We describe a novel history coupling probability matrix for this purpose, and identify architecture debts using 4 patterns of architectural flaws shown to correlate with reduced software quality. We evaluate our approach on 7 large-scale open source projects, and show that a significant portion of total project maintenance effort is consumed by paying interest on architectural debts. The top 5 architectural debts, covering a small portion (8% to 25%) of each project's error-prone files, capture a significant portion (20% to 61%) of each project's maintenance effort. Finally, we show that our approach reveals how architectural issues evolve into debts over time.
[architectural debt quantification, Economic indicators, Maintenance engineering, costing, software quality, History, software maintenance, software system, Couplings, software architecture, Software architecture, architectural debt identification, architecturally connected files, history coupling probability matrix, Computer architecture, Software, total project maintenance effort, maintenance costs, error-prone source files]
Decoupling Level: A New Metric for Architectural Maintenance Complexity
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Despite decades of research on software metrics, we still cannot reliably measure if one design is more maintainable than another. Software managers and architects need to understand whether their software architecture is "good enough\
[open source projects, architectural maintenance complexity metric, public domain software, architecture maintainability metric, Software Architecture, Observers, canonical principles, Complexity theory, software maintenance, Software Quality, software architecture, Software architecture, decoupling level metric, Computer architecture, Software Metrics, Software, Software measurement, software metrics]
On The Limits of Mutation Reduction Strategies
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Although mutation analysis is considered the best way to evaluate the effectiveness of a test suite, hefty computational cost often limits its use. To address this problem, various mutation reduction strategies have been proposed, all seeking to reduce the number of mutants while maintaining the representativeness of an exhaustive mutation analysis. While research has focused on the reduction achieved, the effectiveness of these strategies in selecting representative mutants, and the limits in doing so have not been investigated, either theoretically or empirically. We investigate the practical limits to the effectiveness of mutation reduction strategies, and provide a simple theoretical framework for thinking about the absolute limits. Our results show that the limit in improvement of effectiveness over random sampling for real-world open source programs is a mean of only 13.078%. Interestingly, there is no limit to the improvement that can be made by addition of new mutation operators. Given that this is the maximum that can be achieved with perfect advance knowledge of mutation kills, what can be practically achieved may be much worse. We conclude that more effort should be focused on enhancing mutations than removing operators in the name of selective mutation for questionable benefit.
[Correlation, program testing, program diagnostics, public domain software, software testing, random sampling, software testing; statistical analysis; theoretical analysis; mutation analysis, Sociology, Computer bugs, mutation operators, mutation reduction strategies, Syntactics, open source programs, Software engineering, Testing]
Comparing White-Box and Black-Box Test Prioritization
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Although white-box regression test prioritization has been well-studied, the more recently introduced black-box prioritization approaches have neither been compared against each other nor against more well-established white-box techniques. We present a comprehensive experimental comparison of several test prioritization techniques, including well-established white-box strategies and more recently introduced black-box approaches. We found that Combinatorial Interaction Testing and diversity-based techniques (Input Model Diversity and Input Test Set Diameter) perform best among the black-box approaches. Perhaps surprisingly, we found little difference between black-box and white-box performance (at most 4% fault detection rate difference). We also found the overlap between black- and white-box faults to be high: the first 10% of the prioritized test suites already agree on at least 60% of the faults found. These are positive findings for practicing regression testers who may not have source code available, thereby making white-box techniques inapplicable. We also found evidence that both black-box and white-box prioritization remain robust over multiple system releases.
[black-box test prioritization, Regression Testing, Flexible printed circuits, program testing, Instruments, white-box regression test prioritization, black-box prioritization approach, White-box, regression analysis, diversity-based techniques, Servers, input test set diameter, input model diversity, combinatorial interaction testing, Fault detection, Black-box, Robustness, Software, Testing]
How Does Regression Test Prioritization Perform in Real-World Software Evolution?
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
In recent years, researchers have intensively investigated various topics in test prioritization, which aims to re-order tests to increase the rate of fault detection during regression testing. While the main research focus in test prioritization is on proposing novel prioritization techniques and evaluating on more and larger subject systems, little effort has been put on investigating the threats to validity in existing work on test prioritization. One main threat to validity is that existing work mainly evaluates prioritization techniques based on simple artificial changes on the source code and tests. For example, the changes in the source code usually include only seeded program faults, whereas the test suite is usually not augmented at all. On the contrary, in real-world software development, software systems usually undergo various changes on the source code and test suite augmentation. Therefore, it is not clear whether the conclusions drawn by existing work in test prioritization from the artificial changes are still valid for real-world software evolution. In this paper, we present the first empirical study to investigate this important threat to validity in test prioritization. We reimplemented 24 variant techniques of both the traditional and time-aware test prioritization, and investigated the impacts of software evolution on those techniques based on the version history of 8 real-world Java programs from GitHub. The results show that for both traditional and time-aware test prioritization, test suite augmentation significantly hampers their effectiveness, whereas source code changes alone do not influence their effectiveness much.
[Schedules, program testing, software development, Instruments, GitHub, fault detection, prioritization techniques, real-world software evolution, test suite augmentation, Software systems, software engineering, Time factors, regression test prioritization, Testing, Software engineering]
The Impact of Test Case Summaries on Bug Fixing Performance: An Empirical Investigation
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Automated test generation tools have been widely investigated with the goal of reducing the cost of testing activities. However, generated tests have been shownnot to help developers in detecting and finding more bugs even though they reach higher structural coverage compared to manual testing. The main reason is that generated tests are difficult to understand and maintain. Our paper proposes an approach, coined TestDescriber, which automatically generates test case summaries of the portion of code exercised by each individual test, thereby improving understandability. We argue that this approach can complement the current techniques around automated unit test generation or search-based techniques designed to generate a possibly minimal set of test cases. In evaluating our approach we found that (1) developers find twice as many bugs, and (2) test case summaries significantly improve the comprehensibility of test cases, which is considered particularly useful by developers.
[bug fixing performance, Software testing, Java, program debugging, program testing, Test Case Summarization, Natural languages, search-based techniques, automated unit test generation, Pragmatics, test case summaries, Computer bugs, TestDescriber approach, Empirical Study, Software, Testing, Software engineering, automated test generation tools]
Reducing Combinatorics in GUI Testing of Android Applications
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
The rising popularity of Android and the GUI-driven nature of its apps have motivated the need for applicable automated GUI testing techniques. Although exhaustive testing of all possible combinations is the ideal upper bound in combinatorial testing, it is often infeasible, due to the combinatorial explosion of test cases. This paper presents TrimDroid, a framework for GUI testing of Android apps that uses a novel strategy to generate tests in a combinatorial, yet scalable, fashion. It is backed with automated program analysis and formally rigorous test generation engines. TrimDroid relies on program analysis to extract formal specifications. These specifications express the app's behavior (i.e., control flow between the various app screens) as well as the GUI elements and their dependencies. The dependencies among the GUI elements comprising the app are used to reduce the number of combinations with the help of a solver. Our experiments have corroborated TrimDroid's ability to achieve a comparable coverage as that possible under exhaustive GUI testing using significantly fewer test cases.
[program testing, graphical user interfaces, program diagnostics, Humanoid robots, graphical user interface, Software Testing, Input Generation, TrimDroid framework, formal specification, formal specifications, Android, test generation, mobile computing, combinatorial testing, program analysis, Android applications, GUI testing, Androids, Erbium, Smart phones, Testing, Graphical user interfaces]
MobiPlay: A Remote Execution Based Record-and-Replay Tool for Mobile Applications
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
The record-and-replay approach for software testing is important and valuable for developers in designing mobile applications. However, the existing solutions for recording and replaying Android applications are far from perfect. When considering the richness of mobile phones' input capabilities including touch screen, sensors, GPS, etc., existing approaches either fall short of covering all these different input types, or require elevated privileges that are not easily attained and can be dangerous. In this paper, we present a novel system, called MobiPlay, which aims to improve record-and-replay testing. By collaborating between a mobile phone and a server, we are the first to capture all possible inputs by doing so at the application layer, instead of at the Android framework layer or the Linux kernel layer, which would be infeasible without a server. MobiPlay runs the to-be-tested application on the server under exactly the same environment as the mobile phone, and displays the GUI of the application in real time on a thin client application installed on the mobile phone. From the perspective of the mobile phone user, the application appears to be local. We have implemented our system and evaluated it with tens of popular mobile applications showing that MobiPlay is efficient, flexible, and comprehensive. It can record all input data, including all sensor data, all touchscreen gestures, and GPS. It is able to record and replay on both the mobile phone and the server. Furthermore, it is suitable for both white-box and black-box testing.
[white-box testing, sensor data, program testing, graphical user interfaces, record-and-replay tool, software testing, graphical user interface, black-box testing, Mobile communication, touchscreen gestures, Linux kernel layer, Servers, mobile computing, Operating systems, MobiPlay tool, mobile phone, mobile applications, Android applications, GUI, Sensors, remote execution, Smart phones, Testing]
VDTest: An Automated Framework to Support Testing for Virtual Devices
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
The use of virtual devices in place of physical hardware is increasing in activities such as design, testing and debugging. Yet virtual devices are simply software applications, and like all software they are prone to faults. A full system simulator (FSS), is a class of virtual machine that includes a large set of virtual devices - enough to run the full target software stack. Defects in an FSS virtual device may have cascading effects as the incorrect behavior can be propagated forward to many different platforms as well as to guest programs. In this work we present VDTest, a novel framework for testing virtual devices within an FSS. VDTest begins by generat- ing a test specification obtained through static analysis. It then employs a two-phase testing approach to test virtual components both individually and in combination. It lever- ages a differential oracle strategy, taking advantage of the existence of a physical or golden device to eliminate the need for manually generating test oracles. In an empirical study using both open source and commercial FSSs, we found 64 faults, 83% more than random testing.
[Frequency selective surfaces, Computers, test specification, program testing, Test Oracles, program diagnostics, Device Drivers, VDTest, Debugging, FSS, static analysis, Registers, two-phase testing, formal specification, differential oracle strategy, virtual machine, virtual device testing, virtual machines, Software, Hardware, Virtual Devices, full system simulator, Testing]
Automated Test Suite Generation for Time-Continuous Simulink Models
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
All engineering disciplines are founded and rely on models, although they may differ on purposes and usages of modeling. Interdisciplinary domains such as Cyber Physical Systems (CPSs) seek approaches that incorporate different modeling needs and usages. Specifically, the Simulink modeling platform greatly appeals to CPS engineers due to its seamless support for simulation and code generation. In this paper, we propose a test generation approach that is applicable to Simulink models built for both purposes of simulation and code generation. We define test inputs and outputs as signals that capture evolution of values over time. Our test generation approach is implemented as a meta-heuristic search algorithm and is guided to produce test outputs with diverse shapes according to our proposed notion of diversity. Our evaluation, performed on industrial and public domain models, demonstrates that: (1) In contrast to the existing tools for testing Simulink models that are only applicable to a subset of code generation models, our approach is applicable to both code generation and simulation Simulink models. (2) Our new notion of diversity for output signals outperforms random baseline testing and an existing notion of signal diversity in revealing faults in Simulink models. (3) The fault revealing ability of our test generation approach outperforms that of the Simulink Design Verifier, the only testing toolbox for Simulink.
[Software testing, Shape, program testing, simulation Simulink models, Fuels, program compilers, automatic testing, Signal features, Structural coverage, Mathematical model, search problems, Testing, industrial domain models, test inputs, Computational modeling, automated test suite generation, Search-based software testing, public domain models, Simulink Design Verifier (SLDV), signal diversity, Output diversity, Software packages, code generation, time-continuous Simulink models, test outputs, fault revealing ability, Simulink models, Time-continuous behaviors, meta-heuristic search algorithm]
Missing Data Imputation Based on Low-Rank Recovery and Semi-Supervised Regression for Software Effort Estimation
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Software effort estimation (SEE) is a crucial step in software development. Effort data missing usually occurs in real-world data collection. Focusing on the missing data problem, existing SEE methods employ the deletion, ignoring, or imputation strategy to address the problem, where the imputation strategy was found to be more helpful for improving the estimation performance. Current imputation methods in SEE use classical imputation techniques for missing data imputation, yet these imputation techniques have their respective disadvantages and might not be appropriate for effort data. In this paper, we aim to provide an effective solution for the effort data missing problem. Incompletion includes the drive factor missing case and effort label missing case. We introduce the low-rank recovery technique for addressing the drive factor missing case. And we employ the semi-supervised regression technique to perform imputation in the case of effort label missing. We then propose a novel effort data imputation approach, named low-rank recovery and semi-supervised regression imputation (LRSRI). Experiments on 7 widely used software effort datasets indicate that: (1) the proposed approach can obtain better effort data imputation effects than other methods; (2) the imputed data using our approach can apply to multiple estimators well.
[Adaptation models, drive factor missing case, regression analysis, Low-rank recovery and semi-supervised regression imputation (LRSRI), deletion strategy, Missing data problem, LRSRI, Focusing, ignoring strategy, software engineering, effort label missing case, Drive factor missing case, software development, Estimation, Effort label missing case, imputation strategy, data collection, Software effort estimation, missing data imputation, SEE, software effort estimation, Organizations, Software, Data models, data handling, semisupervised regression, Software engineering, low-rank recovery]
Multi-objective Software Effort Estimation
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
We introduce a bi-objective effort estimation algorithm that combines Confidence Interval Analysis and assessment of Mean Absolute Error. We evaluate our proposed algorithm on three different alternative formulations, baseline comparators and current state-of-the-art effort estimators applied to five real-world datasets from the PROMISE repository, involving 724 different software projects in total. The results reveal that our algorithm outperforms the baseline, state-of-the-art and all three alternative formulations, statistically significantly (p &lt;; 0.001) and with large effect size (A<sub>12</sub> &#x2265; 0.9) over all five datasets. We also provide evidence that our algorithm creates a new state-of-the-art, which lies within currently claimed industrial human-expert-based thresholds, thereby demonstrating that our findings have actionable conclusions for practicing software engineers.
[mean absolute error, software engineers, Uncertainty, multi objective software effort estimation, Software algorithms, Estimation, Predictive models, confidence interval analysis, multi-objective evolutionary algorithm, Software effort estimation, estimates uncertainty, Software, software engineering, software projects, confidence interval, Mathematical model, Software engineering]
A Practical Guide to Select Quality Indicators for Assessing Pareto-Based Search Algorithms in Search-Based Software Engineering
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Many software engineering problems are multi-objective in nature, which has been largely recognized by the Search-based Software Engineering (SBSE) community. In this regard, Pareto- based search algorithms, e.g., Non-dominated Sorting Genetic Algorithm II, have already shown good performance for solving multi-objective optimization problems. These algorithms produce Pareto fronts, where each Pareto front consists of a set of non- dominated solutions. Eventually, a user selects one or more of the solutions from a Pareto front for their specific problems. A key challenge of applying Pareto-based search algorithms is to select appropriate quality indicators, e.g., hypervolume, to assess the quality of Pareto fronts. Based on the results of an extended literature review, we found that the current literature and practice in SBSE lacks a practical guide for selecting quality indicators despite a large number of published SBSE works. In this direction, the paper presents a practical guide for the SBSE community to select quality indicators for assessing Pareto-based search algorithms in different software engineering contexts. The practical guide is derived from the following complementary theoretical and empirical methods: 1) key theoretical foundations of quality indicators; 2) evidence from an extended literature review; and 3) evidence collected from an extensive experiment that was conducted to evaluate eight quality indicators from four different categories with six Pareto-based search algorithms using three real industrial problems from two diverse domains.
[Pareto optimisation, Pareto front, Practical Guide, Software algorithms, nondominated sorting genetic algorithm II, Search problems, SBSE community, Pareto-based Search Algorithms, Classification algorithms, software quality, quality indicators, Convergence, Bibliographies, Pareto-based search algorithms, Sociology, search-based software engineering, Multi-objective Software Engineering Problems, multiobjective optimization problems, Quality Indicators, search problems, Software engineering]
A Comparison of 10 Sampling Algorithms for Configurable Systems
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Almost every software system provides configuration options to tailor the system to the target platform and application scenario. Often, this configurability renders the analysis of every individual system configuration infeasible. To address this problem, researchers have proposed a diverse set of sampling algorithms. We present a comparative study of 10 state-of-the-art sampling algorithms regarding their fault-detection capability and size of sample sets. The former is important to improve software quality and the latter to reduce the time of analysis. In a nutshell, we found that sampling algorithms with larger sample sets are able to detect higher numbers of faults, but simple algorithms with small sample sets, such as most-enabled-disabled, are the most efficient in most contexts. Furthermore, we observed that the limiting assumptions made in previous work influence the number of detected faults, the size of sample sets, and the ranking of algorithms. Finally, we have identified a number of technical challenges when trying to avoid the limiting assumptions, which questions the practicality of certain sampling algorithms.
[Algorithm design and analysis, program diagnostics, Software algorithms, configurable systems, software quality, software system configuration, Configurable Systems, Configuration-Related Faults, Linux, Fault detection, Software quality, Sampling Algorithms, Software systems, fault-detection capability, Kernel, sampling algorithms]
Featured Model-Based Mutation Analysis
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.
[software product lines, Featured Transition Systems, optimization technique, software product line paradigm, Computational modeling, Scalability, program diagnostics, Unified modeling language, Variability, Software product lines, Analytical models, mutant generation, featured mutant model, mutant configuration, mutant execution, Mutation Analysis, Software, mutant execution process, featured transition systems, featured model-based mutation analysis, Testing, behavioural variability models]
Feature-Model Interfaces: The Highway to Compositional Analyses of Highly-Configurable Systems
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Today's software systems are often customizable by means of load-time or compile-time configuration options. These options are typically not independent and their dependencies can be specified by means of feature models. As many industrial systems contain thousands of options, the maintenance and utilization of feature models is a challenge for all stakeholders. In the last two decades, numerous approaches have been presented to support stakeholders in analyzing feature models. Such analyses are commonly reduced to satisfiability problems, which suffer from the growing number of options. While first attempts have been made to decompose feature models into smaller parts, they still require to compose all parts for analysis. We propose the concept of a feature-model interface that only consists of a subset of features, typically selected by experts, and hides all other features and dependencies. Based on a formalization of feature-model interfaces, we prove compositionality properties. We evaluate feature-model interfaces using a three-month history of an industrial feature model from the automotive domain with 18,616 features. Our results indicate performance benefits especially under evolution as often only parts of the feature model need to be analyzed again.
[load-time configuration option, feature-model interface, program diagnostics, highly-configurable systems, compile-time configuration option, Indexes, compositional analysis, Modularity, Automotive engineering, Analytical models, Software Product Line, satisfiability problems, Configurable Software, compositionality properties, Feature Model, Software systems, Variability Modeling, Stakeholders, Compositionality, Load modeling]
How Does the Degree of Variability Affect Bug Finding?
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Software projects embrace variability to increase adaptability and to lower cost; however, others blame variability for increasing complexity and making reasoning about programs more difficult. We carry out a controlled experiment to quantify the impact of variability on debugging of preprocessor- based programs. We measure speed and precision for bug finding tasks defined at three different degrees of variability on several subject programs derived from real systems. The results show that the speed of bug finding decreases linearly with the degree of variability, while effectiveness of finding bugs is relatively independent of the degree of variability. Still, identifying the set of configurations in which the bug manifests itself is difficult already for a low degree of variability. Surprisingly, identifying the exact set of affected configurations appears to be harder than finding the bug in the first place. The difficulty in reasoning about several configurations is a likely reason why the variability bugs are actually introduced in configurable programs. We hope that the detailed findings presented here will inspire the creation of programmer support tools addressing the challenges faced by developers when reasoning about configurations, contributing to more effective debugging and, ultimately, fewer bugs in highly-configurable systems.
[program debugging, Variability, Debugging, Color, Cognition, bug finding, Bug Finding, configurable program, variability degree, Linux, Computer bugs, Preprocessors, software projects, Kernel, preprocessor-based program debugging]
Angelix: Scalable Multiline Program Patch Synthesis via Symbolic Analysis
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Since debugging is a time-consuming activity, automated program repair tools such as GenProg have garnered interest. A recent study revealed that the majority of GenProg repairs avoid bugs simply by deleting functionality. We found that SPR, a state-of-the-art repair tool proposed in 2015, still deletes functionality in their many "plausible" repairs. Unlike generate-and-validate systems such as GenProg and SPR, semantic analysis based repair techniques synthesize a repair based on semantic information of the program. While such semantics-based repair methods show promise in terms of quality of generated repairs, their scalability has been a concern so far. In this paper, we present Angelix, a novel semantics-based repair method that scales up to programs of similar size as are handled by search-based repair tools such as GenProg and SPR. This shows that Angelix is more scalable than previously proposed semantics based repair methods such as SemFix and DirectFix. Furthermore, our repair method can repair multiple buggy locations that are dependent on each other. Such repairs are hard to achieve using SPR and GenProg. In our experiments, Angelix generated repairs from large-scale real-world software such as wireshark and php, and these generated repairs include multi-location repairs. We also report our experience in automatically repairing the well-known Heartbleed vulnerability.
[program debugging, Automated program repair, program repair tools, Scalability, program diagnostics, Semantic analysis, Maintenance engineering, Angelix method, generate-and-validate systems, multilocation repairs, Heartbleed vulnerability, symbolic analysis, Semantics, Computer bugs, scalable multiline program patch synthesis, semantic analysis, SPR tool, Software, semantic information, Testing, Software engineering]
An Analysis of the Search Spaces for Generate and Validate Patch Generation Systems
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
We present the first systematic analysis of key characteristics of patch search spaces for automatic patch generation systems. We analyze sixteen different configurations of the patch search spaces of SPR and Prophet, two current state-of-the-art patch generation systems. The analysis shows that 1) correct patches are sparse in the search spaces (typically at most one correct patch per search space per defect), 2) incorrect patches that nevertheless pass all of the test cases in the validation test suite are typically orders of magnitude more abundant, and 3) leveraging information other than the test suite is therefore critical for enabling the system to successfully isolate correct patches. We also characterize a key tradeoff in the structure of the search spaces. Larger and richer search spaces that contain correct patches for more defects can actually cause systems to find fewer, not more, correct patches. We identify two reasons for this phenomenon: 1) increased validation times because of the presence of more candidate patches and 2) more incorrect patches that pass the test suite and block the discovery of correct patches. These fundamental properties, which are all characterized for the first time in this paper, help explain why past systems often fail to generate correct patches and help identify challenges, opportunities, and productive future directions for the field.
[Program repair, automatic patch generation systems, Scalability, Conferences, program diagnostics, program repair, software maintenance, Patch generation, Search space, Systematics, SPR, Benchmark testing, Software, Space exploration, Software engineering, Prophet]
PAC Learning-Based Verification and Model Synthesis
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
We introduce a novel technique for verification and model synthesis of sequential programs. Our technique is based on learning an approximate regular model of the set of feasible paths in a program, and testing whether this model contains an incorrect behavior. Exact learning algorithms require checking equivalence between the model and the program, which is a difficult problem, in general undecidable. Our learning procedure is therefore based on the framework of probably approximately correct (PAC) learning, which uses sampling instead, and provides correctness guarantees expressed using the terms error probability and confidence. Besides the verification result, our procedure also outputs the model with the said correctness guarantees. Obtained preliminary experiments show encouraging results, in some cases even outperforming mature software verifiers.
[probably approximately correct learning, Frequency modulation, program verification, software verification, PAC learning, finite automata, model synthesis, sampling, error probability, confidence, sequential programs, PAC learning-based verification, Syntactics, Approximation algorithms, Picture archiving and communication systems, Software, learning (artificial intelligence), Testing, Software engineering, error statistics]
StubDroid: Automatic Inference of Precise Data-Flow Summaries for the Android Framework
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Smartphone users suffer from insucient information on how commercial as well as malicious apps handle sensitive data stored on their phones. Automated taint analyses address this problem by allowing users to detect and investigate how applications access and handle this data. A current problem with virtually all those analysis approaches is, though, that they rely on explicit models of the Android runtime library. In most cases, the existence of those models is taken for granted, despite the fact that the models are hard to come by: Given the size and evolution speed of a modern smartphone operating system it is prohibitively expensive to derive models manually from code or documentation. In this work, we therefore present StubDroid, the first fully automated approach for inferring precise and efficient library models for taint-analysis problems. StubDroid automatically constructs these summaries from a binary distribution of the library. In our experiments, we use StubDroid-inferred models to prevent the static taint analysis FlowDroid from having to re-analyze the Android runtime library over and over again for each analyzed app. As the results show, the models make it possible to analyze apps in seconds whereas most complete re-analyses would time out after 30 minutes. Yet, StubDroid yields comparable precision. In comparison to manually crafted summaries, StubDroid's cause the analysis to be more precise and to use less time and memory.
[summary, model inference, StubDroid-inferred models, program diagnostics, Humanoid robots, Android runtime library, smart phones, Android framework, Analytical models, Android (operating system), library, Operating systems, framework model, sensitive data handling, Static analysis, Libraries, Androids, StubDroid approach, smart phone operating system, automated taint analysis, precise data-flow summaries, Smart phones, Software engineering]
Exploring Language Support for Immutability
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Programming languages can restrict state change by preventing it entirely (immutability) or by restricting which clients may modify state (read-only restrictions). The benefits of immutability and read-only restrictions in software structures have been long-argued by practicing software engineers, researchers, and programming language designers. However, there are many proposals for language mechanisms for restricting state change, with a remarkable diversity of techniques and goals, and there is little empirical data regarding what practicing software engineers want in their tools and what would benefit them. We systematized the large collection of techniques used by programming languages to help programmers prevent undesired changes in state. We interviewed expert software engineers to discover their expectations and requirements, and found that important requirements, such as expressing immutability constraints, were not reflected in features available in the languages participants used. The interview results informed our design of a new language extension for specifying immutability in Java. Through an iterative, participatory design process, we created a tool that reflects requirements from both our interviews and the research literature.
[immutability, Java, C++ languages, language extension, Data structures, Mutability, Immutability, Programming language usability, Empirical studies of programmers, Programmer productivity, Programming language design, Software, Concrete, programming language, Interviews]
The Evolution of C Programming Practices: A Study of the Unix Operating System 1973-2015
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Tracking long-term progress in engineering and applied science allows us to take stock of things we have achieved, appreciate the factors that led to them, and set realistic goals for where we want to go. We formulate seven hypotheses associated with the long term evolution of C programming in the Unix operating system, and examine them by extracting, aggregating, and synthesising metrics from 66 snapshots obtained from a synthetic software configuration management repository covering a period of four decades. We found that over the years developers of the Unix operating system appear to have evolved their coding style in tandem with advancements in hardware technology, promoted modularity to tame rising complexity, adopted valuable new language features, allowed compilers to allocate registers on their behalf, and reached broad agreement regarding code formatting. The progress we have observed appears to be slowing or even reversing prompting the need for new sources of innovation to be discovered and followed.
[Measurement, Unix, synthetic software configuration management repository, C, coding practices, Encoding, Complexity theory, Registers, C language, Programming profession, configuration management, BSD, Unix operating system, Software, C programming, coding style, FreeBSD]
An Empirical Study on the Impact of C++ Lambdas and Programmer Experience
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Lambdas have seen increasing use in mainstream programming languages, notably in Java 8 and C++ 11. While the technical aspects of lambdas are known, we conducted the first randomized controlled trial on the human factors impact of C++ 11 lambdas compared to iterators. Because there has been recent debate on having students or professionals in experiments, we recruited undergraduates across the academic pipeline and professional programmers to evaluate these findings in a broader context. Results afford some doubt that lambdas benefit developers and show evidence that students are negatively impacted in regard to how quickly they can write correct programs to a test specification and whether they can complete a task. Analysis from log data shows that participants spent more time with compiler errors, and have more errors, when using lambdas as compared to iterators, suggesting difficulty with the syntax chosen for C++. Finally, experienced users were more likely to complete tasks, with or without lambdas, and could do so more quickly, with experience as a factor explaining 45.7% of the variance in our sample in regard to completion time.
[Context, Algorithm design and analysis, test specification, Java, Lambda Expressions, C++ lambda, Java 8 languages, C++ 11 languages, C++ languages, C++11, programmer experience, C++ language, formal specification, mainstream programming languages, Syntactics, Libraries, Human Factors]
Understanding and Fixing Multiple Language Interoperability Issues: The C/Fortran Case
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
We performed an empirical study to understand interoperability issues in C and Fortran programs. C/Fortran interoperability is very common and is representative of general language interoperability issues, such as how interfaces between languages are defined and how data types are shared. Fortran presents an additional challenge, since several ad hoc approaches to C/Fortran interoperability were in use long before a standard mechanism was defined. We explored 20 applications, automatically analyzing over 12 million lines of code. We found that only 3% of interoperability instances follow the ISO standard to describe interfaces; the rest follow a combination of compiler-dependent ad hoc approaches. Several parameters in cross-language functions did not have standards-compliant interoperable types, and about one-fourth of the parameters that were passed by reference could be passed by value. We propose that automated refactoring tools may provide a viable way to migrate programs to use the new interoperability features. We present two refactorings to transform code for this purpose and one refactoring to evolve code thereafter; all of these are instances of multiple language refactorings.
[multiple language refactorings, Java, open systems, multiple language interoperability issues, C, ISO standards, Fortran programs, Transforms, cross-language functions, polyglot, Security, C language, compiler-dependent ad hoc approaches, program compilers, Interoperability, Fortran, ISO Standards, refactoring, C programs, FORTRAN, language interoperability, ISO standard, Software engineering]
BigDebug: Debugging Primitives for Interactive Big Data Processing in Spark
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Developers use cloud computing platforms to process a large quantity of data in parallel when developing big data analytics. Debugging the massive parallel computations that run in today's data-centers is time consuming and error-prone. To address this challenge, we design a set of interactive, real-time debugging primitives for big data processing in Apache Spark, the next generation data-intensive scalable cloud computing platform. This requires re-thinking the notion of step-through debugging in a traditional debugger such as gdb, because pausing the entire computation across distributed worker nodes causes significant delay and naively inspecting millions of records using a watchpoint is too time consuming for an end user.First, BigDebug's simulated breakpoints and on-demand watchpoints allow users to selectively examine distributed, intermediate data on the cloud with little overhead. Second, a user can also pinpoint a crash-inducing record and selectively resume relevant sub-computations after a quick fix. Third, a user can determine the root causes of errors (or delays) at the level of individual records through a fine-grained data provenance capability. Our evaluation shows that BigDebug scales to terabytes and its record-level tracing incurs less than 25% overhead on average. It determines crash culprits orders of magnitude more accurately and provides up to 100% time saving compared to the baseline replay debugger. The results show that BigDebug supports debugging at interactive speeds with minimal performance impact.
[Cloud computing, program debugging, record-level tracing, Distributed databases, interactive tools, interactive systems, data-intensive scalable computing (DISC), distributed worker nodes, intermediate data, big data analytics, data-centers, Debugging, interactive big data processing, Big Data, debugging primitives, crash-inducing record, Computer crashes, fine-grained data provenance capability, Sparks, computer centres, Apache Spark, Delays, fault localization and recovery, cloud computing platforms, distributed data, BigDebug]
Debugging for Reactive Programming
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Reactive programming is a recent programming technique that provides dedicated language abstractions for reactive software. Reactive programming relieves developers from manually updating outputs when the inputs of a computation change, it overcomes a number of well-know issues of the Observer design pattern, and it makes programs more comprehensible. Unfortunately, complementing the new paradigm with proper tools is a vastly unexplored area. Hence, as of now, developers can embrace reactive programming only at the cost of a more challenging development process. In this paper, we investigate a primary issue in the field: debugging programs in the reactive style. We analyze the problem of debugging reactive programs, show that the reactive style requires a paradigm shift in the concepts needed for debugging, and propose RP Debugging, a methodology for effectively debugging reactive programs. These ideas are implemented in Reactive Inspector, a debugger for reactive programs integrated with the Eclipse Scala IDE. Evaluation based on a controlled experiment shows that RP Debugging outperforms traditional debugging techniques.
[program debugging, Eclipse Scala IDE, Reactive Inspector, Functional-reactive Programming, Debugging, Programming, Observers, Reactive power, Runtime, reactive programming, Libraries, Data models, RP Debugging, programming environments]
Revisit of Automatic Debugging via Human Focus-Tracking Analysis
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
In many fields of software engineering, studies on human behavior have attracted a lot of attention; however, few such studies exist in automated debugging. Parnin and Orso conducted a pioneering study comparing the performance of programmers in debugging with and without a ranking-based fault localization technique, namely Spectrum-Based Fault Localization (SBFL). In this paper, we revisit the actual helpfulness of SBFL, by addressing some major problems that were not resolved in Parnin and Orso's study. Our investigation involved 207 participants and 17 debugging tasks. A user-friendly SBFL tool was adopted. It was found that SBFL tended not to be helpful in improving the efficiency of debugging. By tracking and analyzing programmers' focus of attention, we characterized their source code navigation patterns and provided in-depth explanations to the observations. Results indicated that (1) a short &#x201C;first scan&#x201D; on the source code tended to result in inefficient debugging; and (2) inspections on the pinpointed statements during the &#x201C;follow-up browsing&#x201D; were normally just quick skimming. Moreover, we found that the SBFL assistbrowsing&#x201D; were normally just quick skimming. Moreover, we found that the SBFL assistanceance may even slightly weaken programmers' abilities in fault detection. Our observations imply interference between the mechanism of automated fault localization and the actual assistance needed by programmers in debugging. To resolve this interference, we provide several insights and suggestions.
[human focus-tracking analysis, program debugging, automatic debugging, fault comprehension, Navigation, spectrum-based fault localization, program diagnostics, Debugging, Interference, attention tracking, Automated debugging, Fault detection, Computer bugs, user studies, human behavior, navigation pattern, debugging efficiency, follow-up browsing, Software, software engineering, SBFL assist browsing, ranking-based fault localization technique, Software engineering]
RETracer: Triaging Crashes by Reverse Execution from Partial Memory Dumps
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Many software providers operate crash reporting services to automatically collect crashes from millions of customers and file bug reports. Precisely triaging crashes is necessary and important for software providers because the millions of crashes that may be reported every day are critical in identifying high impact bugs. However, the triaging accuracy of existing systems is limited, as they rely only on the syntactic information of the stack trace at the moment of a crash without analyzing program semantics. In this paper, we present RETracer, the first system to triage software crashes based on program semantics reconstructed from memory dumps. RETracer was designed to meet the requirements of large-scale crash reporting services. RETracer performs binary-level backward taint analysis without a recorded execution trace to understand how functions on the stack contribute to the crash. The main challenge is that the machine state at an earlier time cannot be recovered completely from a memory dump, since most instructions are information destroying. We have implemented RETracer for x86 and x86-64 native code, and compared it with the existing crash triaging tool used by Microsoft. We found that RETracer eliminates two thirds of triage errors based on a manual analysis of 140 bugs fixed in Microsoft Windows and Office. RETracer has been deployed as the main crash triaging system on Microsoft's crash reporting service.
[program debugging, software crashes triage, Instruction sets, reverse execution, Registers, program semantics, Semantics, machine state, software crash triaging, RETracer, stack trace, program diagnostics, large-scale crash reporting services, Microsoft Office, syntactic information, programming language semantics, partial memory dumps, software providers, backward taint analysis, file bug reports, Microsoft Windows, binary-level backward taint analysis, Microsoft crash reporting service, triaging, Computer bugs, high impact bugs, x86-64 native code, x86 native code]
Are "Non-functional" Requirements really Non-functional? An Investigation of Non-functional Requirements in Practice
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Non-functional requirements (NFRs) are commonly distinguished from functional requirements by differentiating how the system shall do something in contrast to what the system shall do. This distinction is not only prevalent in research, but also influences how requirements are handled in practice. NFRs are usually documented separately from functional requirements, without quantitative measures, and with relatively vague descriptions.As a result, they remain difficult to analyze and test.Several authors argue, however, that many so-called NFRs actually describe behavioral properties and may be treated the same way as functional requirements. In this paper, we empirically investigate this point of view and aim to increase our understanding on the nature of NFRs addressing system properties. We report on the classification of 530 NFRs extracted from 11 industrial requirements specifications and analyze to which extent these NFRs describe system behavior.Our results suggest that most "non-functional" requirements are not non-functional as they describe behavior of a system. Consequently, we argue that many so-called NFRs can be handled similarly to functional requirements.
[Non-functional requirements, Unified modeling language, Documentation, Security, classification, quantitative measures, empirical studies, nonfunctional requirements, model-based development, Software, software engineering, NFR, Interviews, Software engineering]
Probing for Requirements Knowledge to Stimulate Architectural Thinking
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Software requirements specifications (SRSs) often lack the detail needed to make informed architectural decisions. Architects therefore either make assumptions, which can lead to incorrect decisions, or conduct additional stakeholder interviews, resulting in potential project delays. We previously observed that software architects ask Probing Questions (PQs) to gather information crucial to architectural decision-making. Our goal is to equip Business Analysts with appropriate PQs so that they can ask these questions themselves. We report a new study with over 40 experienced architects to identify reusable PQs for five areas of functionality and organize them into structured flows. These PQflows can be used by Business Analysts to elicit and specify architecturally relevant information. Additionally, we leverage machine learning techniques to determine when a PQ-flow is appropriate for use in a project, and to annotate individual PQs with relevant information extracted from the existing SRS. We trained and evaluated our approach on over 8,000 individual requirements from 114 requirements specifications and also conducted a pilot study to validate its usefulness.
[probing questions, Barium, software requirements specifications, Probing Questions (PQs), PQ-flows, machine learning techniques, formal specification, PQ-flow, software architecture, requirements knowledge, SRS, automated requirement classification, Batch production systems, Software, functional requirements, Stakeholders, Requirements engineering, learning (artificial intelligence), Architecturally significant requirements, Interviews, architecturally significant requirements]
Risk-Driven Revision of Requirements Models
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Requirements incompleteness is often the result of unanticipated adverse conditions which prevent the software and its environment from behaving as expected. These conditions represent risks that can cause severe software failures. The identification and resolution of such risks is therefore a crucial step towards requirements completeness. Obstacle analysis is a goal-driven form of risk analysis that aims at detecting missing conditions that can obstruct goals from being satisfied in a given domain, and resolving them. This paper proposes an approach for automatically revising goals that may be under-specified or (partially) wrong to resolve obstructions in a given domain. The approach deploys a learning-based revision methodology in which obstructed goals in a goal model are iteratively revised from traces exemplifying obstruction and non-obstruction occurrences. Our revision methodology computes domain-consistent, obstruction-free revisions that are automatically propagated to other goals in the model in order to preserve the correctness of goal models whilst guaranteeing minimal change to the original model. We present the formal foundations of our learning-based approach, and show that it preserves the properties of our formal framework. We validate it against the benchmarking case study of the London Ambulance Service.
[iterative methods, London Ambulance Service, Computational modeling, Government, learning-based revision methodology, Automobiles, Risk analysis, iterative method, formal specification, Analytical models, obstacle analysis, requirements models, Software, Requirements engineering, learning (artificial intelligence), learning-based approach, risk-driven revision]
Discovering "Unknown Known" Security Requirements
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Security is one of the biggest challenges facing organisations in the modern hyper-connected world. A number of theoretical security models are available that provide best practice security guidelines and are widely utilised as a basis to identify and operationalise security requirements. Such models often capture high-level security concepts (e.g., whitelisting, secure configurations, wireless access control, data recovery, etc.), strategies for operationalising such concepts through specific security controls, and relationships between the various concepts and controls. The threat landscape, however, evolves leading to new tacit knowledge that is embedded in or across a variety of security incidents. These unknown knowns alter, or at least demand reconsideration of the theoretical security models underpinning security requirements. In this paper, we present an approach to discover such unknown knowns through multi-incident analysis. The approach is based on a novel combination of grounded theory and incident fault trees. We demonstrate the effectiveness of the approach through its application to identify revisions to a theoretical security model widely used in industry.
[Industries, Adaptation models, Biological system modeling, grounded theory, fault trees, Security, formal specification, Guidelines, theoretical security model, incident analysis, multiincident analysis, security of data, Logic gates, security requirements, incident fault trees, Security requirements, Fault trees]
Behavioral Log Analysis with Statistical Guarantees
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Scalability is a major challenge for existing behavioral log analysis algorithms, which extract finite-state automaton models or temporal properties from logs generated by running systems. In this paper we present statistical log analysis, which addresses scalability using statistical tools. The key to our approach is to consider behavioral log analysis as a statistical experiment.Rather than analyzing the entire log, we suggest to analyze only a sample of traces from the log and, most importantly, provide means to compute statistical guarantees for the correctness of the analysis result.We present the theoretical foundations of our approach and describe two example applications, to the classic k-Tails algorithm and to the recently presented BEAR algorithm.Finally, based on experiments with logs generated from real-world models and with real-world logs provided to us by our industrial partners, we present extensive evidence for the need for scalable log analysis and for the effectiveness of statistical log analysis.
[Algorithm design and analysis, Context, statistical log analysis, behavioral log analysis algorithm, Scalability, Computational modeling, program diagnostics, Software algorithms, Log analysis, specification mining, Analytical models, k-tails algorithm, Software, BEAR algorithm, statistical analysis, statistical guarantees]
Efficient Large-Scale Trace Checking Using MapReduce
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
The problem of checking a logged event trace against a temporal logic specification arises in many practical cases. Unfortunately, known algorithms for an expressive logic like MTL (Metric Temporal Logic) do not scale with respect to two crucial dimensions: the length of the trace and the size of the time interval of the formula to be checked. The former issue can be addressed by distributed and parallel trace checking algorithms that can take advantage of modern cloud computing and programming frameworks like MapReduce. Still, the latter issue remains open with current state-of-the-art approaches. In this paper we address this memory scalability issue by proposing a new semantics for MTL, called lazy semantics. This semantics can evaluate temporal formulae and boolean combinations of temporal-only formulae at any arbitrary time instant. We prove that lazy semantics is more expressive than point-based semantics and that it can be used as a basis for a correct parametric decomposition of any MTL formula into an equivalent one with smaller, bounded time intervals. We use lazy semantics to extend our previous distributed trace checking algorithm for MTL. The evaluation shows that the proposed algorithm can check formulae with large intervals, on large traces, in a memory-efficient way.
[Measurement, distributed trace checking algorithms, parallel algorithms, Scalability, Heuristic algorithms, Programming, temporal logic, point-based semantics, formal specification, lazy semantics, trace checking, MapReduce, MTL logic, parallel trace checking algorithms, Semantics, Clustering algorithms, large-scale trace checking, temporal logic specification, Data models, Boolean combination, memory scalability issue, metric temporal logic]
Feedback-Directed Instrumentation for Deployed JavaScript Applications
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Many bugs in JavaScript applications manifest themselves as objects that have incorrect property values when a failure occurs. For this type of error, stack traces and log files are often insufficient for diagnosing problems. In such cases, it is helpful for developers to know the control flow path from the creation of an object to a crashing statement. Such crash paths are useful for understanding where the object originated and whether any properties of the object were corrupted since its creation.We present a feedback-directed instrumentation technique for computing crash paths that allows the instrumentation overhead to be distributed over a crowd of users and to reduce it for users who do not encounter the crash. We implemented our technique in a tool, Crowdie, and evaluated it on 10 real-world issues for which error messages and stack traces are insufficient to isolate the problem. Our results show that feedback-directed instrumentation requires 5% to 25% of the program to be instrumented, that the same crash must be observed 3 to 10 times to discover the crash path, and that feedback-directed instrumentation typically slows down execution by a factor 2x-9x compared to 8x-90x for an approach where applications are fully instrumented.
[crash paths, Java, feedback-directed instrumentation technique, crowdsourcing, Instruments, Debugging, JavaScript applications, dynamic analysis, Servers, system recovery, javascript, Reactive power, Computer bugs, instrumentation overhead, CROWDIE, debugging, Software, instrumentation]
DoubleTake: Fast and Precise Error Detection via Evidence-Based Dynamic Analysis
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Programs written in unsafe languages like C and C++ often suffer from errors like buffer overflows, dangling pointers, and memory leaks. Dynamic analysis tools like Valgrind can detect these errors, but their overhead - primarily due to the cost of instrumenting every memory read and write - makes them too heavyweight for use in deployed applications and makes testing with them painfully slow. The result is that much deployed software remains susceptible to these bugs, which are notoriously difficult to track down.This paper presents evidence-based dynamic analysis, an approach that enables these analyses while imposing minimal overhead (under 5%), making it practical for the first time to perform these analyses in deployed settings. The key insight of evidence-based dynamic analysis is that for a class of errors, it is possible to ensure that evidence that they happened at some point in the past remains for later detection. Evidence-based dynamic analysis allows execution to proceed at nearly full speed until the end of an epoch (e.g., a heavyweight system call). It then examines program state to check for evidence that an error occurred at some time during that epoch. If so, it rolls back execution and re-executes the code with instrumentation activated to pinpoint the error.We present DoubleTake, a prototype evidence-based dynamic analysis framework. DoubleTake is practical and easy to deploy, requiring neither custom hardware, compiler, nor operating system support. We demonstrate DoubleTake's generality and efficiency by building dynamic analyses that find buffer overflows, memory use-after-free errors, and memory leaks. Our evaluation shows that DoubleTake is efficient, imposing under 5% overhead on average, making it the fastest such system to date. It is also precise: DoubleTake pinpoints the location of these errors to the exact line and memory addresses where they occur, providing valuable debugging information to programmers.
[Use-After-Free Detection, program debugging, memory use-after-free errors, Instruments, program diagnostics, DoubleTake, buffer overflow, Buffer Overflow Detection, Debugging, Dynamic Analysis, error detection, Software Quality, debugging information, memory leaks, Operating systems, Detectors, Valgrind tool, Hardware, Resource management, evidence-based dynamic analysis, Testing, Software engineering, Leak Detection]
Automated Partitioning of Android Applications for Trusted Execution Environments
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
The co-existence of critical and non-critical applications on computing devices, such as mobile phones, is becoming commonplace. The sensitive segments of a critical application should be executed in isolation on Trusted Execution Environments (TEE) so that the associated code and data can be protected from malicious applications. TEE is supported by different technologies and platforms, such as ARM Trustzone, that allow logical separation of "secure" and "normal" worlds. We develop an approach for automated partitioning of critical Android applications into "client" code to be run in the "normal" world and "TEE commands" encapsulating the handling of confidential data to be run in the "secure" world. We also reduce the overhead due to transitions between the two worlds by choosing appropriate granularity for the TEE commands. The advantage of our proposed solution is evidenced by efficient partitioning of real-world applications.
[Java, Google, Android application partitioning, Humanoid robots, confidential data handling, Security, trusted execution environments, ARM Trustzone, Android (operating system), mobile computing, TEE, Software, Hardware, Androids, trusted computing]
"Jumping Through Hoops": Why do Java Developers Struggle with Cryptography APIs?
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
To protect sensitive data processed by current applications, developers, whether security experts or not, have to rely on cryptography. While cryptography algorithms have become increasingly advanced, many data breaches occur because developers do not correctly use the corresponding APIs. To guide future research into practical solutions to this problem, we perform an empirical investigation into the obstacles developers face while using the Java cryptography APIs, the tasks they use the APIs for, and the kind of (tool) support they desire. We triangulate data from four separate studies that include the analysis of 100 StackOverflow posts, 100 GitHub repositories, and survey input from 48 developers. We find that while developers find it difficult to use certain cryptographic algorithms correctly, they feel surprisingly confident in selecting the right cryptography concepts (e.g., encryption vs. signatures). We also find that the APIs are generally perceived to be too low-level and that developers prefer more task-based solutions.
[Java, application program interfaces, StackOverflow, GitHub repositories, cryptography, cryptography algorithms, Java developers, Encryption, Complexity theory, application program interface, API misuse, Public key, cryptography API, sensitive data protection, empirical software engineering, Libraries, Face, Cryptography]
Finding Security Bugs in Web Applications Using a Catalog of Access Control Patterns
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
We propose a specification-free technique for finding missing security checks in web applications using a catalog of access control patterns in which each pattern models a common access control use case. Our implementation, SPACE, checks that every data exposure allowed by an application's code matches an allowed exposure from a security pattern in our catalog. The only user-provided input is a mapping from application types to the types of the catalog; the rest of the process is entirely automatic. In an evaluation on the 50 most watched Ruby on Rails applications on Github, SPACE reported 33 possible bugs---23 previously unknown security bugs, and 10 false positives.
[Access control, program debugging, object-oriented programming, access control patterns catalog, specification-free technique, Github, Aerospace electronics, Web applications, data exposure, bug finding, Open source software, Rails, access control use case, SPACE, Databases, Computer bugs, security pattern, authorisation, security bugs, access control, Internet, web application security]
Reference Hijacking: Patching, Protecting and Analyzing on Unmodified and Non-rooted Android Devices
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Many efforts have been paid to enhance the security of Android. However, less attention has been given to how to practically adopt the enhancements on off-the-shelf devices. In particular, securing Android devices often requires modifying their write-protected underlying system component files (especially the system libraries) by flashing or rooting devices, which is unacceptable in many realistic cases. In this paper, a novel technique, called reference hijacking, is presented to address the problem. By introducing a specially designed reset procedure, a new execution environment is constructed for the target application, in which the reference to the underlying system libraries will be redirected to the security-enhanced alternatives. The technique can be applicable to both the Dalvik and Android Runtime (ART) environments and to almost all mainstream Android versions (2.x to 5.x). To demonstrate the capability of reference hijacking, we develop three prototype systems, PatchMan, ControlMan, and TaintMan, to enforce specific security enhancements, involving patching vulnerabilities, protecting inter-component communications, and performing dynamic taint analysis for the target application. These three prototypes have been successfully deployed on a number of popular Android devices from different manufacturers, without modifying the underlying system. The evaluation results show that they are effective and do not introduce noticeable overhead. They strongly support that reference hijacking can substantially improve the practicability of many security enhancement efforts for Android.
[Access control, Android security, PatchMan system, Android devices, Humanoid robots, Dalvik environment, Practicability, security enhancements, reset procedure, Android, security-enhanced alternatives, TaintMan system, Android (operating system), mobile computing, ControlMan system, security of data, Prototypes, Security enhancement, reference hijacking technique, Libraries, Malware, Androids, Android Runtime environment]
Building a Theory of Job Rotation in Software Engineering from an Instrumental Case Study
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Job Rotation is an organizational practice in which individuals are frequently moved from a job (or project) to another in the same organization. Studies in other areas have found that this practice has both negative and positive effects on individuals' work. However, there are only few studies addressing this issue in software engineering so far. The goal of our study is to investigate the effects of job rotation on work related factors in software engineering by performing a qualitative case study on a large software organization that uses job rotation as an organizational practice. We interviewed senior managers, project managers, and software engineers that had experienced this practice. Altogether, 48 participants were involved in all phases of this research. Collected data was analyzed using qualitative coding techniques and the results were checked and validated with participants through member checking. Our findings suggest that it is necessary to find balance between the positive effects on work variety and learning opportunities, and negative effects on cognitive workload and performance. Further, the lack of feedback resulting from constant movement among projects and teams may have a negative impact on performance feedback. We conclude that job rotation is an important organizational practice with important positive results. However, managers must be aware of potential negative effects and deploy tactics to balance them. We discuss such tactics in this article.
[Context, work related factors, software development management, Companies, software teams, Multiskilling, job rotation theory, qualitative case study, case study, qualitative coding techniques, cognitive workload, performance, Software, software engineering, cognitive performance, Interviews, job rotation, organisational aspects, organizational practice, Software engineering]
The Challenges of Staying Together While Moving Fast: An Exploratory Study
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
We report on the results of an empirical study conducted with 35 experienced software developers from 22 high-tech companies, including Google, Facebook, Microsoft, Intel, and others. The goal of the study was to elicit challenges that these developers face, potential solutions that they envision to these challenges, and research initiatives that they think would deliver useful results. Challenges identified by the majority of the study participants relate to the collaborative nature of the work: the availability and discoverability of information, communication, collaborative planning and integration with work of others. Almost all participants also addressed the advantages and disadvantages of the current &#x201C;fast to the market&#x201D; trend, and the toll it takes on the quality of the software that they are able to deliver and on their professional and personal satisfaction as software engineers. We describe in depth the identified challenges, supporting our findings with explicit quotes from the study participants. We also put these findings in context of work done by the software engineering community and outline a roadmap for possible future research initiatives.
[industrial software development, Google, challenges, software development management, Companies, software developers, empirical study, Research initiatives, software quality, Intel, information availability, Software, Microsoft, software engineering, Face, information discoverability, Interviews, Facebook, Software engineering]
The Sky Is Not the Limit: Multitasking Across GitHub Projects
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Software development has always inherently required multitasking: developers switch between coding, reviewing, testing, designing, and meeting with colleagues. The advent of software ecosystems like GitHub has enabled something new: the ability to easily switch between projects. Developers also have social incentives to contribute to many projects; prolific contributors gain social recognition and (eventually) economic rewards. Multitasking, however, comes at a cognitive cost: frequent context-switches can lead to distraction, sub-standard work, and even greater stress. In this paper, we gather ecosystem-level data on a group of programmers working on a large collection of projects. We develop models and methods for measuring the rate and breadth of a developers' context-switching behavior, and we study how context-switching affects their productivity. We also survey developers to understand the reasons for and perceptions of multitasking. We find that the most common reason for multitasking is interrelationships and dependencies between projects. Notably, we find that the rate of switching and breadth (number of projects) of a developer's work matter. Developers who work on many projects have higher productivity if they focus on few projects per day. Developers that switch projects too much during the course of a day have lower productivity as they work on more projects overall. Despite these findings, developers perceptions of the benefits of multitasking are varied.
[Productivity, Context, project management, software development, Interrupters, software development management, GitHub projects, multitasking, GitHub, Switches, Multitasking, Encoding, context-switching behavior, ecosystem-level data gathering, productivity, Software]
Quantifying and Mitigating Turnover-Induced Knowledge Loss: Case Studies of Chrome and a Project at Avaya
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
The utility of source code, as of other knowledge artifacts, is predicated on the existence of individuals skilled enough to derive value by using or improving it. Developers leaving a software project deprive the project of the knowledge of the decisions they have made. Previous research shows that the survivors and newcomers maintaining abandoned code have reduced productivity and are more likely to make mistakes. We focus on quantifying the extent of abandoned source files and adapt methods from financial risk analysis to assess the susceptibility of the project to developer turnover. In particular, we measure the historical loss distribution and find (1) that projects are susceptible to losses that are more than three times larger than the expected loss. Using historical simulations we find (2) that projects are susceptible to large losses that are over five times larger than the expected loss. We use Monte Carlo simulations of disaster loss scenarios and find (3) that simplistic estimates of the `truck factor' exaggerate the potential for loss. To mitigate loss from developer turnover, we modify Cataldo et al's coordination requirements matrices. We find (4) that we can recommend the correct successor 34% to 48% of the time. We also find that having successors reduces the expected loss by as much as 15%. Our approach helps large projects assess the risk of turnover thereby making risk more transparent and manageable.
[Adaptation models, knowledge artifacts, Loss measurement, knowledge management, financial risk analysis, Monte Carlo methods, Mining Software Repositories, Turnover, Quantitative Risk Management, turnover-induced knowledge loss, Chrome, Productivity, project management, software development management, source code utility, Knowledge Distribution, software project, historical loss distribution, Truck Factor, truck factor estimation, Software, Successors, Risk management, Monte Carlo simulation, Software engineering]
Quality Experience: A Grounded Theory of Successful Agile Projects without Dedicated Testers
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Context: While successful conventional software development regularly employs separate testing staff, there are successful agile teams with as well as without separate testers. Question: How does successful agile development work without separate testers? What are advantages and disadvantages? Method: A case study, based on Grounded Theory evaluation of interviews and direct observation of three agile teams; one having separate testers, two without. All teams perform long-term development of parts of e-business web portals. Results: Teams without testers use a quality experience work mode centered around a tight field-use feedback loop, driven by a feeling of responsibility, supported by test automation, resulting in frequent deployments. Conclusion: In the given domain, hand-overs to separate testers appear to hamper the feedback loop more than they contribute to quality, so working without testers is preferred. However, Quality Experience is achievable only with modular architectures and in suitable domains.
[Context, program testing, software prototyping, Software Quality Assurance, Companies, agile projects, Industrial Case Study, agile development, e-business Web portals, quality experience, quality of experience, modular architectures, grounded theory evaluation, feedback loop, software architecture, Grounded Theory Methodology, Data collection, Software, Interviews, Agile Development, Portals, Testing]
Code Review Quality: How Developers See It
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
In a large, long-lived project, an effective code review process is key to ensuring the long-term quality of the code base. In this work, we study code review practices of a large, open source project, and we investigate how the developers themselves perceive code review quality. We present a qualitative study that summarizes the results from a survey of 88 Mozilla core developers. The results provide developer insights into how they define review quality, what factors contribute to how they evaluate submitted code, and what challenges they face when performing review tasks. We found that the review quality is primarily associated with the thoroughness of the feedback, the reviewer's familiarity with the code, and the perceived quality of the code itself. Also, we found that while different factors are perceived to contribute to the review quality, reviewers often find it difficult to keep their technical skills up-to-date, manage personal priorities, and mitigate context switching.
[Measurement, long-term quality, open source project, effective code review process, Electronic mail, software quality, Data mining, developer perception, Computer science, Code review, Mozilla core developers, review quality, Computer bugs, survey, Software, Face]
Revisiting Code Ownership and Its Relationship with Software Quality in the Scope of Modern Code Review
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Code ownership establishes a chain of responsibility for modules in large software systems. Although prior work uncovers a link between code ownership heuristics and software quality, these heuristics rely solely on the authorship of code changes. In addition to authoring code changes, developers also make important contributions to a module by reviewing code changes. Indeed, recent work shows that reviewers are highly active in modern code review processes, often suggesting alternative solutions or providing updates to the code changes. In this paper, we complement traditional code ownership heuristics using code review activity. Through a case study of six releases of the large Qt and OpenStack systems, we find that: (1) 67%-86% of developers did not author any code changes for a module, but still actively contributed by reviewing 21%-39% of the code changes, (2) code ownership heuristics that are aware of reviewing activity share a relationship with software quality, and (3) the proportion of reviewers without expertise shares a strong, increasing relationship with the likelihood of having post-release defects. Our results suggest that reviewing activity captures an important aspect of code ownership, and should be included in approximations of it in future studies.
[source code (software), Ownership, Conferences, large software systems, code review activity, large Qt systems, Birds, modern code review processes, code change authorship, software quality, Software Quality, Software design, code ownership heuristics, OpenStack systems, Software quality, Organizations, Software systems, post-release defects, Expertise]
IntEQ: Recognizing Benign Integer Overflows via Equivalence Checking across Multiple Precisions
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Integer overflow (IO) vulnerabilities can be exploited by attackers to compromise computer systems. In the meantime, IOs can be used intentionally by programmers for benign purposes such as hashing and random number generation. Hence, differentiating exploitable and harmful IOs from intentional and benign ones is an important challenge. It allows reducing the number of false positives produced by IO vulnerability detection techniques, helping developers or security analysts to focus on fxing critical IOs without inspecting the numerous false alarms. The difficulty of recognizing benign IOs mainly lies in inferring the intent of programmers from source code. In this paper, we present a novel technique to recognize benign IOs via equivalence checking across multiple precisions. We determine if an IO is benign by comparing the effects of an overflowed integer arithmetic operation in the actual world (with limited precision) and the same operation in the ideal world (with sufficient precision to evade the IO). Specifically, we first extract the data flow path from the overflowed integer arithmetic operation to a security-related program point (i.e., sink) and then create a new version of the path using more precise types with sufficient bits to represent integers so that the IO can be avoided. Using theorem proving we check whether these two versions are equivalent, that is, if they yield the same values at the sink under all possible inputs. If so, the IO is benign. We implement a prototype, named IntEQ, based on the GCC compiler and the Z3 solver, and evaluate it using 26 harmful IO vulnerabilities from 20 real-world programs, and 444 benign IOs from SPECINT 2000, SPECINT 2006, and 7 real-world applications. The experimental results show that IntEQ does not misclassify any harmful IO bugs (no false negatives) and recognizes 355 out of 444 (about 79.95%) benign IOs, whereas the state of the art can only recognize 19 benign IOs.
[SPECINT, program debugging, program verification, IntEQ, harmful IO bugs, data flow analysis, precision, overflowed integer arithmetic operation, Encoding, random number generation, security analysts, Computer science, GCC compiler, security of data, Computer bugs, Integer Overflow, IO vulnerability detection techniques, benign, Software, data flow path extraction, benign integer overflow recognition, equivalence checking, Random number generation, Indexing]
Nomen est Omen: Exploring and Exploiting Similarities between Argument and Parameter Names
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Programmer-provided identifier names convey information about the semantics of a program. This information can complement traditional program analyses in various software engineering tasks, such as bug finding, code completion, and documentation. Even though identifier names appear to be a rich source of information, little is known about their properties and their potential usefulness. This paper presents an empirical study of the lexical similarity between arguments and parameters of methods, which is one prominent situation where names can provide otherwise missing information. The study involves 60 real-world Java programs. We find that, for most arguments, the similarity is either very high or very low, and that short and generic names often cause low similarities. Furthermore, we show that inferring a set of low-similarity parameter names from one set of programs allows for pruning such names in another set of programs. Finally, the study shows that many arguments are more similar to thecorresponding parameter than any alternative argument available in the call site's scope. As applications of our findings, we present an anomaly detection technique that identifies 144 renaming opportunities and incorrect arguments in 14 programs, and a code recommendation system that suggests correct arguments with a precision of 83%.
[Java, text analysis, low-similarity parameter names, program diagnostics, anomaly detection technique, Documentation, lexical similarity, name-based program analysis, Open source software, Semantics, Data collection, software engineering, code recommendation system, real-world Java programs]
Floating-Point Precision Tuning Using Blame Analysis
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
While tremendously useful, automated techniques for tuning the precision of floating-point programs face important scalability challenges. We present Blame Analysis, a novel dynamic approach that speeds up precision tuning. Blame Analysis performs floating-point instructions using different levels of accuracy for their operands. The analysis determines the precision of all operands such that a given precision is achieved in the final result of the program. Our evaluation on ten scientific programs shows that Blame Analysis is successful in lowering operand precision. As it executes the program only once, the analysis is particularly useful when targeting reductions in execution time. In such case, the analysis needs to be combined with search-based tools such as Precimonious. Our experiments show that combining Blame Analysis with Precimonious leads to obtaining better results with significant reduction in analysis time: the optimized programs execute faster (in three cases, we observe as high as 39.9% program speedup) and the combined analysis time is 9&#x00D7; faster on average, and up to 38&#x00D7; faster than Precimonious alone.
[operand precision, Scalability, program diagnostics, Government, floating-point instructions, Search problems, blame analysis approach, Precimonious tool, Tuning, program optimization, floating-point precision tuning, Numerical analysis, Software, Performance analysis, mixed precision, floating point]
Crowdsourcing Program Preconditions via a Classification Game
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Invariant discovery is one of the central problems in software verification. This paper reports on an approach that addresses this problem in a novel way; it crowdsources logical expressions for likely invariants by turning invariant discovery into a computer game. The game, called Binary Fission, employs a classification model. In it, players compose preconditions by separating program states that preserve or violate program assertions. The players have no special expertise in formal methods or programming, and are not specifically aware they are solving verification tasks. We show that Binary Fission players discover concise, general, novel, and human readable program preconditions. Our proof of concept suggests that crowdsourcing offers a feasible and promising path towards the practical application of verification technology.
[Crowdsourcing, Computers, Binary Fission, Technological innovation, Gold, pattern classification, program assertions, crowdsourcing, program verification, verification technology, software verification, computer game, classification game, game with a purpose, program precondition crowdsourcing, invariant discovery, computer games, Games, Software, Software engineering]
Scalable Thread Sharing Analysis
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
We present two scalable algorithms for identifying program locations that access thread-shared data in concurrent programs. The static algorithm, though simple, and without performing the expensive whole program information flow analysis, is much more efficient, less memory-demanding, and even more precise than the classical escape analysis algorithm. The dynamic algorithm, powered by a location- based approach, achieves significant runtime speedups over a precise dynamic escape analysis. Our evaluation on a set of large real world complex multithreaded systems such as Apache Derby and Eclipse shows that our algorithms achieve unprecedented scalability. Used by client applications, our algorithms reduce the recording overhead of a record-replay system by 9X on average (as much as 16X) and increase the runtime logging speed of a data race detector by 32% on average (as much as 52%).
[Algorithm design and analysis, Java, multi-threading, Heuristic algorithms, Instruction sets, program diagnostics, location-based approach, multithreaded systems, Eclipse, concurrent programs, program locations identification, scalable algorithms, thread-shared data access, Runtime, thread sharing analysis, data race detector, Detectors, record-replay system, static algorithm, dynamic algorithm, Apache Derby, Arrays, program information flow analysis]
Fixing Deadlocks via Lock Pre-Acquisitions
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Manual deadlock fixing is error-prone and time-consuming. Exist-ing generic approach (GA) simply inserts gate locks to fix dead-locks by serializing executions, which could introduce various new deadlocks and incur high runtime overhead. We propose a novel approach DFixer to fix deadlocks without introducing any new deadlocks by design. DFixer only selects one thread of a deadlock to pre-acquire a lock w together with another lock h, where before fixing, the deadlock occurs when the thread holds lock h and waits for lock w. As such, DFixer eliminates a hold-and-wait necessary condition, preventing the deadlock from occurring. The thread per-forming pre-acquisition is carefully selected such that no other syn-chronization exists in between the two original acquisitions. Other-wise, DFixer further introduces a context-aware conditional protect-ed by above lock w to guarantee the correctness of DFixer. The evaluation is on 20 deadlocks, including 17 from widely-used real-world C/C++ programs. It shows that DFixer successfully fixed all deadlocks. Whereas GA introduced 9 new deadlocks; a latest work Grail failed to fix 8 deadlocks and introduced 3 new deadlocks on others. On average, DFixer incurred only 2.1% overhead, where GA and Grail incurred 15.8% and 11.5% overhead, respectively.
[fixing, DFixer, Synchronization, C++ programs, system recovery, Concurrent computing, Deadlock, lock pre-acquisitions, Computer bugs, concurrency control, C programs, System recovery, Logic gates, multithreaded program, Software, lock order, manual deadlock fixing, Message systems]
Coverage-Driven Test Code Generation for Concurrent Classes
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Previous techniques on concurrency testing have mainly focused on exploring the interleaving space of manually written test code to expose faulty interleavings of shared memory accesses. These techniques assume the availability of failure-inducing tests. In this paper, we present AutoConTest, a coverage-driven approach to generate effective concurrent test code that achieve high interleaving coverage. AutoConTest consists of three components. First, it computes the coverage requirements dynamically and iteratively during sequential test code generation, using a coverage metric that captures the execution context of shared memory accesses. Second, it smartly selects these sequential codes based on the computed result and assembles them for concurrent tests, achieving increased context-sensitive interleaving coverage. Third, it explores the newly covered interleavings. We have implemented AutoConTest as an automated tool and evaluated it using 6 real-world concurrent Java subjects. The results show that AutoConTest is able to generate effective concurrent tests that achieve high interleaving coverage and expose concurrency faults quickly. AutoConTest took less than 65 seconds (including program analysis, test generation and execution) to expose the faults in the program subjects.
[shared memory accesses, context-sensitive interleaving coverage, program testing, Instruction sets, Interleaving coverage criteria, sequential test code generation, Automated test generation, Programming, coverage-driven test code generation, concurrent test code, Synchronization, sequential codes, AutoConTest, Concurrent computing, shared memory systems, Space exploration, concurrency (computers), Testing, coverage metric]
Locking Discipline Inference and Checking
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Concurrency is a requirement for much modern software, but the implementation of multithreaded algorithms comes at the risk of errors such as data races.Programmers can prevent data races by documenting and obeying a locking discipline, which indicates which locks must be held in order to access which data.This paper introduces a formal semantics for locking specifications that gives a guarantee of race freedom.A notable difference from most other semantics is that it is in terms of values (which is what the runtime system locks) rather than variables.The paper also shows how to express the formal semantics in two different styles of analysis:abstract interpretation and type theory.We have implemented both analyses, in tools that operate on Java.To the best of our knowledge, these are the first tools that can soundly infer and check a locking discipline for Java.Our experiments compare the implementations with one another and with annotations written by programmers, showing that the ambiguities and unsoundness of previous formulations are a problem in practice.
[Java, locking specifications, multi-threading, abstract interpretation, type theory, Synchronization, formal specification, Concurrent computing, Runtime, formal semantics, multithreaded algorithms, Semantics, concurrency control, locking discipline, Software, Monitoring, software concurrency]
Improving Refactoring Speed by 10X
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Refactoring engines are standard tools in today's Integrated Development Environments (IDEs). They allow programmers to perform one refactoring at a time, but programmers need more. Most design patterns in the Gang-of-Four text can be written as a refactoring script - a programmatic sequence of refactorings. In this paper, we present R3, a new Java refactoring engine that supports refactoring scripts. It builds a main-memory, non-persistent database to encode Java entity declarations (e.g., packages, classes, methods), their containment relationships, and language features such as inheritance and modifiers. Unlike classical refactoring engines that modify Abstract Syntax Trees (ASTs), R3 refactorings modify only the database; refactored code is produced only when pretty-printing ASTs that reference database changes. R3 performs comparable precondition checks to those of the Eclipse Java Development Tools (JDT) but R3's codebase is about half the size of the JDT refactoring engine and runs an order of magnitude faster. Further, a user study shows that R3 improved the success rate of retrofitting design patterns by 25% up to 50%.
[Java refactoring engine, Java, AST, Gang-of-Four text, refactoring script, Java entity declarations, Maintenance engineering, IDE, software maintenance, refactoring sequence, Engines, refactoring speed, abstract syntax trees, Graphics, Databases, Computer bugs, retrofitting design patterns, integrated development environment, Eclipse Java development tools, JDT, Graphical user interfaces]
SourcererCC: Scaling Code Clone Detection to Big-Code
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
Despite a decade of active research, there has been a marked lack in clone detection techniques that scale to large repositories for detecting near-miss clones. In this paper, we present a token-based clone detector, SourcererCC, that can detect both exact and near-miss clones from large inter-project repositories using a standard workstation. It exploits an optimized inverted-index to quickly query the potential clones of a given code block. Filtering heuristics based on token ordering are used to significantly reduce the size of the index, the number of code-block comparisons needed to detect the clones, as well as the number of required token-comparisons needed to judge a potential clone. We evaluate the scalability, execution time, recall and precision of SourcererCC, and compare it to four publicly available and state-of-the-art tools. To measure recall, we use two recent benchmarks: (1) a big benchmark of real clones, BigCloneBench, and (2) a Mutation/Injection-based framework of thousands of fine-grained artificial clones. We find SourcererCC has both high recall and precision, and is able to scale to a large inter-project repository (25K projects, 250MLOC) using a standard workstation.
[Scalability, mutation-injection-based framework, SourcererCC, precision, scalability, token-comparisons, code-block comparisons, Detectors, big-code, recall, Benchmark testing, software engineering, code clone detection technique, Java, clone detection, open-source Java systems, Cloning, BigCloneBench, Indexes, Standards, software repositories, code clones, large software repositories, Software, large-scale]
Understanding Asynchronous Interactions in Full-Stack JavaScript
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
JavaScript has become one of the most popular languages in practice. Developers now use JavaScript not only for the client-side but also for server-side programming, leading to "full-stack" applications written entirely in JavaScript. Understanding such applications is challenging for developers, due to the temporal and implicit relations of asynchronous and event-driven entities spread over the client and server side. We propose a technique for capturing a behavioural model of full-stack JavaScript applications' execution. The model is temporal and context-sensitive to accommodate asynchronous events, as well as the scheduling and execution of lifelines of callbacks. We present a visualization of the model to facilitate program understanding for developers. We implement our approach in a tool, called Sahand, and evaluate it through a controlled experiment. The results show that Sahand improves developers' performance in completing program comprehension tasks by increasing their accuracy by a factor of three.
[Visualization, Java, Program comprehension, object-oriented programming, program comprehension task, full-stack JavaScript, Servers, asynchronous events, Concurrent computing, Reactive power, program understanding, Sahand, asynchronicity, Writing, asynchronous interaction, server-side programming, Context modeling]
Shadow of a Doubt: Testing for Divergences between Software Versions
2016 IEEE/ACM 38th International Conference on Software Engineering
None
2016
While developers are aware of the importance of comprehensively testing patches, the large effort involved in coming up with relevant test cases means that such testing rarely happens in practice. Furthermore, even when test cases are written to cover the patch, they often exercise the same behaviour in the old and the new version of the code. In this paper, we present a symbolic execution-based technique that is designed to generate test inputs that cover the new program behaviours introduced by a patch. The technique works by executing both the old and the new version in the same symbolic execution instance, with the old version shadowing the new one. During this combined shadow execution, whenever a branch point is reached where the old and the new version diverge, we generate a test case exercising the divergence and comprehensively test the new behaviours of the new version. We evaluate our technique on the Coreutils patches from the CoREBench suite of regression bugs, and show that it is able to generate test inputs that exercise newly added behaviours and expose some of the regression bugs.
[Coreutils patches, program debugging, program testing, symbolic execution-based technique, Random access memory, shadow execution, software versions, regression bugs, divergence testing, CoREBench, Shadow mapping, Computer bugs, Software, Concrete, Performance analysis, Testing]
Foreword
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Provides a listing of current committee members and society officers.
[]
Program Committee
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Provides a listing of current committee members and society officers.
[]
Semantically Enhanced Software Traceability Using Deep Learning Techniques
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts, however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links, however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing.
[source code (software), domain knowledge, Recurrent neural networks, program testing, machine learning technique, bidirectional gated recurrent unit, latent semantic indexing, Semantic Representation, sentence semantics, safety-critical software, test cases, positive train control domain, requirements artifacts, word vectors, Training, information retrieval technique, recurrent neural network model, Traceability, Semantics, Recurrent Neural Network, Natural language processing, learning (artificial intelligence), trace link generation, domain corpus, program diagnostics, recurrent neural nets, vector space model, information retrieval, source code, programming language semantics, safety-critical domains, Standards, Deep Learning, semantically enhanced software traceability, vectors, knowledge representation, requirements artifact semantics, Machine learning, deep learning techniques, Software, word embedding model, tracing network architecture, BI-GRU]
Can Latent Topics in Source Code Predict Missing Architectural Tactics?
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Architectural tactics such as heartbeat, resource pooling, and scheduling provide solutions to satisfy reliability, security, performance, and other critical characteristics of a software system. Current design practices advocate rigorous up-front analysis of the system's quality concerns to identify tactics and where in the code they should be used. In this paper, we explore a bottom-up approach to recommend architectural tactics based on latent topics discovered in the source code of projects. We present a recommender system developed by building predictor models which capture relationships between topical concepts in source code and the use of specific architectural tactics in that code. Based on an extensive analysis of over 116,000 open source systems, we identify significant correlations between latent topics in source code and the usage of architectural tactics. We use this information to construct a predictor for generating tactic recommendations. Our approach is validated through a series of experiments which demonstrate the ability to generate package-level tactic recommendations. We provide further validation via two large-scale studies of Apache Hive and Hadoop to illustrate that our recommender system predicts tactics that are actually implemented by developers in later releases.
[source code (software), predictor models, Machine learning algorithms, Architectural design and implementation, public domain software, software reliability, software quality, Security, software system, Training, software architecture, missing architectural tactics prediction, tactic recommender, Recommender systems, source code, bottom-up approach, Hadoop, system quality analysis, Software reliability, package-level tactic recommendation generation, emergent design, recommender systems, recommender system, latent topics, Inference algorithms, Apache Hive, open source systems]
Analyzing APIs Documentation and Code to Detect Directive Defects
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Application Programming Interface (API) documents represent one of the most important references for API users. However, it is frequently reported that the documentation is inconsistent with the source code and deviates from the API itself. Such inconsistencies in the documents inevitably confuse the API users hampering considerably their API comprehension and the quality of software built from such APIs. In this paper, we propose an automated approach to detect defects of API documents by leveraging techniques from program comprehension and natural language processing. Particularly, we focus on the directives of the API documents which are related to parameter constraints and exception throwing declarations. A first-order logic based constraint solver is employed to detect such defects based on the obtained analysis results. We evaluate our approach on parts of well documented JDK 1.8 APIs. Experiment results show that, out of around 2000 API usage constraints, our approach can detect 1158 defective document directives, with a precision rate of 81.6%, and a recall rate of 82.0%, which demonstrates its practical feasibility.
[exception throwing declarations, JDK 1.8 API, application program interfaces, system documentation, parameter constraints, API code analysis, first-order logic based constraint solver, software quality, API documentation, Data mining, API documentation analysis, formal logic, defective document directives, Natural language processing, application programming interface documents, program comprehension, precision rate, natural language processing, program diagnostics, Documentation, static analysis, software maintenance, Computer science, directive defect detection, recall rate, API comprehension, API usage constraints, Null value, Feature extraction, Software]
An Unsupervised Approach for Discovering Relevant Tutorial Fragments for APIs
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Developers increasingly rely on API tutorials to facilitate software development. However, it remains a challenging task for them to discover relevant API tutorial fragments explaining unfamiliar APIs. Existing supervised approaches suffer from the heavy burden of manually preparing corpus-specific annotated data and features. In this study, we propose a novel unsupervised approach, namely Fragment Recommender for APIs with PageRank and Topic model (FRAPT). FRAPT can well address two main challenges lying in the task and effectively determine relevant tutorial fragments for APIs. In FRAPT, a Fragment Parser is proposed to identify APIs in tutorial fragments and replace ambiguous pronouns and variables with related ontologies and API names, so as to address the pronoun and variable resolution challenge. Then, a Fragment Filter employs a set of non-explanatory detection rules to remove non-explanatory fragments, thus address the non-explanatory fragment identification challenge. Finally, two correlation scores are achieved and aggregated to determine relevant fragments for APIs, by applying both topic model and PageRank algorithm to the retained fragments. Extensive experiments over two publicly open tutorial corpora show that, FRAPT improves the state-of-the-art approach by 8.77% and 12.32% respectively in terms of F-Measure. The effectiveness of key components of FRAPT is also validated.
[variable resolution challenge, Application Programming Interface, Correlation, Fragment Recommender for APIs with PageRank and Topic model, application program interfaces, F-Measure, Manuals, application programming interfaces, Ontologies, Programming, pronoun challenge, PageRank Algorithm, Filtering algorithms, nonexplanatory fragment identification, computer science education, fragment filter, API tutorials, fragment parser, Tutorials, Unsupervised Approaches, unsupervised learning, recommender systems, grammars, unsupervised approach, FRAPT, Software, computer aided instruction, Topic Model]
Detecting User Story Information in Developer-Client Conversations to Generate Extractive Summaries
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
User stories are descriptions of functionality that a software user needs. They play an important role in determining which software requirements and bug fixes should be handled and in what order. Developers elicit user stories through meetings with customers. But user story elicitation is complex, and involves many passes to accommodate shifting and unclear customer needs. The result is that developers must take detailed notes during meetings or risk missing important information. Ideally, developers would be freed of the need to take notes themselves, and instead speak naturally with their customers. This paper is a step towards that ideal. We present a technique for automatically extracting information relevant to user stories from recorded conversations between customers and developers. We perform a qualitative study to demonstrate that user story information exists in these conversations in a sufficient quantity to extract automatically. From this, we found that roughly 10.2% of these conversations contained user story information. Then, we test our technique in a quantitative study to determine the degree to which our technique can extract user story information. In our experiment, our process obtained about 70.8% precision and 18.3% recall on the information.
[Algorithm design and analysis, user story information detection, developer-client conversations, extractive summaries, Software algorithms, transcripts, Electronic mail, Data mining, software functionality, productivity, Computer bugs, developer communication, Software, software engineering, user story generation, Software engineering]
Clone Refactoring with Lambda Expressions
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Lambda expressions have been introduced in Java 8 to support functional programming and enable behavior parameterization by passing functions as parameters to methods. The majority of software clones (duplicated code) are known to have behavioral differences (i.e., Type-2 and Type-3 clones). However, to the best of our knowledge, there is no previous work to investigate the utility of Lambda expressions for parameterizing such behavioral differences in clones. In this paper, we propose a technique that examines the applicability of Lambda expressions for the refactoring of clones with behavioral differences. Moreover, we empirically investigate the applicability and characteristics of the Lambda expressions introduced to refactor a large dataset of clones. Our findings show that Lambda expressions enable the refactoring of a significant portion of clones that could not be refactored by any other means.
[Java, lambda calculus, Code duplication, functional programming, Cloning, Europe, Type-2 clones, Type-3 clones, Data mining, software maintenance, Open source software, Java 8, software clones, lambda expressions, Lambda expressions, clone refactoring, duplicated code, behavior parameterization, Refactoring, behavioral differences, Testing]
Characterizing and Detecting Anti-Patterns in the Logging Code
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Snippets of logging code are output statements (e.g., LOG.info or System.out.println) that developers insert into a software system. Although more logging code can provide more execution context of the system's behavior during runtime, it is undesirable to instrument the system with too much logging code due to maintenance overhead. Furthermore, excessive logging may cause unexpected side-effects like performance slow-down or high disk I/O bandwidth. Recent studies show that there are no well-defined coding guidelines for performing effective logging. Previous research on the logging code mainly tackles the problems of where-to-log and what-to-log. There are very few works trying to address the problem of how-to-log (developing and maintaining high-quality logging code). In this paper, we study the problem of how-to-log by characterizing and detecting the anti-patterns in the logging code. As the majority of the logging code is evolved together with the feature code, the remaining set of logging code changes usually contains the fixes to the anti-patterns. We have manually examined 352 pairs of independently changed logging code snippets from three well-maintenance open source systems: ActiveMQ, Hadoop and Maven. Our analysis has resulted in six different anti-patterns in the logging code. To demonstrate the value of our findings, we have encoded these anti-patterns into a static code analysis tool, LCAnalyzer. Case studies show that LCAnalyzer has an average recall of 95% and precision of 60% and can be used to automatically detect previously unknown anti-patterns in the source code. To gather feedback, we have filed 64 representative instances of the logging code anti-patterns from the most recent releases of ten open source software systems. Among them, 46 instances (72%) have already been accepted by their developers.
[source code (software), Maven, public domain software, Data mining, parallel processing, Open source software, Runtime, high disk I/O bandwidth, static code analysis tool, open source software systems, anti-pattern detection, logging code, program diagnostics, maintenance overhead, Hadoop, source code, Tools, anti-patterns, anti-pattern characterization, Computer crashes, software maintenance, ActiveMQ, how-to-log problem, empirical studies, logging code snippets, Software systems, data handling, LCAnalyzer, system behavior, logging practices]
Automated Refactoring of Legacy Java Software to Default Methods
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Java 8 default methods, which allow interfaces to contain (instance) method implementations, are useful for the skeletal implementation software design pattern. However, it is not easy to transform existing software to exploit default methods as it requires analyzing complex type hierarchies, resolving multiple implementation inheritance issues, reconciling differences between class and interface methods, and analyzing tie-breakers (dispatch precedence) with overriding class methods to preserve type-correctness and confirm semantics preservation. In this paper, we present an efficient, fully-automated, type constraint-based refactoring approach that assists developers in taking advantage of enhanced interfaces for their legacy Java software. The approach features an extensive rule set that covers various corner-cases where default methods cannot be used. To demonstrate applicability, we implemented our approach as an Eclipse plug-in and applied it to 19 real-world Java projects, as well as submitted pull requests to popular GitHub repositories. The indication is that it is useful in migrating skeletal implementation methods to interfaces as default methods, sheds light onto the pattern's usage, and provides insight to language designers on how this new construct applies to existing software.
[Printing, Java, fully-automated type constraint-based refactoring approach, software design pattern, interfaces, default methods, GitHub repositories, Eclipse plug-in, legacy Java software, software maintenance, automated refactoring, java, refactoring, Semantics, Syntactics, Software, Concrete, Face]
Supporting Software Developers with a Holistic Recommender System
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
The promise of recommender systems is to provide intelligent support to developers during their programming tasks. Such support ranges from suggesting program entities to taking into account pertinent Q&amp;A pages. However, current recommender systems limit the context analysis to change history and developers' activities in the IDE, without considering what a developer has already consulted or perused, e.g., by performing searches from the Web browser. Given the faceted nature of many programming tasks, and the incompleteness of the information provided by a single artifact, several heterogeneous resources are required to obtain the broader picture needed by a developer to accomplish a task. We present Libra, a holistic recommender system. It supports the process of searching and navigating the information needed by constructing a holistic meta-information model of the resources perused by a developer, analyzing their semantic relationships, and augmenting the web browser with a dedicated interactive navigation chart. The quantitative and qualitative evaluation of Libra provides evidence that a holistic analysis of a developer's information context can indeed offer comprehensive and contextualized support to information navigation and retrieval during software development.
[Mining unstructured data, heterogeneous resources, software developers, Uniform resource locators, holistic meta-information model, programming tasks, interactive programming, software engineering, Recommender systems, Navigation, software development, holistic recommender system, information retrieval, pertinent Q&amp;A pages, information analysis, Browsers, information navigation, program entities, recommender systems, interactive navigation chart, Libra, intelligent support, Web browser, Web pages, information context analysis, online front-ends, User interfaces, information search, Web search]
Recommending and Localizing Change Requests for Mobile Apps Based on User Reviews
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Researchers have proposed several approaches to extract information from user reviews useful for maintaining and evolving mobile apps. However, most of them just perform automatic classification of user reviews according to specific keywords (e.g., bugs, features). Moreover, they do not provide any support for linking user feedback to the source code components to be changed, thus requiring a manual, time-consuming, and error-prone task. In this paper, we introduce CHANGEADVISOR, a novel approach that analyzes the structure, semantics, and sentiments of sentences contained in user reviews to extract useful (user) feedback from maintenance perspectives and recommend to developers changes to software artifacts. It relies on natural language processing and clustering algorithms to group user reviews around similar user needs and suggestions for change. Then, it involves textual based heuristics to determine the code artifacts that need to be maintained according to the recommended software changes. The quantitative and qualitative studies carried out on 44,683 user reviews of 10 open source mobile apps and their original developers showed a high accuracy of CHANGEADVISOR in (i) clustering similar user change requests and (ii) identifying the code components impacted by the suggested changes. Moreover, the obtained results show that ChangeAdvisor is more accurate than a baseline approach for linking user feedback clusters to the source code in terms of both precision (+47%) and recall (+38%).
[source code (software), mobile apps, natural language processing, Impact Analysis, Natural Language Processing, Tools, Maintenance engineering, Mobile communication, CHANGEADVISOR, mobile computing, Mining User Reviews, textual based heuristics, Computer bugs, Feature extraction, Software, user feedback, Mobile Apps, Joining processes, source code components]
Software Development Waste
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Context: Since software development is a complex socio-technical activity that involves coordinating different disciplines and skill sets, it provides ample opportunities for waste to emerge. Waste is any activity that produces no value for the customer or user. Objective: The purpose of this paper is to identify and describe different types of waste in software development. Method: Following Constructivist Grounded Theory, we conducted a two-year five-month participant-observation study of eight software development projects at Pivotal, a software development consultancy. We also interviewed 33 software engineers, interaction designers, and product managers, and analyzed one year of retrospection topics. We iterated between analysis and theoretical sampling until achieving theoretical saturation. Results: This paper introduces the first empirical waste taxonomy. It identifies nine wastes and explores their causes, underlying tensions, and overall relationship to the waste taxonomy found in Lean Software Development. Limitations: Grounded Theory does not support statistical generalization. While the proposed taxonomy appears widely applicable, organizations with different software development cultures may experience different waste types. Conclusion: Software development projects manifest nine types of waste: building the wrong feature or product, mismanaging the backlog, rework, unnecessarily complex solutions, extraneous cognitive load, psychological distress, waiting/multitasking, knowledge loss, and ineffective communication.
[Production systems, lean software development, Taxonomy, Software engineering waste, Programming, constructivist grounded theory, complex socio-technical activity, Extreme Programming, software development waste, Lean Software Development, Software, software engineering, Manufacturing, Interviews, software development consultancy, Pivotal]
Becoming Agile: A Grounded Theory of Agile Transitions in Practice
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Agile adoption is typically understood as a one-off organizational process involving a staged selection of agile development practices. This view of agility fails to explain the differences in the pace and effectiveness of individual teams transitioning to agile development. Based on a Grounded Theory study of 31 agile practitioners drawn from 18 teams across five countries, we present a grounded theory of becoming agile as a network of on-going transitions across five dimensions: software development practices, team practices, management approach, reflective practices, and culture. The unique position of a software team through this network, and their pace of progress along the five dimensions, explains why individual agile teams present distinct manifestations of agility and unique transition experiences. The theory expands the current understanding of agility as a holistic and complex network of on-going multidimensional transitions, and will help software teams, their managers, and organizations better navigate their individual agile journeys.
[software development practices, software prototyping, teams, Finance, software development management, Banking, grounded theory, cultural aspects, Encoding, Telecommunications, reflective practices, selforganizing, transition, team working, team practices, agile development practices, software team, management, culture, management approach, Software, agile software development, Interviews, theory]
From Diversity by Numbers to Diversity as Process: Supporting Inclusiveness in Software Development Teams with Brainstorming
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Negative experiences in diverse software development teams have the potential to turn off minority participants from future team-based software development activity. We examine the use of brainstorming as one concrete team processes that may be used to improve the satisfaction of minority developers when working in a group. Situating our study in time-intensive hackathon-like environments where engagement of all team members is particularly crucial, we use a combination of survey and interview data to test our propositions. We find that brainstorming strategies are particularly effective for team members who identify as minorities, and support satisfaction with both the process and outcomes of teamwork through different mechanisms.
[time-intensive hackathon-like environments, software development, Diversity, Cultural differences, hackathons, satisfaction, teamwork, Organizations, Software, Concrete, software engineering, Teamwork, software engineering management, Software engineering, brainstorming]
Classifying Developers into Core and Peripheral: An Empirical Study on Count and Network Metrics
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Knowledge about the roles developers play in a software project is crucial to understanding the project's collaborative dynamics. In practice, developers are often classified according to the dichotomy of core and peripheral roles. Typically, count-based operationalizations, which rely on simple counts of individual developer activities (e.g., number of commits), are used for this purpose, but there is concern regarding their validity and ability to elicit meaningful insights. To shed light on this issue, we investigate whether count-based operationalizations of developer roles produce consistent results, and we validate them with respect to developers' perceptions by surveying 166 developers. Improving over the state of the art, we propose a relational perspective on developer roles, using fine-grained developer networks modeling the organizational structure, and by examining developer roles in terms of developers' positions and stability within the developer network. In a study of 10 substantial open-source projects, we found that the primary difference between the count-based and our proposed network-based core-peripheral operationalizations is that the network-based ones agree more with developer perception than count-based ones. Furthermore, we demonstrate that a relational perspective can reveal further meaningful insights, such as that core developers exhibit high positional stability, upper positions in the hierarchy, and high levels of coordination with other core developers, which confirms assumptions of previous work.
[Measurement, Systems architecture, mining software repositories, Stability analysis, fine-grained developer network, network-based core-peripheral operationalizations, software project, count-based operationalizations, open-source projects, classification, Open source software, Computer bugs, developer networks, Collaboration, software engineering, Developer roles]
Decoding the Representation of Code in the Brain: An fMRI Study of Code Review and Expertise
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Subjective judgments in software engineering tasks are of critical importance but can be difficult to study with conventional means. Medical imaging techniques hold the promise of relating cognition to physical activities and brain structures. In a controlled experiment involving 29 participants, we examine code comprehension, code review and prose review using functional magnetic resonance imaging. We find that the neural representations of programming languages vs. natural languages are distinct. We can classify which task a participant is undertaking based solely on brain activity (balanced accuracy 79%, p &lt;; 0.001). Further, we find that the same set of brain regions distinguish between code and prose (near-perfect correlation, r = 0.99, p &lt;; 0.001). Finally, we find that task distinctions are modulated by expertise, such that greater skill predicts a less differentiated neural representation (r = -0.44, p = 0.016) indicating that more skilled participants treat code and prose more similarly at a neural activation level.
[Brain, biomedical MRI, fMRI study, Natural languages, Tools, prose review, programming languages, code comprehension, Computer science, medical imaging techniques, neural representations, natural languages, Software, software engineering, software engineering tasks, code review, medical image processing, functional magnetic resonance imaging, medical imaging, Biomedical imaging, Software engineering]
Understanding the Impressions, Motivations, and Barriers of One Time Code Contributors to FLOSS Projects: A Survey
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Successful Free/Libre Open Source Software (FLOSS) projects must attract and retain high-quality talent. Researchers have invested considerable effort in the study of core and peripheral FLOSS developers. To this point, one critical subset of developers that have not been studied are One-Time code Contributors (OTC) - those that have had exactly one patch accepted. To understand why OTCs have not contributed another patch and provide guidance to FLOSS projects on retaining OTCs, this study seeks to understand the impressions, motivations, and barriers experienced by OTCs. We conducted an online survey of OTCs from 23 popular FLOSS projects. Based on the 184 responses received, we observed that OTCs generally have positive impressions of their FLOSS project and are driven by a variety of motivations. Most OTCs primarily made contributions to fix bugs that impeded their work and did not plan on becoming long term contributors. Furthermore, OTCs encounter a number of barriers that prevent them from continuing to contribute to the project. Based on our findings, there are some concrete actions FLOSS projects can take to increase the chances of converting OTCs into long-term contributors.
[free-Libre open source software project, project management, public domain software, Tools, FLOSS projects, Encoding, Electronic mail, Open source software, OSS, Computer science, Newcomers, One Time Contributors, Survey, Computer bugs, FLOSS, Concrete, software engineering, one time code contributors, Qualitative Research, Open source, OTC]
Search-Driven String Constraint Solving for Vulnerability Detection
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Constraint solving is an essential technique for detecting vulnerabilities in programs, since it can reason about input sanitization and validation operations performed on user inputs. However, real-world programs typically contain complex string operations that challenge vulnerability detection. State-of-the-art string constraint solvers support only a limited set of string operations and fail when they encounter an unsupported one, this leads to limited effectiveness in finding vulnerabilities. In this paper we propose a search-driven constraint solving technique that complements the support for complex string operations provided by any existing string constraint solver. Our technique uses a hybrid constraint solving procedure based on the Ant Colony Optimization meta-heuristic. The idea is to execute it as a fallback mechanism, only when a solver encounters a constraint containing an operation that it does not support. We have implemented the proposed search-driven constraint solving technique in the ACO-Solver tool, which we have evaluated in the context of injection and XSS vulnerability detection for Java Web applications. We have assessed the benefits and costs of combining the proposed technique with two state-of-the-art constraint solvers (Z3-str2 and CVC4). The experimental results, based on a benchmark with 104 constraints derived from nine realistic Web applications, show that our approach, when combined in a state-of-the-art solver, significantly improves the number of detected vulnerabilities (from 4.7% to 71.9% for Z3-str2, from 85.9% to 100.0% for CVC4), and solves several cases on which the solver fails when used stand-alone (46 more solved cases for Z3-str2, and 11 more for CVC4), while still keeping the execution time affordable in practice.
[XSS vulnerability detection, Java, Ant colony optimization, search-driven string constraint solving technique, program verification, Search problems, vulnerability detection, Security, complex string operations, Standards, program validation operations, Java Web applications, hybrid constraint solving procedure, Automata, search-based software engineering, Libraries, fallback mechanism, ACO-solver tool, Internet, input sanitization, ant colony optimization metaheuristic, ant colony optimisation, string constraint solving, search problems]
A Guided Genetic Algorithm for Automated Crash Reproduction
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
To reduce the effort developers have to make for crash debugging, researchers have proposed several solutions for automatic failure reproduction. Recent advances proposed the use of symbolic execution, mutation analysis, and directed model checking as underling techniques for post-failure analysis of crash stack traces. However, existing approaches still cannot reproduce many real-world crashes due to such limitations as environment dependencies, path explosion, and time complexity. To address these challenges, we present EvoCrash, a post-failure approach which uses a novel Guided Genetic Algorithm (GGA) to cope with the large search space characterizing real-world software programs. Our empirical study on three open-source systems shows that EvoCrash can replicate 41 (82%) of real-world crashes, 34 (89%) of which are useful reproductions for debugging purposes, outperforming the state-of-the-art in crash replication.
[GGA, program debugging, Search-Based Software Testing, Automated Crash Reproduction, Debugging, post-failure approach, Tools, crash debugging, genetic algorithms, Genetic algorithms, guided genetic algorithm, automatic failure reproduction, Computer bugs, EvoCrash, Model checking, Software, automated crash reproduction, Genetic Algorithms]
Stochastic Optimization of Program Obfuscation
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Program obfuscation is a common practice in software development to obscure source code or binary code, in order to prevent humans from understanding the purpose or logic of software. It protects intellectual property and deters malicious attacks. While tremendous efforts have been devoted to the development of various obfuscation techniques, we have relatively little knowledge on how to most effectively use them together. The biggest challenge lies in identifying the most effective combination of obfuscation techniques. This paper presents a unified framework to optimize program obfuscation. Given an input program P and a set T of obfuscation transformations, our technique can automatically identify a sequence seq = &#x2329;t<sub>1</sub>, t<sub>2</sub>, ..., t<sub>n</sub>&#x232A; (&#x2200;i &#x2208; [1, n]. t<sub>i</sub> &#x2208; T), such that applying ti in order on P yields the optimal obfuscation performance. We model the process of searching for seq as a mathematical optimization problem. The key technical contributions of this paper are: (1) an obscurity language model to assess obfuscation effectiveness/optimality, and (2) a guided stochastic algorithm based on Markov chain Monte Carlo methods to search for the optimal solution seq. We have realized the framework in a tool Closure* for JavaScript, and evaluated it on 25 most starred JavaScript projects on GitHub (19K lines of code). Our machinery study shows that Closure* outperforms the well-known Google Closure Compiler by defending 26% of the attacks initiated by JSNice. Our human study also reveals that Closure* is practical and can reduce the human attack success rate by 30%.
[obscurity language model, Google, Java, mathematical optimization problem, software development, guided stochastic algorithm, source code, markov chain monte carlo methods, Optimization, stochastic optimization, Reactive power, Monte Carlo methods, program obfuscation, JavaScript, Markov processes, Software, software engineering, binary code, Mathematical model, Markov chain Monte Carlo methods, Lenses]
ZenIDS: Introspective Intrusion Detection for PHP Applications
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Since its first appearance more than 20 years ago, PHP has steadily increased in popularity, and has become the foundation of the Internet's most popular content management systems (CMS). Of the world's 1 million most visited websites, nearly half use a CMS, and WordPress alone claims 25% market share of all websites. While their easy-to-use templates and components have greatly simplified the work of developing high quality websites, it comes at the cost of software vulnerabilities that are inevitable in such large and rapidly evolving frameworks. Intrusion Detection Systems (IDS) are often used to protect Internet-facing applications, but conventional techniques struggle to keep up with the fast pace of development in today's web applications. Rapid changes to application interfaces increase the workload of maintaining an IDS whitelist, yet the broad attack surface of a web application makes for a similarly verbose blacklist. We developed ZenIDS to dynamically learn the trusted execution paths of an application during a short online training period and report execution anomalies as potential intrusions. We implement ZenIDS as a PHP extension supported by 8 hooks instrumented in the PHP interpreter. Our experiments demonstrate its effectiveness monitoring live web traffic for one year to 3 large PHP applications, detecting malicious requests with a false positive rate of less than .01% after training on fewer than 4,000 requests. ZenIDS excludes the vast majority of deployed PHP code from the whitelist because it is never used for valid requests-yet could potentially be exploited by a remote adversary. We observe 5% performance overhead (or less) for our applications vs. an optimized vanilla LAMP stack.
[vanilla LAMP stack, intrusion detection systems, Content management, malicious requests detection, Training, ZENIDS, authoring languages, security of data, Authentication, Intrusion detection, introspective intrusion detection, system monitoring, PHP interpreter, Internet, live Web traffic monitoring, PHP applications, PHP extension, Monitoring]
Statically Checking Web API Requests in JavaScript
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Many JavaScript applications perform HTTP requests to web APIs, relying on the request URL, HTTP method, and request data to be constructed correctly by string operations. Traditional compile-time error checking, such as calling a non-existent method in Java, are not available for checking whether such requests comply with the requirements of a web API. In this paper, we propose an approach to statically check web API requests in JavaScript. Our approach first extracts a request's URL string, HTTP method, and the corresponding request data using an inter-procedural string analysis, and then checks whether the request conforms to given web API specifications. We evaluated our approach by checking whether web API requests in JavaScript files mined from GitHub are consistent or inconsistent with publicly available API specifications. From the 6575 requests in scope, our approach determined whether the request's URL and HTTP method was consistent or inconsistent with web API specifications with a precision of 96.0%. Our approach also correctly determined whether extracted request data was consistent or inconsistent with the data requirements with a precision of 87.9% for payload data and 99.9% for query data. In a systematic analysis of the inconsistent cases, we found that many of them were due to errors in the client code. The here proposed checker can be integrated with code editors or with continuous integration tools to warn programmers about code containing potentially erroneous requests.
[Java, application program interfaces, HTTP requests, program diagnostics, URL string, GitHub, Tools, Media, Data mining, Uniform resource locators, Reactive power, JavaScript, Static analysis, Writing, interprocedural string analysis, Web API requests, Web APIs, compile-time error checking, Payloads]
On Cross-Stack Configuration Errors
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Today's web applications are deployed on powerful software stacks such as MEAN (JavaScript) or LAMP (PHP), which consist of multiple layers such as an operating system, web server, database, execution engine and application framework, each of which provide resources to the layer just above it. These powerful software stacks unfortunately are plagued by so-called cross-stack configuration errors (CsCEs), where a higher layer in the stack suddenly starts to behave incorrectly or even crash due to incorrect configuration choices in lower layers. Due to differences in programming languages and lack of explicit links between configuration options of different layers, sysadmins and developers have a hard time identifying the cause of a CsCE, which is why this paper (1) performs a qualitative analysis of 1,082 configuration errors to understand the impact, effort and complexity of dealing with CsCEs, then (2) proposes a modular approach that plugs existing source code analysis (slicing) techniques, in order to recommend the culprit configuration option. Empirical evaluation of this approach on 36 real CsCEs of the top 3 LAMP stack layers shows that our approach reports the misconfigured option with an average rank of 2.18 for 32 of the CsCEs, and takes only few minutes, making it practically useful.
[source code (software), Web applications, Slicing, programming languages, culprit configuration option, CsCE, Databases, Operating systems, configuration errors, Empirical Study, PHP, Multi-layer Systems, cross-stack configuration errors, program slicing, Testing, Software Configuration, Debugging, Web servers, Qualitative Study, software stacks, qualitative analysis, Internet, source code slicing, source code analysis, LAMP stack layers, Software Stack]
Efficient Detection of Thread Safety Violations via Coverage-Guided Generation of Concurrent Tests
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
As writing concurrent programs is challenging, developers often rely on thread-safe classes, which encapsulate most synchronization issues. Testing such classes is crucial to ensure the correctness of concurrent programs. An effective approach to uncover otherwise missed concurrency bugs is to automatically generate concurrent tests. Existing approaches either create tests randomly, which is inefficient, build on a computationally expensive analysis of potential concurrency bugs exposed by sequential tests, or focus on exposing a particular kind of concurrency bugs, such as atomicity violations. This paper presents CovCon, a coverage-guided approach to generate concurrent tests. The key idea is to measure how often pairs of methods have already been executed concurrently and to focus the test generation on infrequently or not at all covered pairs of methods. The approach is independent of any particular bug pattern, allowing it to find arbitrary concurrency bugs, and is computationally inexpensive, allowing it to generate many tests in short time. We apply CovCon to 18 thread-safe Java classes, and it detects concurrency bugs in 17 of them. Compared to five state of the art approaches, CovCon detects more bugs than any other approach while requiring less time. Specifically, our approach finds bugs faster in 38 of 47 cases, with speedups of at least 4x for 22 of 47 cases.
[coverage-guided generation, bug pattern, thread-safe Java classes, coverage, Schedules, Java, program debugging, program testing, Instruction sets, Synchronization, synchronisation, concurrency, Concurrent computing, test generation, thread safety violation detection, sequential tests, Computer bugs, concurrent tests, synchronization, missed concurrency bugs, concurrent program writing, concurrency (computers), Testing, CovCon]
RClassify: Classifying Race Conditions in Web Applications via Deterministic Replay
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Race conditions are common in web applications but are difficult to diagnose and repair. Although there exist tools for detecting races in web applications, they all report a large number of false positives. That is, the races they report are either bogus, meaning they can never occur in practice, or benign, meaning they do not lead to erroneous behaviors. Since manually diagnosing them is tedious and error prone, reporting these race warnings to developers would be counter-productive. We propose a platform-agnostic, deterministic replay-based method for identifying not only the real but also the truly harmful race conditions. It relies on executing each pair of racing events in two different orders and assessing their impact on the program state: we say a race is harmful only if (1) both of the two executions arefeasible and (2) they lead to different program states. We have evaluated our evidence-based classification method on a large set of real websites from Fortune-500 companies and demonstrated that it significantly outperforms all state-of-the-art techniques.
[pattern classification, platform-agnostic deterministic replay-based method, web application, Companies, Tools, Web applications, program state, program repair, HTML, Browsers, race condition classification, Standards, evidence-based classification method, JavaScript, Benchmark testing, Race condition, Robustness, Internet, deterministic replay, Web sites, RClassify]
Repairing Event Race Errors by Controlling Nondeterminism
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Modern web applications are written in an event-driven style, in which event handlers execute asynchronously in response to user or system events. The nondeterminism arising from this programming style can lead to pernicious errors. Recent work focuses on detecting event races and classifying them as harmful or harmless. However, since modifying the source code to prevent harmful races can be a difficult and error-prone task, it may be preferable to steer away from the bad executions. In this paper, we present a technique for automated repair of event race errors in JavaScript web applications. Our approach relies on an event controller that restricts event handler scheduling in the browser according to a specified repair policy, by intercepting and carefully postponing or discarding selected events. We have implemented the technique in a tool called EventRaceCommander, which relies entirely on source code instrumentation, and evaluated it by repairing more than 100 event race errors that occur in the web applications from the largest 20 of the Fortune 500 companies. Our results show that application-independent repair policies usually suffice to repair event race errors without excessive negative impact on performance or user experience, though application-specific repair policies that target specific event races are sometimes desirable.
[Schedules, object-oriented programming, source code instrumentation, Instruments, Thumb, high level languages, Maintenance engineering, Tools, Programming, automated repair, Browsers, application-specific repair policies, software maintenance, event handler scheduling, EventRaceCommander, event-driven programming, application-independent repair policies, online front-ends, JavaScript Web applications, browser, JavaScript, event race error repair, Internet]
Making Malory Behave Maliciously: Targeted Fuzzing of Android Execution Environments
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Android applications, or apps, provide useful features to end-users, but many apps also contain malicious behavior. Modern malware makes understanding such behavior challenging by behaving maliciously only under particular conditions. For example, a malware app may check whether it runs on a real device and not an emulator, in a particular country, and alongside a specific target app, such as a vulnerable banking app. To observe the malicious behavior, a security analyst must find out and emulate all these app-specific constraints. This paper presents FuzzDroid, a framework for automatically generating an Android execution environment where an app exposes its malicious behavior. The key idea is to combine an extensible set of static and dynamic analyses through a search-based algorithm that steers the app toward a configurable target location. On recent malware, the approach reaches the target location in 75% of the apps. In total, we reach 240 code locations within an average time of only one minute. To reach these code locations, FuzzDroid generates 106 different environments, too many for a human analyst to create manually.
[Algorithm design and analysis, invasive software, search-based algorithm, malware, Heuristic algorithms, Instruments, static analysis, Mobile communication, smart phones, security analyst, dynamic analysis, app-specific constraints, Security, code locations, malicious behavior, FuzzDroid, malory, Android applications, Malware, Smart phones, Android execution environment targeted fuzzing]
A SEALANT for Inter-App Security Holes in Android
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Android's communication model has a major security weakness: malicious apps can manipulate other apps into performing unintended operations and can steal end-user data, while appearing ordinary and harmless. This paper presents SEALANT, a technique that combines static analysis of app code, which infers vulnerable communication channels, with runtime monitoring of inter-app communication through those channels, which helps to prevent attacks. SEALANT's extensive evaluation demonstrates that (1) it detects and blocks inter-app attacks with high accuracy in a corpus of over 1,100 real-world apps, (2) it suffers from fewer false alarms than existing techniques in several representative scenarios, (3) its performance overhead is negligible, and (4) end-users do not find it challenging to adopt.
[Sealing materials, program diagnostics, Humanoid robots, static analysis, Inter-app vulnerability, Security, vulnerable communication channels, Android, app code, Runtime, Android (operating system), SEALANT technique, security of data, inter-app communication runtime monitoring, Focusing, inter-app security holes, Android communication model, Androids, Monitoring]
An Efficient, Robust, and Scalable Approach for Analyzing Interacting Android Apps
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
When multiple apps on an Android platform interact, faults and security vulnerabilities can occur. Software engineers need to be able to analyze interacting apps to detect such problems. Current approaches for performing such analyses, however, do not scale to the numbers of apps that may need to be considered, and thus, are impractical for application to real-world scenarios. In this paper, we introduce JITANA, a program analysis framework designed to analyze multiple Android apps simultaneously. By using a classloader-based approach instead of a compiler-based approach such as SOOT, JITANA is able to simultaneously analyze large numbers of interacting apps, perform on-demand analysis of large libraries, and effectively analyze dynamically generated code. Empirical studies of JITANA show that it is substantially more efficient than a state-of-the-art approach, and that it can effectively and efficiently analyze complex apps including Facebook, Pokemon Go, and Pandora that the state-of-the-art approach cannot handle.
[Java, Pandora, Android Apps, Humanoid robots, SOOT, Tools, classloader-based approach, Security, program compilers, JITANA, inter-app communication, Android, Android (operating system), mobile computing, Pokemon Go, program analysis, Libraries, Robustness, Androids, Facebook]
LibD: Scalable and Precise Third-Party Library Detection in Android Markets
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
With the thriving of the mobile app markets, third-party libraries are pervasively integrated in the Android applications. Third-party libraries provide functionality such as advertisements, location services, and social networking services, making multi-functional app development much more productive. However, the spread of vulnerable or harmful third-party libraries may also hurt the entire mobile ecosystem, leading to various security problems. The Android platform suffers severely from such problems due to the way its ecosystem is constructed and maintained. Therefore, third-party Android library identification has emerged as an important problem which is the basis of many security applications such as repackaging detection and malware analysis. According to our investigation, existing work on Android library detection still requires improvement in many aspects, including accuracy and obfuscation resilience. In response to these limitations, we propose a novel approach to identifying third-party Android libraries. Our method utilizes the internal code dependencies of an Android app to detect and classify library candidates. Different from most previous methods which classify detected library candidates based on similarity comparison, our method is based on feature hashing and can better handle code whose package and method names are obfuscated. Based on this approach, we have developed a prototypical tool called LibD and evaluated it with an update-to-date and large-scale dataset. Our experimental results on 1,427,395 apps show that compared to existing tools, LibD can better handle multi-package third-party libraries in the presence of name-based obfuscation, leading to significantly improved precision without the loss of scalability.
[mobile ecosystem, Humanoid robots, LibD tool, Mobile communication, Security, mobile app markets, third-party Android library identification, software mining, software libraries, malware analysis, Android (operating system), mobile computing, multipackage third-party libraries, Libraries, Android markets, software tools, name-based obfuscation, Java, pattern classification, Tools, repackaging detection, third-party library, security problems, feature hashing, Android, security of data, internal code dependencies, library candidate classification, Androids, precise third-party library detection]
Analysis and Testing of Notifications in Android Wear Applications
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Android Wear (AW) is Google's platform for developing applications for wearable devices. Our goal is to make a first step toward a foundation for analysis and testing of AW apps. We focus on a core feature of such apps: notifications issued by a handheld device (e.g., a smartphone) and displayed on a wearable device (e.g., a smartwatch). We first define a formal semantics of AW notifications in order to capture the core features and behavior of the notification mechanism. Next, we describe a constraint-based static analysis to build a model of this run-time behavior. We then use this model to develop a novel testing tool for AW apps. The tool contains a testing framework together with components to support AW-specific coverage criteria and to automate the generation of GUI events on the wearable. These contributions advance the state of the art in the increasingly important area of software for wearable devices.
[wearable devices, program testing, graphical user interfaces, program diagnostics, Humanoid robots, AW-specific coverage criteria, wearable computers, constraint-based static analysis, Android (operating system), Handheld computers, Android wear applications, Google platform, Semantics, handheld device, notification analysis, GUI event generation, Software, notification testing, software tools, AW apps analysis, Androids, AW apps testing, Smart phones, Testing]
Adaptive Unpacking of Android Apps
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
More and more app developers use the packing services (or packers) to prevent attackers from reverse engineering and modifying the executable (or Dex files) of their apps. At the same time, malware authors also use the packers to hide the malicious component and evade the signature-based detection. Although there are a few recent studies on unpacking Android apps, it has been shown that the evolving packers can easily circumvent them because they are not adaptive to the changes of packers. In this paper, we propose a novel adaptive approach and develop a new system, named PackerGrind, to unpack Android apps. We also evaluate PackerGrind with real packed apps, and the results show that PackerGrind can successfully reveal the packers' protection mechanisms and recover the Dex files with low overhead, showing that our approach can effectively handle the evolution of packers.
[invasive software, Subspace constraints, Humanoid robots, reverse engineering, App Unpacking, Dynamic Analysis, Android apps, signature-based detection, Runtime, Android (operating system), PackerGrind, Loading, malware authors, Data collection, Androids, adaptive unpacking, Monitoring]
Performance Diagnosis for Inefficient Loops
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Writing efficient software is difficult. Design and implementation defects can cause severe performance degradation. Unfortunately, existing performance diagnosis techniques like profilers are still preliminary. They can locate code regions that consume resources, but not the ones that waste resources. In this paper, we first design a root-cause and fix-strategy taxonomy for inefficient loops, one of the most common performance problems in the field. We then design a static-dynamic hybrid analysis tool, LDoctor, to provide accurate performance diagnosis for loops. We further use sampling techniques to lower the run-time overhead without degrading the accuracy or latency of LDoctor diagnosis. Evaluation using real-world performance problems shows that LDoctor can provide better coverage and accuracy than existing techniques, with low overhead.
[program diagnostics, LDoctor diagnosis, Taxonomy, Redundancy, Debugging, Tools, static-dynamic hybrid analysis tool, loop inefficiency, root-cause and fix-strategy taxonomy, Computer bugs, performance diagnosis, debugging, Software, software engineering, software performance degradation, Anodes]
How Do Developers Fix Cross-Project Correlated Bugs? A Case Study on the GitHub Scientific Python Ecosystem
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
GitHub, a popular social-software-development platform, has fostered a variety of software ecosystems where projects depend on one another and practitioners interact with each other. Projects within an ecosystem often have complex inter-dependencies that impose new challenges in bug reporting and fixing. In this paper, we conduct an empirical study on cross-project correlated bugs, i.e., causally related bugs reported to different projects, focusing on two aspects: 1) how developers track the root causes across projects, and 2) how the downstream developers coordinate to deal with upstream bugs. Through manual inspection of bug reports collected from the scientific Python ecosystem and an online survey with developers, this study reveals the common practices of developers and the various factors in fixing cross-project bugs. These findings provide implications for future software bug analysis in the scope of ecosystem, as well as shed light on the requirements of issue trackers for such bugs.
[program debugging, project management, coordinate, Ecosystems, software development management, Maintenance engineering, Tools, GitHub ecosystems, configuration management, cross-project correlated bugs, Computer bugs, root causes tracking, Collaboration, object-oriented languages, GitHub scientific Python ecosystem, Software, bug reports, Software engineering, upstream bugs]
Feedback-Based Debugging
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Software debugging has long been regarded as a time and effort consuming task. In the process of debugging, developers usually need to manually inspect many program steps to see whether they deviate from their intended behaviors. Given that intended behaviors usually exist nowhere but in human mind, the automation of debugging turns out to be extremely hard, if not impossible. In this work, we propose a feedback-based debugging approach, which (1) builds on light-weight human feedbacks on a buggy program and (2) regards the feedbacks as partial program specification to infer suspicious steps of the buggy execution. Given a buggy program, we record its execution trace and allow developers to provide light-weight feedback on trace steps. Based on the feedbacks, we recommend suspicious steps on the trace. Moreover, our approach can further learn and approximate bug-free paths, which helps reduce required feedbacks to expedite the debugging process. We conduct an experiment to evaluate our approach with simulated feedbacks on 3409 mutated bugs across 3 open source projects. The results show that our feedback-based approach can detect 92.8% of the bugs and 65% of the detected bugs require less than 20 feedbacks. In addition, we implement our proof-of-concept tool, Microbat, and conduct a user study involving 16 participants on 3 debugging tasks. The results show that, compared to the participants using the baseline tool, Whyline, the ones using Microbat can spend on average 55.8% less time to locate the bugs.
[program debugging, slicing, approximation, feedback-based debugging, Microbat, partial program specification, Debugging, path pattern, Tools, Inspection, Software debugging, formal specification, Whyline, human feedbacks, execution trace, feedback, Reactive power, buggy execution, Computer bugs, light-weight feedback, debugging, buggy program, Software engineering]
Learning Syntactic Program Transformations from Examples
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average.
[automatic programming, code edits, syntactic program transformations learning, programming-by-example methodology, Program transformation, Tools, program synthesis, C# languages, Programming profession, Open source software, REFAZER, C# open-source projects, program processors, domain-specific language, refactoring, tutoring systems, DSL, domain-specific deductive algorithms, Pattern matching]
Precise Condition Synthesis for Program Repair
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Due to the difficulty of repairing defect, many research efforts have been devoted into automatic defect repair. Given a buggy program that fails some test cases, a typical automatic repair technique tries to modify the program to make all tests pass. However, since the test suites in real world projects are usually insufficient, aiming at passing the test suites often leads to incorrect patches. This problem is known as weak test suites or overfitting. In this paper we aim to produce precise patches, that is, any patch we produce has a relatively high probability to be correct. More concretely, we focus on condition synthesis, which was shown to be able to repair more than half of the defects in existing approaches. Our key insight is threefold. First, it is important to know what variables in a local context should be used in an "if" condition, and we propose a sorting method based on the dependency relations between variables. Second, we observe that the API document can be used to guide the repair process, and propose document analysis technique to further filter the variables. Third, it is important to know what predicates should be performed on the set of variables, and we propose to mine a set of frequently used predicates in similar contexts from existing projects. Based on the insight, we develop a novel program repair system, ACS, that could generate precise conditions at faulty locations. Furthermore, given the generated conditions are very precise, we can perform a repair operation that is previously deemed to be too overfitting: directly returning the test oracle to repair the defect. Using our approach, we successfully repaired 18 defects on four projects of Defects4J, which is the largest number of fully automatically repaired defects reported on the dataset so far. More importantly, the precision of our approach in the evaluation is 78.3%, which is significantly higher than previous approaches, which are usually less than 40%.
[document analysis technique, Text analysis, application program interfaces, program testing, Input variables, weak test suites, sorting, Benchmark testing, faulty locations, Defects4J, dependency relations, document handling, automatic defect repair, Java, repair operation, precise condition synthesis, sorting method, fault location, Maintenance engineering, software maintenance, program repair system, API document, ACS, Software, Software engineering]
Heuristically Matching Solution Spaces of Arithmetic Formulas to Efficiently Reuse Solutions
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Many symbolic program analysis techniques rely on SMT solvers to verify properties of programs. Despite the remarkable progress made in the development of such tools, SMT solvers still represent a main bottleneck to the scalability of these techniques. Recent approaches tackle this bottleneck by reusing solutions of formulas that recur during program analysis, thus reducing the number of queries to SMT solvers. Current approaches only reuse solutions across formulas that are equivalent to, contained in or implied by other formulas, as identified through a set of predefined rules, and cannot reuse solutions across formulas that differ in their structure, even if they share some potentially reusable solutions. In this paper, we propose a novel approach that can reuse solutions across formulas that share at least one solution, regardless of their structural resemblance. Our approach exploits a novel heuristic to efficiently identify solutions computed for previously solved formulas and most likely shared by new formulas. The results of an empirical evaluation of our approach on two different logics show that our approach can identify on average more reuse opportunities and is markedly faster than competing approaches.
[Terminology, Scalability, Heuristic algorithms, program diagnostics, Mission critical systems, Tools, computability, SMT-based program analysis, SMT solvers, solution reuse, heuristically matching solution spaces, symbolic program analysis techniques, Operating systems, symbolic execution, arithmetic formulas, Software engineering, structural resemblance]
Exploring API Embedding for API Usages and Applications
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Word2Vec is a class of neural network models that as being trainedfrom a large corpus of texts, they can produce for each unique word acorresponding vector in a continuous space in which linguisticcontexts of words can be observed. In this work, we study thecharacteristics of Word2Vec vectors, called API2VEC or API embeddings, for the API elements within the API sequences in source code. Ourempirical study shows that the close proximity of the API2VEC vectorsfor API elements reflects the similar usage contexts containing thesurrounding APIs of those API elements. Moreover, API2VEC can captureseveral similar semantic relations between API elements in API usagesvia vector offsets. We demonstrate the usefulness of API2VEC vectorsfor API elements in three applications. First, we build a tool thatmines the pairs of API elements that share the same usage relationsamong them. The other applications are in the code migrationdomain. We develop API2API, a tool to automatically learn the APImappings between Java and C# using a characteristic of the API2VECvectors for API elements in the two languages: semantic relationsamong API elements in their usages are observed in the two vectorspaces for the two languages as similar geometric arrangements amongtheir API2VEC vectors. Our empirical evaluation shows that API2APIrelatively improves 22.6% and 40.1% top-1 and top-5 accuracy over astate-of-the-art mining approach for API mappings. Finally, as anotherapplication in code migration, we are able to migrate equivalent APIusages from Java to C# with up to 90.6% recall and 87.2% precision.
[source code (software), C#, text analysis, application program interfaces, code migration, data mining, API embedding, C# languages, API elements, vector offsets, linguistic words contexts, Semantics, neural network models, Word2Vec, geometric arrangements, API applications, Java, semantic relations, API mappings, text corpus, API sequences, source code, Tools, API2VEC, vectors, Neural networks, API usages, Syntactics, migration, geometry, Word2Vec vectors, code migration domain, neural nets]
Unsupervised Software-Specific Morphological Forms Inference from Informal Discussions
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Informal discussions on social platforms (e.g., Stack Overflow) accumulates a large body of programming knowledge in natural language text. Natural language process (NLP) techniques can be exploited to harvest this knowledge base for software engineering tasks. To make an effective use of NLP techniques, consistent vocabulary is essential. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms in informal discussions, such as abbreviations, synonyms and misspellings. Existing techniques to deal with such morphological forms are either designed for general English or predominantly rely on domain-specific lexical rules. A thesaurus of software-specific terms and commonly-used morphological forms is desirable for normalizing software engineering text, but very difficult to build manually. In this work, we propose an automatic approach to build such a thesaurus. Our approach identifies software-specific terms by contrasting software-specific and general corpuses, and infers morphological forms of software-specific terms by combining distributed word semantics, domain-specific lexical rules and transformations, and graph analysis of morphological relations. We evaluate the coverage and accuracy of the resulting thesaurus against community-curated lists of software-specific terms, abbreviations and synonyms. We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus. We demonstrate the usefulness of our thesaurus in a case study of normalizing questions from Stack Overflow and CodeProject.
[Electronic publishing, text analysis, thesauri, Dictionaries, natural language processing, software-specific terms, Encyclopedias, thesaurus, morphological form, unsupervised software-specific morphological forms inference, Thesauri, abbreviation, Stack Overflow, synonym, word embedding, domain-specific lexical rules, informal discussions, natural language process, NLP techniques, software engineering, software engineering tasks, Internet, Software engineering]
SPAIN: Security Patch Analysis for Binaries towards Understanding the Pain and Pills
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Software vulnerability is one of the major threats to software security. Once discovered, vulnerabilities are often fixed by applying security patches. In that sense, security patches carry valuable information about vulnerabilities, which could be used to discover, understand and fix (similar) vulnerabilities. However, most existing patch analysis approaches work at the source code level, while binary-level patch analysis often heavily relies on a lot of human efforts and expertise. Even worse, some vulnerabilities may be secretly patched without applying CVE numbers, or only the patched binary programs are available while the patches are not publicly released. These practices greatly hinder patch analysis and vulnerability analysis. In this paper, we propose a scalable binary-level patch analysis framework, named SPAIN, which can automatically identify security patches and summarize patch patterns and their corresponding vulnerability patterns. Specifically, given the original and patched versions of a binary program, we locate the patched functions and identify the changed traces (i.e., a sequence of basic blocks) that may contain security or non-security patches. Then we identify security patches through a semantic analysis of these traces and summarize the patterns through a taint analysis on the patched functions. The summarized patterns can be used to search similar patches or vulnerabilities in binary programs. Our experimental results on several real-world projects have shown that: i) SPAIN identified security patches with high accuracy and high scalability, ii) SPAIN summarized 5 patch patterns and their corresponding vulnerability patterns for 5 vulnerability types, and iii) SPAIN discovered security patches that were not documented, and discovered 3 zero-day vulnerabilities.
[Scalability, Tools, Portable document format, Registers, Security, binary programs, scalable binary-level patch analysis framework, security of data, security patch analysis, Semantics, Software, software engineering, software vulnerability, software security, SPAIN]
Travioli: A Dynamic Analysis for Detecting Data-Structure Traversals
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Traversal is one of the most fundamental operations on data structures, in which an algorithm systematically visits some or all of the data items of a data structure. We propose a dynamic analysis technique, called Travioli, for detecting data-structure traversals. We introduce the concept of acyclic execution contexts, which enables precise detection of traversals of arrays and linked data structures such as lists and trees in the presence of both loops and recursion. We describe how the information reported by Travioli can be used for visualizing data-structure traversals, manually generating performance regression tests, and for discovering performance bugs caused by redundant traversals. We evaluate Travioli on five real-world JavaScript programs. In our experiments, Travioli produced fewer than 4% false positives. We were able to construct performance tests for 93.75% of the reported true traversals. Travioli also found two asymptotic performance bugs in widely used JavaScript frameworks D3 and express.
[Java, Travioli, dynamic analysis, real-world JavaScript programs, acyclic execution contexts, performance bugs, Reactive power, Computer bugs, Data visualization, data-structure traversals, system monitoring, data structures, Performance analysis, regression tests, Arrays, Lenses]
ProEva: Runtime Proactive Performance Evaluation Based on Continuous-Time Markov Chains
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Software systems, especially service-based software systems, need to guarantee runtime performance. If their performance is degraded, some reconfiguration countermeasures should be taken. However, there is usually some latency before the countermeasures take effect. It is thus important not only to monitor the current system status passively but also to predict its future performance proactively. Continuous-time Markov chains (CTMCs) are suitable models to analyze time-bounded performance metrics (e.g., how likely a performance degradation may occur within some future period). One challenge to harness CTMCs is the measurement of model parameters (i.e., transition rates) in CTMCs at runtime. As these parameters may be updated by the system or environment frequently, it is difficult for the model builder to provide precise parameter values. In this paper, we present a framework called ProEva, which extends the conventional technique of time-bounded CTMC model checking by admitting imprecise, interval-valued estimates for transition rates. The core method of ProEva computes asymptotic expressions and bounds for the imprecise model checking output. We also present an evaluation of accuracy and computational overhead for ProEva.
[continuous-time Markov chain, Quality-of-Service, continuous-time Markov chains, time-bounded CTMC model checking, imprecise parameters, formal verification, performance, Markov processes, service-based software systems, software performance evaluation, ProEva, runtime proactive performance evaluation, Software engineering]
Glacier: Transitive Class Immutability for Java
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Though immutability has been long-proposed as a way to prevent bugs in software, little is known about how to make immutability support in programming languages effective for software engineers. We designed a new formalism that extends Java to support transitive class immutability, the form of immutability for which there is the strongest empirical support, and implemented that formalism in a tool called Glacier. We applied Glacier successfully to two real-world systems. We also compared Glacier to Java's final in a user study of twenty participants. We found that even after being given instructions on how to express immutability with final, participants who used final were unable to express immutability correctly, whereas almost all participants who used Glacier succeeded. We also asked participants to make specific changes to immutable classes and found that participants who used final all incorrectly mutated immutable state, whereas almost all of the participants who used Glacier succeeded. Glacier represents a promising approach to enforcing immutability in Java and provides a model for enforcement in other languages.
[immutability, Java, program debugging, transitive class immutability, Tools, software bugs prevention, programming languages, Runtime, programming language usability, Glacier tool, Computer bugs, empirical studies of programmers, software tools, Usability]
Challenges for Static Analysis of Java Reflection - Literature Review and Empirical Study
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
The behavior of software that uses the Java Reflection API is fundamentally hard to predict by analyzing code. Only recent static analysis approaches can resolve reflection under unsound yet pragmatic assumptions. We survey what approaches exist and what their limitations are. We then analyze how real-world Java code uses the Reflection API, and how many Java projects contain code challenging state-of-the-art static analysis. Using a systematic literature review we collected and categorized all known methods of statically approximating reflective Java code. Next to this we constructed a representative corpus of Java systems and collected descriptive statistics of the usage of the Reflection API. We then applied an analysis on the abstract syntax trees of all source code to count code idioms which go beyond the limitation boundaries of static analysis approaches. The resulting data answers the research questions. The corpus, the tool and the results are openly available. We conclude that the need for unsound assumptions to resolve reflection is widely supported. In our corpus, reflection can not be ignored for 78% of the projects. Common challenges for analysis tools such as non-exceptional exceptions, programmatic filtering meta objects, semantics of collections, and dynamic proxies, widely occur in the corpus. For Java software engineers prioritizing on robustness, we list tactics to obtain more easy to analyze reflection code, and for static analysis tool builders we provide a list of opportunities to have significant impact on real Java code.
[source code (software), application program interfaces, public domain software, computational linguistics, real-world Java code analysis, abstract syntax trees, collected descriptive statistics, Systematics, Bibliographies, Semantics, reflective Java code, programmatic filtering meta objects, Empirical Study, software tools, Static Analysis, nonexceptional exceptions, Java, program diagnostics, collections semantics, trees (mathematics), Systematic Literature Review, Java systems, source code, Tools, Reflection, Grammar, literature review, Java projects, reflection code analysis, Java Reflection API, dynamic proxies, software behavior, code idioms, Software, static analysis tool]
Machine-Learning-Guided Selectively Unsound Static Analysis
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
We present a machine-learning-based technique for selectively applying unsoundness in static analysis. Existing bug-finding static analyzers are unsound in order to be precise and scalable in practice. However, they are uniformly unsound and hence at the risk of missing a large amount of real bugs. By being sound, we can improve the detectability of the analyzer but it often suffers from a large number of false alarms. Our approach aims to strike a balance between these two approaches by selectively allowing unsoundness only when it is likely to reduce false alarms, while retaining true alarms. We use an anomaly-detection technique to learn such harmless unsoundness. We implemented our technique in two static analyzers for full C. One is for a taint analysis for detecting format-string vulnerabilities, and the other is for an interval analysis for buffer-overflow detection. The experimental results show that our approach significantly improves the recall of the original unsound analysis without sacrificing the precision.
[Scalability, program diagnostics, machine-learning-guided selectively unsound static Analysis, Tools, buffer-overflow detection, bug-finding static analyzers, Machine Learning, Support vector machines, taint analysis, Computer bugs, interval analysis, Benchmark testing, Bug-finding, Libraries, learning (artificial intelligence), Static Analysis, Software engineering, anomaly-detection technique]
How Good Is a Security Policy against Real Breaches? A HIPAA Case Study
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Policy design is an important part of software development. As security breaches increase in variety, designing a security policy that addresses all potential breaches becomes a nontrivial task. A complete security policy would specify rules to prevent breaches. Systematically determining which, if any, policy clause has been violated by a reported breach is a means for identifying gaps in a policy. Our research goal is to help analysts measure the gaps between security policies and reported breaches by developing a systematic process based on semantic reasoning. We propose SEMAVER, a framework for determining coverage of breaches by policies via comparison of individual policy clauses and breach descriptions. We represent a security policy as a set of norms. Norms (commitments, authorizations, and prohibitions) describe expected behaviors of users, and formalize who is accountable to whom and for what. A breach corresponds to a norm violation. We develop a semantic similarity metric for pairwise comparison between the norm that represents a policy clause and the norm that has been violated by a reported breach. We use the US Health Insurance Portability and Accountability Act (HIPAA) as a case study. Our investigation of a subset of the breaches reported by the US Department of Health and Human Services (HHS) reveals the gaps between HIPAA and reported breaches, leading to a coverage of 65%. Additionally, our classification of the 1,577 HHS breaches shows that 44% of the breaches are accidental misuses and 56% are malicious misuses. We find that HIPAA's gaps regarding accidental misuses are significantly larger than its gaps regarding malicious misuses.
[Measurement, Taxonomy, Medical services, Ontologies, semantic similarity metric, semantic reasoning, semantic similarity, Cognition, Security, HHS, Semantics, HIPAA, breach ontology, norm violation, security breaches, software engineering, SEMAVER, software development, inference mechanisms, security of data, US Department of Health and Human Services, security policies, US Health Insurance Portability and Accountability Act, social norms, accidental misuses, malicious misuses, Security and privacy breaches]
Adaptive Coverage and Operational Profile-Based Testing for Reliability Improvement
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
We introduce covrel, an adaptive software testing approach based on the combined use of operational profile and coverage spectrum, with the ultimate goal of improving the delivered reliability of the program under test. Operational profile-based testing is a black-box technique that selects test cases having the largest impact on failure probability in operation, as such, it is considered well suited when reliability is a major concern. Program spectrum is a characterization of a program's behavior in terms of the code entities (e.g., branches, statements, functions) that are covered as the program executes. The driving idea of covrel is to complement operational profile information with white-box coverage measures based on count spectra, so as to dynamically select the most effective test cases for reliability improvement. In particular, we bias operational profile-based test selection towards those entities covered less frequently. We assess the approach by experiments with 18 versions from 4 subjects commonly used in software testing research, comparing results with traditional operational and coverage testing. Results show that exploiting operational and coverage data in a combined adaptive way actually pays in terms of reliability improvement, with covrel overcoming conventional operational testing in more than 80% of the cases.
[Software testing, count spectra, program testing, software reliability, Operational profile, Reliability engineering, reliability improvement, black-box technique, operational profile-based test selection, covrel, Optimization, program spectrum, code entities, adaptive coverage testing, Testing, Operational coverage, coverage spectrum, program behavior, operational profile, probability, white-box coverage measures, Test case selection, program-under-test, Software reliability, adaptive software testing approach, coverage data, operational profile-based testing, operational data, failure probability, Resource management, Reliability, Program count spectrum]
RADAR: A Lightweight Tool for Requirements and Architecture Decision Analysis
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Uncertainty and conflicting stakeholders' objectives make many requirements and architecture decisions particularly hard. Quantitative probabilistic models allow software architects to analyse such decisions using stochastic simulation and multi-objective optimisation, but the difficulty of elaborating the models is an obstacle to the wider adoption of such techniques. To reduce this obstacle, this paper presents a novel modelling language and analysis tool, called RADAR, intended to facilitate requirements and architecture decision analysis. The language has relations to quantitative AND/OR goal models used in requirements engineering and to feature models used in software product lines. However, it simplifies such models to a minimum set of language constructs essential for decision analysis. The paper presents RADAR's modelling language, automated support for decision analysis, and evaluates its application to four real-world examples.
[Requirements and Architecture Decision Analyser, RADAR, Software Architecture, requirements decision analysis, Expected Value of Information, Optimization, modelling language, Analytical models, software architecture, architecture decision analysis, Radar, Computer architecture, Mathematical model, Monte-Carlo Simulation, software product lines, Goal Modelling, Search-Based Software Engineering, Decision Analysis, Requirements Engineering, Decision analysis, requirements engineering, systems analysis, Software, quantitative AND/OR goal models, Multi-Objective Optimisation]
PEoPL: Projectional Editing of Product Lines
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
The features of a software product line - a portfolio of system variants - can be realized using various implementation techniques (a. k. a., variability mechanisms). Each technique represents the software artifacts of features differently, typically classified into annotative (e.g., C preprocessor) and modular representations (e.g., feature modules), each with distinct advantages and disadvantages. Annotative representations are easy to realize, but annotations clutter source code and hinder program comprehension. Modular representations support comprehension, but are difficult to realize. Most importantly, to engineer feature artifacts, developers need to choose one representation and adhere to it for evolving and maintaining the same artifacts. We present PEoPL, an approach to combine the advantages of annotative and modular representations. When engineering a feature artifact, developers can choose the most-suited representation and even use different representations in parallel. PEoPL relies on separating a product line into an internal and external representation, the latter by providing editable projections used by the developers. We contribute a programming-language-independent internal representation of variability, five editable projections reflecting different variability representations, a supporting IDE, and a tailoring of PEoPL to Java. We evaluate PEoPL's expressiveness, scalability, and flexibility in eight Java-based product lines, finding that all can be realized, that projections are feasible, and that variant computation is fast (&lt;;45ms on average for our largest subject Berkeley DB).
[software product lines, Java, Visualization, projectional editing of product lines, Latches, Switches, Clutter, software product line, software artifacts, programming-language-independent internal representation, modular representations, Software, PEoPL, Java-based product lines, Bars]
Do Developers Read Compiler Error Messages?
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
In integrated development environments, developers receive compiler error messages through a variety of textual and visual mechanisms, such as popups and wavy red underlines. Although error messages are the primary means of communicating defects to developers, researchers have a limited understanding on how developers actually use these messages to resolve defects. To understand how developers use error messages, we conducted an eye tracking study with 56 participants from undergraduate and graduate software engineering courses at our university. The participants attempted to resolve common, yet problematic defects in a Java code base within the Eclipse development environment. We found that: 1) participants read error messages and the difficulty of reading these messages is comparable to the difficulty of reading source code, 2) difficulty reading error messages significantly predicts participants' task performance, and 3) participants allocate a substantial portion of their total task to reading error messages (13%-25%). The results of our study offer empirical justification for the need to improve compiler error messages for developers.
[visual attention, Java, Visualization, Google, Navigation, compiler error messages, source code, integrated development environments, reading, software engineering courses, Java code base, program compilers, eye tracking, Gaze tracking, Libraries, programmer comprehension, Eclipse development environment, compiler errors, Software engineering]
A General Framework for Dynamic Stub Injection
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Stub testing is a standard technique to simulate the behavior of dependencies of an application under test such as the file system. Even though existing frameworks automate the actual stub injection, testers typically have to implement manually where and when to inject stubs, in addition to the stub behavior. This paper presents a novel framework that reduces this effort. The framework provides a domain specific language to describe stub injection strategies and stub behaviors via declarative rules, as well as a tool that automatically injects stubs dynamically into binary code according to these rules. Both the domain specific language and the injection are language independent, which enables the reuse of stubs and injection strategies across applications. We implemented this framework for both unmanaged (assembly) and managed (.NET) code and used it to perform fault injection for twelve large applications, which revealed numerous crashes and bugs in error handling code. We also show how to prioritize the analysis of test failures based on a comparison of the effectiveness of stub injection rules across applications.
[binary codes, program testing, Instruments, program diagnostics, Debugging, declarative rules, stub injection strategies, domain specific language, stub testing, Runtime, Computer bugs, specification languages, fault injection, binary code, DSL, Testing]
An Empirical Study on Mutation, Statement and Branch Coverage Fault Revelation That Avoids the Unreliable Clean Program Assumption
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Many studies suggest using coverage concepts, such as branch coverage, as the starting point of testing, while others as the most prominent test quality indicator. Yet the relationship between coverage and fault-revelation remains unknown, yielding uncertainty and controversy. Most previous studies rely on the Clean Program Assumption, that a test suite will obtain similar coverage for both faulty and fixed ('clean') program versions. This assumption may appear intuitive, especially for bugs that denote small semantic deviations. However, we present evidence that the Clean Program Assumption does not always hold, thereby raising a critical threat to the validity of previous results. We then conducted a study using a robust experimental methodology that avoids this threat to validity, from which our primary finding is that strong mutation testing has the highest fault revelation of four widely-used criteria. Our findings also revealed that fault revelation starts to increase significantly only once relatively high levels of coverage are attained.
[Java, test quality indicator, Correlation, program testing, code coverage, Tools, mutation testing, software quality, semantic deviations, Mutation testing, Standards, software fault tolerance, test adequacy, real faults, unreliable clean program assumption avoidance, branch coverage fault revelation, test effectiveness, Robustness, Testing]
Evaluating and Improving Fault Localization
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Most fault localization techniques take as input a faulty program, and produce as output a ranked list of suspicious code locations at which the program may be defective. When researchers propose a new fault localization technique, they typically evaluate it on programs with known faults. The technique is scored based on where in its output list the defective code appears. This enables the comparison of multiple fault localization techniques to determine which one is better. Previous research has evaluated fault localization techniques using artificial faults, generated either by mutation tools or manually. In other words, previous research has determined which fault localization techniques are best at finding artificial faults. However, it is not known which fault localization techniques are best at finding real faults. It is not obvious that the answer is the same, given previous work showing that artificial faults have both similarities to and differences from real faults. We performed a replication study to evaluate 10 claims in the literature that compared fault localization techniques (from the spectrum-based and mutation-based families). We used 2995 artificial faults in 6 real-world programs. Our results support 7 of the previous claims as statistically significant, but only 3 as having non-negligible effect sizes. Then, we evaluated the same 10 claims, using 310 real faults from the 6 programs. Every previous result was refuted or was statistically and practically insignificant. Our experiments show that artificial faults are not useful for predicting which fault localization techniques perform best on real faults. In light of these results, we identified a design space that includes many previously-studied fault localization techniques as well as hundreds of new techniques. We experimentally determined which factors in the design space are most important, using an overall set of 395 real faults. Then, we extended this design space with new techniques. Several of our novel techniques outperform all existing techniques, notably in terms of ranking defective code in the top-5 or top-10 reports.
[source code (software), Java, program testing, defective code, Debugging, Manuals, Maintenance engineering, Tools, suspicious code locations, fault localization techniques, Computer bugs, Focusing, mutation-based families, replication study, design space, faulty program, spectrum-based families]
Syntactic and Semantic Differencing for Combinatorial Models of Test Designs
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Combinatorial test design (CTD) is an effective test design technique, considered to be a testing best practice. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. As the system under test evolves, e.g., due to iterative development processes and bug fixing, so does the test space, and thus, in the context of CTD, evolution translates into frequent manual model definition updates. Manually reasoning about the differences between versions of real-world models following such updates is infeasible due to their complexity and size. Moreover, representing the differences is challenging. In this work, we propose a first syntactic and semantic differencing technique for combinatorial models of test designs. We define a concise and canonical representation for differences between two models, and suggest a scalable algorithm for automatically computing and presenting it. We use our differencing technique to analyze the evolution of 42 real-world industrial models, demonstrating its applicability and scalability. Further, a user study with 16 CTD practitioners shows that comprehension of differences between real-world combinatorial model versions is challenging and that our differencing tool significantly improves the performance of less experienced practitioners. The analysis and user study provide evidence for the potential usefulness of our differencing approach. Our work advances the state-of-the-art in CTD with better capabilities for change comprehension and management.
[Analytical models, CTD practitioners, program testing, Computational modeling, Semantics, Binary decision diagrams, Tools, Syntactics, combinatorial test design, Data models, automatic test plan generation]
Balancing Soundness and Efficiency for Practical Testing of Configurable Systems
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Testing configurable systems is important and challenging due to the enormous space of configurations where errors can hide. Existing approaches to test these systems are often costly or unreliable. This paper proposes S-SPLat, a technique that combines heuristic sampling with symbolic search to obtain both breadth and depth in the exploration of the configuration space. S-SPLat builds on SPLat, our previously developed technique, that explores all reachable configurations from tests. In contrast to its predecessor, S-SPLat sacrifices soundness in favor of efficiency. We evaluated our technique on eight software product lines of various sizes and on a large configurable system - GCC. Considering the results for GCC, S-SPLat was able to reproduce all five bugs that we previously found in a previous study with SPLat but much faster and it was able to find two new bugs in a recent release of GCC. Results suggest that it is preferable to use a combination of simple heuristics to drive the symbolic search as opposed to a single heuristic. S-SPLat and our experimental infrastructure are publicly available.
[software product lines, heuristic sampling, program testing, configuration, GCC, sampling, testing, Software product lines, Complexity theory, symbolic search, configurable system testing, configuration space, Computer bugs, Space exploration, Reliability, Testing, S-SPLat technique]
Automatic Text Input Generation for Mobile Testing
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Many designs have been proposed to improve the automated mobile testing. Despite these improvements, providing appropriate text inputs remains a prominent obstacle, which hinders the large-scale adoption of automated testing approaches. The key challenge is how to automatically produce the most relevant text in a use case context. For example, a valid website address should be entered in the address bar of a mobile browser app to continue the testing of the app, a singer's name should be entered in the search bar of a music recommendation app. Without the proper text inputs, the testing would get stuck. We propose a novel deep learning based approach to address the challenge, which reduces the problem to a minimization problem. Another challenge is how to make the approach generally applicable to both the trained apps and the untrained apps. We leverage the Word2Vec model to address the challenge. We have built our approaches as a tool and evaluated it with 50 iOS mobile apps including Firefox and Wikipedia. The results show that our approach significantly outperforms existing automatic text input generation methods.
[text analysis, program testing, deep learning based approach, Neurons, Predictive models, Mobile communication, Wikipedia, music recommendation app, iOS mobile apps, mobile browser app, Web site address, automatic text input generation method, Biological neural networks, Firefox, Training, Word2Vec model, mobile computing, automated mobile testing, minimization problem, learning (artificial intelligence), Testing, Context modeling]
A Test-Suite Diagnosability Metric for Spectrum-Based Fault Localization Approaches
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Current metrics for assessing the adequacy of a test-suite plainly focus on the number of components (be it lines, branches, paths) covered by the suite, but do not explicitly check how the tests actually exercise these components and whether they provide enough information so that spectrum-based fault localization techniques can perform accurate fault isolation. We propose a metric, called DDU, aimed at complementing adequacy measurements by quantifying a test-suite's diagnosability, i.e., the effectiveness of applying spectrum-based fault localization to pinpoint faults in the code in the event of test failures. Our aim is to increase the value generated by creating thorough test-suites, so they are not only regarded as error detection mechanisms but also as effective diagnostic aids that help widely-used fault-localization techniques to accurately pinpoint the location of bugs in the system. Our experiments show that optimizing a test suite with respect to DDU yields a 34% gain in spectrum-based fault localization report accuracy when compared to the standard branch-coverage metric.
[program debugging, fault diagnosis, branch-coverage metric, Density measurement, program testing, Cognition, Coverage, error detection, density-diversity-uniqueness, fault isolation, Diagnosability, error detection mechanisms, bugs, test-suite diagnosability metric, DDU, Computer bugs, Measurement uncertainty, Gain measurement, Software, spectrum-based fault localization techniques, Testing]
Automated Transplantation and Differential Testing for Clones
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Code clones are common in software. When applying similar edits to clones, developers often find it difficult to examine the runtime behavior of clones. The problem is exacerbated when some clones are tested, while their counterparts are not. To reuse tests for similar but not identical clones, Grafter transplants one clone to its counterpart by (1) identifying variations in identifier names, types, and method call targets, (2) resolving compilation errors caused by such variations through code transformation, and (3) inserting stub code to transfer input data and intermediate output values for examination. To help developers examine behavioral differences between clones, Grafter supports fine-grained differential testing at both the test outcome level and the intermediate program state level. In our evaluation on three open source projects, Grafter successfully reuses tests in 94% of clone pairs without inducing build errors, demonstrating its automated code transplantation capability. To examine the robustness of G RAFTER, we systematically inject faults using a mutation testing tool, Major, and detect behavioral differences induced by seeded faults. Compared with a static cloning bug finder, Grafter detects 31% more mutants using the test-level comparison and almost 2X more using the state-level comparison. This result indicates that Grafter should effectively complement static cloning bug finders.
[input data transfer, code transformation, program testing, automated differential testing, behavioral differences detection, public domain software, Test Reuse, intermediate output value transfer, targets, Code Clones, mutation testing tool, Runtime, static cloning bug finders, Grafter, test outcome level, Safety, fault injection, automated transplantation testing, Testing, Java, runtime behavior, open source projects, test-level comparison, program diagnostics, Cloning, compilation errors, stub code insertion, state-level comparison, Code Transplantation, code clones, variation identification, Major, test reuse, clone pairs, Computer bugs, intermediate program state level, software reusability, Software, identifier names, Differential Testing]
Code Defenders: Crowdsourcing Effective Tests and Subtle Mutants with a Mutation Testing Game
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Writing good software tests is difficult and not every developer's favorite occupation. Mutation testing aims to help by seeding artificial faults (mutants) that good tests should identify, and test generation tools help by providing automatically generated tests. However, mutation tools tend to produce huge numbers of mutants, many of which are trivial, redundant, or semantically equivalent to the original program, automated test generation tools tend to produce tests that achieve good code coverage, but are otherwise weak and have no clear purpose. In this paper, we present an approach based on gamification and crowdsourcing to produce better software tests and mutants: The Code Defenders web-based game lets teams of players compete over a program, where attackers try to create subtle mutants, which the defenders try to counter by writing strong tests. Experiments in controlled and crowdsourced scenarios reveal that writing tests as part of the game is more enjoyable, and that playing Code Defenders results in stronger test suites and mutants than those produced by automated tools.
[Crowdsourcing, Code Defenders, crowdsourcing, program testing, software testing, Tools, mutation testing, gamification, mutation testing game, Computer bugs, computer games, Games, Writing, Software, Internet, software tests, Web-based game, Testing]
Optimizing Test Placement for Module-Level Regression Testing
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Modern build systems help increase developer productivity by performing incremental building and testing. These build systems view a software project as a group of interdependent modules and perform regression test selection at the module level. However, many large software projects have imprecise dependency graphs that lead to wasteful test executions. If a test belongs to a module that has more dependencies than the actual dependencies of the test, then it is executed unnecessarily whenever a code change impacts those additional dependencies. In this paper, we formulate the problem of wasteful test executions due to suboptimal placement of tests in modules. We propose a greedy algorithm to reduce the number of test executions by suggesting test movements while considering historical build information and actual dependencies of tests. We have implemented our technique, called TestOptimizer, on top of CloudBuild, the build system developed within Microsoft over the last few years. We have evaluated the technique on five large proprietary projects. Our results show that the suggested test movements can lead to a reduction of 21.66 million test executions (17.09%) across all our subject projects. We received encouraging feedback from the developers of these projects; they accepted and intend to implement &#x2248;80% of our reported suggestions.
[Greedy algorithms, Productivity, Google, program testing, greedy algorithms, Buildings, regression analysis, Metadata, TestOptimizer, CloudBuild, regression test selection, module-level regression testing, software project, test placement optimization, Software, greedy algorithm, dependency graphs, Testing, build system]
Learning to Prioritize Test Programs for Compiler Testing
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Compiler testing is a crucial way of guaranteeing the reliability of compilers (and software systems in general). Many techniques have been proposed to facilitate automated compiler testing. These techniques rely on a large number of test programs (which are test inputs of compilers) generated by some test-generation tools (e.g., CSmith). However, these compiler testing techniques have serious efficiency problems as they usually take a long period of time to find compiler bugs. To accelerate compiler testing, it is desirable to prioritize the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. In this paper, we propose the idea of learning to test, which learns the characteristics of bug-revealing test programs from previous test programs that triggered bugs. Based on the idea of learning to test, we propose LET, an approach to prioritizing test programs for compiler testing acceleration. LET consists of a learning process and a scheduling process. In the learning process, LET identifies a set of features of test programs, trains a capability model to predict the probability of a new test program for triggering compiler bugs and a time model to predict the execution time of a test program. In the scheduling process, LET prioritizes new test programs according to their bug-revealing probabilities in unit time, which is calculated based on the two trained models. Our extensive experiments show that LET significantly accelerates compiler testing. In particular, LET reduces more than 50% of the testing time in 24.64% of the cases, and reduces between 25% and 50% of the testing time in 36.23% of the cases.
[program testing, learning process, software reliability, software systems, Predictive models, test-generation tools, bug-revealing probabilities, time model, program compilers, compiler reliability, bug-revealing test programs, Training, Program processors, compiler bugs, Computer bugs, Life estimation, scheduling, Feature extraction, scheduling process, LET, learning (artificial intelligence), automated compiler testing, Testing]
What Causes My Test Alarm? Automatic Cause Analysis for Test Alarms in System and Integration Testing
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Driven by new software development processes and testing in clouds, system and integration testing nowadays tends to produce enormous number of alarms. Such test alarms lay an almost unbearable burden on software testing engineers who have to manually analyze the causes of these alarms. The causes are critical because they decide which stakeholders are responsible to fix the bugs detected during the testing. In this paper, we present a novel approach that aims to relieve the burden by automating the procedure. Our approach, called Cause Analysis Model, exploits information retrieval techniques to efficiently infer test alarm causes based on test logs. We have developed a prototype and evaluated our tool on two industrial datasets with more than 14,000 test alarms. Experiments on the two datasets show that our tool achieves an accuracy of 58.3% and 65.8%, respectively, which outperforms the baseline algorithms by up to 13.3%. Our algorithm is also extremely efficient, spending about 0.1s per cause analysis. Due to the attractive experimental results, our industrial partner, a leading information and communication technology company in the world, has deployed the tool and it achieves an average accuracy of 72% after two months of running, nearly three times more accurate than a previous strategy based on regular expressions.
[Software testing, Product codes, program testing, Instruments, program diagnostics, software testing, information retrieval, system and integration testing, test alarm analysis, test logs, Analytical models, multiclass classification, Computer bugs, automatic cause analysis, information retrieval techniques, Software, cause analysis model, test alarms, SIT]
Symbolic Model Extraction for Web Application Verification
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Modern web applications use complex data models and access control rules which lead to data integrity and access control errors. One approach to find such errors is to use formal verification techniques. However, as a first step, most formal verification techniques require extraction of a formal model which is a difficult problem in itself due to dynamic features of modern languages, and it is typically done either manually, or using ad hoc techniques. In this paper, we present a technique called symbolic model extraction for extracting formal data models from web applications. The key ideas of symbolic model extraction are 1) to use the source language interpreter for model extraction, which enables us to handle dynamic features of the language, 2) to use code instrumentation so that execution of each instrumented piece of code returns the formal model that corresponds to that piece of code, 3) to instrument the code dynamically so that the models of methods that are created at runtime can also be extracted, and 4) to execute both sides of branches during instrumented execution so that all program behaviors can be covered in a single instrumented execution. We implemented the symbolic model extraction technique for the Rails framework and used it to extract data and access control models from web applications. Our experiments demonstrate that symbolic model extraction is scalable and extracts formal models that are precise enough to find bugs in real-world applications without reporting too many false positives.
[source language interpreter, formal verification techniques, Model Extraction, Instruments, program behaviors, data integrity, Data mining, Rails, access control rules, Runtime, Formal Verification, formal verification, symbolic model extraction technique, Web application verification, authorisation, Feature extraction, Data models, complex data models, Web Applications, Load modeling]
UML Diagram Refinement (Focusing on Class-and Use Case Diagrams)
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Large and complicated UML models are not useful, because they are difficult to understand. This problem can be solved by using several diagrams of the same system at different levels of abstraction. Unfortunately, UML does not define an explicit set of rules for ensuring that diagrams at different levels of abstraction are consistent. We define such a set of rules, that we call diagram refinement. Diagram refinement is intuitive, and applicable to several kinds of UML diagrams (mostly to structural diagrams but also to use case diagrams), yet it rests on a solid mathematical basis-the theory of graph homomorphisms. We illustrate its usefulness with a series of examples.
[graph homomorphisms, Graph homomorphism, Unified Modeling Language, Class diagram, Design patterns, Unified modeling language, graph theory, Lattices, Refinement, Electronic mail, UML diagram refinement, Semantics, UML, Concrete, Mathematical model, Use case diagram, Software engineering]
Fuzzy Fine-Grained Code-History Analysis
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Existing software-history techniques represent source-code evolution as an absolute and unambiguous mapping of lines of code in prior revisions to lines of code in subsequent revisions. However, the true evolutionary lineage of a line of code is often complex, subjective, and ambiguous. As such, existing techniques are predisposed to, both, overestimate and underestimate true evolution lineage. In this paper, we seek to address these issues by providing a more expressive model of code evolution, the fuzzy history graph, by representing code lineage as a continuous (i.e., fuzzy) metric rather than a discrete (i.e., absolute) one. Using this more descriptive model, we additionally provide a novel multi-revision code-history analysis - fuzzy history slicing. In our experiments over three real-world software systems, we found that the fuzzy history graph provides a tunable balance of precision and recall, and an overall improved accuracy over existing code-evolution models. Furthermore, we found that the use of such a fuzzy model of history provided improved accuracy for code-history analysis tasks.
[Measurement, novel multirevision code-history analysis, Computational modeling, graph theory, code lineage, Cloning, fuzzy set theory, fuzzy fine-grained code-history analysis, fuzzy history slicing, History, software maintenance, Analytical models, Computer bugs, Solids, software engineering, computer aided software engineering, reasoning about programs, fuzzy history graph, program slicing]
To Type or Not to Type: Quantifying Detectable Bugs in JavaScript
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
JavaScript is growing explosively and is now used in large mature projects even outside the web domain. JavaScript is also a dynamically typed language for which static type systems, notably Facebook's Flow and Microsoft's TypeScript, have been written. What benefits do these static type systems provide? Leveraging JavaScript project histories, we select a fixed bug and check out the code just prior to the fix. We manually add type annotations to the buggy code and test whether Flow and TypeScript report an error on the buggy code, thereby possibly prompting a developer to fix the bug before its public release. We then report the proportion of bugs on which these type systems reported an error. Evaluating static type systems against public bugs, which have survived testing and review, is conservative: it understates their effectiveness at detecting bugs during private development, not to mention their other benefits such as facilitating code search/completion and serving as documentation. Despite this uneven playing field, our central finding is that both static type systems find an important percentage of public bugs: both Flow 0.30 and TypeScript 2.0 successfully detect 15%!.
[Java, program debugging, mining software repositories, Documentation, bug detection, History, Flow, TypeScript, Web domain, Computer bugs, Measurement uncertainty, Surgery, JavaScript, Software, Facebook, static type systems]
The Evolution of Continuous Experimentation in Software Product Development: From Data to a Data-Driven Organization at Scale
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Software development companies are increasingly aiming to become data-driven by trying to continuously experiment with the products used by their customers. Although familiar with the competitive edge that the A/B testing technology delivers, they seldom succeed in evolving and adopting the methodology. In this paper, and based on an exhaustive and collaborative case study research in a large software-intense company with highly developed experimentation culture, we present the evolution process of moving from ad-hoc customer data analysis towards continuous controlled experimentation at scale. Our main contribution is the "Experimentation Evolution Model" in which we detail three phases of evolution: technical, organizational and business evolution. With our contribution, we aim to provide guidance to practitioners on how to develop and scale continuous experimentation in software organizations with the purpose of becoming data-driven at scale.
[ad-hoc customer data analysis, Experiment Owner, software-intense company, continuous product innovation, A/B testing, experimentation evolution model, technical evolution, Experimentation Evolution Model, A/B testing technology, organizational evolution, data science, customer feedback, product development, continuous experimentation, software product development, software engineering, data-driven organization, business evolution, product value, organisational aspects, software organizations, Software engineering]
[Publisher's information]
2017 IEEE/ACM 39th International Conference on Software Engineering
None
2017
Provides a listing of current committee members and society officers.
[]
